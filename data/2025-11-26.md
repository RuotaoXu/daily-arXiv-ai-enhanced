<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 192]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PuzzlePoles: Cylindrical Fiducial Markers Based on the PuzzleBoard Pattern](https://arxiv.org/abs/2511.19448)
*Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 提出PuzzlePole：一种基于PuzzleBoard的圆柱形视觉标记，可360°可靠识别与位姿估计，具高精度与抗遮挡，适用于导航、SLAM与交互。


<details>
  <summary>Details</summary>
Motivation: 自动系统需要稳定环境感知；传统平面标记在视角受限、遮挡与位姿歧义下表现欠佳，难以满足360°场景中的标定与定位需求。

Method: 将PuzzleBoard的组合结构映射到圆柱表面，形成环绕式（360°）PuzzlePole标记；利用其唯一编码进行鲁棒检测与位姿估计，同时设计对遮挡不敏感的识别流程。

Result: PuzzlePole在识别可靠性、位置与姿态估计精度上优于或可与现有标记相当，并在存在遮挡与多视角情况下仍保持稳定性能。

Conclusion: PuzzlePole作为圆柱形全向视觉标记，提供高精度、强鲁棒与易部署的解决方案，适合机器人导航、SLAM与可触交互等多种自主系统应用。

Abstract: Reliable perception of the environment is a key enabler for autonomous systems, where calibration and localization tasks often rely on robust visual markers. We introduce the PuzzlePole, a new type of fiducial markers derived from the recently proposed PuzzleBoard calibration pattern. The PuzzlePole is a cylindrical marker, enabling reliable recognition and pose estimation from 360° viewing direction. By leveraging the unique combinatorial structure of the PuzzleBoard pattern, PuzzlePoles provide a high accuracy in localization and orientation while being robust to occlusions. The design offers flexibility for deployment in diverse autonomous systems scenarios, ranging from robot navigation and SLAM to tangible interfaces.

</details>


### [2] [Personalized Reward Modeling for Text-to-Image Generation](https://arxiv.org/abs/2511.19458)
*Jeongeun Lee,Ryang Heo,Dongha Lee*

Main category: cs.CV

TL;DR: PIGReward提出一种个性化文本到图像评估与优化框架，通过链式思维推理和动态生成用户条件的评估维度，解决现有通用/相似度指标无法捕捉个人偏好的问题；并构建PIGBench基准，实验显示在准确性与可解释性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评测多为通用奖励或相似度度量，难以反映个人视觉偏好的多样性与复杂性；用户数据稀缺又限制个性化模型训练。因此需要一种在小样本场景下、能进行推理并输出可解释个性化评价与反馈的方案。

Method: 提出PIGReward：1) 通过CoT推理动态生成用户条件的评估维度；2) 自举式策略，在有限参考数据上推理构建丰富的用户上下文，无需用户特定训练即可个性化；3) 利用个性化反馈驱动用户特定的提示词优化；并提出PIGBench，收集同一提示在不同用户下的多样化偏好，作为评测基准。

Result: 在广泛实验中，PIGReward在准确性和可解释性上优于现有方法；能有效用于用户特定的提示优化，使生成图像更契合个体意图。

Conclusion: PIGReward提供了可扩展、基于推理的个性化T2I评估与优化基础设施，并通过PIGBench验证其有效性，是朝个体对齐生成迈出的关键一步。

Abstract: Recent text-to-image (T2I) models generate semantically coherent images from textual prompts, yet evaluating how well they align with individual user preferences remains an open challenge. Conventional evaluation methods, general reward functions or similarity-based metrics, fail to capture the diversity and complexity of personal visual tastes. In this work, we present PIGReward, a personalized reward model that dynamically generates user-conditioned evaluation dimensions and assesses images through CoT reasoning. To address the scarcity of user data, PIGReward adopt a self-bootstrapping strategy that reasons over limited reference data to construct rich user contexts, enabling personalization without user-specific training. Beyond evaluation, PIGReward provides personalized feedback that drives user-specific prompt optimization, improving alignment between generated images and individual intent. We further introduce PIGBench, a per-user preference benchmark capturing diverse visual interpretations of shared prompts. Extensive experiments demonstrate that PIGReward surpasses existing methods in both accuracy and interpretability, establishing a scalable and reasoning-based foundation for personalized T2I evaluation and optimization. Taken together, our findings highlight PIGReward as a robust steptoward individually aligned T2I generation.

</details>


### [3] [SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data](https://arxiv.org/abs/2511.19466)
*Penghao Rao,Runmin Jiang,Min Xu*

Main category: cs.CV

TL;DR: 提出SG-OIF：在训练过程中以算法稳定性为控制器的在线影响估计框架，低成本维护IHVP并结合稳定性阈值、异常门控与置信度校准，提升训练样本对测试预测影响的估计，在多数据集噪声标签与OOD检测上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 影响函数可用于评估单个训练样本对测试预测的影响，帮助定位脏数据与关键样本。但在深度视觉模型中：逆曲率（Hessian逆向矢量积）计算昂贵；训练非平稳使静态近似失效；现有迭代/低秩方法多为离线、滞后训练动态，且未做置信度校准，影响排序脆弱，误判关键样本。需要一种可实时、稳健且高效的在线影响估计方法。

Method: 提出稳定性引导的在线影响框架SG-OIF：1) 将算法稳定性作为实时控制器；2) 以随机Richardson与预条件Neumann维护轻量的锚点IHVP近似；3) 设计可插拔曲率后端，通过稳定性引导的残差阈值、异常门控与置信度校准来调制每样本影响分数；4) 在线更新，适配训练非平稳性。

Result: 在含多种腐蚀/噪声的数据集上进行噪声标签与OOD检测实验，均达SOTA：例如CIFAR-10(20%不对称噪声)中，在top 1%预测样本上准确率达91.1%；在MNIST上AUPR达99.8%。

Conclusion: SG-OIF实现了实用的在线影响估计：在计算上低开销、在训练过程中稳定且自适应，并通过稳定性控制与置信度校准获得更可靠的影响排序，从而有效识别噪声与异常数据。

Abstract: Approximating training-point influence on test predictions is critical for deploying deep-learning vision models, essential for locating noisy data. Though the influence function was proposed for attributing how infinitesimal up-weighting or removal of individual training examples affects model outputs, its implementation is still challenging in deep-learning vision models: inverse-curvature computations are expensive, and training non-stationarity invalidates static approximations. Prior works use iterative solvers and low-rank surrogates to reduce cost, but offline computation lags behind training dynamics, and missing confidence calibration yields fragile rankings that misidentify critical examples. To address these challenges, we introduce a Stability-Guided Online Influence Framework (SG-OIF), the first framework that treats algorithmic stability as a real-time controller, which (i) maintains lightweight anchor IHVPs via stochastic Richardson and preconditioned Neumann; (ii) proposes modular curvature backends to modulate per-example influence scores using stability-guided residual thresholds, anomaly gating, and confidence. Experimental results show that SG-OIF achieves SOTA (State-Of-The-Art) on noise-label and out-of-distribution detection tasks across multiple datasets with various corruption. Notably, our approach achieves 91.1\% accuracy in the top 1\% prediction samples on the CIFAR-10 (20\% asym), and gets 99.8\% AUPR score on MNIST, effectively demonstrating that this framework is a practical controller for online influence estimation.

</details>


### [4] [Pistachio: Towards Synthetic, Balanced, and Long-Form Video Anomaly Benchmarks](https://arxiv.org/abs/2511.19474)
*Jie Li,Hongyi Cai,Mingkang Dong,Muxin Pu,Shan You,Fei Wang,Tao Huang*

Main category: cs.CV

TL;DR: Pistachio 是一个基于生成式管线构建的新型视频异常检测/理解（VAD/VAU）基准，提供可控的场景、多样异常与复杂时间叙事，以弥补现有数据集在多样性与标注难度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有 VAD 基准场景单一、异常类型不均衡、时间结构简单，难以评估真实世界性能；VAU 需要更深层语义与因果推理，但人工标注成本高、难以基准化。

Method: 提出一个全生成式、可控的数据构建管线：1) 以场景条件驱动的异常分配；2) 多步骤剧情/叙事生成；3) 时间一致性的长视频合成策略。利用最新视频生成模型，最小化人工干预，统一控制场景、异常种类与时间结构，生成约41秒的连贯视频。

Result: 生成了规模化、具多样性与高时间复杂度的 Pistachio 数据集；实验显示该基准对现有方法提出新挑战，揭示其不足。

Conclusion: 生成式、可控的基准能消除互联网抓取数据的偏差与限制，为动态、多事件的异常理解提供更可靠评测平台，并促进未来方法研究。

Abstract: Automatically detecting abnormal events in videos is crucial for modern autonomous systems, yet existing Video Anomaly Detection (VAD) benchmarks lack the scene diversity, balanced anomaly coverage, and temporal complexity needed to reliably assess real-world performance. Meanwhile, the community is increasingly moving toward Video Anomaly Understanding (VAU), which requires deeper semantic and causal reasoning but remains difficult to benchmark due to the heavy manual annotation effort it demands. In this paper, we introduce Pistachio, a new VAD/VAU benchmark constructed entirely through a controlled, generation-based pipeline. By leveraging recent advances in video generation models, Pistachio provides precise control over scenes, anomaly types, and temporal narratives, effectively eliminating the biases and limitations of Internet-collected datasets. Our pipeline integrates scene-conditioned anomaly assignment, multi-step storyline generation, and a temporally consistent long-form synthesis strategy that produces coherent 41-second videos with minimal human intervention. Extensive experiments demonstrate the scale, diversity, and complexity of Pistachio, revealing new challenges for existing methods and motivating future research on dynamic and multi-event anomaly understanding.

</details>


### [5] [Tracking and Segmenting Anything in Any Modality](https://arxiv.org/abs/2511.19475)
*Tianlu Zhang,Qiang Zhang,Guiguang Ding,Jungong Han*

Main category: cs.CV

TL;DR: 提出SATA通用跟踪与分割框架，用DeMoE解决跨模态/跨任务分布与表征差异，并以TaMOT统一输出与ID校准，实现多任务多模态下的更强泛化，在18个基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解中跟踪与分割虽目标一致，但多依赖特化架构或模态特定参数，导致泛化与可扩展性差；同时忽视跨模态分布差异与跨任务表征差异，阻碍知识共享，难以打造真正通用模型。

Method: 提出SATA通用框架：1) Decoupled Mixture-of-Expert（DeMoE）将统一表征学习解耦为“跨模态共享知识”与“模态/任务特定信息”两类专家，兼顾灵活性与泛化；2) Task-aware Multi-object Tracking（TaMOT）将各任务输出统一为带校准ID的实例集合，缓解多任务训练中的任务特定知识退化。

Result: 在涵盖18个跟踪与分割的具有挑战性基准上取得优于现有方法的性能，显示出更强的跨任务与跨模态泛化能力。

Conclusion: 通过DeMoE与TaMOT，SATA实现了多模态输入、跨任务统一的跟踪与分割，兼顾共享与特定表征，提升了知识共享效率与泛化，推动通用视频理解模型发展。

Abstract: Tracking and segmentation play essential roles in video understanding, providing basic positional information and temporal association of objects within video sequences. Despite their shared objective, existing approaches often tackle these tasks using specialized architectures or modality-specific parameters, limiting their generalization and scalability. Recent efforts have attempted to unify multiple tracking and segmentation subtasks from the perspectives of any modality input or multi-task inference. However, these approaches tend to overlook two critical challenges: the distributional gap across different modalities and the feature representation gap across tasks. These issues hinder effective cross-task and cross-modal knowledge sharing, ultimately constraining the development of a true generalist model. To address these limitations, we propose a universal tracking and segmentation framework named SATA, which unifies a broad spectrum of tracking and segmentation subtasks with any modality input. Specifically, a Decoupled Mixture-of-Expert (DeMoE) mechanism is presented to decouple the unified representation learning task into the modeling process of cross-modal shared knowledge and specific information, thus enabling the model to maintain flexibility while enhancing generalization. Additionally, we introduce a Task-aware Multi-object Tracking (TaMOT) pipeline to unify all the task outputs as a unified set of instances with calibrated ID information, thereby alleviating the degradation of task-specific knowledge during multi-task training. SATA demonstrates superior performance on 18 challenging tracking and segmentation benchmarks, offering a novel perspective for more generalizable video understanding.

</details>


### [6] [The Determinant Ratio Matrix Approach to Solving 3D Matching and 2D Orthographic Projection Alignment Tasks](https://arxiv.org/abs/2511.19511)
*Andrew J. Hanson,Sonya M. Hanson*

Main category: cs.CV

TL;DR: 提出基于行列式比矩阵（DRaM）的正交投影（OnP）与三维到三维（EnP）姿态估计闭式解，并在噪声情形下给出简易旋转校正；统一并扩展了以往QR与伪逆方法的框架。


<details>
  <summary>Details</summary>
Motivation: 姿态估计广泛应用，但在噪声条件下：3D-3D配准（EnP）已有精确闭式（SVD、四元数特征分解），而3D-2D正交（OnP）缺乏可比的闭式解。需要一个统一、解析、可推广的方法处理无噪与含噪情形。

Method: 引入行列式比矩阵（Determinant Ratio Matrix, DRaM）框架，推导无噪EnP与OnP的最小二乘解析解；在含噪情况下，采用直接的旋转修正方案。将既有基于QR分解与Moore-Penrose伪逆的做法纳入DRaM家族，并进行系统比较。

Result: 获得EnP与OnP在无噪条件下的闭式解；对含噪EnP，SVD与最优四元数本已精确，DRaM保持一致；对含噪OnP，提供DRaM类可行求解（此前无同等闭式）。展示DRaM与QR/伪逆方法在性能与适用性上的关系与差异。

Conclusion: DRaM提供统一、历史可追溯（可追溯至高斯时代）的解析途径，既给出3D与2D正交姿态估计的新解，又阐明各类方法的内在联系，并可推广到任意维欧氏空间的姿态估计问题。

Abstract: Pose estimation is a general problem in computer vision with wide applications. The relative orientation of a 3D reference object can be determined from a 3D rotated version of that object, or from a projection of the rotated object to a 2D planar image. This projection can be a perspective projection (the PnP problem) or an orthographic projection (the OnP problem). We restrict our attention here to the OnP problem and the full 3D pose estimation task (the EnP problem). Here we solve the least squares systems for both the error-free EnP and OnP problems in terms of the determinant ratio matrix (DRaM) approach. The noisy-data case can be addressed with a straightforward rotation correction scheme. While the SVD and optimal quaternion eigensystem methods solve the noisy EnP 3D-3D alignment exactly, the noisy 3D-2D orthographic (OnP) task has no known comparable closed form, and can be solved by DRaM-class methods. We note that while previous similar work has been presented in the literature exploiting both the QR decomposition and the Moore-Penrose pseudoinverse transformations, here we place these methods in a larger context that has not previously been fully recognized in the absence of the corresponding DRaM solution. We term this class of solutions as the DRaM family, and conduct comparisons of the behavior of the families of solutions for the EnP and OnP rotation estimation problems. Overall, this work presents both a new solution to the 3D and 2D orthographic pose estimation problems and provides valuable insight into these classes of problems. With hindsight, we are able to show that our DRaM solutions to the exact EnP and OnP problems possess derivations that could have been discovered in the time of Gauss, and in fact generalize to all analogous N-dimensional Euclidean pose estimation problems.

</details>


### [7] [Single Image to High-Quality 3D Object via Latent Features](https://arxiv.org/abs/2511.19512)
*Huanning Dong,Yinuo Huang,Fan Li,Ping Kuang*

Main category: cs.CV

TL;DR: 提出LatentDreamer：从单张图像快速生成高保真3D对象的框架，借助预训练VAE把3D几何映射到潜空间，分阶段生成粗几何→精修几何→真实纹理，约70秒完成，少量训练即可达到与同期方法竞争的效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动3D生成（如image-to-3D）难以同时满足“快、细节丰富、高保真”。需要一种降低生成难度、提升速度与质量并兼顾数据与训练成本的方法。

Method: 核心是预训练变分自编码器（VAE）将3D几何压缩到潜特征空间，降低直接在显式3D空间生成的难度。以潜特征为起点，采用顺序式三阶段管线：1）生成粗几何；2）几何精修；3）纹理生成为逼真外观。

Result: 在单图像到3D任务上，生成的对象与输入图像高度一致，端到端生成时间短（典型约70秒），在有限训练数据/时间下，定量与定性结果均与当代方法具有竞争力。

Conclusion: 通过VAE潜空间建模与阶段式生成，LatentDreamer在速度、细节和保真度之间取得平衡，表明轻量训练即可获得强性能，为高效单图像3D生成提供可行方案。

Abstract: 3D assets are essential in the digital age. While automatic 3D generation, such as image-to-3d, has made significant strides in recent years, it often struggles to achieve fast, detailed, and high-fidelity generation simultaneously. In this work, we introduce LatentDreamer, a novel framework for generating 3D objects from single images. The key to our approach is a pre-trained variational autoencoder that maps 3D geometries to latent features, which greatly reducing the difficulty of 3D generation. Starting from latent features, the pipeline of LatentDreamer generates coarse geometries, refined geometries, and realistic textures sequentially. The 3D objects generated by LatentDreamer exhibit high fidelity to the input images, and the entire generation process can be completed within a short time (typically in 70 seconds). Extensive experiments show that with only a small amount of training, LatentDreamer demonstrates competitive performance compared to contemporary approachs.

</details>


### [8] [Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning](https://arxiv.org/abs/2511.19515)
*Shawn Young,Xingyu Zeng,Lijian Xu*

Main category: cs.CV

TL;DR: 研究建立“模型容量—最少视觉token”关系：模型越大，所需token越少；并提出正交过滤模块将冗余token聚为正交基，且发布一个视觉长上下文数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer常用固定或经验阈值的token数，缺乏关于“保持图像语义所需最小token数”的理论与实证刻画；在计算与内存受限、长上下文任务兴起的背景下，理解并减少token冗余可显著提升效率。

Method: 受MDL启发，将图像token视为视觉语义空间中的向量，定义“内在语义复杂度”为可张成该空间的最小基向量集。基于此提出Orthogonal Filtering：轻量模块，自适应聚合冗余token，输出一组近似两两正交的基；在多种ViT规模上实验，测量在保证语义/性能下所需最少token。

Result: 跨多种ViT与任务，发现一致的缩放定律：模型越大，能够以更少token张成语义空间，性能保持或略降；Orthogonal Filtering在相同性能下显著减少token数与计算；同时提供一个视觉长上下文数据集用于评测。

Conclusion: 视觉语义可由少量正交基刻画，且所需基数随模型容量增加而下降；正交过滤为通用、轻量的token压缩策略，可提升推理效率并适配长上下文场景；数据集促进相关研究。

Abstract: This paper investigates the fundamental relationship between model capacity and the minimal number of visual tokens required to preserve image semantics. Inspired by the Minimum Description Length principle, we reinterpret image tokens as vectors in a visual semantic space and define the intrinsic semantic complexity of an image as the smallest set of basis vectors needed to span this space. Building on this perspective, we propose Orthogonal Filtering, a lightweight module that adaptively clusters redundant tokens into a compact set of orthogonal bases. Through extensive experiments across a range of ViT models, we reveal a consistent token, model scaling law: larger models require significantly fewer tokens to span visual semantic space. Besides, we also contribute a visual long-context dataset.

</details>


### [9] [Connecting the Dots: Training-Free Visual Grounding via Agentic Reasoning](https://arxiv.org/abs/2511.19516)
*Liqin Luo,Guangyao Chen,Xiawu Zheng,Yongxing Dai,Yixiong Zou,Yonghong Tian*

Main category: cs.CV

TL;DR: 提出GroundingAgent，一个无需特定任务微调的“智能体式”视觉指代定位框架，利用开放词汇检测器、MLLM与LLM的迭代推理，在RefCOCO家族实现平均65.1%的零样本精度，且选择阶段用原始查询可达约90%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉指代方法高度依赖大规模任务标注与微调，迁移到新域/分布外场景泛化差、成本高、可解释性弱；需要一种无需微调、能稳健泛化并具备透明推理过程的方法。

Method: 构建一个多模块协作的Agent：以开放词汇目标检测器产生候选区域；MLLM生成描述与多模态理解；LLM对文本与视觉候选进行结构化、迭代的语义与空间推理与筛选，逐步缩小候选集合。还比较使用MLLM生成描述与直接使用原始查询文本在选择阶段的差异。

Result: 在RefCOCO/RefCOCO+/RefCOCOg三基准上实现零样本平均65.1%定位准确率，无任何任务特定微调；当在候选选择阶段以原始查询替代MLLM生成描述时，选择环节准确率约90%，接近有监督方法；方法过程具有高可解释性，能展示逐步推理与决策依据。

Conclusion: Agent式迭代推理结合开放词汇检测与LLM/MLLM可在零样本视觉指代中取得强竞争力与可解释性；LLM的推理能力对性能至关重要，为减少标注与提升泛化提供了有效路径。

Abstract: Visual grounding, the task of linking textual queries to specific regions within images, plays a pivotal role in vision-language integration. Existing methods typically rely on extensive task-specific annotations and fine-tuning, limiting their ability to generalize effectively to novel or out-of-distribution scenarios. To address these limitations, we introduce GroundingAgent, a novel agentic visual grounding framework that operates without any task-specific fine-tuning. GroundingAgent employs a structured, iterative reasoning mechanism that integrates pretrained open-vocabulary object detectors, multimodal large language models (MLLMs), and large language models (LLMs) to progressively refine candidate regions through joint semantic and spatial analyses. Remarkably, GroundingAgent achieves an average zero-shot grounding accuracy of 65.1 % on widely-used benchmarks (RefCOCO, RefCOCO+, RefCOCOg), entirely without fine-tuning. Furthermore, by substituting MLLM-generated captions with the original query texts, the accuracy at the selection stage alone reaches approximately 90 %, closely matching supervised performance and underscoring the critical role of LLM reasoning capabilities. GroundingAgent also offers strong interpretability, transparently illustrating each reasoning step and providing clear insights into its decision-making process.

</details>


### [10] [Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning](https://arxiv.org/abs/2511.19518)
*Zhaoqi Xu,Yingying Zhang,Jian Li,Jianwei Guo,Qiannan Zhu,Hua Huang*

Main category: cs.CV

TL;DR: 提出InfoPrune：以信息瓶颈为基础，对VLM进行自适应结构压缩，结合eRank与KS距离评估注意力头与结构变更的信息保持，提供训练期头剪枝与训练免调FFN低秩压缩两方案；在VQAv2、TextVQA、GQA上实现最高3.2× FLOPs降低与1.8×加速，性能几乎不降。


<details>
  <summary>Details</summary>
Motivation: VLM规模持续增大、部署与推理成本高；现有压缩多依赖启发式指标/经验规则，缺乏关于信息保真的理论保证，难以在效率与效果间做可解释、可控的权衡。

Method: 以信息瓶颈建模剪枝为“保留任务相关信息、丢弃冗余依赖”的权衡；提出熵驱动的effective rank（eRank）衡量注意力头贡献，并用KS距离度量原始与压缩结构的分布偏离，形成兼顾结构稀疏与信息效率的统一准则；在此之上设计两种方案：(1) 训练期基于信息损失目标的注意力头剪枝；(2) 训练免调的FFN自适应低秩近似。

Result: 在VQAv2、TextVQA、GQA三项基准上，达成最高3.2× FLOPs减少与1.8×推理加速，精度几乎不受影响。

Conclusion: InfoPrune以信息论为基础，为VLM提供可解释与可控的结构压缩路径，统一考虑信息保真与结构稀疏，在多数据集上验证了实用性与高效性。

Abstract: Recent advances in vision-language models (VLMs) have shown remarkable performance across multimodal tasks, yet their ever-growing scale poses severe challenges for deployment and efficiency. Existing compression methods often rely on heuristic importance metrics or empirical pruning rules, lacking theoretical guarantees about information preservation. In this work, we propose InfoPrune, an information-theoretic framework for adaptive structural compression of VLMs. Grounded in the Information Bottleneck principle, we formulate pruning as a trade-off between retaining task-relevant semantics and discarding redundant dependencies. To quantify the contribution of each attention head, we introduce an entropy-based effective rank (eRank) and employ the Kolmogorov--Smirnov (KS) distance to measure the divergence between original and compressed structures. This yields a unified criterion that jointly considers structural sparsity and informational efficiency. Building on this foundation, we further design two complementary schemes: (1) a training-based head pruning guided by the proposed information loss objective, and (2) a training-free FFN compression via adaptive low-rank approximation. Extensive experiments on VQAv2, TextVQA, and GQA demonstrate that InfoPrune achieves up to 3.2x FLOP reduction and 1.8x acceleration with negligible performance degradation, establishing a theoretically grounded and practically effective step toward efficient multimodal large models.

</details>


### [11] [Blinking Beyond EAR: A Stable Eyelid Angle Metric for Driver Drowsiness Detection and Data Augmentation](https://arxiv.org/abs/2511.19519)
*Mathis Wolter,Julie Stephany Berrio Perez,Mao Shan*

Main category: cs.CV

TL;DR: 提出一种基于3D人脸标志点的眼睑角（ELA）指标，稳健于视角变化，用于眨眼/困倦检测，并用ELA驱动3D头像生成可控合成数据，提升困倦识别。


<details>
  <summary>Details</summary>
Motivation: 现有困倦检测依赖二分类眼状态或2D度量（如EAR），对相机角度敏感、稳定性不足，且自然困倦数据难采集且有风险，限制了模型训练与泛化。

Method: 1) 定义ELA：从3D人脸关键点计算眼睑几何角度，作为连续眼部开合度量，对视角更鲁棒。2) 基于ELA的眨眼检测：提取闭合、闭眼、重开等时序特征，与困倦相关。3) 数据合成：用ELA信号驱动Blender绑定头像动画，控制噪声、视角与眨眼动力学，生成逼真的合成训练集。4) 在公共驾驶监控数据集上评估稳健性与检测性能。

Result: ELA在视角变化下方差低于EAR，实现准确眨眼检测；基于ELA的合成数据增强扩大了训练多样性，并改善困倦识别表现。

Conclusion: ELA同时是稳健的生物计量特征和可扩展数据生成工具，可用于驾驶员状态监测中可靠的眨眼/困倦估计与数据增强。

Abstract: Detecting driver drowsiness reliably is crucial for enhancing road safety and supporting advanced driver assistance systems (ADAS). We introduce the Eyelid Angle (ELA), a novel, reproducible metric of eye openness derived from 3D facial landmarks. Unlike conventional binary eye state estimators or 2D measures, such as the Eye Aspect Ratio (EAR), the ELA provides a stable geometric description of eyelid motion that is robust to variations in camera angle. Using the ELA, we design a blink detection framework that extracts temporal characteristics, including the closing, closed, and reopening durations, which are shown to correlate with drowsiness levels. To address the scarcity and risk of collecting natural drowsiness data, we further leverage ELA signals to animate rigged avatars in Blender 3D, enabling the creation of realistic synthetic datasets with controllable noise, camera viewpoints, and blink dynamics. Experimental results in public driver monitoring datasets demonstrate that the ELA offers lower variance under viewpoint changes compared to EAR and achieves accurate blink detection. At the same time, synthetic augmentation expands the diversity of training data for drowsiness recognition. Our findings highlight the ELA as both a reliable biometric measure and a powerful tool for generating scalable datasets in driver state monitoring.

</details>


### [12] [VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.19524)
*Boyu Chen,Zikang Wang,Zhengrong Yue,Kainan Yan,Chenyun Yu,Yi Huang,Zijun Liu,Yafei Wen,Xiaoxin Chen,Yang Liu,Peng Li,Yali Wang*

Main category: cs.CV

TL;DR: 提出VideoChat-M1：一个具有协同政策规划（CPP）与多智能体强化学习（MARL）的多智能体视频理解系统，动态生成、执行并交流工具调用策略，在多项基准上达SOTA，尤其在LongVideoBench上超越Gemini 2.5 pro与GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解多智能体/MLLM方法多采用静态、不可学习的工具调用策略，难以在时空复杂视频中发掘多样线索，限制了稳健的感知与推理能力。

Method: 引入协同政策规划（CPP）：(1) 政策生成——每个智能体面向用户问题生成其个性化工具调用策略；(2) 政策执行——按序调用相关工具以探索视频；(3) 政策沟通——执行中期相互交流并更新策略。配以简洁的多智能体强化学习（MARL），以最终答案奖励与中间协作反馈联合优化团队策略。

Result: 在8个基准、4类任务上取得SOTA。在LongVideoBench上，相比Gemini 2.5 pro提升3.6%，相比GPT-4o提升15.6%。

Conclusion: 动态、可学习、协同的多智能体策略与MARL能显著提升视频理解中的感知与推理效果，优于静态工具调用机制。

Abstract: By leveraging tool-augmented Multimodal Large Language Models (MLLMs), multi-agent frameworks are driving progress in video understanding. However, most of them adopt static and non-learnable tool invocation mechanisms, which limit the discovery of diverse clues essential for robust perception and reasoning regarding temporally or spatially complex videos. To address this challenge, we propose a novel Multi-agent system for video understanding, namely VideoChat-M1. Instead of using a single or fixed policy, VideoChat-M1 adopts a distinct Collaborative Policy Planning (CPP) paradigm with multiple policy agents, which comprises three key processes. (1) Policy Generation: Each agent generates its unique tool invocation policy tailored to the user's query; (2) Policy Execution: Each agent sequentially invokes relevant tools to execute its policy and explore the video content; (3) Policy Communication: During the intermediate stages of policy execution, agents interact with one another to update their respective policies. Through this collaborative framework, all agents work in tandem, dynamically refining their preferred policies based on contextual insights from peers to effectively respond to the user's query. Moreover, we equip our CPP paradigm with a concise Multi-Agent Reinforcement Learning (MARL) method. Consequently, the team of policy agents can be jointly optimized to enhance VideoChat-M1's performance, guided by both the final answer reward and intermediate collaborative process feedback. Extensive experiments demonstrate that VideoChat-M1 achieves SOTA performance across eight benchmarks spanning four tasks. Notably, on LongVideoBench, our method outperforms the SOTA model Gemini 2.5 pro by 3.6% and GPT-4o by 15.6%.

</details>


### [13] [Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.19526)
*Jonathan Lee,Xingrui Wang,Jiawei Peng,Luoxin Ye,Zehan Zheng,Tiezheng Zhang,Tao Wang,Wufei Ma,Siyi Chen,Yu-Cheng Chou,Prakhar Kaushik,Alan Yuille*

Main category: cs.CV

TL;DR: 提出“感知分类学”基准，评估从物体与空间识别到材料、可供性、功能与物理属性等多步属性推理的视觉理解；现有VLM在属性驱动与多步推理上显著掉分，提示结构化理解缺口；利用模拟场景的链式示例可提升真实与专家题表现。


<details>
  <summary>Details</summary>
Motivation: 人类在理解场景时会先识别物体与空间关系，再基于任务推断材料、可供性、功能与物理属性以支持目标导向推理。然而主流视觉-语言基准多偏重表层识别或图文对齐，缺乏对这类物理扎根、结构化推理能力的系统评估。

Method: 构建“感知分类学”基准：1) 标注3173个对象的四类属性家族、共84个细粒度属性；2) 覆盖合成与真实两域的5802张图像；3) 生成28033道模板题（物体描述、空间推理、属性匹配、分类学推理）与50道专家题；4) 评测多种领先VLM，并尝试以模拟场景的在上下文推理示例进行提示。

Result: 领先VLM在识别类问题表现良好，但在属性驱动问题上准确率下降10–20%，尤其是在需要跨多属性的多步推理时。引入基于感知分类学的在上下文示例能提升在真实域与专家题上的表现。

Conclusion: 当前VLM过度依赖模式匹配，缺乏对结构化、物理扎根属性的稳健推理。所提基准揭示并量化这一缺口，并表明以感知分类学为导向的提示可缓解该问题，为后续模型与训练策略提供评测与改进路径。

Abstract: We propose Perceptual Taxonomy, a structured process of scene understanding that first recognizes objects and their spatial configurations, then infers task-relevant properties such as material, affordance, function, and physical attributes to support goal-directed reasoning. While this form of reasoning is fundamental to human cognition, current vision-language benchmarks lack comprehensive evaluation of this ability and instead focus on surface-level recognition or image-text alignment.
  To address this gap, we introduce Perceptual Taxonomy, a benchmark for physically grounded visual reasoning. We annotate 3173 objects with four property families covering 84 fine-grained attributes. Using these annotations, we construct a multiple-choice question benchmark with 5802 images across both synthetic and real domains. The benchmark contains 28033 template-based questions spanning four types (object description, spatial reasoning, property matching, and taxonomy reasoning), along with 50 expert-crafted questions designed to evaluate models across the full spectrum of perceptual taxonomy reasoning.
  Experimental results show that leading vision-language models perform well on recognition tasks but degrade by 10 to 20 percent on property-driven questions, especially those requiring multi-step reasoning over structured attributes. These findings highlight a persistent gap in structured visual understanding and the limitations of current models that rely heavily on pattern matching. We also show that providing in-context reasoning examples from simulated scenes improves performance on real-world and expert-curated questions, demonstrating the effectiveness of perceptual-taxonomy-guided prompting.

</details>


### [14] [MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training](https://arxiv.org/abs/2511.19527)
*Hongyu Lyu,Thomas Monninger,Julie Stephany Berrio Perez,Mao Shan,Zhenxing Ming,Stewart Worrall*

Main category: cs.CV

TL;DR: MapRF 是一种只用2D图像标注、通过NeRF生成高质量伪标签并自训练的弱监督在线HD地图构建框架，性能接近全监督方法且优于其他仅用2D标注的方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线HD地图构建依赖昂贵的3D地图标注，限制了在多样驾驶环境中的泛化与可扩展性；需要一种仅用廉价2D标注也能得到高质量3D地图的方法。

Method: 提出MapRF：1) 以地图预测为条件的NeRF模块重建视角一致的3D几何与语义以生成高质量伪标签；2) 采用自训练迭代用伪标签不断精炼地图网络；3) 设计Map-to-Ray Matching，将来自2D标注的相机光线与地图预测对齐，缓解自训练中的误差累积。

Result: 在Argoverse 2与nuScenes上，MapRF在无需3D标注的条件下达到约全监督基线的75%性能，并超过多种仅用2D标注的对比方法。

Conclusion: MapRF在弱监督场景下实现可扩展、低成本的在线HD地图构建，展示了以2D标注训练仍可获得接近全监督质量的潜力。

Abstract: Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.

</details>


### [15] [Vidi2: Large Multimodal Models for Video Understanding and Creation](https://arxiv.org/abs/2511.19529)
*Vidi Team,Celong Liu,Chia-Wen Kuo,Chuang Huang,Dawei Du,Fan Chen,Guang Chen,Haoji Zhang,Haojun Zhao,Lingxi Zhang,Lu Guo,Lusha Li,Longyin Wen,Qihang Fan,Qingyu Chen,Rachel Deng,Sijie Zhu,Stuart Siew,Tong Jin,Weiyan Tao,Wen Zhong,Xiaohui Shen,Xin Gu,Zhenfang Chen,Zuhua Lin*

Main category: cs.CV

TL;DR: Vidi2提出面向大规模、长时长视频的端到端时空定位与检索/问答系统，并在新基准VUE-STG与升级的VUE-TR-V2上显著超越主流专有与同规模开源模型。


<details>
  <summary>Details</summary>
Motivation: 网络视频创作与理解需求激增，现有模型多聚焦检索或短视频场景，缺乏对长视频中目标的细粒度时空定位与可用、统一评测基准。

Method: 1) 提出Vidi2：统一框架同时做时间检索与空间目标框定，实现端到端时空定位，并扩展到视频问答；2) 构建VUE-STG基准，覆盖10秒到30分钟长时长视频，以名词短语为主的查询，人工高精度标注时间段与框，并设计精炼的vIoU/tIoU/vIoU-Intersection指标；3) 升级VUE-TR到VUE-TR-V2，改进视频长度分布与用户式查询。

Result: Vidi2在VUE-TR-V2与VUE-STG上大幅优于领先专有系统（如Gemini 3 Pro(Preview)与GPT-5），在视频问答基准上与同规模热门开源模型竞争力相当。

Conclusion: 端到端的细粒度时空定位与统一评测基准显著提升长视频理解与创作相关任务表现，Vidi2为复杂编辑、叙事理解与自动构图等应用提供可落地能力。

Abstract: Video has emerged as the primary medium for communication and creativity on the Internet, driving strong demand for scalable, high-quality video production. Vidi models continue to evolve toward next-generation video creation and have achieved state-of-the-art performance in multimodal temporal retrieval (TR). In its second release, Vidi2 advances video understanding with fine-grained spatio-temporal grounding (STG) and extends its capability to video question answering (Video QA), enabling comprehensive multimodal reasoning. Given a text query, Vidi2 can identify not only the corresponding timestamps but also the bounding boxes of target objects within the output time ranges. This end-to-end spatio-temporal grounding capability enables potential applications in complex editing scenarios, such as plot or character understanding, automatic multi-view switching, and intelligent, composition-aware reframing and cropping. To enable comprehensive evaluation of STG in practical settings, we introduce a new benchmark, VUE-STG, which offers four key improvements over existing STG datasets: 1) Video duration: spans from roughly 10s to 30 mins, enabling long-context reasoning; 2) Query format: queries are mostly converted into noun phrases while preserving sentence-level expressiveness; 3) Annotation quality: all ground-truth time ranges and bounding boxes are manually annotated with high accuracy; 4) Evaluation metric: a refined vIoU/tIoU/vIoU-Intersection scheme. In addition, we upgrade the previous VUE-TR benchmark to VUE-TR-V2, achieving a more balanced video-length distribution and more user-style queries. Remarkably, the Vidi2 model substantially outperforms leading proprietary systems, such as Gemini 3 Pro (Preview) and GPT-5, on both VUE-TR-V2 and VUE-STG, while achieving competitive results with popular open-source models with similar scale on video QA benchmarks.

</details>


### [16] [Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment](https://arxiv.org/abs/2511.19537)
*Muhao Guo,Yang Weng*

Main category: cs.CV

TL;DR: 研究提出利用多模态大模型（LLM）在全球范围卫星影像中识别与量化分布式光伏（PV），通过结构化提示与微调，将检测、定位、计量统一到一个框架，并在跨区域评测中表现出最小性能衰减，优于传统CV与Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 分布式光伏装机增长迅速且大量未登记，给电网管理带来不确定性；卫星影像覆盖广但传统CNN/U-Net依赖大量标注且跨区域泛化差，需要一种可扩展、可迁移、可解释的全球PV评估方法。

Method: 采用多模态LLM，结合结构化提示（prompts）与微调，将“检测-定位-数量估计”统一建模；以跨区域的ΔF1作为域外泛化评估指标，与传统CV与Transformer基线对比。

Result: 在未见过的区域上，多模态LLM的ΔF1性能下降最小，超过CNN、U-Net及Transformer基线，显示较强的跨域稳健性。

Conclusion: 多模态LLM在卫星影像PV识别与量化中对域移具有更高鲁棒性，具备可扩展、可迁移与可解释的全球映射潜力，可减少对大规模标注的依赖，适合用于全球PV普查与电网规划支持。

Abstract: The rapid expansion of distributed photovoltaic (PV) systems poses challenges for power grid management, as many installations remain undocumented. While satellite imagery provides global coverage, traditional computer vision (CV) models such as CNNs and U-Nets require extensive labeled data and fail to generalize across regions. This study investigates the cross-domain generalization of a multimodal large language model (LLM) for global PV assessment. By leveraging structured prompts and fine-tuning, the model integrates detection, localization, and quantification within a unified schema. Cross-regional evaluation using the $Δ$F1 metric demonstrates that the proposed model achieves the smallest performance degradation across unseen regions, outperforming conventional CV and transformer baselines. These results highlight the robustness of multimodal LLMs under domain shift and their potential for scalable, transferable, and interpretable global PV mapping.

</details>


### [17] [Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration](https://arxiv.org/abs/2511.19538)
*Remi Petitpierre*

Main category: cs.CV

TL;DR: 构建超大规模历史地图数据集（77万条记录、近10万幅图像，1492–1948），结合语义分割与目标检测，量化分析制图的空间关注、符号体系与文化扩散，揭示地图与政治经济（殖民、战争、贸易）之间的联动及制图符号演化规律。


<details>
  <summary>Details</summary>
Motivation: 尽管已有海量历史地图被数字化并可自动识别，但相关方法很少结合制图史与文化视角，把地图当作承载政治与知识期待的符号系统来研究。需要一种既具规模又具语义的方式，系统刻画地图生产、空间关注、符号演化与文化扩散。

Method: - 聚合38个目录的771,561条记录与99,715幅图像，规范化出236,925名贡献者，时间跨度1492–1948。
- 构建可描绘地理结构与出版时间轴的元数据框架。
- 训练通用语义分割与目标检测模型（基于标注与合成数据），用于地类与制图符号（记号）识别。
- 将6,300万符号和2,500万图像片段嵌入潜在视觉空间，进行符号体系与风格演化分析。
- 分析合作与扩散网络，考察合法性与中心城市在符号规范传播中的作用。

Result: - 量化出制图空间关注与政治动态的关联：大西洋航海测绘与三角贸易、殖民扩张相关；战争显著影响出版量与国家/国内关注度的演进。
- 地类分析显示地图是“设计的图像”，通过居中与语义对称等构图强调特征。
- 揭示符号学变迁：如以等高线取代山影短线（hachures），且符号在局部形成一致体系。
- 合作与扩散分析显示：合法性、规模大机构与中心城市推动图式规范与符号文化的传播。

Conclusion: 以大规模数据与计算机视觉为桥梁，将制图史的文化—符号视角量化：地图生产与政治经济过程相互嵌套，符号体系呈地方一致性并随时间演化；中心机构与城市是符号规范扩散的关键。方法与数据为后续跨学科研究与数字人文提供通用框架。

Abstract: This thesis presents methods and datasets to investigate cartographic heritage on a large scale and from a cultural perspective. Heritage institutions worldwide have digitized more than one million maps, and automated techniques now enable large-scale recognition and extraction of map content. Yet these methods have engaged little with the history of cartography, or the view that maps are semantic-symbolic systems, and cultural objects reflecting political and epistemic expectations. This work leverages a diverse corpus of 771,561 map records and 99,715 digitized images aggregated from 38 digital catalogs. After normalization, the dataset includes 236,925 contributors and spans six centuries, from 1492 to 1948. These data make it possible to chart geographic structures and the global chronology of map publication. The spatial focus of cartography is analyzed in relation to political dynamics, evidencing links between Atlantic maritime charting, the triangular trade, and colonial expansion. Further results document the progression of national, domestic focus and the impact of military conflicts on publication volumes. The research introduces semantic segmentation techniques and object detection models for the generic recognition of land classes and cartographic signs, trained on annotated data and synthetic images. The analysis of land classes shows that maps are designed images whose framing and composition emphasize features through centering and semantic symmetries. The study of cartographic figuration encodes 63 M signs and 25 M fragments into a latent visual space, revealing figurative shifts such as the replacement of relief hachures by terrain contours and showing that signs tend to form locally consistent systems. Analyses of collaboration and diffusion highlight the role of legitimacy, larger actors, and major cities in the spread of figurative norms and semiotic cultures.

</details>


### [18] [Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation](https://arxiv.org/abs/2511.19542)
*Jaeyeong Kim,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.CV

TL;DR: SpLap 提出一种无需代理的高斯斑点（GS）变形方法：基于“表面感知”的斑点图构建拉普拉斯算子，通过斑点相交关系建图并配合高斯核自适应，实现在保持细节与拓扑的同时进行更可信的变形与渲染。


<details>
  <summary>Details</summary>
Motivation: 现有 GS 变形依赖笼/网格等代理，受代理质量影响且有额外开销；直接把斑点当点云做拉普拉斯变形又难以体现表面结构，导致形变不真实、细节丢失与拓扑破坏。需要一种既不依赖代理又能编码表面信息的变形框架。

Method: 1) 构建“表面感知”的斑点图：邻接不以中心距离为准，而以斑点间几何相交关系定义；2) 由该图计算拉普拉斯算子进行变形，促进局部结构与拓扑保持；3) 提出高斯核自适应，在变形过程中调整斑点核参数以维护表面结构并提升变形后渲染质量。

Result: 在 ShapeNet、Objaverse、Sketchfab 的 50 个具有挑战性的对象及 NeRF-Synthetic 上评估，相比代理式与无代理基线，SpLap 在变形可信度、细节与拓扑保持以及渲染质量方面均显著优于对手。

Conclusion: 表面感知斑点图+拉普拉斯变形+高斯核自适应可在无代理框架下实现高质量 GS 变形，兼顾细节保真与拓扑稳定，具有实用性与通用性；代码已开源。

Abstract: We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph. Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead. An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds. However, this often fail to properly capture surface information due to lack of explicit structure. To address this, we propose a novel method that constructs a surface-aware splat graph, enabling the Laplacian operator derived from it to support more plausible deformations that preserve details and topology. Our key idea is to leverage the spatial arrangement encoded in splats, defining neighboring splats not merely by the distance between their centers, but by their intersections. Furthermore, we introduce a Gaussian kernel adaptation technique that preserves surface structure under deformation, thereby improving rendering quality after deformation. In our experiments, we demonstrate the superior performance of our method compared to both proxy-based and proxy-free baselines, evaluated on 50 challenging objects from the ShapeNet, Objaverse, and Sketchfab datasets, as well as the NeRF-Synthetic dataset. Code is available at https://github.com/kjae0/SpLap.

</details>


### [19] [Think First, Assign Next (ThiFAN-VQA): A Two-stage Chain-of-Thought Framework for Post-Disaster Damage Assessment](https://arxiv.org/abs/2511.19557)
*Ehsan Karimi,Nhut Le,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出ThiFAN-VQA：在灾害场景中通过两阶段“推理—答案选择”的视觉问答框架，结合CoT与ICL、信息检索和领域提示，减少监督成本并提升准确性、可解释性与适应性；在FloodNet与RescueNet-VQA上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无人机航拍灾害评估需要快速、准确的VQA能力，但训练数据昂贵且稀缺；传统分类式方法答案空间固定，难以泛化；直接用大模型ICL虽灵活但易幻觉、缺乏领域相关性。

Method: 两阶段框架ThiFAN-VQA：1) 通过领域化提示、信息检索与CoT生成结构化推理轨迹；2) 设答案选择模块对多候选响应做一致性与上下文相关性评估，选出最优答案。整体用ICL减监督、用检索降低幻觉、用推理提高可解释性。

Result: 在UAV灾害数据集FloodNet与RescueNet-VQA上，ThiFAN-VQA在准确率、可解释性与适应性方面优于零样本与监督式基线。

Conclusion: 融合检索、领域提示与推理引导的答案选择，可在有限标注条件下实现开放式、稳定且可解释的灾后VQA；有效弥合零样本与监督方法的差距，适用于真实场景的灾损评估。

Abstract: Timely and accurate assessment of damages following natural disasters is essential for effective emergency response and recovery. Recent AI-based frameworks have been developed to analyze large volumes of aerial imagery collected by Unmanned Aerial Vehicles, providing actionable insights rapidly. However, creating and annotating data for training these models is costly and time-consuming, resulting in datasets that are limited in size and diversity. Furthermore, most existing approaches rely on traditional classification-based frameworks with fixed answer spaces, restricting their ability to provide new information without additional data collection or model retraining. Using pre-trained generative models built on in-context learning (ICL) allows for flexible and open-ended answer spaces. However, these models often generate hallucinated outputs or produce generic responses that lack domain-specific relevance. To address these limitations, we propose ThiFAN-VQA, a two-stage reasoning-based framework for visual question answering (VQA) in disaster scenarios. ThiFAN-VQA first generates structured reasoning traces using chain-of-thought (CoT) prompting and ICL to enable interpretable reasoning under limited supervision. A subsequent answer selection module evaluates the generated responses and assigns the most coherent and contextually accurate answer, effectively improve the model performance. By integrating a custom information retrieval system, domain-specific prompting, and reasoning-guided answer selection, ThiFAN-VQA bridges the gap between zero-shot and supervised methods, combining flexibility with consistency. Experiments on FloodNet and RescueNet-VQA, UAV-based datasets from flood- and hurricane-affected regions, demonstrate that ThiFAN-VQA achieves superior accuracy, interpretability, and adaptability for real-world post-disaster damage assessment tasks.

</details>


### [20] [HunyuanOCR Technical Report](https://arxiv.org/abs/2511.19575)
*Hunyuan Vision Team,Pengyuan Lyu,Xingyu Wan,Gengluo Li,Shangpin Peng,Weinong Wang,Liang Wu,Huawen Shen,Yu Zhou,Canhui Tang,Qi Yang,Qiming Peng,Bin Luo,Hower Yang,Houwen Peng,Hongming Yang,Senhao Xie,Binghong Wu,Mana Yang,Sergey Wang,Raccoon Liu,Dick Zhu,Jie Jiang,Linus,Han Hu,Chengquan Zhang*

Main category: cs.CV

TL;DR: HunyuanOCR 是一个约10亿参数的轻量级开源商用级视觉语言模型，面向多种 OCR 任务，在感知与语义两端均优于多种商业 API、传统流水线和更大模型，并在多项基准（含 ICDAR 2025 DIMT 小模型赛道与 <3B VLM 的 OCRBench）夺得领先。


<details>
  <summary>Details</summary>
Motivation: 现有 OCR 方案两极分化：狭窄的“专家模型”功能单一、泛化弱；通用 VLM 又臃肿低效、依赖繁杂的预处理模块，且多任务一致性与端到端能力不足。作者希望用小模型统一多任务、提升效率并解决流水线误差传播问题，同时验证高质量数据与 RL 在 OCR 中的有效性。

Method: 构建由原生 ViT 视觉编码器与轻量 LLM 通过 MLP 适配器衔接的端到端架构；统一支持文本定位与识别（spotting）、结构解析（parsing）、信息抽取（IE）、视觉问答（VQA）及文本图像翻译；采用数据驱动的配方与强化学习策略进行指令/奖励优化；提供基于 vLLM 的高性能推理部署。

Result: 在感知任务（Text Spotting、Parsing）和语义任务（IE、Text Image Translation）超越商用 API、传统流水线及更大模型（如 Qwen3-VL-4B）；获得 ICDAR 2025 DIMT 小模型赛道第一；在 <3B 参数的 VLM 中，OCRBench 达到 SOTA。

Conclusion: 一个约1B参数的端到端 VLM 通过高质量数据与 RL 可在多类 OCR 任务上实现既强又快的统一解决方案，减少预处理依赖与误差传播，并具备工业落地与开源生态价值。

Abstract: This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.
  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.
  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.

</details>


### [21] [Leveraging Unlabeled Scans for NCCT Image Segmentation in Early Stroke Diagnosis: A Semi-Supervised GAN Approach](https://arxiv.org/abs/2511.19576)
*Maria Thoma,Michalis A. Savelonas,Dimitris K. Iakovidis*

Main category: cs.CV

TL;DR: 提出一种基于GAN的半监督分割方法，在少量标注与大量未标注NCCT下识别超早期缺血灶，显著提升AIS早期检出并减轻标注负担。


<details>
  <summary>Details</summary>
Motivation: NCCT是溶栓取栓前的首选影像，但超急性期缺血征象细微、低对比，常被遗漏，延误治疗；临床标注代价高，需能利用未标注数据的算法提升早期检出率。

Method: 构建对抗式半监督分割框架：使用少量标注NCCT与大量未标注NCCT；损失函数组合包括Dice损失、交叉熵、特征匹配损失与自训练损失，以稳定GAN训练并从未标注样本中学习；目标是精确勾画体积小、对比弱的早期梗死区。

Result: 在公开的AISD数据集上进行实验，结果显示所提方法在识别及勾画超早期缺血灶方面具有潜力（文中摘要未给出具体数值，但声称性能与诊断能力提升）。

Conclusion: 该半监督GAN分割策略可在有限标注下提升NCCT对早期脑梗死的可见性与分割精度，降低人工标注负担，并有望加速临床决策流程。

Abstract: Ischemic stroke is a time-critical medical emergency where rapid diagnosis is essential for improving patient outcomes. Non-contrast computed tomography (NCCT) serves as the frontline imaging tool, yet it often fails to reveal the subtle ischemic changes present in the early, hyperacute phase. This limitation can delay crucial interventions. To address this diagnostic challenge, we introduce a semi-supervised segmentation method using generative adversarial networks (GANs) to accurately delineate early ischemic stroke regions. The proposed method employs an adversarial framework to effectively learn from a limited number of annotated NCCT scans, while simultaneously leveraging a larger pool of unlabeled scans. By employing Dice loss, cross-entropy loss, a feature matching loss and a self-training loss, the model learns to identify and delineate early infarcts, even when they are faint or their size is small. Experiments on the publicly available Acute Ischemic Stroke Dataset (AISD) demonstrate the potential of the proposed method to enhance diagnostic capabilities, reduce the burden of manual annotation, and support more efficient clinical decision-making in stroke care.

</details>


### [22] [Multiscale Vector-Quantized Variational Autoencoder for Endoscopic Image Synthesis](https://arxiv.org/abs/2511.19578)
*Dimitrios E. Diamantis,Dimitris K. Iakovidis*

Main category: cs.CV

TL;DR: 提出一种多尺度向量量化VAE（MSVQ‑VAE），可在正常WCE图像中有条件地无缝注入多类异常，生成的合成异常图像用于训练分类器，其性能接近仅用真实数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: WCE产生海量图像需人工筛查，而深度学习CDS受限于隐私与标注成本导致的数据稀缺；现有GAN/VAE合成方法在稳定性和多样性，尤其是异常病灶合成上存在不足，需要更稳健且可控的医学图像生成方法。

Method: 提出MSVQ‑VAE：在VQ‑VAE基础上进行多尺度扩展；支持条件生成，将多种异常（息肉、血管类、炎症）无缝注入到正常WCE图像中；对比实验通过用合成异常图像训练分类器来评估其对CDS的效用。

Result: 使用MSVQ‑VAE生成的异常图像训练的CDS分类器，获得与仅使用真实数据训练的分类器相当的性能；展示了对多种异常类型的生成能力和可控性。

Conclusion: MSVQ‑VAE能稳定、可控地生成多样的WCE异常图像，缓解医学数据稀缺问题，提升CDS训练的可行性，方法具有跨医学多媒体领域的通用性。

Abstract: Gastrointestinal (GI) imaging via Wireless Capsule Endoscopy (WCE) generates a large number of images requiring manual screening. Deep learning-based Clinical Decision Support (CDS) systems can assist screening, yet their performance relies on the existence of large, diverse, training medical datasets. However, the scarcity of such data, due to privacy constraints and annotation costs, hinders CDS development. Generative machine learning offers a viable solution to combat this limitation. While current Synthetic Data Generation (SDG) methods, such as Generative Adversarial Networks and Variational Autoencoders have been explored, they often face challenges with training stability and capturing sufficient visual diversity, especially when synthesizing abnormal findings. This work introduces a novel VAE-based methodology for medical image synthesis and presents its application for the generation of WCE images. The novel contributions of this work include a) multiscale extension of the Vector Quantized VAE model, named as Multiscale Vector Quantized Variational Autoencoder (MSVQ-VAE); b) unlike other VAE-based SDG models for WCE image generation, MSVQ-VAE is used to seamlessly introduce abnormalities into normal WCE images; c) it enables conditional generation of synthetic images, enabling the introduction of different types of abnormalities into the normal WCE images; d) it performs experiments with a variety of abnormality types, including polyps, vascular and inflammatory conditions. The utility of the generated images for CDS is assessed via image classification. Comparative experiments demonstrate that training a CDS classifier using the abnormal images generated by the proposed methodology yield comparable results with a classifier trained with only real data. The generality of the proposed methodology promises its applicability to various domains related to medical multimedia.

</details>


### [23] [SkillSight: Efficient First-Person Skill Assessment with Gaze](https://arxiv.org/abs/2511.19629)
*Chi Hsuan Wu,Kumar Ashutosh,Kristen Grauman*

Main category: cs.CV

TL;DR: SkillSight提出利用第一人称视角中的注视信息与视频联合建模进行技能评估，并蒸馏出仅用注视即可推断的学生模型，在多领域数据集上达SOTA且功耗显著下降。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备（如智能眼镜）上进行自动技能评估面临功耗与实时性瓶颈；现有方法主要依赖连续视频处理，能耗高且难以在野外广泛部署。作者假设“技能不仅体现在动作上，也体现在注意力分配（注视轨迹）上”，从而可借助注视信号降低算力需求。

Method: 提出两阶段框架SkillSight：1）教师模型阶段：联合建模第一人称视频与注视（gaze），学习预测技能等级；2）知识蒸馏阶段：将教师模型的能力迁移给仅输入注视的学生模型，使其在推理时无须连续视频处理，从而节能。

Result: 在涵盖烹饪、音乐、体育的三个数据集上，首次系统证明注视对技能理解的价值。教师模型取得SOTA表现；仅注视的学生模型在保持较高准确率的同时，相比竞品方法功耗降低约73倍。

Conclusion: 注视信号可作为高效代理，实现低功耗的在野技能评估；SkillSight为智能眼镜等设备上的AI辅助技能学习奠定基础，兼顾准确性与能效。

Abstract: Egocentric perception on smart glasses could transform how we learn new skills in the physical world, but automatic skill assessment remains a fundamental technical challenge. We introduce SkillSight for power-efficient skill assessment from first-person data. Central to our approach is the hypothesis that skill level is evident not only in how a person performs an activity (video), but also in how they direct their attention when doing so (gaze). Our two-stage framework first learns to jointly model gaze and egocentric video when predicting skill level, then distills a gaze-only student model. At inference, the student model requires only gaze input, drastically reducing power consumption by eliminating continuous video processing. Experiments on three datasets spanning cooking, music, and sports establish, for the first time, the valuable role of gaze in skill understanding across diverse real-world settings. Our SkillSight teacher model achieves state-of-the-art performance, while our gaze-only student variant maintains high accuracy using 73x less power than competing methods. These results pave the way for in-the-wild AI-supported skill learning.

</details>


### [24] [On the Utility of Foundation Models for Fast MRI: Vision-Language-Guided Image Reconstruction](https://arxiv.org/abs/2511.19641)
*Ruimin Feng,Xingxin He,Ronald Mercer,Zachary Stewart,Fang Liu*

Main category: cs.CV

TL;DR: 提出一种利用视觉-语言基础模型语义先验来提升欠采样MRI重建的框架，通过对齐重建特征与目标语义分布，在保持数据一致性的同时显著改善感知质量和结构细节，并可由图像或图像-文本先验灵活控制重建属性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI重建多依赖手工或低级别先验（稀疏性、TV、DL正则等），难以利用高层语义信息；而视觉-语言基础模型具备跨模态、高层语义表达能力，可能为重建提供更强的上下文约束与可控性。

Method: 构建“语义分布引导”的重建框架：将重建图像与辅助信息（仅图像或图像+文本）输入预训练视觉-语言模型，提取高层语义特征；以对比学习目标将重建表示与目标语义分布对齐，同时与数据一致性项结合。该语义目标可与多种深度重建网络配合，并灵活注入多模态先验以控制重建属性。

Result: 在膝关节与脑部数据集上，基于图像的语义先验能更好保留细微解剖结构，LPIPS更低、Tenengrad更高、读片评分更优，优于传统正则；加入图像-文本信息可拓展语义分布并实现更高层的重建可控性。对比目标在各项评估中均有效地将重建特征牵引至期望语义分布，同时维持数据保真。

Conclusion: 视觉-语言基础模型通过语义空间优化能显著改进欠采样MRI重建的感知质量与结构保真，并提供可控的高层先验，显示该方向在医学成像重建中的潜力。

Abstract: Purpose: To investigate whether a vision-language foundation model can enhance undersampled MRI reconstruction by providing high-level contextual information beyond conventional priors. Methods: We proposed a semantic distribution-guided reconstruction framework that uses a pre-trained vision-language foundation model to encode both the reconstructed image and auxiliary information into high-level semantic features. A contrastive objective aligns the reconstructed representation with the target semantic distribution, ensuring consistency with high-level perceptual cues. The proposed objective works with various deep learning-based reconstruction methods and can flexibly incorporate semantic priors from multimodal sources. To test the effectiveness of these semantic priors, we evaluated reconstruction results guided by priors derived from either image-only or image-language auxiliary information. Results: Experiments on knee and brain datasets demonstrate that semantic priors from images preserve fine anatomical structures and achieve superior perceptual quality, as reflected in lower LPIPS values, higher Tenengrad scores, and improved scores in the reader study, compared with conventional regularization. The image-language information further expands the semantic distribution and enables high-level control over reconstruction attributes. Across all evaluations, the contrastive objective consistently guided the reconstructed features toward the desired semantic distributions while maintaining data fidelity, demonstrating the effectiveness of the proposed optimization framework. Conclusion: The study highlights that vision-language foundation models can improve undersampled MRI reconstruction through semantic-space optimization.

</details>


### [25] [Navigating Gigapixel Pathology Images with Large Multimodal Models](https://arxiv.org/abs/2511.19652)
*Thomas A. Buckley,Kian R. Weihrauch,Katherine Latham,Andrew Z. Zhou,Padmini A. Manrai,Arjun K. Manrai*

Main category: cs.CV

TL;DR: 提出GIANT框架与MultiPathQA基准，使通用LMM以病理学家式导航与推理，在WSI任务上显著优于缩略图/随机patch方法，部分场景逼近或超过专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有通用LMM在医疗影像尤其是病理WSI上的表现不佳，既有评测多用低分辨率缩略图或随机patch，可能低估能力；需要检验并提升LMM在超大分辨率图像上的连贯、可解释推理能力。

Method: 提出GIANT：一个代理式迭代导航框架，使LMM在全视野WSI中像病理医生般多尺度放大、平移、选区取patch、再查询与推理；并发布MultiPathQA基准（934个WSI级问题，覆盖5类临床相关任务，含128个由病理专家撰写、需直接读片的问题）以系统评测。

Result: 在MultiPathQA上，GIANT配合通用LMM显著优于传统缩略图或随机patch基线，性能接近或超过基于大量专门数据训练的模型；例如在专家编写题上，GPT-5+GIANT达62.5%准确率，优于TITAN(43.8%)与SlideChat(37.5%)。

Conclusion: 通过代理式导航与多尺度读片，通用LMM可在病理WSI上实现更强的连贯推理与判读，显示出基础模型用于专家级病理推理的潜力与局限，并为后续LMM在病理学应用的开发与评估提供方向。

Abstract: Despite being widely used to support clinical care, general-purpose large multimodal models (LMMs) have generally shown poor or inconclusive performance in medical image interpretation, particularly in pathology, where gigapixel images are used. However, prior studies have used either low-resolution thumbnails or random patches, which likely underestimated model performance. Here, we ask whether LMMs can be adapted to reason coherently and accurately in the evaluation of such images. In this study, we introduce Gigapixel Image Agent for Navigating Tissue (GIANT), the first framework that allows LMMs to iteratively navigate whole-slide images (WSIs) like a pathologist. Accompanying GIANT, we release MultiPathQA, a new benchmark, which comprises 934 WSI-level questions, encompassing five clinically-relevant tasks ranging from cancer diagnosis to open-ended reasoning. MultiPathQA also includes 128 questions, authored by two professional pathologists, requiring direct slide interpretation. Using MultiPathQA, we show that our simple agentic system substantially outperforms conventional patch- and thumbnail-based baselines, approaching or surpassing the performance of specialized models trained on millions of images. For example, on pathologist-authored questions, GPT-5 with GIANT achieves 62.5% accuracy, outperforming specialist pathology models such as TITAN (43.8%) and SlideChat (37.5%). Our findings reveal the strengths and limitations of current foundation models and ground future development of LMMs for expert reasoning in pathology.

</details>


### [26] [CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization](https://arxiv.org/abs/2511.19661)
*Xinhai Hou,Shaoyuan Xu,Manan Biyani,Mayan Li,Jia Liu,Todd C. Hollon,Bryan Wang*

Main category: cs.CV

TL;DR: 提出CodeV与TAPO，通过对视觉工具调用过程的可验证奖励，提升视觉代理的“忠实”工具使用，同时保持或提升最终准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言代理虽能通过调用图像工具获得高答案准确率，但常出现不忠实的中间推理：对无关区域操作或忽视工具输出也能猜对，导致可解释性与可信度不足。需要一种能评估并提升“工具使用是否真正依据证据”的方法。

Method: 1) 设计忠实度评估协议：检查中间视觉工具输出（如裁剪区域）是否真正包含被查询证据。2) 提出CodeV代理与TAPO（Tool-Aware Policy Optimization）：在代码化工具接口下，以过程级强化学习为核心，直接对视觉工具的输入与输出定义稠密奖励（扩展GRPO），绕开对思维链令牌的奖励，降低奖励投机/黑箱风险。奖励仅基于问题与工具输出，鼓励必要且与证据一致的工具使用。采用两阶段SFT+RL训练。

Result: 在多项视觉搜索基准上，现有方法虽高准确但忠实度低；CodeV在保持或提升最终准确率的同时，显著提高忠实工具使用比例。除视觉搜索外，CodeV在多模态推理与数学基准上也表现强劲。

Conclusion: 仅看最终答案不足以衡量视觉代理的可靠性；通过对中间工具行为进行显式、可验证的监督（TAPO），可构建更可信的具身/代理式视觉推理系统。

Abstract: Agentic vision-language models are increasingly trained to "think with images" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.

</details>


### [27] [OncoVision: Integrating Mammography and Clinical Data through Attention-Driven Multimodal AI for Enhanced Breast Cancer Diagnosis](https://arxiv.org/abs/2511.19667)
*Istiak Ahmed,Galib Ahmed,K. Shahriar Sanjid,Md. Tanzim Hossain,Md. Nishan Khan,Md. Misbah Khan,Md. Arifur Rahman,Sheikh Anisul Haque,Sharmin Akhtar Rupa,Mohammed Mejbahuddin Mia,Mahmud Hasan Mostofa Kamal,Md. Mostafa Kamal Sarker,M. Monir Uddin*

Main category: cs.CV

TL;DR: OncoVision 是一个结合乳腺X线影像与临床数据的多模态AI系统，通过注意力型编码器-解码器同时分割多类ROI并预测结构化临床特征，利用晚期融合策略显著提升诊断准确性与一致性，并以可部署的安全网页应用提供可解释、可扩展的早期乳腺癌筛查与教学支持。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌筛查需要同时理解影像与临床信息，现有方法要么聚焦单一模态、要么缺乏对多种病灶与组织的联合建模，且临床可用性（可解释性、工作流整合、在资源匮乏地区的部署）不足，导致诊断准确性与一致性受限。

Method: 构建注意力驱动的encoder–decoder主干，联合分割四类ROI（肿块、钙化、腋窝征象、乳腺组织），并预测10项结构化临床特征（如肿块形态、钙化类型、ACR密度、BI-RADS等级）。提出两种晚期融合策略将影像输出与临床特征整合。系统以安全、易用的Web应用呈现，提供双重置信度评分与注意力加权可视化，并生成结构化报告。

Result: 在分割四类ROI方面达到SOTA准确度；基于多模态互补性的晚期融合显著提升总体诊断精度并降低阅片者间差异；系统实现实时决策支持与教学可视化。

Conclusion: OncoVision 通过精准分割与临床特征预测并行、加之晚期融合与可解释部署，提升乳腺癌早期检测与诊断一致性；其可扩展、可在低资源地区部署的特性，有望促进公平可及的筛查与及时干预。

Abstract: OncoVision is a multimodal AI pipeline that combines mammography images and clinical data for better breast cancer diagnosis. Employing an attention-based encoder-decoder backbone, it jointly segments four ROIs - masses, calcifications, axillary findings, and breast tissues - with state-of-the-art accuracy and robustly predicts ten structured clinical features: mass morphology, calcification type, ACR breast density, and BI-RADS categories. To fuse imaging and clinical insights, we developed two late-fusion strategies. By utilizing complementary multimodal data, late fusion strategies improve diagnostic precision and reduce inter-observer variability. Operationalized as a secure, user-friendly web application, OncoVision produces structured reports with dual-confidence scoring and attention-weighted visualizations for real-time diagnostic support to improve clinician trust and facilitate medical teaching. It can be easily incorporated into the clinic, making screening available in underprivileged areas around the world, such as rural South Asia. Combining accurate segmentation with clinical intuition, OncoVision raises the bar for AI-based mammography, offering a scalable and equitable solution to detect breast cancer at an earlier stage and enhancing treatment through timely interventions.

</details>


### [28] [INTERLACE: Interleaved Layer Pruning and Efficient Adaptation in Large Vision-Language Models](https://arxiv.org/abs/2511.19676)
*Parsa Madinei,Ryan Solgi,Ziqi Wen,Jonathan Skaza,Miguel Eckstein,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: INTERLACE提出一种针对多模态视觉语言模型（VLM）的层裁剪框架，通过“交错式微调-冻结”在很少数据下保持性能：在每个连续三层中检测前两层的冗余，删除更冗余者、微调保留层、并冻结第三层作为稳定锚点。用1% FineVision、仅1个epoch，在删掉25%层数下仍保留88.9%平均性能，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有层裁剪在VLM上通常导致显著性能下降，尤其缺少在少样本情境下的稳定微调策略。需要一种能识别局部冗余、并在极少数据与计算下可靠恢复性能的方法。

Method: 将网络分割为连续三层的triplet：对前两层做冗余度分析（判定哪一层更冗余），删除更冗余者；对保留的那一层进行小规模微调以弥补容量损失；将第三层冻结为“锚点”，在微调过程中提供稳定特征分布。该“交错式微调-冻结”在裁剪后快速收敛。仅微调子集层、用FineVision 1%数据、1个epoch即可。

Result: 在删去25%网络层的情况下，平均性能保留88.9%；在相同压缩比例下达到SOTA。收敛更快、样本效率高。

Conclusion: INTERLACE通过基于三层局部冗余分析与交错式微调-冻结策略，实现对VLM的有效层裁剪，显著减少参数与计算同时保持高性能，且对数据需求极低，适合资源受限或快速部署场景。

Abstract: We introduce INTERLACE, a novel framework that prunes redundant layers in VLMs while maintaining performance through sample-efficient finetuning. Existing layer pruning methods lead to significant performance drop when applied to VLMs. Instead, we analyze triplets of consecutive layers to identify local redundancy, removing the most redundant of the first two layers, finetune the remaining layer to compensate for the lost capacity, and freeze the third layer to serve as a stable anchor during finetuning. We found that this interleaved finetune-freeze design enables rapid convergence with minimal data after pruning. By finetuning only a subset of layers on just 1% of the FineVision dataset for one epoch, Interlace achieves 88.9% average performance retention after dropping 25% of the network, achieving SOTA performance. Our code is available at: https://github.com/pmadinei/Interlace.git

</details>


### [29] [IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric Assistants](https://arxiv.org/abs/2511.19684)
*Vivek Chavan,Yasmina Imgrund,Tung Dao,Sanwantri Bai,Bosong Wang,Ze Lu,Oliver Heimann,Jörg Krüger*

Main category: cs.CV

TL;DR: IndEgo 是一个聚焦工业场景、兼具第一视角与第三视角、多模态、强调协作的高难度基准数据集，并提供多种任务与基线，显示当前SOTA模型仍有明显挑战。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中的装配/拆卸、物流整理、检修维修、木工等任务复杂且多为多人协作，现有数据集对工业协作、错误检测与程序性/非程序性理解的覆盖不足，缺乏含注视、语音解说、手部姿态等丰富模态的数据与系统评测基准。

Method: 构建包含3460段第一视角（约197小时）和1092段第三视角（约97小时）的多模态数据集，重点采集协作任务；在第一视角中同步记录眼动、解说、声音、运动等信号；提供细粒度标注（动作、摘要、错误标注、解说）、元数据与处理结果（注视点、手部姿态、半稠密点云），并设计基准任务：程序性/非程序性理解、错误检测、推理型问答等；给出若干SOTA模型的基线评测。

Result: 基线实验显示在错误检测、问答与协作任务理解上，现有多模态模型表现受限，说明该数据集难度较高，能有效刻画工业协作与多模态理解中的挑战。

Conclusion: IndEgo 提供了覆盖多任务、多视角、多模态且强调协作与错误感知的工业场景数据与评测框架，为多模态理解、流程认知与人机协作研究提供困难且有代表性的基准，数据已在 Hugging Face 开源。

Abstract: We introduce IndEgo, a multimodal egocentric and exocentric dataset addressing common industrial tasks, including assembly/disassembly, logistics and organisation, inspection and repair, woodworking, and others. The dataset contains 3,460 egocentric recordings (approximately 197 hours), along with 1,092 exocentric recordings (approximately 97 hours). A key focus of the dataset is collaborative work, where two workers jointly perform cognitively and physically intensive tasks. The egocentric recordings include rich multimodal data and added context via eye gaze, narration, sound, motion, and others. We provide detailed annotations (actions, summaries, mistake annotations, narrations), metadata, processed outputs (eye gaze, hand pose, semi-dense point cloud), and benchmarks on procedural and non-procedural task understanding, Mistake Detection, and reasoning-based Question Answering. Baseline evaluations for Mistake Detection, Question Answering and collaborative task understanding show that the dataset presents a challenge for the state-of-the-art multimodal models. Our dataset is available at: https://huggingface.co/datasets/FraunhoferIPK/IndEgo

</details>


### [30] [CountXplain: Interpretable Cell Counting with Prototype-Based Density Map Estimation](https://arxiv.org/abs/2511.19686)
*Abdurahman Ali Mohammed,Wallapak Tavanapong,Catherine Fonder,Donald S. Sakaguchi*

Main category: cs.CV

TL;DR: 提出一种基于原型（prototype）的可解释细胞计数方法，通过密度图估计实现，在两套公开数据上保持计数精度同时提升可解释性，并得到生物学家问卷验证；代码开源。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生物医学影像细胞计数中虽准确，但缺乏可解释性，难以获得临床信任与采纳；需要一种能揭示模型依据且不牺牲性能的方法。

Method: 在密度估计网络中嵌入原型层，学习代表“细胞”和“背景伪影”的视觉原型；通过原型相似度在输入图像上生成解释热图，展示模型如何定位与计数；并以生物学家问卷评估原型的生物学相关性；在两套公开数据集上进行实验。

Result: 在两套公开数据集上，方法在不降低计数精度的情况下，提供了清晰的原型级解释；生物学家调查确认了所学原型与实际细胞/伪影模式的相关性；总体实现“可解释性不牺牲效果”。

Conclusion: 原型驱动的密度图细胞计数框架实现了透明、可验证的决策过程，兼顾性能与可解释性，有助于提升研究与临床对深度学习细胞计数的信任与采用；代码已开源，便于复现与扩展。

Abstract: Cell counting in biomedical imaging is pivotal for various clinical applications, yet the interpretability of deep learning models in this domain remains a significant challenge. We propose a novel prototype-based method for interpretable cell counting via density map estimation. Our approach integrates a prototype layer into the density estimation network, enabling the model to learn representative visual patterns for both cells and background artifacts. The learned prototypes were evaluated through a survey of biologists, who confirmed the relevance of the visual patterns identified, further validating the interpretability of the model. By generating interpretations that highlight regions in the input image most similar to each prototype, our method offers a clear understanding of how the model identifies and counts cells. Extensive experiments on two public datasets demonstrate that our method achieves interpretability without compromising counting effectiveness. This work provides researchers and clinicians with a transparent and reliable tool for cell counting, potentially increasing trust and accelerating the adoption of deep learning in critical biomedical applications. Code is available at https://github.com/NRT-D4/CountXplain.

</details>


### [31] [RADSeg: Unleashing Parameter and Compute Efficient Zero-Shot Open-Vocabulary Segmentation Using Agglomerative Models](https://arxiv.org/abs/2511.19704)
*Omar Alama,Darshil Jariwala,Avigyan Bhattacharya,Seungchan Kim,Wenshan Wang,Sebastian Scherer*

Main category: cs.CV

TL;DR: 提出RADSeg：基于RADIO的零样本开放词汇语义分割方法，在mIoU、推理延迟和参数效率三方面同时提升，小规模模型即达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有OVSS方法要么依赖有限分割数据泛化差，要么对CLIP等VLM做零样本启发式处理效果有限；最强结果常需多模型组合，计算与显存成本高。作者希望找到既高效又高精度、可零样本泛化的统一方案。

Method: 利用被忽视的聚合型视觉基础模型RADIO作为骨干，提出RADSeg：1）自相关递归注意力，2）自相关全局聚合，3）高效的掩码精炼。并对RADIO在零样本OVSS场景进行系统研究与改进。

Result: 在ViT-base级别模型上，mIoU提升6–30%；推理速度提高约3.95倍；参数量减少约2.5倍。RADSeg-base（105M参数）在mIoU上超过由超大视觉模型（850–1350M）组合的以往方法。

Conclusion: RADIO作为聚合型视觉基础模型在零样本OVSS中极具潜力；通过自相关注意与高效精炼，RADSeg以更少参数与更低延迟取得SOTA，为低成本高精度的通用语义分割提供新范式。

Abstract: Open-vocabulary semantic segmentation (OVSS) underpins many vision and robotics tasks that require generalizable semantic understanding. Existing approaches either rely on limited segmentation training data, which hinders generalization, or apply zero-shot heuristics to vision-language models (e.g CLIP), while the most competitive approaches combine multiple models to improve performance at the cost of high computational and memory demands. In this work, we leverage an overlooked agglomerative vision foundation model, RADIO, to improve zero-shot OVSS along three key axes simultaneously: mIoU, latency, and parameter efficiency. We present the first comprehensive study of RADIO for zero-shot OVSS and enhance its performance through self-correlating recursive attention, self-correlating global aggregation, and computationally efficient mask refinement. Our approach, RADSeg, achieves 6-30% mIoU improvement in the base ViT class while being 3.95x faster and using 2.5x fewer parameters. Surprisingly, RADSeg-base (105M) outperforms previous combinations of huge vision models (850-1350M) in mIoU, achieving state-of-the-art accuracy with substantially lower computational and memory cost.

</details>


### [32] [Rethinking Vision Transformer Depth via Structural Reparameterization](https://arxiv.org/abs/2511.19718)
*Chengwei Zhou,Vipin Chaudhary,Gourav Datta*

Main category: cs.CV

TL;DR: 提出一种分支式结构重参数化训练法，将多分支Transformer块在推理时精确合并为单路径，从而用更少层数保持表征能力，并在移动CPU上获得最高37%的加速。


<details>
  <summary>Details</summary>
Motivation: 现有ViT加速多集中在令牌剪枝与注意力加速等算法层面，忽视了从网络深度结构本身降复杂度的可能性；问题是能否在不损失表征能力的前提下减少堆叠层数。

Method: 在训练阶段为每个Transformer块（FFN与MHSA）引入并行分支，并设计在非线性单元入口处逐步合并的重参数化机制；到推理时，将多分支精确数学合并为单路径，无近似误差；据此将ViT-Tiny从12层压缩为6/4/3层。

Result: 在ImageNet-1K上保持分类精度的同时，将ViT-Tiny层数显著减少；在移动CPU上推理最高提速约37%。

Conclusion: 超深堆叠并非构建高效视觉Transformer的唯一途径；分支式重参数化为以更少层数获得相当表征能力提供了新思路，开启构建高效ViT的新机会。

Abstract: The computational overhead of Vision Transformers in practice stems fundamentally from their deep architectures, yet existing acceleration strategies have primarily targeted algorithmic-level optimizations such as token pruning and attention speedup. This leaves an underexplored research question: can we reduce the number of stacked transformer layers while maintaining comparable representational capacity? To answer this, we propose a branch-based structural reparameterization technique that operates during the training phase. Our approach leverages parallel branches within transformer blocks that can be systematically consolidated into streamlined single-path models suitable for inference deployment. The consolidation mechanism works by gradually merging branches at the entry points of nonlinear components, enabling both feed-forward networks (FFN) and multi-head self-attention (MHSA) modules to undergo exact mathematical reparameterization without inducing approximation errors at test time. When applied to ViT-Tiny, the framework successfully reduces the original 12-layer architecture to 6, 4, or as few as 3 layers while maintaining classification accuracy on ImageNet-1K. The resulting compressed models achieve inference speedups of up to 37% on mobile CPU platforms. Our findings suggest that the conventional wisdom favoring extremely deep transformer stacks may be unnecessarily restrictive, and point toward new opportunities for constructing efficient vision transformers.

</details>


### [33] [Maritime Small Object Detection from UAVs using Deep Learning with Altitude-Aware Dynamic Tiling](https://arxiv.org/abs/2511.19728)
*Sakib Ahmed,Oscar Pizarro*

Main category: cs.CV

TL;DR: 提出一种结合高度感知缩放与自适应切片的动态分块方法，用于提升UAV海上搜救中小目标检测；在SeaDronesSee+YOLOv5+SAHI上，小目标mAP提升38%，推理速度较静态分块提升超过2倍。


<details>
  <summary>Details</summary>
Motivation: 高空航拍中小目标因目标像素占比低而难以检测，传统静态切片要么计算开销大、要么检测效果不稳，需要一种既提升小目标检测、又控制计算量的方法以满足SAR实时性与精度。

Method: 依据无人机飞行高度进行图像缩放，并结合自适应的切片因子按需对图像进行动态分块；将该策略集成至SAHI推理流程，与YOLOv5检测器联动，实现对不同高度条件下的最佳分块与尺度选择，以减少冗余计算并保持/提升小目标召回。

Result: 在SeaDronesSee数据集上评估：相较基线，小目标mAP提升38%；与静态切片方案相比，推理速度提升超过2倍，同时保持或提升检测性能。

Conclusion: 高度感知的动态分块在SAR场景中显著提升小目标检测的效率与准确性，可在多样飞行高度和海况下更稳健工作，为实际UAV搜救部署提供更佳的实时性与性能平衡。

Abstract: Unmanned Aerial Vehicles (UAVs) are crucial in Search and Rescue (SAR) missions due to their ability to monitor vast maritime areas. However, small objects often remain difficult to detect from high altitudes due to low object-to-background pixel ratios. We propose an altitude-aware dynamic tiling method that scales and adaptively subdivides the image into tiles for enhanced small object detection. By integrating altitude-dependent scaling with an adaptive tiling factor, we reduce unnecessary computation while maintaining detection performance. Tested on the SeaDronesSee dataset [1] with YOLOv5 [2] and Slicing Aided Hyper Inference (SAHI) framework [3], our approach improves Mean Average Precision (mAP) for small objects by 38% compared to a baseline and achieves more than double the inference speed compared to static tiling. This approach enables more efficient and accurate UAV-based SAR operations under diverse conditions.

</details>


### [34] [Efficient Transferable Optimal Transport via Min-Sliced Transport Plans](https://arxiv.org/abs/2511.19741)
*Xinran Liu,Elaheh Akbari,Rocio Diaz Martin,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.CV

TL;DR: 论文提出并系统研究最小切片输运计划（min-STP）的可迁移性：在一对分布上学到的最佳一维投影（slicer）能否在分布轻微变化时仍产生高质量的跨域匹配。理论给出在小扰动下 slicer 稳定与可迁移性；算法上引入小批量版本并给出统计保证；实证显示在点云配准与流式生成建模中具备强的一次性匹配与摊销训练效果。


<details>
  <summary>Details</summary>
Motivation: 经典最优传输（OT）计算昂贵，限制了在形状分析、图像生成与多模态对齐中的可扩展性。基于切片的方案利用1D OT闭式解加速，但尚不清楚“学到的最佳投影”在分布移位下是否可复用，这对动态数据或重复、相近任务的OT尤为关键。

Method: 研究并扩展 min-Sliced Transport Plan（min-STP）框架：通过优化一维投影得到条件传输计划，最小化高维传输代价；给出对分布微扰时优化投影的稳定性理论；提出小批量（minibatch）min-STP并建立统计精度保证；在点云对齐与流式生成中以“可迁移 slicer”进行一次性匹配与摊销训练。

Result: 理论结果：当数据分布发生小扰动时，优化得到的 slicer 与其最优解保持接近，从而支持跨相近任务的高效迁移。算法结果：小批量 min-STP 在计算上更可扩展，并具备有界统计误差。实验结果：在点云配准与流式生成建模场景中，迁移式 min-STP 实现强的一次性匹配性能，并提升摊销训练效率。

Conclusion: min-STP 的优化投影在分布轻微变化下具有稳定性与可迁移性；小批量设计进一步提升规模化能力并有统计保证。该方法在实践中实现高效且可迁移的OT近似，可用于重复或相关分布对的匹配与生成任务。

Abstract: Optimal Transport (OT) offers a powerful framework for finding correspondences between distributions and addressing matching and alignment problems in various areas of computer vision, including shape analysis, image generation, and multimodal tasks. The computation cost of OT, however, hinders its scalability. Slice-based transport plans have recently shown promise for reducing the computational cost by leveraging the closed-form solutions of 1D OT problems. These methods optimize a one-dimensional projection (slice) to obtain a conditional transport plan that minimizes the transport cost in the ambient space. While efficient, these methods leave open the question of whether learned optimal slicers can transfer to new distribution pairs under distributional shift. Understanding this transferability is crucial in settings with evolving data or repeated OT computations across closely related distributions. In this paper, we study the min-Sliced Transport Plan (min-STP) framework and investigate the transferability of optimized slicers: can a slicer trained on one distribution pair yield effective transport plans for new, unseen pairs? Theoretically, we show that optimized slicers remain close under slight perturbations of the data distributions, enabling efficient transfer across related tasks. To further improve scalability, we introduce a minibatch formulation of min-STP and provide statistical guarantees on its accuracy. Empirically, we demonstrate that the transferable min-STP achieves strong one-shot matching performance and facilitates amortized training for point cloud alignment and flow-based generative modeling.

</details>


### [35] [Leveraging Foundation Models for Histological Grading in Cutaneous Squamous Cell Carcinoma using PathFMTools](https://arxiv.org/abs/2511.19751)
*Abdul Rahman Diab,Emily E. Karn,Renchin Wu,Emily S. Ruiz,William Lotter*

Main category: cs.CV

TL;DR: 提出PathFMTools，一个轻量可扩展的Python工具包，用于高效运行、分析和可视化病理基础模型，并用其在cSCC组织学分级任务上评估CONCH与MUSK，比较多种适配策略，显示用基础模型嵌入训练小型专科模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 病理基础模型潜力巨大，但在特定临床任务上的适配困难：WSI处理复杂、特征不可解释、适配策略繁多且效果不明，需要统一、便捷的工具来执行与评估，并在真实任务上验证其临床可用性。

Method: 开发PathFMTools，支持高效执行、分析、可视化病理基础模型；以440张cSCC H&E全切片为数据集，接入并评估两种SOTA视觉-语言基础模型（CONCH、MUSK）在组织学分级任务上的表现；基准测试多种模型适配策略（直接预测、嵌入下游训练等），比较其权衡。

Result: 在cSCC分级任务中，不同适配策略存在性能与资源开销的权衡；使用基础模型嵌入训练小型专科模型表现出良好潜力；PathFMTools实现了高效的实验执行与可视化评估。

Conclusion: PathFMTools为病理基础模型的临床落地提供了实用基础设施；通过cSCC分级案例验证了其价值，并表明利用基础模型嵌入构建小型专科模型是一条有前景的路径，支持在真实临床场景中的高效分析与验证。

Abstract: Despite the promise of computational pathology foundation models, adapting them to specific clinical tasks remains challenging due to the complexity of whole-slide image (WSI) processing, the opacity of learned features, and the wide range of potential adaptation strategies. To address these challenges, we introduce PathFMTools, a lightweight, extensible Python package that enables efficient execution, analysis, and visualization of pathology foundation models. We use this tool to interface with and evaluate two state-of-the-art vision-language foundation models, CONCH and MUSK, on the task of histological grading in cutaneous squamous cell carcinoma (cSCC), a critical criterion that informs cSCC staging and patient management. Using a cohort of 440 cSCC H&E WSIs, we benchmark multiple adaptation strategies, demonstrating trade-offs across prediction approaches and validating the potential of using foundation model embeddings to train small specialist models. These findings underscore the promise of pathology foundation models for real-world clinical applications, with PathFMTools enabling efficient analysis and validation.

</details>


### [36] [What You See is (Usually) What You Get: Multimodal Prototype Networks that Abstain from Expensive Modalities](https://arxiv.org/abs/2511.19752)
*Muchang Bahng,Charlie Berens,Jon Donnelly,Eric Chen,Chaofan Chen,Cynthia Rudin*

Main category: cs.CV

TL;DR: 提出一种可解释、成本感知的多模态原型网络，用图像与基因两种模态按权重集成，并能在无需昂贵基因数据时自动跳过，保持与双模态模型相当的准确度。


<details>
  <summary>Details</summary>
Motivation: 生态监测与入侵物种识别需要自动、准确且可解释的物种判定；传统多模态网络难以解释且基因数据获取昂贵且侵入性强，亟需一种既可解释又能减少昂贵模态使用的方案。

Method: 将ProtoPNet扩展到多模态与成本感知场景：为每个模态学习与聚合原型（prototype），通过可学习权重反映预测对各模态的依赖；设计策略识别在图像模态已足够自信时跳过基因模态，仅在细粒度难例上请求基因数据。

Result: 模型能智能分配基因数据的使用：对于视觉易区分的样本主要依赖图像，对于细粒度相似物种才调用基因信息；在减少基因数据开销的同时，其准确率与始终使用双模态的模型相当。

Conclusion: 可解释的多模态ProtoPNet在保持性能的同时降低昂贵基因数据需求并提升决策透明度，适用于生态监测与保育中的成本受限物种识别。

Abstract: Species detection is important for monitoring the health of ecosystems and identifying invasive species, serving a crucial role in guiding conservation efforts. Multimodal neural networks have seen increasing use for identifying species to help automate this task, but they have two major drawbacks. First, their black-box nature prevents the interpretability of their decision making process. Second, collecting genetic data is often expensive and requires invasive procedures, often necessitating researchers to capture or kill the target specimen. We address both of these problems by extending prototype networks (ProtoPNets), which are a popular and interpretable alternative to traditional neural networks, to the multimodal, cost-aware setting. We ensemble prototypes from each modality, using an associated weight to determine how much a given prediction relies on each modality. We further introduce methods to identify cases for which we do not need the expensive genetic information to make confident predictions. We demonstrate that our approach can intelligently allocate expensive genetic data for fine-grained distinctions while using abundant image data for clearer visual classifications and achieving comparable accuracy to models that consistently use both modalities.

</details>


### [37] [Vision--Language Enhanced Foundation Model for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.19759)
*Jiaqi Guo,Mingzhen Li,Hanyu Su,Santiago López,Lexiaozi Fan,Daniel Kim,Aggelos Katsaggelos*

Main category: cs.CV

TL;DR: 提出VESSA：把VLM增强的参考引导分割融入半监督医学分割，两阶段训练/使用，显著提升低标注条件下的精度。


<details>
  <summary>Details</summary>
Motivation: 医学分割对大量专家标注依赖高；SSL可减标注但伪标签质量与语义泛化有限。VLM具跨域泛化与小样本能力，但尚未有效融入医学SSL。需要将VLM的视觉-语义先验用于提升SSL中的伪标签与监督质量。

Method: 两阶段：1) 训练VLM增强的分割基础模型VESSA。构建金标准模板库；给定输入-模板对，通过视觉特征匹配提取模板分割的语义与空间线索，生成结构化提示，送入受SAM2启发的mask decoder输出掩码，模拟小样本学习。2) 将VESSA嵌入先进SSL框架，与学生模型动态交互：随学生预测变好，将其预测回馈给VESSA作为提示，VESSA据此生成更高质量伪标签与指导，实现迭代提升。

Result: 在多数据集与多领域医学分割实验中，VESSA增强的SSL在极少标注场景下显著提升分割精度，超越当前SOTA基线。

Conclusion: 将VLM的基础级视觉-语义理解引入SSL，通过参考引导与动态交互提高伪标签与监督质量，实证显示在低标注条件下能稳定且显著提升医学分割性能。

Abstract: Semi-supervised learning (SSL) has emerged as an effective paradigm for medical image segmentation, reducing the reliance on extensive expert annotations. Meanwhile, vision-language models (VLMs) have demonstrated strong generalization and few-shot capabilities across diverse visual domains. In this work, we integrate VLM-based segmentation into semi-supervised medical image segmentation by introducing a Vision-Language Enhanced Semi-supervised Segmentation Assistant (VESSA) that incorporates foundation-level visual-semantic understanding into SSL frameworks. Our approach consists of two stages. In Stage 1, the VLM-enhanced segmentation foundation model VESSA is trained as a reference-guided segmentation assistant using a template bank containing gold-standard exemplars, simulating learning from limited labeled data. Given an input-template pair, VESSA performs visual feature matching to extract representative semantic and spatial cues from exemplar segmentations, generating structured prompts for a SAM2-inspired mask decoder to produce segmentation masks. In Stage 2, VESSA is integrated into a state-of-the-art SSL framework, enabling dynamic interaction with the student model: as student predictions become more refined, they are fed back to VESSA as prompts, allowing it to generate higher-quality pseudo-labels and stronger guidance. Extensive experiments across multiple segmentation datasets and domains show that VESSA-augmented SSL significantly enhances segmentation accuracy, outperforming state-of-the-art baselines under extremely limited annotation conditions.

</details>


### [38] [A Storage-Efficient Feature for 3D Concrete Defect Segmentation to Replace Normal Vector](https://arxiv.org/abs/2511.19760)
*Linxin Hua,Jianghua Deng,Ye Lu*

Main category: cs.CV

TL;DR: 提出相对角特征替代法向量用于混凝土表面损伤点云分析，在几乎不降精度下显著压缩输入与存储。


<details>
  <summary>Details</summary>
Motivation: 图像法易受背景噪声影响，而点云虽稳健但三维数据量大、算力与存储开销高；需要找到能保留损伤判别信息、又能降低数据维度与冗余的点云特征。

Method: 定义“相对角”：单点法向与其母点云平均法向之间的夹角，形成1维方向性特征；用熵/信息增益评估其在损伤与未损伤区域的信息性与冗余抑制能力；以PointNet++为骨干，分别以相对角和完整法向为输入训练/测试，对比性能与资源占用。

Result: 相对角能在未损伤区滤除冗余、在损伤区保留有效信息；与法向输入模型精度相当，同时实现27.6%存储降低与83%输入通道压缩。

Conclusion: 相对角作为单维特征在不改动网络结构的前提下，接近法向效果却显著减负，可在资源受限硬件上支持更大batch与更高效的点云损伤检测。

Abstract: Point cloud reconstruction of damage offers an effective solution to image-based methods vulnerable to background noise, yet its application is constrained by the high volume of 3D data. This study proposes a new feature, relative angle, computed as the angle between the normal vector of a point and the average normal vector of its parent point cloud. This single-dimensional feature provides directionality information equivalent to normal vectors for concrete surface defect characteristics. Through entropy-based feature evaluation, this study demonstrates the ability of relative angle to filter out redundant information in undamaged sections while retaining effective information in damaged sections. By training and testing with PointNet++, models based on the relative angles achieved similar performance to that of models based on normal vectors while delivering 27.6% storage reduction and 83% input channel compression. This novel feature has the potential to enable larger-batch execution on resource-constrained hardware without the necessity of architectural modifications to models.

</details>


### [39] [Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2511.19765)
*Ali Torabi,Sanjog Gaihre,Yaqoob Majeed*

Main category: cs.CV

TL;DR: 提出CrispFormer：在SegFormer解码器上做三点小改动（边界分支、基于不确定性的精炼、动态多尺度融合），在弱监督语义分割中显著提升边界与小目标表现和mIoU，且计算开销小、实现简单。


<details>
  <summary>Details</summary>
Motivation: WSSS需要从嘈杂、欠充分的弱标注中学习致密掩码，常出现边界模糊、尺度选择不当、噪声标签放大等问题。现有方法常依赖重后处理或改动主干网络，复杂且泛化受限。作者想以最小改动提升弱监督下的解码质量。

Method: 保持MiT主干与WSSS流水线（seed→student→EMA重标注）不变，仅在SegFormer解码器加入三项：1) 边界分支：轻量边缘头+边界感知损失，专门监督细边界；2) 不确定性引导精炼：预测像素级偶然不确定性，用于损失加权并门控对分割logits的残差修正；3) 动态多尺度融合：以空间softmax门控替换静态级联，对多分辨率特征进行位置自适应选择，可选用不确定性调制。

Result: 在相同seed条件下，相比SegFormer基线，CrispFormer单次前向即可得到边界更清晰、尺度选择更佳且更抗噪的掩码，稳定提升边界F-score、小目标召回率和mIoU，同时仅增加极小计算量。

Conclusion: 以解码器为核心的小改动即可在WSSS中获得更高保真度的掩码；方法实现简单、与各类SegFormer变体兼容，为基于图像级监督的分割提供可复现实用路径。

Abstract: Weakly supervised semantic segmentation (WSSS) must learn dense masks from noisy, under-specified cues. We revisit the SegFormer decoder and show that three small, synergistic changes make weak supervision markedly more effective-without altering the MiT backbone or relying on heavy post-processing. Our method, CrispFormer, augments the decoder with: (1) a boundary branch that supervises thin object contours using a lightweight edge head and a boundary-aware loss; (2) an uncertainty-guided refiner that predicts per-pixel aleatoric uncertainty and uses it to weight losses and gate a residual correction of the segmentation logits; and (3) a dynamic multi-scale fusion layer that replaces static concatenation with spatial softmax gating over multi-resolution features, optionally modulated by uncertainty. The result is a single-pass model that preserves crisp boundaries, selects appropriate scales per location, and resists label noise from weak cues. Integrated into a standard WSSS pipeline (seed, student, and EMA relabeling), CrispFormer consistently improves boundary F-score, small-object recall, and mIoU over SegFormer baselines trained on the same seeds, while adding minimal compute. Our decoder-centric formulation is simple to implement, broadly compatible with existing SegFormer variants, and offers a reproducible path to higher-fidelity masks from image-level supervision.

</details>


### [40] [Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering](https://arxiv.org/abs/2511.19768)
*Noah Frahm,Prakrut Patel,Yue Zhang,Shoubin Yu,Mohit Bansal,Roni Sengupta*

Main category: cs.CV

TL;DR: 提出Prune-Then-Plan，通过统计检验式剪枝+覆盖率规划，缓解VLM在EQA探索中的前沿振荡与过度自信，显著提升导航稳定性与问答表现。


<details>
  <summary>Details</summary>
Motivation: VLM在EQA中提供强语义先验，但直接用其逐步探索会因过度自信与校准不良导致前沿振荡（来回折返）、低效导航与答案退化，需要一种能稳定化、可解释地校准步级决策的方法。

Method: 两阶段框架：1) Prune：对VLM对各前沿候选的打分进行Holm-Bonferroni启发的多重检验式剪枝，剔除不可信/不合理候选，降低过度自信影响；2) Plan：将剩余候选交给覆盖率驱动的规划器，根据场景覆盖最大化选择下一步行动。该分离使VLM从高自信预测转化为保守、可解释的动作。方法集成到3D-Mem EQA。

Result: 在OpenEQA与EXPRESS-Bench上，在相同探索预算下获得更好的场景覆盖；相对基线在视觉落地SPL提升最高49%，在LLM-Match提升33%。

Conclusion: 通过统计剪枝+覆盖规划的分离式校准，可稳健地抑制VLM逐步探索中的振荡并提升EQA整体性能，具备简单有效、可解释、可集成的优点。

Abstract: Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.

</details>


### [41] [One Attention, One Scale: Phase-Aligned Rotary Positional Embeddings for Mixed-Resolution Diffusion Transformer](https://arxiv.org/abs/2511.19778)
*Haoyu Wu,Jingyi Xu,Qiaomu Miao,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 论文指出：在扩散Transformer进行混合分辨率去噪时，直接用线性插值重映射RoPE会导致注意力塌缩。作者提出训练无关的CRPA方法，通过在每次注意力计算中将所有Q/K位置统一到查询的步长坐标，从而对齐相位、消除混叠，恢复稳定注意力与高保真生成。


<details>
  <summary>Details</summary>
Motivation: 混合分辨率（不同网格/步长）场景下，常见做法是对RoPE进行线性坐标重映射。但当不同分辨率token混合时，单个注意力头需要比较以不同采样率获取的相位，导致相位混叠与不稳定。预训练的DiT对RoPE相位极度敏感，轻微的不一致就会造成模糊、伪影或完全失效。因此需要一种无需再训练、可直接修复该结构性问题的方法。

Method: 提出Cross-Resolution Phase-Aligned Attention (CRPA)：不改动模型权重，只在每次注意力调用时修改RoPE索引映射。具体做法是将所有键值的位置坐标映射到与查询相同的步长（stride）/分辨率，使得相同的物理距离对应相同的RoPE相位增量，从而避免跨采样率比较导致的相位别名。该方案训练无关、可直接作为drop-in替换。

Result: CRPA在预训练DiT上即插即用，能在所有头与层中统一稳定注意力，避免混合分辨率时的塌缩与伪影，实现更高保真且高效的混合分辨率生成。实验显示在图像与视频生成上优于此前SOTA方法。

Conclusion: 混合分辨率下RoPE线性重映射会引发结构性相位混叠，是DiT注意力塌缩的核心根因。CRPA通过将Q/K位置统一到查询步长实现相位对齐，训练无关、兼容预训练模型并显著提升稳定性与质量，是混合分辨率生成的有效解决方案。

Abstract: We identify a core failure mode that occurs when using the usual linear interpolation on rotary positional embeddings (RoPE) for mixed-resolution denoising with Diffusion Transformers. When tokens from different spatial grids are mixed, the attention mechanism collapses. The issue is structural. Linear coordinate remapping forces a single attention head to compare RoPE phases sampled at incompatible rates, creating phase aliasing that destabilizes the score landscape. Pretrained DiTs are especially brittle-many heads exhibit extremely sharp, periodic phase selectivity-so even tiny cross-rate inconsistencies reliably cause blur, artifacts, or full collapse.
  To this end, our main contribution is Cross-Resolution Phase-Aligned Attention (CRPA), a training-free drop-in fix that eliminates this failure at its source. CRPA modifies only the RoPE index map for each attention call: all Q/K positions are expressed on the query's stride so that equal physical distances always induce identical phase increments. This restores the precise phase patterns that DiTs rely on. CRPA is fully compatible with pretrained DiTs, stabilizes all heads and layers uniformly. We demonstrate that CRPA enables high-fidelity and efficient mixed-resolution generation, outperforming previous state-of-the-art methods on image and video generation.

</details>


### [42] [Reading Between the Lines: Abstaining from VLM-Generated OCR Errors via Latent Representation Probes](https://arxiv.org/abs/2511.19806)
*Jihan Yao,Achin Kulshrestha,Nathalie Rauschmayr,Reed Roberts,Banghua Zhu,Yulia Tsvetkov,Federico Tombari*

Main category: cs.CV

TL;DR: 提出LRP，用轻量探针读出VLM内部表征的不确定性，较现有基于输出校准/一致性的方法在STVQA等任务上显著提升弃答准确率。


<details>
  <summary>Details</summary>
Motivation: VLM在安全关键场景（如含OCR的STVQA）需要在不确定时可靠弃答；现有方法依赖失校准概率或语义一致性，不适用于OCR，提示应从模型内部表征寻找不确定性信号。

Method: 提出Latent Representation Probing（LRP）：在VLM的隐藏状态或注意力上训练轻量探针用于不确定性/弃答判别。设计三类探针：1) 跨所有层的隐藏表示拼接；2) 聚合视觉token注意力；3) 多单层探针集成并以多数投票融合。

Result: 在涵盖图像与视频的四个基准上，LRP相较最佳基线将弃答准确率提升7.6%。探针能跨多种不确定性来源与数据集泛化，且最有效信号多出现在中间层而非最终层。

Conclusion: 通过从VLM内部状态而非不可靠的输出分布提取置信信号，可构建更可部署的AI系统；LRP为安全关键应用中的可靠弃答提供了原则化框架。

Abstract: As VLMs are deployed in safety-critical applications, their ability to abstain from answering when uncertain becomes crucial for reliability, especially in Scene Text Visual Question Answering (STVQA) tasks. For example, OCR errors like misreading "50 mph" as "60 mph" could cause severe traffic accidents. This leads us to ask: Can VLMs know when they can't see? Existing abstention methods suggest pessimistic answers: they either rely on miscalibrated output probabilities or require semantic agreement unsuitable for OCR tasks. However, this failure may indicate we are looking in the wrong place: uncertainty signals could be hidden in VLMs' internal representations.
  Building on this insight, we propose Latent Representation Probing (LRP): training lightweight probes on hidden states or attention patterns. We explore three probe designs: concatenating representations across all layers, aggregating attention over visual tokens, and ensembling single layer probes by majority vote. Experiments on four benchmarks across image and video modalities show LRP improves abstention accuracy by 7.6\% over best baselines. Our analysis reveals: probes generalize across various uncertainty sources and datasets, and optimal signals emerge from intermediate rather than final layers. This establishes a principled framework for building deployment-ready AI systems by detecting confidence signals from internal states rather than unreliable outputs.

</details>


### [43] [Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization](https://arxiv.org/abs/2511.19811)
*Debin Meng,Chen Jin,Zheng Gao,Yanran Li,Ioannis Patras,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 提出TPSO：通过在token与prompt嵌入空间中可学习扰动，训练免且模型无关地提升文本到图像扩散模型的多样性，同时保持画质。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成常坍缩到强模态，导致多样性低、输出重复；现有方法（重采样、重写提示、引导控制）要么仍坍缩，要么引入失真降低质量，缺乏既提升多样性又不牺牲保真度的通用训练免方案。

Method: 提出Token-Prompt embedding Space Optimization（TPSO）。在不改动主模型与无需再训练的前提下，引入可学习参数对token嵌入空间进行探索，偏向被低估/代表性不足的区域以避免强模态复用；同时在prompt级嵌入空间施加全局语义约束，调节分布偏移，抑制画质和语义漂移。可作为即插即用模块，适用于多种扩散骨干。

Result: 在MS-COCO与三种扩散骨干上，TPSO显著提升生成多样性，报告的基线多样性指标从1.10提升到4.18点，且未观察到画质下降。

Conclusion: TPSO提供了一种训练免、模型无关的多样性增强机制，通过在token与prompt嵌入空间联合优化，缓解模态坍缩并保持高保真度，对创意生成与下游应用具有实用价值；代码将于接收后开源。

Abstract: Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.

</details>


### [44] [CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception](https://arxiv.org/abs/2511.19820)
*Miguel Carvalho,Helder Dias,Bruno Martins*

Main category: cs.CV

TL;DR: 提出CropVLM：在不改动/微调原VLM的前提下，通过“动态放大/裁剪”相关图像区域，显著提升细粒度视觉任务表现。模型用强化学习训练，无需人工框框或合成评测，训练一次可泛化搭配多种开源/闭源VLM，尤其提升跨域高分辨率理解任务，同时避免遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在需要细粒度视觉理解（如场景文字识别、文档解析）时易受感知分辨率不足与视觉碎片化影响，导致识别细节能力弱；同时直接微调大模型成本高且有遗忘风险。

Method: 设计一个外接的裁剪/缩放策略器CropVLM：在推理时动态选择并放大与问题相关的子区域；使用强化学习训练选择策略，不依赖人工标注框或昂贵的合成监督；训练完成后作为通用前端，与不同VLM无缝组合。

Result: 在多项需要高分辨率细节的基准上显著提升性能，尤其在目标VLM的域外数据上效果明显；无需修改或微调底层VLM即可获得改进。

Conclusion: 通过RL驱动的动态裁剪机制，低成本增强VLM的细粒度视觉理解能力，实现一次训练、广泛适配、避免灾难性遗忘，并在高分辨率/跨域任务上带来显著收益。

Abstract: Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.

</details>


### [45] [ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding](https://arxiv.org/abs/2511.19827)
*Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: ReDirector提出一种可控相机的视频重拍生成方法，通过纠正RoPE使用方式并引入相机条件的RoCE，使模型在不同相机轨迹与视频长度下保持几何一致性和可控性，提升动态目标定位与静态背景保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频重拍/相机可控生成方法在动态场景与不同长度视频上泛化差、相机轨迹可控性弱；尤其常见的RoPE用法未对齐输入视频与目标重拍的时空位置，导致几何不一致与漂移。

Method: 1) 重新对齐输入与目标视频的时空位置，纠正以往对RoPE的误用；2) 提出Rotary Camera Encoding（RoCE），在RoPE中引入由相机参数驱动的相位偏移，实现跨视角与跨视频的关系建模；3) 将相机条件融入时空位置编码，使模型适配分布外的相机轨迹与可变视频长度。

Result: 在多种相机轨迹和视频长度的实验中，相比以往方法显著提升：相机可控性、更好的几何一致性、更高的视频质量；并在动态物体定位与静态背景保持方面表现更优。

Conclusion: 通过相机条件化的RoPE（RoCE）与时空对齐，ReDirector能有效泛化到OOD相机轨迹和视频长度，实现更稳定、可控的重拍视频生成。

Abstract: We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.

</details>


### [46] [Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19834)
*Haoqing Li,Jun Shi,Xianmeng Chen,Qiwei Jia,Rui Wang,Wei Wei,Hong An,Xiaowen Hu*

Main category: cs.CV

TL;DR: 提出BHD-RAG：将DCLD领域知识与临床先例检索注入MLLM，以改进CT下BHD诊断的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: DCLD（弥漫性囊性肺病）样本稀缺、类别间差异细微，深度学习易过拟合且难区分；通用MLLM在医学影像缺乏领域知识与可参考放射学要点，易产生幻觉，影响对BHD等罕见病的诊断。需要一种将领域知识与证据引入的机制来增强可靠性与可解释性。

Method: 提出多模态RAG框架BHD-RAG：1）训练/设计专用代理从CT生成影像表现描述，构建DCLD多模态病例语料库（图像-描述对）；2）用余弦相似度检索器为查询CT找到最相关的图像-描述证据；3）将检索到的证据与当前影像一并输入MLLM，由其综合生成诊断与证据性描述。

Result: 在包含四类DCLD的数据集上验证，BHD-RAG获得更高的诊断准确率，并生成与专家意见高度一致、基于证据的影像描述。

Conclusion: 融合领域知识与检索证据的多模态RAG可显著缓解MLLM幻觉，提升BHD等罕见DCLD的CT诊断准确性与可解释性。

Abstract: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnosis via Computed Tomography (CT) imaging. While Multimodal Large Language Models (MLLMs) demonstrate diagnostic potential fo such rare diseases, the absence of domain-specific knowledge and referable radiological features intensify hallucination risks. To address this problem, we propose BHD-RAG, a multimodal retrieval-augmented generation framework that integrates DCLD-specific expertise and clinical precedents with MLLMs to improve BHD diagnostic accuracy. BHDRAG employs: (1) a specialized agent generating imaging manifestation descriptions of CT images to construct a multimodal corpus of DCLDs cases. (2) a cosine similarity-based retriever pinpointing relevant imagedescription pairs for query images, and (3) an MLLM synthesizing retrieved evidence with imaging data for diagnosis. BHD-RAG is validated on the dataset involving four types of DCLDs, achieving superior accuracy and generating evidence-based descriptions closely aligned with expert insights.

</details>


### [47] [Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation](https://arxiv.org/abs/2511.19835)
*Xuewen Liu,Zhikai Li,Jing Zhang,Mengjuan Chen,Qingyi Gu*

Main category: cs.CV

TL;DR: 提出Rectified SpaAttn：在保持视频扩散Transformer生成质量的同时，用纠偏稀疏注意力实现2–3.3×加速。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer视频生成计算瓶颈在于注意力的二次复杂度。现有稀疏注意力虽降算量，但因过度强调“关键token”和完全忽略“非关键token”而系统性偏置，导致显著性能劣化。

Method: 以“隐式全注意力”为参照，校正稀疏注意力分配，使稀疏图与全注意力图对齐。具体包括：1) 对关键token，证明其偏置与稀疏权重成比例，比例受放大权控制；据此提出“孤立-池化注意力再分配”（Isolated-Pooling Attention Reallocation），通过多模态池化权重再分配精确估计校正因子。2) 对非关键token，从池化的query-key恢复注意力可带来收益但引入池化误差；为此提出“增益感知池化校正”（Gain-Aware Pooling Rectification），确保纠正后的收益超过误差。并在Triton中实现融合内核Rectified SpaAttn。

Result: 在HunyuanVideo与Wan 2.1上集成后，分别获得最高3.33×与2.08×速度提升，同时保持高质量生成（无明显退化）。

Conclusion: 通过显式建模并纠正稀疏注意力的系统性偏差，Rectified SpaAttn在不牺牲质量前提下显著加速视频扩散Transformer，方法工程可用（开源），具备通用性与可扩展性。

Abstract: Diffusion Transformers dominate video generation, but the quadratic complexity of attention computation introduces substantial latency. Attention sparsity reduces computational costs by focusing on critical tokens while ignoring non-critical tokens. However, existing methods suffer from severe performance degradation. In this paper, we revisit attention sparsity and reveal that existing methods induce systematic biases in attention allocation: (1) excessive focus on critical tokens amplifies their attention weights; (2) complete neglect of non-critical tokens causes the loss of relevant attention weights. To address these issues, we propose Rectified SpaAttn, which rectifies attention allocation with implicit full attention reference, thereby enhancing the alignment between sparse and full attention maps. Specifically: (1) for critical tokens, we show that their bias is proportional to the sparse attention weights, with the ratio governed by the amplified weights. Accordingly, we propose Isolated-Pooling Attention Reallocation, which calculates accurate rectification factors by reallocating multimodal pooled weights. (2) for non-critical tokens, recovering attention weights from the pooled query-key yields attention gains but also introduces pooling errors. Therefore, we propose Gain-Aware Pooling Rectification, which ensures that the rectified gain consistently surpasses the induced error. Moreover, we customize and integrate the Rectified SpaAttn kernel using Triton, achieving up to 3.33 and 2.08 times speedups on HunyuanVideo and Wan 2.1, respectively, while maintaining high generation quality. We release Rectified SpaAttn as open-source at https://github.com/BienLuky/Rectified-SpaAttn .

</details>


### [48] [4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models](https://arxiv.org/abs/2511.19836)
*Yiting Lu,Wei Luo,Peiyan Tu,Haoran Li,Hanxin Zhu,Zihao Yu,Xingrui Wang,Xinyi Chen,Xinge Peng,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出4DWorldBench，一套统一评测“世界生成”模型（3D/4D）的基准，覆盖感知质量、条件-4D对齐、物理真实感与4D一致性，并通过将多模态条件映射到文本空间与多种裁判（LLM/MLLM/传统网络）融合，实现更全面一致的评估，且与人类评价更一致。


<details>
  <summary>Details</summary>
Motivation: 现有基准分散、关注维度不一，缺乏对“世界真实感”（空间/时间/物理/指令一致性）的统一评测；世界生成应用广泛（VR、自动驾驶、具身智能、内容创作），急需客观对比与推进。

Method: 构建4DWorldBench：
- 任务覆盖Image-to-3D/4D、Video-to-4D、Text-to-3D/4D。
- 四大评测维度：感知质量、条件-4D对齐、物理真实、4D一致性。
- 自适应多模态条件：将各模态条件统一映射到文本空间。
- 评审融合：LLM-as-judge、MLLM-as-judge与传统网络指标结合；根据任务自适应选择工具。
- 进行初步人类研究验证一致性。

Result: 统一评测框架能更全面衡量对齐性、物理性与跨模态一致性；自适应工具选择与文本统一条件带来更稳定、与人类评价更接近的评分。

Conclusion: 4DWorldBench为世界生成模型提供客观、统一的评测基准，有望加速从“视觉生成”向“世界生成”的转变；项目代码与资源已公开。

Abstract: World Generation Models are emerging as a cornerstone of next-generation multimodal intelligence systems. Unlike traditional 2D visual generation, World Models aim to construct realistic, dynamic, and physically consistent 3D/4D worlds from images, videos, or text. These models not only need to produce high-fidelity visual content but also maintain coherence across space, time, physics, and instruction control, enabling applications in virtual reality, autonomous driving, embodied intelligence, and content creation. However, prior benchmarks emphasize different evaluation dimensions and lack a unified assessment of world-realism capability. To systematically evaluate World Models, we introduce the 4DWorldBench, which measures models across four key dimensions: Perceptual Quality, Condition-4D Alignment, Physical Realism, and 4D Consistency. The benchmark covers tasks such as Image-to-3D/4D, Video-to-4D, Text-to-3D/4D. Beyond these, we innovatively introduce adaptive conditioning across multiple modalities, which not only integrates but also extends traditional evaluation paradigms. To accommodate different modality-conditioned inputs, we map all modality conditions into a unified textual space during evaluation, and further integrate LLM-as-judge, MLLM-as-judge, and traditional network-based methods. This unified and adaptive design enables more comprehensive and consistent evaluation of alignment, physical realism, and cross-modal coherence. Preliminary human studies further demonstrate that our adaptive tool selection achieves closer agreement with subjective human judgments. We hope this benchmark will serve as a foundation for objective comparisons and improvements, accelerating the transition from "visual generation" to "world generation." Our project can be found at https://yeppp27.github.io/4DWorldBench.github.io/.

</details>


### [49] [Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum](https://arxiv.org/abs/2511.19846)
*Thomas M Metz,Matthew Q Hill,Alice J O'Toole*

Main category: cs.CV

TL;DR: 提出IMIC策略，将物体识别、不同质量人脸识别与全身人物识别统一到单一嵌入空间，几乎不遗忘，基于EVA-02与CLIP的模型在四任务上达域内专家水平并优于人类多任务表现，同时保持OOD泛化。


<details>
  <summary>Details</summary>
Motivation: 基础视觉模型在零样本物体分类与经微调的人脸/人物识别上各有长处，但常因单任务微调导致灾难性遗忘，难以统一多身份相关任务且保持泛化能力。需要一种训练方案能同时学习多域身份表征、避免遗忘，并维持基础模型的分布外泛化。

Method: 提出Interleaved Multi-Domain Identity Curriculum (IMIC) 的两种变体：一种梯度耦合、交错式多域训练日程，同时在四个任务（物体分类、高/低质人脸识别、全身人物识别）上微调同一基础骨干（DINOv3、CLIP、EVA-02）。采用共享嵌入空间，交替喂入各域数据，梯度耦合以平衡多任务更新，形成统一的多域身份表征。

Result: 在EVA-02与CLIP基座上，IMIC模型在四项任务上与各自领域专家模型表现相当，并在跨任务多任务评测上优于人类；DINOv3也有效。方法未显著损害分布外泛化能力。对最优变体（EVA-02+IMIC A/B）的分析显示：四任务在统一嵌入中可线性可分且存在大量特征共享；任一任务提取的前<100个主成分即可在其它任务上几乎无性能损失。

Conclusion: IMIC实现了在单一嵌入空间内的多域身份与物体识别，显著缓解灾难性遗忘并保持OOD泛化；该统一表征具备线性可分与高共享性，显示基础模型可被稳健地多任务微调而不牺牲泛化。

Abstract: Vision foundation models can perform generalized object classification in zero-shot mode, and face/person recognition when they are fine-tuned. However, fine-tuned models suffer from catastrophic forgetting. We create models that perform four tasks (object recognition, face recognition from high- and low-quality images, and person recognition from whole-body images) in a single embedding space -- without incurring substantial catastrophic forgetting. To accomplish this, we introduce two variants of the Interleaved Multi-Domain Identity Curriculum (IMIC): a gradient-coupled, interleaving training schedule that fine-tunes a foundation backbone simultaneously on all four tasks. The IMIC method proved effective with three foundation model bases: DINOv3, CLIP, and EVA-02. Two of these (EVA-02 and CLIP) performed comparably with domain experts on all four tasks concurrently and were more accurate than humans at multi-tasking across face, body, and object datasets. Further, we demonstrate that our approach does not substantially harm out-of-distribution generalization, thus maintaining a key property of foundation models. Analysis of the most accurate model variants (EVA-02 + IMIC A and B) showed linearly separable representations of the four tasks in the unified embedding space, but with substantial sharing of features across tasks. Fewer than 100 PCs calculated from any one task could perform all other tasks with nearly zero performance degradation.

</details>


### [50] [DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction](https://arxiv.org/abs/2511.19850)
*Jiahui Sun,Junran Lu,Jinhui Yin,Yishuo Xu,Yuanqi Li,Yanwen Guo*

Main category: cs.CV

TL;DR: 提出DOGE框架：无需曲线标注，直接从分割掩码全局优化得到贝塞尔图，实现高精度道路网络矢量化，并在SpaceNet与CityScale上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于折线的道路抽取方法难以准确表达弯曲道路的曲率与连续性，且高质量曲线矢量真值难以构建，限制了曲线化表示的训练与应用。

Method: 以可微参数曲线为核心，引入“贝塞尔图”作为道路网络表示。将从影像到矢量的学习重构为在贝塞尔图空间上的全局优化：1) DiffAlign：通过可微渲染将曲线与分割掩码对齐，连续优化几何参数；2) TopoAdapt：通过离散运算子对图的拓扑（节点、边、连通性）进行重整；两模块交替迭代，无需曲线GT，仅依赖栅格分割监督。

Result: 在SpaceNet与CityScale大规模基准上取得新的SOTA，生成高保真道路网络矢量图，优于现有多项方法。

Conclusion: 曲线先验的贝塞尔图结合可微渲染与离散拓扑优化，提供了无需曲线标注的端到端道路矢量化新范式，兼顾几何精度与拓扑正确性；代码与数据将开源。

Abstract: Automatic extraction of road networks from aerial imagery is a fundamental task, yet prevailing methods rely on polylines that struggle to model curvilinear geometry. We maintain that road geometry is inherently curve-based and introduce the Bézier Graph, a differentiable parametric curve-based representation. The primary obstacle to this representation is to obtain the difficult-to-construct vector ground-truth (GT). We sidestep this bottleneck by reframing the task as a global optimization problem over the Bézier Graph. Our framework, DOGE, operationalizes this paradigm by learning a parametric Bézier Graph directly from segmentation masks, eliminating the need for curve GT. DOGE holistically optimizes the graph by alternating between two complementary modules: DiffAlign continuously optimizes geometry via differentiable rendering, while TopoAdapt uses discrete operators to refine its topology. Our method sets a new state-of-the-art on the large-scale SpaceNet and CityScale benchmarks, presenting a new paradigm for generating high-fidelity vector maps of road networks. We will release our code and related data.

</details>


### [51] [STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction](https://arxiv.org/abs/2511.19854)
*Jiankuo Zhao,Xiangyu Zhu,Zidu Wang,Zhen Lei*

Main category: cs.CV

TL;DR: STAvatar提出基于UV的软绑定与时间自适应密度控制，在单目视频下重建高保真、可动画三维头部头像，显著改善细节与遮挡区域重建，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有以3D Gaussian Splatting为基础的头像重建常将高斯绑定到网格三角形，并仅用LBS建模形变，导致运动僵硬、表情表达力不足；同时缺乏对易遮挡区域（口腔内部、眼睑）的专门处理，细节与鲁棒性受限。

Method: 1) UV-Adaptive Soft Binding：在UV空间中为每个高斯学习特征偏移，结合图像与几何先验进行软绑定；支持动态重采样，与Adaptive Density Control(ADC)全面兼容，提高形状与纹理自适应性。2) Temporal ADC：先对结构相似帧聚类以更有针对性地计算致密化准则；再提出融合感知误差的克隆准则，同时度量几何与纹理差异，引导在需要细节的区域进行致密化。

Result: 在四个基准数据集上取得SOTA重建质量，尤其在细粒度细节和频繁遮挡区域（口腔、眼睑）重建方面显著优于现有方法。

Conclusion: 通过UV软绑定与时间维度的自适应致密化，STAvatar克服了传统LBS刚性与遮挡处理不足的问题，提升单目视频三维头像的可动画性、细节与稳健性；方法通用且将开源。

Abstract: Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task. Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness. Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids). To address these limitations, we propose STAvatar, which consists of two key components: (1) a UV-Adaptive Soft Binding framework that leverages both image-based and geometric priors to learn per-Gaussian feature offsets within the UV space. This UV representation supports dynamic resampling, ensuring full compatibility with Adaptive Density Control (ADC) and enhanced adaptability to shape and textural variations. (2) a Temporal ADC strategy, which first clusters structurally similar frames to facilitate more targeted computation of the densification criterion. It further introduces a novel fused perceptual error as clone criterion to jointly capture geometric and textural discrepancies, encouraging densification in regions requiring finer details. Extensive experiments on four benchmark datasets demonstrate that STAvatar achieves state-of-the-art reconstruction performance, especially in capturing fine-grained details and reconstructing frequently occluded regions. The code will be publicly available.

</details>


### [52] [Temporal-Visual Semantic Alignment: A Unified Architecture for Transferring Spatial Priors from Vision Models to Zero-Shot Temporal Tasks](https://arxiv.org/abs/2511.19856)
*Xiangkai Ma,Han Zhang,Wenzhong Li,Sanglu Lu*

Main category: cs.CV

TL;DR: TimeArtist提出将时间序列与视觉概念在语义层面对齐的跨模态生成框架，通过“预热-对齐”两阶段学习，实现从时间序列直接生成高质量多样图像并进行风格/模式迁移，同时在零样本时间序列任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在文本-图像对齐出色，但将非视觉的连续时间序列作为条件信号用于高保真图像生成仍未被充分探索；把序列转为“伪图像”的做法缺乏语义级对齐，无法捕捉时间波动与视觉概念之间的语义映射。

Method: 提出TimeArtist：1) 预热阶段（self-supervised）训练双自编码器与共享量化器，学习跨模态共享离散表示；2) 冻结编码器与量化器后，引入投影头，将时间序列与视觉样本在表示空间对齐；3) 在该对齐空间中进行条件图像生成，实现基于时间波动模式的风格迁移与多样图像合成。

Result: 在图像生成指标上取得满意表现，并在零样本时间序列任务上优于现有方法；能够从时间序列生成高质量且多样的图像，同时捕捉时间波动并体现在图像风格上。

Conclusion: TimeArtist建立了时间动态与视觉语义之间的语义级桥梁，提出通用的跨模态生成新范式，为由时间序列驱动的图像生成与风格迁移提供了有效路径。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress in aligning and generating content across text and image modalities. However, the potential of using non-visual, continuous sequential, as a conditioning signal for high-fidelity image generation remains largely unexplored. Furthermore, existing methods that convert series into "pseudo-images" for temporal forecasting fail to establish semantic-level alignment. In this paper, we propose TimeArtist, a temporal-visual conversion framework that pioneers semantic-level alignment between time series fluctuations and visual concepts. It pioneers a "warmup-align" paradigm: first, a dual-autoencoder and shared quantizer are self-supervised trained on large-scale datasets to learn modality-shared representations. Then, the encoders and quantizer are frozen, and a projection is introduced to align temporal and visual samples at the representation level. TimeArtist establishes a versatile cross-modal framework, enabling high-quality, diverse image generation directly from time series, while capturing temporal fluctuation patterns to render images as styles transfer. Extensive experiments show that TimeArtist achieves satisfactory performance in image generation metrics, while also attaining superior results in zero-shot temporal tasks. Our work establishes a new paradigm for cross-modal generation, bridging the gap between temporal dynamics and visual semantics.

</details>


### [53] [GigaWorld-0: World Models as Data Engine to Empower Embodied AI](https://arxiv.org/abs/2511.19861)
*GigaWorld Team,Angen Ye,Boyuan Wang,Chaojun Ni,Guan Huang,Guosheng Zhao,Haoyun Li,Jiagang Zhu,Kerui Li,Mengyuan Xu,Qiuping Deng,Siting Wang,Wenkang Qin,Xinze Chen,Xiaofeng Wang,Yankai Wang,Yu Cao,Yifan Chang,Yuan Xu,Yun Ye,Yang Wang,Yukun Zhou,Zhengyuan Zhang,Zhehao Dong,Zheng Zhu*

Main category: cs.CV

TL;DR: 提出GigaWorld-0，一个用于视觉-语言-行动（VLA）学习的数据引擎式统一世界模型，结合可控视频生成与物理一致的3D生成/重建/规划，以规模化合成高质量具身交互数据，并通过高效训练框架支撑大规模训练，使在其合成数据上训练的VLA在真实机器人上实现强泛化与任务成功，无需真实交互数据。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能需要大规模、高多样性且可控的交互数据，但真实采集昂贵且难以覆盖，现有生成方法常缺乏几何与物理一致性或难以精细控制外观与动作语义，限制了VLA的可扩展训练与真实世界迁移。

Method: 构建统一世界模型数据引擎，包括：1）GigaWorld-0-Video：大规模视频生成，支持对外观、相机视角与动作语义的细粒度控制，生成多样且时序连贯的具身视频；2）GigaWorld-0-3D：融合3D生成建模、3D高斯点云重建、可微物理系统辨识与可执行运动规划，保证几何一致与物理可信；两者联合优化以合成视觉上可信、空间一致、物理合理且指令对齐的数据。配套GigaTrain训练框架，使用FP8与稀疏注意力降低算力与显存开销，实现规模化训练。

Result: 系统性评估表明生成数据质量高、可控性强、维度多样。以这些数据训练的VLA（如GigaBrain-0）在真实机器人任务上表现强，泛化与任务成功率显著提升，且训练阶段不需要任何真实世界交互。

Conclusion: 以可控视频生成与物理一致3D建模为核心的统一世界模型数据引擎能规模化合成高质量具身数据，并有效提升VLA在现实环境中的表现，为数据高效的具身智能提供可扩展路径。

Abstract: World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.

</details>


### [54] [MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization](https://arxiv.org/abs/2511.19878)
*Chengyue Huang,Mellon M. Zhang,Robert Azarcon,Glen Chou,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出MAPS：一种针对视觉-语言-行动(VLA)模型的模块级接近度调度细化方法，在不增参不增数据的前提下显著提升ID与OOD泛化（最高+30%）。


<details>
  <summary>Details</summary>
Motivation: VLA从预训练VLM继承强先验，但常规微调会破坏表示、削弱泛化。现有做法要么冻结模块导致过度约束，要么对各子模块一刀切正则，忽视它们在感知/语言/动作中的差异角色，难以兼顾稳定性与可塑性。

Method: 提出MAPS（Module-Wise Proximity Scheduling）：基于对各模块对泛化影响的系统性分析，按经验确定“放松顺序”，线性调度各子模块与预训练先验的接近度约束——视觉编码器保持更强接近度（更稳），与动作相关的语言层获得更大自由度（更塑）。无需新增参数或数据，可直接集成现有VLA。

Result: 在MiniVLA-VQ、MiniVLA-OFT、OpenVLA-OFT及SimplerEnv、CALVIN、LIBERO等基准与Franka Panda真实机器人评测中，MAPS稳定提升ID与OOD表现，最高提升约30%。

Conclusion: 针对VLM到VLA迁移，按模块经验引导的“接近预训练先验”的调度是一种简单而有效的原则：既保持广泛泛化，又允许动作相关层充分适应。MAPS提供了稳健的细化框架，可无缝落地。

Abstract: Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.

</details>


### [55] [ChessMamba: Structure-Aware Interleaving of State Spaces for Change Detection in Remote Sensing Images](https://arxiv.org/abs/2511.19882)
*Lei Ding,Tong Liu,Xuanguang Liu,Xiangyun Liu,Haitao Guo,Jun Lu*

Main category: cs.CV

TL;DR: 提出ChessMamba，用棋盘交错+蛇形扫描将多时相特征在一次前向中序列化，并结合结构感知多膨胀卷积融合中心与角点邻域，在多种遥感变化检测任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 多时相遥感影像存在异源性与时空错位，现有Transformer/状态空间方法在时间序列化时破坏局部结构一致性，导致判别线索被掩盖、定位不稳。需要一种既能稳健对齐互动，又保留局部结构约束的CD框架。

Method: 提出ChessMamba框架：1) SpatialMamba编码器+轻量跨源交互模块；2) 棋盘式交错配合蛇形扫描，将多时相特征交错为单一序列，一次前向完成时相间直接对比、缩短交互路径；3) 结构感知融合，利用多膨胀卷积选择性捕获单时相内中心与角点邻域上下文，实现鲁棒融合。

Result: 在三个任务（二值CD、语义CD、多模态建筑损伤评估）上，ChessMamba有效融合异构特征，显著提升精度，超越多种SOTA方法；代码将开源于github.com/DingLei14/ChessMamba。

Conclusion: 通过结构感知的交错序列化与多尺度卷积融合，ChessMamba在存在时空错位与异源性的多时相遥感CD中实现更准确、稳健的变化定位，具备通用性与高效性。

Abstract: Change detection (CD) in multitemporal remote sensing imagery presents significant challenges for fine-grained recognition, owing to heterogeneity and spatiotemporal misalignment. However, existing methodologies based on vision transformers or state-space models typically disrupt local structural consistency during temporal serialization, obscuring discriminative cues under misalignment and hindering reliable change localization. To address this, we introduce ChessMamba, a structure-aware framework leveraging interleaved state-space modeling for robust CD with multi-temporal inputs. ChessMamba integrates a SpatialMamba encoder with a lightweight cross-source interaction module, featuring two key innovations: (i) Chessboard interleaving with snake scanning order, which serializes multi-temporal features into a unified sequence within a single forward pass, thereby shortening interaction paths and enabling direct comparison for accurate change localization; and (ii) Structure-aware fusion via multi-dilated convolutions, selectively capturing center-and-corner neighborhood contexts within each mono-temporal. Comprehensive evaluations on three CD tasks, including binary CD, semantic CD and multimodal building damage assessment, demonstrate that ChessMamba effectively fuses heterogeneous features and achieves substantial accuracy improvements over state-of-the-art methods.The relevant code will be available at: github.com/DingLei14/ChessMamba.

</details>


### [56] [Distilling Cross-Modal Knowledge via Feature Disentanglement](https://arxiv.org/abs/2511.19887)
*Junhong Liu,Yuan Zhang,Tao Huang,Wenchao Xu,Renyu Yang*

Main category: cs.CV

TL;DR: 提出频率解耦的跨模态蒸馏：低频强对齐、高频弱对齐，并配合尺度一致性损失和共享分类器，显著提升视觉→语言等跨模态KD效果。


<details>
  <summary>Details</summary>
Motivation: 传统KD在跨模态（如视觉到语言）场景中表现不佳，主要因为不同模态表征不一致，尤其是细节与噪声成分导致难以直接迁移，亟需一种能稳定提取并对齐可迁移知识的机制。

Method: 在频域对特征进行解耦：观测到低频跨模态一致性高、高频一致性低，于是对低频特征施加强对齐损失，对高频特征采用宽松对齐；同时引入尺度一致性损失以缓解模态间分布偏移，并使用共享分类器统一表征空间。

Result: 在多个基准数据集上，相较传统KD及最新跨模态KD方法均有显著性能提升；开源代码表明方法可复现。

Conclusion: 频域解耦与差异化对齐策略能够有效提升跨模态知识迁移的稳定性与效率，结合尺度一致性与共享分类器构成通用、可复用的跨模态蒸馏框架。

Abstract: Knowledge distillation (KD) has proven highly effective for compressing large models and enhancing the performance of smaller ones. However, its effectiveness diminishes in cross-modal scenarios, such as vision-to-language distillation, where inconsistencies in representation across modalities lead to difficult knowledge transfer. To address this challenge, we propose frequency-decoupled cross-modal knowledge distillation, a method designed to decouple and balance knowledge transfer across modalities by leveraging frequency-domain features. We observed that low-frequency features exhibit high consistency across different modalities, whereas high-frequency features demonstrate extremely low cross-modal similarity. Accordingly, we apply distinct losses to these features: enforcing strong alignment in the low-frequency domain and introducing relaxed alignment for high-frequency features. We also propose a scale consistency loss to address distributional shifts between modalities, and employ a shared classifier to unify feature spaces. Extensive experiments across multiple benchmark datasets show our method substantially outperforms traditional KD and state-of-the-art cross-modal KD approaches. Code is available at https://github.com/Johumliu/FD-CMKD.

</details>


### [57] [LiMT: A Multi-task Liver Image Benchmark Dataset](https://arxiv.org/abs/2511.19889)
*Zhe Liu,Kai Han,Siqi Ma,Yan Zhu,Jun Chen,Chongwen Lyu,Xinyi Qiu,Chengxuan Qian,Yuqing Song,Yi Liu,Liyuan Tian,Yang Ji,Yuefeng Li*

Main category: cs.CV

TL;DR: 提出一个动脉期增强CT的多任务肝脏数据集LiMT，支持分割、检测与多标签分类，并给出基线与相关综述。


<details>
  <summary>Details</summary>
Motivation: 现有肝脏CAD数据集多为单任务、分散且存在跨数据集异质性，限制了多任务学习与任务间关联的探索与应用。

Method: 构建包含150例CT体积的公共多任务数据集LiMT，覆盖四类肝病与正常；由资深临床医生精标肝脏、肿瘤、病灶检测框及多标签；统一源自动脉期增强CT，提供基线实验与对相关数据集/方法的回顾。

Result: 形成可同时用于肝与肿瘤分割、病灶检测和多标签分类的高质量数据集；给出了相应任务的基线结果，并整理了相关研究现状。

Conclusion: LiMT为肝脏影像领域提供了一个统一的多任务公共资源，有望促进多任务学习、减少跨数据集异质性影响并推动CAD研究。

Abstract: Computer-aided diagnosis (CAD) technology can assist clinicians in evaluating liver lesions and intervening with treatment in time. Although CAD technology has advanced in recent years, the application scope of existing datasets remains relatively limited, typically supporting only single tasks, which has somewhat constrained the development of CAD technology. To address the above limitation, in this paper, we construct a multi-task liver dataset (LiMT) used for liver and tumor segmentation, multi-label lesion classification, and lesion detection based on arterial phase-enhanced computed tomography (CT), potentially providing an exploratory solution that is able to explore the correlation between tasks and does not need to worry about the heterogeneity between task-specific datasets during training. The dataset includes CT volumes from 150 different cases, comprising four types of liver diseases as well as normal cases. Each volume has been carefully annotated and calibrated by experienced clinicians. This public multi-task dataset may become a valuable resource for the medical imaging research community in the future. In addition, this paper not only provides relevant baseline experimental results but also reviews existing datasets and methods related to liver-related tasks. Our dataset is available at https://drive.google.com/drive/folders/1l9HRK13uaOQTNShf5pwgSz3OTanWjkag?usp=sharing.

</details>


### [58] [VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering](https://arxiv.org/abs/2511.19899)
*Yuyi Li,Daoyuan Chen,Zhen Wang,Yutong Lu,Yaliang Li*

Main category: cs.CV

TL;DR: 提出“生成-验证”框架，构建跨模态一致性校验的SVQA数据集VeriSciQA（2万+问答），显著提升开源LVLM在科学图文问答上的表现，并作为更具挑战的基准。


<details>
  <summary>Details</summary>
Motivation: 开源LVLM在科学图文问答（SVQA）上表现落后，关键瓶颈是缺乏大规模高质量的公开数据集。现有用LVLM合成数据的方法存在系统性错误，源于模型局限与图文信息不对称，导致问答对质量不稳。

Method: 提出以“先生成、后验证”为核心的流程：首先结合图表关联文本生成QA对；随后对照图像进行跨模态一致性检查，并结合多种辅助过滤器，剔除错误问答。以此流程构建VeriSciQA，覆盖20个学科领域与12类图表，总计20,351个QA对。

Result: VeriSciQA对开源模型形成挑战：领先开源模型在该集上的准确率约64%，而专有模型约82%，存在明显差距。在VeriSciQA上微调的模型在多项SVQA基准上均有稳定提升，且随数据规模增加而收益扩大，优于用现有数据集训练的模型。人工评估也验证了VeriSciQA正确性更高。

Conclusion: 验证为中心的可扩展数据构建框架能持续扩展并提升SVQA数据质量，从而推动开源LVLM在科学图文问答能力上的进步；VeriSciQA既是高质量训练资源，也是更具区分度的评测基准。

Abstract: Large Vision-Language Models (LVLMs) show promise for scientific applications, yet open-source models still struggle with Scientific Visual Question Answering (SVQA), namely answering questions about figures from scientific papers. A key bottleneck lies in the lack of public, large-scale, high-quality SVQA datasets. Although recent work uses LVLMs to synthesize data at scale, we identify systematic errors in their resulting QA pairs, stemming from LVLMs' inherent limitations and information asymmetry between figures and text. To address these challenges, we propose a verification-centric Generate-then-Verify framework that first generates QA pairs with figure-associated textual context, then applies cross-modal consistency checks against figures along with auxiliary filters to eliminate erroneous pairs. We instantiate this framework to curate VeriSciQA, a dataset of 20,351 QA pairs spanning 20 scientific domains and 12 figure types. VeriSciQA poses a challenging benchmark for open-source models, with a substantial accuracy gap between the leading open-source models (64%) and a proprietary model (82%). Moreover, models fine-tuned on VeriSciQA achieve consistent improvements on SVQA benchmarks, with performance gains that scale with data size and surpass models trained on existing datasets. Human evaluation further validates the superior correctness of VeriSciQA. Together, these evidences demonstrate that continued data expansion by our scalable framework can further advance SVQA capability in the open-source community.

</details>


### [59] [Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning](https://arxiv.org/abs/2511.19900)
*Jiaqi Liu,Kaiwen Xiong,Peng Xia,Yiyang Zhou,Haonian Ji,Lu Feng,Siwei Han,Mingyu Ding,Huaxiu Yao*

Main category: cs.CV

TL;DR: 提出Agent0-VL：一种无需外部奖励、通过工具集成实现自我演化的视觉-语言智能体，在几何与视觉科学任务上较基座模型提升12.5%。


<details>
  <summary>Details</summary>
Motivation: 纯文本自评难以验证复杂视觉推理、易产生评估幻觉；人类标注与外部奖励受限。需要一种能够利用工具进行证据支撑的自我评估与自我修正机制，实现持续自改进。

Method: 在单一LVLM中统一两种角色：Solver执行多轮工具集成推理，Verifier以工具支撑生成结构化反馈与细粒度自奖励。两者在“自进化推理循环”中交互：通过工具验证与强化学习共同对齐推理与评估分布，实现稳定自改进。全流程零外部奖励与零人工标注。

Result: 在几何问题求解与视觉科学分析基准上，相较基座模型总体提升约12.5%，展示出持续自提升能力。

Conclusion: 将工具集成用于推理、评估与修复，可在无外部奖励与人工标注条件下稳定提升VL智能体的推理与自评一致性，带来持续性能增长。

Abstract: Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at \href{https://github.com/aiming-lab/Agent0/Agent0-VL}{this https URL}.

</details>


### [60] [MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition](https://arxiv.org/abs/2511.19907)
*Mingyu Zhao,Zhanfu Yang,Yang Zhou,Zhaoyang Xia,Can Jin,Xiaoxiao He,Carol Neidle,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出一种用于连续手语识别的多模态方法：先用机器学习检测手语片段的起止边界，再对片段进行识别；融合3D骨架与3D手型信息以稳健定位边界，并将边界用于后续识别，在ASLLRP上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 连续手语识别难点在于自动分割手语词的起止边界，且自然语境中的手型与动作在边界处有系统性变化；单一视觉模态或仅依赖2D信息鲁棒性不足。作者希望通过3D骨架与手型知识提高边界检测与整体识别性能。

Method: 1) 提取3D骨架特征，利用其在手语边界处的聚类特性训练边界检测器；2) 预训练87类语言学定义的规范手型分类器（整合多数据集构建训练集），捕捉常在词首/词末出现的手型；3) 设计多模态融合模块，将视频分割网络与手型分类器统一用于边界检测；4) 用估计到的边界进行分段，并在包含孤立词与人工切分的连续词的大规模数据上训练/应用识别模型。

Result: 在ASLLRP语料库上，相较先前方法取得显著性能提升（边界检测更准确，整体识别率提高）。

Conclusion: 多模态融合的边界检测（3D骨架+手型）能有效提升连续手语识别；使用不同来源的训练数据（孤立词与连续词片段）进一步增强识别效果。

Abstract: This paper presents a multimodal approach for continuous sign recognition that first uses machine learning to detect the start and end frames of signs in videos of American Sign Language (ASL) sentences, and then recognizes the segmented signs. For improved robustness, we use 3D skeletal features extracted from sign language videos to capture the convergence of sign properties and their dynamics, which tend to cluster at sign boundaries. Another focus of this work is the incorporation of information from 3D handshape for boundary detection. To detect handshapes normally expected at the beginning and end of signs, we pretrain a handshape classifier for 87 linguistically defined canonical handshape categories using a dataset that we created by integrating and normalizing several existing datasets. A multimodal fusion module is then used to unify the pretrained sign video segmentation framework and the handshape classification models. Finally, the estimated boundaries are used for sign recognition, where the recognition model is trained on a large database containing both citation-form isolated signs and signs pre-segmented (based on manual annotations) from continuous signing, as such signs often differ in certain respects. We evaluate our method on the ASLLRP corpus and demonstrate significant improvements over previous work.

</details>


### [61] [Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance](https://arxiv.org/abs/2511.19909)
*Haoxuan Wang,Jiachen Tao,Junyi Wu,Gaowen Liu,Ramana Rao Kompella,Yan Yan*

Main category: cs.CV

TL;DR: Motion Marionette 是一种零样本的刚体运动迁移框架，将单目源视频的运动转移到单视角目标图像上。它不依赖外部几何/生成/仿真先验，而是从源视频中提取仅描述时空变换的内部先验（SpaT），再与目标对象融合，生成可控速度场并用基于位置的动力学（PBD）细化，最终实现高泛化、时序一致且可控的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部先验（几何、生成模型或物理仿真）来约束运动迁移，带来泛化性与时序一致性的权衡与额外限制。作者希望去除这些外部依赖，用一种能同时泛化到多对象且保持时间一致性的内部时空先验来驱动迁移。

Method: 1) 将源视频与目标单张图像统一提升到同一3D表示空间；2) 从源视频提取运动轨迹，构造与几何/语义无关的空间-时间（SpaT）先验，编码相对空间随时间的变化；3) 将该先验与目标对象融合，合成可控的速度场；4) 采用 Position-Based Dynamics 对速度场进行细化，减少伪影并提升视觉一致性；5) 使用得到的速度场进行高效视频生成与控制。

Result: 在多种对象与场景上实现了良好的零样本泛化，生成的视频在时间上一致、与源运动高度对齐，并且支持可控的运动生成；实验证明其相较依赖外部先验的方法有更好的稳定性与通用性。

Conclusion: 内部的SpaT时空先验可有效驱动刚体运动迁移，避免外部先验的局限；结合PBD细化的速度场能提升视觉质量与一致性，使得从单目视频到单视角图像的零样本运动迁移变得通用、稳定且可控。

Abstract: We present Motion Marionette, a zero-shot framework for rigid motion transfer from monocular source videos to single-view target images. Previous works typically employ geometric, generative, or simulation priors to guide the transfer process, but these external priors introduce auxiliary constraints that lead to trade-offs between generalizability and temporal consistency. To address these limitations, we propose guiding the motion transfer process through an internal prior that exclusively captures the spatial-temporal transformations and is shared between the source video and any transferred target video. Specifically, we first lift both the source video and the target image into a unified 3D representation space. Motion trajectories are then extracted from the source video to construct a spatial-temporal (SpaT) prior that is independent of object geometry and semantics, encoding relative spatial variations over time. This prior is further integrated with the target object to synthesize a controllable velocity field, which is subsequently refined using Position-Based Dynamics to mitigate artifacts and enhance visual coherence. The resulting velocity field can be flexibly employed for efficient video production. Empirical results demonstrate that Motion Marionette generalizes across diverse objects, produces temporally consistent videos that align well with the source motion, and supports controllable video generation.

</details>


### [62] [Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving](https://arxiv.org/abs/2511.19912)
*Dapeng Zhang,Zhenlong Yuan,Zhangquan Chen,Chih-Ting Liao,Yinda Chen,Fei Shen,Qingguo Zhou,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出Reasoning-VLA，一种以可学习动作查询并行生成连续轨迹、结合CoT推理数据与SFT+RL微调的VLA框架，在多基准上达SOTA且推理更快、泛化更强。


<details>
  <summary>Details</summary>
Motivation: 现有VLA在自动驾驶上存在两大痛点：推理效率低、对新车辆配置与新场景的泛化差。需要一种既快又能泛化、可进行有效推理与动作生成的统一框架。

Method: 1) 设计一组可学习的action queries，从训练语料中的真实轨迹高斯采样初始化；2) 让这些查询与“推理增强”的视觉-语言特征交互，并行生成连续动作轨迹；3) 将8个公开自动驾驶数据集标准化为带Chain-of-Thought（CoT）推理的统一数据格式；4) 采用监督学习训练并辅以强化学习微调，以提升策略质量与泛化性。

Result: 在多个自动驾驶基准上取得SOTA表现；相较现有VLA展现更强的跨场景与车辆配置的泛化能力；推理速度达当前报告中最佳水平。

Conclusion: Reasoning-VLA通过可学习动作查询+推理增强特征+统一CoT数据与SFT+RL的训练范式，实现高效并行轨迹生成，兼顾精度、速度与泛化能力，适合广泛的自动驾驶决策任务。

Abstract: Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.

</details>


### [63] [Coupled Physics-Gated Adaptation: Spatially Decoding Volumetric Photochemical Conversion in Complex 3D-Printed Objects](https://arxiv.org/abs/2511.19913)
*Maryam Eftekharifar,Churun Zhang,Jialiang Wei,Xudong Cao,Hossein Heidari*

Main category: cs.CV

TL;DR: 提出一种从3D视觉数据预测复杂3D打印物体体素级光化学转化的新框架，并以物理耦合引导的多模态融合模型C-PGA显著优于传统视觉模型，能在无需后测的情况下实现虚拟化学表征与精确控制。


<details>
  <summary>Details</summary>
Motivation: 传统计算机视觉模型难以预测由光学(衍射、吸收)与材料(扩散、对流)耦合决定的体内化学转化状态；现有流程依赖耗时的打印后化学表征。作者希望直接依据打印过程与3D视觉数据，预测终端化学转化，从而实现快速、无破坏的质量控制与设计优化。

Method: 构建有史以来最大的光学打印3D样本数据集（参数化最小曲面结构，含终端化学表征）。提出C-PGA：以稀疏几何与工艺参数（如表面传输、层高）作为Query，通过FiLM对密集视觉特征进行动态门控与自适应，显式建模物理耦合。视觉分支为双路3D-CNN：分别处理原始投影堆栈与经扩散-衍射校正的版本，物理上下文对两路特征进行空间调制与融合。

Result: 在该新任务上，C-PGA相较于标准特征拼接等常规融合方法表现更优，能准确预测体素级化学转化分布，实现对最终化学状态的高保真重建与控制（文中暗示消除或显著减少后测需求）。

Conclusion: 通过物理耦合门控的多模态融合，模型可从3D视觉数据直接推断非可见的体内化学性质，开创“虚拟化学表征”路线，为复杂3D打印的在线质量控制与工艺优化提供新范式。

Abstract: We present a framework that pioneers the prediction of photochemical conversion in complex three-dimensionally printed objects, introducing a challenging new computer vision task: predicting dense, non-visual volumetric physical properties from 3D visual data. This approach leverages the largest-ever optically printed 3D specimen dataset, comprising a large family of parametrically designed complex minimal surface structures that have undergone terminal chemical characterisation. Conventional vision models are ill-equipped for this task, as they lack an inductive bias for the coupled, non-linear interactions of optical physics (diffraction, absorption) and material physics (diffusion, convection) that govern the final chemical state. To address this, we propose Coupled Physics-Gated Adaptation (C-PGA), a novel multimodal fusion architecture. Unlike standard concatenation, C-PGA explicitly models physical coupling by using sparse geometrical and process parameters (e.g., surface transport, print layer height) as a Query to dynamically gate and adapt the dense visual features via feature-wise linear modulation (FiLM). This mechanism spatially modulates dual 3D visual streams-extracted by parallel 3D-CNNs processing raw projection stacks and their diffusion-diffraction corrected counterparts allowing the model to recalibrate its visual perception based on the physical context. This approach offers a breakthrough in virtual chemical characterisation, eliminating the need for traditional post-print measurements and enabling precise control over the chemical conversion state.

</details>


### [64] [Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models](https://arxiv.org/abs/2511.19917)
*Qin Ren,Yufei Wang,Lanqing Guo,Wen Zhang,Zhiwen Fan,Chenyu You*

Main category: cs.CV

TL;DR: 提出LoTTS：一种“局部化测试时扩展”(Localized TTS)方法，在推理阶段只对缺陷区域进行再采样，提升质量并节省计算。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展(TTS)在整幅图上增加计算，但图像质量空间上异质：好区域被重复计算、坏区域修复不足。需要一种能定位缺陷并在不破坏全局一致性的前提下仅修复局部的方法。

Method: 完全免训练框架LoTTS：1) 缺陷定位：在“高质量/低质量”等质量感知提示下，对比交叉注意力与自注意力信号，检测缺陷区域并生成连贯的掩膜；2) 一致性维护：仅扰动掩膜内噪声并进行局部去噪，限制修正影响范围，保持未掩膜区域不变。

Result: 在SD2.1、SDXL、FLUX上，LoTTS相较Best-of-N采样以2-4倍更低GPU成本，显著提升局部质量与全局保真，达到SOTA。

Conclusion: Localized TTS是可行且高效的新范式；LoTTS无需训练即可在推理时自适应修复局部缺陷，兼顾质量与计算开销，值得作为扩散模型推理扩展的主流方向。

Abstract: Diffusion models have become the dominant paradigm in text-to-image generation, and test-time scaling (TTS) further improves quality by allocating more computation during inference. However, existing TTS methods operate at the full-image level, overlooking the fact that image quality is often spatially heterogeneous. This leads to unnecessary computation on already satisfactory regions and insufficient correction of localized defects. In this paper, we explore a new direction - Localized TTS - that adaptively resamples defective regions while preserving high-quality regions, thereby substantially reducing the search space. This paradigm poses two central challenges: accurately localizing defects and maintaining global consistency. We propose LoTTS, the first fully training-free framework for localized TTS. For defect localization, LoTTS contrasts cross- and self-attention signals under quality-aware prompts (e.g., high-quality vs. low-quality) to identify defective regions, and then refines them into coherent masks. For consistency, LoTTS perturbs only defective regions and denoises them locally, ensuring that corrections remain confined while the rest of the image remains undisturbed. Extensive experiments on SD2.1, SDXL, and FLUX demonstrate that LoTTS achieves state-of-the-art performance: it consistently improves both local quality and global fidelity, while reducing GPU cost by 2-4x compared to Best-of-N sampling. These findings establish localized TTS as a promising new direction for scaling diffusion models at inference time.

</details>


### [65] [HybriDLA: Hybrid Generation for Document Layout Analysis](https://arxiv.org/abs/2511.19919)
*Yufan Chen,Omar Moured,Ruiping Liu,Junwei Zheng,Kunyu Peng,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 提出HybriDLA：在同一层内融合扩散与自回归的生成式框架，用于复杂文档版面分析；在DocLayNet与M6Doc上达SOTA，mAP达83.5%。


<details>
  <summary>Details</summary>
Motivation: 传统DLA依赖经验先验或固定数量的可学习查询，适用于区域数目小且结构固定的早期文档，但在当代文档中，元素数量多样、布局复杂，导致泛化与精度受限。需要能处理可变数量区域并结合语义上下文的更强生成式方法。

Method: 提出HybriDLA：在单层内联合两种生成机制——(1) 扩散模块用于迭代细化候选框（bounding boxes），逐步逼近真实区域；(2) 自回归模块注入语义与上下文信息，引导框生成与分类，提升在高度多变布局下的精确度。辅以多尺度特征融合编码器，整合细粒度与高层视觉线索。

Result: 在DocLayNet与M^6Doc基准上取得SOTA，报告mAP为83.5%，全面优于既有方法。

Conclusion: 将扩散的几何细化与自回归的语义建模在同一层融合，能更好地处理现代文档中可变数量与复杂布局的区域预测；多尺度特征进一步提升检测质量。数据与模型将公开。

Abstract: Conventional document layout analysis (DLA) traditionally depends on empirical priors or a fixed set of learnable queries executed in a single forward pass. While sufficient for early-generation documents with a small, predetermined number of regions, this paradigm struggles with contemporary documents, which exhibit diverse element counts and increasingly complex layouts. To address challenges posed by modern documents, we present HybriDLA, a novel generative framework that unifies diffusion and autoregressive decoding within a single layer. The diffusion component iteratively refines bounding-box hypotheses, whereas the autoregressive component injects semantic and contextual awareness, enabling precise region prediction even in highly varied layouts. To further enhance detection quality, we design a multi-scale feature-fusion encoder that captures both fine-grained and high-level visual cues. This architecture elevates performance to 83.5% mean Average Precision (mAP). Extensive experiments on the DocLayNet and M$^6$Doc benchmarks demonstrate that HybriDLA sets a state-of-the-art performance, outperforming previous approaches. All data and models will be made publicly available at https://yufanchen96.github.io/projects/HybriDLA.

</details>


### [66] [Intelligent Image Search Algorithms Fusing Visual Large Models](https://arxiv.org/abs/2511.19920)
*Kehan Wang,Tingqiong Cui,Yang Zhang,Yu Chen,Shifeng Wu,Zhenzhang Li*

Main category: cs.CV

TL;DR: DetVLM提出将YOLO检测与VLM结合的两阶段检索框架，先高召回筛组件，再用VLM做语义与状态校验与补召，支持细粒度“状态检索”和零样本检索，在车辆部件数据集上取得94.82%总体准确率并显著优于只用检测的基线。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征鲁棒性差；纯检测器虽能判定是否存在部件，却难以做细粒度状态判别与零样本检索；VLM具备语义与零样本能力但空间定位弱、计算开销大，不适合直接检索。需一种既高效又具语义与状态理解的检索方案。

Method: 两阶段融合：1) 用YOLO进行组件级高召回筛查，快速判定部件存在与候选区域；2) 用VLM作为召回增强与校验单元，对漏检进行二次验证，并在提示词引导下进行组件存在与状态判定，实现状态与零样本检索。

Result: 在车辆部件数据集上，整体检索准确率94.82%，明显优于仅检测基线；零样本任务（如“驾驶员戴口罩”）准确率94.95%；多种状态检索的平均准确率超过90%。

Conclusion: DetVLM通过检测器与VLM的互补融合，实现高效且具语义理解与状态判别的细粒度图像检索，兼顾效率、召回与零样本能力，达到SOTA表现。

Abstract: Fine-grained image retrieval, which aims to find images containing specific object components and assess their detailed states, is critical in fields like security and industrial inspection. However, conventional methods face significant limitations: manual features (e.g., SIFT) lack robustness; deep learning-based detectors (e.g., YOLO) can identify component presence but cannot perform state-specific retrieval or zero-shot search; Visual Large Models (VLMs) offer semantic and zero-shot capabilities but suffer from poor spatial grounding and high computational cost, making them inefficient for direct retrieval. To bridge these gaps, this paper proposes DetVLM, a novel intelligent image search framework that synergistically fuses object detection with VLMs. The framework pioneers a search-enhancement paradigm via a two-stage pipeline: a YOLO detector first conducts efficient, high-recall component-level screening to determine component presence; then, a VLM acts as a recall-enhancement unit, performing secondary verification for components missed by the detector. This architecture directly enables two advanced capabilities: 1) State Search: Guided by task-specific prompts, the VLM refines results by verifying component existence and executing sophisticated state judgments (e.g., "sun visor lowered"), allowing retrieval based on component state. 2) Zero-shot Search: The framework leverages the VLM's inherent zero-shot capability to recognize and retrieve images containing unseen components or attributes (e.g., "driver wearing a mask") without any task-specific training. Experiments on a vehicle component dataset show DetVLM achieves a state-of-the-art overall retrieval accuracy of 94.82\%, significantly outperforming detection-only baselines. It also attains 94.95\% accuracy in zero-shot search for driver mask-wearing and over 90\% average accuracy in state search tasks.

</details>


### [67] [CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding](https://arxiv.org/abs/2511.19923)
*Yuefei Chen,Jiang Liu,Xiaodong Lin,Ruixiang Tang*

Main category: cs.CV

TL;DR: 提出CounterVQA基准评估视频VLM的反事实推理，并用CFGPT后训练从语言模态蒸馏提升视觉反事实能力，显著改善在多难度设置上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在视频理解上进步显著，但主要停留在模式识别与指令跟随，对“若…会怎样”的反事实推理缺乏系统评测与能力。反事实对稳健视频理解关键，需挖掘因果结构与未观测可能性。

Method: 1) 构建CounterVQA视频问答基准，含三层递进难度，分别考察不同维度的反事实推理（从简单改变量到多跳因果链）。2) 系统评测开源与闭源SOTA VLM，量化能力差距。3) 提出后训练方法CFGPT：从语言模态蒸馏反事实推理能力，迁移至视觉-语言模型以增强视觉端反事实推理。

Result: 发现现有模型在简单反事实问题上尚可，但在复杂多跳因果链上显著退化。引入CFGPT后，在CounterVQA的各难度层级均获得一致提升。

Conclusion: 反事实推理是当前视频VLM的短板。CounterVQA提供系统评测基准，CFGPT作为有效后训练手段可提升模型在视觉反事实上的能力；数据与代码将发布以促进后续研究。

Abstract: Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.

</details>


### [68] [Context-Aware Token Pruning and Discriminative Selective Attention for Transformer Tracking](https://arxiv.org/abs/2511.19928)
*Janani Kugarajeevan,Thanikasalam Kokul,Amirthalingam Ramanan,Subha Fernando*

Main category: cs.CV

TL;DR: 提出CPDATrack：在一体式Transformer跟踪中，通过估计搜索token与目标相关性进行裁剪，并引入选择性注意策略，抑制背景/干扰并提升效率，SOTA，GOT-10k AO 75.1%。


<details>
  <summary>Details</summary>
Motivation: 一体式Transformer将模板与搜索token拼接做联合注意力，但大量背景token会关注模板导致判别性下降；现有裁剪易误删目标邻域上下文且难以应对干扰物，性能受损。

Method: 1) 在两层编码器之间加入可学习相关性估计模块，预测每个搜索token与目标关联概率，据此裁剪低信息背景token，同时保留目标周围上下文；2) 分辨选择性注意：早期层完全阻断搜索→模板注意力以抑制背景干扰；后续层仅从局部区域选择高概率目标token与模板交互，降低背景与干扰影响；整体提升效率与判别性。

Result: 在多基准上达SOTA，尤其GOT-10k取得AO 75.1%。同时计算更高效（文中隐含减少token数与注意开销）。

Conclusion: 针对一体式Transformer跟踪的背景与干扰问题，CPDATrack通过相关性驱动的token裁剪与阶段性选择性注意，有效保留关键上下文、抑制无关与干扰token并提升效率与精度，在多数据集上验证有效。

Abstract: One-stream Transformer-based trackers have demonstrated remarkable performance by concatenating template and search region tokens, thereby enabling joint attention across all tokens. However, enabling an excessive proportion of background search tokens to attend to the target template tokens weakens the tracker's discriminative capability. Several token pruning methods have been proposed to mitigate background interference; however, they often remove tokens near the target, leading to the loss of essential contextual information and degraded tracking performance. Moreover, the presence of distractors within the search tokens further reduces the tracker's ability to accurately identify the target. To address these limitations, we propose CPDATrack, a novel tracking framework designed to suppress interference from background and distractor tokens while enhancing computational efficiency. First, a learnable module is integrated between two designated encoder layers to estimate the probability of each search token being associated with the target. Based on these estimates, less-informative background tokens are pruned from the search region while preserving the contextual cues surrounding the target. To further suppress background interference, a discriminative selective attention mechanism is employed that fully blocks search-to-template attention in the early layers. In the subsequent encoder layers, high-probability target tokens are selectively extracted from a localized region to attend to the template tokens, thereby reducing the influence of background and distractor tokens. The proposed CPDATrack achieves state-of-the-art performance across multiple benchmarks, particularly on GOT-10k, where it attains an average overlap of 75.1 percent.

</details>


### [69] [Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos](https://arxiv.org/abs/2511.19936)
*Youngseo Kim,Dohyun Kim,Geonhee Han,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 利用图像扩散模型的自注意力作为语义标签传播核，在视频中进行零样本目标跟踪与分割；结合测试时优化与SAM细化，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽为生成而生，但其内部表征蕴含强语义与对应关系，若能转为像素级传播机制，可在无需监督的情况下完成视频目标跟踪与分割，填补生成模型在下游识别/定位任务的应用空白。

Method: 将扩散模型的自注意力重解释为语义标签传播核，提供像素级对应；跨帧扩展形成时间传播核以进行零样本跟踪分割。引入三种测试时优化：DDIM inversion（将观测视频帧嵌回扩散潜空间）、textual inversion（自适应文本嵌入以稳固语义）、adaptive head weighting（对多头注意力自适应加权）。提出DRIFT框架：用预训练图像扩散模型提特征与传播，结合SAM进行掩码细化。

Result: 在标准视频目标分割（VOS）基准上实现零样本设定下的SOTA表现，显示出在复杂场景中稳健的一致性与鲁棒传播能力。

Conclusion: 扩散模型的自注意力可作为强大的语义标签传播工具；配合测试时优化与SAM细化，DRIFT实现无需额外训练的强零样本视频目标跟踪/分割，证明扩散生成模型在时空理解任务中的广泛潜力。

Abstract: Image diffusion models, though originally developed for image generation, implicitly capture rich semantic structures that enable various recognition and localization tasks beyond synthesis. In this work, we investigate their self-attention maps can be reinterpreted as semantic label propagation kernels, providing robust pixel-level correspondences between relevant image regions. Extending this mechanism across frames yields a temporal propagation kernel that enables zero-shot object tracking via segmentation in videos. We further demonstrate the effectiveness of test-time optimization strategies-DDIM inversion, textual inversion, and adaptive head weighting-in adapting diffusion features for robust and consistent label propagation. Building on these findings, we introduce DRIFT, a framework for object tracking in videos leveraging a pretrained image diffusion model with SAM-guided mask refinement, achieving state-of-the-art zero-shot performance on standard video object segmentation benchmarks.

</details>


### [70] [Low-Resolution Editing is All You Need for High-Resolution Editing](https://arxiv.org/abs/2511.19945)
*Junsung Lee,Hyunsoo Lee,Yong Jae Lee,Bohyung Han*

Main category: cs.CV

TL;DR: 提出高分辨率图像编辑的新任务与测试时优化框架；通过补丁级优化、细粒度细节迁移与跨补丁同步策略，实现超过1K的高质量一致性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有生成/编辑方法多局限低分辨率（≤1K），难以满足与用户意图一致的高分辨率内容创作需求；需要一种可控、有效且可保持一致性的高分辨率图像编辑机制。

Method: 在推理阶段进行测试时优化：对高分辨率源图像进行分块（patch-wise）优化；随后用细粒度的细节迁移模块恢复与增强高频细节；并提出新颖的同步策略在块间共享/约束特征与边界以维持全局一致性。

Result: 在大量实验中，方法能在高分辨率设置下生成高质量编辑结果，超越以往仅支持到1K的方案，保持跨补丁的一致性与细节丰富度。

Conclusion: 测试时优化结合补丁级处理、细节迁移与同步机制，可有效实现可控的高分辨率图像编辑，为高分辨率内容生成铺路。

Abstract: High-resolution content creation is rapidly emerging as a central challenge in both the vision and graphics communities. While images serve as the most fundamental modality for visual expression, content generation that aligns with the user intent requires effective, controllable high-resolution image manipulation mechanisms. However, existing approaches remain limited to low-resolution settings, typically supporting only up to 1K resolution. In this work, we introduce the task of high-resolution image editing and propose a test-time optimization framework to address it. Our method performs patch-wise optimization on high-resolution source images, followed by a fine-grained detail transfer module and a novel synchronization strategy to maintain consistency across patches. Extensive experiments show that our method produces high-quality edits, facilitating the way toward high-resolution content creation.

</details>


### [71] [Supervise Less, See More: Training-free Nuclear Instance Segmentation with Prototype-Guided Prompting](https://arxiv.org/abs/2511.19953)
*Wen Zhang,Qin Ren,Wenjing Liu,Haibin Ling,Chenyu You*

Main category: cs.CV

TL;DR: 提出SPROUT：一种完全免训练与免标注、基于提示的核实例分割框架，利用组织学先验构建切片特定原型并通过部分最优传输对齐特征，将前景/背景特征转为SAM的点提示，在多基准上零监督达竞品表现。


<details>
  <summary>Details</summary>
Motivation: 现有核分割方法多依赖密集标注和昂贵微调；虽有大模型零样本分割的潜力，但训练/标注成本与域差异限制实际落地，迫切需要训练自由、可扩展且对域移不敏感的方法。

Method: 1) 基于组织学知识从目标切片自动提取前景/背景先验并构建切片特定参考原型；2) 采用部分最优传输逐步引导特征对齐，缩小域差；3) 将对齐后的前景/背景特征转化为SAM的正/负点提示；4) 在不更新任何参数的情况下调用SAM生成核实例分割。

Result: 在多个病理切片基准上，无需监督或再训练即可获得与现有有监督或微调方法相当的性能，核边界刻画精确、泛化强。

Conclusion: SPROUT确立了病理核实例分割的可扩展训练自由范式：结合切片特定原型与最优传输对齐，再以点提示驱动SAM，无需标注与训练即可实现高质量分割。

Abstract: Accurate nuclear instance segmentation is a pivotal task in computational pathology, supporting data-driven clinical insights and facilitating downstream translational applications. While large vision foundation models have shown promise for zero-shot biomedical segmentation, most existing approaches still depend on dense supervision and computationally expensive fine-tuning. Consequently, training-free methods present a compelling research direction, yet remain largely unexplored. In this work, we introduce SPROUT, a fully training- and annotation-free prompting framework for nuclear instance segmentation. SPROUT leverages histology-informed priors to construct slide-specific reference prototypes that mitigate domain gaps. These prototypes progressively guide feature alignment through a partial optimal transport scheme. The resulting foreground and background features are transformed into positive and negative point prompts, enabling the Segment Anything Model (SAM) to produce precise nuclear delineations without any parameter updates. Extensive experiments across multiple histopathology benchmarks demonstrate that SPROUT achieves competitive performance without supervision or retraining, establishing a novel paradigm for scalable, training-free nuclear instance segmentation in pathology.

</details>


### [72] [GFT-GCN: Privacy-Preserving 3D Face Mesh Recognition with Spectral Diffusion](https://arxiv.org/abs/2511.19958)
*Hichem Felouat,Hanrui Wang,Isao Echizen*

Main category: cs.CV

TL;DR: 提出GFT-GCN框架：用GFT+GCN提取3D人脸网格的谱特征，并以谱扩散生成不可逆、可更新、不可链接的模板；在BU-3DFE与FaceScape上兼顾高识别率与强隐私。


<details>
  <summary>Details</summary>
Motivation: 3D人脸识别对光照、姿态与攻击更稳健，但模板一旦泄露难以更换，需兼顾高安全与隐私保护。

Method: 将图傅里叶变换与图卷积结合，学习紧凑判别的谱域特征；再通过谱扩散机制对特征进行不可逆扰动，实现可更新与不可链接；采用轻量级端-云分离架构，原始生物特征不出端侧。

Result: 在BU-3DFE和FaceScape数据集上获得高识别准确率；对重建攻击展现强抵御能力，隐私-性能取得良好平衡。

Conclusion: GFT-GCN为3D人脸认证提供实用的隐私保护方案，在保持性能的同时实现模板的不可逆、可更新、不可链接，并具备部署友好的架构。

Abstract: 3D face recognition offers a robust biometric solution by capturing facial geometry, providing resilience to variations in illumination, pose changes, and presentation attacks. Its strong spoof resistance makes it suitable for high-security applications, but protecting stored biometric templates remains critical. We present GFT-GCN, a privacy-preserving 3D face recognition framework that combines spectral graph learning with diffusion-based template protection. Our approach integrates the Graph Fourier Transform (GFT) and Graph Convolutional Networks (GCN) to extract compact, discriminative spectral features from 3D face meshes. To secure these features, we introduce a spectral diffusion mechanism that produces irreversible, renewable, and unlinkable templates. A lightweight client-server architecture ensures that raw biometric data never leaves the client device. Experiments on the BU-3DFE and FaceScape datasets demonstrate high recognition accuracy and strong resistance to reconstruction attacks. Results show that GFT-GCN effectively balances privacy and performance, offering a practical solution for secure 3D face authentication.

</details>


### [73] [MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing](https://arxiv.org/abs/2511.19963)
*Changho Choi,Minho Kim,Jinkyu Kim*

Main category: cs.CV

TL;DR: MambaEye 提出一种因果、输入尺寸无关的视觉编码器，用纯 Mamba2 顺序骨干与相对位移嵌入，实现线性复杂度、任意分辨率与扫描顺序下逐步预测，并在高分辨率 ImageNet-1K 上表现强劲。


<details>
  <summary>Details</summary>
Motivation: 现有视觉编码器通常依赖固定输入尺寸和双向处理，难以像人类视觉那样随时做出预测并适配任意分辨率/扫描方式；同时在高分辨率下成本高。作者希望构建一个真正输入尺寸无关、因果可随时输出、且计算/显存线性可扩展的编码器。

Method: 以纯 Mamba2 状态空间模型为骨干，严格单向（因果）顺序编码；提出相对移动嵌入（编码相邻patch的空间位移）以注入平移不变先验并适配任意网格/扫描；设计扩散启发的逐步监督损失，让模型随步长累积证据、逐步提升置信度；由此实现任意点可输出预测。

Result: 在 ImageNet-1K 上，跨广泛分辨率均表现稳健，尤其在 1536×1536 高分辨率设置下表现强；同时时间和显存复杂度对patch数线性增长。

Conclusion: MambaEye 通过因果单向 SSM + 相对位移嵌入 + 扩散式逐步监督，实现真正输入尺寸无关、可任意时刻预测的视觉编码器，在高分辨率场景具有效率与性能优势。

Abstract: Despite decades of progress, a truly input-size agnostic visual encoder-a fundamental characteristic of human vision-has remained elusive. We address this limitation by proposing \textbf{MambaEye}, a novel, causal sequential encoder that leverages the low complexity and causal-process based pure Mamba2 backbone. Unlike previous Mamba-based vision encoders that often employ bidirectional processing, our strictly unidirectional approach preserves the inherent causality of State Space Models, enabling the model to generate a prediction at any point in its input sequence. A core innovation is our use of relative move embedding, which encodes the spatial shift between consecutive patches, providing a strong inductive bias for translation invariance and making the model inherently adaptable to arbitrary image resolutions and scanning patterns. To achieve this, we introduce a novel diffusion-inspired loss function that provides dense, step-wise supervision, training the model to build confidence as it gathers more visual evidence. We demonstrate that MambaEye exhibits robust performance across a wide range of image resolutions, especially at higher resolutions such as $1536^2$ on the ImageNet-1K classification task. This feat is achieved while maintaining linear time and memory complexity relative to the number of patches.

</details>


### [74] [HiCoGen: Hierarchical Compositional Text-to-Image Generation in Diffusion Models via Reinforcement Learning](https://arxiv.org/abs/2511.19965)
*Hongji Yang,Yucheng Zhou,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出HiCoGen：以“合成链”（CoS）逐步构建图像，并结合层级奖励与衰减随机性调度的强化学习，显著提升复杂提示下的概念覆盖与组合准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在简单提示下效果好，但复杂提示（多物体、层级关系）时易出现遗漏、混淆与差组合性，缺乏可靠的指令遵循与探索能力。需要一种能按层级分解并逐步合成、且训练阶段能充分探索的生成框架。

Method: 1) HiCoGen框架：用LLM将复杂提示分解为最小语义单元；采用CoS范式逐步合成，每步都以当前生成图像作为下一步视觉上下文，确保所有概念被纳入。2) 强化学习：提出“衰减随机性调度”，理论上证明早期集中随机性可最大化样本多样性；在此基础上进行RL探索。3) 层级奖励：从全局场景、主体级与关系级联合评估生成质量。4) 构建HiCoPrompt基准用于层级化提示评测。

Result: 在HiCoPrompt及现有基准上，HiCoGen在概念覆盖率与组合准确性上显著优于现有方法，复杂提示的指令遵循更稳健。

Conclusion: 通过分解-递进式合成与理论支撑的探索增强策略，HiCoGen有效克服扩散模型在复杂提示下的组合与遗漏问题，为层级化文本到图像生成提供了强有力的解决方案与评测基准。

Abstract: Recent advances in diffusion models have demonstrated impressive capability in generating high-quality images for simple prompts. However, when confronted with complex prompts involving multiple objects and hierarchical structures, existing models struggle to accurately follow instructions, leading to issues such as concept omission, confusion, and poor compositionality. To address these limitations, we propose a Hierarchical Compositional Generative framework (HiCoGen) built upon a novel Chain of Synthesis (CoS) paradigm. Instead of monolithic generation, HiCoGen first leverages a Large Language Model (LLM) to decompose complex prompts into minimal semantic units. It then synthesizes these units iteratively, where the image generated in each step provides crucial visual context for the next, ensuring all textual concepts are faithfully constructed into the final scene. To further optimize this process, we introduce a reinforcement learning (RL) framework. Crucially, we identify that the limited exploration of standard diffusion samplers hinders effective RL. We theoretically prove that sample diversity is maximized by concentrating stochasticity in the early generation stages and, based on this insight, propose a novel Decaying Stochasticity Schedule to enhance exploration. Our RL algorithm is then guided by a hierarchical reward mechanism that jointly evaluates the image at the global, subject, and relationship levels. We also construct HiCoPrompt, a new text-to-image benchmark with hierarchical prompts for rigorous evaluation. Experiments show our approach significantly outperforms existing methods in both concept coverage and compositional accuracy.

</details>


### [75] [VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction](https://arxiv.org/abs/2511.19971)
*Yu Hu,Chong Cheng,Sicheng Yu,Xiaoyang Guo,Hao Wang*

Main category: cs.CV

TL;DR: VGGT4D在不训练的前提下，将3D基础模型VGGT扩展到4D重建，通过挖掘并放大模型内部全局注意力中的动态线索，生成高质量动态/静态分离掩膜，结合投影梯度细化并在早期推理阶段使用，从而提升动态分割、位姿估计与稠密重建，支持>500帧单次推理。


<details>
  <summary>Details</summary>
Motivation: 现有4D重建在大量运动目标场景下效果差，常依赖外部先验、重后处理或需在4D数据上微调；而VGGT等3D基础模型虽然几何准确，但在动态场景中性能明显下降。作者观察到VGGT全局注意力层已隐式编码丰富的逐层动态线索，若能无训练地挖掘与利用，可稳健解耦动态与静态以提升4D重建。

Method: 提出无训练的VGGT4D：1) 在VGGT的全局注意力中计算层级Gram相似以挖掘全局动态线索，并在时间窗口内聚合，得到初始静/动掩膜；2) 引入基于投影梯度的掩膜边界细化策略以获得更锐利的边界；3) 将精确掩膜注入VGGT早期推理阶段，抑制运动干扰，联合优化相机位姿与几何重建；整体可单次前向、长序列运行。

Result: 在6个数据集上，VGGT4D在动态目标分割、相机位姿估计和稠密重建方面均优于现有方法，且无需外部先验、后期优化或参数微调；支持>500帧的单次推理。

Conclusion: 通过挖掘VGGT内部注意力的动态线索并结合投影梯度细化，VGGT4D以零训练代价稳健分离动态与静态并改进4D重建，在多数据集上取得领先表现与长序列推理能力。

Abstract: Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.

</details>


### [76] [Boosting Reasoning in Large Multimodal Models via Activation Replay](https://arxiv.org/abs/2511.19972)
*Yun Xing,Xiaobin Hu,Qingdong He,Jiangning Zhang,Shuicheng Yan,Shijian Lu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文研究RLVR对LMM推理的机制影响，并提出无需训练的Activation Replay，在测试时复用低熵激活以提升多模态推理与覆盖度。


<details>
  <summary>Details</summary>
Motivation: RLVR能显著提升LMM推理，但其为何有效、作用于哪些内部表征尚不清楚；同时RLVR训练昂贵且可能收窄推理覆盖，需要更便宜、可解释、可泛化的改进手段。

Method: 1) 用logit lens分析多个RLVR后训练模型，比较输入阶段的激活分布与熵特性；2) 控制实验将激活熵变化与推理能力关联；3) 提出Activation Replay：在测试时对视觉token进行干预，从基座LMM的输入上下文“回放”低熵激活到RLVR模型，以调节其激活态，无需再训练或策略优化；4) 与多种替代方案对比：回放高熵激活、跨模型直接干预、不改输入token等。

Result: 发现RLVR主要改变低熵激活而对高熵激活影响较小；操纵低熵激活与推理增强相关。Activation Replay在数学、o3式视觉代理、视频推理等任务上提升推理质量与Pass@K，并缓解RLVR导致的推理覆盖变窄。其实现优于替代方案。

Conclusion: RLVR的有效性与低熵激活的调制相关；通过在测试时回放低熵激活可在无需昂贵训练的前提下提升多模态推理与覆盖，提供了可解释、简单有效的推理增强路径。

Abstract: Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach to incentivizing reasoning capability in Large Multimodal Models (LMMs), while the underlying mechanisms behind this post-training paradigm are poorly understood. We begin by exploring how input activations are affected by RLVR through the perspective of logit lens. Our systematic investigations across multiple post-trained LMMs suggest that RLVR shifts low-entropy activations unexpectedly, while high-entropy ones are less affected. We further demonstrate that such phenomena are associated with LMM reasoning by controlled experiments, suggesting a potentially beneficial role of modulating low-entropy activations. To this end, we propose Activation Replay, a novel simple yet effective training-free approach that boosts multimodal reasoning of post-trained LMMs without requiring expensive policy optimization. Our design involves manipulation of visual tokens at test time, replaying low-entropy activations from the input context of base LMMs to regulating the RLVR counterparts. Activation Replay triggers better reasoning across diverse scenarios, including mathematics, o3-like visual agents, and video reasoning. We further show that Activation Replay boosts Pass@K and mitigates narrower reasoning coverage of RLVR. Our design is compared against alternative choices, such as replaying high-entropy activations instead of low-entropy ones, or direct cross-model intervention instead of manipulating input tokens, demonstrating the superiority of our implementation. Codes will be made publicly available.

</details>


### [77] [EmoFeedback2: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback](https://arxiv.org/abs/2511.19982)
*Jingyang Jia,Kai Shu,Gang Yang,Long Xing,Xun Chen,Aiping Liu*

Main category: cs.CV

TL;DR: 提出EmoFeedback2：用微调LVLM对生成图像进行情感评估与文本反馈，结合奖励与自我促动提示迭代，显著提升连续情感图像生成的连续性与情感忠实度，SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有连续情感图像生成缺少来自生成图像的情感反馈，导致情感连续性控制弱；同时仅将情感与简单文本对齐，不能根据图像内容自适应调整情感提示，情感忠实度不足。

Method: 提出“生成-理解-反馈”强化范式EmoFeedback2：使用微调的大型视觉-语言模型(LVLM)对生成图像进行(1)情感值评估并与目标情感计算奖励，驱动生成模型的强化式微调以增强情感连续性；(2)自促进文本反馈，LVLM迭代解析图像情感与细粒度内容，生成下一轮提示的改进建议，从而自适应调整情感提示、提升情感忠实度。

Result: 在自建数据集上，生成图片质量与目标情感一致性优于现有SOTA；能稳定产生符合连续情感目标的高质量图像。代码与数据集即将开源。

Conclusion: 通过引入LVLM的情感奖励与文本自反馈，EmoFeedback2实现了对连续情感的更可控生成与更高情感忠实度，优于现有方法，并具备可复现与扩展潜力。

Abstract: Continuous emotional image generation (C-EICG) is emerging rapidly due to its ability to produce images aligned with both user descriptions and continuous emotional values. However, existing approaches lack emotional feedback from generated images, limiting the control of emotional continuity. Additionally, their simple alignment between emotions and naively generated texts fails to adaptively adjust emotional prompts according to image content, leading to insufficient emotional fidelity. To address these concerns, we propose a novel generation-understanding-feedback reinforcement paradigm (EmoFeedback2) for C-EICG, which exploits the reasoning capability of the fine-tuned large vision-language model (LVLM) to provide reward and textual feedback for generating high-quality images with continuous emotions. Specifically, we introduce an emotion-aware reward feedback strategy, where the LVLM evaluates the emotional values of generated images and computes the reward against target emotions, guiding the reinforcement fine-tuning of the generative model and enhancing the emotional continuity of images. Furthermore, we design a self-promotion textual feedback framework, in which the LVLM iteratively analyzes the emotional content of generated images and adaptively produces refinement suggestions for the next-round prompt, improving the emotional fidelity with fine-grained content. Extensive experimental results demonstrate that our approach effectively generates high-quality images with the desired emotions, outperforming existing state-of-the-art methods in our custom dataset. The code and dataset will be released soon.

</details>


### [78] [SONIC: Spectral Optimization of Noise for Inpainting with Consistency](https://arxiv.org/abs/2511.19985)
*Seungyeon Baek,Erqun Dong,Shadan Namazifard,Mark J. Matthews,Kwang Moo Yi*

Main category: cs.CV

TL;DR: 提出一种无需再训练的文本到图像模型补全方法：通过优化初始噪声（频域中、线性近似传播）以匹配未遮挡区域，再用常规无训练补全，显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于引导的无训练逆问题（如补全）在实践中效果不佳，常需专门训练的补全模型。作者认为缺失的关键在于对扩散采样初始噪声的合理优化，使生成与已知可见区域一致。

Method: 在推理时对初始种子噪声进行小步数优化，使其生成结果与未遮挡区域近似一致；为避免对采样过程反向展开，采用线性近似将初始噪声对最终图像的影响线性化；为稳定与高效，改在频谱域上优化噪声。随后在优化后的噪声上运行常规的无训练补全流程。

Result: 在多种补全任务上优于现有最先进方法，且仅需几十步优化即可取得显著增益。提供项目页与演示。

Conclusion: 无需训练、仅通过优化初始噪声（线性近似+频域优化）即可显著提升通用文本到图像模型的补全能力，减少对专用补全模型的依赖。

Abstract: We propose a novel training-free method for inpainting with off-the-shelf text-to-image models. While guidance-based methods in theory allow generic models to be used for inverse problems such as inpainting, in practice, their effectiveness is limited, leading to the necessity of specialized inpainting-specific models. In this work, we argue that the missing ingredient for training-free inpainting is the optimization (guidance) of the initial seed noise. We propose to optimize the initial seed noise to approximately match the unmasked parts of the data - with as few as a few tens of optimization steps. We then apply conventional training-free inpainting methods on top of our optimized initial seed noise. Critically, we propose two core ideas to effectively implement this idea: (i) to avoid the costly unrolling required to relate the initial noise and the generated outcome, we perform linear approximation; and (ii) to stabilize the optimization, we optimize the initial seed noise in the spectral domain. We demonstrate the effectiveness of our method on various inpainting tasks, outperforming the state of the art. Project page: https://ubc-vision.github.io/sonic/

</details>


### [79] [GazeProphetV2: Head-Movement-Based Gaze Prediction Enabling Efficient Foveated Rendering on Mobile VR](https://arxiv.org/abs/2511.19988)
*Farhaan Ebadulla,Chiraag Mudlpaur,Shreya Chaurasia,Gaurav BV*

Main category: cs.CV

TL;DR: 提出一种多模态VR视线预测方法，融合历史注视、头部运动与场景视觉信息，通过门控融合与跨模态注意力自适应加权，较单一模态显著提升1-3帧未来视线预测精度，并具良好跨场景泛化（验证准确率93.1%），为渲染优化与交互设计提供支持，可在无昂贵眼动仪下预估注意。


<details>
  <summary>Details</summary>
Motivation: VR中准确预测用户注视可用于渲染资源分配（如焦点区域高质量渲染）、界面布局与交互设计，但单一信号（仅历史注视或仅头动或仅场景）难以稳定、泛化预测，需要一种能融合多源信息并根据上下文动态权重的模型。

Method: 构建多模态模型：输入包含时间序列的注视轨迹、头部姿态/运动，以及场景视觉特征；采用门控融合机制结合跨模态注意力，根据上下文相关性对各模态贡献进行自适应加权；预测短期未来（1-3帧）的注视位置。

Result: 在包含22个VR场景、约530万条注视样本的数据集上评测，融合多模态比单模态显著提高预测准确率；跨场景泛化良好，报告验证准确率93.1%，并在时间上保持预测轨迹的一致性与平滑性。

Conclusion: 多模态自适应融合能有效提升VR视线短期预测与跨场景泛化，阐释虚拟环境中的注意机制；方法可用于渲染优化、交互与体验评估，并为无需昂贵眼动追踪硬件的注意预估提供可行路径。

Abstract: Predicting gaze behavior in virtual reality environments remains a significant challenge with implications for rendering optimization and interface design. This paper introduces a multimodal approach to VR gaze prediction that combines temporal gaze patterns, head movement data, and visual scene information. By leveraging a gated fusion mechanism with cross-modal attention, the approach learns to adaptively weight gaze history, head movement, and scene content based on contextual relevance. Evaluations using a dataset spanning 22 VR scenes with 5.3M gaze samples demonstrate improvements in predictive accuracy when combining modalities compared to using individual data streams alone. The results indicate that integrating past gaze trajectories with head orientation and scene content enhances prediction accuracy across 1-3 future frames. Cross-scene generalization testing shows consistent performance with 93.1% validation accuracy and temporal consistency in predicted gaze trajectories. These findings contribute to understanding attention mechanisms in virtual environments while suggesting potential applications in rendering optimization, interaction design, and user experience evaluation. The approach represents a step toward more efficient virtual reality systems that can anticipate user attention patterns without requiring expensive eye tracking hardware.

</details>


### [80] [OmniRefiner: Reinforcement-Guided Local Diffusion Refinement](https://arxiv.org/abs/2511.19990)
*Yaoli Liu,Ziheng Ouyang,Shengtao Lou,Yiren Song*

Main category: cs.CV

TL;DR: 提出一种两阶段、参考驱动的细节感知图像细化框架，通过联合输入“草图图像+参考图像”的扩散编辑与后续强化学习优化，显著提升微观细节与语义一致性，优于开源与商用方法。


<details>
  <summary>Details</summary>
Motivation: 现有参考引导的扩散生成在细粒度细节上易丢失身份与属性线索：VAE潜空间压缩削弱纹理；基于后编辑的细节增强常造成光照、纹理或形状不一致。需要能在保持全局结构的同时，提升像素级一致性与局部细节的方案。

Method: 提出两阶段的\ourMthd：1) 微调单图扩散编辑器，使其联合接收草图图像与参考图像，实现全局一致、结构保真的初步细化；2) 引入强化学习，面向“细节准确性+语义一致性”目标对局部编辑能力进行优化，强化局部纹理与属性的精确纠正。

Result: 在多项参考引导修复基准上实现更好的参考对齐与细粒度细节保留，在视觉一致性与忠实度上超过开源与商业模型。

Conclusion: 两阶段参考驱动细化（联合输入微调+RL局部优化）有效弥补VAE压缩带来的细节损失，兼顾全局结构与局部纹理，生成更忠实、连贯的编辑结果。

Abstract: Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining a generated image using a reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identity- and attribute-specific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce \ourMthd{}, a detail-aware refinement framework that performs two consecutive stages of reference-driven correction to enhance pixel-level consistency. We first adapt a single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintaining structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that \ourMthd{} significantly improves reference alignment and fine-grained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks.

</details>


### [81] [CREward: A Type-Specific Creativity Reward Model](https://arxiv.org/abs/2511.19995)
*Jiyeon Han,Ali Mahdavi-Amiri,Hao Zhang,Haedong Jeong*

Main category: cs.CV

TL;DR: 提出CREward：首个按类型（几何、材质、纹理）分解的创意度奖励模型，用人类标注与LVLM对齐训练，可用于评估、解释与引导创意图像生成。


<details>
  <summary>Details</summary>
Motivation: 创意往往被粗糙地视为单一维度，忽略了图像生成流程中不同要素（几何、材质、纹理）的差异性。需要一种能分类型刻画与评估创意、同时与人类感知一致并可用于生成的模型。

Method: 1) 进行人类基准测试，分别收集几何、材质、纹理三轴上的创意感知标注；2) 分析LVLM对创意判断与人类的一致性并验证其较强对齐；3) 利用LVLM生成的标签（在与人类一致性基础上）训练类型化创意奖励模型CREward；4) 将CREward用于评估与生成环节。

Result: 表明LVLM在三类创意判断上与人类感知高度相关；训练得到的CREward能在多任务中稳定工作：量化评估创意、输出可解释的类型分解分数、并作为奖励或偏置信号改进创意样本获取与生成。

Conclusion: 将创意分解为几何、材质、纹理三轴并以CREward建模是可行且有效；该模型既能贴近人类感知，又能在评估、解释与引导生成中发挥作用，推动更细粒度、可操作的创意研究与应用。

Abstract: Creativity is a complex phenomenon. When it comes to representing and assessing creativity, treating it as a single undifferentiated quantity would appear naive and underwhelming. In this work, we learn the \emph{first type-specific creativity reward model}, coined CREward, which spans three creativity ``axes," geometry, material, and texture, to allow us to view creativity through the lens of the image formation pipeline. To build our reward model, we first conduct a human benchmark evaluation to capture human perception of creativity for each type across various creative images. We then analyze the correlation between human judgments and predictions by large vision-language models (LVLMs), confirming that LVLMs exhibit strong alignment with human perception. Building on this observation, we collect LVLM-generated labels to train our CREward model that is applicable to both evaluation and generation of creative images. We explore three applications of CREward: creativity assessment, explainable creativity, and creative sample acquisition for both human design inspiration and guiding creative generation through low-rank adaptation.

</details>


### [82] [On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation](https://arxiv.org/abs/2511.20002)
*Changyue Li,Jiaying Li,Youliang Yuan,Jiaming He,Zhicong Huang,Pinjia He*

Main category: cs.CV

TL;DR: 提出语义感知的通用扰动（SAUP），用单一小扰动同时操纵多步决策链中多个预定义输出，在多模态大模型上以简单对抗边框实现多目标70%攻击成功率，并发布带细粒度语义标注的新数据集RIST。


<details>
  <summary>Details</summary>
Motivation: 现实系统往往是序贯决策，单一步误判可被后续纠正，但连锁错误会带来高风险。现有对抗攻击多针对单一决策或单一目标，缺乏对“多目标、按语义条件触发”的通用扰动研究。作者旨在揭示：是否存在一次性扰动即可劫持整个决策链、并按输入语义导向不同错误的威胁。

Method: 提出SAUP：一种语义感知的通用对抗扰动。核心做法：在归一化空间中搜索扰动，并采用语义分离策略，让同一扰动对不同语义类别诱导到各自预设的错误目标。优化上使用面向多目标的联合损失，保证跨类别的目标可控且互不干扰。为实测威胁，构建RIST数据集，提供细粒度语义标注；在多模态大语言模型上以“对抗边框”形式注入扰动进行评测。

Result: 在三种多模态大语言模型上，使用单一对抗边框，即可同时控制5个不同语义目标，攻击成功率达70%。同时展示对场景标志（如“非机动车道”→“机动车道”）与实体识别（“行人”→“塑料袋”）等多类任务的可控误导能力。

Conclusion: 单一通用扰动可以语义条件化地劫持序贯决策链，多模态大模型对此脆弱。SAUP与RIST揭示了现实部署中的系统性风险，提示需要面向语义条件与多目标的稳健防御与评测基准。

Abstract: Conventional adversarial attacks focus on manipulating a single decision of neural networks. However, real-world models often operate in a sequence of decisions, where an isolated mistake can be easily corrected, but cascading errors can lead to severe risks.
  This paper reveals a novel threat: a single perturbation can hijack the whole decision chain. We demonstrate the feasibility of manipulating a model's outputs toward multiple, predefined outcomes, such as simultaneously misclassifying "non-motorized lane" signs as "motorized lane" and "pedestrian" as "plastic bag".
  To expose this threat, we introduce Semantic-Aware Universal Perturbations (SAUPs), which induce varied outcomes based on the semantics of the inputs. We overcome optimization challenges by developing an effective algorithm, which searches for perturbations in normalized space with a semantic separation strategy. To evaluate the practical threat of SAUPs, we present RIST, a new real-world image dataset with fine-grained semantic annotations. Extensive experiments on three multimodal large language models demonstrate their vulnerability, achieving a 70% attack success rate when controlling five distinct targets using just an adversarial frame.

</details>


### [83] [Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network](https://arxiv.org/abs/2511.20008)
*Yuanzhe Li,Steffen Müller*

Main category: cs.CV

TL;DR: 提出一种多模态融合网络，用于预测行人过街意图，融合视觉与运动七种模态特征，借助深度引导注意力、模态注意力与时间注意力，在JAAD数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 行人过街意图预测对自动驾驶安全至关重要，但受行人行为多样性和多种上下文因素影响，预测困难，需要有效融合多模态信息以捕捉关键线索。

Method: 构建多模态融合网络：从视觉与运动分支的原始输入中，使用多种基于Transformer的特征提取模块获取特征；引入深度引导注意力模块，用深度信息引导跨模态的空间注意以突出显著区域；设计模态注意力与时间注意力，分别自适应选择重要模态并建模时间依赖；综合7种模态特征进行融合与决策。

Result: 在JAAD数据集上进行大量实验，所提方法在预测行人过街意图任务上取得优于多种基线方法的性能。

Conclusion: 利用深度引导的跨模态注意与模态/时间注意力机制的多模态融合网络能有效整合视觉与运动信息，提升行人过街意图预测精度，验证了方法的有效性与实用潜力。

Abstract: Pedestrian crossing intention prediction is essential for the deployment of autonomous vehicles (AVs) in urban environments. Ideal prediction provides AVs with critical environmental cues, thereby reducing the risk of pedestrian-related collisions. However, the prediction task is challenging due to the diverse nature of pedestrian behavior and its dependence on multiple contextual factors. This paper proposes a multimodal fusion network that leverages seven modality features from both visual and motion branches, aiming to effectively extract and integrate complementary cues across different modalities. Specifically, motion and visual features are extracted from the raw inputs using multiple Transformer-based extraction modules. Depth-guided attention module leverages depth information to guide attention towards salient regions in another modality through comprehensive spatial feature interactions. To account for the varying importance of different modalities and frames, modality attention and temporal attention are designed to selectively emphasize informative modalities and effectively capture temporal dependencies. Extensive experiments on the JAAD dataset validate the effectiveness of the proposed network, achieving superior performance compared to the baseline methods.

</details>


### [84] [Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments](https://arxiv.org/abs/2511.20011)
*Yuanzhe Li,Hang Zhong,Steffen Müller*

Main category: cs.CV

TL;DR: 提出多上下文融合Transformer（MFT），通过行为、环境、定位、车运动四类数值上下文，分阶段注意力融合以提升行人过街意图预测，在JAADbeh/JAADall/PIE上达73%/93%/90%准确率，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 城市交通中行人意图受多因素影响，传统方法难以充分利用多源上下文且融合不充分，导致预测不准；需要一种能有效聚合异构上下文信息并捕获相互作用的模型，以提高自动驾驶对行人过街意图的早期、可靠预测。

Method: 构建多上下文融合Transformer（MFT）。输入四类数值上下文：行人行为、环境、行人定位、车辆运动。采用分阶段融合：1) 互相的“域内注意力”（mutual intra-context attention）在每个上下文内做双向交互，融合序列并得到各自的context token；2) 互相的“跨域注意力”（mutual cross-context attention）用全局CLS token与各context交互，形成紧凑的多上下文表示；3) 由全局/上下文引导的intra-与cross-注意力进一步细化各context token并强化CLS token，实现更深的引导式信息传播与融合。并进行大量消融验证各模块与各输入的贡献。

Result: 在JAADbeh、JAADall、PIE三数据集上分别达到73%、93%、90%准确率，超越现有SOTA；消融实验表明分阶段注意力与多上下文输入均显著提升性能。

Conclusion: 多上下文、分阶段的注意力融合能有效捕捉行人与环境、车辆间复杂交互，显著提升过街意图预测精度；方法通用、可扩展，代码已开源。

Abstract: Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer.

</details>


### [85] [ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction](https://arxiv.org/abs/2511.20020)
*Yuanzhe Li,Steffen Müller*

Main category: cs.CV

TL;DR: 提出ACIT模型，用跨模态注意力与Transformer融合六种视觉与运动模态，显著提升行人过街意图预测，在JAADbeh/JAADall上达70%/89%准确率，并通过消融验证各模块贡献。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需提前判断行人是否要过街以避免碰撞；不同模态（RGB、光流、语义图、速度、框等）包含互补信息，但有效提取与融合这些线索困难。

Method: 构建注意力引导的跨模态交互Transformer（ACIT）。将六种模态分为三对：全局语义图-全局光流、本地RGB-本地光流、车速-行人框。视觉对内采用双路径注意力：主模态的自注意力强化显著区域；以光流为辅的引导注意力实现深度交互。运动对内用跨模态注意力建模动态互补。时间步层面引入多模态特征融合模块促进跨模态交互，并用Transformer型时序聚合捕获序列依赖。

Result: 在JAADbeh与JAADall数据集上分别取得70%与89%准确率，优于最新方法；并提供广泛消融实验分析各模块作用。

Conclusion: 跨模态注意力与分组交互结合时序Transformer能有效整合视觉与运动线索，提升行人过街意图预测性能；各组件（双路径注意、跨模态运动交互、融合与时序聚合）均有贡献。

Abstract: Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.

</details>


### [86] [WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving](https://arxiv.org/abs/2511.20022)
*Seungjun Yu,Seonho Lee,Namho Kim,Jaeyo Shin,Junsung Park,Wonjeong Ryu,Raehyuk Jung,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出安全关键推理任务：用多视角输入，先处理眼前风险，再缓解决策引入的下游风险；并发布WaymoQA（3.5万问答，图像/视频、多选/开放）以微调MLLMs，显著提升在高风险驾驶场景中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在驾驶场景理解上进步明显，但在安全关键情境中需要多步、高层次推理；单一前视角难以全面把握环境，避免一个风险可能引发新的风险，急需系统化任务与数据来提升模型在此类场景中的推理与决策能力。

Method: 1) 定义“安全关键推理”任务，采用多视角（多相机）输入；2) 将推理分两阶段：先化解即时风险，再评估并缓解由决策带来的后续风险；3) 构建WaymoQA数据集（3.5万人工标注QA，覆盖复杂高风险场景，包含图像与视频、选择题与开放题）；4) 以现有MLLM为基底进行评测与微调。

Result: 实验证明：现有MLLM在安全关键场景显著劣于普通场景；使用WaymoQA微调后，在该任务上的推理表现显著提升。

Conclusion: 多视角输入与两阶段推理范式对安全关键驾驶推理有效；WaymoQA为训练更安全、更具推理能力的驾驶智能体提供了有力数据支持。

Abstract: Recent advancements in multimodal large language models (MLLMs) have shown strong understanding of driving scenes, drawing interest in their application to autonomous driving. However, high-level reasoning in safety-critical scenarios, where avoiding one traffic risk can create another, remains a major challenge. Such reasoning is often infeasible with only a single front view and requires a comprehensive view of the environment, which we achieve through multi-view inputs. We define Safety-Critical Reasoning as a new task that leverages multi-view inputs to address this challenge. Then, we distill Safety-Critical Reasoning into two stages: first resolve the immediate risk, then mitigate the decision-induced downstream risks. To support this, we introduce WaymoQA, a dataset of 35,000 human-annotated question-answer pairs covering complex, high-risk driving scenarios. The dataset includes multiple-choice and open-ended formats across both image and video modalities. Experiments reveal that existing MLLMs underperform in safety-critical scenarios compared to normal scenes, but fine-tuning with WaymoQA significantly improves their reasoning ability, highlighting the effectiveness of our dataset in developing safer and more reasoning-capable driving agents.

</details>


### [87] [SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM](https://arxiv.org/abs/2511.20027)
*Lin Chen,Yingjian Zhu,Qi Yang,Xin Niu,Kun Ding,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出SAM-MI框架，将SAM高质量分割与开放词汇语义分割(OVSS)更紧密融合：稀疏文本引导点提示、浅层掩码聚合与解耦掩码注入三招，既提速又抑制过分割，并避免掩码与标签的生硬绑定。实验在多基准显著优于现有方法（如在MESS上较Grounded-SAM相对提升16.7% mIoU、速度提升1.6×）。


<details>
  <summary>Details</summary>
Motivation: 现有OVSS想借力SAM的强大分割，但面临两大痛点：SAM易过分割，且将“固定掩码”与“开放词汇标签”硬绑定带来噪声与鲁棒性问题；同时，常用致密网格提示导致生成开销大、速度慢。需要一种既高效又稳健的整合方式。

Method: 提出SAM-MI三组件：1) 文本引导稀疏点提示器（Text-guided Sparse Point Prompter），由文本先验筛选潜在类别/区域，只采样少量关键点提示SAM，替代密集网格提示以显著加速；2) 浅层掩码聚合（SMAgg），将SAM的局部/部分掩码在浅层进行合并，缓解过分割问题；3) 解耦掩码注入（DMI），在模型不同频段（低频/高频）通路分别注入SAM掩码作为引导，而非直接与标签拼接，从而减少冲突与噪声传播。

Result: 在多个基准上验证优越性：在MESS基准上相对Grounded-SAM的mIoU相对提升16.7%，推理速度提升1.6倍；总体显示更快且更稳健的开放词汇分割性能。

Conclusion: SAM-MI提供了一条有效将SAM注入OVSS模型的路径：通过稀疏提示、浅层掩码聚合与频域解耦注入，既缓解SAM过分割，又避免掩码-标签硬对齐，达成更高精度与更高效率，可作为装备OVSS的可行替代范式。

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM's tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM's over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.

</details>


### [88] [Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention](https://arxiv.org/abs/2511.20032)
*Jianfei Zhao,Feng Zhang,Xin Sun,Chong Feng,Zhixing Tan*

Main category: cs.CV

TL;DR: 提出一种无需训练的视觉引导注意力（VGA）方法，通过从视觉token中构建精确的语义定位来引导MLLM关注相关区域，显著降低幻觉并兼容高效注意力实现。


<details>
  <summary>Details</summary>
Motivation: MLLM依赖视觉注意力理解图像，但定位能力不足，导致生成幻觉。尽管模型能从视觉token提取语义，却未在推理时充分利用这些语义信息，需要一种利用显式视觉引导来提升对齐与定位的机制。

Method: 提出VGA：1）利用视觉token的语义内容先构建精确的视觉grounding；2）以该grounding在推理时重加权注意力，聚焦相关区域；3）在图像描述任务中，随生成过程动态抑制已描述区域以避免重复和幻觉；4）方法为训练自由，仅需每个token一次前向传播，增加约4.36%延迟，并与FlashAttention等高效注意力实现兼容。

Result: 在多种MLLM与多套幻觉基准上，VGA取得SOTA去幻觉效果；分析显示显式视觉引导显著增强模型的视觉理解。

Conclusion: 通过在推理阶段引入基于视觉token语义的显式grounding与注意力引导，VGA在几乎不增加开销的情况下有效降低幻觉、提升视觉对齐，且具备良好通用性与工程兼容性。

Abstract: Visual attention serves as the primary mechanism through which MLLMs interpret visual information; however, its limited localization capability often leads to hallucinations. We observe that although MLLMs can accurately extract visual semantics from visual tokens, they fail to fully leverage this advantage during subsequent inference. To address this limitation, we propose Vision-Guided Attention (VGA), a training-free method that first constructs precise visual grounding by exploiting the semantic content of visual tokens, and then uses this grounding to guide the model's focus toward relevant visual regions. In image captioning, VGA further refines this guidance dynamically during generation by suppressing regions that have already been described. In VGA, each token undergoes only a single forward pass, introducing a negligible latency overhead of just 4.36\%. In addition, VGA is fully compatible with efficient attention implementations such as FlashAttention. Extensive experiments across diverse MLLMs and multiple hallucination benchmarks demonstrate that VGA achieves state-of-the-art dehallucination performance. Further analysis confirms that explicit visual guidance plays a crucial role in enhancing the visual understanding capabilities of MLLMs.

</details>


### [89] [Clair Obscur: an Illumination-Aware Method for Real-World Image Vectorization](https://arxiv.org/abs/2511.20034)
*Xingyue Lin,Shuai Peng,Xiangyu Xie,Jianhua Zhu,Yuxuan Zhou,Liangcai Gao*

Main category: cs.CV

TL;DR: 提出COVec：一种受明暗对照（Clair-Obscur）启发、具有固有图像分解的矢量化框架，在保持高视觉保真度的同时显著提升可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有图像矢量化方法在真实复杂场景下往往产生碎片化形状，难以兼顾语义简洁与视觉保真，编辑操作也受限。需要一种能以语义一致、物理合理的方式表示与编辑自然图像的矢量化方案。

Method: 1) 在矢量域首次引入固有图像分解，将图像统一表示为反照率（albedo）、阴影（shade）与光照（light）三个层。2) 采用语义引导的初始化生成结构化初始图元。3) 通过可微渲染进行两阶段优化，逐步细化各层参数与形状，使渲染结果与目标图像对齐。

Result: 在多种数据集上，相比现有方法取得更高的视觉保真度，并显著减少碎片化、提升结果的语义简洁与后续编辑的便捷性。

Conclusion: 将明暗对照思想与固有图像分解引入矢量化可在保证拟真度的同时提供物理可解释、语义友好的表示，适合更强的编辑与重用；COVec为复杂真实图像矢量化提供了有效途径。

Abstract: Image vectorization aims to convert raster images into editable, scalable vector representations while preserving visual fidelity. Existing vectorization methods struggle to represent complex real-world images, often producing fragmented shapes at the cost of semantic conciseness. In this paper, we propose COVec, an illumination-aware vectorization framework inspired by the Clair-Obscur principle of light-shade contrast. COVec is the first to introduce intrinsic image decomposition in the vector domain, separating an image into albedo, shade, and light layers in a unified vector representation. A semantic-guided initialization and two-stage optimization refine these layers with differentiable rendering. Experiments on various datasets demonstrate that COVec achieves higher visual fidelity and significantly improved editability compared to existing methods.

</details>


### [90] [MFM-point: Multi-scale Flow Matching for Point Cloud Generation](https://arxiv.org/abs/2511.20041)
*Petr Molodyk,Jaemoo Choi,David W. Romero,Ming-Yu Liu,Yongxin Chen*

Main category: cs.CV

TL;DR: 提出 MFM-Point：一种多尺度 Flow Matching 的点云生成方法，在保持点方法简单高效的同时显著提升可扩展性与性能。通过结构化降采样/升采样实现跨分辨率的几何一致和分布平滑过渡，采用粗到细生成且无额外训练/推理开销，达到点方法SOTA并接近/挑战表征法。


<details>
  <summary>Details</summary>
Motivation: 点方法直接生成点云，训练代价低、实现简单，但性能落后于使用网格/体素/潜变量的表征法。需要一种能在不牺牲简单高效的前提下，提升点方法质量与可扩展性的方案。

Method: 构建多尺度 Flow Matching 框架：1) 粗到细的多尺度生成流程；2) 结构化降采样确保低分辨率保留几何结构；3) 与之配套的结构化升采样在尺度间对齐分布，保证从粗到细的平滑一致过渡；4) 设计使多尺度不增加训练与推理开销。

Result: 在多类别与高分辨率点云生成任务上，MFM-Point 在点方法中达到SOTA，并对最强的表征法形成挑战，显示更好的生成质量与可扩展性。

Conclusion: 多尺度 Flow Matching + 结构化降/升采样能在保持点方法简洁高效的同时，显著提升生成质量与可扩展性，为点云生成提供了实用的多尺度范式。

Abstract: In recent years, point cloud generation has gained significant attention in 3D generative modeling. Among existing approaches, point-based methods directly generate point clouds without relying on other representations such as latent features, meshes, or voxels. These methods offer low training cost and algorithmic simplicity, but often underperform compared to representation-based approaches. In this paper, we propose MFM-Point, a multi-scale Flow Matching framework for point cloud generation that substantially improves the scalability and performance of point-based methods while preserving their simplicity and efficiency. Our multi-scale generation algorithm adopts a coarse-to-fine generation paradigm, enhancing generation quality and scalability without incurring additional training or inference overhead. A key challenge in developing such a multi-scale framework lies in preserving the geometric structure of unordered point clouds while ensuring smooth and consistent distributional transitions across resolutions. To address this, we introduce a structured downsampling and upsampling strategy that preserves geometry and maintains alignment between coarse and fine resolutions. Our experimental results demonstrate that MFM-Point achieves best-in-class performance among point-based methods and challenges the best representation-based methods. In particular, MFM-point demonstrates strong results in multi-category and high-resolution generation tasks.

</details>


### [91] [History-Augmented Contrastive Meta-Learning for Unsupervised Blind Super-Resolution of Planetary Remote Sensing Images](https://arxiv.org/abs/2511.20045)
*Huijia Zhao,Jie Lu,Yunqing Jiang,Xiao-Ping Lu,Kaichang Di*

Main category: cs.CV

TL;DR: 提出HACBSR：一种无需真实高分图和先验核的无监督行星影像盲超分方法，结合对比核采样与历史增强对比学习，并发布行星数据集Ceres-50；在多倍率上与SOTA无监督方法竞争且给出收敛分析。


<details>
  <summary>Details</summary>
Motivation: 行星遥感受复杂未知退化（环境/硬件）影响，缺乏高质量真值，难以进行有监督盲超分；现有方法依赖简单高斯核或外部先验，存在分布偏差与优化贪心等问题。

Method: 1) 对比式核采样：在高斯采样基础上引入核相似度控制，生成多样且可控的退化核，缓解分布偏置；2) 历史增强对比学习：利用历代模型生成负样本，抑制贪婪优化，在无真值情形下诱导“强凸性”，并提供收敛性分析；整体框架无需外部核先验与GT。

Result: 在新建的Ceres-50行星数据集及多种放大倍率下，HACBSR取得与当前无监督SOTA相当或更优的性能；代码与数据公开可复现。

Conclusion: HACBSR在缺乏真值和核先验的行星盲超分任务中有效，核心在于对比核采样与历史增强对比学习的结合；方法收敛性有理论支持，并通过Ceres-50验证其实用性。

Abstract: Planetary remote sensing images are affected by diverse and unknown degradations caused by imaging environments and hardware constraints. These factors limit image quality and hinder supervised blind super-resolution due to the lack of ground-truth images. This work presents History-Augmented Contrastive Blind Super-Resolution (HACBSR), an unsupervised framework for blind super-resolution that operates without ground-truth images and external kernel priors. HACBSR comprises two components: (1) a contrastive kernel sampling mechanism with kernel similarity control to mitigate distribution bias from Gaussian sampling, and (2) a history-augmented contrastive learning that uses historical models to generate negative samples to enable less greedy optimization and to induce strong convexity without ground-truth. A convergence analysis of the history-augmented contrastive learning is given in the Appendix. To support evaluation in planetary applications, we introduce Ceres-50, a dataset with diverse geological features simulated degradation patterns. Experiments show that HACBSR achieves competitive performance compared with state-of-the-art unsupervised methods across multiple upscaling factors. The code is available at https://github.com/2333repeat/HACBSR, and the dataset is available at https://github.com/2333repeat/Ceres-50.

</details>


### [92] [DeLightMono: Enhancing Self-Supervised Monocular Depth Estimation in Endoscopy by Decoupling Uneven Illumination](https://arxiv.org/abs/2511.20058)
*Mingyang Ou,Haojin Li,Yifeng Zhang,Ke Niu,Zhongxi Qiu,Heng Li,Jiang Liu*

Main category: cs.CV

TL;DR: 提出DeLight-Mono：一种利用光照解耦的自监督单目深度估计框架，针对内窥镜图像不均匀照明导致的深度退化问题，通过将图像分解为照明-反射-深度成分并联合优化专门的损失，显著提升低光与不均匀照明场景下的深度估计性能，已在两个公共数据集上经消融与对比验证有效。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像常存在强烈且不均匀的照明，低亮区域信息受损，导致自监督单目深度估计性能下降；通用低光增强无法有效服务深度网络，自动驾驶领域的方法又依赖良好照明且数据成本高，亟需面向内窥镜的照明鲁棒深度估计方案。

Method: 1) 建立内窥镜图像的照明-反射-深度（I-R-D）成像模型；2) 通过辅助网络将图像分解为照明与反射分量并与深度协同；3) 设计自监督联合优化框架，结合新型损失函数，利用解耦后的照明、反射与深度信息抑制不均匀照明对深度学习的干扰。

Result: 在两个公开内窥镜数据集上进行广泛对比与消融实验，方法在低光与不均匀照明场景中取得优于现有技术的深度估计效果，验证了所提框架与损失设计的有效性。

Conclusion: 通过显式的照明解耦与联合自监督优化，DeLight-Mono有效缓解内窥镜图像不均匀照明对深度估计的负面影响，提供了在低光条件下更稳健的单目深度估计方案。

Abstract: Self-supervised monocular depth estimation serves as a key task in the development of endoscopic navigation systems. However, performance degradation persists due to uneven illumination inherent in endoscopic images, particularly in low-intensity regions. Existing low-light enhancement techniques fail to effectively guide the depth network. Furthermore, solutions from other fields, like autonomous driving, require well-lit images, making them unsuitable and increasing data collection burdens. To this end, we present DeLight-Mono - a novel self-supervised monocular depth estimation framework with illumination decoupling. Specifically, endoscopic images are represented by a designed illumination-reflectance-depth model, and are decomposed with auxiliary networks. Moreover, a self-supervised joint-optimizing framework with novel losses leveraging the decoupled components is proposed to mitigate the effects of uneven illumination on depth estimation. The effectiveness of the proposed methods was rigorously verified through extensive comparisons and an ablation study performed on two public datasets.

</details>


### [93] [FLaTEC: Frequency-Disentangled Latent Triplanes for Efficient Compression of LiDAR Point Clouds](https://arxiv.org/abs/2511.20065)
*Xiaoge Zhang,Zijie Wu,Mingtao Feng,Zichen Geng,Mehwish Nasim,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出FLaTEC：一种频率感知的点云压缩模型，通过解耦低频结构与高频纹理并采用潜变量三平面表示，在高压缩比下保持高重建质量，达到SOTA的率失真表现。


<details>
  <summary>Details</summary>
Motivation: 传统点云压缩难以在同一分辨率下兼顾压缩率与重建质量，因为低频（结构）与高频（细节）的贡献不同；此外体素化带来稀疏性与高计算/存储开销。需要一种能分别处理频段、降低稀疏与开销、并保留3D相关性的方案。

Method: 1) 将体素嵌入转换为潜变量三平面(triplane)以降低稀疏性、计算与存储成本；2) 频率解耦：提取紧凑的低频内容并跨尺度收集高频细节；3) 以二进制格式分别存储低频/高频分量；4) 解码时通过调制模块逐步恢复全频谱信号；5) 频率型注意力补偿3D相关性缺失，促进局部连接，并可输出任意分辨率点。

Result: 在SemanticKITTI与Ford数据集上取得SOTA率失真表现，相比标准编解码器在BD-rate上分别提升78%与94%。

Conclusion: 频率感知+三平面潜表示的组合能在高压缩比下有效保真地重建点云；频率解耦、逐步调制恢复及频率注意力共同带来显著的率失真优势并具备可变分辨率输出能力。

Abstract: Point cloud compression methods jointly optimize bitrates and reconstruction distortion. However, balancing compression ratio and reconstruction quality is difficult because low-frequency and high-frequency components contribute differently at the same resolution. To address this, we propose FLaTEC, a frequency-aware compression model that enables the compression of a full scan with high compression ratios. Our approach introduces a frequency-aware mechanism that decouples low-frequency structures and high-frequency textures, while hybridizing latent triplanes as a compact proxy for point cloud. Specifically, we convert voxelized embeddings into triplane representations to reduce sparsity, computational cost, and storage requirements. We then devise a frequency-disentangling technique that extracts compact low-frequency content while collecting high-frequency details across scales. The decoupled low-frequency and high-frequency components are stored in binary format. During decoding, full-spectrum signals are progressively recovered via a modulation block. Additionally, to compensate for the loss of 3D correlation, we introduce an efficient frequency-based attention mechanism that fosters local connectivity and outputs arbitrary resolution points. Our method achieves state-of-the-art rate-distortion performance and outperforms the standard codecs by 78\% and 94\% in BD-rate on both SemanticKITTI and Ford datasets.

</details>


### [94] [PRADA: Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images](https://arxiv.org/abs/2511.20068)
*Simon Damm,Jonas Ricker,Henning Petzka,Asja Fischer*

Main category: cs.CV

TL;DR: 提出PRADA：用条件/无条件概率比来检测并归因自回归图像生成模型的图像，简单、可解释、效果强。


<details>
  <summary>Details</summary>
Motivation: AR图像生成迅速发展，生成图像逼真，现有检测方法不足，缺少专门针对AR生成图像的可靠检测与归因手段。

Method: 将图像表示为AR标记序列，计算给定模型对该序列的条件概率与无条件概率之比（probability ratio）。发现同源模型生成的图像在该比值上呈现独特模式。通过为每个模型校准一个简单的阈值化打分函数，实现检测与归因。

Result: 在8个类到图像和4个文到图像AR模型上验证，PRADA能稳健区分真实与生成图像，并准确归因到具体源模型，效果显著。

Conclusion: 基于概率比的简单可解释框架能有效检测并归因AR生成图像；通过模型特定阈值校准实现可靠落地，跨多种AR生成器表现良好。

Abstract: Autoregressive (AR) image generation has recently emerged as a powerful paradigm for image synthesis. Leveraging the generation principle of large language models, they allow for efficiently generating deceptively real-looking images, further increasing the need for reliable detection methods. However, to date there is a lack of work specifically targeting the detection of images generated by AR image generators. In this work, we present PRADA (Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images), a simple and interpretable approach that can reliably detect AR-generated images and attribute them to their respective source model. The key idea is to inspect the ratio of a model's conditional and unconditional probability for the autoregressive token sequence representing a given image. Whenever an image is generated by a particular model, its probability ratio shows unique characteristics which are not present for images generated by other models or real images. We exploit these characteristics for threshold-based attribution and detection by calibrating a simple, model-specific score function. Our experimental evaluation shows that PRADA is highly effective against eight class-to-image and four text-to-image models.

</details>


### [95] [Learning Procedural-aware Video Representations through State-Grounded Hierarchy Unfolding](https://arxiv.org/abs/2511.20073)
*Jinghan Zhao,Yifei Huang,Feng Lu*

Main category: cs.CV

TL;DR: 提出Task-Step-State (TSS) 框架，引入“状态”作为视觉可观测的文本锚点，并配合渐进式预训练，将任务-步骤-状态层级对齐，从而在COIN与CrossTask上提升任务识别、步骤识别与下一步预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于任务/步骤文本对齐的方法过于抽象，难以与视频中的可观测细节稳健对齐，导致程序化语义难以有效注入视频表征。需要一个更贴近视觉证据的语义层来桥接抽象流程与具体画面。

Method: 引入“状态”（对象配置的文本快照）作为视觉锚点，提出TSS层级：任务由步骤构成，步骤促成状态之间的可观测转移。设计渐进式预训练策略：先以状态为核心进行表征学习，再逐步关联到步骤与任务层级，从而强化层级与可视化对齐。与联合训练对比，渐进式更好地施加结构约束。

Result: 在COIN与CrossTask数据集上，相较基线在任务识别、步骤识别、下一步预测等下游任务上均取得更优性能；消融表明状态监督是性能提升的关键因素。

Conclusion: 通过以状态为中心的TSS框架与渐进式预训练，可更好地将程序化语义落地到可观测视觉证据中，提升多种程序理解任务的表现，且分阶段训练优于传统的联合训练以强化层级结构。

Abstract: Learning procedural-aware video representations is a key step towards building agents that can reason about and execute complex tasks. Existing methods typically address this problem by aligning visual content with textual descriptions at the task and step levels to inject procedural semantics into video representations. However, due to their high level of abstraction, 'task' and 'step' descriptions fail to form a robust alignment with the concrete, observable details in visual data. To address this, we introduce 'states', i.e., textual snapshots of object configurations, as a visually-grounded semantic layer that anchors abstract procedures to what a model can actually see. We formalize this insight in a novel Task-Step-State (TSS) framework, where tasks are achieved via steps that drive transitions between observable states. To enforce this structure, we propose a progressive pre-training strategy that unfolds the TSS hierarchy, forcing the model to ground representations in states while associating them with steps and high-level tasks. Extensive experiments on the COIN and CrossTask datasets show that our method outperforms baseline models on multiple downstream tasks, including task recognition, step recognition, and next step prediction. Ablation studies show that introducing state supervision is a key driver of performance gains across all tasks. Additionally, our progressive pretraining strategy proves more effective than standard joint training, as it better enforces the intended hierarchical structure.

</details>


### [96] [Blind Adaptive Local Denoising for CEST Imaging](https://arxiv.org/abs/2511.20081)
*Chu Chen,Aitor Artola,Yang Liu,Se Weon Park,Raymond H. Chan,Jean-Michel Morel,Kannie W. Y. Chan*

Main category: cs.CV

TL;DR: 提出一种用于CEST MRI的盲自适应局部去噪（BALD），通过自相似驱动的方差稳定与局部SVD两阶段线性域去噪，在幻影与在体实验中优于现有方法，并提升APT等定量图与癌症检测性能。


<details>
  <summary>Details</summary>
Motivation: CEST MRI可在分子水平成像低浓度代谢物，但硬件与多样协议导致空间变噪与异方差，破坏APT等定量对比度的准确性；现有去噪不适配这种复杂噪声且易扭曲生物学信号，亟需兼顾异方差与信号保真的去噪方案。

Method: 提出BALD：1）基于CEST数据自相似性，自适应估计并构造方差稳定变换，使各像素噪声分布均化且无需先验噪声模型；2）在经线性变换的域内进行两阶段去噪以分离分子信号与噪声；3）使用局部SVD作为线性变换，降低空间与谱域伪影；并在多种幻影与在体数据上验证。

Result: BALD在多组数据中在去噪质量指标上领先于最先进CEST去噪器；在下游任务（分子浓度图估计、癌症检测）上也获得更高准确性与稳健性。

Conclusion: BALD有效处理CEST中的异方差与复杂噪声，在保证信号保真的同时提升定量成像与临床相关任务表现，推动CEST MRI向临床转化。

Abstract: Chemical Exchange Saturation Transfer (CEST) MRI enables molecular-level visualization of low-concentration metabolites by leveraging proton exchange dynamics. However, its clinical translation is hindered by inherent challenges: spatially varying noise arising from hardware limitations, and complex imaging protocols introduce heteroscedasticity in CEST data, perturbing the accuracy of quantitative contrast mapping such as amide proton transfer (APT) imaging. Traditional denoising methods are not designed for this complex noise and often alter the underlying information that is critical for biomedical analysis. To overcome these limitations, we propose a new Blind Adaptive Local Denoising (BALD) method. BALD exploits the self-similar nature of CEST data to derive an adaptive variance-stabilizing transform that equalizes the noise distributions across CEST pixels without prior knowledge of noise characteristics. Then, BALD performs two-stage denoising on a linear transformation of data to disentangle molecular signals from noise. A local SVD decomposition is used as a linear transform to prevent spatial and spectral denoising artifacts. We conducted extensive validation experiments on multiple phantoms and \textit{in vivo} CEST scans. In these experiments, BALD consistently outperformed state-of-the-art CEST denoisers in both denoising metrics and downstream tasks such as molecular concentration maps estimation and cancer detection.

</details>


### [97] [Explainable Visual Anomaly Detection via Concept Bottleneck Models](https://arxiv.org/abs/2511.20088)
*Arianna Stropeni,Valentina Zaccaria,Francesco Borsatti,Davide Dalle Pezze,Manuel Barusco,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 提出将概念瓶颈模型（CBM）扩展到视觉异常检测（VAD），在仅用正常样本训练的前提下，实现“概念级+可视化”双重可解释性，并通过合成异常保持无监督范式，性能与经典方法相当且更可解释。


<details>
  <summary>Details</summary>
Motivation: 现有无监督VAD虽能高亮异常区域，但缺少直接、语义化的解释，难以让用户理解“为何异常”。作者希望引入人类可理解的中间概念，使异常解释从热力图上升到语义描述，提升可解释性与信任度。

Method: 1) 构建面向VAD的概念数据集，定义并标注与异常相关的语义概念；2) 改进CBM架构：先预测概念（概念瓶颈），再据概念判定异常，同时生成视觉定位结果，实现语义—空间双解释；3) 设计合成异常生成流程，在不依赖真实异常的前提下训练与评估概念学习与检测能力；提出整体框架CONVAD。

Result: CONVAD在标准VAD指标上取得与经典无监督方法相当的性能，同时输出概念级解释（哪些概念异常）与像素级热力图，显著提升解释性；在只用正常样本训练的设置下仍能稳定工作。

Conclusion: 将CBM引入VAD可在保持检测性能的同时提供更丰富的、可人类理解的异常解释；概念数据集、改进架构与合成异常管线为语义可解释VAD提供实用路径，提升系统可信度。

Abstract: In recent years, Visual Anomaly Detection (VAD) has gained significant attention due to its ability to identify anomalous images using only normal images during training. Many VAD models work without supervision but are still able to provide visual explanations by highlighting the anomalous regions within an image. However, although these visual explanations can be helpful, they lack a direct and semantically meaningful interpretation for users. To address this limitation, we propose extending Concept Bottleneck Models (CBMs) to the VAD setting. By learning meaningful concepts, the network can provide human-interpretable descriptions of anomalies, offering a novel and more insightful way to explain them. Our contributions are threefold: (i) we develop a Concept Dataset to support research on CBMs for VAD; (ii) we improve the CBM architecture to generate both concept-based and visual explanations, bridging semantic and localization interpretability; and (iii) we introduce a pipeline for synthesizing artificial anomalies, preserving the VAD paradigm of minimizing dependence on rare anomalous samples. Our approach, Concept-Aware Visual Anomaly Detection (CONVAD), achieves performance comparable to classic VAD methods while providing richer, concept-driven explanations that enhance interpretability and trust in VAD systems.

</details>


### [98] [WPT: World-to-Policy Transfer via Online World Model Distillation](https://arxiv.org/abs/2511.20095)
*Guangfeng Jiang,Yueru Luo,Jun Liu,Yi Huang,Yiyao Zhu,Zhan Qu,Dave Zhenyu Chen,Bingbing Liu,Xu Yan*

Main category: cs.CV

TL;DR: 提出WPT范式：用端到端世界模型在线蒸馏出轻量学生策略，兼顾精度与实时性，在开放/闭环驾驶基准上达SOTA并显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法要么与推理时规划强耦合导致开销大，要么依赖离线奖励信号难以端到端优化，影响实时部署与性能。需要一种既能利用世界模型的长时预测与因果结构，又能产出可实时部署的轻量策略的方法。

Method: 构建端到端世界模型并引入可训练的奖励模型：通过将候选轨迹与世界模型预测的未来动态对齐，为教师策略提供世界知识引导；随后进行两种蒸馏——策略蒸馏与世界奖励蒸馏——把教师的推理/规划能力迁移到轻量学生策略，实现在线蒸馏与端到端优化。

Result: 在开放环与闭环自动驾驶基准上取得SOTA：开放环碰撞率0.11；闭环Driving Score 79.23，准确性与安全性均优于基于世界模型的在线规划和纯模仿学习方法。学生模型在保留大部分性能提升的同时，推理速度最高提升4.9倍。

Conclusion: WPT通过世界到策略的在线知识迁移，兼顾性能与实时性：在不依赖离线奖励的前提下，将世界模型的预测与推理能力内化到轻量策略中，适合实际部署的高效安全决策。

Abstract: Recent years have witnessed remarkable progress in world models, which primarily aim to capture the spatio-temporal correlations between an agent's actions and the evolving environment. However, existing approaches often suffer from tight runtime coupling or depend on offline reward signals, resulting in substantial inference overhead or hindering end-to-end optimization. To overcome these limitations, we introduce WPT, a World-to-Policy Transfer training paradigm that enables online distillation under the guidance of an end-to-end world model. Specifically, we develop a trainable reward model that infuses world knowledge into a teacher policy by aligning candidate trajectories with the future dynamics predicted by the world model. Subsequently, we propose policy distillation and world reward distillation to transfer the teacher's reasoning ability into a lightweight student policy, enhancing planning performance while preserving real-time deployability. Extensive experiments on both open-loop and closed-loop benchmarks show that our WPT achieves state-of-the-art performance with a simple policy architecture: it attains a 0.11 collision rate (open-loop) and achieves a 79.23 driving score (closed-loop) surpassing both world-model-based and imitation-learning methods in accuracy and safety. Moreover, the student sustains up to 4.9x faster inference, while retaining most of the gains.

</details>


### [99] [Exploring State-of-the-art models for Early Detection of Forest Fires](https://arxiv.org/abs/2511.20096)
*Sharjeel Ahmed,Daim Armaghan,Fatima Naweed,Umair Yousaf,Ahmad Zubair,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出并评测一个用于森林火灾早期视觉检测的数据集，并比较YOLOv7与多种DETR模型在烟柱/初始火焰识别与定位上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有森林火灾检测多依赖大火场景，缺少针对早期烟/小火样本的规模化数据与专门模型，导致漏检率高，难以及时预警。

Method: 1) 构建早期火情视觉数据集，重点包含烟柱与初起火焰；2) 采用游戏模拟器（如RDR2）合成数据，并与公开图像混合；3) 在该数据集上评估图像分类与目标定位方法，重点比较YOLOv7与多种Detection Transformer（DETR）模型。

Result: 在所提出的数据集上完成了分类与定位实验，对比了YOLOv7与不同DETR变体的性能差异（文摘未给出具体数值，但表明完成了系统性比较）。

Conclusion: 面向早期森林火情的专用数据集是可行且必要的；合成数据（游戏引擎）与真实图片结合能丰富样本；主流检测/定位框架（YOLOv7、DETR）可用于早期烟/火识别，为后续优化与部署早期预警系统奠定基础。

Abstract: There have been many recent developments in the use of Deep Learning Neural Networks for fire detection. In this paper, we explore an early warning system for detection of forest fires. Due to the lack of sizeable datasets and models tuned for this task, existing methods suffer from missed detection. In this work, we first propose a dataset for early identification of forest fires through visual analysis. Unlike existing image corpuses that contain images of wide-spread fire, our dataset consists of multiple instances of smoke plumes and fire that indicates the initiation of fire. We obtained this dataset synthetically by utilising game simulators such as Red Dead Redemption 2. We also combined our dataset with already published images to obtain a more comprehensive set. Finally, we compared image classification and localisation methods on the proposed dataset. More specifically we used YOLOv7 (You Only Look Once) and different models of detection transformer.

</details>


### [100] [Multi Head Attention Enhanced Inception v3 for Cardiomegaly Detection](https://arxiv.org/abs/2511.20101)
*Abishek Karthik,Pandiyaraju V*

Main category: cs.CV

TL;DR: 提出一种结合InceptionV3与多头注意力的胸片心脏增大（心脏扩大）自动检测方法，经过充分的数据收集与预处理，模型在多指标上取得约96%左右的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病诊断中，心脏扩大（心脏肥大）是重要结构异常。传统读取X光片依赖人工、易主观，期望用深度学习与注意力机制提升自动化、可用性与诊断敏感性。

Method: 构建以CNN为基础、以Inception V3为核心特征提取模块的模型，并引入多层/多头注意力：对输入图像进行选择性关注，计算并复制注意力权重以增强关键区域表征；完整流程包括数据收集标注、图像预处理、模型训练与严格评估。

Result: 在测试中报告：准确率95.6%、精确率95.2%、召回率96.2%、敏感度95.7%、特异度96.1%、AUC 96.0，并提供相应可视化曲线。

Conclusion: 所提方法能有效、敏感地从X光片中检测心脏增大，具临床潜在价值；注意力机制提升对关键区域的表征与判别能力。

Abstract: The healthcare industry has been revolutionized significantly by novel imaging technologies, not just in the diagnosis of cardiovascular diseases but also by the visualization of structural abnormalities like cardiomegaly. This article explains an integrated approach to the use of deep learning tools and attention mechanisms for automatic detection of cardiomegaly using X-ray images. The initiation of the project is grounded on a strong Data Collection phase and gathering the data of annotated X-ray images of various types. Then, while the Preprocessing module fine-tunes image quality, it is feasible to utilize the best out of the data quality in the proposed system. In our proposed system, the process is a CNN configuration leveraging the inception V3 model as one of the key blocks. Besides, we also employ a multilayer attention mechanism to enhance the strength. The most important feature of the method is the multi-head attention mechanism that can learn features automatically. By exact selective focusing on only some regions of input, the model can thus identify cardiomegaly in a sensitive manner. Attention rating is calculated, duplicated, and applied to enhance representation of main data, and therefore there is a successful diagnosis. The Evaluation stage will be extremely strict and it will thoroughly evaluate the model based on such measures as accuracy and precision. This will validate that the model can identify cardiomegaly and will also show the clinical significance of this method. The model has accuracy of 95.6, precision of 95.2, recall of 96.2, sensitivity of 95.7, specificity of 96.1 and an Area Under Curve(AUC) of 96.0 and their respective graphs are plotted for visualisation.

</details>


### [101] [LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening](https://arxiv.org/abs/2511.20116)
*Johannes Brandt,Maulik Chevli,Rickmer Braren,Georgios Kaissis,Philip Müller,Daniel Rueckert*

Main category: cs.CV

TL;DR: 提出LungEvaty：基于Transformer、端到端、仅用单次LDCT即可预测1–6年肺癌风险，输入为整肺体积，无需像素级标注；在>90,000例CT上训练，达到SOTA，可选AIAG损失引导注意力至解剖相关区域；开源、简单、可扩展。


<details>
  <summary>Details</summary>
Motivation: 人口级LDCT筛查使得肺癌风险估计需求激增。现有方法要么依赖像素级/结节级标注，难以扩展；要么分块分析导致上下文丢失与性能下降。亟需能在整肺尺度上、高效从大规模无像素标注数据中学习的端到端模型。

Method: 提出全Transformer架构LungEvaty：以整肺LDCT为输入，直接从大规模筛查数据学习与恶性风险相关的全局/局部解剖病理线索；训练时不使用区域级监督，仅影像数据。另提供可选的AIAG（解剖感知注意力引导）损失，促使注意力集中在解剖相关区域，从而精炼性能。

Result: 在>90,000例CT上训练，其中>28,000用于微调、6,000用于评估；仅凭影像与无区划监督即可匹配SOTA表现，加入AIAG可进一步提升并产生更解剖聚焦的注意力。

Conclusion: LungEvaty实现了简单、数据高效、可扩展且开源的整肺级Transformer框架，为后续纵向与多模态肺癌风险预测研究提供通用基础。

Abstract: Lung cancer risk estimation is gaining increasing importance as more countries introduce population-wide screening programs using low-dose CT (LDCT). As imaging volumes grow, scalable methods that can process entire lung volumes efficiently are essential to tap into the full potential of these large screening datasets. Existing approaches either over-rely on pixel-level annotations, limiting scalability, or analyze the lung in fragments, weakening performance. We present LungEvaty, a fully transformer-based framework for predicting 1-6 year lung cancer risk from a single LDCT scan. The model operates on whole-lung inputs, learning directly from large-scale screening data to capture comprehensive anatomical and pathological cues relevant for malignancy risk. Using only imaging data and no region supervision, LungEvaty matches state-of-the-art performance, refinable by an optional Anatomically Informed Attention Guidance (AIAG) loss that encourages anatomically focused attention. In total, LungEvaty was trained on more than 90,000 CT scans, including over 28,000 for fine-tuning and 6,000 for evaluation. The framework offers a simple, data-efficient, and fully open-source solution that provides an extensible foundation for future research in longitudinal and multimodal lung cancer risk prediction.

</details>


### [102] [UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers](https://arxiv.org/abs/2511.20123)
*Min Zhao,Hongzhou Zhu,Yingze Wang,Bokai Yan,Jintao Zhang,Guande He,Ling Yang,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 提出UltraViCo：一种无需再训练的衰减注意力策略，抑制训练窗外token的影响，统一解决视频扩展生成中的重复与质量下降，将扩展上限从2倍推至4倍，并在多模型与任务上显著提升质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散Transformer在超出训练长度生成时表现不稳：会出现周期性内容重复和整体画质/动态劣化。既有工作多从位置编码入手，只缓解重复而忽视质量退化，且可扩展长度有限。作者希望从更本质的注意力行为出发，找到统一成因并给出通用、训练无关的解决方案。

Method: 从注意力图分析提出“注意力弥散”现象：训练窗外token稀释了已学注意力模式，导致质量下降；当弥散呈周期结构（受位置编码谐波影响）则产生重复。基于此，提出UltraViCo：对超出训练窗口的token施加常数衰减因子，抑制其注意力贡献；作为训练免、即插即用的推理时策略，通用于各类视频扩散Transformer，并与现有位置编码方法兼容。

Result: 在多模型与多倍率外推上普遍优于基线，将视频长度外推从2×提升到4×；在4×外推时，相比此前最佳方法，Dynamic Degree与Imaging Quality分别提升约233%与40.5%。在可控视频合成与编辑等下游任务中也无缝泛化并带来一致改进。

Conclusion: 视频长度外推的两类失败模式源于统一的注意力弥散；通过对训练窗外token施加常数衰减，可在无需再训练的前提下同时抑制重复与画质下降，显著扩展可外推长度并提升多任务生成质量。

Abstract: Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.

</details>


### [103] [Vision-Language Models for Automated 3D PET/CT Report Generation](https://arxiv.org/abs/2511.20145)
*Wenpei Jiao,Kun Shang,Hui Li,Ke Yan,Jiajin Zhang,Guangjie Yang,Lijuan Guo,Yan Wan,Xing Yang,Dakai Jin,Zhaoheng Xie*

Main category: cs.CV

TL;DR: 提出PETRG-3D：一个面向淋巴瘤PET/CT自动报告生成的端到端3D双分支模型，结合风格自适应提示与新数据集/评测，显著提升自然语言与临床有效性指标。


<details>
  <summary>Details</summary>
Motivation: 临床PET/CT报告需求激增而专业人力不足；PET功能成像相较结构成像更难：代谢模式依赖示踪剂、生理多样且需全身3D上下文；跨医院报告风格差异也影响自动化生成的稳定性与可用性。

Method: 构建PETRG-3D：分别对PET与CT体数据进行双分支3D编码，融合体素级双模态信息；引入风格自适应提示以缓解跨机构报告风格差异。数据方面：收集多中心淋巴瘤数据集PETRG-Lym（4院，824份报告，245,509对切片）；并基于公开影像制作AutoPET-RG-Lym基准（135例，专家撰写、临床验证报告）。评测方面：提出淋巴瘤特异的PETRG-Score，联合度量代谢与结构发现并按解剖区域汇总。

Result: 在自然语言指标上如ROUGE-L提升31.49%；在临床效用指标如PET-All提升8.18%；总体显著优于现有方法，验证了3D体积双模态建模与风格提示的优势。

Conclusion: 工作为PET/CT专用的报告生成奠定基础，强调疾病感知推理与临床可靠评估；代码、模型与AutoPET-RG-Lym将公开。

Abstract: Positron emission tomography/computed tomography (PET/CT) is essential in oncology, yet the rapid expansion of scanners has outpaced the availability of trained specialists, making automated PET/CT report generation (PETRG) increasingly important for reducing clinical workload. Compared with structural imaging (e.g., X-ray, CT, and MRI), functional PET poses distinct challenges: metabolic patterns vary with tracer physiology, and whole-body 3D contextual information is required rather than local-region interpretation. To advance PETRG, we propose PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes PET and CT volumes and incorporates style-adaptive prompts to mitigate inter-hospital variability in reporting practices. We construct PETRG-Lym, a multi-center lymphoma dataset collected from four hospitals (824 reports w/ 245,509 paired PET/CT slices), and construct AutoPET-RG-Lym, a publicly accessible PETRG benchmark derived from open imaging data but equipped with new expert-written, clinically validated reports (135 cases). To assess clinical utility, we introduce PETRG-Score, a lymphoma-specific evaluation protocol that jointly measures metabolic and structural findings across curated anatomical regions. Experiments show that PETRG-3D substantially outperforms existing methods on both natural language metrics (e.g., +31.49\% ROUGE-L) and clinical efficacy metrics (e.g., +8.18\% PET-All), highlighting the benefits of volumetric dual-modality modeling and style-aware prompting. Overall, this work establishes a foundation for future PET/CT-specific models emphasizing disease-aware reasoning and clinically reliable evaluation. Codes, models, and AutoPET-RG-Lym will be released.

</details>


### [104] [Hybrid Convolution and Frequency State Space Network for Image Compression](https://arxiv.org/abs/2511.20151)
*Haodong Pan,Hao Wei,Yusong Wang,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出HCFSSNet：结合卷积与频域状态空间的混合架构，用CNN抓取局部高频，用VFSS建模长程低频，并通过频率自适应模块改进主干与熵模；在Kodak、Tecnick、CLIC上较VTM显著降BD-rate，参数量小于同类SSM方法。


<details>
  <summary>Details</summary>
Motivation: 现有LIC中CNN擅长局部细节但欠缺长程依赖；Transformer/SSM虽擅长全局建模，却易丢结构或忽略频率特性，不利于码率分配与可解释性。需要一种既保留局部高频又高效建模长程低频、并具频域感知的可解释压缩架构。

Method: 提出HCFSSNet：1) CNN分支提取局部高频结构；2) VFSS块用于长程低频建模，由VONSS（水平/垂直/对角全向扫描的邻域状态空间）与AFMM（基于DCT分解并进行内容自适应的频率加权/调制）组成；3) 将AFMM与Swin Transformer融合成FSTAM，用于频率感知的侧信息（熵模型）建模，以减冗余并优化比特分配。

Result: 在Kodak、Tecnick、CLIC Pro验证集上取得与最新SSM编解码器（如MambaIC）相当或更优的RD性能，且参数显著更少；相对VTM基线的BD-rate分别降低18.06%、24.56%、22.44%。

Conclusion: 混合卷积与频域状态空间的设计能同时捕捉局部高频与长程低频，并通过频率自适应机制提升熵模型效率；实现了高效、可解释、参数更精简的学习式图像压缩，为后续LIC研究提供可扩展架构。

Abstract: Learned image compression (LIC) has recently benefited from Transformer based and state space model (SSM) based architectures. Convolutional neural networks (CNNs) effectively capture local high frequency details, whereas Transformers and SSMs provide strong long range modeling capabilities but may cause structural information loss or ignore frequency characteristics that are crucial for compression. In this work we propose HCFSSNet, a Hybrid Convolution and Frequency State Space Network for LIC. HCFSSNet uses CNNs to extract local high frequency structures and introduces a Vision Frequency State Space (VFSS) block that models long range low frequency information. The VFSS block combines an Omni directional Neighborhood State Space (VONSS) module, which scans features horizontally, vertically and diagonally, with an Adaptive Frequency Modulation Module (AFMM) that applies content adaptive weighting of discrete cosine transform frequency components for more efficient bit allocation. To further reduce redundancy in the entropy model, we integrate AFMM with a Swin Transformer to form a Frequency Swin Transformer Attention Module (FSTAM) for frequency aware side information modeling. Experiments on the Kodak, Tecnick and CLIC Professional Validation datasets show that HCFSSNet achieves competitive rate distortion performance compared with recent SSM based codecs such as MambaIC, while using significantly fewer parameters. On Kodak, Tecnick and CLIC, HCFSSNet reduces BD rate over the VTM anchor by 18.06, 24.56 and 22.44 percent, respectively, providing an efficient and interpretable hybrid architecture for future learned image compression systems.

</details>


### [105] [Restora-Flow: Mask-Guided Image Restoration with Flow Matching](https://arxiv.org/abs/2511.20152)
*Arnela Hadzic,Franz Thaler,Lea Bogensperger,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 提出Restora-Flow：一种无需训练的基于flow matching的图像复原采样引导方法，通过退化掩码与轨迹校正，实现更快且更保真的修复，在自然与医学多任务（修复、超分、去噪）上优于扩散与现有flow方法的感知质量和速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样慢但质量高；flow matching更快且轨迹可控，适合作为复原先验。然而现有基于flow的复原方法仍存在推理时间长或结果过平滑的问题，需要一种既快又保持细节真实度的方案。

Method: 提出Restora-Flow：训练free。1) 基于退化掩码的引导，在采样过程中对受损区域施加更强的先验驱动，未受损区域保持一致性；2) 轨迹校正机制，在生成轨迹的多步进行与退化观测对齐的校正，保证与输入一致；整体在flow matching采样框架中实现，无需额外训练。

Result: 在自然与医学数据集上的掩码型退化任务（图像修复、超分辨、去噪）评测，感知质量优于扩散与现有flow基线，同时推理时间更短。

Conclusion: 结合掩码引导与轨迹校正的训练free flow matching采样能有效提升图像复原质量与效率，显示flow作为复原先验的优势与实用性。

Abstract: Flow matching has emerged as a promising generative approach that addresses the lengthy sampling times associated with state-of-the-art diffusion models and enables a more flexible trajectory design, while maintaining high-quality image generation. This capability makes it suitable as a generative prior for image restoration tasks. Although current methods leveraging flow models have shown promising results in restoration, some still suffer from long processing times or produce over-smoothed results. To address these challenges, we introduce Restora-Flow, a training-free method that guides flow matching sampling by a degradation mask and incorporates a trajectory correction mechanism to enforce consistency with degraded inputs. We evaluate our approach on both natural and medical datasets across several image restoration tasks involving a mask-based degradation, i.e., inpainting, super-resolution and denoising. We show superior perceptual quality and processing time compared to diffusion and flow matching-based reference methods.

</details>


### [106] [Alzheimers Disease Progression Prediction Based on Manifold Mapping of Irregularly Sampled Longitudinal Data](https://arxiv.org/abs/2511.20154)
*Xin Hong,Ying Shi,Yinhao Li,Yen-Wei Chen*

Main category: cs.CV

TL;DR: 提出R‑TNAG框架：将纵向不规则采样的sMRI特征映射到黎曼流形上，用时间感知NODE建模连续演化，再用注意力黎曼GRU融合历史与当前信息，实现对AD进程的稳健预测，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床随访存在不确定性，导致纵向影像观测时间间隔不规则；欧氏空间假设难以刻画此类数据的非线性连续几何结构，限制了对阿尔茨海默病进程的建模与预测。

Method: 1) 高维sMRI特征经流形映射，保留疾病进程的内在几何；2) 在流形表示上引入时间感知Neural ODE（TNODE），对观测间的潜在状态进行连续动力学建模；3) 设计注意力型黎曼GRU（ARGRU），自适应整合历史与当前信息以处理不规则间隔；整体形成R‑TNAG联合框架，提升时间一致性与鲁棒性。

Result: 在疾病状态预测与认知评分回归上持续优于SOTA；消融实验证明各模块均有贡献且互补；在不同序列长度和缺失率下表现稳定，具备强时间泛化；跨数据集验证显示出良好鲁棒性与适用性。

Conclusion: 将纵向sMRI置于黎曼流形并结合时间连续动力学与注意力记忆单元，可有效处理不规则采样，提升AD进程建模与预测的准确性与泛化能力。

Abstract: The uncertainty of clinical examinations frequently leads to irregular observation intervals in longitudinal imaging data, posing challenges for modeling disease progression.Most existing imaging-based disease prediction models operate in Euclidean space, which assumes a flat representation of data and fails to fully capture the intrinsic continuity and nonlinear geometric structure of irregularly sampled longitudinal images. To address the challenge of modeling Alzheimers disease (AD) progression from irregularly sampled longitudinal structural Magnetic Resonance Imaging (sMRI) data, we propose a Riemannian manifold mapping, a Time-aware manifold Neural ordinary differential equation, and an Attention-based riemannian Gated recurrent unit (R-TNAG) framework. Our approach first projects features extracted from high-dimensional sMRI into a manifold space to preserve the intrinsic geometry of disease progression. On this representation, a time-aware Neural Ordinary Differential Equation (TNODE) models the continuous evolution of latent states between observations, while an Attention-based Riemannian Gated Recurrent Unit (ARGRU) adaptively integrates historical and current information to handle irregular intervals. This joint design improves temporal consistency and yields robust AD trajectory prediction under irregular sampling.Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art models in both disease status prediction and cognitive score regression. Ablation studies verify the contributions of each module, highlighting their complementary roles in enhancing predictive accuracy. Moreover, the model exhibits stable performance across varying sequence lengths and missing data rates, indicating strong temporal generalizability. Cross-dataset validation further confirms its robustness and applicability in diverse clinical settings.

</details>


### [107] [Map-World: Masked Action planning and Path-Integral World Model for Autonomous Driving](https://arxiv.org/abs/2511.20156)
*Bin Hu,Zijian Lu,Haicheng Liao,Chengran Yuan,Bin Rao,Yongkang Li,Guofa Li,Zhiyong Cui,Cheng-zhong Xu,Zhenning Li*

Main category: cs.CV

TL;DR: 提出MAP-World：一种无需先验锚点的多模态自动驾驶规划框架，通过掩码动作规划与路径加权世界模型联合训练，在不依赖RL的前提下生成并评估多种未来轨迹，实现SOTA的世界模型规划性能与实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有端到端与世界模型规划虽能预测多模态未来，但通常需手工锚点或RL选择单一路径用于训练与控制，导致丢失其他可行未来信息并使优化复杂化，影响数据效率与鲁棒性。

Method: 1) 掩码动作规划（MAP）：将未来自车轨迹预测视为掩码序列补全；过去航点为可见token，未来航点为mask，借助驾驶意图路径作粗骨架。以紧凑潜在规划状态为起点，注入噪声扩展为多条轨迹查询，获得多样且时序一致的模式，无需锚点库或教师策略。2) 路径加权世界模型：对每个候选轨迹条件生成未来BEV语义；训练时以轨迹概率作为离散路径权重，对语义损失做期望，使学习覆盖整分布而非单一路径。3) 实时、轻量实现，端到端可微，不使用RL。

Result: 在NAVSIM基准上，与锚点法性能相当；在世界模型类方法中达成SOTA，同时保持实时推理延迟。

Conclusion: 通过将规划多样性与世界模型评估耦合，并以路径加权的期望损失训练，MAP-World无需先验与RL即可高效学得和控制多模态规划，兼顾性能与实时性。

Abstract: Motion planning for autonomous driving must handle multiple plausible futures while remaining computationally efficient. Recent end-to-end systems and world-model-based planners predict rich multi-modal trajectories, but typically rely on handcrafted anchors or reinforcement learning to select a single best mode for training and control. This selection discards information about alternative futures and complicates optimization. We propose MAP-World, a prior-free multi-modal planning framework that couples masked action planning with a path-weighted world model. The Masked Action Planning (MAP) module treats future ego motion as masked sequence completion: past waypoints are encoded as visible tokens, future waypoints are represented as mask tokens, and a driving-intent path provides a coarse scaffold. A compact latent planning state is expanded into multiple trajectory queries with injected noise, yielding diverse, temporally consistent modes without anchor libraries or teacher policies. A lightweight world model then rolls out future BEV semantics conditioned on each candidate trajectory. During training, semantic losses are computed as an expectation over modes, using trajectory probabilities as discrete path weights, so the planner learns from the full distribution of plausible futures instead of a single selected path. On NAVSIM, our method matches anchor-based approaches and achieves state-of-the-art performance among world-model-based methods, while avoiding reinforcement learning and maintaining real-time inference latency.

</details>


### [108] [SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery](https://arxiv.org/abs/2511.20157)
*Da Li,Ji-Ping Jin,Xuanlong Yu,Wei Liu,Xiaodong Cun,Kai Chen,Rui Fan,Jiangang Kong,Shen Xi*

Main category: cs.CV

TL;DR: 提出SKEL-CF：一个用于解算解剖精准骨架模型SKEL参数的粗到细框架，通过Transformer先粗预测再逐层细化，并显式建模相机以缓解深度/尺度歧义；将4DHuman转为SKEL对齐数据集4DHuman-SKEL用于监督；在MOYO上显著超越前SOTA（MPJPE 85.0 vs 104.5，PA-MPJPE 51.4 vs 79.6）。


<details>
  <summary>Details</summary>
Motivation: SMPL类参数化人体模型推动了姿态/形状估计，但其简化的运动学限制了生物力学真实度。新模型SKEL具解剖准确骨架，但其参数估计受限于训练数据稀缺、透视歧义和人体关节复杂性，现有方法效果有限，需一种既可扩展又解剖一致的估计框架。

Method: 提出SKEL-CF：Transformer编码器-解码器。编码器预测粗相机与SKEL参数，解码器层间逐步细化；显式相机建模以缓解深度/尺度歧义并适配多视角；将SMPL标注数据集4DHuman转换为与SKEL对齐的4DHuman-SKEL以提供解剖一致监督；在训练与评测中使用逐层细化和相机融合策略。

Result: 在MOYO数据集上达到MPJPE 85.0、PA-MPJPE 51.4，显著优于SKEL基线HSMR（104.5、79.6）；跨多视角实验显示显式相机建模的重要性；广泛实验验证设计有效。

Conclusion: SKEL-CF实现对解剖精准骨架SKEL的可扩展、粗到细参数估计，结合新构建的4DHuman-SKEL数据与显式相机建模，提升准确度并增强生物力学可信度，推动计算机视觉与生物力学融合的人体运动分析。

Abstract: Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.

</details>


### [109] [Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs](https://arxiv.org/abs/2511.20158)
*Ziqi Wang,Chang Che,Qi Wang,Hui Ma,Zenglin Shi,Cees G. M. Snoek,Meng Wang*

Main category: cs.CV

TL;DR: 提出HPA用于安全对齐的MLLM在持续视觉指令微调中同时维持任务能力与安全性，显著减缓遗忘与安全退化。


<details>
  <summary>Details</summary>
Motivation: 现有CVIT研究多在未安全对齐的MLLM上进行，忽视了真实场景中对安全性的刚需；在安全对齐模型上持续适配时，既有任务遗忘也有安全性下降，如何兼顾二者是关键难题。

Method: 提出Harmonious Parameter Adaptation (HPA) 后训练框架，包含：1) 基于“关注度”的参数划分，将参数按偏向安全或偏向任务性能进行分类；2) 和谐平衡的参数选择，从平衡视角选择需重点保留的聚焦参数以保护相应能力/安全；3) 正交参数调整，对参数更新施加正交性约束，减少对已学知识的干扰以缓解灾难性遗忘。

Result: 在CVIT基准与安全评测数据集上，HPA较现有方法更好地维持高安全性并降低遗忘，整体表现优于各类基线。

Conclusion: HPA为安全对齐MLLM的持续视觉指令调优提供了兼顾安全与任务表现的有效方案，通过参数划分、平衡选择与正交更新，显著提升了持续学习中的安全保持与抗遗忘能力。

Abstract: While continual visual instruction tuning (CVIT) has shown promise in adapting multimodal large language models (MLLMs), existing studies predominantly focus on models without safety alignment. This critical oversight ignores the fact that real-world MLLMs inherently require such mechanisms to mitigate potential risks. In this work, we shift our focus to CVIT for safety-aligned MLLMs and observe that during continual adaptation, the model not only suffers from task forgetting but also exhibits degradation in its safety. Achieving a harmonious balance between safety and task performance remains a crucial challenge. To address this, we propose Harmonious Parameter Adaptation (HPA), a post-training framework composed of focusing-based parameter partition, harmoniously balanced parameter selection, and orthogonal parameter adjustment. Specifically, HPA partitions parameters into two types based on their focus on safety or task performance, and selects the focused ones to preserve from a balanced perspective. In addition, HPA imposes orthogonality constraints on parameter updates to further alleviate catastrophic forgetting. Extensive experiments on the CVIT benchmark and safety evaluation datasets demonstrate that HPA better maintains high safety and mitigates forgetting than existing baselines.

</details>


### [110] [While recognizing actions, LMMs struggle to detect core interaction events](https://arxiv.org/abs/2511.20162)
*Daniel Harari,Michael Sidorov,Liel David,Chen Shterental,Abrham Kahsay Gebreselasie,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 研究评估大规模多模态模型在视频互动事件中的“接触/释放”时空定位能力，发现能识物述事却难以精准标定互动开始/结束帧与位置，显示感知落地不足。


<details>
  <summary>Details</summary>
Motivation: 虽然LMM在图像/视频理解上生成描述与推理强，但不清楚其语义是否真正扎根于视觉输入，尤其对动态交互中决定性瞬间（手与物体的接触/分离）的时空把握能力。

Method: 构建首个大规模‘接触/释放’事件标注数据集：基于Something-Something-V2，超过2万条互动标注，由约250名AMTurk标注员标记手-物体附着（contact）与分离（release）的时间与位置。选取短视频（每段包含单一事件），让两种LMM（Qwen-2.5VL、GPT-4o）定位这些事件发生的帧与场景位置，并评估其对象识别、动作识别与定位能力。

Result: 模型能稳定命名目标对象、识别动作并给出连贯解释，但在确定互动开始/结束的精确帧与在画面中定位事件上持续失败。

Conclusion: 当前LMM缺乏对动态场景中物理接触瞬间的精确感知与时空定位能力，语义理解未充分与视觉输入对齐，需要更强的感知落地与时空对齐机制。

Abstract: Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.

</details>


### [111] [ADNet: A Large-Scale and Extensible Multi-Domain Benchmark for Anomaly Detection Across 380 Real-World Categories](https://arxiv.org/abs/2511.20169)
*Hai Ling,Jia Guo,Zhulin Tao,Yunkang Cao,Donglin Di,Hongyan Xu,Xiu Su,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: 提出ADNet，一个涵盖380类、49数据集的大规模多域异常检测基准，并提出Dinomaly-m以应对多类可扩展性问题，显著提升多类场景I-AUROC与P-AUROC。


<details>
  <summary>Details</summary>
Motivation: 现有AD基准（如MVTec-AD仅15类）类别与场景受限，难以评估跨领域泛化与大规模多类设置下的可扩展性；缺乏统一标注与多模态信息以推进更通用的AD研究。

Method: 1) 构建ADNet：聚合电子、工业、农食、基础设施、医疗5大域49数据集共380类，标准化为MVTec式像素级标注与结构化文本描述，支持多模态任务；2) 系统评测SOTA方法在单类与多类设置下的表现，揭示可扩展性缺口；3) 提出Dinomaly-m：在Dinomaly基础上引入上下文引导的Mixture-of-Experts解码器，扩展解码能力但不增加推理成本。

Result: ADNet包含196,294张RGB图像（116,192正常训练，80,102测试，其中60,311异常）。SOTA在one-for-one达90.6% I-AUROC，但在380类多类设置降至78.5%。Dinomaly-m在大规模多类上达83.2% I-AUROC与93.1% P-AUROC，优于现有方法。

Conclusion: ADNet为标准化、可扩展的大规模多域AD基准，促进跨领域与多模态研究，并为面向基础模型的AD提供数据基础；Dinomaly-m有效缓解多类可扩展性瓶颈，推理成本不增而性能提升。

Abstract: Anomaly detection (AD) aims to identify defects using normal-only training data. Existing anomaly detection benchmarks (e.g., MVTec-AD with 15 categories) cover only a narrow range of categories, limiting the evaluation of cross-context generalization and scalability. We introduce ADNet, a large-scale, multi-domain benchmark comprising 380 categories aggregated from 49 publicly available datasets across Electronics, Industry, Agrifood, Infrastructure, and Medical domains. The benchmark includes a total of 196,294 RGB images, consisting of 116,192 normal samples for training and 80,102 test images, of which 60,311 are anomalous. All images are standardized with MVTec-style pixel-level annotations and structured text descriptions spanning both spatial and visual attributes, enabling multimodal anomaly detection tasks. Extensive experiments reveal a clear scalability challenge: existing state-of-the-art methods achieve 90.6% I-AUROC in one-for-one settings but drop to 78.5% when scaling to all 380 categories in a multi-class setting. To address this, we propose Dinomaly-m, a context-guided Mixture-of-Experts extension of Dinomaly that expands decoder capacity without increasing inference cost. It achieves 83.2% I-AUROC and 93.1% P-AUROC, demonstrating superior performance over existing approaches. ADNet is designed as a standardized and extensible benchmark, supporting the community in expanding anomaly detection datasets across diverse domains and providing a scalable foundation for future anomaly detection foundation models. Dataset: https://grainnet.github.io/ADNet

</details>


### [112] [Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware](https://arxiv.org/abs/2511.20175)
*Federico Paredes-Valles,Yoshitaka Miyatani,Kirk Y. W. Scheper*

Main category: cs.CV

TL;DR: 提出首个电池供电、全设备端集成的可穿戴事件相机+神经形态眼动追踪系统，在<5 mW/眼功耗下实现100 Hz双目瞳孔中心定位。


<details>
  <summary>Details</summary>
Motivation: 可穿戴平台需要高频、鲁棒且超低功耗的眼动追踪；事件相机具备微秒级和稀疏数据优势，但缺乏低功耗、实时推理的全栈集成处理方案。

Method: 在Speck2f SoC上实现事件相机与神经形态推理端到端集成：设计带不确定性量化与门控时序解码的脉冲神经网络（SNN），在严格存储/带宽约束下优化；在超低功耗MCU上进行轻量级坐标解码；配套系统化部署机制以弥合仿真-实物“现实鸿沟”。

Result: 构建多用户数据集并进行验证；制作可穿戴原型，双神经形态设备实现双目瞳孔跟踪，频率100 Hz，平均功耗<5 mW/眼，并表现出鲁棒性。

Conclusion: 端到端神经形态计算与事件视觉的结合可实现实用、常开式、能效优异的可穿戴眼动追踪，为下一代低功耗可穿戴系统奠定基础。

Abstract: Eye tracking is fundamental to numerous applications, yet achieving robust, high-frequency tracking with ultra-low power consumption remains challenging for wearable platforms. While event-based vision sensors offer microsecond resolution and sparse data streams, they have lacked fully integrated, low-power processing solutions capable of real-time inference. In this work, we present the first battery-powered, wearable pupil-center-tracking system with complete on-device integration, combining event-based sensing and neuromorphic processing on the commercially available Speck2f system-on-chip with lightweight coordinate decoding on a low-power microcontroller. Our solution features a novel uncertainty-quantifying spiking neural network with gated temporal decoding, optimized for strict memory and bandwidth constraints, complemented by systematic deployment mechanisms that bridge the reality gap. We validate our system on a new multi-user dataset and demonstrate a wearable prototype with dual neuromorphic devices achieving robust binocular pupil tracking at 100 Hz with an average power consumption below 5 mW per eye. Our work demonstrates that end-to-end neuromorphic computing enables practical, always-on eye tracking for next-generation energy-efficient wearable systems.

</details>


### [113] [Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis](https://arxiv.org/abs/2511.20186)
*Mohammad Mahdi,Yuqian Fu,Nedko Savov,Jiancheng Pan,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出Exo2EgoSyn，在WAN 2.2基础上实现从外视角到第一视角的跨视角视频生成，通过对齐潜空间、聚合多外视角条件与注入相机姿态，实现无需从零重训的高保真Egocentric视频合成，并在ExoEgo4D上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有基础视频生成模型虽在文本/图像条件下效果强，但多局限于同视角生成，难以从第三人称外视角生成第一人称视频；现实应用（AR/机器人/运动理解）需要跨视角合成能力且希望复用现有强基座。

Method: 在WAN 2.2上加入三模块：1）EgoExo-Align：对齐外视角与自我视角首帧潜表示，重定向生成空间至ego视角；2）MultiExoCon：将多外视角视频特征聚合成统一条件，扩展WAN 2.2从单图/文本到多视频条件；3）PoseInj：向潜状态注入外到内相机相对姿态，提供几何感知引导。整体实现无需从头训练，只做适配与微调。

Result: 在ExoEgo4D数据集上，方法显著提升Exo→Ego（文中一句可能写作Ego2Exo但上下文应为Exo2Ego）生成质量与一致性，能高保真地从第三人称观测生成第一人称视频。

Conclusion: Exo2EgoSyn使基础视频模型具备可扩展的跨视角生成能力，实证有效并将开源代码与模型，为更广泛的多视角视频合成铺路。

Abstract: Foundation video generation models such as WAN 2.2 exhibit strong text- and image-conditioned synthesis abilities but remain constrained to the same-view generation setting. In this work, we introduce Exo2EgoSyn, an adaptation of WAN 2.2 that unlocks Exocentric-to-Egocentric(Exo2Ego) cross-view video synthesis. Our framework consists of three key modules. Ego-Exo View Alignment(EgoExo-Align) enforces latent-space alignment between exocentric and egocentric first-frame representations, reorienting the generative space from the given exo view toward the ego view. Multi-view Exocentric Video Conditioning (MultiExoCon) aggregates multi-view exocentric videos into a unified conditioning signal, extending WAN2.2 beyond its vanilla single-image or text conditioning. Furthermore, Pose-Aware Latent Injection (PoseInj) injects relative exo-to-ego camera pose information into the latent state, guiding geometry-aware synthesis across viewpoints. Together, these modules enable high-fidelity ego view video generation from third-person observations without retraining from scratch. Experiments on ExoEgo4D validate that Exo2EgoSyn significantly improves Ego2Exo synthesis, paving the way for scalable cross-view video generation with foundation models. Source code and models will be released publicly.

</details>


### [114] [SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA](https://arxiv.org/abs/2511.20190)
*Haibin He,Qihuang Zhong,Juhua Liu,Bo Du,Peng Wang,Jing Zhang*

Main category: cs.CV

TL;DR: 提出SFA：一种无需训练的、面向视频文本VQA的Video-LLM框架，通过自适应扫描帧、聚焦关键区域并放大，强化与问题相关的文字线索，显著提升多数据集性能并创SOTA。


<details>
  <summary>Details</summary>
Motivation: 视频中文本往往尺度、方向、清晰度变化大，且需跨时间与语义整合；问题相关线索稀疏而噪声多。现有方法难以稳健识别并聚焦最相关的场景文字，视频LLM尚缺少针对TextVQA的机制。

Method: 提出SFA（Scan–Focus–Amplify）：训练无关的流程，用Video-LLM为核心。先自适应扫描视频帧，选择疑似含关键信息的帧与区域；再有选择地聚焦问题相关区域；最后直接对这些区域进行放大/强化，以引导Video-LLM注意关键文本与语境，从而生成答案。

Result: 在多个公开Video TextVQA数据集上取得新的SOTA，较以往方法有显著优势，显示出强泛化能力。

Conclusion: 训练免调的SFA通过帧级自适应策略与区域放大，能有效引导Video-LLM抓住关键文字线索，显著提升TextVQA表现，并具备通用性与实用性。

Abstract: Video text-based visual question answering (Video TextVQA) task aims to answer questions about videos by leveraging the visual text appearing within the videos. This task poses significant challenges, requiring models to accurately perceive and comprehend scene text that varies in scale, orientation, and clarity across frames, while effectively integrating temporal and semantic context to generate precise answers. Moreover, the model must identify question-relevant textual cues and filter out redundant or irrelevant information to ensure answering is guided by the most relevant and informative cues. To address these challenges, we propose SFA, a training-free framework and the first Video-LLM-based method tailored for Video TextVQA, motivated by the human process of answering questions. By adaptively scanning video frames, selectively focusing on key regions, and directly amplifying them, SFA effectively guides the Video-LLM's attention toward essential cues, enabling it to generate more accurate answers. SFA achieves new state-of-the-art results across several public Video TextVQA datasets and surpasses previous methods by a substantial margin, demonstrating its effectiveness and generalizability.

</details>


### [115] [GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering](https://arxiv.org/abs/2511.20201)
*Dionysia Danai Brilli,Dimitrios Mallis,Vassilis Pitsikalis,Petros Maragos*

Main category: cs.CV

TL;DR: 提出GHR-VQA：以人节点为根、结合场景图与层次推理的Video QA框架，在AGQA上显著提升，物体-关系推理较SOTA提升7.3%。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答多依赖像素级特征、难以显式建模跨帧的人-物体交互与时空关系，导致可解释性与复杂关系推理能力不足。

Method: 1) 将每帧视频构建为场景图（人、物体、关系）。2) 跨帧将所有人节点连接到全局根，形成以人为中心的视频级图，实现跨帧对齐与聚合。3) 以图神经网络(GNN)对视频级图编码，得到上下文感知的节点/图嵌入。4) 将图嵌入与问题特征在层次化网络中融合（局部到全局多粒度），进行答案预测。

Result: 在AGQA数据集上整体性能显著提升，尤其在物体-关系推理维度较当前最优方法提升7.3%。

Conclusion: 以人为中心的图引导层次推理能够显式分解人-物体交互、提升时空理解与可解释性，并带来对Video QA的实证增益。

Abstract: We propose GHR-VQA, Graph-guided Hierarchical Relational Reasoning for Video Question Answering (Video QA), a novel human-centric framework that incorporates scene graphs to capture intricate human-object interactions within video sequences. Unlike traditional pixel-based methods, each frame is represented as a scene graph and human nodes across frames are linked to a global root, forming the video-level graph and enabling cross-frame reasoning centered on human actors. The video-level graphs are then processed by Graph Neural Networks (GNNs), transforming them into rich, context-aware embeddings for efficient processing. Finally, these embeddings are integrated with question features in a hierarchical network operating across different abstraction levels, enhancing both local and global understanding of video content. This explicit human-rooted structure enhances interpretability by decomposing actions into human-object interactions and enables a more profound understanding of spatiotemporal dynamics. We validate our approach on the Action Genome Question Answering (AGQA) dataset, achieving significant performance improvements, including a 7.3% improvement in object-relation reasoning over the state of the art.

</details>


### [116] [Robust 3D Brain MRI Inpainting with Random Masking Augmentation](https://arxiv.org/abs/2511.20202)
*Juexin Zhang,Ying Weng,Ke Chen*

Main category: cs.CV

TL;DR: 提出一种用于脑肿瘤MRI健康组织合成（修补）的3D深度学习框架，在BraTS-Inpainting 2025挑战中夺冠，全面超越历年获胜方案。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤MRI数据集存在偏差，限制了深度学习在量化分析中的泛化与可靠性；通过健康组织合成（inpainting）可缓解偏差、提升模型稳健性与临床适用性。

Method: 基于3D U-Net的修补网络，对合成破坏区域进行重建；引入随机遮挡（random masking）增强策略以提升泛化；以SSIM、PSNR、MSE/RMSE等指标在验证集与在线测试集评估。

Result: 验证集表现：SSIM 0.873±0.004，PSNR 24.996±4.694，MSE 0.005±0.087；在线测试集表现：SSIM 0.919±0.088，PSNR 26.932±5.057，RMSE 0.052±0.026。

Conclusion: 方法在BraTS-Inpainting 2025挑战中获第一，性能优于2023与2024冠军方案，证明所提框架在3D脑MRI健康组织合成任务中的有效性与先进性。

Abstract: The ASNR-MICCAI BraTS-Inpainting Challenge was established to mitigate dataset biases that limit deep learning models in the quantitative analysis of brain tumor MRI. This paper details our submission to the 2025 challenge, a novel deep learning framework for synthesizing healthy tissue in 3D scans. The core of our method is a U-Net architecture trained to inpaint synthetically corrupted regions, enhanced with a random masking augmentation strategy to improve generalization. Quantitative evaluation confirmed the efficacy of our approach, yielding an SSIM of 0.873$\pm$0.004, a PSNR of 24.996$\pm$4.694, and an MSE of 0.005$\pm$0.087 on the validation set. On the final online test set, our method achieved an SSIM of 0.919$\pm$0.088, a PSNR of 26.932$\pm$5.057, and an RMSE of 0.052$\pm$0.026. This performance secured first place in the BraTS-Inpainting 2025 challenge and surpassed the winning solutions from the 2023 and 2024 competitions on the official leaderboard.

</details>


### [117] [OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation](https://arxiv.org/abs/2511.20211)
*Hao Yu,Jiabo Zhan,Zile Wang,Jinglin Wang,Huaisong Zhang,Hongyu Li,Xinrui Chen,Yongxian Wei,Chun Yuan*

Main category: cs.CV

TL;DR: 提出OmniAlpha：首个面向RGBA序列到序列生成与编辑的统一多任务生成框架，结合新RoPE变体MSRoPE-BiL与新数据集AlphaLayers，在21项任务上优于专用基线，显著提升无掩码抠图与分层补全等能力。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要直接处理RGBA（包含alpha透明度）图像的生成与编辑，但现有要么是只处理单一任务的专用模型（能用alpha但缺乏通用性），要么是多任务的统一框架却局限于RGB，导致难以跨任务共享表示并在多层场景中高质量操作。

Method: 提出OmniAlpha统一多任务框架，基于Diffusion Transformer (DiT) 主干，并引入MSRoPE-BiL：一种具有双向可扩展“层轴”的改进旋转位置编码，使模型能并行处理多个输入/目标RGBA层；同时构建AlphaLayers数据集（1000个高质量多层三元组），通过自动合成与筛选流水线获得；在该数据集上对21种多样任务进行联合训练与评测。

Result: 在广泛实验中，统一模型持续优于强力专用基线：在AIM-500上无掩码抠图SAD相对降低84.8%；在层条件补全任务中获得超过90%的人工偏好率；在其余多任务上也取得稳定领先。

Conclusion: 统一的多任务生成模型能够学习到更优的RGBA共享表示，相较专用模型在多种分层编辑/生成任务上更稳健、更强大，为面向图层感知的下一代生成系统奠定基础。

Abstract: Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.

</details>


### [118] [Text-guided Controllable Diffusion for Realistic Camouflage Images Generation](https://arxiv.org/abs/2511.20218)
*Yuhang Qian,Haiyan Chen,Wentong Li,Ningzhong Liu,Jie Qin*

Main category: cs.CV

TL;DR: 提出CT-CIG：一种可控文本引导的伪装图像生成方法，通过对话式标注、可控扩散与频域细化，生成语义合理、视觉自然的伪装图像，并在CLIPScore与伪装有效性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有伪装图像生成方法要么把前景物体融合到特定背景，要么基于物体引导的扩散进行背景外延，但忽视了伪装目标与环境之间的逻辑关系，导致结果不自然、不可信。需要一种能显式建模语义逻辑与形态位置、并提升纹理细节的生成框架。

Method: 1) 构建Camouflage-Revealing Dialogue Mechanism（CRDM）：利用大型视觉语言模型为现有伪装数据集生成高质量文本提示，强调目标与环境的语义逻辑；2) 用图文对微调Stable Diffusion，并加入轻量级控制器，约束伪装目标的位置与形状以提升场景契合度；3) 设计Frequency Interaction Refinement Module（FIRM），捕获高频纹理特征以学习复杂伪装纹理与图案。

Result: 在包含CLIPScore的语义一致性评估与伪装有效性测试上，所提CT-CIG生成的提示与图像具备更好语义对齐，生成的伪装图像更具写实性与隐蔽性，优于现有方法。

Conclusion: 通过文本引导的语义逻辑建模、位置形状可控的扩散生成与频域纹理细化，CT-CIG能产生更自然、合理且隐蔽性强的伪装图像，验证了对话式标注与频域交互在伪装生成中的有效性。

Abstract: Camouflage Images Generation (CIG) is an emerging research area that focuses on synthesizing images in which objects are harmoniously blended and exhibit high visual consistency with their surroundings. Existing methods perform CIG by either fusing objects into specific backgrounds or outpainting the surroundings via foreground object-guided diffusion. However, they often fail to obtain natural results because they overlook the logical relationship between camouflaged objects and background environments. To address this issue, we propose CT-CIG, a Controllable Text-guided Camouflage Images Generation method that produces realistic and logically plausible camouflage images. Leveraging Large Visual Language Models (VLM), we design a Camouflage-Revealing Dialogue Mechanism (CRDM) to annotate existing camouflage datasets with high-quality text prompts. Subsequently, the constructed image-prompt pairs are utilized to finetune Stable Diffusion, incorporating a lightweight controller to guide the location and shape of camouflaged objects for enhanced camouflage scene fitness. Moreover, we design a Frequency Interaction Refinement Module (FIRM) to capture high-frequency texture features, facilitating the learning of complex camouflage patterns. Extensive experiments, including CLIPScore evaluation and camouflage effectiveness assessment, demonstrate the semantic alignment of our generated text prompts and CT-CIG's ability to produce photorealistic camouflage images.

</details>


### [119] [Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder](https://arxiv.org/abs/2511.20221)
*Juexin Zhang,Qifeng Zhong,Ying Weng,Ke Chen*

Main category: cs.CV

TL;DR: 针对BraTS-Path 2025挑战，作者微调预训练ViT用于全切片病理图像分类，在验证集取得MCC 0.7064/F1 0.7676，测试集MCC 0.6509/F1 0.5330，获第二名，提供ViT在病理分析的基线并指出需缩小泛化差距。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤具有显著分子与病理异质性，传统组织病理评估主观、费时且难以稳定分层；深度学习可为全切片图像提供客观自动化分析。作者希望构建强健的基线方法并参与挑战以量化其有效性。

Method: 采用预训练Vision Transformer作为编码器，在官方训练集上微调，并加上专用分类头进行端到端训练；在Synapse在线验证平台评测模型表现，并在最终测试集提交结果进行排名比较。

Result: 在线验证集：MCC 0.7064，F1 0.7676；最终测试集：MCC 0.6509，F1 0.5330；团队在BraTS-Pathology 2025挑战中获得第二名。

Conclusion: ViT微调对病理全切片分析有效，提供了稳健基线；但在未见数据上存在性能下滑，未来需改进泛化（如数据分布差异处理、正则化与更强数据增强等）。

Abstract: The significant molecular and pathological heterogeneity of glioblastoma, an aggressive brain tumor, complicates diagnosis and patient stratification. While traditional histopathological assessment remains the standard, deep learning offers a promising path toward objective and automated analysis of whole slide images. For the BraTS-Path 2025 Challenge, we developed a method that fine-tunes a pre-trained Vision Transformer (ViT) encoder with a dedicated classification head on the official training dataset. Our model's performance on the online validation set, evaluated via the Synapse platform, yielded a Matthews Correlation Coefficient (MCC) of 0.7064 and an F1-score of 0.7676. On the final test set, the model achieved an MCC of 0.6509 and an F1-score of 0.5330, which secured our team second place in the BraTS-Pathology 2025 Challenge. Our results establish a solid baseline for ViT-based histopathological analysis, and future efforts will focus on bridging the performance gap observed on the unseen validation data.

</details>


### [120] [V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs](https://arxiv.org/abs/2511.20223)
*Sen Nie,Jie Zhang,Jianxin Yan,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出V-Attack：在LVLM中直接操纵Transformer注意力块的Value特征，实现可控的局部语义攻击，较SOTA平均提升攻击成功率36%。


<details>
  <summary>Details</summary>
Motivation: 现有对LVLM的对抗攻击多在补丁/Token层面操作，因自注意力引入的全局上下文导致语义纠缠，难以对图像中具体概念进行精确、可控的语义操纵。需要一种能解耦局部语义并精确定位与修改目标概念的方法。

Method: 关键观察：注意力块中的Value特征(V)相较于patch特征受全局上下文抑制更强，保留高熵、解耦的局部语义，更适合作为操纵“手柄”。据此提出V-Attack，包括：1) Self-Value Enhancement，增强V的语义丰富度与可操控性；2) Text-Guided Value Manipulation，利用文本提示定位源概念并将其优化/迁移至目标概念。整体绕过纠缠的patch特征，直接在V上进行梯度驱动的局部语义编辑与攻击。

Result: 在LLaVA、InternVL、DeepSeek-VL、GPT-4o等多种LVLM上，V-Attack在语义可控攻击任务中较现有SOTA平均提升约36%的成功率，表现为更精确的概念替换/操纵与更稳定的跨模型迁移。

Conclusion: Value特征是实现精准局部语义操纵的有效入口。V-Attack通过增强与文本引导的V操作，显著提升对LVLM的可控语义攻击能力，揭示当前视觉-语言理解在局部语义鲁棒性与防御机制上的关键脆弱性。

Abstract: Adversarial attacks have evolved from simply disrupting predictions on conventional task-specific models to the more complex goal of manipulating image semantics on Large Vision-Language Models (LVLMs). However, existing methods struggle with controllability and fail to precisely manipulate the semantics of specific concepts in the image. We attribute this limitation to semantic entanglement in the patch-token representations on which adversarial attacks typically operate: global context aggregated by self-attention in the vision encoder dominates individual patch features, making them unreliable handles for precise local semantic manipulation. Our systematic investigation reveals a key insight: value features (V) computed within the transformer attention block serve as much more precise handles for manipulation. We show that V suppresses global-context channels, allowing it to retain high-entropy, disentangled local semantic information. Building on this discovery, we propose V-Attack, a novel method designed for precise local semantic attacks. V-Attack targets the value features and introduces two core components: (1) a Self-Value Enhancement module to refine V's intrinsic semantic richness, and (2) a Text-Guided Value Manipulation module that leverages text prompts to locate source concept and optimize it toward a target concept. By bypassing the entangled patch features, V-Attack achieves highly effective semantic control. Extensive experiments across diverse LVLMs, including LLaVA, InternVL, DeepseekVL and GPT-4o, show that V-Attack improves the attack success rate by an average of 36% over state-of-the-art methods, exposing critical vulnerabilities in modern visual-language understanding. Our code and data are available https://github.com/Summu77/V-Attack.

</details>


### [121] [HistoSpeckle-Net: Mutual Information-Guided Deep Learning for high-fidelity reconstruction of complex OrganAMNIST images via perturbed Multimode Fibers](https://arxiv.org/abs/2511.20245)
*Jawaria Maqbool,M. Imran Cheema*

Main category: cs.CV

TL;DR: 提出HistoSpeckle-Net，通过直方图互信息与多尺度SSIM联合约束，从MMF散斑重建复杂医学图像（OrganAMNIST），在小样本与弯曲扰动下优于U-Net、Pix2Pix。


<details>
  <summary>Details</summary>
Motivation: 现有MMF成像深度学习方法多基于简单数据集、依赖海量数据，难以应对复杂真实医学图像，且忽略散斑与重建图像的统计分布特性，限制临床可用性。

Method: 1) 搭建SLM→MMF的光学系统，将OrganAMNIST图像经SLM调制注入MMF并采集输出散斑，构建临床相关数据集；2) 设计HistoSpeckle-Net网络，包含：a) 直方图计算单元，估计平滑边缘与联合直方图用于互信息损失（分布感知）；b) 三尺度特征细化模块，支持多尺度SSIM损失（结构保持）；3) 以互信息损失+多尺度SSIM联合训练，提升统计对齐与结构保真。

Result: 在复杂的OrganAMNIST任务上，相比U-Net、Pix2Pix取得更高重建保真度；在训练样本受限及光纤弯曲条件变化下仍保持领先表现。

Conclusion: 分布感知的互信息损失与多尺度结构约束可在小样本与扰动下实现对复杂解剖结构的高保真MMF散斑重建，推动MMF成像向临床实际部署迈进。

Abstract: Existing deep learning methods in multimode fiber (MMF) imaging often focus on simpler datasets, limiting their applicability to complex, real-world imaging tasks. These models are typically data-intensive, a challenge that becomes more pronounced when dealing with diverse and complex images. In this work, we propose HistoSpeckle-Net, a deep learning architecture designed to reconstruct structurally rich medical images from MMF speckles. To build a clinically relevant dataset, we develop an optical setup that couples laser light through a spatial light modulator (SLM) into an MMF, capturing output speckle patterns corresponding to input OrganAMNIST images. Unlike previous MMF imaging approaches, which have not considered the underlying statistics of speckles and reconstructed images, we introduce a distribution-aware learning strategy. We employ a histogram-based mutual information loss to enhance model robustness and reduce reliance on large datasets. Our model includes a histogram computation unit that estimates smooth marginal and joint histograms for calculating mutual information loss. It also incorporates a unique Three-Scale Feature Refinement Module, which leads to multiscale Structural Similarity Index Measure (SSIM) loss computation. Together, these two loss functions enhance both the structural fidelity and statistical alignment of the reconstructed images. Our experiments on the complex OrganAMNIST dataset demonstrate that HistoSpeckle-Net achieves higher fidelity than baseline models such as U-Net and Pix2Pix. It gives superior performance even with limited training samples and across varying fiber bending conditions. By effectively reconstructing complex anatomical features with reduced data and under fiber perturbations, HistoSpeckle-Net brings MMF imaging closer to practical deployment in real-world clinical environments.

</details>


### [122] [Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation](https://arxiv.org/abs/2511.20250)
*Daniel Kienzle,Katja Ludwig,Julian Lorenz,Shin'ichi Satoh,Rainer Lienhart*

Main category: cs.CV

TL;DR: 提出一个两阶段管线：前端用真实2D监督做检测/关键点，后端用物理一致的合成数据把2D轨迹提升为3D并估计旋转，并对缺检与变帧率鲁棒，最终实现实用的端到端乒乓球3D轨迹与旋转分析。


<details>
  <summary>Details</summary>
Motivation: 现实单目视频中精确恢复乒乓球3D轨迹与旋转很难：真实数据缺少3D与旋转标注，且合成训练的方法在面对真实检测噪声（漏检、表检测不准、帧率不稳）时泛化差。

Method: 将问题拆为前端感知与后端2D→3D提升。前端集成球检测器与球台关键点检测器，并用新建的TTHQ数据集进行充足2D监督训练；后端仅用物理正确的合成数据训练改造后的提升网络，使其对缺失检测与帧率变化鲁棒，从2D观测恢复3D轨迹与球旋转。

Result: 把原本概念验证级的提升方法变为在真实视频上鲁棒的高性能端到端系统，能够在存在噪声、漏检与变帧率的情况下稳定估计3D轨迹与旋转。

Conclusion: 通过前后端解耦与跨域训练（真实2D监督+合成物理监督），实现了在实际场景中可用的乒乓球3D轨迹与旋转估计，并显著提升对现实噪声的鲁棒性与实用性。

Abstract: Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.

</details>


### [123] [PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling](https://arxiv.org/abs/2511.20251)
*Bo-Kai Ruan,Teng-Fang Hsiao,Ling Lo,Yi-Lun Wu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 论文发现长提示会降低生成多样性，提出评测基准LPD-Bench与训练无关的方法PromptMoG，通过在提示嵌入空间的高斯混合采样提升多样性且保持语义一致。


<details>
  <summary>Details</summary>
Motivation: 长提示包含丰富内容/空间/风格信息可提升保真度，但常导致多样性下降与重复输出；现有T2I模型在长提示下的行为缺乏系统研究与统一评测。

Method: 1) 实证分析长提示长度对主流整流流模型的多样性影响；2) 构建同时衡量保真与多样性的长提示评测基准LPD-Bench；3) 提出理论框架，以提示重构提升采样熵；4) 训练无关方法PromptMoG：在提示嵌入空间用高斯混合模型采样多组嵌入，进行生成以增加多样性，同时控制语义漂移。

Result: 在SD3.5-Large、Flux.1-Krea-Dev、CogView4、Qwen-Image上，PromptMoG在长提示设置下显著、稳定地提升生成多样性指标，同时维持语义一致与保真，验证了方法有效性。

Conclusion: 长提示导致的保真-多样性权衡可通过在嵌入空间的MoG采样缓解；LPD-Bench为该问题提供统一评测；PromptMoG无需训练即可广泛适配主流T2I模型，提升多样性而不牺牲语义。

Abstract: Recent advances in text-to-image (T2I) generation have achieved remarkable visual outcomes through large-scale rectified flow models. However, how these models behave under long prompts remains underexplored. Long prompts encode rich content, spatial, and stylistic information that enhances fidelity but often suppresses diversity, leading to repetitive and less creative outputs. In this work, we systematically study this fidelity-diversity dilemma and reveal that state-of-the-art models exhibit a clear drop in diversity as prompt length increases. To enable consistent evaluation, we introduce LPD-Bench, a benchmark designed for assessing both fidelity and diversity in long-prompt generation. Building on our analysis, we develop a theoretical framework that increases sampling entropy through prompt reformulation and propose a training-free method, PromptMoG, which samples prompt embeddings from a Mixture-of-Gaussians in the embedding space to enhance diversity while preserving semantics. Extensive experiments on four state-of-the-art models, SD3.5-Large, Flux.1-Krea-Dev, CogView4, and Qwen-Image, demonstrate that PromptMoG consistently improves long-prompt generation diversity without semantic drifting.

</details>


### [124] [Zoo3D: Zero-Shot 3D Object Detection at Scene Level](https://arxiv.org/abs/2511.20253)
*Andrey Lemeshko,Bulat Gabdullin,Nikita Drozdov,Anton Konushin,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: Zoo3D提出首个无需训练的开放词汇3D目标检测框架，通过2D实例掩膜聚合生成3D框和开放词汇语义赋予，在ScanNet200与ARKitScenes上零样本与自监督两种模式均达SOTA，零样本版本甚至超过现有自监督方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中对象多样且常含未见类别，闭集3D检测难以泛化；现有开放词汇3D方法仍依赖训练数据（点云或图像），标注成本与领域迁移受限。因此需要一种无需训练、可直接在新场景上运行且能识别开放词汇的3D检测方案。

Method: 1) 以多视角2D实例掩膜为基础，进行图聚类跨视角关联，重建并生成3D边界框；2) 提出开放词汇语义模块：选择最佳视角（best-view selection），融合多视角一致性（view-consensus）生成掩膜，并用开放词汇分类器（类似CLIP式文本图像匹配）为3D框赋予语义标签；3) 两种运行模式：Zoo3D_0零训练，直接输出3D框与标签；Zoo3D_1自监督，用Zoo3D_0的伪标注训练一个类无关3D检测器以细化3D框；4) 扩展到仅用带位姿或无位姿的图像，也可用于点云。

Result: 在ScanNet200与ARKitScenes基准上，Zoo3D_0与Zoo3D_1均取得开放词汇3D检测SOTA性能，其中零样本Zoo3D_0超越了所有现有自监督方法；同时展示了在点云、带位姿图像与无位姿图像上的通用性与鲁棒性。

Conclusion: 训练免费的开放词汇3D检测可行且强大；通过跨视角聚合与开放词汇语义赋予，Zoo3D在多数据形态与基准上实现SOTA，显示出在真实世界3D理解中的适应性与实用价值。

Abstract: 3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .

</details>


### [125] [XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface](https://arxiv.org/abs/2511.20254)
*Alexander C. Jenke,Gregor Just,Claas de Boer,Martin Wagner,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: 提出一个轻量级CNN管线，在达芬奇Xi内窥镜手术视频中自动定位UI中的相机瓷块并判定其是否激活，跨三数据集达70k帧评测，活跃相机检测F1达到0.993–1.000，定位无误检，支持实时元数据提取以助下游任务。


<details>
  <summary>Details</summary>
Motivation: 内镜手术仅依赖视频视觉反馈。达芬奇Xi系统UI包含各机械臂状态，其中内镜臂（相机）激活提示可提供镜头运动等元数据。自动检测相机激活状态能辅助工具跟踪、技能评估、相机控制等外科数据科学任务，减少人工标注与规则工程。

Method: 基于ResNet18的轻量级CNN管线：在SurgToolLoc数据集上手工标注相机瓷块位置与激活状态，微调模型以同时定位UI中的相机tile并进行二分类激活判断；在三个公开数据集（>70,000帧）上评估。

Result: 二分类激活检测F1=0.993–1.000；相机tile定位全部正确，且无多相机误检。

Conclusion: 该管线可可靠、实时地从手术视频中提取相机激活元数据，支持自动预处理与多种下游应用；代码、模型与标注全部公开。

Abstract: Purpose: Robot-assisted minimally invasive surgery relies on endoscopic video as the sole intraoperative visual feedback. The DaVinci Xi system overlays a graphical user interface (UI) that indicates the state of each robotic arm, including the activation of the endoscope arm. Detecting this activation provides valuable metadata such as camera movement information, which can support downstream surgical data science tasks including tool tracking, skill assessment, or camera control automation.
  Methods: We developed a lightweight pipeline based on a ResNet18 convolutional neural network to automatically identify the position of the camera tile and its activation state within the DaVinci Xi UI. The model was fine-tuned on manually annotated data from the SurgToolLoc dataset and evaluated across three public datasets comprising over 70,000 frames.
  Results: The model achieved F1-scores between 0.993 and 1.000 for the binary detection of active cameras and correctly localized the camera tile in all cases without false multiple-camera detections.
  Conclusion: The proposed pipeline enables reliable, real-time extraction of camera activation metadata from surgical videos, facilitating automated preprocessing and analysis for diverse downstream applications. All code, trained models, and annotations are publicly available.

</details>


### [126] [The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation](https://arxiv.org/abs/2511.20256)
*Weijia Mao,Hao Chen,Zhenheng Yang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出Adv-GRPO：用对抗式奖励与生成器联合迭代，避免奖励黑客并提升图像质量与美学；以参考图+视觉基础模型提供密集视觉奖励，实现质量、审美与特定任务的全面提升与风格迁移，真人评测显著胜出。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好模型的标量奖励难以全面刻画人类感知，易被“奖励黑客”利用，导致高分但差图；同时KL正则限制参数而非视觉效果，且各类手工或单指标奖励（如PickScore、OCR）存在系统性偏差、牺牲美学或质量。

Method: 提出Adv-GRPO：1) 构建对抗式奖励学习框架，奖励模型以参考图为正样本监督并与生成器交替更新；2) 抛弃单一标量奖励，使用参考图与视觉基础模型（如DINO）产生密集视觉信号作为奖励，直接在图像空间指导生成；3) 将该奖励与生成器RL训练结合，实现分布迁移与风格自定义。

Result: 密集视觉奖励在图像质量、美学和任务指标上均获得一致提升；在人工评测中，相较Flow-GRPO与SD3，分别在图像质量与审美上取得70.0%与72.4%的胜率；能实现分布迁移与灵活风格定制。

Conclusion: 对抗式、图像空间的密集视觉奖励能有效抑制奖励黑客，绕过偏好模型与KL正则的局限，稳定提升质量与美学，并支持风格/分布控制；方法通用、可扩展且实证优于现有RL图像生成方案。

Abstract: A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.

</details>


### [127] [Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization](https://arxiv.org/abs/2511.20258)
*Xiaohan Wang,Zhangtao Cheng,Ting Zhong,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: 提出MBCD协作蒸馏框架，解决多模态域泛化中权重平均（WA）对快收敛模态偏置的问题，通过自适应模态dropout、梯度一致性与基于WA的教师跨模态蒸馏，引导更平滑、平坦解，显著提升未知域鲁棒性与准确率。


<details>
  <summary>Details</summary>
Motivation: WA能提高泛化性且与更平坦的损失景观相关，但在多模态场景中因不同模态收敛速度不一致，直接WA会早期偏向快模态，抑制慢且互补的模态，导致融合受损与更尖锐的极小值，从而削弱OOD性能。

Method: 提出MBCD统一协作蒸馏：1) 学生端自适应模态dropout，缓解早期对主导模态的偏置；2) 引入梯度一致性约束，使单模态分支与融合表示的学习信号对齐，促进协调与平滑优化；3) 基于WA的教师进行跨模态蒸馏，将融合知识传回各单模态分支，强化交互并引导到更平坦解。

Result: 在多模态域泛化基准上，MBCD在多个未知域上取得更高准确率与鲁棒性，稳定优于现有方法。

Conclusion: MBCD在保留WA诱导平坦性的同时，克服其在多模态中的偏置问题，通过协作蒸馏与优化一致性实现更有效的模态融合与更平坦的收敛点，从而提升OOD泛化性能。

Abstract: Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.

</details>


### [128] [Advancing Image Classification with Discrete Diffusion Classification Modeling](https://arxiv.org/abs/2511.20263)
*Omer Belhasin,Shelly Golan,Ran El-Yaniv,Michael Elad*

Main category: cs.CV

TL;DR: 提出DiDiCM：用离散扩散过程建模图像分类中的后验分布，在高不确定场景（腐败图像、少样本）显著优于标准分类器，少量扩散步即可在ImageNet上提升准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分类器直接从图像预测标签，在噪声强、训练数据少等高不确定条件下易退化。需要一种能更好刻画标签后验不确定性的建模方法，以提升鲁棒性与样本效率。

Method: 提出离散扩散分类建模（DiDiCM）：将标签后验p(y|x)通过扩散/反扩散过程来建模，既可在类概率空间扩散，也可在离散标签空间扩散，按迭代步数进行推断，实现计算与内存的权衡。

Result: 在ImageNet上的系统实验表明，DiDiCM在多种困难设置中优于标准分类器；仅需少量扩散迭代即可超过基线，且任务越困难，精度提升越明显。

Conclusion: 基于离散扩散的后验建模为图像分类提供了鲁棒且可调推断的新范式，在高不确定性情形相对传统直接判别模型具有明显优势。

Abstract: Image classification is a well-studied task in computer vision, and yet it remains challenging under high-uncertainty conditions, such as when input images are corrupted or training data are limited. Conventional classification approaches typically train models to directly predict class labels from input images, but this might lead to suboptimal performance in such scenarios. To address this issue, we propose Discrete Diffusion Classification Modeling (DiDiCM), a novel framework that leverages a diffusion-based procedure to model the posterior distribution of class labels conditioned on the input image. DiDiCM supports diffusion-based predictions either on class probabilities or on discrete class labels, providing flexibility in computation and memory trade-offs. We conduct a comprehensive empirical study demonstrating the superior performance of DiDiCM over standard classifiers, showing that a few diffusion iterations achieve higher classification accuracy on the ImageNet dataset compared to baselines, with accuracy gains increasing as the task becomes more challenging. We release our code at https://github.com/omerb01/didicm .

</details>


### [129] [DRL-Guided Neural Batch Sampling for Semi-Supervised Pixel-Level Anomaly Detection](https://arxiv.org/abs/2511.20270)
*Amirhossein Khadivi Noghredeh,Abdollah Safari,Fatemeh Ziaeetabar,Firoozeh Haghighi*

Main category: cs.CV

TL;DR: 提出一种半监督深度强化学习框架用于工业视觉异常检测：用RL采样器挑选信息量大的图像块，AE生成损失图，预测器在损失空间做分割，少量标注也能学到异常模式，在MVTec AD上显著提升F1_max与AUC。


<details>
  <summary>Details</summary>
Motivation: 工业外观检验异常样本稀缺，纯无监督重建方法常过拟合正常模式，难以定位细微缺陷；需要一种能在有限标注下更好利用数据并提高微小异常检测与定位能力的方法。

Method: 构建由三部分组成的半监督框架：1) 基于深度强化学习的神经批采样器（平衡探索/利用，通过复合奖励自适应选择信息量大的图像块）；2) 自编码器对输入生成重建损失分布/损失剖面以突出异常区域；3) 预测器在损失剖面空间进行语义分割。三者交互训练，使系统同时学习正常与异常模式。

Result: 在MVTec AD数据集上相较近期SOTA取得更高精度与更好细粒度定位：平均F1_max提升0.15、AUC提升0.06，最佳案例F1_max最高提升0.37，同时模型复杂度较低。

Conclusion: RL驱动的自适应采样结合AE损失空间分割的半监督方案有效缓解异常样本稀缺与重建过拟合问题，可在低复杂度下显著提升工业视觉异常检测与定位性能。

Abstract: Anomaly detection in industrial visual inspection is challenging due to the scarcity of defective samples. Most existing methods rely on unsupervised reconstruction using only normal data, often resulting in overfitting and poor detection of subtle defects. We propose a semi-supervised deep reinforcement learning framework that integrates a neural batch sampler, an autoencoder, and a predictor. The RL-based sampler adaptively selects informative patches by balancing exploration and exploitation through a composite reward. The autoencoder generates loss profiles highlighting abnormal regions, while the predictor performs segmentation in the loss-profile space. This interaction enables the system to effectively learn both normal and defective patterns with limited labeled data. Experiments on the MVTec AD dataset demonstrate that our method achieves higher accuracy and better localization of subtle anomalies than recent state-of-the-art approaches while maintaining low complexity, yielding an average improvement of 0.15 in F1_max and 0.06 in AUC, with a maximum gain of 0.37 in F1_max in the best case.

</details>


### [130] [VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs](https://arxiv.org/abs/2511.20272)
*Tianxiang Jiang,Sheng Xia,Yicheng Xu,Linquan Wu,Xiangyu Zeng,Limin Wang,Yu Qiao,Yi Wang*

Main category: cs.CV

TL;DR: 提出VKnowU基准评估“视觉知识”（物理与社会常识）能力，并以VKnowQA与VideoKnow+提升MLLM在该能力上的表现，显著缩小与人类差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽能识物，但缺乏对物理与社会原则的直觉性理解，即“视觉知识”，导致感知到推理的桥接不足；社区缺少系统评测与有效训练方案。

Method: 1) 构建VKnowU：包含1,249段视频、1,680问，覆盖8类视觉知识（世界中心与人类中心）。2) 评测23个SOTA MLLM，量化与人类差距。3) 构建训练数据VKnowQA。4) 提出VideoKnow+：采用See-Think-Answer结构化范式，并用“视觉知识奖励”的强化学习优化。

Result: +3.7%在VKnowU；在MVBench、Video-MME、MMVU等基准上也有一致收益。观察到现有领先模型在世界中心（如直觉物理）方面差距更大。

Conclusion: 视觉知识是MLLM迈向可泛化理解的关键缺失环节。系统评测与引入视觉知识显式建模（含奖励与结构化推理）可有效提升模型的物理与社会理解能力。

Abstract: While Multimodal Large Language Models (MLLMs) have become adept at recognizing objects, they often lack the intuitive, human-like understanding of the world's underlying physical and social principles. This high-level vision-grounded semantics, which we term visual knowledge, forms a bridge between perception and reasoning, yet remains an underexplored area in current MLLMs. To systematically evaluate this capability, we present VKnowU, a comprehensive benchmark featuring 1,680 questions in 1,249 videos, covering 8 core types of visual knowledge spanning both world-centric (e.g., intuitive physics) and human-centric (e.g., subjective intentions). Evaluation of 23 SOTA MLLMs reveals that leading models still fall short of human performance, with particularly notable gaps in the world-centric. To bridge this gap, we introduce a new dataset, VKnowQA, and VideoKnow+, a baseline model that explicitly incorporates visual knowledge into MLLMs. VideoKnow+ follows a structured See-Think-Answer paradigm and adopts reinforcement learning with visual knowledge reward, achieving a +3.7% improvement on VKnowU and consistent gains on MVBench, Video-MME, and MMVU. Our work highlights visual knowledge as a missing cornerstone for developing more generalizable MLLMs that can not only see but also truly understand our physical and social worlds.

</details>


### [131] [ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis](https://arxiv.org/abs/2511.20274)
*Advik Sinha,Saurabh Atreya,Aashutosh A,Sk Aziz Ali,Abhijit Das*

Main category: cs.CV

TL;DR: 提出ScenarioCLIP，通过引入关系与场景级标注，提升多对象/多关系场景理解与检索；构建并公开数据与基准，零样本与微调均优。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP系方法多聚焦单对象分类或短文本检索，改进常依赖更难负样本与提示工程，但仍受限于预定义类目，缺乏对多对象间关系与组合结构的显式建模。PyramidCLIP虽对齐全局/局部特征，但仍未显式建模跨对象关系，难以满足真实场景的复杂组合理解需求。

Method: 提出ScenarioCLIP：输入包含文本、显式关系（动作、对象、关系三元组等）与图像，并提供聚焦关系的区域；在精心整理的“场景数据”上预训练，再对跨模态检索与细粒度视觉理解等任务进行微调。为弥补领域数据缺口，基于公开室内外数据集的图文对，使用现有语言模型流水线进行动作/对象/关系标注与定位，并结合人工与自动方式清洗与审核，生成新数据集与基准。

Result: 在多种场景相关任务上，相比多种基线方法，ScenarioCLIP在零样本与微调设置下都表现稳健且优越。

Conclusion: 显式引入关系与场景级监督并配合区域对齐，可显著提升CLIP类模型对复杂真实场景的组合与关系理解能力；所发布的数据与代码为相关研究提供了可复现基准与资源。

Abstract: Until recently, the general corpus of CLIP-type fundamental models has widely explored either the retrieval of short descriptions or the classification of objects in the scene as SINGLE-object image classification task. The same holds for retrieving the image embedding (image retrieval task) given a text prompt. However, real-world scene images exhibit rich compositional structure involving multiple objects and actions. The latest methods in the CLIP-based literature improve class-level discrimination by mining harder negative image-text pairs and by refining permanent text prompts, often using LLMs. However, these improvements remain confined to predefined class lists and do not explicitly model relational or compositional structure. PyramidCLIP partially addresses this gap by aligning global and local visual features, yet it still lacks explicit modeling of inter-object relations. Hence, to further leverage this aspect for scene analysis, the proposed ScenarioCLIP model accepts input texts, grounded relations, and input images, along with focused regions highlighting relations. The proposed model is pretrained on curated scenario data, and finetuned for specialized downstream tasks, such as cross-modal retrieval and fine-grained visual understanding tasks. To address the lack of domain-specific datasets, we generate a novel dataset by extending image-text pairs from existing diverse indoor and outdoor scenario datasets that are publicly available. We used a pipeline of existing language models to ground action, object, and relations, filled by manual and automatic curation. We established a comprehensive benchmark for several scenario-based tasks and compared it with many baseline methods. ScenarioCLIP demonstrates robust zero-shot and finetune performance on various domain-specific tasks. Our code and dataset are available at https://github.com/scenario-clip/ScenarioCLIP

</details>


### [132] [DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion](https://arxiv.org/abs/2511.20278)
*Yinghui Li,Qianyu Zhou,Di Shao,Hao Yang,Ye Zhu,Richard Dazeley,Xuequan Lu*

Main category: cs.CV

TL;DR: 提出DAPointMamba，用状态空间模型解决跨域点云补全，兼具全局感受野与线性复杂度，并通过三类跨域对齐模块在局部与全局层面缩小几何与语义差异，实验优于SOTA且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有跨域点云补全方法基于CNN或ViT，要么感受野有限、要么复杂度二次；直接把SSM用于该任务会破坏点云空间拓扑与局部几何，同时缺乏域无关表示的设计，导致适配效果差。

Method: 提出DAPointMamba框架（基于SSM，线性复杂度、全局感受野），包含三模块：1）跨域补丁级扫描（建立补丁几何对应，做局部对齐）；2）跨域空间SSM对齐（依据跨域相似度调制补丁特征，缓解细粒度结构差异）；3）跨域通道SSM对齐（交织并对齐特征通道，处理全局语义差距）。

Result: 在合成与真实基准上优于最新方法，同时计算复杂度和推理延迟更低。

Conclusion: SSM可在DA点云补全中取得强适配性；通过补丁级、空间与通道三重跨域对齐，DAPointMamba有效缩小几何与语义差异，达到更好精度与效率。

Abstract: Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.

</details>


### [133] [SelfMOTR: Revisiting MOTR with Self-Generating Detection Priors](https://arxiv.org/abs/2511.20279)
*Fabian Gülhan,Emil Mededovic,Yuli Wu,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 提出SelfMOTR：一种在端到端跟踪Transformer中使用“自生成检测先验”的方法，解决检测性能差与检测-关联冲突问题，在DanceTrack上达到与SOTA端到端方法竞争的表现。


<details>
  <summary>Details</summary>
Motivation: 端到端Transformer跟踪仍面临两大痛点：检测性能不足、以及在联合框架中检测与关联目标相互掣肘。现有缓解方案依赖复杂的去噪/标签分配，或引入外部检测器先验（蒸馏/锚框），带来额外依赖与复杂度。作者观察到MOTR类模型本身潜藏强检测能力，动机是挖掘并系统性利用这种“内生检测先验”。

Method: 提出SelfMOTR，在同一Transformer框架内生成并使用自有的检测先验，无需外部检测器。通过系统分析与消融，设计一套实用工具（如先验生成、利用策略与训练细节）以显式挖掘并强化MOTR-like模型的检测能力，从而缓解检测与关联的冲突。

Result: 在DanceTrack数据集上取得强竞争力，与近期端到端SOTA方法相当。消融与分析验证了自生成先验的有效性以及模型潜藏的检测能力。

Conclusion: MOTR类模型具备被低估的检测潜力；通过自生成检测先验并妥善利用，可在不依赖外部检测器的情况下同时改善检测与关联，实现具竞争力的端到端多目标跟踪。

Abstract: Despite progress toward end-to-end tracking with transformer architectures, poor detection performance and the conflict between detection and association in a joint architecture remain critical concerns. Recent approaches aim to mitigate these issues by (i) employing advanced denoising or label assignment strategies, or (ii) incorporating detection priors from external object detectors via distillation or anchor proposal techniques. Inspired by the success of integrating detection priors and by the key insight that MOTR-like models are secretly strong detection models, we introduce SelfMOTR, a novel tracking transformer that relies on self-generated detection priors. Through extensive analysis and ablation studies, we uncover and demonstrate the hidden detection capabilities of MOTR-like models, and present a practical set of tools for leveraging them effectively. On DanceTrack, SelfMOTR achieves strong performance, competing with recent state-of-the-art end-to-end tracking methods.

</details>


### [134] [Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement](https://arxiv.org/abs/2511.20280)
*Yang Liu,Xilin Zhao,Peisong Wen,Siran Dai,Qingming Huang*

Main category: cs.CV

TL;DR: 提出一个训练免、即插即用的物理一致性视频生成自迭代自纠框架，通过多模态链式思维与VLM/LLM反馈迭代修正提示，显著提升PhyIQ物理智力分数（56.31→62.38）。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成虽画质优但常违背物理规律（如动力学、碰撞、重力），缺乏系统性物理约束；需要一种无需重训、可通用于现有模型的物理一致性增强方法。

Method: 引入多模态链式思维（MM-CoT）：利用VLM检测生成视频中的物理不一致并生成反馈；LLM基于反馈迭代重写/细化生成提示（含物理约束、场景参数、动作描述）；循环生成-评估-改写，直到一致性改进或达到迭代上限。方法为训练免、可插拔，适配多种视频生成器。

Result: 在PhyIQ基准上显著提升Physics-IQ得分，从56.31提升到62.38；同时主观上生成质量与物理一致性更好。

Conclusion: 多模态自迭代提示优化能在不改动底层模型的情况下提升视频的物理一致性，是迈向物理一致视频生成的可行方向，为后续融入更强物理先验与更严谨评估提供基础。

Abstract: Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.

</details>


### [135] [Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations](https://arxiv.org/abs/2511.20295)
*Chao Wang,Chengan Che,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: 提出BTTF框架，面向视频分类器生成物理可行、时序连贯的反事实视频，通过噪声反推与两阶段优化在输入附近搜索对分类有影响的最小修改，并配合渐进式优化以加速收敛；在多数据集上验证有效与逼真。


<details>
  <summary>Details</summary>
Motivation: 现有视觉反事实方法多针对图像分类，难以保证视频反事实的时序一致性、平滑运动与物理可行性；缺乏能忠实依赖目标分类器并在输入附近寻优的视频CFE方法。

Method: 提出“Back To The Feature (BTTF)”优化框架：1) 反推初始潜在噪声（由视频首帧条件化）以进入与原视频相近的扩散潜空间；2) 两阶段优化，先粗搜找到改变预测的候选，再精调最小化与原视频的差异并保持物理与时序约束；3) 渐进式优化，逐步增加去噪步数以稳定且加速收敛；全程仅以目标分类器的反馈为引导，保证忠实性。

Result: 在Shape-Moving、MEAD、NTU RGB+D上产生有效的反事实视频：改变预测同时与原视频视觉相似、运动平滑、物理合理；实验显示方法收敛快、解释具体，优于图像式CFE基线。

Conclusion: BTTF能为视频分类器生成逼真且忠实的反事实解释，弥补视频CFE研究空白；其基于噪声反推与两阶段+渐进式优化的策略实现了时序连贯、物理可行与最小修改，并在多任务上验证有效。

Abstract: Counterfactual explanations (CFEs) are minimal and semantically meaningful modifications of the input of a model that alter the model predictions. They highlight the decisive features the model relies on, providing contrastive interpretations for classifiers. State-of-the-art visual counterfactual explanation methods are designed to explain image classifiers. The generation of CFEs for video classifiers remains largely underexplored. For the counterfactual videos to be useful, they have to be physically plausible, temporally coherent, and exhibit smooth motion trajectories. Existing CFE image-based methods, designed to explain image classifiers, lack the capacity to generate temporally coherent, smooth and physically plausible video CFEs. To address this, we propose Back To The Feature (BTTF), an optimization framework that generates video CFEs. Our method introduces two novel features, 1) an optimization scheme to retrieve the initial latent noise conditioned by the first frame of the input video, 2) a two-stage optimization strategy to enable the search for counterfactual videos in the vicinity of the input video. Both optimization processes are guided solely by the target classifier, ensuring the explanation is faithful. To accelerate convergence, we also introduce a progressive optimization strategy that incrementally increases the number of denoising steps. Extensive experiments on video datasets such as Shape-Moving (motion classification), MEAD (emotion classification), and NTU RGB+D (action classification) show that our BTTF effectively generates valid, visually similar and realistic counterfactual videos that provide concrete insights into the classifier's decision-making mechanism.

</details>


### [136] [Prompting Lipschitz-constrained network for multiple-in-one sparse-view CT reconstruction](https://arxiv.org/abs/2511.20296)
*Baoshun Shi,Ke Jiang,Qiusheng Lian,Xinran Yu,Huazhu Fu*

Main category: cs.CV

TL;DR: 提出PromptCT：在单一模型中处理多种稀疏视角CT重建，通过可证Lipschitz先验网络LipNet与显式提示模块实现，保证收敛、提升质量并降低存储。


<details>
  <summary>Details</summary>
Motivation: 深度展开SVCT方法的先验网络多为经验设计，难以证明满足Lipschitz约束，影响理论收敛性；同时多视角设置需分别训练多模型，造成存储负担并不利于临床应用。

Method: 1) 设计可显式证明Lipschitz连续且具边界性质的先验网络LipNet；2) 引入显式提示（prompt）模块，编码不同稀疏采样设置的判别信息，实现多视角“一体化”处理；3) 将LipNet嵌入深度展开框架，形成PromptCT，并给出对应迭代算法的收敛性分析与证明。

Result: 在仿真与真实数据上，PromptCT在多视角合一的SVCT重建任务上优于基线方法，取得更高成像质量，同时显著降低存储开销。

Conclusion: 通过可证Lipschitz先验LipNet与提示驱动的一体化展开框架PromptCT，可在保证算法收敛的同时兼顾多配置重建与存储效率，具备良好的理论与实践价值；代码与数据已公开。

Abstract: Despite significant advancements in deep learning-based sparse-view computed tomography (SVCT) reconstruction algorithms, these methods still encounter two primary limitations: (i) It is challenging to explicitly prove that the prior networks of deep unfolding algorithms satisfy Lipschitz constraints due to their empirically designed nature. (ii) The substantial storage costs of training a separate model for each setting in the case of multiple views hinder practical clinical applications. To address these issues, we elaborate an explicitly provable Lipschitz-constrained network, dubbed LipNet, and integrate an explicit prompt module to provide discriminative knowledge of different sparse sampling settings, enabling the treatment of multiple sparse view configurations within a single model. Furthermore, we develop a storage-saving deep unfolding framework for multiple-in-one SVCT reconstruction, termed PromptCT, which embeds LipNet as its prior network to ensure the convergence of its corresponding iterative algorithm. In simulated and real data experiments, PromptCT outperforms benchmark reconstruction algorithms in multiple-in-one SVCT reconstruction, achieving higher-quality reconstructions with lower storage costs. On the theoretical side, we explicitly demonstrate that LipNet satisfies boundary property, further proving its Lipschitz continuity and subsequently analyzing the convergence of the proposed iterative algorithms. The data and code are publicly available at https://github.com/shibaoshun/PromptCT.

</details>


### [137] [CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2511.20302)
*Shilei Cao,Ziyang Gong,Hehai Lin,Yang Liu,Jiashun Cheng,Xiaoxing Hu,Haoyuan Liang,Guowen Li,Chengwei Qin,Hong Cheng,Xue Yang,Juepeng Zheng,Haohuan Fu*

Main category: cs.CV

TL;DR: 提出CrossEarth-Gate：在遥感语义分割中，通过Fisher信息引导，从包含空间/语义/频域模块的工具箱中按层自适应激活关键模块，实现高效域间适配，在16个跨域基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在大规模遥感跨域任务上常失效，原因是无法同时应对遥感数据中的多重且不可预测的域间差异（空间、语义、频率迁移）。需要一种既参数高效又能细粒度处理多类型域差的适配机制。

Method: 1) 构建RS模块工具箱：包含针对不同域差的空间模块、语义模块与频域模块；2) 设计Fisher信息引导的自适应选择机制：计算各模块对任务梯度流的贡献，以Fisher信息度量其重要性，并在不同网络层动态激活最关键模块，从而引导梯度流、提高适配效率与效果。

Result: 在16个遥感语义分割的跨域基准上取得SOTA表现，验证了方法的有效性与可泛化性；同时实现参数高效的微调。

Conclusion: CrossEarth-Gate通过Fisher引导的模块选择，在多维域差场景下实现高效、有效的适配，显著优于现有PEFT方法，具有良好通用性；代码将开源。

Abstract: In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.

</details>


### [138] [TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection](https://arxiv.org/abs/2511.20306)
*Han Guo,Chenyang Liu,Haotian Zhang,Bowen Chen,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出TaCo网络，通过显式建模“语义转移”来改进遥感二时相变化检测，在仅使用掩码监督的框架下引入时空语义联合约束，显著提升CD与SCD任务上的一致性与精度，推理无额外开销。


<details>
  <summary>Details</summary>
Motivation: 仅掩码监督的方法能定位空间变化，但缺乏对跨时相语义过渡的约束，导致预测虽连贯却存在语义不一致（如类别/语义错配）。需要一种机制同时约束空间与时间语义，使变化被视为从t1到t2的语义转换而非仅像素差异。

Method: 构建TaCo：1）将变化定义为双时相状态的语义转移；2）提出文本引导的转移生成器（Text-guided Transition Generator, TTG），融合文本语义与双时相视觉特征，生成跨时相转移特征；3）设计时空语义联合约束，包括：a) 双时相重构约束，使由对端+转移重建的特征与原特征对齐；b) 转移约束，提升变化与非变化在转移空间的可分性；整体在掩码监督框架中训练，推理阶段不增加计算量。

Result: 在六个公共数据集上（覆盖二值CD与语义CD），TaCo达成一致的SOTA表现；在保证空间一致性的同时显著降低语义不一致错误，并在不增加推理开销的前提下带来显著的性能增益。

Conclusion: 将变化显式建模为“语义转移”并用文本先验引导，配合时空联合约束，可在掩码监督下有效解决语义不一致问题，实现通用且高效的RSCD改进，兼顾精度与推理效率。

Abstract: Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images. Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions. Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies. To address this limitation, we propose TaCo, a spatio-temporal semantic consistent network, which enriches the existing mask-supervised framework with a spatio-temporal semantic joint constraint. TaCo conceptualizes change as a semantic transition between bi-temporal states, in which one temporal feature representation can be derived from the other via dedicated transition features. To realize this, we introduce a Text-guided Transition Generator that integrates textual semantics with bi-temporal visual features to construct the cross-temporal transition features. In addition, we propose a spatio-temporal semantic joint constraint consisting of bi-temporal reconstruct constraints and a transition constraint: the former enforces alignment between reconstructed and original features, while the latter enhances discrimination for changes. This design can yield substantial performance gains without introducing any additional computational overhead during inference. Extensive experiments on six public datasets, spanning both binary and semantic change detection tasks, demonstrate that TaCo consistently achieves SOTA performance.

</details>


### [139] [TReFT: Taming Rectified Flow Models For One-Step Image Translation](https://arxiv.org/abs/2511.20307)
*Shengqian Li,Ming Gao,Yi Liu,Zuzeng Lin,Feng Wang,Feng Dai*

Main category: cs.CV

TL;DR: 提出TReFT，使Rectified Flow模型可一跳图像翻译，绕过多步去噪，达实时推理；基于“末端速度≈原点到洁净图像向量”的观察，结合对抗训练与高效正则，在SD3.5/FLUX上达到与SOTA相当表现。


<details>
  <summary>Details</summary>
Motivation: RF在生成质量强，但做图像到图像翻译仍需多步去噪，速度慢；直接套用CycleGAN-Turbo式的一步对抗训练在RF上易发散，需要新的稳定训练与推理设计。

Method: 提出TReFT：用预训练RF（DiT/UNet）的速度场作为直接输出目标，进行一步推理的对抗训练；理论与实证表明在去噪末期，速度近似指向洁净图像，从而稳定优化。训练时加入内存友好的潜空间循环一致与身份损失，并做轻量结构简化以加速推理；在大RF如SD3.5、FLUX上进行微调。

Result: 经TReFT微调的RF在多种图像翻译数据集上与SOTA相当，同时实现实时（一步）推理；训练更稳定，收敛问题显著缓解。

Conclusion: 通过直接回归RF末端速度向量并配合高效损失与架构简化，TReFT成功驯化大规模RF为一步图像翻译模型，兼顾质量与速度，适合实时应用。

Abstract: Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.

</details>


### [140] [IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection](https://arxiv.org/abs/2511.20319)
*Xuelin Qian,Jiaming Lu,Zixuan Wang,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出IrisNet：用图像到解码器的Transformer按图像状态动态生成解码器参数，并融合高频信息，在NUDT-SIRST、NUAA-SIRST、IRSTD-1K上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测在低信噪、复杂背景、跨场景（昼/夜、天空/海面/地面）下存在模式漂移；传统编码器-解码器静态参数难以适应多场景，鲁棒性不足。

Method: 设计元学习框架IrisNet：将解码器参数表示为保留层级相关性的2D结构张量；使用“图像到解码器”的Transformer，通过自注意力建模解码器层间依赖，通过交叉注意力根据输入图像生成整套解码器参数，实现动态解码策略；并引入高频分量增强目标位置与场景边缘感知。

Result: 在NUDT-SIRST、NUAA-SIRST、IRSTD-1K数据集上取得最优（SOTA）检测性能，全面优于现有方法。

Conclusion: 动态参数化解码和高频增强能有效缓解模式漂移，显著提升跨场景红外小目标检测的鲁棒性与精度。

Abstract: Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.

</details>


### [141] [AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models](https://arxiv.org/abs/2511.20325)
*Tianyi Yan,Tao Tang,Xingtai Gui,Yongkang Li,Jiasen Zhesng,Weiyao Huang,Lingdong Kong,Wencheng Han,Xia Zhou,Xueyang Zhang,Yifei Zhan,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出一种用于自动驾驶端到端系统的后训练策略优化框架：构建“公正世界模型”并作为内置评论家，配合反事实合成生成危险场景，以减少安全违规。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶模型难以保证安全、对长尾危险事件处理不足；现有基于世界模型的RL存在乐观偏置，导致对风险预测不准，限制了RL在自动驾驶中的效果。

Method: 1) 诊断：发现世界模型对风险存在系统性乐观偏差；2) 提出“公正世界模型”（Impartial World Model），核心是“诚实面对危险”；3) 设计反事实合成（Counterfactual Synthesis）数据管线，系统生成合理的碰撞与越界等危险轨迹，保持动作-结果的因果一致性，把模型从被动场景补全器变成能忠实预测后果的前瞻预测器；4) 将该模型嵌入闭环RL作为内置评论家，策略在 refinement 阶段查询评论家‘做梦’评估候选动作后果；5) 构建风险预见基准并进行大量实验评估。

Result: 在新提出的Risk Foreseeing Benchmark及其他实验中，模型在失败/风险事件预测上显著优于基线；作为评论家用于策略精炼后，在具有挑战性的仿真中显著减少安全违规。

Conclusion: 通过让世界模型“梦见危险”、忠实地反映行动-后果关系，可有效克服RL中的乐观偏置，提升端到端自动驾驶的安全性与智能性；作为通用后训练策略优化路线具有潜力。

Abstract: End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.

</details>


### [142] [3D Motion Perception of Binocular Vision Target with PID-CNN](https://arxiv.org/abs/2511.20332)
*Shi Jiazhao,Pan Pan,Shi Haotian*

Main category: cs.CV

TL;DR: 提出并验证了一种基于“PID”思想的轻量级卷积网络，用双目视频实时估计目标三维位置/速度/加速度，精度接近由输入分辨率决定的上限。


<details>
  <summary>Details</summary>
Motivation: 面向双目视觉中动态目标的三维时空感知，传统方法难以在非线性、噪声与实时性之间兼顾；作者希望用神经网络在保持小模型与高效计算的同时，获得对位置—速度—加速度的联合估计，并从控制理论（PID）视角理解网络拟合非线性能力与结构设计原则。

Method: 将单层网络视为“二阶差分+非线性”的局部建模单元，多层网络即多次组合以逐步完成表征变换；据此设计17层、约41.3万参数的PID卷积网络，采用拼接+池化进行特征复用；使用模拟的随机运动球双目数据进行训练与测试；并分析高维卷积对效率与特征空间利用的影响，以及用PID信息实现记忆与注意的潜力。

Result: 在合成数据上，该网络可实时输出三维坐标、速度与加速度，预测精度接近由输入图像分辨率所能表示的上限；对误差来源与表现进行了定量与定性分析。

Conclusion: 基于PID理念的轻量级CNN能够有效完成双目动态目标三维时空估计，具备良好效率与精度；高维卷积与特征复用提升了计算与表征利用率。仍存在改进空间（如更强记忆/注意机制、更复杂场景与真实数据验证）。

Abstract: This article trained a network for perceiving three-dimensional motion information of binocular vision target, which can provide real-time three-dimensional coordinate, velocity, and acceleration, and has a basic spatiotemporal perception capability. Understood the ability of neural networks to fit nonlinear problems from the perspective of PID. Considered a single-layer neural network as using a second-order difference equation and a nonlinearity to describe a local problem. Multilayer networks gradually transform the raw representation to the desired representation through multiple such combinations. Analysed some reference principles for designing neural networks. Designed a relatively small PID convolutional neural network, with a total of 17 layers and 413 thousand parameters. Implemented a simple but practical feature reuse method by concatenation and pooling. The network was trained and tested using the simulated randomly moving ball datasets, and the experimental results showed that the prediction accuracy was close to the upper limit that the input image resolution can represent. Analysed the experimental results and errors, as well as the existing shortcomings and possible directions for improvement. Finally, discussed the advantages of high-dimensional convolution in improving computational efficiency and feature space utilization. As well as the potential advantages of using PID information to implement memory and attention mechanisms.

</details>


### [143] [ShelfRectNet: Single View Shelf Image Rectification with Homography Estimation](https://arxiv.org/abs/2511.20335)
*Onur Berk Tore,Ibrahim Samil Yalciner,Server Calap*

Main category: cs.CV

TL;DR: 提出一个单幅图像的货架矫正方法：用ConvNeXt骨干与归一化坐标回归预测四点参数化单应性，辅以合成单应数据增强；在测试集上平均角点误差1.298像素，精度和速度均具竞争力，并开源ShelfRectSet与代码。


<details>
  <summary>Details</summary>
Motivation: 零售场景通常只有单一视角，需从任意角度拍摄的货架图像中进行几何矫正以利于货架监控与商品对齐；传统与现有深度方法在单视图条件下的稳健性与可用性仍有限，且标注/数据稀缺。

Method: - 任务：从单幅图像估计单应性，实现货架图像整形矫正。
- 表征：以4点参数化单应矩阵（角点回归）。
- 网络：ConvNeXt作为骨干，提升特征表达。
- 回归策略：归一化坐标回归，提升训练稳定性。
- 数据策略：通过建模与采样合成单应，提出新型数据增强以缓解数据稀缺并提升泛化。

Result: 在测试集上达到平均角点误差1.298像素；与经典计算机视觉与深度学习基线相比，精度与推理速度均具竞争力。

Conclusion: 所提方法在单视图货架矫正上稳健高效，适用于真实场景；将公开ShelfRectSet数据集与代码以促进后续研究。

Abstract: Estimating homography from a single image remains a challenging yet practically valuable task, particularly in domains like retail, where only one viewpoint is typically available for shelf monitoring and product alignment. In this paper, we present a deep learning framework that predicts a 4-point parameterized homography matrix to rectify shelf images captured from arbitrary angles. Our model leverages a ConvNeXt-based backbone for enhanced feature representation and adopts normalized coordinate regression for improved stability. To address data scarcity and promote generalization, we introduce a novel augmentation strategy by modeling and sampling synthetic homographies. Our method achieves a mean corner error of 1.298 pixels on the test set. When compared with both classical computer vision and deep learning-based approaches, our method demonstrates competitive performance in both accuracy and inference speed. Together, these results establish our approach as a robust and efficient solution for realworld single-view rectification. To encourage further research in this domain, we will make our dataset, ShelfRectSet, and code publicly available

</details>


### [144] [AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend](https://arxiv.org/abs/2511.20343)
*Hengyi Wang,Lourdes Agapito*

Main category: cs.CV

TL;DR: AMB3R 是一种多视角前馈模型，基于稀疏而紧凑的体素表示，实现度量尺度的致密三维重建，并在无需特定微调的情况下泛化到视觉里程计与大规模SfM，整体在位姿、深度、尺度与重建质量上达SOTA，甚至超越部分优化型SLAM/SfM。


<details>
  <summary>Details</summary>
Motivation: 现有基于点图(PointMap)或优化的SLAM/SfM方法在通用性、效率或密集重建质量上存在权衡；希望用一个统一、前馈的多视角模型，在保持空间紧凑性的同时做出强几何推断，兼顾尺度、位姿与密集重建，并减少任务特定调优。

Method: 提出以稀疏且紧凑的体素化场景表示为后端，通过多视角前馈网络进行几何推理与密集重建。模型只以多视角重建任务进行训练，但其表示与推理流程可无缝用于无标定在线VO与大规模SfM，无需测试时优化或任务特定微调。

Result: 在相机位姿、深度与度量尺度估计、三维重建等指标上达到或刷新SOTA；在公共基准上，相较既有点图方法表现更优，且在有致密先验的条件下甚至超过部分优化驱动的SLAM与SfM方法。

Conclusion: 稀疏紧凑体素作为后端的多视角前馈模型可统一多种3D视觉任务，具备强泛化能力与度量尺度一致性，在无需额外优化的前提下实现高质量致密重建与精确位姿估计。

Abstract: We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.

</details>


### [145] [Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin](https://arxiv.org/abs/2511.20348)
*João Malheiro Silva,Andy Huynh,Tong Duy Son,Holger Caesar*

Main category: cs.CV

TL;DR: 提出一种仅用相机的数字孪生重建与仿真管线：多视图3D Gaussian Splatting重建→视觉模型提取材料语义→高斯转网格并投影材料标签→赋予基于物理的材料属性，实现与LiDAR-相机融合相当的传感器仿真保真度且无需硬件校准。以车载内部数据集验证，并用LiDAR作反射率“真值”和图像相似度评估。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生多依赖LiDAR，虽几何精确但缺乏图像的语义与纹理；相机-雷达融合需要繁琐标定，且玻璃等材料在点云中表现差。需要一种既具照片级外观又含物理材料属性、还能简化硬件与标定的方案。

Method: 完全基于相机：1) 用多视图图像进行3D Gaussian Splatting重建；2) 通过视觉模型分割/识别材料语义并生成掩码；3) 将高斯表征转为网格并把材料标签投影到网格；4) 在现代图形引擎/仿真器中为每类材料赋予基于物理的属性（如反射/折射/粗糙度等）以进行传感器仿真。

Result: 达成与传统LiDAR-相机融合相近的传感器仿真保真度，同时消除硬件复杂度与标定；在车载内部数据集上验证，用LiDAR作为反射率验证基准，并辅以图像相似度指标。

Conclusion: 相机唯一输入即可实现照片级3D重建与物理材料赋值，支持高保真传感器仿真，规避融合系统的复杂性；对玻璃等材料更有优势。未来可在更大规模与更多传感器模型上扩展与量化。

Abstract: 3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.

</details>


### [146] [Thinking in 360°: Humanoid Visual Search in the Wild](https://arxiv.org/abs/2511.20351)
*Heyang Yu,Yinan Han,Xiangyu Zhang,Baiqiao Yin,Bowen Chang,Xiangyu Han,Xinhao Liu,Jing Zhang,Marco Pavone,Chen Feng,Saining Xie,Yiming Li*

Main category: cs.CV

TL;DR: 提出“类人视觉搜索”任务：在360°全景图中用头眼协同策略主动旋转搜索目标/路径；构建涵盖复杂公共场景的H* Bench；发现现有顶尖模型成功率仅约30%；通过后训练大幅提升开源Qwen2.5-VL在目标与路径搜索上的成功率，但路径搜索仍更难。


<details>
  <summary>Details</summary>
Motivation: 现有视觉搜索多基于静态图像，忽视具身性的头眼协同与与3D世界交互；需要在不依赖昂贵不便的实体硬件下，研究接近人类效率的具身视觉搜索能力，尤其是在拥挤、复杂、真实世界场景中。

Method: 1) 定义“类人视觉搜索”设定：在360°全景图中，类人代理通过主动“转头”执行搜索（模拟头-眼协同）。2) 构建H* Bench基准：覆盖交通枢纽、大型零售、城市街道、公共机构等在野场景，要求较强视觉-空间推理；包含目标搜索与路径搜索两类任务。3) 评测专有与开源MLLM；对开源Qwen2.5-VL进行后训练（如指令微调/偏好优化等）以提升具身搜索能力。

Result: 顶级专有模型在对象与路径搜索上成功率约30%；原始Qwen2.5-VL成功率低（对象14.83%、路径6.44%）；经后训练后分别提升至47.38%与24.94%，实现三倍以上相对提升；路径搜索上限更低，表现显著落后。

Conclusion: 具身视觉搜索在真实复杂场景中远未解决；后训练能显著提升开源模型，但路径搜索因需要更复杂空间常识而更难。H* Bench为评测提供挑战性平台，结果量化了将MLLM无缝融入日常生活仍面临的巨大挑战。

Abstract: Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.

</details>


### [147] [GS-Checker: Tampering Localization for 3D Gaussian Splatting](https://arxiv.org/abs/2511.20354)
*Haoliang Han,Ziyuan Luo,Jun Qi,Anderson Rocha,Renjie Wan*

Main category: cs.CV

TL;DR: 提出GS-Checker，用于在3D Gaussian Splatting模型中定位被篡改区域；通过在高斯参数中引入“篡改属性”、构建3D对比机制与循环优化策略，在无需昂贵3D标注的情况下实现精确篡改定位。


<details>
  <summary>Details</summary>
Motivation: 3DGS编辑变得容易后，3D场景可被轻易操控，带来潜在恶意篡改与安全风险。现有工作缺乏对3DGS模型中篡改区域的有效定位方法，需要一种可自动、可靠地在3D层面发现被改动部分的工具，以防滥用并提升内容取证能力。

Method: 1) 在每个3D高斯上引入“3D篡改属性”（一个可学习指示量），标识该高斯是否被篡改；2) 设计3D对比机制：比较高斯间关键属性（如位置、颜色、法向或不透明度等）的相似性，挖掘3D层面的不一致以作为篡改线索；3) 循环优化（cyclic optimization）策略迭代细化篡改属性，提高定位精度；4) 不依赖昂贵的3D人工标注进行监督。

Result: 在多组实验中，GS-Checker能够有效、稳定地在3DGS模型中定位被篡改区域，相比基线展现更高的准确率与鲁棒性。

Conclusion: GS-Checker通过在3DGS中内嵌篡改属性、结合3D对比学习与循环优化，在无需3D标注的前提下实现高精度篡改定位，为3D内容安全与取证提供了实用方案。

Abstract: Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.

</details>


### [148] [From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations](https://arxiv.org/abs/2511.20359)
*Zhiqing Guo,Dongdong Xi,Songlin Li,Gaobo Yang*

Main category: cs.CV

TL;DR: 提出BoxPromptIML：一种用框提示与记忆引导的弱监督图像篡改定位框架，靠低成本粗区域标注+SAM教师蒸馏+双引导特征融合，在多数据集上超越或媲美全监督精度，同时具备强泛化与高效部署。


<details>
  <summary>Details</summary>
Motivation: IML在“标注成本低 vs. 定位精细度高”之间两难：全监督需密集像素掩码，扩展差；弱监督多用图像级标签，成本低但空间定位粗。需要一种兼顾成本与精度、可部署性强的方法。

Method: 1) 粗区域标注策略：用低成本的框/区域提示生成较准确的伪掩码。2) 教师-学生框架：教师为基于SAM的固定模型，学生为高效轻量网络，通过知识蒸馏学习细粒度定位。3) 特征融合模块（双引导）：借鉴潜意识记忆，利用原型（长期记忆）与输入实时线索（短期观测）双引导，动态回忆并情境化特征，而非被动提取。

Result: 在分布内与分布外数据集上，定位精度优于或可与全监督方法相当，同时保持更低标注成本与更强泛化；轻量学生模型带来高效推理、便于部署。

Conclusion: BoxPromptIML通过“低成本框提示+SAM蒸馏+记忆式双引导融合”打破弱监督与精细定位的权衡，实现高精度、强泛化与高效部署，为IML提供可扩展的实用方案。

Abstract: Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.

</details>


### [149] [VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild](https://arxiv.org/abs/2511.20366)
*Xin Ming,Yuxuan Han,Tianyu Huang,Feng Xu*

Main category: cs.CV

TL;DR: 提出VGGTFace：把3D基础模型VGGT与Pixel3DMM结合，实现从野外多视图图像自动重建拓扑一致的人脸网格；并引入拓扑感知BA（含Laplacian能量）融合点云，速度快、泛化强、性能SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要大量手工、对野外数据泛化差、要么受3DMM表达力限制；VGGT具备强泛化与点图表示的表达力，但缺乏拓扑信息，难以得到一致网格。

Method: 1) 用VGGT从多视图生成像素对齐的点图；2) 通过将Pixel3DMM的像素对齐UV注入，给点云赋予拓扑（将点图转换为带已知拓扑的点云/网格）；3) 设计拓扑感知捆绑调整（Bundle Adjustment），在目标中加入Laplacian能量以保持局部形状一致与平滑，跨视角融合；实现端到端自动化；单4090上16视角约10秒。

Result: 在公开基准上达到SOTA，并对野外多视图用户拍摄数据显示强泛化与高质量重建。

Conclusion: 通过将VGGT的强表达与Pixel3DMM的拓扑先验结合，并以拓扑感知BA融合，实现快速、鲁棒、拓扑一致的人脸重建，适用于数字头像生产流程。

Abstract: Reconstructing topologically consistent facial geometry is crucial for the digital avatar creation pipelines. Existing methods either require tedious manual efforts, lack generalization to in-the-wild data, or are constrained by the limited expressiveness of 3D Morphable Models. To address these limitations, we propose VGGTFace, an automatic approach that innovatively applies the 3D foundation model, \emph{i.e.} VGGT, for topologically consistent facial geometry reconstruction from in-the-wild multi-view images captured by everyday users. Our key insight is that, by leveraging VGGT, our method naturally inherits strong generalization ability and expressive power from its large-scale training and point map representation. However, it is unclear how to reconstruct a topologically consistent mesh from VGGT, as the topology information is missing in its prediction. To this end, we augment VGGT with Pixel3DMM for injecting topology information via pixel-aligned UV values. In this manner, we convert the pixel-aligned point map of VGGT to a point cloud with topology. Tailored to this point cloud with known topology, we propose a novel Topology-Aware Bundle Adjustment strategy to fuse them, where we construct a Laplacian energy for the Bundle Adjustment objective. Our method achieves high-quality reconstruction in 10 seconds for 16 views on a single NVIDIA RTX 4090. Experiments demonstrate state-of-the-art results on benchmarks and impressive generalization to in-the-wild data. Code is available at https://github.com/grignarder/vggtface.

</details>


### [150] [FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers](https://arxiv.org/abs/2511.20390)
*Xinwan Wen,Bowen Li,Jiajun Luo,Ye Li,Zhi Wang*

Main category: cs.CV

TL;DR: 提出FREE与FREE (relax)两种用于DiT的无损并行推断框架，通过在顶层Transformer特征上做轻量级自回归起草与并行验证，显著提升采样速度（最高2.25×）且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: DiT生成质量高但去噪步数长、推理延迟大；现有面向U-Net的speculative sampling在DiT上起草精度不足，导致加速有限。作者希望在不牺牲质量的前提下提高DiT推断速度。

Method: 分析DiT特征动态，发现顶层（top-block）特征时间一致性强、语义抽象丰富；据此设计FREE：用轻量级drafter对顶层特征做特征级自回归草拟，并与verifier并行校验，提供无损加速与理论保证。考虑到后期去噪不确定性上升导致接受率下降，提出不确定性引导的放宽策略FREE (relax)，动态调节接受概率以提升通过率。

Result: 在ImageNet-512×512上，FREE实现最高1.86×加速；FREE (relax)在保持高感知与量化质量的同时进一步达成2.25×加速。

Conclusion: 在DiT中，利用顶层特征的时间一致性进行特征级speculative采样可实现无损并行加速；配合不确定性引导的放宽策略可在保持质量下进一步提升速度，对高分辨率生成具有实际价值。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art generation quality but require long sequential denoising trajectories, leading to high inference latency. Recent speculative inference methods enable lossless parallel sampling in U-Net-based diffusion models via a drafter-verifier scheme, but their acceleration is limited on DiTs due to insufficient draft accuracy during verification. To address this limitation, we analyze the DiTs' feature dynamics and find the features of the final transformer layer (top-block) exhibit strong temporal consistency and rich semantic abstraction. Based on this insight, we propose FREE, a novel framework that employs a lightweight drafter to perform feature-level autoregression with parallel verification, guaranteeing lossless acceleration with theoretical and empirical support. Meanwhile, prediction variance (uncertainty) of DiTs naturally increases in later denoising steps, reducing acceptance rates under speculative sampling. To mitigate this effect, we further introduce an uncertainty-guided relaxation strategy, forming FREE (relax), which dynamically adjusts the acceptance probability in response to uncertainty levels. Experiments on ImageNet-$512^2$ show that FREE achieves up to $1.86 \times$ acceleration, and FREE (relax) further reaches $2.25 \times$ speedup while maintaining high perceptual and quantitative fidelity in generation quality.

</details>


### [151] [A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control](https://arxiv.org/abs/2511.20401)
*Jiawei Lin,Guanlong Jiao,Jianjin Xu*

Main category: cs.CV

TL;DR: 提出MultiID：一种无需训练的多身份定制生成方法，通过ID解耦交叉注意力和三种可控生成策略，缓解复制粘贴伪影并提升文本可控性，性能与训练式方法相当或更优。


<details>
  <summary>Details</summary>
Motivation: 多身份（multi-ID）定制比单身份更难：1）基于人物区域重建的训练易在推理时出现“复制-粘贴”式伪影，画质差；2）文本可控性弱，模型往往仅简单将多个人拼在一起，忽视指令语义。需要一种既能保真保ID、又能强可控且避免再训练的方案。

Method: 在训练自由（training-free）框架下，将单ID定制模型适配到多ID：1）ID解耦的跨注意力（ID-decoupled cross-attention），为各图像区域注入对应的ID嵌入，实现多身份融合；2）三项可控策略：a) 局部提示（local prompt）以区域化地约束语义；b) 基于深度的空间控制（depth-guided spatial control）稳定布局与遮挡关系；c) 扩展自注意力（extended self-attention）强化全局一致与细节融合。并构建评测基准IDBench。

Result: 在定性与定量实验上显著减少复制-粘贴问题，提升文本与ID一致性；整体效果可与、甚至优于需要训练的多ID方法。

Conclusion: 无需再训练即可实现高质量多身份定制，既保留各自身份特征又提升文本与空间可控性；方法通用、可与现有单ID模型兼容，并在新建基准IDBench上验证有效性。

Abstract: Multi-ID customization is an interesting topic in computer vision and attracts considerable attention recently. Given the ID images of multiple individuals, its purpose is to generate a customized image that seamlessly integrates them while preserving their respective identities. Compared to single-ID customization, multi-ID customization is much more difficult and poses two major challenges. First, since the multi-ID customization model is trained to reconstruct an image from the cropped person regions, it often encounters the copy-paste issue during inference, leading to lower quality. Second, the model also suffers from inferior text controllability. The generated result simply combines multiple persons into one image, regardless of whether it is aligned with the input text. In this work, we propose MultiID to tackle this challenging task in a training-free manner. Since the existing single-ID customization models have less copy-paste issue, our key idea is to adapt these models to achieve multi-ID customization. To this end, we present an ID-decoupled cross-attention mechanism, injecting distinct ID embeddings into the corresponding image regions and thus generating multi-ID outputs. To enhance the generation controllability, we introduce three critical strategies, namely the local prompt, depth-guided spatial control, and extended self-attention, making the results more consistent with the text prompts and ID images. We also carefully build a benchmark, called IDBench, for evaluation. The extensive qualitative and quantitative results demonstrate the effectiveness of MultiID in solving the aforementioned two challenges. Its performance is comparable or even better than the training-based multi-ID customization methods.

</details>


### [152] [Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs](https://arxiv.org/abs/2511.20410)
*Bao Tang,Shuai Zhang,Yueting Zhu,Jijun Xiang,Xin Yang,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出Trajectory-Backward Consistency Model（TBCM），无需外部数据，通过教师扩散模型的生成轨迹提取潜在表示进行一致性蒸馏，在一步生成下兼顾高质量与高效率，并显著降低训练时间与显存占用。


<details>
  <summary>Details</summary>
Motivation: 现有连续时间一致性蒸馏（CM范式）虽能实现少步高质生成，但强依赖大规模训练数据和算力，难以在资源受限环境部署，且训练与推理分布存在鸿沟，限制跨域可扩展性。

Method: 提出TBCM：不再使用外部数据或VAE编码，而是直接从教师模型的扩散生成轨迹中提取潜在表示，构造自包含的蒸馏数据；以轨迹为监督进行一致性训练，缩小训练与推理分布差异；并系统分析采样策略对蒸馏性能的影响，揭示扩散-生成空间差异。

Result: 在MJHQ-30k上一步生成实现FID 6.52、CLIP 28.08；相比Sana-Sprint训练时间降低约40%，并显著节省GPU显存，在不牺牲质量的前提下提升效率。

Conclusion: TBCM通过轨迹反向的一致性蒸馏实现无数据、低资源的高效扩散模型加速，缓解训练-推理分布偏移；为后续一致性蒸馏提供关于生成空间差异与采样策略的实践与理论启示。

Abstract: Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.

</details>


### [153] [MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts](https://arxiv.org/abs/2511.20415)
*Zilong Huang,Jun He,Xiaobin Huang,Ziyi Xiong,Yang Luo,Junyan Ye,Weijia Li,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: MajutsuCity提出一个文本驱动、可控且风格多样的3D城市生成框架，结合数据集与评测指标，达到当前最优的结构一致性与可编辑性，并支持交互式对象级编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D城市生成要在“文本创意灵活性”和“显式结构的对象级可编辑性”间权衡，难以同时满足结构一致、风格多样和精细可控的需求。

Method: 将城市表示为布局、资产与材质的可控组合，采用四阶段流水线实现从语义到几何与渲染的生成；引入语言驱动的交互式编辑代理MajutsuAgent，支持五种对象级操作；构建MajutsuDataset，含2D语义布局与高程图、丰富的3D建筑资产、PBR材质与天空盒，并提供详注；提出覆盖结构一致性、复杂度、材质逼真度与光照氛围的评测指标。

Result: 在实验中，相比CityDreamer布局FID降低83.7%，相比CityCraft降低20.1%；在AQS与RDR各项评分中均排名第一，表明在几何保真度、风格适应性和语义可控性上显著优于现有方法。

Conclusion: MajutsuCity成为3D城市生成的新SOTA，实现结构一致与风格多样并重且支持对象级编辑；其框架、数据集与指标有望推动该领域进一步研究，代码与数据将开源。

Abstract: Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.

</details>


### [154] [StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections](https://arxiv.org/abs/2511.20418)
*Matvei Shelukhan,Timur Mamedov,Karina Kvanchiani*

Main category: cs.CV

TL;DR: StableTrack 针对低频检测场景提出稳定多目标跟踪：以两阶段匹配、BBox距离度量与将视觉跟踪融入卡尔曼滤波为核心，在1Hz下显著提升HOTA（+11.6%），同时在常规频率上保持SOTA水准。


<details>
  <summary>Details</summary>
Motivation: 现有MOT方法假设高频逐帧检测，算力受限或低频检测时关联不稳、漂移严重，导致整体性能崩塌。需要一种在低检测帧率下仍能稳健关联的跟踪框架。

Method: 1) 两阶段匹配：先进行稳健粗匹配，再细化跨帧关联；2) 用“基于BBox的距离”替代传统马氏距离，结合Re-ID更有效匹配目标；3) 将视觉跟踪器信号融入卡尔曼滤波与全流程，提升在检测缺失间隙中的目标状态估计稳定性。

Result: 在MOT17-val低频1Hz检测条件下，HOTA提升11.6%；在MOT17、MOT20、DanceTrack标准全频检测设置下性能与SOTA持平。

Conclusion: StableTrack在低频检测资源受限条件下显著强化跨帧关联与稳定性，同时不牺牲常规场景表现，适合作为通用MOT管线的稳健化方案。

Abstract: Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.

</details>


### [155] [Block Cascading: Training Free Acceleration of Block-Causal Video Models](https://arxiv.org/abs/2511.20426)
*Hmrishav Bandyopadhyay,Nikhil Pinnaparaju,Rahim Entezari,Jim Scott,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: 提出“Block Cascading”训练无关的并行化策略，让视频生成的块级去噪可并行进行，在多GPU下把顺序块因果管线改为级联并行，速度显著提升且质量无显著下降。


<details>
  <summary>Details</summary>
Motivation: 块因果视频生成存在速度-质量两难：小模型尚可但大模型极慢，交互体验差；且上下文切换需要KV缓存重建产生额外开销。需要一种在不牺牲质量、无需再训练的方式提升推理吞吐与交互响应。

Method: 关键洞见：后续视频块无需等待前一块完全去噪即可开始。以部分去噪的前块作为上下文触发后块去噪，将原本严格顺序的块级去噪改为并行级联。利用时间维的并行性，在多GPU上同时对多个块去噪，并避免频繁的KV re-caching。该方法对现有块因果管线为推理期替换，无需训练改动。

Result: 在5块GPU下，各模型规模约2倍加速：1.3B从16→30 FPS；14B从4.5→12.5 FPS；同时移除交互生成中的约200ms KV重缓存开销，且与多种块因果管线对比评测显示生成质量无显著下降。

Conclusion: Block Cascading在不改训练的前提下，将块因果视频生成由串行转为并行级联，实测在多GPU上显著加速且保持质量，为高质量、低延迟的视频生成提供了通用可落地的推理方案。

Abstract: Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/

</details>


### [156] [BRIC: Bridging Kinematic Plans and Physical Control at Test Time](https://arxiv.org/abs/2511.20431)
*Dohun Lim,Minji Kim,Jaewoon Lim,Sungchan Kim*

Main category: cs.CV

TL;DR: BRIC 是一种在测试时自适应（TTA）的框架，用于把扩散式运动规划与基于强化学习的物理控制衔接起来，实现稳定、长期、物理可信的人体运动生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能生成多样、富表现力的文本/场景条件人体运动轨迹，但常不物理、在仿真执行时会漂移；而RL物理控制器虽能稳定执行，但难以直接跟随带噪的规划。需要在不重新训练大模型的前提下，消除规划-控制失配，保证长期一致与可物理执行。

Method: 提出 BRIC：1) 测试时自适应物理控制器，对带噪的扩散规划进行在线适配，同时通过防遗忘损失保持已有技能；2) 轻量级测试时引导，对扩散模型做信号空间的引导而不更新其参数；两者联合以减少执行漂移、提高物理可行性与一致性。

Result: 在长期任务（运动组合、避障、人-场景交互）上取得SOTA，表现为更稳定、长期一致、物理可行，且高效。

Conclusion: 通过在测试阶段同时适配控制器并引导扩散规划，BRIC 有效弥合规划与控制的差距，实现在多样环境中的长期、物理可信的人体运动生成。

Abstract: We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.

</details>


### [157] [Object-Centric Vision Token Pruning for Vision Language Models](https://arxiv.org/abs/2511.20439)
*Guangyuan Li,Rongzhen Zhao,Jinhong Deng,Yanbo Wang,Joni Pajarinen*

Main category: cs.CV

TL;DR: 提出OC-VTP：一种直接且有保证的视觉token剪枝方法，通过最小化重构误差选择最具代表性的视觉token，在不微调VLM的前提下提升推理效率并保持精度。


<details>
  <summary>Details</summary>
Motivation: VLM中的视觉token数量多但信息分散，导致计算冗余。现有剪枝方法多为间接启发式，缺乏选择最优token的保证，且可能牺牲准确率，需要一种可解释、可插拔、具理论保证的高效剪枝方法。

Method: 预训练一个小型“以对象为中心”的视觉token剪枝器（object-centric pruner），学习从原始token中选择子集，使得用所选token重构未剪枝token时的误差最小。该剪枝器轻量预训练后可直接插入各类主流VLM，在不对VLM或数据集进行任何微调的情况下工作，可在任意剪枝率下运行。

Result: 在不同视觉剪枝率（即不同推理加速需求）下，OC-VTP在主流VLM上稳定保持最高的推理精度，同时显著减少计算量；剪枝选择呈现出与对象相关的可解释性。

Conclusion: OC-VTP提供了一种直接、可证明保持代表性token的剪枝框架，兼顾效率与准确率、可即插即用且具有良好可解释性，为VLM推理加速提供了更可靠的解决方案。

Abstract: In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.

</details>


### [158] [Learning to Generate Human-Human-Object Interactions from Textual Descriptions](https://arxiv.org/abs/2511.20446)
*Jeonghyeon Na,Sangwon Baik,Inhee Lee,Junyoung Lee,Hanbyul Joo*

Main category: cs.CV

TL;DR: 提出并验证了一种面向人-人-物交互（HHOI）的生成框架：构建数据集与合成方法，训练文本到HOI/HHI扩散模型，并统一融合以一次采样生成完整多人的场景与动作，生成质量优于只关注单人HOI的方法。


<details>
  <summary>Details</summary>
Motivation: 人类交互的空间距离、布局与运动随情境变化而复杂；现有研究多聚焦单人HOI或双人HHI，缺乏能同时建模两人围绕共享物体的交互（HHOI）及相应数据集，限制了机器理解与生成复杂多人的场景能力。

Method: 1) 定义HHOI问题并采集专用HHOI数据集；2) 借助图像生成模型合成补充HHOI数据；3) 将HHOI分解为HOI与HHI两个中间任务，从HHOI数据中提取相应标注；4) 分别训练文本到HOI与文本到HHI的基于score-based diffusion的生成模型；5) 设计统一生成框架，将两模型在高级采样过程中融合，对文本条件一次性生成完整HHOI；6) 进一步扩展到多于两人的交互生成与多人体动作生成（含物体）。

Result: 在文本条件下生成的HHOI在真实性与一致性上优于仅关注单人HOI的基线；框架可扩展到多人的交互与含物体的多人体运动生成。

Conclusion: 通过数据与方法的双重创新，提出的统一扩散生成框架能有效建模与生成两人共享物体的复杂交互，并具备可扩展到多人的能力，填补了HHOI领域数据与方法的空白。

Abstract: The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.

</details>


### [159] [Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search](https://arxiv.org/abs/2511.20460)
*Yunqi Zhou,Chengjie Jiang,Chun Yuan,Jing Li*

Main category: cs.CV

TL;DR: ZoomSearch 是一个免训练、可插拔的超高分辨率遥感 VQA 方案：先通过自适应多分支分层放大搜索定位与问题相关区域，再按版式感知地重组补丁，最后交给现成大模型回答；在两大 Ultra-HR 基准上取得显著精度与推理效率提升。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感图像日益普及，但现有基础模型要么整图编码导致算力/Token爆炸，要么下采样丢失关键细节，导致 RS-VQA 表现受限。需要在回答前“看对地方”，把“看哪里”和“怎么答”解耦。

Method: 提出 ZoomSearch：1) 自适应多分支缩放搜索（分层在图像块上放大与筛选，定位与问题相关的细粒度区域）；2) 版式感知补丁重组（将选中补丁在保持空间布局的同时压缩到紧凑画布）；3) 作为免训练、可插拔前端接入现有多模态大模型（如 LLaVA-ov）。

Result: 在 LRS-VQA 与 MME-RealWorld-RS 上，接入 LLaVA-ov 后取得 SOTA：相对 LLaVA-ov 基线准确率分别提升 26.3% 与 114.8%；相比既有搜索式方法，推理速度提升约 20%~44%。

Conclusion: 在超高分辨率 RS-VQA 中，先定位再作答的搜索+重组前端能在不训练主干模型的前提下显著提升准确率与效率；ZoomSearch 为 Ultra-HR 场景提供了实用的、可泛化的即插即用解决方案。

Abstract: With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.

</details>


### [160] [STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow](https://arxiv.org/abs/2511.20462)
*Jiatao Gu,Ying Shen,Tianrong Chen,Laurent Dinh,Yuyang Wang,Miguel Angel Bautista,David Berthelot,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow-V是一个基于归一化流的自回归视频生成模型，在时空潜空间采用全局-局部架构，引入flow-score matching轻量因果去噪器与视频感知的Jacobi迭代以提升一致性与采样效率，支持T2V/I2V/V2V并在视觉质量、时序一致性和吞吐上与扩散基线具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 视频生成相比图像具有更高的时空复杂度与计算成本，当前主流方案几乎被扩散模型垄断，存在自回归误差累积、采样慢、难以端到端似然学习等问题。作者希望验证归一化流（具备端到端似然与可逆性）能否在视频领域实现高质量生成，提供扩散之外的可行路线。

Method: 在STARFlow基础上扩展到视频：1) 在时空潜空间构建全局-局部结构，将因果依赖限制在全局潜变量上，同时保留帧内丰富的局部交互，缓解长期误差累积；2) 提出flow-score matching，为模型配备轻量因果去噪器，提升自回归一致性；3) 设计视频感知的Jacobi迭代，将内部更新重写为可并行的迭代而不破坏因果性；4) 由于可逆性，统一支持文本到视频、图像到视频、视频到视频。

Result: 在多项基准上实现强视觉保真度与时间一致性，采样吞吐在实际可用范围内，并与扩散模型基线具有竞争力；作者声称首次证明NFs可进行高质量自回归视频生成。

Conclusion: 归一化流可作为视频生成（尤其自回归与世界模型构建）的有前景方向。STARFlow-V通过全局-局部因果设计、轻量去噪与并行可行的迭代优化，在质量与效率上取得兼顾，并天然提供似然估计与多任务统一框架。

Abstract: Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.

</details>


### [161] [Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features](https://arxiv.org/abs/2511.20469)
*Ben Hamscher,Arnold Brosch,Nicolas Binninger,Maksymilian Jan Dejna,Kira Maag*

Main category: cs.CV

TL;DR: 提出一种基于视频姿态估计的轻量级舞蹈风格分类框架，通过时空与频域特征（受Laban动作分析启发的关节动态+FFT节律特征）在无需复杂模型的情况下实现鲁棒、可解释的风格识别。


<details>
  <summary>Details</summary>
Motivation: 舞蹈风格识别困难在于不同流派在姿态、手势与时序模式上高度相似，传统人类活动识别方法难以稳定区分且成本高。需要低计算量、可解释的表示来捕捉风格细微差异。

Method: 从视频中提取人体姿态估计（重点上半身）。构建受Laban Movement Analysis启发的时空描述符，度量局部关节速度、加速度与角运动及空间协调结构；再引入FFT频域特征编码节奏与周期性运动模式。利用这些手工特征进行分类，无需复杂深度架构。

Result: 该方法以较低计算开销实现对多种舞蹈风格的稳健分类，表现表明所提可解释运动表示能有效捕捉风格细微差异。

Conclusion: 可解释、轻量的时空+频域特征（结合LMA理念与FFT）足以在无需复杂模型的前提下区分舞蹈风格，为低资源环境下的人体动作风格识别提供有效方案。

Abstract: Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories. Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns. This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos. We propose temporal-spatial descriptors inspired by Laban Movement Analysis. These features capture local joint dynamics such as velocity, acceleration, and angular movement of the upper body, enabling a structured representation of spatial coordination. To further encode rhythmic and periodic aspects of movement, we integrate Fast Fourier Transform features that characterize movement patterns in the frequency domain. The proposed approach achieves robust classification of different dance styles with low computational effort, as complex model architectures are not required, and shows that interpretable motion representations can effectively capture stylistic nuances.

</details>


### [162] [Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification](https://arxiv.org/abs/2511.20474)
*Akshit Pramod Anchan,Jewelith Thomas,Sritama Roy*

Main category: cs.CV

TL;DR: 研究提出并评估三模块感知架构：眼睛状态检测CNN、面部表情识别深度CNN、基于语音的LSTM说话人识别；在各自数据集上准确率约93.0%、97.8%、96.89%，为资源受限辅助设备的实时多模态融合奠定基础。


<details>
  <summary>Details</summary>
Motivation: 辅助技术需同时处理视觉与听觉信息，但在资源受限设备上实现高可靠、可扩展的多模态感知仍具挑战。作者希望验证通过轻量、任务专用且可独立升级的模块化架构，能否在关键感知任务上达到高精度，并为后续实时多模态融合打底。

Method: 设计三个独立模型模块：1) 基于CNN的眼睛开闭/注意-疲劳检测，使用Eyes Image数据集；2) 深层CNN进行面部表情识别，使用FER2013；3) 基于LSTM的语音说话人识别，使用自定义音频数据集。分别训练与评测，基于标准准确率指标进行基准测试。

Result: 三模块在对应数据集上分别获得93.0%（眼睛状态）、97.8%（表情识别）、96.89%（说话人识别）的准确率。

Conclusion: 轻量、领域专用的独立感知模块在离散任务上能达到高保真度，验证了在资源受限辅助设备中采用模块化设计的可行性，并为未来的实时多模态集成提供了可靠基础。

Abstract: Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.

</details>


### [163] [A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences](https://arxiv.org/abs/2511.20501)
*Muhammad Irfan,Nasir Rahim,Khalid Mahmood Malik*

Main category: cs.CV

TL;DR: 提出一种面向DSA脑血管分割的物理启发损失(PIL)，把预测与真值边界的相互作用建模为弹性过程，显著提升边界连贯性与F1/敏感度，并在多种网络与两大数据集上稳定优胜。


<details>
  <summary>Details</summary>
Motivation: 传统分割损失多依赖像素重叠(CE/Dice等)，忽视血管边界的几何连续性与物理一致性，易产生断裂、锯齿或不稳定的管状预测；需要一种能显式约束边界平滑与结构一致性的机制。

Method: 设计Physics-Informed Loss：受材料物理中位错/弹性相互作用启发，将预测与真值轮廓的偏差视为弹性能，构建物理正则项以鼓励平滑的等高线演化与结构一致；将PIL无缝融入U-Net、U-Net++、SegFormer、MedFormer中进行端到端训练，并与CE、Dice、Active Contour、Surface loss等对比。

Result: 在DIAS与DSCA两公开基准上，PIL在敏感度、F1分数与边界连贯性上均显著优于对照损失；在多架构上均获得一致增益，表现更稳定、更鲁棒。

Conclusion: 将基于物理的边界相互作用引入深度网络可显著提升动态血管造影分割的精度与鲁棒性，尤其改善细小血管几何与边界一致性；代码开源，具备可复现性与可扩展性。

Abstract: Accurate extraction and segmentation of the cerebral arteries from digital subtraction angiography (DSA) sequences is essential for developing reliable clinical management models of complex cerebrovascular diseases. Conventional loss functions often rely solely on pixel-wise overlap, overlooking the geometric and physical consistency of vascular boundaries, which can lead to fragmented or unstable vessel predictions. To overcome this limitation, we propose a novel \textit{Physics-Informed Loss} (PIL) that models the interaction between the predicted and ground-truth boundaries as an elastic process inspired by dislocation theory in materials physics. This formulation introduces a physics-based regularization term that enforces smooth contour evolution and structural consistency, allowing the network to better capture fine vascular geometry. The proposed loss is integrated into several segmentation architectures, including U-Net, U-Net++, SegFormer, and MedFormer, and evaluated on two public benchmarks: DIAS and DSCA. Experimental results demonstrate that PIL consistently outperforms conventional loss functions such as Cross-Entropy, Dice, Active Contour, and Surface losses, achieving superior sensitivity, F1 score, and boundary coherence. These findings confirm that the incorporation of physics-based boundary interactions into deep neural networks improves both the precision and robustness of vascular segmentation in dynamic angiographic imaging. The implementation of the proposed method is publicly available at https://github.com/irfantahir301/Physicsis_loss.

</details>


### [164] [DesignPref: Capturing Personal Preferences in Visual Design Generation](https://arxiv.org/abs/2511.20513)
*Yi-Hao Peng,Jeffrey P. Bigham,Jason Wu*

Main category: cs.CV

TL;DR: 提出DesignPref数据集，揭示专业设计师在UI生成偏好上显著不一致，并表明个性化建模优于多数投票式聚合，对个体偏好预测更准且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有对生成式模型（LLMs、扩散模型）在视觉设计领域的微调与评测多依赖人类偏好标注，但视觉设计偏好高度主观与个体化，导致聚合标注可能无法代表个人真实偏好。需要一个数据与方法框架来系统研究并建模个体化设计偏好。

Method: 构建DesignPref：包含12k对UI设计生成的成对比较，由20位专业设计师提供多级偏好评分与自然语言理由。量化一致性（Krippendorff’s alpha，二元偏好为0.25），分析分歧来源；比较多数投票训练的聚合评审模型与多种个性化策略（包括针对设计师微调、以及在RAG管线中纳入设计师特定注释）。

Result: 发现设计师间存在显著分歧；多数投票训练的聚合评审模型对个人偏好的拟合较差。个性化模型在预测个体设计师偏好上稳定优于聚合基线，即便只使用其1/20的数据量也能取得更好表现。

Conclusion: 个性化建模对于视觉设计偏好评估至关重要。DesignPref为研究个体化设计品味提供首个数据集，证明以个体数据进行微调或RAG个性化能显著提升对个人偏好的预测效果，优于传统的多数投票聚合方式。

Abstract: Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.

</details>


### [165] [AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs](https://arxiv.org/abs/2511.20515)
*Kuniaki Saito,Risa Shinoda,Shohei Tanaka,Tosho Hirasawa,Fumio Okura,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: AlignBench 提出一种通过详细图文对来评估图文对齐的新基准，揭示当前VLM/CLIP评估器在细粒度一致性上失效（盲目、前序偏置和自偏好）。


<details>
  <summary>Details</summary>
Motivation: 现有对齐评测多依赖规则扰动或短描述，难以反映细粒度的视觉-语言对齐质量，需更贴近真实生成场景、可直接评价VLM作为对齐判别器的能力。

Method: 构建 AlignBench：从多种图生文与文生图模型收集详细图文对；以句子级标注真伪/正确性；将这些样本输入各类解码式VLM/CLIP评估器，衡量它们对细粒度对齐的判别能力与误差模式。

Result: 大规模基准实验得到三点：1）CLIP系模型（含面向组合理解者）对细粒度对齐几乎“失明”；2）判别器对序列早期句子系统性高打分（前序偏置）；3）明显自偏好，倾向给自身生成更高分，导致检测性能下降。

Conclusion: 句级标注的细粒度对齐评测揭示主流VLM/CLIP作为对齐判别器存在系统缺陷；AlignBench为更可靠的图文对齐评估提供了新指标与诊断工具，提示需抑制前序与自偏好并提升细粒度对齐识别能力。

Abstract: Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.

</details>


### [166] [HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2511.20520)
*Xiang Wang,Zhifei Zhang,He Zhang,Zhe Lin,Yuqian Zhou,Qing Liu,Shiwei Zhang,Yijun Li,Shaoteng Liu,Haitian Zheng,Jason Kuen,Yuehuan Wang,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: HBridge 提出一种非对称“H”型架构，将理解专家（LLM）与生成专家（扩散模型）以选择性中层桥接方式融合，减少40%+共享注意力，提升效率与生成质量，并通过语义重建token强化跨模态一致性。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型多采用对称的Mixture-of-Transformers（MoT）设计（如 BAGEL、LMFusion），为初始化与融合方便而镜像两类专家，但忽视文本与图像等模态先验与表示差异，导致融合低效和次优性能。

Method: 提出HBridge：1）非对称H形拓扑，让各自专家保留各自模态先验；2）选择性桥接，仅在中间层通过共享注意力连接，浅层与深层解耦以保持模态特异表示，较密集融合减少40%+注意力共享；3）加入“语义重建token”，显式引导生成专家重建目标图像的视觉语义token，从而增强跨模态语义对齐。

Result: 在多项基准上取得显著性能提升（文中声称更高生成质量与效率），优于采用对称MoT与密集共享注意力的先进方法。

Conclusion: 异构专家应采用非对称、选择性中层桥接与语义重建引导的统一范式，以更好利用各自模态先验，提升跨模态一致性、生成质量与推理效率。

Abstract: Recent unified models integrate understanding experts (e.g., LLMs) with generative experts (e.g., diffusion models), achieving strong multimodal performance. However, recent advanced methods such as BAGEL and LMFusion follow the Mixture-of-Transformers (MoT) paradigm, adopting a symmetric design that mirrors one expert to another for convenient initialization and fusion, which remains suboptimal due to inherent modality discrepancies. In this work, we propose HBridge, an asymmetric H-shaped architecture that enables heterogeneous experts to optimally leverage pretrained priors from their respective modality domains. Unlike prior dense fusion strategies that straightforwardly connect all layers between experts via shared attention, HBridge selectively bridges intermediate layers, reducing over 40% attention sharing, which improves efficiency and enhances generation quality. Shallow and deep layers, which capture modality-specific representations, are decoupled, while mid-layer bridging promotes semantic alignment. To further strengthen cross-modal coherence, we introduce semantic reconstruction tokens that explicitly guide the generative expert to reconstruct visual semantic tokens of the target image. Extensive experiments across multiple benchmarks demonstrate the effectiveness and superior performance of HBridge, establishing a new paradigm for unified multimodal generation.

</details>


### [167] [Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos](https://arxiv.org/abs/2511.20525)
*Yayuan Li,Aadit Jain,Filippos Bellos,Jason J. Corso*

Main category: cs.CV

TL;DR: 提出MATT任务与MisEngine数据引擎、MisFormer模型，实现对自我视角视频中人类错误的细粒度归因（是什么、何时、何处），并在新构建的大规模数据集上显著优于多种强基线。


<details>
  <summary>Details</summary>
Motivation: 现有“错误理解”研究输出粗糙，难以回答错误究竟违反了指令的哪一部分、何时不可挽回、以及空间上出现在哪里；缺乏大规模、可归因的标注数据来系统研究这些问题。

Method: 1) 定义Mistake Attribution (MATT)任务：对指令-视频对进行三维度归因——语义角色(what)、不可回返点PNR(when)、PNR帧中的空间位置(where)。2) 设计MisEngine数据引擎：从现有自我视角数据集中自动构造带归因标签的错误样本，继承并整合其原有标注，生成EPIC-KITCHENS-M与Ego4D-M两大数据集。3) 提出MisFormer：统一的基于注意力的模型，联合学习语义、时间与空间归因，使用MisEngine监督进行训练。

Result: 在新构建的数据集与既有基准上，MisFormer在视频-语言理解、时间定位、手-物体交互以及错误检测等多类强基线上取得领先表现。

Conclusion: 细粒度错误归因可以通过统一的注意力模型在大规模自动构建的数据上有效学习；所提出的数据与方法为理解人类在自我视角任务中的错误提供了强大基线与资源。

Abstract: We introduce Mistake Attribution (MATT), a task for fine-grained understanding of human mistakes in egocentric video. Unlike prior mistake understanding work, which lacks fine-grained output, MATT concretely attributes mistakes to the input instruction text or the attempt video. MATT determines what part of the instruction is violated (semantic role), when the deviation becomes irreversible (the Point-of-No-Return, PNR), and where the mistake appears in the PNR frame. We develop MisEngine, a data engine that automatically constructs attribution-rich mistake samples from existing datasets and inherits their annotations. Applied to large egocentric corpora, MisEngine yields EPIC-KITCHENS-M and Ego4D-M, two datasets that are up to two orders of magnitude larger than prior mistake datasets. We then present MisFormer, a unified attention-based model for mistake attribution across semantic (what), temporal (when), and spatial (where) dimensions, trained using MisEngine supervision. Experiments on our new datasets and prior benchmarks show that MisFormer outperforms strong video-language, temporal localization, hand-object interaction, and mistake-detection baselines.

</details>


### [168] [Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation](https://arxiv.org/abs/2511.20541)
*Andrea Ranieri,Giorgio Palmieri,Silvia Biasotti*

Main category: cs.CV

TL;DR: 对文化遗产裂缝检测进行语义分割对比研究：基于U‑Net与不同CNN编码器，在OmniCrack30k上量化评估并在真实未标注文物图像上做OOD质性验证，显示良好泛化。


<details>
  <summary>Details</summary>
Motivation: 文化遗产保护需要高效、客观、可扩展的裂缝识别，而人工巡检耗时且主观；现有方法泛化性与精细分割能力不足，需系统比较不同编码器的U‑Net以找到更优权衡。

Method: 构建多种U‑Net变体，替换编码器为不同主流CNN；在OmniCrack30k测试集上用mIoU、Dice、Jaccard进行量化比较；并在未标注的真实文物裂缝图像上做分布外质性评估，观察可迁移性与细粒度分割表现。

Result: 不同CNN编码器的U‑Net在细粒度裂缝分割上均取得有竞争力的指标；在未见过的文物场景上仍能产生合理的裂缝掩码，显示出一定的跨域泛化能力。

Conclusion: 选用合适的CNN编码器可提升U‑Net在裂缝语义分割中的表现；即使未在文物数据上训练，这些模型对文物裂缝仍具可迁移性，为文化遗产场景的自动化检测提供了可行路径。

Abstract: This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.

</details>


### [169] [New York Smells: A Large Multimodal Dataset for Olfaction](https://arxiv.org/abs/2511.20544)
*Ege Ozguroglu,Junbang Liang,Ruoshi Liu,Mia Chiquier,Michael DeTienne,Wesley Wei Qian,Alexandra Horowitz,Andrew Owens,Carl Vondrick*

Main category: cs.CV

TL;DR: 提出“New York Smells”数据集：7,000条“气味-图像”配对，3,500个对象，覆盖室内外；并设立三项基准任务，展示视觉数据可促进跨模态嗅觉表征学习，所学表征优于手工特征。


<details>
  <summary>Details</summary>
Motivation: 嗅觉对动物认知至关重要，但机器嗅觉受限，核心瓶颈是缺乏在自然环境中采集的多样化、跨模态训练数据。现有嗅觉数据集规模小、对象单一，难以支持有效的表征学习与下游任务评估。

Method: 构建大规模“在野外”采集的图像-嗅觉信号配对数据集（7,000对，3,500对象，室内外混合），并设计三类基准任务：1) 嗅觉到图像跨模态检索；2) 仅凭嗅觉识别场景、物体与材料；3) 细粒度区分草类物种。进行跨模态训练，用视觉数据辅助嗅觉表征学习，并与手工嗅觉特征进行对比实验。

Result: 在提出的数据集与基准上，跨模态训练成功学习到可迁移的嗅觉表征：视觉辅助可显著提升任务表现；所学表征在检索与分类任务上优于广泛使用的手工特征。

Conclusion: 大规模、在野外采集的图像-嗅觉配对数据能有效推动机器嗅觉研究；视觉信号为嗅觉表征提供有力先验，促成更强的跨模态理解。数据集与基准为后续研究奠定基础。

Abstract: While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.

</details>


### [170] [Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning](https://arxiv.org/abs/2511.20549)
*Guanjie Chen,Shirui Huang,Kai Liu,Jianchen Zhu,Xiaoye Qu,Peng Chen,Yu Cheng,Yifu Sun*

Main category: cs.CV

TL;DR: 提出Flash-DMD：在扩散模型的少步采样场景下，通过高效的时间步蒸馏与同时进行的RL微调，实现更快收敛与更高质量生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样慢；现有时间步蒸馏需大量训练且画质下降；对蒸馏后模型做RL微调容易不稳定与奖励黑客。需要一种既高效又稳定、能提升质量与偏好对齐的方法。

Method: 1) 时间步感知蒸馏：设计高效的timestep-aware策略，用更少训练成本提升真实感，相比DMD2只用约2.1%训练开销仍更优。2) 联合训练：在持续进行蒸馏的同时引入RL目标联合优化；把稳定、明确的蒸馏损失作为正则，抑制RL训练中的不稳定与策略坍塌。适用于score-based与flow-matching模型，针对少步采样。

Result: 相较现有方法，收敛显著更快；在少步采样下达到SOTA：更高视觉质量、更强人偏好一致性与文图对齐；训练成本远低于DMD2（~2.1%）。

Conclusion: Flash-DMD给出一种高效、稳定的扩散模型训练范式：用时间步感知蒸馏加上与RL的联合优化，在显著降低成本的同时提升生成质量与稳定性，适合少步快速采样的高保真生成。

Abstract: Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.

</details>


### [171] [Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward](https://arxiv.org/abs/2511.20561)
*Yuwei Niu,Weiyang Jin,Jiaqi Liao,Chaoran Feng,Peng Jin,Bin Lin,Zongjian Li,Bin Zhu,Weihao Yu,Li Yuan*

Main category: cs.CV

TL;DR: 论文提出UniSandbox框架与合成可控数据，揭示统一多模态模型在“理解→生成”之间存在显著鸿沟，主要体现在推理生成与知识迁移两方面；显式CoT与自训练可缩小推理差距，CoT也有助于知识检索与迁移；查询式架构天然具备类CoT属性。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型快速发展，但尚不清楚感知/理解能力是否真正转化为高质量生成。现有评测常混淆因素多、数据泄漏风险高，难以精细分析理解与生成之间的因果关系与机制，需要一个解耦、可控、可追溯的评测平台。

Method: 提出解耦评测框架UniSandbox：构建可控的合成数据集，分别评估“理解模块”和“生成模块”，避免数据泄漏并能精细操控变量；设计两类任务维度—推理生成与知识迁移；在推理任务中比较显式CoT、隐式推理与自训练策略；在知识迁移任务中考察CoT对新知识检索与生成的影响，并分析查询式（query-based）架构的隐性类CoT特性。

Result: 发现显著的“理解-生成”差距：模型即便在理解侧表现良好，生成时仍显著退化。推理任务上，给理解模块加入显式CoT可明显缩小差距，自训练能将这种能力内化，使生成阶段出现隐式推理。知识迁移任务上，CoT有助于在生成时检索新学知识；查询式架构天然呈现类CoT迹象，影响知识迁移效果。

Conclusion: UniSandbox揭示并量化了统一多模态模型在理解与生成之间的结构性鸿沟，并提供证据表明显式/隐式CoT与自训练可有效缓解推理差距，CoT对知识检索与迁移亦有正面作用；查询式架构具有内在类CoT特性。该框架为今后设计真正弥合理解与生成的统一架构与训练策略提供了初步指导。

Abstract: Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox

</details>


### [172] [PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding](https://arxiv.org/abs/2511.20562)
*Haoze Zhang,Tianyu Huang,Zichen Wan,Xiaowei Jin,Hongzhi Zhang,Hui Li,Wangmeng Zuo*

Main category: cs.CV

TL;DR: PhysChoreo提出从单张图像生成具备物理可控性与逼真动态的视频的两阶段框架：先重建物体部件级静态物理属性，再通过时间指令与可编辑物理模拟合成视频，显著提升物理真实感与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成虽然画面精细，但缺乏明确的物理可控性与可信的物理行为；以物理渲染指导的方法难以准确建模复杂物性，且在长时序控制上受限。需要一种既能从静态图像推断物理属性，又能稳定生成长时序物理一致动态的方案。

Method: 两阶段架构：1) 部件感知的物理属性重建，从单张图像估计场景中各对象（及其部件）的静态初始物理参数（如质量、材质、摩擦、可变形性等）。2) 时间指令驱动与可编辑的物理模拟，将用户/模型的时序指导信号与估计物性耦合，进行可控的多步物理仿真，再渲染为高质量视频。强调可编辑性（可修改物性与动作指令）与长时序稳定性。

Result: 在多项评测指标上优于现有方法，生成的视频展现更丰富的动态行为与更高的物理真实感；能够在多样控制条件下保持一致的物理可控性。

Conclusion: PhysChoreo从单图出发，实现对视频物理行为的显式可控与高真实感，克服以往方法在复杂物性建模与长时序控制上的不足，为物理驱动的视频生成提供通用且可编辑的解决方案。

Abstract: While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.

</details>


### [173] [A Reason-then-Describe Instruction Interpreter for Controllable Video Generation](https://arxiv.org/abs/2511.20563)
*Shengqiong Wu,Weicai Ye,Yuanxing Zhang,Jiahao Wang,Quande Liu,Xintao Wang,Pengfei Wan,Kun Gai,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出ReaDe：面向视频扩散Transformer的通用、模型无关的指令解释器，将简短含糊的用户输入转化为可控生成所需的精细规范，显著提升指令一致性、描述准确度与下游视频质量，并对复杂推理与未见指令具强泛化。


<details>
  <summary>Details</summary>
Motivation: 现实用户给出的提示往往简短含糊、包含组合性复杂的意图，而现有视频生成模型训练时依赖冗长、细粒度的提示，导致意图-输出不匹配与可控性不足。需要一个桥接层，将原始自然语言意图解析成可执行的细化指导，从而提升可控生成与对齐。

Method: 提出Reason-then-Describe范式的解释器ReaDe：先进行需求解析与消歧（reason），再生成结构化且细粒度的指导性描述（describe）。训练采用两阶段：1) 基于“推理增强监督”的教师式训练，提供逐步解析轨迹与密集caption，教会模型解析与细化；2) 设计多维奖励的指令-描述质量评估器，进行稳定的反馈驱动优化，使生成的自然风格caption既可控又流畅。该方法对下游视频生成模型保持模型无关、可即插即用。

Result: 在单条件与多条件控制场景中，ReaDe均提升了指令忠实度、caption准确性与生成视频质量；并在需要复杂推理及未见类型指令上表现出良好泛化能力。

Conclusion: ReaDe为将用户意图准确解释并对齐到可控视频生成提供了实用路径：通过先解析后描述的范式与两阶段训练，显著缓解意图-输出错配，提升可控性与一致性，且具备跨模型与场景的通用性。

Abstract: Diffusion Transformers have significantly improved video fidelity and temporal coherence, however, practical controllability remains limited. Concise, ambiguous, and compositionally complex user inputs contrast with the detailed prompts used in training, yielding an intent-output mismatch. We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation. We train ReaDe via a two-stage optimization: (i) reasoning-augmented supervision imparts analytic parsing with stepwise traces and dense captions, and (ii) a multi-dimensional reward assigner enables stable, feedback-driven refinement for natural-style captions. Experiments across single- and multi-condition scenarios show consistent gains in instruction fidelity, caption accuracy, and downstream video quality, with strong generalization to reasoning-intensive and unseen inputs. ReaDe offers a practical route to aligning controllable video generation with accurately interpreted user intent. Project Page: https://sqwu.top/ReaDe/.

</details>


### [174] [DINO-Tok: Adapting DINO for Visual Tokenizers](https://arxiv.org/abs/2511.20565)
*Mingkai Jia,Mingxiao Li,Liaoyuan Fan,Tianxing Shi,Jiaxin Guo,Zeming Li,Xiaoyang Guo,Xiao-Xiao Long,Qian Zhang,Ping Tan,Wei Yin*

Main category: cs.CV

TL;DR: 提出DINO-Tok：基于DINO的视觉分词器，融合浅层细节与深层语义，并通过全局PCA重加权稳定高维VQ，在ImageNet 256×256上实现SOTA重建（PSNR 28.54 AE / 23.98 VQ）。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器多从零训练，难以在高维潜空间同时兼顾语义对齐与重建保真，且VQ易丢失关键信息并发生码本坍塌。需要一种能充分利用强大预训练视觉表征、并在量化阶段保持信息完备性的分词器。

Method: 以DINO为骨干，将多层次（浅层细节+深层语义）特征统一到信息完备的潜空间；在高维VQ中引入全局PCA重加权，对各维重要性进行归一与放大，缓解信息丢失和码本坍塌，从而稳定量化；用于自编码与VQ建模的统一视觉Tokenizer。

Result: 在ImageNet 256×256上，重建性能达PSNR 28.54（自编码）与23.98（VQ），显著优于以往分词器，并可与百亿级数据训练模型（如Hunyuan、万）相媲美。

Conclusion: 将DINO等强预训练视觉模型适配为Tokenizer，可获得语义对齐且高保真的潜表示；全局PCA重加权有效稳定高维VQ，为下一代视觉生成模型提供更优的视觉分词基础。

Abstract: Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.

</details>


### [175] [VQ-VA World: Towards High-Quality Visual Question-Visual Answering](https://arxiv.org/abs/2511.20573)
*Chenhui Gou,Zilong Chen,Zeyu Wang,Feng Li,Deyao Zhu,Zicheng Duan,Kunchang Li,Chaorui Deng,Hongyi Yuan,Haoqi Fan,Cihang Xie,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 论文提出并开放一整套用于“视觉问—视觉答”(VQ-VA)的训练数据与评测基准，使开源模型也能在看到问题后直接生成图像作为答案，并显著缩小与专有系统的差距。


<details>
  <summary>Details</summary>
Motivation: 专有系统（如 NanoBanana、GPT-Image）已展现出根据视觉问题直接生成图像作答的能力，但开源社区缺少高质量数据、系统化评测与可复现的训练流水线，难以复现与对比。作者希望通过数据中心的方法与标准化评测推动开源 VQ-VA 研究。

Method: 1) 提出 VQ-VA World：一个以“代理式流水线”为核心的大规模、定向数据构造框架，基于网页级部署自动抓取并筛选约 180 万条高质量图文交错样本用于训练；2) 发布 IntelligentBench：由人工策划的评测集，从世界知识、设计知识与推理三个维度系统评估 VQ-VA 能力；3) 在开源模型 LightFusion 上用所构建数据进行训练与对比。

Result: 使用 VQ-VA World 数据训练后，LightFusion 在 IntelligentBench 上得分 53.06，远高于开源基线（vanilla LightFusion 7.78、UniWorld-V1 1.94），并将与专有系统（NanoBanana 81.67、GPT-Image 82.64）的差距显著缩小。

Conclusion: 数据驱动、可扩展的 VQ-VA 数据构造与标准化评测能大幅提升开源模型的图像生成作答能力。作者同时开源模型权重、数据与流水线，期望推动该方向的后续研究与迭代。

Abstract: This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.

</details>


### [176] [The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment](https://arxiv.org/abs/2511.20614)
*Ziheng Ouyang,Yiren Song,Yaoli Liu,Shihao Zhu,Qibin Hou,Ming-Ming Cheng,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出ImageCritic：一种参考引导的后期编辑方法，利用注意力对齐损失与细节编码器，在多轮/局部编辑框架中自动检测并修复生成图像的细粒度不一致，显著提升各类定制生成任务的细节一致性。


<details>
  <summary>Details</summary>
Motivation: 定制图像生成尽管能根据参考图像生成目标，但常出现细粒度细节（纹理、标志、局部结构）不一致或失真，现有方法难以稳定保真。为此需要一种能在生成后自动识别并修正不一致的机制。

Method: 1) 构建参考-退化-目标三元组数据集：借助VLM选择与显式退化策略，模拟主流生成模型常见的不准确/不一致问题；2) 模型机制分析：基于对注意力与内部表征的研究，设计注意力对齐损失，使生成与参考在关键区域的注意力分布对齐；3) 细节编码器：提取参考的细粒度纹理/结构特征指导修复；4) 代理式工作流：在Agent框架内自动检测不一致，支持多轮与局部编辑以逐步校正复杂场景。

Result: 在多种定制生成场景中大幅缓解细节相关问题，相比现有方法取得显著性能提升（实验广泛，涵盖复杂场景与多轮/局部编辑）。

Conclusion: 参考引导的后期编辑（ImageCritic）能有效对齐注意力并注入细节特征，配合Agent化多轮局部修复，系统性提升生成图像的细节一致性，优于现有方案。

Abstract: Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.

</details>


### [177] [Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities](https://arxiv.org/abs/2511.20615)
*Seyede Niloofar Hosseini,Ali Mojibi,Mahdi Mohseni,Navid Arjmand,Alireza Taheri*

Main category: cs.CV

TL;DR: 用BLSTM与Transformer两类时序深度网络，基于前25%动作与任务条件预测剩余75%全身3D姿态；提出长度一致性约束的新损失函数，显著降低四肢预测误差；Transformer长程预测优于BLSTM（RMSE≈47 mm，长程提升约58%）。


<details>
  <summary>Details</summary>
Motivation: 在手工搬运等动态负载抓取任务中，提前预测全身姿态有助于人机协作、安全评估与生物力学分析。既有方法难以同时捕捉长程时序依赖并保持人体各段长度不变性，导致预测漂移与解剖学不一致。

Method: 收集20名健康男性在多种抬举/搬运技术下完成204次任务的3D全身运动学数据。以手-负载位置、抬举/搬运方式、身高体重、以及前25%时段的全身3D坐标为输入，训练BLSTM与Transformer模型预测剩余75%姿态。提出新损失函数在训练中强制各身体段长度恒定（几何一致性约束），以提升解剖学合理性与精度。

Result: 加入长度一致性损失后，手臂与腿部模型的预测误差分别降低约8%与21%。整体上Transformer优于BLSTM，在长时预测上约提升58%，达成全身RMSE约47.0 mm。

Conclusion: 时序深度网络可有效预测动态负载抓取中的全身3D姿态；引入身体段长度一致性约束能显著提升解剖学合理性与精度。Transformer在长程依赖与长期预测上表现更佳，适合用于人机协作、风险评估与动作理解等场景。

Abstract: This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.

</details>


### [178] [Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI](https://arxiv.org/abs/2511.20620)
*Xinhao Liu,Jiaqi Li,Youming Deng,Ruxin Chen,Yingjia Zhang,Yifei Ma,Li Guo,Yiming Li,Jing Zhang,Chen Feng*

Main category: cs.CV

TL;DR: Wanderland 提出一个从真实到仿真的高保真城市环境评测框架，用多传感器采集与精确几何重建，显著缩小视听与交互的 sim-to-real 差距，支持可复现的闭环评测、导航策略训练与3D重建/NVS基准。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能（如视觉导航）的闭环评测难以复现，依赖的模拟器在视觉与几何上与真实差距大；尽管 video-3DGS 方便获取开放世界场景，但存在显著 sim-to-real gap，难以作为可靠基准。

Method: 构建“Wanderland”真实到仿真的流水线：多传感器同步采集（多模态）、可靠重建、高精度几何与稳健视图合成；基于该流水线整理多样的城市场景（室内外）数据集，并进行系统性消融与对比。

Result: 实验证明：仅依赖图像的方法在开放世界场景中扩展性差；几何质量直接影响新视角合成；上述问题进一步恶化导航策略的学习与评测稳定性。Wanderland 数据与管线在导航、3D重建与NVS基准上表现为更可靠的测试床。

Conclusion: Wanderland 为开放世界具身智能提供了可复现、高保真的评测与训练基础设施，既能缩小仿真与真实差距，又能统一评测导航、重建与视图合成模型，奠定可复现研究的新基线。

Abstract: Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.

</details>


### [179] [ShapeGen: Towards High-Quality 3D Shape Synthesis](https://arxiv.org/abs/2511.20624)
*Yangguang Li,Xianglong He,Zi-Xin Zou,Zexiang Liu,Wanli Ouyang,Ding Liang,Yan-Pei Cao*

Main category: cs.CV

TL;DR: 提出ShapeGen，实现高质量单图像到3D形状生成，结合改进的3D表示与监督、提升分辨率以及线性Transformer，显著提升细节与拓扑完整性，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像到3D生成仍存在细节匮乏、表面过度平滑、薄壳结构破碎等问题，难以满足艺术家级资产生产需求，需要一种能生成可无缝融入3D管线的高保真方法。

Method: 提出ShapeGen，从三方面改进：1）3D表示与监督改良（更强的体素/网格/隐式表示及三维一致监督）；2）分辨率放大（更高分辨率重建与渲染管线以保留细节）；3）采用线性Transformer提高长序列建模与效率；整体框架实现从单张图像到可用3D资产的端到端生成。

Result: 通过广泛实验，方法在细节保真、表面光顺与拓扑完整性方面明显优于现有方法，生成资产可直接用于3D生产流程，达成新的SOTA表现。

Conclusion: 综合多项改进带来协同增益，ShapeGen在单图像到3D生成上实现显著跃升，生成结果更高质量、可生产、可广泛应用。

Abstract: Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.

</details>


### [180] [MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models](https://arxiv.org/abs/2511.20629)
*Chieh-Yun Chen,Zhonghao Wang,Qi Chen,Zhifan Ye,Min Shi,Yue Zhao,Yinan Zhao,Hui Qu,Wei-An Lin,Yiru Shen,Ajinkya Kale,Irfan Essa,Humphrey Shi*

Main category: cs.CV

TL;DR: 提出MapReduce LoRA与RaTE两种方法，在多奖励（多偏好）场景下显著减少“对齐税”，在图像、视频与语言多模态任务上取得SOTA提升。


<details>
  <summary>Details</summary>
Motivation: RLHF在多奖励同时优化时常出现此消彼长的“对齐税”，某一维度提升会牺牲其他维度；需要能在多偏好间兼顾、可组合、且训练与推理高效的通用方法。

Method: 1) MapReduce LoRA：为每个偏好/奖励并行训练其对应的LoRA专家（Map）；随后迭代式地将这些专家合并到共享基座模型（Reduce），形成逐步精炼的多偏好基座。2) RaTE（Reward-aware Token Embedding）：为不同奖励学习独立的token embedding；推理时可按所需偏好将这些嵌入进行组合，实现灵活的偏好控制与权衡。

Result: 在文生图（SD3.5 Medium、FLUX.1-dev）上：GenEval +36.1%/32.7%，PickScore +4.6%/4.3%，OCR +55.7%/67.1%。在文生视频（HunyuanVideo）上：视觉质量 +48.1%，运动质量 +90.0%。在语言任务Helpful Assistant（Llama-2 7B）上：helpful +43.4%，harmless +136.7%。

Conclusion: 两种互补技术实现高效多偏好对齐，缓解多奖励冲突并在多模态基座上取得SOTA；方法具备可组合与可控性，适合扩展到更多偏好与模态。

Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.

</details>


### [181] [iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation](https://arxiv.org/abs/2511.20635)
*Zhoujie Fu,Xianfang Zeng,Jinghong Lan,Xinyao Liao,Cheng Chen,Junyi Chen,Jiacheng Wei,Wei Cheng,Shiyu Liu,Yunuo Chen,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: iMontage 将强时序一致性的视频先验注入到图像域，统一处理可变长度的图像集，实现多种“多入多出”的生成与编辑任务，并显著提升跨图像一致性与动态表现。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视频模型具有出色的时序一致性，但受连续视频数据分布限制，动态范围与内容多样性受限；而海量图像数据内容丰富却缺乏时序连贯。作者希望把图像数据的多样性注入视频模型的连贯先验，获得既自然过渡又具更广动态与多任务能力的图像集生成框架。

Method: 提出 iMontage：将强大的视频生成模型以最小侵入方式适配为“可变长度图像集”的生成与编辑器。核心包括：1) 统一输入输出为变长图像集的接口与目标，使多种任务（生成、风格/内容编辑、合成）在同一框架下表述；2) 轻量参数适配策略，避免破坏原有运动先验；3) 专门的数据筛选与训练范式，将丰富的图像数据融入训练，使模型获得广泛图像操控能力，同时保持时序（跨图像）一致性。

Result: 在多种主流的多输入多输出任务上取得领先表现，既保持强跨图像上下文一致性，又能生成超出常规范围的“动态”变化与自然过渡的图像集。

Conclusion: iMontage 证明了将视频先验迁移到图像集生成的有效性：通过轻量适配与数据策划，可在不损伤运动先验的前提下，统一并提升多任务图像生成/编辑，获得更强的跨图像一致性与动态表现。

Abstract: Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.

</details>


### [182] [MotionV2V: Editing Motion in a Video](https://arxiv.org/abs/2511.20640)
*Ryan Burgert,Charles Herrmann,Forrester Cole,Michael S Ryoo,Neal Wadhwa,Andrey Voynov,Nataniel Ruiz*

Main category: cs.CV

TL;DR: 提出一种基于稀疏轨迹编辑的精确运动控制视频编辑方法，并通过“运动反事实”数据与运动条件扩散模型实现高质量、可起始于任意时间点并自然传播的编辑，用户偏好超65%。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型虽具高保真与一致性，但视频编辑、尤其是精确运动控制仍困难；已有工作多用于文本生成或图像动画的运动可控性，而对现有视频的精确运动编辑研究不足。

Method: 从输入视频提取稀疏轨迹，直接在轨迹空间进行编辑，将输入与输出轨迹的偏差定义为“运动编辑”；构建“运动反事实”数据（同内容、不同运动的视频对），并在此数据上微调运动条件的视频扩散模型，使模型在给定运动编辑条件下生成编辑后视频。

Result: 实现可在任意时间戳开始并自然向前传播的运动编辑；在人类四方对比实验中，相较于现有方法总体获得超过65%的偏好率。

Conclusion: 稀疏轨迹级的精确运动编辑结合运动条件扩散生成，能显著提升视频编辑能力，提供稳定、可控、通用的运动修改范式。

Abstract: While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a "motion edit" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V

</details>


### [183] [Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition](https://arxiv.org/abs/2511.20641)
*Wei Tang,Zuo-Zheng Wang,Kun Zhang,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 提出CAPNET，通过从CLIP文本端显式建模标签相关性并结合图卷积与软提示，在长尾多标签识别上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 长尾多标签数据头尾分布极不平衡，现有方法依赖不充分的样本统计导致尾类相关性不可靠；同时CLIP零样本偏向单标签匹配，不适合多标签，需要一种既能稳健建模多标签关系又能缓解长尾偏置的方案。

Method: 提出Correlation Adaptation Prompt NETwork（CAPNET）：1）从CLIP文本编码器构建标签语义图，使用GCN进行标签感知传播以显式建模相关性；2）引入可学习软提示优化文本嵌入；3）训练时用分布均衡的Focal Loss并进行类感知重加权；4）采用参数高效微调对齐视觉-文本模态，避免对尾类过拟合且不损害头类；5）测试时进行集成提升泛化。

Result: 在VOC-LT、COCO-LT、NUS-WIDE等基准上，通过大量实验与消融显示，CAPNET在多项指标上较现有SOTA取得显著提升。

Conclusion: 显式利用CLIP文本端语义与GCN建模标签相关性、结合长尾重加权与高效微调，可有效提升长尾多标签识别的整体与尾类性能。

Abstract: Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.

</details>


### [184] [Concept-Aware Batch Sampling Improves Language-Image Pretraining](https://arxiv.org/abs/2511.20643)
*Adhiraj Ghosh,Vishaal Udandarao,Thao Nguyen,Matteo Farina,Mehdi Cherti,Jenia Jitsev,Sewoong Oh,Elisa Ricci,Ludwig Schmidt,Matthias Bethge*

Main category: cs.CV

TL;DR: 提出DataConcept与CABS，用概念感知的在线批次采样来替代离线、与概念无关的数据过滤，在CLIP/SigLIP上跨28个基准显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据筛选多为离线静态、依赖模型打分且与概念无关，易引入偏差，且难以针对任务自适应地控制训练数据的概念分布；需要一种能够按目标任务需求动态、可控地选择训练样本的方法。

Method: 1) 构建DataConcept：收集1.28亿图文对，并标注细粒度概念组成（对象、属性、关系等）。2) 提出概念感知批次采样CABS：在训练中按目标概念分布在线组批。给出两种策略：CABS-DM（多样性最大化，覆盖更多概念）与CABS-FM（频次最大化，提高对象多重出现）。方法在CLIP/SigLIP训练流程中无缝集成。

Result: 在28个基准上，CABS显著提升CLIP/SigLIP类模型的下游表现，相比传统离线或黑盒在线策划方法更具竞争力；能根据任务需求定制概念分布，获得高性能开源替代方案。

Conclusion: 概念感知、在线、自适应的数据策划优于离线、与概念无关的方法。DataConcept与CABS为开源社区提供了灵活可控的训练数据采样框架，可按目标任务优化概念分布，提升多模态模型性能。

Abstract: What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.

</details>


### [185] [Vision-Language Memory for Spatial Reasoning](https://arxiv.org/abs/2511.20644)
*Zuntao Liu,Yi Du,Taimeng Fu,Shaoshu Su,Cherie Ho,Chen Wang*

Main category: cs.CV

TL;DR: 提出VLM^2：在仅用2D视频的前提下，通过视图一致的3D感知表示与持久记忆，提高视频空间推理能力，达到了视频-only模型的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在视频空间推理上不及人类，主要因(1) 语义-几何不对齐，难以形成一致的3D理解；(2) 缺乏可持续的记忆，无法在时间维度上保持3D表示与理解。

Method: 提出VLM^2，核心是基于2D视频学习视图一致、具3D意识的表示，并引入双重记忆：工作记忆（滑动窗口，关注近期上下文）与情景记忆（整合并存储关键长期信息），在固定计算成本下进行高效长时程推理。

Result: 在多个基准上对视频-only设定取得SOTA，显著提升视觉-空间智能的表现。

Conclusion: 通过3D一致表示与持久记忆的结合，VLM^2有效缓解语义-几何不对齐与长期依赖问题，推动视频空间推理向更高水平发展。

Abstract: Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.

</details>


### [186] [PixelDiT: Pixel Diffusion Transformers for Image Generation](https://arxiv.org/abs/2511.20645)
*Yongsheng Yu,Wei Xiong,Weili Nie,Yichen Sheng,Shiqiu Liu,Jiebo Luo*

Main category: cs.CV

TL;DR: 提出PixelDiT：无需自编码器，直接在像素空间进行扩散建模的单阶段端到端DiT模型；采用补丁级与像素级双层Transformer，兼顾全局语义与细节；在ImageNet 256上得FID 1.61，并扩展到高分辨率文本生成（1024），GenEval 0.74、DPG-bench 83.5，接近最佳潜空间方法。


<details>
  <summary>Details</summary>
Motivation: 潜空间DiT依赖预训练自编码器，存在重建损失、误差累积和难以联合优化的问题，且像素空间扩散效率和细节保真常受限。需要一种既能保留细节、又能高效训练、并避免两阶段依赖的像素空间方案。

Method: 提出PixelDiT：完全基于Transformer的单阶段像素扩散模型。采用双层设计：1）补丁级DiT捕获全局语义；2）像素级DiT进行细节纹理精炼。强调有效的像素级token建模对像素扩散成功至关重要；在像素空间直接学习扩散过程，并可扩展到文本-图像生成与高分辨率预训练（1024x1024）。

Result: 在ImageNet 256x256上取得FID 1.61，显著优于现有像素生成模型；在文本到图像任务上，1024分辨率预训练后，GenEval 0.74、DPG-bench 83.5，性能逼近顶级潜空间扩散模型。

Conclusion: 通过取消自编码器并在像素空间直接扩散，结合补丁级与像素级Transformer的双层架构，PixelDiT在保持细节与全局语义的同时实现高效训练与卓越性能，缩小了像素扩散与潜空间扩散的性能差距，并具备良好的可扩展性。

Abstract: Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.

</details>


### [187] [3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding](https://arxiv.org/abs/2511.20646)
*Xiaoye Wang,Chen Tang,Xiangyu Yue,Wei-Hong Li*

Main category: cs.CV

TL;DR: 提出在多任务学习中引入跨视角相关（代价体）以注入3D几何一致性，通过轻量共享的Cross-view Module提升分割、深度等密集预测任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有MTL多在2D图像空间建模跨任务关系，导致特征缺乏3D感知，限制场景理解效果；作者认为3D几何一致性对跨任务关联至关重要。

Method: 在MTL网络中加入轻量的跨视角模块（CvM），在任务间共享，用于跨视角信息交换并构建代价体以显式捕获几何一致性；模块与MTL编码器特征结合，适配单视/多视输入，架构无关。

Result: 在NYUv2与PASCAL-Context等数据集上，基于多种现有MTL方法，注入CvM后性能普遍提升，显示几何一致性带来增益。

Conclusion: 通过跨视角代价体为MTL注入3D意识，可作为通用可插拔模块稳健提升多任务密集预测的表现。

Abstract: This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.

</details>


### [188] [Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization](https://arxiv.org/abs/2511.20647)
*Tahira Kazimi,Connor Dunlop,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出DPP-GRPO，将确定性点过程(DPP)与群相对策略优化(GRPO)结合，在文本生成视频中把“多样性”变成显式奖励，从集合层面优化采样，显著提升多样性且不损失一致性与质量。


<details>
  <summary>Details</summary>
Motivation: T2V扩散模型虽在质量与文案对齐上进步明显，但对同一提示多次采样时往往缺乏多样性。需要一种能在不牺牲保真度的前提下覆盖“合理但多样”的视频结果的机制。

Method: 将问题表述为集合级策略优化：一次生成一组候选视频，对该集合计算奖励。用DPP对相似样本实施“边际收益递减”，把多样性转化为可优化的信号；用GRPO在候选组内提供稳定的相对反馈并更新生成策略。方法即插即用、与底模无关，适配WAN与CogVideoX。

Result: 在VBench、VideoScore及人为偏好评测上，较基线显著提升多样性，同时保持提示对齐与感知质量。展示在外观、机位运动、场景结构等维度的多样化提升。并开源代码与包含3万高多样性提示的新基准数据集。

Conclusion: 集合层面的策略优化（DPP+GRPO）可有效解决T2V低多样性难题，在不损质量的前提下系统性提升多样性，并为社区提供可复现工具与数据，推动多样化视频生成研究。

Abstract: While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.

</details>


### [189] [LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight](https://arxiv.org/abs/2511.20648)
*Yunze Man,Shihao Wang,Guowen Zhang,Johan Bjorck,Zhiqi Li,Liang-Yan Gui,Jim Fan,Jan Kautz,Yu-Xiong Wang,Zhiding Yu*

Main category: cs.CV

TL;DR: LocateAnything3D 将3D目标检测转化为VLM的下一个token预测问题，通过“视链（CoS）”先2D再3D的序列化输出，实现开放词汇、无专门头的多目标3D检测，在Omni3D上达SOTA（AP_3D 49.89），超前一基线+15.51，并具零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VLM善于2D开放描述与定位，但缺乏多目标3D检测能力；需要一种既保留开放词汇与视觉提示优势、又能稳健输出3D框的VLM原生接口。

Method: 提出“Chain-of-Sight（CoS）”序列，将3D检测表述为next-token生成：先输出2D检测作为“视觉思维链”，再按“由易到难”的课程学习预测3D框。跨目标层面：按从近到远排序减少早期歧义、符合自中心实用性；单目标层面：按从相机中心的位移、尺寸、旋转的因子化顺序，依信息稳定性与可学性排序。无需专门检测头，保持开放词汇与视觉提示能力。

Result: 在Omni3D基准上取得SOTA，AP_3D 49.89，即使与使用GT 2D框的强基线相比也提升+15.51；对未见类别的零样本泛化和鲁棒性良好。

Conclusion: 通过把3D检测规约为规范化的token生成流程，LocateAnything3D为VLM提供实用的3D感知基础，兼顾开放词汇、可泛化与高精度性能。

Abstract: To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.

</details>


### [190] [Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout](https://arxiv.org/abs/2511.20649)
*Hidir Yesiltepe,Tuna Han Salih Meral,Adil Kaan Akan,Kaan Oktay,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出∞-RoPE，一个在推理阶段即可应用的统一框架，通过三部分（Block-Relativistic RoPE、KV Flush、RoPE Cut）解决自回归视频扩散的时间范围有限、长序列动作跟随迟滞、无法单流实现镜头切换三大瓶颈，实现无限时长、可控且具电影化过渡的视频生成，并在VBench上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型受3D-RoPE时间位置上限限制，长视频生成中对提示的细粒度动作控制响应慢，且难以在一次生成中实现不连续的镜头切换，限制了实用性与可控性。

Method: 提出∞-RoPE推理框架：1) Block-Relativistic RoPE：将时间编码改为移动的局部参考系；新生成的潜块相对模型最大帧窗进行旋转，早期块反向旋转以保相对时间几何，从而打破固定时间位置上限；2) KV Flush：在不重编码的前提下刷新KV缓存，仅保留全局sink与最新潜帧，使模型对新提示立即响应；3) RoPE Cut：在RoPE时间坐标中引入受控不连续，允许在单次连续rollout中实现多次场景切换。

Result: 在综合实验与VBench评测上，∞-RoPE在整体分数上持续优于此前自回归方法，并展示出无限时长生成、快速动作响应与多段镜头切换能力。

Conclusion: ∞-RoPE在无需额外训练的前提下，为自回归视频扩散提供了可无限延展、可精细控制且具电影化过渡的基础方案，克服了时间位置上限、提示响应迟滞与单流镜头切换缺失三大难题。

Abstract: Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.

</details>


### [191] [MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities](https://arxiv.org/abs/2511.20650)
*Tooba Tehreem Sheikh,Jean Lahoud,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 提出MedROV：首个用于医学影像的实时开放词汇目标检测模型，依托自建60万样本、九种模态的Omnis数据集与伪标注策略，并融入大模型知识与对比学习，实现已知与新类别的高效检测，性能大幅超越现有方法且达70 FPS。


<details>
  <summary>Details</summary>
Motivation: 传统医学目标检测多为封闭集设置，无法识别新类别；医学领域OVOD研究受限于数据稀缺与文本-图像对齐弱，急需兼顾开放词汇能力、泛化与实时性的方案与数据资源。

Method: 1) 构建大规模多模态检测数据集Omnis（60万样本、九种成像模态）；2) 针对多源数据缺失标注，引入伪标注策略统一与补全；3) 将大规模预训练基座模型的知识蒸馏/迁移到检测框架；4) 采用对比学习与跨模态表征对齐，实现开放词汇检测；5) 设计实时推理架构（70 FPS）。

Result: 在医学图像检测任务中，MedROV相较此前最强基础模型平均mAP50提升约40点，相较封闭集检测器提升超过3点，并在70 FPS下保持实时推理。

Conclusion: 通过数据、对齐与知识迁移的协同，MedROV实现医学影像开放词汇检测的显著精度与速度突破，树立新基准；提供代码、数据与模型以促进社区复现与扩展。

Abstract: Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.

</details>


### [192] [RubricRL: Simple Generalizable Rewards for Text-to-Image Generation](https://arxiv.org/abs/2511.20651)
*Xuelu Feng,Yunsheng Li,Ziyu Wan,Zixuan Gao,Junsong Yuan,Dongdong Chen,Chunming Qiao*

Main category: cs.CV

TL;DR: RubricRL提出以可分解评分细则（rubric）替代黑盒单标量奖励，按提示动态生成并加权各细项，由多模态判别器逐项评估，用于优化文本到图像模型的RL训练，提升忠实度、细节与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法多依赖固定权重的复合指标或由偏好模型蒸馏的单一标量奖励，导致可解释性差、难以按需求调整、对不同提示的适配性不足。需要一种既可解释又可组合、能对不同提示自适应加权、并允许用户直接干预奖励维度的设计。

Method: 为每个输入提示动态构建“评分细则”（如物体正确性、属性准确性、OCR、写实度等），并引入提示自适应加权机制突出最相关维度；每个细则由多模态评审模型独立打分，得到结构化奖励向量，再用于GRPO/PPO等RL优化；用户可直接增减或重权某些细则，实现可控对齐。

Result: 在自回归文生图模型上，RubricRL相较基线提升了提示忠实度、视觉细节与泛化能力，并展示了对奖励维度的灵活控制与可扩展性。

Conclusion: 基于细则的奖励设计为文生图RL对齐提供了可解释、可组合、可控的统一框架，优于黑盒单标量或固定加权复合指标，并具备跨架构的扩展潜力。

Abstract: Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.

</details>
