<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 144]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: 论文提出一个用于共享空间MR会议的实时消隐现实系统：通过语义分割与精确选取目标，结合移动深度相机视角的实时视频修复（改进DSTT），在720p下>20fps运行，实现对隐私物体的动态移除。


<details>
  <summary>Details</summary>
Motivation: MR会议中他人可见用户环境中的隐私/敏感物体，现有方案常依赖固定相机或预建图，难以在动态共享空间下实时、便携地保护隐私。需要一种无需固定视点与预扫描、仍可高质量实时“消隐”目标的方案。

Method: - 使用YOLOv11进行目标检测，辅助语义分割与精确对象选择。
- 使用移动ZED 2i深度相机作为二观察者视角，提供深度与视频流。
- 采用改进的Decoupled Spatial-Temporal Transformer (DSTT)进行高质量视频修复（inpainting），在被移除物体区域以背景填充。
- 整体流水线支持实时处理与多视角对齐，满足720p>20fps。

Result: 系统在720p分辨率下实现超过20 fps的实时性能，达成高质量视频修复与稳定的目标移除效果，并在无需固定次要视点或预先3D扫描条件下运行。

Conclusion: 该系统验证了在实际MR隐私场景中进行实时消隐现实的可行性：可便携、鲁棒、无需预扫描与固定视角，能够为共享空间会议提供有效的隐私控制。

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [2] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: 提出SurgLaVi大规模外科视觉-语言数据集（约24万视频片段-文本对，覆盖200+术式，含相位/步骤/任务三级层次），并基于此训练SurgCLIP对比学习模型，在多项识别任务显著超越SOTA；同时开放公共子集SurgLaVi-η（11.3万对）。


<details>
  <summary>Details</summary>
Motivation: 现有外科VLP受限于数据规模小、术式多样性不足、语义质量不高、缺乏层次结构，影响跨任务迁移与通用表征的学习。

Method: 构建全自动数据生成管线：对手术视频进行细粒度转写与语义分段，形成相位/步骤/任务层次的连贯单元；采用图像与文本双模态过滤剔除无关/噪声样本；为片段生成上下文增强的丰富描述；发布完整SurgLaVi与开源公共子集SurgLaVi-η。提出SurgCLIP：双编码器、CLIP式视频-文本对比学习框架作为基线。

Result: SurgCLIP在相位、步骤、动作、器械识别等任务上稳定提升，且多项指标显著超过以往SOTA，展示更强与更可泛化的表征能力。

Conclusion: 大规模、语义丰富且具层次结构的外科视觉-语言数据能直接提升下游任务表现与泛化。SurgLaVi为构建外科领域基础模型提供关键资源，SurgCLIP验证了其有效性。

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [3] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 提出一个高分辨率、面向3D结构脑MRI的SimCLR自监督基础模型，在11个公开数据集共18,759名患者（44,958次扫描）上预训练，跨多任务与分布外场景优于MAE和监督基线，并在仅用20%标注数据微调时预测阿尔茨海默病仍有优势；模型与代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有3D脑MRI深度模型多为任务专用、依赖有限标注，泛化差；2D领域的SSL已成功，但3D脑MRI的基础模型在分辨率、范围或可获取性上受限，亟需一个通用、高分辨率、可复用且可获取的基础模型。

Method: 构建SimCLR框架的自监督预训练，在多样且大规模的3D结构MRI（来自11个公开数据集、覆盖多种神经疾病）上学习表征；随后在四个不同下游预测任务上微调与评估，包含分布内与分布外设置；与MAE和两个监督基线进行对比；评估数据效率（如仅20%标注样本微调）。

Result: 在所有下游任务、无论分布内或分布外，微调后的SimCLR模型均优于MAE与监督基线；在阿尔茨海默病预测任务上，即便只用20%标注样本微调也保持最佳性能。

Conclusion: 高分辨率3D脑MRI的SimCLR自监督基础模型在多任务、多分布下具备强泛化与数据效率优势，优于现有MAE和监督方法；依托公开数据与代码并释出已训练模型，为临床脑MRI分析提供可广泛适用与可获取的基础模型。

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [4] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: 提出USCTNet：一种物理约束与可学习近端相结合的深度展开方法，从单张RGB重建高光谱图像，同时显式估计相机光谱响应(CSS)与光照，利用可学习变换域的核范数正则与低秩子空间SVT，实现色度一致且高精度的重建。


<details>
  <summary>Details</summary>
Motivation: 从单张RGB恢复HSI高度不适定，且在CSS与光照未知或误设时易产生物理不一致与色彩失真。现有基于学习的方法往往忽视物理成像过程，或在低秩约束中依赖完整SVD带来高计算与不稳定。需要一种兼顾物理可解释性、计算效率与色度一致性的解法。

Method: 将RGB到HSI重建建模为物理先验的逆问题：在每次迭代中显式估计CSS与光照并嵌入前向算子，约束重建满足成像物理与色度一致；在可学习变换域上施加核范数正则。为避免完整SVD的代价，提出数据自适应的低秩子空间SVT算子。基于上述设计构建深度展开网络USCTNet：包含参数估计模块与可学习近端更新模块，端到端训练。

Result: 在标准基准上，相比最先进的RGB驱动方法，USCTNet在重建精度上取得一致提升（文中未列具体数值，但强调广泛实验与稳健收益）。

Conclusion: 通过将CSS/光照显式估计与可学习低秩正则融入深度展开，USCTNet实现物理一致且高精度的RGB到HSI重建，同时以低秩子空间SVT提高效率与稳定性；方法通用、可扩展，代码已开源。

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [5] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: 研究比较LLM与3D CNN在BraTS2020上的胶质瘤分级与分割：CNN整体更优；LLM（通用与微调后）在分类特异性与分割空间理解上明显不足，微调改善有限。


<details>
  <summary>Details</summary>
Motivation: LLM在文本医疗任务表现强，但其在图像（尤其医学影像）上的效用不明。作者希望量化LLM在胶质瘤分类与分割中的能力，并与专用CNN对比，以评估LLM当前可用性与改进空间。

Method: 使用BraTS 2020多模态脑MRI。对比：定制3D CNN与通用视觉-语言LLM（LLaMA 3.2 Instruct），并对LLM进行微调。任务1：低级别/高级别胶质瘤分类；任务2：分割，采用三种输出策略（中心点、边界框、多边形）。评估准确率、精确率/召回率、特异性，以及分割空间定位质量与输出结构化程度。

Result: 分类：CNN准确率80%，性能均衡；通用LLM准确率76%但特异性仅18%，偏向将低级别误判为高级别；微调后特异性升至55%，但整体下降（准确率72%）。分割：CNN能较好定位（偶漏小肿瘤）；LLM预测集中于图像中心，忽视肿瘤大小/位置；微调仅改善输出格式，不提升空间准确；多边形方法输出随机、无结构。

Conclusion: 在胶质瘤分类与分割中，CNN显著优于LLM。现阶段LLM缺乏有效空间理解，微调带来有限改进，不适合直接用于医学影像任务。需更严格的微调或替代训练策略以提升性能、鲁棒性与临床实用性。

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [6] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: SP4D提出一种双分支扩散模型，从单目输入同时生成RGB视频与与之对齐的“运动学部件”分割，并通过颜色编码共享VAE、双向融合与对比一致性损失，得到跨时空一致的部件。生成的2D部件可提升到3D以恢复骨架与蒙皮；并构建KinematicParts20K数据集验证其在真实视频、生成物体与罕见姿态上的泛化。


<details>
  <summary>Details</summary>
Motivation: 传统基于外观的语义分割难以与物体的关节/运动学结构对齐，且跨视角与时间一致性弱；缺乏能同时生成RGB与与运动学一致的部件分割的模型与大规模配对数据。

Method: - 双分支扩散：同时合成RGB帧和部件分割图。
- 空间颜色编码：将离散部件掩码映射为连续RGB式图像，使分割分支与RGB分支共享VAE，并可灵活支持不同部件数量；通过后处理恢复离散分割。
- BiDiFuse双向扩散融合：增强两分支的一致性。
- 部件一致性对比损失：促进跨空间与时间的对齐。
- 2D→3D提升：由生成的2D部件估计骨架与谐波蒙皮权重，少量人工调整。

Result: 在自建KinematicParts20K（从Objaverse XL筛选超2万带骨骼绑定的对象，提供多视角RGB与部件视频）上训练与评测。SP4D对真实视频、生成物体和稀有姿态具有强泛化，输出具有运动学意识，可直接用于动画与运动任务。

Conclusion: SP4D实现了从单目输入生成跨时空一致的运动学部件与RGB视频的统一框架，通过颜色编码共享表示与双向融合提高一致性，2D到3D的提升能力使其适用于下游动画/重建工作流；并提供了支撑训练与评估的新数据集。

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [7] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: 提出SegSLR：用可提示的零样本视频分割，把RGB与姿态信息有效融合，实现孤立手语识别并在ChaLearn249 IsoGD上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统将RGB与姿态（关键点/骨架）融合时，常以框或粗糙区域定位手与身体，导致手部形状、朝向等细节丢失，限制识别精度。需要一种既利用姿态的粗定位又能保留RGB细粒度外观的融合方式。

Method: 利用姿态提供手和身体的粗定位，作为提示驱动零样本视频分割，对视频序列中手与身体进行精细分割，保留形状与朝向信息；随后仅对分割出的相关部位进行RGB特征提取和识别，从而实现对关键信息的聚焦与多模态有效融合；并进行消融实验验证聚焦策略的贡献。

Result: 在复杂的ChaLearn249 IsoGD数据集上取得优于现有方法的性能；消融显示聚焦于手与身体显著提升识别效果。

Conclusion: 通过以姿态引导的可提示视频分割精确提取手与身体区域，SegSLR能够保留关键细节并有效融合RGB与姿态信息，从而在ISLR上取得SOTA；设计选择（关注手与身体）得到实验证实。

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [8] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: 提出SCOPE：以语音引导的大模型-视觉基础模型协同框架，实现术中视频中器械与解剖结构的零样本开集分割、标注与跟踪，减少手动提示依赖，适配动态手术场景。


<details>
  <summary>Details</summary>
Motivation: 现有术中感知多依赖特定领域的有监督模型和标注数据，泛化与扩展到新器械/新解剖困难；虽有开集零样本的视觉基础模型出现，但依赖人工视觉/文本提示，不便术中无手操作与实时部署。

Method: 提出SCOPE框架：将LLM的语言/推理能力与开集VFM的感知能力耦合。设计算法代理生成VFM分割候选，并通过临床医生的直观语音反馈迭代选择/修正；随后以已识别的器械作为交互指针，进一步引导场景中其它要素的标注。实现在线分割与跟踪，并在手术视频流中运行。

Result: 在Cataract1k子集和院内离体颅底数据集上验证，展示可在线生成分割与跟踪；并在离体模拟实验中演示其动态能力。

Conclusion: 人机协作、语音引导的开集感知可在手术室实现免手、可适配的场景理解，为构建以外科医生为中心的动态术中辅助工具提供了可行路径。

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [9] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出4D-GRT：将4D高斯描绘与物理真实感光线追踪结合，用于生成带真实相机效应（鱼眼畸变、rolling shutter等）的训练视频，速度快、质量佳，并发布包含八个动态室内场景、四类相机效应的基准。


<details>
  <summary>Details</summary>
Motivation: 现有视觉系统多假设针孔相机，遇到真实相机效应时性能下降；数据合成方法要么成本高、要么存在仿真到现实鸿沟，或无法精确建模相机效应，限制了对这类效应的学习。

Method: 两阶段流程：1）基于多视角视频，用4D Gaussian Splatting重建动态场景；2）在重建结果上进行物理正确的光线追踪，生成可控参数的相机效应视频（如鱼眼畸变、rolling shutter等）。强调渲染速度快且质量高。

Result: 在渲染速度上达到当前最快，并在渲染质量上优于或可比于现有基线；同时构建了包含八个室内动态场景、覆盖四类相机效应的合成基准用于评测。

Conclusion: 4D-GRT为带真实相机效应的数据生成提供了高效、物理准确的方案，缓解训练数据缺乏真实相机效应的问题，并通过新基准推动该方向的研究与评测。

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [10] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: 提出一个多智能体自动视频编辑系统：Editor依据视频片段与文本指令进行剪辑，Critic基于生成序列给出自然语言反馈或通过；并引入跨代理沟通学习框架与LLM评审指标。实验显示在覆盖度、时长约束与人偏好上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作偏重检索或交互界面，真正的剪辑仍需人工完成；需要把“文本驱动的视频编辑”从半自动提升到自动化，并能在复杂指令、时长约束和连贯性下做出高质量编辑。

Method: 将视频编辑建模为序列决策问题，采用多代理架构：Editor根据视频集合与自然语言指令，使用常见剪辑工具（选段、拼接、转场等）生成序列；Critic对结果给出自然语言反馈直到满意为止或通过。提出学习式跨代理沟通机制以提升反馈有效性；同时设计LLM-as-a-judge作为自动评测指标，并与人类偏好对比。

Result: 通过用户研究与定量指标评估，系统在覆盖指令、满足时长约束及人类偏好上显著胜出，优于现有方法；并报告LLM裁判指标与人类偏好具有一定一致性。

Conclusion: 多代理协作与学习增强的语言沟通可有效自动化视频编辑流程；引入LLM评审可作为实用评测手段。该方法在多项关键维度上优于以往方案。

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [11] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: 提出一个稳定性感知的机器学习框架，从无对比T1WI预测胶质瘤的对比增强，跨四个多中心数据集验证，平均外部测试准确率约0.93，最佳管线为MI+ETr，强调稳健与可复现性，旨在减少对GBCAs依赖。


<details>
  <summary>Details</summary>
Motivation: GBCAs虽是胶质瘤影像学关键，但存在安全、成本与可及性问题；而对比增强与肿瘤侵袭性和治疗规划密切相关。因此希望用无对比MRI与ML预测增强表型，提升安全性与可推广性。但多中心扫描仪与队列差异导致模型选择不稳健，需要一种能识别跨中心可复现管线的策略。

Method: 汇集TCIA四个数据集共1,446例，使用非对比T1WI为输入，配对的后对比T1WI提供增强标签。按IBSI规范用PyRadiomics提取108个影像组学特征，组合48种降维方法与25种分类器，构成约1,200条管线。采用“旋转验证”（三训练一外测，四数据集轮换）与交叉验证评估性能与稳定性，并比较各管线排名。

Result: 交叉验证准确率0.91–0.96；外部测试：UCSF-PDGM 0.87、UPENN-GB 0.98、BRATS-Africa 0.95，平均约0.93。F1/精确率/召回率在0.87–0.96区间较稳定；ROC-AUC波动较大（0.50–0.82），反映队列异质性。MI结合ETr的管线在准确性与稳定性上综合排名最佳。

Conclusion: 稳定性感知的模型选择可在多中心环境下可靠地从无对比T1WI预测胶质瘤对比增强，降低对GBCAs依赖并提升跨中心泛化性；所给流程为神经肿瘤学等领域可复现ML研究提供可扩展模板。

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [12] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: 提出一种与检测器无关的后处理框架，通过重叠切片、空间与语义聚类、置信度重加权与类感知NMS，将重叠冗余转化为群体证据，显著提升UAV密集小目标召回（+0.093），以一定精度为代价，实现召回优先的应用收益。


<details>
  <summary>Details</summary>
Motivation: UAV影像中的小目标因远距离视角、遮挡与背景杂乱而易漏检；现有检测器在密集场景下对低置信候选与冗余框处理不足，需要一种无需重训、可泛化的后处理策略来提升召回并稳定融合重叠信息。

Method: 1) 重叠平铺：通过重叠切片恢复低置信候选；2) 空间门：对候选框中心点做DBSCAN，形成几何一致的群；3) 语义门：以ResNet-18嵌入对候选做DBSCAN，确保外观一致；4) 对通过双门验证的群体进行受控置信度重加权；5) 进行类感知NMS融合，得到最终检测。

Result: 在VisDrone上，召回从0.685提升到0.778（+0.093），精度从0.801调整到0.595，F1为0.669；后处理平均时延0.095秒/张。消融表明：重叠切片曝光遗漏目标；空间聚类稳定几何；语义聚类保证外观一致；重加权实现与基线的标定式融合。

Conclusion: 框架无需重训、可与现代检测器即插即用，体现“召回优先、精度权衡”的特性，适用于远距计数与监测等对召回敏感的应用。后续将降低语义门成本并结合时序线索。

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [13] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出InternScenes，一个约4万室内场景、196万对象、15类场景与288类物体的大规模可模拟3D数据集，保留大量小物体、物理无碰撞并具交互性，显著提升布局生成与导航任务的挑战与可扩展训练能力，并将全面开源。


<details>
  <summary>Details</summary>
Motivation: 现有Embodied AI所需的3D场景数据集要么规模/多样性不足，要么场景过度“清洁”缺少小物体，且常存在严重物体碰撞，导致布局不真实、交互/仿真受限，制约布局生成与导航等下游任务。

Method: 整合三源数据：真实扫描、程序生成、设计师创建；构建处理流水线：为真实扫描构建可仿真复刻（real-to-sim）、在场景中注入可交互对象、通过物理仿真消解物体碰撞；保留并强调小物体以形成复杂、真实布局（平均每区域41.5物体）。

Result: 得到InternScenes数据集：约4万多样室内场景、1.96M对象、覆盖15种场景与288物体类，布局复杂且可交互、碰撞已消解；在两项基准（场景布局生成、点目标导航）上验证其价值，展现更高难度并支持大规模训练带来性能提升与可行性。

Conclusion: InternScenes提供大规模、真实、可模拟且可交互的室内场景，弥补现有数据集缺陷，为复杂布局下的生成与导航奠定基础，并通过开源数据/模型/基准促进社区发展。

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [14] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

TL;DR: 研究比较用于数学手写轨迹（数字墨迹）多项式表示的不同正交基（Legendre/Legendre-Sobolev 与 Chebyshev/Chebyshev-Sobolev）的精度-计算成本权衡，利用条件数与内积诱导范数分析稳定性与形状差异度量。


<details>
  <summary>Details</summary>
Motivation: 以参数化平面曲线的多项式表示为手写符号提供紧凑几何编码，但不同正交基与多项式阶数影响数值稳定性、计算效率与重建精度，需系统评估与选择。

Method: 在Legendre、Legendre-Sobolev、Chebyshev、Chebyshev-Sobolev分级基上，分析并比较多项式评估的条件数；研究相应内积（含Sobolev型导数权重）所诱导的范数如何界定符号间的变动；据此讨论在给定精度下的最低阶数与成本。

Result: 给出了各基在评估条件数上的差异与趋势，建立了由不同内积定义的范数界，从而量化符号变动；据此揭示在低阶情况下的稳定性与表示精度的折中，并指出何种基在何种阶数范围更具鲁棒与经济性。

Conclusion: 基选择与多项式阶数应联合优化：在目标精度下选取条件数更小、范数度量更契合形状差异的基与最低必要阶数，可在保持准确建模的同时显著降低计算成本。

Abstract: Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [15] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

TL;DR: 提出一个多任务扩散框架，基于稀疏不规则的纵向MRI，仅用早期两次随访就能在任意未来时间点生成FLAIR影像并输出基于SDF的像素级肿瘤演化概率图；结合预训练形变模块与定向数据增广（补全随访与模态），并加入放疗加权的focal损失以突出临床关键区域，公私数据集上均表现良好。


<details>
  <summary>Details</summary>
Motivation: 临床纵向MRI随访稀疏且不完整，时间间隔不规则，导致肿瘤进展建模与预测困难；需要一个能在任意时间点给出像素级、具有不确定性量化的进展预测方法，并能在数据稀缺条件下稳定训练，且兼顾放疗区域的临床重要性。

Method: - 多任务扩散模型：同时预测未来任意时间点的FLAIR序列与SDF导出的空间概率演化图，实现像素级不确定性量化。
- 预训练形变模块：用形变场刻画跨时间点的扫描变化，建模任意时间间隔的时序动力学。
- 定向数据增广：合成完整三次随访序列，并从可用模态推断缺失MRI模态，缓解数据稀缺与不平衡。
- 放疗加权focal损失：引入放射剂量图作为权重，加强临床关键区的学习。
- 仅需两次早期随访，输出可查询的时间依赖概率图。

Result: 在公共数据集上训练，并在内部私有数据集上评估，取得“有前景”的结果（定量指标未给出），显示方法在不同数据来源上具备有效性与泛化潜力。

Conclusion: 该时间无关的多任务扩散框架能在稀疏、缺失的临床纵向MRI条件下，为任意未来时点生成影像与概率演化图并量化不确定性；结合形变建模、数据补全与放疗加权损失提升临床相关性与预测稳定性，具有临床辅助决策潜力。

Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [16] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

TL;DR: 提出一种仅用LiDAR数据、通过点-平面投影学习2D表征来增强点云语义分割的方法，并配合几何感知的数据增强，在小样本下显著提升，且在SemanticKITTI与PandaSet上具竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前强方法常依赖多模态或外部数据、复杂模型与大量训练样本，导致计算开销大、在数据稀缺场景泛化差。需要一种在仅有LiDAR且数据有限时也能有效学习的方案。

Method: 在点为基础的框架中引入点-平面投影，将3D点云投影到多个信息丰富的2D表征上，从这些2D视图学习互补特征并回馈给点级表示；同时提出符合LiDAR传感器几何特性的增强策略，缓解类别不平衡。实现了多种2D投影通道并与点特征融合。

Result: 在受限数据设置下显著提升分割性能；在标准数据集SemanticKITTI与PandaSet上取得具竞争力的结果。

Conclusion: 点-平面投影能够在仅LiDAR条件下从2D表征中挖掘互补信息，配合几何感知的数据增强可在小样本下显著提升，同时保持在大规模基准上的竞争力；代码已开源（3PNet）。

Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [17] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

TL;DR: 提出OpenUrban3D：首个无需对齐多视图图像、无需预训练点云分割网络或人工标注的3D开放词汇语义分割框架，用于大规模城市点云；通过多视图多粒度渲染、掩码级视觉-语言特征提取与样本均衡融合，并蒸馏到3D骨干，支持任意文本零样本分割，在SensatUrban与SUM上显著提升精度与跨场景泛化。


<details>
  <summary>Details</summary>
Motivation: 开放词汇分割可按任意自然语言识别/分割细粒度或功能性类别，但在大规模城市点云中尚缺研究。现实难点：1）数据集常缺高质量、严格对齐的多视图图像；2）现有3D分割方案在几何、尺度、外观差异巨大的城市场景间泛化差。需要一种不依赖对齐影像、预训练模型或人工标注，且具备强泛化的3D开放词汇方案。

Method: OpenUrban3D直接从原始点云生成鲁棒语义特征：先进行多视图、多粒度渲染；对渲染结果做掩码级视觉-语言特征抽取；进行样本均衡融合；再将所得语义特征蒸馏到3D骨干网络，从而实现对任意文本查询的零样本分割，同时兼顾语义丰富性与几何先验。

Result: 在SensatUrban与SUM等大规模城市基准上，较现有方法在分割精度与跨场景泛化上均有显著提升。

Conclusion: OpenUrban3D作为无需对齐影像与标注的开放词汇3D分割框架，展现出在城市场景理解中的灵活性、可扩展性与强泛化能力，适用于数字孪生、城市管理与城市分析等应用。

Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [18] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

TL;DR: 提出AutoOEP多模态在线监考系统：双摄像头采集正面与侧面视角，融合人脸/头姿/凝视/口部与手部-违禁物检测特征，经LSTM输出实时作弊概率；在自建数据集达90.7%可疑行为分类准确、违禁物mAP@0.5为0.57、CPU约2.4 FPS。


<details>
  <summary>Details</summary>
Motivation: 在线教育兴起导致远程考试诚信风险上升：人工监考难以扩展、成本高且主观；现有自动化方案要么侵入性强，要么覆盖作弊行为有限、鲁棒性不足。因此需要一种可扩展、资源友好、覆盖多类作弊线索的自动化监考框架。

Method: 构建多模态、并行分析框架AutoOEP：双摄像头（正面+侧面）减少盲区；Face模块用ArcFace持续身份验证，并进行头姿、凝视与口部运动分析；Hand模块用微调的YOLOv11检测违禁物（手机、纸条等）并跟踪手与物体的时空关系；将上述特征汇聚并输入LSTM进行时序建模，输出实时作弊概率；在自建多场景数据集上评估，记录准确率、mAP@0.5与处理速度（CPU）。

Result: 系统对可疑活动分类准确率90.7%；违禁物检测mAP@0.5为0.57；在无GPU条件下实时处理约2.4 FPS；展示对多种作弊线索的检测能力。

Conclusion: AutoOEP在资源受限环境下实现较高的可疑行为识别性能，减少人工干预、提升在线考试诚信。尽管违禁物mAP中等、帧率有限，但框架具备可扩展性与实用性，可作为自动监考的有效方案。

Abstract: The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [19] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

TL;DR: 提出一种结合双摄像头与TV次梯度理论的CASSI重建框架，通过端到端SD-CASSI模型与动态正则化，将辅助相机生成的参考图像的归一化梯度约束融入优化，既提升重建质量又保持可解释性，并在多场景中表现稳健。


<details>
  <summary>Details</summary>
Motivation: CASSI在追求高光谱、空间与时间分辨率的同时面临压缩重建的病态性；传统基于先验的模型算法受限于手工先验，深度学习方法则缺乏物理可解释性。作者希望在不牺牲可解释性的前提下提升高压缩比条件下的重建性能。

Method: - 构建端到端的SD-CASSI数学模型，降低求解逆问题的复杂度，并为多摄系统分析提供统一框架。
- 融合总变差（TV）次梯度理论，提出动态正则化：从RGB/全色辅助相机生成参考图像，提取并归一化其梯度，作为TV次梯度相似性约束，形成严格凸优化问题。
- 设计自适应参考生成与更新机制，在迭代中利用辅助相机的空间先验提供次梯度引导。
- 基于ADMM等可解释的优化流程实现重建。

Result: 实验表明该方法在各类重建场景下更好地保持空间-光谱结构一致性，提升重建质量与稳健性；并验证了理论框架的有效性与可解释性。源码公开于GitHub。

Conclusion: 该方法通过双摄与TV次梯度约束实现对CASSI重建的可解释、稳健提升，兼顾精度与物理一致性，为计算光谱成像提供了统一且可扩展的理论与算法基础。

Abstract: Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [20] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: 提出一个仅2.5M参数的元数据感知专家混合掩码自编码器（MoE-MAE），在BigEarthNet-Landsat预训练、冻结编码器线性探测，在EuroSAT-Landsat也具竞争力，显示小型、元数据感知的MoE-MAE可高效迁移并具标签效率。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测（EO）大型基础模型计算代价高、难以复用与下游迁移；需要探索在保持泛化能力的同时显著降低模型规模与推理成本的可行路径。

Method: 设计紧凑的MoE-MAE：将稀疏专家路由与地理—时间条件注入相结合，除影像外引入经纬度与季节/日周期编码；在BigEarthNet-Landsat上进行自监督掩码预训练；冻结编码器，用线性探测评估表征；在不含显式元数据的EuroSAT-Landsat上测试泛化。

Result: 仅2.5M参数的模型在多个任务上与大规模架构竞争；元数据感知预训练提升迁移性能与标签效率；在缺少显式元数据的数据集上仍取得与数亿参数模型相当的表现。

Conclusion: 小型、元数据感知的MoE-MAE在效率、可扩展性与跨数据集泛化上表现优异，是迈向未来EO基础模型的有效路线。

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [21] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

TL;DR: 提出一种用于HR-pQCT骨微结构成像的运动伪影校正框架，先用优化的正弦图（sinogram）级退化模型合成配对数据，再以引入边缘增强与自注意力的WGAN-GP（ESWGAN-GP）进行监督学习，在模拟与真实数据上均显著提升SNR/SSIM/VIF，但仍是对真实体内运动的简化。


<details>
  <summary>Details</summary>
Motivation: HR-pQCT在体扫描常受刚体运动伪影（皮质条纹、松质模糊）影响，阻碍对骨微结构的可靠评估。虽有运动分级标准，但缺乏通用退化模型与有效校正方法，限制了深度学习的应用。

Method: 1) 优化传统基于sinogram的运动退化生成流程，构建带真值的配对数据；2) 设计ESWGAN-GP：在WGAN-GP框架中引入自注意力以建模长程依赖、通过跳连进行边缘增强以保留小梁边界；3) 融合VGG感知损失以重建细微微结构；4) 在合成源域与真实目标域上训练/评估。

Result: 源数据集：SNR 26.78、SSIM 0.81、VIF 0.76；真实目标数据集：SNR 29.31、SSIM 0.87、VIF 0.81，目标域表现更佳，显示出一定的泛化与纠错能力。

Conclusion: 通过标准化的运动退化模拟与ESWGAN-GP的边缘与注意力设计，初步实现HR-pQCT运动伪影的深度学习校正，在真实数据上有提升。然而退化模型仍简化了体内复杂运动，需进一步逼真建模与更广验证，以推动临床落地。

Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>


### [22] [Gaze Authentication: Factors Influencing Authentication Performance](https://arxiv.org/abs/2509.10969)
*Dillon Lohr,Michael J Proulx,Mehedi Hasan Raju,Oleg V Komogortsev*

Main category: cs.CV

TL;DR: 研究在大规模8,849名受试者、72Hz视频眼动仪（Meta Quest Pro等效）条件下，影响最先进凝视（gaze）式认证性能的关键因素：信号质量、校准策略与简单滤波。结论：相同深度的校准目标、更好信号质量、以及融合校准与未校准凝视可提升认证；简单3点滑动平均通常略降性能，但存在少量例外。


<details>
  <summary>Details</summary>
Motivation: 凝视式认证在VR/AR设备中前景广阔，但实际性能受眼动追踪的信号质量、校准方式与后处理影响。缺乏在超大规模真实设备数据上的系统性量化与最佳实践，阻碍部署与鲁棒性提升。

Method: 使用SOTA神经网络认证架构与视频眼动学管线，在8,849名受试者数据上，系统操控并评估：1) 眼动信号质量分层；2) 校准的多要素（目标深度一致性、是否融合校准/未校准凝视等）；3) 对原始凝视应用简单3点移动平均滤波。比较不同设置下的认证性能变化。

Result: - 相同深度的校准目标显著提升认证表现；- 融合校准与未校准凝视优于单独使用其一；- 提升信号质量一贯带来性能增益；- 3点移动平均滤波通常略微降低认证性能；- 以上规律总体成立但存在例外情形。

Conclusion: 在大规模实测条件下，提升凝视式认证应优先：确保校准目标深度一致、融合使用校准/未校准凝视、与改进信号质量；避免盲目使用简单平滑滤波。尽管总体趋势明确，仍需考虑边缘案例与设备/人群差异。

Abstract: This paper examines the key factors that influence the performance of
state-of-the-art gaze-based authentication. Experiments were conducted on a
large-scale, in-house dataset comprising 8,849 subjects collected with Meta
Quest Pro equivalent hardware running a video oculography-driven gaze
estimation pipeline at 72Hz. The state-of-the-art neural network architecture
was employed to study the influence of the following factors on authentication
performance: eye tracking signal quality, various aspects of eye tracking
calibration, and simple filtering on estimated raw gaze. We found that using
the same calibration target depth for eye tracking calibration, fusing
calibrated and non-calibrated gaze, and improving eye tracking signal quality
all enhance authentication performance. We also found that a simple
three-sample moving average filter slightly reduces authentication performance
in general. While these findings hold true for the most part, some exceptions
were noted.

</details>


### [23] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: 提出TrueSkin数据集（7299张、6类肤色、复杂拍摄条件），并系统评测LMM与生成模型在肤色识别与生成上的偏差；在该数据上训练/微调可显著提升识别准确率（>20%）与生成肤色保真度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态与生成模型在肤色识别与合成上表现不佳，受限于缺乏全面数据与稳健方法，导致公平性与医疗等场景风险；尤其中间色调被误判为更浅、生成易被提示中其他属性偏置。

Method: 构建TrueSkin数据集（7299图像、6类肤色，覆盖多光照、角度、设备）；以该数据对现有识别与生成方法进行基准评测；用TrueSkin训练识别模型并对生成模型进行微调，量化改进。

Result: 基准显示显著偏差：LMM倾向把中间肤色判为更浅；生成模型在提示中受非相关属性（如发型、环境）影响而难以匹配指定肤色。用TrueSkin训练的识别模型较LMM和常规方法准确率提升超20%，微调生成模型后肤色保真度显著提升。

Conclusion: TrueSkin作为覆盖性强的数据集既是评测基准也是训练资源，可提升肤色识别与生成的公平性与准确性；强调构建全面数据集对缓解模型偏见的重要性。

Abstract: Skin tone recognition and generation play important roles in model fairness,
healthcare, and generative AI, yet they remain challenging due to the lack of
comprehensive datasets and robust methodologies. Compared to other human image
analysis tasks, state-of-the-art large multimodal models (LMMs) and image
generation models struggle to recognize and synthesize skin tones accurately.
To address this, we introduce TrueSkin, a dataset with 7299 images
systematically categorized into 6 classes, collected under diverse lighting
conditions, camera angles, and capture settings. Using TrueSkin, we benchmark
existing recognition and generation approaches, revealing substantial biases:
LMMs tend to misclassify intermediate skin tones as lighter ones, whereas
generative models struggle to accurately produce specified skin tones when
influenced by inherent biases from unrelated attributes in the prompts, such as
hairstyle or environmental context. We further demonstrate that training a
recognition model on TrueSkin improves classification accuracy by more than
20\% compared to LMMs and conventional approaches, and fine-tuning with
TrueSkin significantly improves skin tone fidelity in image generation models.
Our findings highlight the need for comprehensive datasets like TrueSkin, which
not only serves as a benchmark for evaluating existing models but also provides
a valuable training resource to enhance fairness and accuracy in skin tone
recognition and generation tasks.

</details>


### [24] [Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring](https://arxiv.org/abs/2509.10995)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: 提出一种基于强化学习（UCB多臂赌博算法）的迁移学习框架，自动从众多预训练模型中选择最优，用于无人机动物检测，提升检测率并降低计算时间。


<details>
  <summary>Details</summary>
Motivation: 无人机+视觉可非侵入监测动物，但标注数据稀缺，深度学习模型训练受限；迁移学习可复用大数据上训练的模型，但预训练架构繁多，新手很难选择最佳模型，需要自动化、系统化的模型选择方法。

Method: 构建一个强化学习框架，把候选预训练模型视作臂，使用UCB算法在有限计算预算下迭代评估各模型在动物检测任务上的表现（如mAP/召回/检测率等），在探索-利用权衡下逐步聚焦高性能模型，从而输出排名和最优选择。

Result: 实验显示，该框架在动物检测任务上相较传统人工或穷举选择方法，达到更高检测率，同时耗费显著更少的计算时间。

Conclusion: RL+UCB驱动的自动模型选择能有效提升低数据场景下的动物检测性能与效率，为野生动物保护与畜牧管理中的UAV监测提供实用工具。

Abstract: Animal health monitoring and population management are critical aspects of
wildlife conservation and livestock management that increasingly rely on
automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)
based systems combined with computer vision offer promising solutions for
non-invasive animal monitoring across challenging terrains, limited
availability of labeled training data remains an obstacle in developing
effective deep learning (DL) models for these applications. Transfer learning
has emerged as a potential solution, allowing models trained on large datasets
to be adapted for resource-limited scenarios such as those with limited data.
However, the vast landscape of pre-trained neural network architectures makes
it challenging to select optimal models, particularly for researchers new to
the field. In this paper, we propose a reinforcement learning (RL)-based
transfer learning framework that employs an upper confidence bound (UCB)
algorithm to automatically select the most suitable pre-trained model for
animal detection tasks. Our approach systematically evaluates and ranks
candidate models based on their performance, streamlining the model selection
process. Experimental results demonstrate that our framework achieves a higher
detection rate while requiring significantly less computational time compared
to traditional methods.

</details>


### [25] [Improving Fungi Prototype Representations for Few-Shot Classification](https://arxiv.org/abs/2509.11020)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: 提出基于原型网络的少样本真菌识别方法，在FungiCLEF 2025数据上显著提升Recall@5（PB与PR榜单均提升30+个百分点），有效识别常见与稀有物种。


<details>
  <summary>Details</summary>
Motivation: 野外采集的真菌观测数据真实而复杂，类别极度不均衡，许多稀有或缺乏记录的物种训练样本极少甚至缺失，导致传统识别系统在长尾类别上效果差。为支持真菌学研究与公民科学、提升生物多样性监测，需要能在小样本与长尾分布下仍具可靠性能的方法。

Method: 采用基于原型网络（Prototypical Networks）的深度学习框架，针对少样本真菌分类强化类别原型表示，从少量样本中学习判别性嵌入并进行度量学习式分类；方法在FungiCLEF 2025场景中对稀有类进行鲁棒处理。

Result: 在FungiCLEF 2025竞赛的公共（PB）与私有（PR）榜单上，相比官方基线，Recall@5提高超过30个百分点，覆盖常见与稀有物种。

Conclusion: 原型网络增强的少样本方法能在真实野外数据与长尾类别分布下有效识别真菌物种，显示出服务科研与公民科学的大规模监测应用潜力。

Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal
species recognition using realistic, field-collected observational data.
Accurate identification tools support both mycologists and citizen scientists,
greatly enhancing large-scale biodiversity monitoring. Effective recognition
systems in this context must handle highly imbalanced class distributions and
provide reliable performance even when very few training samples are available
for many species, especially rare and under-documented taxa that are often
missing from standard training sets. According to competition organizers, about
20\% of all verified fungi observations, representing nearly 20,000 instances,
are associated with these rarely recorded species. To tackle this challenge, we
propose a robust deep learning method based on prototypical networks, which
enhances prototype representations for few-shot fungal classification. Our
prototypical network approach exceeds the competition baseline by more than 30
percentage points in Recall@5 on both the public (PB) and private (PR)
leaderboards. This demonstrates strong potential for accurately identifying
both common and rare fungal species, supporting the main objectives of
FungiCLEF 2025.

</details>


### [26] [Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images](https://arxiv.org/abs/2509.11034)
*Yuedi Zhang,Zhixiang Xia,Guosheng Yin,Bin Liu*

Main category: cs.CV

TL;DR: 提出csMIL：通过全局-局部聚类、簇内注意力与簇级稀疏约束，筛除无关实例，提升鲁棒性、可解释性与效率，并在病理WSI基准上达SOTA，同时给出样本复杂度O(s log K)的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有MIL在WSI等弱标注场景中面临实例冗余与缺乏显式剔除无信息实例的机制，导致鲁棒性与可解释性不足，计算开销大，需要一种能在簇级别进行选择与稀疏化的MIL框架。

Method: csMIL包含三步：1) 跨所有bag进行全局聚类获得K个簇中心；2) 在每个bag内进行局部聚类以赋予实例簇标签；3) 在每个簇内计算注意力以聚合实例，并在簇级权重上施加稀疏正则，保留少数诊断相关簇、抑制无关簇。整体实现为全局-局部聚类+簇内注意力+簇级稀疏化的联合框架。

Result: 理论上给出恢复s个相关簇所需样本复杂度为O(s log K)，与压缩感知一致；实验上在CAMELYON16与TCGA-NSCLC两个公开病理基准上取得SOTA表现，并带来更强抗噪性与更低计算开销。

Conclusion: 簇级稀疏MIL通过显式在簇层面筛选信息，缓解实例冗余，提升鲁棒性、可解释性与效率，且有样本复杂度保证与SOTA实证结果，适合WSI等弱标注复杂场景。

Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly
labeled datasets, such as whole-slide images (WSIs) in computational pathology,
where bags comprise unordered collections of instances with sparse diagnostic
relevance. Traditional MIL approaches, including early statistical methods and
recent attention-based frameworks, struggle with instance redundancy and lack
explicit mechanisms for discarding non-informative instances, limiting their
robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a
novel framework that integrates global-local instance clustering,
within-cluster attention, and cluster-level sparsity induction to address these
challenges. Our csMIL first performs global clustering across all bags to
establish $K$ cluster centers, followed by local clustering within each bag to
assign cluster labels. Attention scores are computed within each cluster, and
sparse regularization is applied to cluster weights, enabling the selective
retention of diagnostically relevant clusters while discarding irrelevant ones.
This approach enhances robustness to noisy instances, improves interpretability
by identifying critical regions, and reduces computational complexity.
Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to
recover $s$ relevant clusters, aligning with compressed sensing principles.
Empirically, csMIL achieves state-of-the-art performance on two public
histopathology benchmarks (CAMELYON16, TCGA-NSCLC).

</details>


### [27] [Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection](https://arxiv.org/abs/2509.11058)
*Canhui Tang,Sanping Zhou,Haoyue Shi,Le Wang*

Main category: cs.CV

TL;DR: 提出一个零样本视频异常检测框架，利用骨架数据并结合语言引导的语义“典型性”学习与测试时“上下文唯一性”分析，在无目标域训练样本下实现场景自适应的异常定位，在四个大型数据集上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: ZS-VAD常受隐私和部署等限制，无法获取目标域数据。传统骨架方法只学习低层特征、依赖受限的“正常性边界”，难以泛化到新场景（正常/异常行为分布差异大）。需要一种利用骨架跨域优势、又能引入更高层语义与场景自适应边界的方法。

Method: 1) 语言引导的语义典型性建模：将骨架片段投射到动作语义空间，蒸馏大语言模型对常见正常/异常行为的知识，以学习动作的“典型性”。2) 测试时上下文唯一性分析：对骨架片段在时空维度的差异进行细粒度分析，构建场景自适应的正常性边界，无需目标域训练数据。两者联合用于异常分数计算与时间定位。

Result: 在无需目标域训练样本的设定下，于ShanghaiTech、UBnormal、NWPU、UCF-Crime四个包含100+未见监控场景的数据集上，相比现有骨架基线获得最新最优性能。

Conclusion: 通过语义典型性蒸馏与测试时唯一性分析，充分挖掘骨架数据的跨域泛化潜力，实现零样本场景自适应的异常定位，显著优于现有骨架方法并具备良好可迁移性。

Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing
anomalies without target domain training data, which is a crucial task due to
various practical concerns, e.g., data privacy or new surveillance deployments.
Skeleton-based approach has inherent generalizable advantages in achieving
ZS-VAD as it eliminates domain disparities both in background and human
appearance. However, existing methods only learn low-level skeleton
representation and rely on the domain-limited normality boundary, which cannot
generalize well to new scenes with different normal and abnormal behavior
patterns. In this paper, we propose a novel zero-shot video anomaly detection
framework, unlocking the potential of skeleton data via action typicality and
uniqueness learning. Firstly, we introduce a language-guided semantic
typicality modeling module that projects skeleton snippets into action semantic
space and distills LLM's knowledge of typical normal and abnormal behaviors
during training. Secondly, we propose a test-time context uniqueness analysis
module to finely analyze the spatio-temporal differences between skeleton
snippets and then derive scene-adaptive boundaries. Without using any training
samples from the target domain, our method achieves state-of-the-art results
against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,
UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.

</details>


### [28] [Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos](https://arxiv.org/abs/2509.11063)
*Xiaoyu Huang,Lauren M Maxson,Trang Nguyen,Cheng Jack Song,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出基于SAM2的“Organoid Tracker”开源GUI平台，用于对肾脏类器官（尤其PKD）时空显微视频进行零样本分割与自动化定量分析，输出囊肿形成率、增长速度、形态学变化等指标，支持可扩展插件、无代码操作，加速筛选与药物发现。


<details>
  <summary>Details</summary>
Motivation: 类器官为肾病研究和药物筛选提供低成本、可扩展且无动物牺牲的体系，但现有分析多为人工、粗粒度（命中/非命中），忽视像素级与纵向时序信息，成为高通量研究瓶颈。

Method: 构建模块化插件式GUI平台Organoid Tracker：以视觉基础模型SAM2为核心，实现零样本分割，对时空显微视频进行自动追踪与量化；工作流支持批处理、报告生成，并开放源码以便扩展。

Result: 平台可自动计算囊肿形成率、增长速度、形态学指标等关键度量，并生成综合报告；展示了对PKD筛选场景的高效、可重复和无代码分析能力。

Conclusion: Organoid Tracker作为开源、可扩展的零样本时空分析工具，提升肾发育、PKD建模与治疗发现的效率与精度，为高通量成像数据提供可重用的标准化分析框架。

Abstract: Recent advances in organoid models have revolutionized the study of human
kidney disease mechanisms and drug discovery by enabling scalable,
cost-effective research without the need for animal sacrifice. Here, we present
a kidney organoid platform optimized for efficient screening in polycystic
kidney disease (PKD). While these systems generate rich spatial-temporal
microscopy video datasets, current manual approaches to analysis remain limited
to coarse classifications (e.g., hit vs. non-hit), often missing valuable
pixel-level and longitudinal information. To help overcome this bottleneck, we
developed Organoid Tracker, a graphical user interface (GUI) platform designed
with a modular plugin architecture, which empowers researchers to extract
detailed, quantitative metrics without programming expertise. Built on the
cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid
Tracker enables zero-shot segmentation and automated analysis of
spatial-temporal microscopy videos. It quantifies key metrics such as cyst
formation rate, growth velocity, and morphological changes, while generating
comprehensive reports. By providing an extensible, open-source framework,
Organoid Tracker offers a powerful solution for improving and accelerating
research in kidney development, PKD modeling, and therapeutic discovery. The
platform is publicly available as open-source software at
https://github.com/hrlblab/OrganoidTracker.

</details>


### [29] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 用LLaVA为基座、结合LoRA/DoRA微调与深度信息融合，并在选择题/是非题推理中采用CoT，基于DriveLM-nuScenes训练，在验证集取得0.7799并登顶排行榜。


<details>
  <summary>Details</summary>
Motivation: 在“以语言驱动的自动驾驶”任务中，提高视觉-语言模型对驾驶场景理解与问答准确度，争取在CVPR 2024 Autonomous Grand Challenge中取得领先。

Method: - 数据：仅使用DriveLM-nuScenes数据集训练。
- 模型：以LLaVA为基础；采用LoRA与DoRA进行高效微调。
- 多模态增强：集成开源深度估计模型输出，将深度信息用于训练与推理。
- 推理策略：对多选题与是非题采用Chain-of-Thought推理以提升准确率。

Result: 在验证集排行榜上获得0.7799的最高分，排名第一。

Conclusion: 通过在LLaVA上引入参数高效微调（LoRA/DoRA）、融合深度估计信息，并在特定题型使用CoT推理，可显著提升驾驶场景VLM的问答性能，达到SOTA水平。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [30] [Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation](https://arxiv.org/abs/2509.11082)
*Zongwu Xie,Kaijie Yun,Yang Liu,Yiming Ji,Han Li*

Main category: cs.CV

TL;DR: 提出一种鲁棒的多模态方法，融合相机与激光雷达生成BEV可通行代价图，采用IMU自监督标注；在多种干扰与消融下性能仅小幅下降，显示几何主导与模型稳健；强调可复现实验环境、IMU标注管线与强基线模型三项贡献，并讨论域泛化与数据扩展等局限。


<details>
  <summary>Details</summary>
Motivation: 行星探测漫游车在未知复杂地形中需要可靠的可通行性评估。现有方法对语义或单一模态依赖大、泛化弱且难以获取人工标注。作者旨在用自监督方式从多模态传感器中学习稳定的地形代价估计，减少对人工标签与语义先验的依赖，并建立可复现实验基准。

Method: - 输入：相机图像+LiDAR点云；输出：鸟瞰BEV地形代价图。
- 编码与融合：采用DINOv3作为图像编码器；用FiLM进行跨模态调制式融合；将多模态特征投影到BEV。
- 训练：利用IMU导出的轨迹/振动等信号生成自监督代价标签；损失为Huber回归项+平滑约束项。
- 评估与消融：去色、遮挡、加噪与稀疏化等实验，比较MAE/MSE变化。

Result: 模型在多种破坏条件下MAE/MSE仅小幅上升，例如稀疏化LiDAR时MAE从约0.0775上升到0.0915；去除颜色或遮挡亦影响甚微。结果表明几何信息（LiDAR/结构）主导学习到的代价，模型鲁棒性强。

Conclusion: 多模态BEV代价图预测在IMU自监督标注下表现稳健，几何而非语义主导性能。贡献包括：高保真可复现实验环境、自监督IMU标注流水线、强多模态模型。局限在于IMU标签对语义不敏感、数据多样性不足与域迁移问题；未来将做域泛化与数据集扩充。

Abstract: We present a robust multi-modal framework for predicting traversability
costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce
a bird's-eye-view (BEV) terrain costmap, trained self-supervised using
IMU-derived labels. Key updates include a DINOv3-based image encoder,
FiLM-based sensor fusion, and an optimization loss combining Huber and
smoothness terms. Experimental ablations (removing image color, occluding
inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases
from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry
dominates the learned cost and the model is highly robust. We attribute the
small performance differences to the IMU labeling primarily reflecting terrain
geometry rather than semantics and to limited data diversity. Unlike prior work
claiming large gains, we emphasize our contributions: (1) a high-fidelity,
reproducible simulation environment; (2) a self-supervised IMU-based labeling
pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss
limitations and future work such as domain generalization and dataset
expansion.

</details>


### [31] [End-to-End Visual Autonomous Parking via Control-Aided Attention](https://arxiv.org/abs/2509.11090)
*Chao Chen,Shunyu Yao,Yuanwu He,Tao Feng,Ruojing Song,Yuliang Guo,Xinyu Huang,Chenxu Wu,Ren Liu,Chen Feng*

Main category: cs.CV

TL;DR: 提出CAA-Policy：用控制信号反向引导视觉注意力的端到端仿真驾驶泊车系统，提升时序稳定性、鲁棒性与可解释性，超越E2E与模块化基线。


<details>
  <summary>Details</summary>
Motivation: 端到端泊车需要感知与控制紧密耦合，但纯Transformer自注意力的空间注意在时间上不稳定，影响策略可靠性；现有方法缺乏让控制需求反向约束感知注意的机制。

Method: 提出Control-Aided Attention（CAA）：通过从控制输出反向传播的梯度以自监督方式训练注意力，引导其聚焦对动作输出方差贡献大的视觉特征；辅以短期路点预测作为辅助任务，以及独立训练的运动预测模块以稳定追踪目标泊车位。整体为端到端模仿学习框架。

Result: 在CARLA中，CAA-Policy在准确性、鲁棒性与可解释性上持续优于端到端基线和“BEV分割+混合A*”模块化管线。

Conclusion: 让控制信号指导视觉注意能显著缓解自注意力的时序不稳定，得到更稳健、泛化更好的泊车策略；多任务与目标跟踪进一步提升系统稳定性与实用性。

Abstract: Precise parking requires an end-to-end system where perception adaptively
provides policy-relevant details-especially in critical areas where fine
control decisions are essential. End-to-end learning offers a unified framework
by directly mapping sensor inputs to control actions, but existing approaches
lack effective synergy between perception and control. We find that
transformer-based self-attention, when used alone, tends to produce unstable
and temporally inconsistent spatial attention, which undermines the reliability
of downstream policy decisions over time. Instead, we propose CAA-Policy, an
end-to-end imitation learning system that allows control signal to guide the
learning of visual attention via a novel Control-Aided Attention (CAA)
mechanism. For the first time, we train such an attention module in a
self-supervised manner, using backpropagated gradients from the control outputs
instead of from the training loss. This strategy encourages the attention to
focus on visual features that induce high variance in action outputs, rather
than merely minimizing the training loss-a shift we demonstrate leads to a more
robust and generalizable policy. To further enhance stability, CAA-Policy
integrates short-horizon waypoint prediction as an auxiliary task, and
introduces a separately trained motion prediction module to robustly track the
target spot over time. Extensive experiments in the CARLA simulator show that
\titlevariable~consistently surpasses both the end-to-end learning baseline and
the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,
robustness, and interpretability. Code is released at
https://github.com/Joechencc/CAAPolicy.

</details>


### [32] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 将全景视频生成视为从透视视频到全景投影的适配问题，利用LoRA以低秩增量高效微调预训练视频扩散模型，在仅约1000段视频的数据下实现高质量360°全景视频，几何一致且优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型基于单视点、窄视场的透视投影，难以覆盖360°环境；为适配全景，需要复杂架构或大规模训练，效率低、质量不稳。作者受LoRA在风格迁移中高效适配能力启发，尝试把“透视→全景”的投影差异当作可由低秩变换捕捉的适配问题。

Method: - 将全景生成建模为对预训练透视视频扩散模型的低秩适配。
- 理论分析：当LoRA秩大于任务自由度时，可充分表示透视到全景投影之间的映射。
- 训练：用约1000条视频对模型进行LoRA微调，保持原模型主能力，重点学习投影几何与全景一致性。
- 评估：比较视觉质量、左右一致性、运动多样性与几何投影正确性。

Result: 在少量数据下（~1000视频）即可生成高质量360°全景视频；几何投影正确、左右一致性好、运动更丰富；在多项指标与主观评测上超过先前SOTA；无需复杂新架构或大规模再训练，训练效率高。

Conclusion: 把全景视频生成视为从透视投影适配的问题，并用LoRA进行低秩微调是有效且高效的路径；理论上可解释、实践上数据需求小，能在保持几何正确性的同时提升画质与一致性，优于现有方法。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [33] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出SMILE方法：用超分辨率（SR）任务在多任务学习框架中引导高光谱解混（HU），并给出理论证明任务亲和性与解混收敛性；实验在合成与真实数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: HU受限于低空间分辨率；将SR与HU直接联合会遇到两难：未知任务亲和性（SR是否对HU有正迁移）与HU优化收敛无保障。因此需要理论上证明可行性并设计能传递SR正信息且可收敛的框架。

Method: 提出SMILE：在多任务学习中同时学习SR与HU，构建共享与任务特定表示以将SR的“正指导”迁移到HU。理论部分给出关系/存在性定理，证明SR对HU的正向作用（任务亲和性）；另给出可达性定理，证明HU的最优解可被算法达到，从而保障收敛。

Result: 在合成与真实数据集上，SMILE优于对比方法（未列具体数值），验证SR引导的多任务框架与理论分析的有效性。

Conclusion: 多任务联合SR与HU在理论上可行且存在正迁移；所提SMILE通过共享与专属表示实现SR对HU的正指导，并在理论上保证HU收敛，实验表明该方法实用有效。

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial
resolution, which can be enhanced using super-resolution in a multitask
learning way. However, integrating super-resolution and unmixing directly may
suffer two challenges: Task affinity is not verified, and the convergence of
unmixing is not guaranteed. To address the above issues, in this paper, we
provide theoretical analysis and propose super-resolution guided multi-task
learning method for hyperspectral unmixing (SMILE). The provided theoretical
analysis validates feasibility of multitask learning way and verifies task
affinity, which consists of relationship and existence theorems by proving the
positive guidance of super-resolution. The proposed framework generalizes
positive information from super-resolution to unmixing by learning both shared
and specific representations. Moreover, to guarantee the convergence, we
provide the accessibility theorem by proving the optimal solution of unmixing.
The major contributions of SMILE include providing progressive theoretical
support, and designing a new framework for unmixing under the guidance of
super-resolution. Our experiments on both synthetic and real datasets have
substantiate the usefulness of our work.

</details>


### [34] [A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing](https://arxiv.org/abs/2509.11096)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出一种基于copula的时序依赖建模方法（Cog-TD）用于多时相高光谱解混，通过显式刻画端元与丰度的时间依赖关系，提升对动态材料演化的刻画与解混精度。


<details>
  <summary>Details</summary>
Motivation: 现有多时相解混方法难以充分建模时序依赖，导致无法有效捕获端元光谱变化与丰度动态。copula理论擅长显式建模依赖结构，适合引入以刻画时间相关性。

Method: 1) 重新定义MTHU数学模型，将copula嵌入以显式描述时间依赖结构；2) 构建copula引导的解混框架，同时估计具时序依赖的端元与丰度；3) 两个关键模块：a) copula函数估计，b) 时序依赖引导，将估计的依赖结构用于解混过程；4) 给出理论证明：估计的copula有效且高光谱图像中确有可被copula表示的时间依赖。

Result: 在合成与真实数据集上验证，Cog-TD较现有方法显示更优性能（文摘未给出具体指标与数值，但强调效用与改进）。

Conclusion: 通过copula显式建模时间依赖，Cog-TD能更好地刻画端元与丰度的动态演化，改进多时相高光谱解混效果，并提供相应理论支持。

Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers
and dynamical abundances, which emphasizes the critical temporal information.
However, existing methods have limitations in modeling temporal dependency,
thus fail to capture the dynamical material evolution. Motivated by the ability
of copula theory in modeling dependency structure explicitly, in this paper, we
propose a copula-guided temporal dependency method (Cog-TD) for multitemporal
hyperspectral unmixing. Cog-TD defines new mathematical model, constructs
copula-guided framework and provides two key modules with theoretical support.
The mathematical model provides explicit formulations for MTHU problem
definition, which describes temporal dependency structure by incorporating
copula theory. The copula-guided framework is constructed for utilizing copula
function, which estimates dynamical endmembers and abundances with temporal
dependency. The key modules consist of copula function estimation and temporal
dependency guidance, which computes and employs temporal information to guide
unmixing process. Moreover, the theoretical support demonstrates that estimated
copula function is valid and the represented temporal dependency exists in
hyperspectral images. The major contributions of this paper include redefining
MTHU problem with temporal dependency, proposing a copula-guided framework,
developing two key modules and providing theoretical support. Our experimental
results on both synthetic and real-world datasets demonstrate the utility of
the proposed method.

</details>


### [35] [3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment](https://arxiv.org/abs/2509.11097)
*Nhut Le,Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出3DAeroRelief：首个面向灾后评估的3D语义分割基准，源自低成本无人机航拍重建点云，并评测SOTA模型以揭示挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 现有自然灾害分析多依赖2D影像，缺乏深度与完整空间上下文，易受遮挡影响；同时现有3D数据集多为室内/城市常态场景，几乎无灾害场景。需要一个面向灾后、可规模化采集、具细粒度破坏标注的3D数据基准来支持紧急响应。

Method: 使用低成本UAV在飓风受灾区采集多视角影像，采用SfM+MVS重建高密度3D点云；通过人工2D语义标注并投影到3D生成标注；构建涵盖大规模室外、细粒度结构破坏类别的数据集，并在其上对多种SOTA 3D语义分割模型进行基准评测。

Result: 得到一个高密度、覆盖真实灾后大尺度室外环境、带有细粒度损伤语义标注的3D点云数据集；基准实验显示现有3D分割模型在灾害场景存在显著挑战，同时展现改进空间与应用潜力。

Conclusion: 3DAeroRelief为灾后评估提供关键数据基础，UAV采集具成本低、灵活与安全优势；该数据集将推动更鲁棒的3D视觉系统在真实灾害响应中的研究与应用。

Abstract: Timely assessment of structural damage is critical for disaster response and
recovery. However, most prior work in natural disaster analysis relies on 2D
imagery, which lacks depth, suffers from occlusions, and provides limited
spatial context. 3D semantic segmentation offers a richer alternative, but
existing 3D benchmarks focus mainly on urban or indoor scenes, with little
attention to disaster-affected areas. To address this gap, we present
3DAeroRelief--the first 3D benchmark dataset specifically designed for
post-disaster assessment. Collected using low-cost unmanned aerial vehicles
(UAVs) over hurricane-damaged regions, the dataset features dense 3D point
clouds reconstructed via Structure-from-Motion and Multi-View Stereo
techniques. Semantic annotations were produced through manual 2D labeling and
projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D
large-scale outdoor environments with fine-grained structural damage in
real-world disaster contexts. UAVs enable affordable, flexible, and safe data
collection in hazardous areas, making them particularly well-suited for
emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate
several state-of-the-art 3D segmentation models on the dataset to highlight
both the challenges and opportunities of 3D scene understanding in disaster
response. Our dataset serves as a valuable resource for advancing robust 3D
vision systems in real-world applications for post-disaster scenarios.

</details>


### [36] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 提出GEMMNet用于在模态缺失场景下提升遥感语义分割的稳健性与精度，结合混合特征提取、具多尺度感知的混合融合及互补一致性损失，优于AE、cGAN与SOTA非生成方法（mmformer、shaspec）于Vaihingen与Potsdam数据集。


<details>
  <summary>Details</summary>
Motivation: 现实多模态遥感数据常因传感器故障/恶劣天气导致模态缺失，现有生成式重建（AE/GAN）对异质模态与复杂场景语义上下文建模不足，且易依赖主导模态产生偏置，影响缺失模态下的鲁棒性与性能。

Method: 提出GEMMNet，包含三部分：1）HyFEx混合特征提取器，学习各模态特有表示；2）HyFMA具多尺度感知的混合融合，建模跨模态、跨尺度的协同语义上下文；3）CoLoss互补损失，鼓励跨模态与跨任务一致性，缓解对主导模态的偏置。

Result: 在Vaihingen与Potsdam两个具有挑战性的遥感语义分割数据集上，GEMMNet在含模态缺失情形下优于生成式基线（AE、cGAN）与先进的非生成方法（mmformer、shaspec）；源代码公开。

Conclusion: 针对多模态缺失问题，GEMMNet通过混合特征提取、跨尺度融合与一致性正则显著提升遥感语义分割的鲁棒性与精度，相较现有生成与非生成方案更有效。

Abstract: Multimodal learning has shown significant performance boost compared to
ordinary unimodal models across various domains. However, in real-world
scenarios, multimodal signals are susceptible to missing because of sensor
failures and adverse weather conditions, which drastically deteriorates models'
operation and performance. Generative models such as AutoEncoder (AE) and
Generative Adversarial Network (GAN) are intuitive solutions aiming to
reconstruct missing modality from available ones. Yet, their efficacy in remote
sensing semantic segmentation remains underexplored. In this paper, we first
examine the limitations of existing generative approaches in handling the
heterogeneity of multimodal remote sensing data. They inadequately capture
semantic context in complex scenes with large intra-class and small inter-class
variation. In addition, traditional generative models are susceptible to heavy
dependence on the dominant modality, introducing bias that affects model
robustness under missing modality conditions. To tackle these limitations, we
propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with
three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn
modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness
(HyFMA) to capture modality-synergistic semantic context across scales and (3)
Complementary Loss (CoLoss) scheme to alleviate the inherent bias by
encouraging consistency across modalities and tasks. Our method, GEMMNet,
outperforms both generative baselines AE, cGAN (conditional GAN), and
state-of-the-art non-generative approaches - mmformer and shaspec - on two
challenging semantic segmentation remote sensing datasets (Vaihingen and
Potsdam). Source code is made available.

</details>


### [37] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: 从单段真实环境视频中自动提取并重建可编辑的动态3D烟雾资产，并支持后续交互式物理模拟与编辑；在“野外”场景中较现有方法显著提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有流体/烟雾3D重建多依赖受控实验室数据，真实世界视频复杂、噪声多、背景干扰大，导致重建与时序一致性差；需要面向“野外”视频的稳健方案以获得可用于设计与编辑的高质量4D烟雾资产。

Method: 提出一条完整管线，围绕三大难点设计对应模块：1) 烟雾提取与背景移除，分离前景烟雾；2) 烟雾粒子与相机位姿初始化，稳定优化起点；3) 单视角推断多视角视频（多视几何先验/视图合成）以提升重建与渲染监督。最终得到可物理模拟的烟雾资产，并融合交互式编辑。

Result: 在真实野外视频上，相较既有重建与生成方法，重建质量显著提升，平均PSNR提升约+2.22；能合成时序一致、逼真的视角，同时产出可用于后续模拟与编辑的4D烟雾资产。

Conclusion: 面向野外视频的烟雾重建与编辑实现端到端管线：稳健提取、可靠初始化与多视补全共同提升质量与可编辑性；不仅优于现有方法，还将重建结果无缝接入物理模拟，支持多样真实的烟雾设计与编辑。

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [38] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 提出SVR-GS：在3D高斯点渲染中，用空间可变的像素级稀疏正则替代全局掩码正则，实现更精准地删除低重要性高斯；在三套数据集上显著减小高斯数量与模型体量，仅带来极小PSNR下降。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS常先增密再剪枝，MaskGS等基于全局掩码均值的正则与决定图像质量的逐像素（逐射线）重建损失不匹配，导致剪枝不够针对性、效率与质量兼顾困难。需要一种与成像机理对齐、能对低贡献高斯施加更强稀疏压力的方法。

Method: 提出空间可变正则SVR-GS：1）从每个高斯对每条相机光线的“有效贡献”渲染得到逐像素空间掩码；2）基于该掩码对低重要性区域施加强稀疏约束；3）设计并比较三种空间掩码聚合策略（并进行CUDA实现）；4）通过梯度分析指导最终正则与聚合形式选择；最终在训练中实现更精确的稀疏化与剪枝。

Result: 在Tanks&Temples、Deep Blending、Mip-NeRF360三数据集上，平均相比MaskGS高斯数量减少1.79×，相比原始3DGS减少5.63×；PSNR仅下降0.50 dB（对MaskGS）与0.40 dB（对3DGS）。由此模型更小、更快、更省内存。

Conclusion: 将稀疏正则从全局转为与渲染过程对齐的逐像素空间掩码，可在几乎不损失画质的前提下大幅降低高斯数量，适用于实时应用（机器人、AR/VR、移动感知）。

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.

</details>


### [39] [No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images](https://arxiv.org/abs/2509.11164)
*Diego Eustachio Farchione,Ramzi Idoughi,Peter Wonka*

Main category: cs.CV

TL;DR: 从多视角2D RGB图像出发，先用预训练模块生成稠密点云并融合置信度，再经双分支DGCNN解码器同时回归体积与表面积及其不确定性，结合高斯NLL（实域与对数域）复合损失，轻量、可扩展，能在稀疏视角下准确估计珊瑚几何并对未知形态具良好泛化。


<details>
  <summary>Details</summary>
Motivation: 珊瑚监测依赖体积与表面积等几何指标，但珊瑚形态复杂、传统三维重建成本高且对视角密度敏感。因此需要一种从少量2D图像即可稳定、准确估计三维几何的学习框架，以提升野外监测的效率与可扩展性。

Method: - 输入：少量多视角RGB图像；
- 特征/几何提取：用预训练的VGGT从每个视图提取稠密点图（point maps），并伴随每视图置信度；
- 融合：将多视图点图合并为统一点云，并将置信度作为点特征；
- 预测头：将点云送入两个并行的DGCNN解码器，分别回归整体体积与表面积，同时输出各自的置信度/不确定性估计；
- 损失：采用基于高斯负对数似然的复合损失，在实域与对数域同时约束，以提升稳定性并量化预测不确定性；
- 目标：端到端学习从稀疏图像到全局几何量的映射。

Result: 在实验中，该方法在体积和表面积预测上达到有竞争力的精度，对未见过的珊瑚形态也能良好泛化；同时提供稳定的预测与不确定性估计，适用于稀疏视角输入。

Conclusion: 提出了一个轻量、可扩展的多视图到几何量回归框架，可直接从稀疏2D图像估计珊瑚体积与表面积并给出不确定性，适合大规模珊瑚生长分析与礁体监测应用。

Abstract: Effective reef monitoring requires the quantification of coral growth via
accurate volumetric and surface area estimates, which is a challenging task due
to the complex morphology of corals. We propose a novel, lightweight, and
scalable learning framework that addresses this challenge by predicting the 3D
volume and surface area of coral-like objects from 2D multi-view RGB images.
Our approach utilizes a pre-trained module (VGGT) to extract dense point maps
from each view; these maps are merged into a unified point cloud and enriched
with per-view confidence scores. The resulting cloud is fed to two parallel
DGCNN decoder heads, which jointly output the volume and the surface area of
the coral, as well as their corresponding confidence estimate. To enhance
prediction stability and provide uncertainty estimates, we introduce a
composite loss function based on Gaussian negative log-likelihood in both real
and log domains. Our method achieves competitive accuracy and generalizes well
to unseen morphologies. This framework paves the way for efficient and scalable
coral geometry estimation directly from a sparse set of images, with potential
applications in coral growth analysis and reef monitoring.

</details>


### [40] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 提出Traffic-MLLM：基于Qwen2.5-VL并结合LoRA微调与知识提示（CoT+RAG）的交通视频多模态大模型，在TrafficQA与DriveQA上达SOTA，具备更强时空因果建模、知识注入与零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有交通视频理解方法难以准确建模时空因果关系，且难以有效融合交通法规等领域知识，导致在复杂场景下表现受限。

Method: 以Qwen2.5-VL为骨干，构建面向交通场景的多模态大模型；使用高质量交通多模态数据与LoRA进行轻量化微调，以增强对视频连续时空特征的建模；提出结合Chain-of-Thought与RAG的知识提示模块，将交通法规与领域知识在推理时精确注入，提升逻辑推理与知识适配。

Result: 在TrafficQA与DriveQA基准上取得SOTA表现；展现出显著的零样本推理与跨场景泛化能力。

Conclusion: 通过LoRA增强的多模态骨干与CoT+RAG知识提示，Traffic-MLLM有效弥补了现有方法在时空因果与领域知识融合上的不足，验证了其在交通视频理解中的优越性与实用潜力。

Abstract: As intelligent transportation systems advance, traffic video understanding
plays an increasingly pivotal role in comprehensive scene perception and causal
analysis. Yet, existing approaches face notable challenges in accurately
modeling spatiotemporal causality and integrating domain-specific knowledge,
limiting their effectiveness in complex scenarios. To address these
limitations, we propose Traffic-MLLM, a multimodal large language model
tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,
our model leverages high-quality traffic-specific multimodal datasets and uses
Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing
its capacity to model continuous spatiotemporal features in video sequences.
Furthermore, we introduce an innovative knowledge prompting module fusing
Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),
enabling precise injection of detailed traffic regulations and domain knowledge
into the inference process. This design markedly boosts the model's logical
reasoning and knowledge adaptation capabilities. Experimental results on
TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art
performance, validating its superior ability to process multimodal traffic
data. It also exhibits remarkable zero-shot reasoning and cross-scenario
generalization capabilities.

</details>


### [41] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: 提出Multispectral-NeRF，将NeRF从RGB扩展到多光谱（6波段），通过增大隐藏层、重设计残差损失以衡量光谱差异、适配高位深压缩模块，实现多光谱一致且高质量的3D重建。


<details>
  <summary>Details</summary>
Motivation: 传统基于2D图像的3D重建多依赖RGB信息；现有多光谱融合方案成本高、精度低、几何细节差。NeRF类方法重建质量高，但主流实现仅支持三通道，无法充分利用多光谱信息，限制了在机器人、自动驾驶、VR等需要准确光谱/材质表现的场景中的应用。

Method: 在NeRF/NeRFacto框架上扩展：1) 将输入/网络隐藏层维度扩充以容纳6波段光谱；2) 重新设计残差/损失函数，显式度量重建与参考图像的跨波段光谱差异（光谱一致性约束）；3) 调整数据压缩与位深处理模块，适配多光谱成像较高位深与数据量。整体训练流程与NeRF相似，但在数据通道、损失与数据管线做针对性修改。

Result: 实验显示该方法能有效处理多波段信息，输出的重建既保持几何与外观质量，又更准确地保留场景的光谱特性（相较基线RGB-NeRF/改进模型）。

Conclusion: Multispectral-NeRF将NeRF推广至多光谱域，通过架构与损失、数据处理的三方面改造，实现高质量、多光谱一致的3D重建，为多光谱感知应用提供更准确的场景表示。

Abstract: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.

</details>


### [42] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出SPHERE，将体素与高斯表示结合用于相机端3D语义场景补全，通过语义引导的高斯初始化与物理感知的谐波增强，实现更现实的几何细节与更高语义一致性，并在SemanticKITTI与SSCBench-KITTI-360上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素/平面的SSC虽有进展，但难以捕捉物理规律，导致几何细节逼真度不足；而NeRF/3DGS等神经重建物理感知强，但在大规模自动驾驶场景中计算昂贵、收敛慢、语义精度受限。需要一种兼顾物理细节与语义效率的表示与学习框架。

Method: 提出SPHERE语义-物理耦合表示：1) 语义引导的高斯初始化(SGI)，利用双分支3D场景表示定位“焦点体素”作为锚点，引导高效的高斯初始化；2) 物理感知谐波增强(PHE)，将语义球谐引入以建模物理相关上下文细节，并通过焦点分布对齐促进语义-几何一致，最终输出具真实细节的SSC结果。

Result: 在SemanticKITTI与SSCBench-KITTI-360上进行大量实验与分析，SPHERE在语义与几何指标上优于现有方法，展示出更真实的几何细节与更高的语义准确性。

Conclusion: 融合体素与高斯的SPHERE在保证计算可行性的同时提升了物理细节与语义一致性，通过SGI与PHE两模块实现高效初始化与物理感知增强，并在主流数据集上验证有效，代码已开源。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [43] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: 提出StegOT：在自编码器框架中引入最优传输的多通道最优传输（MCOT）模块，缓解GAN/VAE式图像隐写的模式崩溃，平衡封面与秘密信息，提升隐写与恢复图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN/VAEs的图像隐写常出现模式崩溃，导致嵌入后封面与秘密图像信息分配失衡，进而影响解密/提取质量，需要一种能稳健控制特征分布并实现信息权衡的方法。

Method: 构建自编码器式隐写模型StegOT，在隐写特征空间引入多通道最优传输（MCOT）模块，将多峰分布映射为单峰分布，从而在编码阶段对封面与秘密图像的信息进行可控权衡；整体无需对抗训练，通过OT正则/匹配引导特征对齐与稳定。

Result: 实验表明StegOT实现了封面与秘密信息的权衡，减轻模式崩溃，提升了隐写后图像（stego）与解码恢复图像的客观/主观质量；与既有方法相比，兼顾隐蔽性与可恢复性。

Conclusion: 以最优传输为核心的MCOT在自编码器隐写中有效缓解模式崩溃并平衡信息分布，带来更高质量的stego与恢复结果；方法代码将开源，具备可复现与拓展潜力。

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [44] [The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models](https://arxiv.org/abs/2509.11184)
*Partha Shah,Durva Sankhe,Maariyah Rashid,Zakaa Khaled,Esther Puyol-Antón,Tiarna Lee,Maram Alqarni,Sweta Rai,Andrew P. King*

Main category: cs.CV

TL;DR: 研究探讨Fitzpatrick皮肤色阶（FST）颗粒度对皮肤病变AI分类（良性/恶性）性能与偏差的影响。结果显示：按三组（1/2、3/4、5/6）训练的FST特定模型优于FST均衡的通用模型；降低颗粒度（把1/2与3/4合并为1/2/3/4）会损害性能。作者建议在公平AI研究中从FST过渡到更能表征肤色多样性的替代量表。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤影像AI在不同肤色上表现不均，FST是常用肤色表示但对浅色肤色分组更细，被质疑引入偏差。需明确FST颗粒度如何影响模型性能与公平性，并评估是否应继续依赖FST。

Method: 构建多组二分类（良性/恶性）模型，使用基于FST不同颗粒度划分的特定训练数据（如三组：1/2、3/4、5/6；以及进一步合并为1/2/3/4）进行训练，与使用FST均衡数据训练的通用模型对比，评估性能差异与偏差。

Result: （i）按三组FST特定数据训练的模型总体性能优于使用FST均衡数据训练的通用模型；（ii）将FST分组颗粒度降低（从1/2与3/4分别建模到合并为1/2/3/4）会带来性能下降。

Conclusion: FST分组颗粒度对病变分类模型至关重要；较粗的分组会削弱性能。鉴于FST分组可能包含人为偏差，应在公平AI研究中考虑弃用FST，转向能更全面表征肤色多样性的替代尺度。

Abstract: Artificial intelligence (AI) models to automatically classify skin lesions
from dermatology images have shown promising performance but also
susceptibility to bias by skin tone. The most common way of representing skin
tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has
been criticised for having greater granularity in its skin tone categories for
lighter-skinned subjects. This paper conducts an investigation of the impact
(on performance and bias) on AI classification models of granularity in the FST
scale. By training multiple AI models to classify benign vs. malignant lesions
using FST-specific data of differing granularity, we show that: (i) when
training models using FST-specific data based on three groups (FST 1/2, 3/4 and
5/6), performance is generally better for models trained on FST-specific data
compared to a general model trained on FST-balanced data; (ii) reducing the
granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a
detrimental effect on performance. Our results highlight the importance of the
granularity of FST groups when training lesion classification models. Given the
question marks over possible human biases in the choice of categories in the
FST scale, this paper provides evidence for a move away from the FST scale in
fair AI research and a transition to an alternative scale that better
represents the diversity of human skin tones.

</details>


### [45] [Scaling Up Forest Vision with Synthetic Data](https://arxiv.org/abs/2509.11201)
*Yihang She,Andrew Blake,David Coomes,Srinivasan Keshav*

Main category: cs.CV

TL;DR: 用物理可信的合成LiDAR森林数据预训练，再用极少真实标注微调，可在树木分割任务上接近全量真实数据训练的效果。关键：物理、场景多样性与规模。


<details>
  <summary>Details</summary>
Motivation: 真实3D森林数据昂贵且稀缺，限制了树木分割模型的鲁棒性；其他领域（如自动驾驶）表明合成数据能减轻标注成本，作者欲验证在森林视觉中的可行性。

Method: 构建基于游戏引擎与物理可信LiDAR仿真的合成数据生成流水线，产生大规模、多样化、带注释的3D森林数据；用该数据对树分割模型预训练，再用极少量真实地块数据微调；对比最先进树分割算法在真实数据集上的表现并做消融分析（关注物理、数据多样性与规模）。

Result: 仅用不到0.1公顷的单块真实林地进行微调，预训练模型即可取得与在全量真实数据上训练的模型相竞争的分割精度；实验显示加入物理真实感、提升多样性与增加规模是性能提升的关键因素。

Conclusion: 合成数据在3D森林树木分割中显著减少真实标注需求且效果可靠；物理仿真、数据多样性和规模是成功要素；开放了数据生成流水线与数据集，推动更鲁棒的3D森林视觉系统。

Abstract: Accurate tree segmentation is a key step in extracting individual tree
metrics from forest laser scans, and is essential to understanding ecosystem
functions in carbon cycling and beyond. Over the past decade, tree segmentation
algorithms have advanced rapidly due to developments in AI. However existing,
public, 3D forest datasets are not large enough to build robust tree
segmentation systems. Motivated by the success of synthetic data in other
domains such as self-driving, we investigate whether similar approaches can
help with tree segmentation. In place of expensive field data collection and
annotation, we use synthetic data during pretraining, and then require only
minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for
forest vision tasks, integrating advances in game-engines with physics-based
LiDAR simulation. As a result, we have produced a comprehensive, diverse,
annotated 3D forest dataset on an unprecedented scale. Extensive experiments
with a state-of-the-art tree segmentation algorithm and a popular real dataset
show that our synthetic data can substantially reduce the need for labelled
real data. After fine-tuning on just a single, real, forest plot of less than
0.1 hectare, the pretrained model achieves segmentations that are competitive
with a model trained on the full scale real data. We have also identified
critical factors for successful use of synthetic data: physics, diversity, and
scale, paving the way for more robust 3D forest vision systems in the future.
Our data generation pipeline and the resulting dataset are available at
https://github.com/yihshe/CAMP3D.git.

</details>


### [46] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出“Beyond Sliders”框架，将GAN与扩散模型结合，通过更细粒度的文本与视觉双重引导、对抗式优化，实现对各类（含真实世界、非AIGC）图像的高质量、可定制编辑，优于传统概念滑块方法。


<details>
  <summary>Details</summary>
Motivation: 现有概念滑块等方法在真实世界拍摄的非AIGC图像上效果不佳，难以兼顾真实感与可控定制化，因而需要一种能跨类别、具备高保真与强操控性的图像编辑方案。

Method: 提出“Beyond Sliders”：融合GAN与扩散模型；在对抗式框架下引入细粒度文本与视觉双重指导，对图像进行逐步细化与操控；相对概念滑块提供更精细的引导信号与鲁棒的优化流程。

Result: 在多种应用与数据设置下进行广泛实验，显示该方法在图像质量、真实感与多样性上取得显著提升，并具备更强的泛化与稳健性，适用于多类图像（含真实世界）。

Conclusion: Beyond Sliders在真实世界与多类别图像编辑中表现出更高的质量与可控性，克服概念滑块在非AIGC场景的不足，具有通用性与实用价值。

Abstract: In the realm of image generation, the quest for realism and customization has
never been more pressing. While existing methods like concept sliders have made
strides, they often falter when it comes to no-AIGC images, particularly images
captured in real world settings. To bridge this gap, we introduce Beyond
Sliders, an innovative framework that integrates GANs and diffusion models to
facilitate sophisticated image manipulation across diverse image categories.
Improved upon concept sliders, our method refines the image through fine
grained guidance both textual and visual in an adversarial manner, leading to a
marked enhancement in image quality and realism. Extensive experimental
validation confirms the robustness and versatility of Beyond Sliders across a
spectrum of applications.

</details>


### [47] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 提出一种针对细粒度视觉分类的概率化、组件级STN，用于在Transformer视觉管线中进行几何规范化，显著提升鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: FGVC对物体的旋转、尺度、透视等几何变化极其敏感；现有等变网络代价高且限制假设空间。需要一种灵活、与骨干无关、低约束的几何规范化机制以增强鲁棒性。

Method: 重访STN，将其作为Transformer视觉管线的规范化模块，并提出概率化的组件级扩展：将仿射变换分解为旋转、缩放、剪切，各组件由共享的定位编码器回归；为每个组件建模高斯变分后验，在推理时通过采样进行规范化；设计利用数据增强参数的组件级对齐损失，指导空间对齐。

Result: 在具有挑战性的飞蛾分类基准上，相比其他STN方法，本方法在鲁棒性上持续提升，并带来更稳定的性能。

Conclusion: 组件化、概率化的STN能在不改变主干结构的前提下，为Transformer视觉系统提供有效的几何规范化，缓解FGVC中的几何变形问题并提升鲁棒性。

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [48] [CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning](https://arxiv.org/abs/2509.11219)
*Rabin Dulal,Lihong Zheng,Ashad Kabir*

Main category: cs.CV

TL;DR: 提出一种用于实时奶牛身份识别的少样本学习框架：在协同的模型无关元学习（CCoMAML）上，配合多头注意力特征融合（MHAFF）作为特征提取器，实现从少量样本快速适应新牛只而无需频繁重训；实验显示F1达98.46%/97.91%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RFID 耳标在畜牧管理中常用但易丢失、损坏、被篡改，且存在安全隐患；基于牛鼻纹的生物特征识别具备稳健性，但深度学习方法受限于数据稀缺、采集干扰、以及群体动态变化导致频繁重训的成本。需要一种能用极少样本、快速适应新个体且可实时部署的方法。

Method: 构建少样本学习框架：以协同式 Co-operative Model-Agnostic Meta-Learning（CCoMAML）为元学习核心，通过在多个任务上元训练实现快速适应；采用多头注意力特征融合（MHAFF）作为特征提取模块，提升从牛鼻纹图像中抽取判别性特征的能力；在实时场景中以少量新样本进行快速微调/适配而非完全重训，并与现有少样本识别方法作对比评测。

Result: 在与当前用于牛只识别的少样本学习 SOTA 方法对比中，所提 CCoMAML+MHAFF 获得更高的识别性能，报告的F1分别为98.46%与97.91%（对应不同设置/数据划分），显示出在准确性和泛化上的优势。

Conclusion: 该框架以少样本实现对新牛只的快速适配，降低对大规模标注与频繁重训的依赖，具备实时应用潜力；相较现有方法效果更优，可作为RFID的稳健替代或补充。

Abstract: Cattle identification is critical for efficient livestock farming management,
currently reliant on radio-frequency identification (RFID) ear tags. However,
RFID-based systems are prone to failure due to loss, damage, tampering, and
vulnerability to external attacks. As a robust alternative, biometric
identification using cattle muzzle patterns similar to human fingerprints has
emerged as a promising solution. Deep learning techniques have demonstrated
success in leveraging these unique patterns for accurate identification. But
deep learning models face significant challenges, including limited data
availability, disruptions during data collection, and dynamic herd compositions
that require frequent model retraining. To address these limitations, this
paper proposes a novel few-shot learning framework for real-time cattle
identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with
Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This
model offers great model adaptability to new data through efficient learning
from few data samples without retraining. The proposed approach has been
rigorously evaluated against current state-of-the-art few-shot learning
techniques applied in cattle identification. Comprehensive experimental results
demonstrate that our proposed CCoMAML with MHAFF has superior cattle
identification performance with 98.46% and 97.91% F1 scores.

</details>


### [49] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: 提出ANROT-HELANet：在小样本学习中用Hellinger距离进行类特征聚合，并引入对抗与自然鲁棒训练和“Hellinger相似度对比损失”，在多基准上提升精度且显著提升对抗与噪声鲁棒性，同时在重建质量（FID）优于VAE/WAE。


<details>
  <summary>Details</summary>
Motivation: 现有FSL方法（含基于KL的贝叶斯估计）虽有效，但对对抗攻击与自然噪声脆弱，需要兼顾精度与鲁棒性的统一框架。

Method: 1) 以Hellinger距离驱动的特征类聚合，用于元/变分式few-shot推断；2) 对抗+自然鲁棒训练策略，能抵抗ε≤0.30的对抗扰动和σ≤0.30的高斯噪声；3) 提出Hellinger相似度对比损失，广义化余弦相似对比损失；4) 结合注意力机制以提升判别性与稳健性；5) 以重建分支评估生成质量。

Result: 在四个few-shot基准上达到SOTA：miniImageNet上1-shot/5-shot分别提升约1.20%/1.40%；对抗与噪声鲁棒性显著（ε=0.30、σ=0.30仍稳健）；重建质量FID=2.75，优于VAE(3.43)和WAE(3.38)。

Conclusion: Hellinger距离的类聚合与新型对比损失、注意力和鲁棒训练相结合，可在FSL中同时提升精度与鲁棒性，并在分类与重建两方面达到新的SOTA。

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a
few data samples, has demonstrated promising and superior performances to
ordinary CNN methods. While Bayesian based estimation approaches using
Kullback-Leibler (KL) divergence have shown improvements, they remain
vulnerable to adversarial attacks and natural noises. We introduce
ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation
Network that significantly advances the state-of-the-art in FSL robustness and
performance. Our approach implements an adversarially and naturally robust
Hellinger distance-based feature class aggregation scheme, demonstrating
resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian
noise up to $\sigma=0.30$. The network achieves substantial improvements across
benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot
scenarios on miniImageNet respectively. We introduce a novel Hellinger
Similarity contrastive loss function that generalizes cosine similarity
contrastive loss for variational few-shot inference scenarios. Our approach
also achieves superior image reconstruction quality with a FID score of 2.75,
outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive
experiments conducted on four few-shot benchmarked datasets verify that
ANROT-HELANet's combination of Hellinger distance-based feature aggregation,
attention mechanisms, and our novel loss function establishes new
state-of-the-art performance while maintaining robustness against both
adversarial and natural perturbations. Our code repository will be available at
https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


### [50] [MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction](https://arxiv.org/abs/2509.11232)
*Seongwan Park,Jieun Woo,Siheon Yang*

Main category: cs.CV

TL;DR: 提出MIS-LSTM：将多通道图像化的连续传感器与1D-CNN编码的离散事件，通过注意力融合后交由LSTM建模，以预测日级睡眠质量与压力；并引入不确定性感知集成（UALRE）以在低置信多数表决时采用高置信个体预测，Macro-F1由0.615提升至0.647，优于多种强基线。


<details>
  <summary>Details</summary>
Motivation: 多模态日常生命日志数据包含连续传感器流与稀疏离散事件，具有长程时序依赖与跨模态异质性。现有方法要么忽视一种模态，要么难以在日级别稳定预测睡眠与压力，且集成时未充分利用置信度信息。作者希望构建一个既能融合模态特征、又能捕捉长依赖、并在不确定性下稳健决策的框架。

Method: 1) 预处理：将连续传感器流按N小时切块，渲染为多通道“图像”；离散事件以专用1D-CNN编码。2) 融合：使用卷积块注意力模块（CBAM）对两路特征进行注意力引导融合，形成块级嵌入。3) 序列建模：将块级嵌入序列输入LSTM，捕捉跨天长程依赖，输出日级预测。4) 集成：提出不确定性感知集成UALRE——当多数表决置信度低时，允许由高置信的个体模型预测覆盖多数。5) 消融：比较多通道成像 vs. 纵向堆叠、不同块粒度（4小时最佳）、以及离散模态专用编码的作用。

Result: 在2025 ETRI Lifelog Challenge数据集上：基础MIS-LSTM Macro-F1=0.615；加入UALRE后Macro-F1=0.647，超过强LSTM、1D-CNN与CNN基线。消融显示多通道成像更优、4小时切块最佳、离散模态专用编码有效。

Conclusion: MIS-LSTM通过模态特定编码+注意力融合+LSTM长依赖建模，有效提升日级睡眠与压力预测；UALRE利用置信度提升集成稳健性并带来额外性能增益。该框架为多模态生命日志预测提供了可扩展、鲁棒的解决方案。

Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with
an LSTM sequence model for sleep quality and stress prediction at the day level
from multimodal lifelog data. Continuous sensor streams are first partitioned
into N-hour blocks and rendered as multi-channel images, while sparse discrete
events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention
Module fuses the two modalities into refined block embeddings, which an LSTM
then aggregates to capture long-range temporal dependencies. To further boost
robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides
lowconfidence majority votes with high-confidence individual predictions.
Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base
MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to
0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm
(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the
benefit of a 4-hour block granularity, and (iii) the efficacy of
modality-specific discrete encoding.

</details>


### [51] [Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States](https://arxiv.org/abs/2509.11247)
*Robert Long,Rongxin Jiang,Mingrui Yan*

Main category: cs.CV

TL;DR: 提出CMLReID：一个面向持续学习的ReID框架，同时处理同衣(SC)与换衣(CC)，通过语义提示与知识融合缓解任务错配与遗忘，实验在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实监控中人员再识别受换衣影响且需持续学习，现有方法多只适配SC或将CC单独处理，导致在混合与连续场景下表现不稳、跨任务表示错配与灾难性遗忘严重。

Method: 基于CLIP的CMLReID框架，包括两项关键任务/模块：1) CASP：上下文感知语义提示，自适应生成prompt并引入语义上下文，将多粒度视觉线索与文本语义空间对齐；2) AKFP：自适应知识融合与投影，采用双路径学习器，生成稳健的SC/CC原型，并通过“衣物状态感知投影损失”对齐特征与原型。

Result: 在多种ReID数据集与顺序学习设置下，CMLReID在SC与CC场景均取得优于现有SOTA的性能，表现出较强鲁棒性与泛化能力。

Conclusion: 将SC与CC统一到持续学习的混合任务中，通过语义对齐与原型融合有效缓解表示错配与遗忘，提供了在现实监控场景下更实用的ReID解决方案。

Abstract: Person Re-Identification (ReID) has several challenges in real-world
surveillance systems due to clothing changes (CCReID) and the need for
maintaining continual learning (LReID). Previous existing methods either
develop models specifically for one application, which is mostly a same-cloth
(SC) setting or treat CCReID as its own separate sub-problem. In this work, we
will introduce the LReID-Hybrid task with the goal of developing a model to
achieve both SC and CC while learning in a continual setting. Mismatched
representations and forgetting from one task to the next are significant
issues, we address this with CMLReID, a CLIP-based framework composed of two
novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive
prompts, and also incorporates context to align richly multi-grained visual
cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection
(AKFP) which produces robust SC/CC prototypes through the use of a dual-path
learner that aligns features with our Clothing-State-Aware Projection Loss.
Experiments performed on a wide range of datasets and illustrate that CMLReID
outperforms all state-of-the-art methods with strong robustness and
generalization despite clothing variations and a sophisticated process of
sequential learning.

</details>


### [52] [Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.11264)
*Kerun Mi,Guoliang Kang,Guangyu Li,Lin Zhao,Tao Zhou,Chen Gong*

Main category: cs.CV

TL;DR: 提出一种在类增量无监督领域自适应（CI-UDA）场景下的无回放方法：通过挖掘并对齐跨域共享、与类别无关的“属性”，减轻灾难性遗忘并缓解域偏移，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: CI-UDA中目标域类别分批出现且互不重叠，需持续适配而不遗忘先前目标类。现有方法常依赖存储旧目标样本（回放）防遗忘，同时仅对共享类做对齐，导致内存增长与不对称对齐引发遗忘。需要一种既不依赖回放、又能稳定跨时间步保留知识且缓解域差异的方案。

Method: 利用CLIP提取与类别无关的“属性”作为跨域共享的表示：以“键-值”对建模属性（键为视觉原型，值为文本提示），分别维护源域与目标域两份属性字典。通过跨域属性对齐实现域适配，具体通过视觉注意力一致性和预测一致性两种约束促进对齐，从而在无回放条件下减少遗忘。

Result: 在三个CI-UDA基准上显著优于之前SOTA，同时有效缓解灾难性遗忘；给出开源代码（VisTA）。

Conclusion: 基于属性的类无关、域不变知识挖掘与对齐，可在CI-UDA中实现无回放的稳定适配，降低遗忘并提升整体性能。

Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a
model from a labeled source domain to an unlabeled target domain, where the
sets of potential target classes appearing at different time steps are disjoint
and are subsets of the source classes. The key to solving this problem lies in
avoiding catastrophic forgetting of knowledge about previous target classes
during continuously mitigating the domain shift. Most previous works
cumbersomely combine two technical components. On one hand, they need to store
and utilize rehearsal target sample from previous time steps to avoid
catastrophic forgetting; on the other hand, they perform alignment only between
classes shared across domains at each time step. Consequently, the memory will
continuously increase and the asymmetric alignment may inevitably result in
knowledge forgetting. In this paper, we propose to mine and preserve
domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task.
Specifically, via using CLIP, we extract the class-agnostic properties which we
name as "attribute". In our framework, we learn a "key-value" pair to represent
an attribute, where the key corresponds to the visual prototype and the value
is the textual prompt. We maintain two attribute dictionaries, each
corresponding to a different domain. Then we perform attribute alignment across
domains to mitigate the domain shift, via encouraging visual attention
consistency and prediction consistency. Through attribute modeling and
cross-domain alignment, we effectively reduce catastrophic knowledge forgetting
while mitigating the domain shift, in a rehearsal-free way. Experiments on
three CI-UDA benchmarks demonstrate that our method outperforms previous
state-of-the-art methods and effectively alleviates catastrophic forgetting.
Code is available at https://github.com/RyunMi/VisTA.

</details>


### [53] [Synthetic Dataset Evaluation Based on Generalized Cross Validation](https://arxiv.org/abs/2509.11273)
*Zhihang Song,Dingyi Yao,Ruibo Ming,Lihui Peng,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: 提出一个评估合成数据质量的通用框架：用任务模型在合成与多真实数据集上交叉训练/测试，构建归一化后的广义交叉验证矩阵，进而给出“模拟质量”和“迁移质量”两项指标；在Virtual KITTI上验证有效。


<details>
  <summary>Details</summary>
Motivation: 合成数据广泛应用，但缺乏统一、可比的质量评估标准。现有研究分散、难以跨数据集/任务比较，阻碍生成方法改进与资源优化利用。作者旨在提供可泛化、可量化、可比较的评估方案。

Method: - 用任务模型（如YOLOv5s）分别在合成数据与多个真实基准（如KITTI、BDD100K）上训练/测试，得到跨域性能矩阵。
- 对矩阵进行归一化，构建广义交叉验证（GCV）矩阵以刻画域可迁移性。
- 设计两项指标：
  1) 模拟质量：量化合成数据与真实数据的相似度；
  2) 迁移质量：评估合成数据对多真实场景的覆盖与多样性。
- 在Virtual KITTI上进行实验验证。

Result: 在Virtual KITTI实验中，框架与指标能有效区分和量化合成数据的逼真度与跨域迁移能力，显示出可扩展、可比较的评估效果。

Conclusion: 该框架提供统一、可量化的合成数据评估方法，克服传统局限，可指导合成数据集的优化与AI研究中的有效利用。

Abstract: With the rapid advancement of synthetic dataset generation techniques,
evaluating the quality of synthetic data has become a critical research focus.
Robust evaluation not only drives innovations in data generation methods but
also guides researchers in optimizing the utilization of these synthetic
resources. However, current evaluation studies for synthetic datasets remain
limited, lacking a universally accepted standard framework. To address this,
this paper proposes a novel evaluation framework integrating generalized
cross-validation experiments and domain transfer learning principles, enabling
generalizable and comparable assessments of synthetic dataset quality. The
framework involves training task-specific models (e.g., YOLOv5s) on both
synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),
forming a cross-performance matrix. Following normalization, a Generalized
Cross-Validation (GCV) Matrix is constructed to quantify domain
transferability. The framework introduces two key metrics. One measures the
simulation quality by quantifying the similarity between synthetic data and
real-world datasets, while another evaluates the transfer quality by assessing
the diversity and coverage of synthetic data across various real-world
scenarios. Experimental validation on Virtual KITTI demonstrates the
effectiveness of our proposed framework and metrics in assessing synthetic data
fidelity. This scalable and quantifiable evaluation solution overcomes
traditional limitations, providing a principled approach to guide synthetic
dataset optimization in artificial intelligence research.

</details>


### [54] [ROSGS: Relightable Outdoor Scenes With Gaussian Splatting](https://arxiv.org/abs/2509.11275)
*Lianjun Liao,Chunhui Zhang,Tong Wu,Henglei Lv,Bailin Deng,Lin Gao*

Main category: cs.CV

TL;DR: 提出ROSGS：一种基于高斯点渲的两阶段管线，实现高效、准确的室外场景重建与重光照分解，兼顾高频阳光与低频天光，从而在渲染效率与重光照精度上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF/3DGS重光照方法要么计算开销大（神经网络重、训练慢），要么使用低频光照表示（如低阶SH），导致渲染低效、细节与高频光照难以还原，尤其是户外无界场景与变化光照下的几何-反射-光照分解困难。

Method: 两阶段ROSGS：1) 利用单目法线先验，采用紧凑的2D Gaussian Splatting（2DGS）重建几何，获得高效且准确的几何基础；2) 在已重建几何上进行外观与光照分解，提出混合光照模型：用球面高斯（SG）描述方向性、高频的太阳直射分量；用球谐（SH）系数学习辐射传递函数以覆盖低频天光。

Result: 在定量与定性实验上均优于现有方法，在室外场景重光照任务中实现更高的重光照精度与更高的渲染/训练效率。

Conclusion: 通过2DGS几何重建与SG+SH混合光照建模，ROSGS兼顾效率与精度，能有效分解与重光照户外无界场景，达成当前最优表现。

Abstract: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.

</details>


### [55] [Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations](https://arxiv.org/abs/2509.11287)
*Yifan Lu,Ziqi Zhang,Chunfeng Yuan,Jun Gao,Congxuan Zhang,Xiaojuan Qi,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: 提出APASI：一种无需外部标注或辅助模型的自驱动偏好对齐方法，用于缓解LVLM产生与图像不一致的幻觉。通过让模型自注入（self-injection）幻觉生成偏好分级的响应对，并结合迭代与课程式训练不断提升可靠性，在多基准上优于或比肩依赖外部数据的方法。


<details>
  <summary>Details</summary>
Motivation: 现有缓解LVLM幻觉的方法多依赖人类偏好标注或额外模型，成本高、难以持续改进且泛化受限。需要一种能在无外部依赖下、可持续更新偏好数据并与真实幻觉模式对齐的方案。

Method: APASI让目标LVLM对自身生成的回答进行“自注入”幻觉：基于三条关于幻觉的关键观察，构造一对响应（偏好与非偏好），使非偏好响应逼真地模拟常见幻觉模式。随后采用迭代式偏好对齐训练，并引入课程学习逐步提高数据难度，周期性刷新偏好数据，实现稳定持续提升。

Result: 在六个基准上，对于三个基线LVLM，APASI显著降低幻觉率，并在总体性能上达到或超过依赖外部偏好对齐方法的效果，显示出有效性与可泛化性。

Conclusion: APASI无需外部标注或辅助模型，即可通过自注入幻觉构造偏好数据并迭代对齐训练，稳定缓解LVLM幻觉问题，且在多基准、多模型上表现与主流外部依赖方法相当或更优。

Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination
problems, where the model-generated responses are inconsistent with the visual
inputs. Existing hallucination mitigation methods are mainly based on
preference alignment and require external human annotations or auxiliary models
for preference data collection, which increase costs and limit sustainable
improvement. To tackle these challenges, we propose Autonomous Preference
Alignment via Self-Injection (APASI), a novel and generalizable method that
mitigates hallucinations without external dependencies. APASI leverages the
target LVLM to self-inject hallucinations into a generated response, creating a
pair of responses with varying preference levels. During the self-injection
process, the dis-preferred response is generated based on three key
observations of hallucinations, ensuring it simulates real hallucination
patterns. This fidelity offers an accurate learning signal for hallucination
mitigation. Moreover, APASI incorporates an iterative alignment training
strategy combined with curriculum learning to periodically update the
preference data with increasing challenge, enabling stable and continuous
enhancement of the LVLM. Extensive experiments across six benchmarks show that
APASI not only effectively mitigates hallucinations for three baseline models
but also achieves comparable or even superior performance to alignment-based
methods with external dependency, thereby demonstrating its effectiveness and
generalization capability. The code is available at
https://github.com/davidluciolu/APASI.

</details>


### [56] [Leveraging Geometric Priors for Unaligned Scene Change Detection](https://arxiv.org/abs/2509.11292)
*Ziling Liu,Ziwei Chen,Mingqi Gao,Jinyu Yang,Feng Zheng*

Main category: cs.CV

TL;DR: 提出一个无需训练的框架，结合几何基础模型的几何先验与视觉基础模型表征，在视角不对齐情况下实现更可靠的场景变化检测，并在PSCD、ChangeSim、PASLCD上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有非对齐场景变化检测仅依赖2D外观匹配来建立跨图像对应，遇到大视角变化时易漂移或失败；且监督仅限小规模数据集的2D变化掩码，缺乏可泛化的多视几何知识，难以判断可见重叠与遮挡，缺乏显式几何推理。

Method: 引入几何基础模型提供几何先验（如可见重叠区域、对应关系与遮挡显式检测），并与视觉基础模型的表征融合，构建一个训练免费（training-free）的检测框架，从而在视角不对齐下实现稳健的跨视角对应与变化判断。

Result: 在PSCD、ChangeSim、PASLCD三个数据集上进行大量评测，方法表现出更优且稳健的性能，优于现有方法。

Conclusion: 显式利用几何先验能够弥补2D外观匹配的局限，显著提升非对齐场景变化检测的可靠性与鲁棒性；所提训练免费框架验证了几何与视觉基础模型结合的有效性。

Abstract: Unaligned Scene Change Detection aims to detect scene changes between image
pairs captured at different times without assuming viewpoint alignment. To
handle viewpoint variations, current methods rely solely on 2D visual cues to
establish cross-image correspondence to assist change detection. However, large
viewpoint changes can alter visual observations, causing appearance-based
matching to drift or fail. Additionally, supervision limited to 2D change masks
from small-scale SCD datasets restricts the learning of generalizable
multi-view knowledge, making it difficult to reliably identify visual overlaps
and handle occlusions. This lack of explicit geometric reasoning represents a
critical yet overlooked limitation. In this work, we are the first to leverage
geometric priors from a Geometric Foundation Model to address the core
challenges of unaligned SCD, including reliable identification of visual
overlaps, robust correspondence establishment, and explicit occlusion
detection. Building on these priors, we propose a training-free framework that
integrates them with the powerful representations of a visual foundation model
to enable reliable change detection under viewpoint misalignment. Through
extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we
demonstrate that our approach achieves superior and robust performance. Our
code will be released at https://github.com/ZilingLiu/GeoSCD.

</details>


### [57] [UnLoc: Leveraging Depth Uncertainties for Floorplan Localization](https://arxiv.org/abs/2509.11301)
*Matthias Wüest,Francis Engelmann,Ondrej Miksik,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: UnLoc提出一种利用楼层平面图进行顺序相机定位的高效数据驱动方法，引入概率深度建模与不确定性估计，结合通用单目深度模型，无需为每个环境训练专用网络；在合成与真实数据上显著提升准确性与鲁棒性，在LaMAR HGE上实现长序列与短序列回忆率分别提升2.7倍与16.7倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于楼层图的定位方法存在两大问题：1）深度预测缺乏不确定性建模，导致匹配与优化易受噪声影响；2）需要为每个环境训练定制深度网络，泛化差、部署成本高。楼层平面图稳定且长期可用，若能有效融合视觉与几何并量化不确定性，可提升跨环境鲁棒性与精度。

Method: 提出UnLoc：构建显式概率深度模型，将单目深度预测表示为概率分布以量化不确定性；利用现成的预训练单目深度模型获取深度分布而非点估计；在顺序定位框架中融合楼层平面图与深度不确定性，通过概率匹配/优化提升姿态估计的稳定性与精度；无需按环境训练专用深度网络，提高泛化与部署效率。

Result: 在大规模合成与真实数据集上验证，较现有方法在准确性与鲁棒性上显著提升；在LaMAR HGE数据集上，100帧长序列的定位召回提升2.7倍，15帧短序列提升16.7倍。

Conclusion: 概率化深度与不确定性建模结合楼层平面图，可在无需环境特定训练的前提下，实现更鲁棒与准确的顺序相机定位；方法对未见空间具有更强的泛化，并在多数据集上达成明显SOTA改进。

Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera
localization within floorplans. Floorplan data is readily available, long-term
persistent, and robust to changes in visual appearance. We address key
limitations of recent methods, such as the lack of uncertainty modeling in
depth predictions and the necessity for custom depth networks trained for each
environment. We introduce a novel probabilistic model that incorporates
uncertainty estimation, modeling depth predictions as explicit probability
distributions. By leveraging off-the-shelf pre-trained monocular depth models,
we eliminate the need to rely on per-environment-trained depth networks,
enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale
synthetic and real-world datasets, demonstrating significant improvements over
existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$
times higher localization recall on long sequences (100 frames) and $16.7$
times higher on short ones (15 frames) than the state of the art on the
challenging LaMAR HGE dataset.

</details>


### [58] [Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding](https://arxiv.org/abs/2509.11323)
*Jian Song,Wei Mei,Yunfeng Xu,Qiang Fu,Renke Kou,Lina Bu,Yucheng Long*

Main category: cs.CV

TL;DR: 提出SIKNet，一种语义独立编码的学习辅助滤波器，用于MOT中的运动估计，相比传统卡尔曼滤波与现有学习滤波器更稳健、精度更高。


<details>
  <summary>Details</summary>
Motivation: 经典MOT多采用线性恒速模型的卡尔曼滤波，但在参数失配或目标非平稳运动时性能不佳，易导致跟踪失败与ID切换。需要一种能自适应复杂运动、提高鲁棒性的学习辅助运动估计方法，并且希望独立评估该模块的贡献。

Method: 提出Semantic-Independent KalmanNet（SIKNet）。核心是Semantic-Independent Encoder（SIE）对状态向量进行两步编码：1）使用kernel=1的一维卷积，沿同语义元素维度在不同状态向量间卷积，提取独立语义信息；2）通过全连接+非线性激活，建模异构语义元素间的非线性与交互依赖；整体作为学习辅助滤波器替代/增强KF的运动估计。构建了大规模半模拟数据集以独立评测运动估计模块。

Result: 在构建的半模拟数据集与若干开源MOT数据上，SIKNet优于传统KF，并在鲁棒性与精度上超过现有学习辅助滤波方法。

Conclusion: 语义独立编码的学习滤波框架能有效缓解KF参数失配与非平稳运动问题，提升MOT运动估计与跟踪稳定性；代码开源，便于复现与扩展。

Abstract: Motion estimation is a crucial component in multi-object tracking (MOT).
  It predicts the trajectory of objects by analyzing the changes in their
positions in consecutive frames of images, reducing tracking failures and
identity switches.
  The Kalman filter (KF) based on the linear constant-velocity model is one of
the most commonly used methods in MOT.
  However, it may yield unsatisfactory results when KF's parameters are
mismatched and objects move in non-stationary.
  In this work, we utilize the learning-aided filter to handle the motion
estimation of MOT.
  In particular, we propose a novel method named Semantic-Independent KalmanNet
(SIKNet), which encodes the state vector (the input feature) using a
Semantic-Independent Encoder (SIE) by two steps.
  First, the SIE uses a 1D convolution with a kernel size of 1, which convolves
along the dimension of homogeneous-semantic elements across different state
vectors to encode independent semantic information.
  Then it employs a fully-connected layer and a nonlinear activation layer to
encode nonlinear and cross-dependency information between
heterogeneous-semantic elements.
  To independently evaluate the performance of the motion estimation module in
MOT, we constructed a large-scale semi-simulated dataset from several
open-source MOT datasets.
  Experimental results demonstrate that the proposed SIKNet outperforms the
traditional KF and achieves superior robustness and accuracy than existing
learning-aided filters.
  The code is available at (https://github.com/SongJgit/filternet and
https://github.com/SongJgit/TBDTracker).

</details>


### [59] [Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency](https://arxiv.org/abs/2509.11328)
*Mingyuan Meng*

Main category: cs.CV

TL;DR: 该论文探讨在医学影像计算中如何高效建模细粒度的长程依赖，发现并论证MLP相比Transformer/CNN在高分辨率特征上的长程依赖建模更具可行性与效率，从而在多类任务上取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像任务同时需要全局上下文与局部微小病灶细节的捕捉。CNN受局部感受野限制，Transformer虽能建模长程依赖但自注意力计算昂贵，难以处理高分辨率细节；MLP被认为在长程依赖方面具计算/内存优势，却在MIC领域研究不足。

Method: 先在像素级（分割、配准）与图像级（分类、回归）任务中创新性使用Transformer；随后转向MLP，设计并开发能在高分辨率特征上建模细粒度长程依赖的MLP视觉模型，并进行广泛实验评估。

Result: 实验表明长程依赖对MIC至关重要；更重要的是，MLP能在包含丰富解剖/病理细节的高分辨率特征上更可行地建模更细粒度的长程依赖，带来在多种医学视觉任务上的一致性能提升。

Conclusion: MLP在医学影像中相较Transformer/CNN更适合作为建模细粒度长程依赖的范式，有望成为下一代医学视觉骨干。

Abstract: Medical Image Computing (MIC) is a broad research topic covering both
pixel-wise (e.g., segmentation, registration) and image-wise (e.g.,
classification, regression) vision tasks. Effective analysis demands models
that capture both global long-range context and local subtle visual
characteristics, necessitating fine-grained long-range visual dependency
modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by
intrinsic locality, transformers excel at long-range modeling; however, due to
the high computational loads of self-attention, transformers typically cannot
process high-resolution features (e.g., full-scale image features before
downsampling or patch embedding) and thus face difficulties in modeling
fine-grained dependency among subtle medical image details. Concurrently,
Multi-layer Perceptron (MLP)-based visual models are recognized as
computation/memory-efficient alternatives in modeling long-range visual
dependency but have yet to be widely investigated in the MIC community. This
doctoral research advances deep learning-based MIC by investigating effective
long-range visual dependency modeling. It first presents innovative use of
transformers for both pixel- and image-wise medical vision tasks. The focus
then shifts to MLPs, pioneeringly developing MLP-based visual models to capture
fine-grained long-range visual dependency in medical images. Extensive
experiments confirm the critical role of long-range dependency modeling in MIC
and reveal a key finding: MLPs provide feasibility in modeling finer-grained
long-range dependency among higher-resolution medical features containing
enriched anatomical/pathological details. This finding establishes MLPs as a
superior paradigm over transformers/CNNs, consistently enhancing performance
across various medical vision tasks and paving the way for next-generation
medical vision backbones.

</details>


### [60] [Dual Band Video Thermography Near Ambient Conditions](https://arxiv.org/abs/2509.11334)
*Sriram Narayanan,Mani Ramanagopal,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 提出首个用双谱段热成像视频分离反射与自发辐射成分的方法，基于双波段成像模型估计表面发射率与随时间变化的温度，并隔离动态背景，在多材质标定与复杂场景中取得定量与定性验证。


<details>
  <summary>Details</summary>
Motivation: 在近室温环境下，热相机测得的红外信号既包含环境光的反射/透射，也包含物体自身的热辐射，两者量级相当且随时间变化；传统方法常假设一项占优或另一项为常量，无法准确恢复发射率、温度、反射率与形状等关键物理属性。

Method: 提出双波段热图像形成模型，使用两台具有不同光谱响应的热相机同步拍摄视频；基于该模型设计估计算法，联合求解表面发射率（时间不变/缓变）、随时间变化的物体温度，并分离出动态背景反射成分；包含标定流程以获得精确的发射率与相机响应。

Result: 在多种材料上通过精确标定的发射率进行定量评估，显示能准确估计发射率与温度并分离反射；在真实复杂场景（如盛有热液体的玻璃杯、背景有人移动）中给出有说服力的定性结果。

Conclusion: 双谱段热成像与相应估计算法可在近室温复杂环境中有效分离反射与自发辐射，恢复物体物理属性并提升热视觉在计算机视觉应用中的可靠性。

Abstract: Long-wave infrared radiation captured by a thermal camera consists of two
components: (a) light from the environment reflected or transmitted by a
surface, and (b) light emitted by the surface after undergoing heat transport
through the object and exchanging heat with the surrounding environment.
Separating these components is essential for understanding object properties
such as emissivity, temperature, reflectance and shape. Previous thermography
studies often assume that only one component is dominant (e.g., in welding) or
that the second component is constant and can be subtracted. However, in
near-ambient conditions, which are most relevant to computer vision
applications, both components are typically comparable in magnitude and vary
over time. We introduce the first method that separates reflected and emitted
components of light in videos captured by two thermal cameras with different
spectral sensitivities. We derive a dual-band thermal image formation model and
develop algorithms to estimate the surface's emissivity and its time-varying
temperature while isolating a dynamic background. We quantitatively evaluate
our approach using carefully calibrated emissivities for a range of materials
and show qualitative results on complex everyday scenes, such as a glass filled
with hot liquid and people moving in the background.

</details>


### [61] [Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning](https://arxiv.org/abs/2509.11344)
*Huaiyuan Qin,Muli Yang,Siyuan Hu,Peng Hu,Yu Zhang,Chen Gong,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 研究在“实例一致性”不成立（非标志性/非iconic图像）时，自监督学习仍能否学到有效表示，并给出视角多样性与性能的定量关系。


<details>
  <summary>Details</summary>
Motivation: 传统SSL假设同一图像的不同视角为正样本对，但在非iconic数据中不同视角可能含有不同目标或语义，此假设失效。需要理解在这种情况下SSL是否仍有效、何种视角多样性有利、以及如何度量视角间信息关联。

Method: 进行系统消融：在正对不具严格实例一致性的设定下训练SSL；通过控制视角多样性（零重叠、更小裁剪尺度）来观察对下游分类与密集预测任务的影响；使用地球移动距离（EMD）作为估计器来度量视角间互信息并分析其与性能的相关性。

Result: 即便正样本对缺乏严格实例一致性，SSL仍能学习到有意义的表示；增加视角多样性通常提升下游性能，但过度多样性会适得其反；EMD与学习效果呈现“适中最优”的相关性，即中等EMD对应更好的SSL表现。该发现对多种设置与数据源具有鲁棒性与可迁移性。

Conclusion: SSL不必严格依赖实例一致性；应控制视角多样性在一个适中范围内。EMD可作为实用指标指导视角设计与SSL框架调参，以提升分类与密集预测性能。

Abstract: Self-supervised learning (SSL) conventionally relies on the instance
consistency paradigm, assuming that different views of the same image can be
treated as positive pairs. However, this assumption breaks down for non-iconic
data, where different views may contain distinct objects or semantic
information. In this paper, we investigate the effectiveness of SSL when
instance consistency is not guaranteed. Through extensive ablation studies, we
demonstrate that SSL can still learn meaningful representations even when
positive pairs lack strict instance consistency. Furthermore, our analysis
further reveals that increasing view diversity, by enforcing zero overlapping
or using smaller crop scales, can enhance downstream performance on
classification and dense prediction tasks. However, excessive diversity is
found to reduce effectiveness, suggesting an optimal range for view diversity.
To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to
measure mutual information between views, finding that moderate EMD values
correlate with improved SSL learning, providing insights for future SSL
framework design. We validate our findings across a range of settings,
highlighting their robustness and applicability on diverse data sources.

</details>


### [62] [Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness](https://arxiv.org/abs/2509.11355)
*Robin Narsingh Ranabhat,Longwei Wang,Amit Kumar Patel,KC santosh*

Main category: cs.CV

TL;DR: 提出两种正则化策略，使CNN更偏向形状表征并提升对常见扰动的鲁棒性；在CIFAR-10-C上提升鲁棒性且不损失干净准确率。


<details>
  <summary>Details</summary>
Motivation: CNN在图像分类上强，但对常见腐蚀/扰动易受影响，原因在于其更依赖局部纹理而非全局形状，和人类感知形成反差。需要方法引导模型学习形状偏置以获得更稳健的表征。

Method: 提出两类互补的损失级正则化：1) 特征一致性损失：对原图与低频滤波输入，约束中间特征的一致性，抑制对高频纹理的依赖；2) 监督对比学习：在特征空间中拉近同类、基于形状相关的表示，拉远异类，从而结构化表征并突出形状线索。

Result: 在CIFAR-10-C基准上，两种方法均提升了抗腐蚀鲁棒性，同时保持干净测试集准确率不下降。

Conclusion: 在损失层面进行正则化可以有效引导CNN学习更加形状感知、对扰动更稳健的表示。

Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain
vulnerable to common corruptions that humans handle with ease. A key reason for
this fragility is their reliance on local texture cues rather than global
object shapes -- a stark contrast to human perception. To address this, we
propose two complementary regularization strategies designed to encourage
shape-biased representations and enhance robustness. The first introduces an
auxiliary loss that enforces feature consistency between original and
low-frequency filtered inputs, discouraging dependence on high-frequency
textures. The second incorporates supervised contrastive learning to structure
the feature space around class-consistent, shape-relevant representations.
Evaluated on the CIFAR-10-C benchmark, both methods improve corruption
robustness without degrading clean accuracy. Our results suggest that
loss-level regularization can effectively steer CNNs toward more shape-aware,
resilient representations.

</details>


### [63] [GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration](https://arxiv.org/abs/2509.11360)
*Wan Xu,Feng Zhu,Yihan Zeng,Yuanfan Guo,Ming Liu,Hang Xu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出GLaVE-Cap框架，通过全局-局部对齐与视觉专家融合，生成更细粒度且上下文一致的视频详细描述，并配套构建更全面的评测基准GLaVE-Bench与训练数据集GLaVE-1.2M，取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有“先局部后全局”的视频详细描述范式常产生不够细致、语境不一致的描述，原因在于缺乏细粒度约束机制、以及局部与全局之间交互不足。作者希望通过新的架构与数据资源，提升细粒度性、连贯性与评测可靠性。

Method: 提出GLaVE-Cap框架，含两大模块：1) TrackFusion：引入多种视觉专家（vision experts）跨帧跟踪与融合，形成跨帧视觉提示，并采用双流结构生成更全面的局部字幕；2) CaptionBridge：建立局部-全局交互，用全局语境引导局部生成，同时自适应地将多个局部字幕汇总为连贯的全局描述。并构建GLaVE-Bench基准（每视频查询数为现有的5倍，覆盖更多视觉维度），以及训练集GLaVE-1.2M（含1.6万高质量细粒度视频字幕与120万相关问答对）。

Result: 在四个基准上取得SOTA表现。消融实验与学生模型分析验证了TrackFusion与CaptionBridge模块的有效性，以及GLaVE-1.2M对视频理解的贡献。

Conclusion: GLaVE-Cap通过全局-局部对齐与视觉专家融合，有效提升视频详细描述的细粒度与上下文一致性；配套的基准与数据集提升了评测与训练质量，代码、模型、基准与数据将开源。

Abstract: Video detailed captioning aims to generate comprehensive video descriptions
to facilitate video understanding. Recently, most efforts in the video detailed
captioning community have been made towards a local-to-global paradigm, which
first generates local captions from video clips and then summarizes them into a
global caption. However, we find this paradigm leads to less detailed and
contextual-inconsistent captions, which can be attributed to (1) no mechanism
to ensure fine-grained captions, and (2) weak interaction between local and
global captions. To remedy the above two issues, we propose GLaVE-Cap, a
Global-Local aligned framework with Vision Expert integration for Captioning,
which consists of two core modules: TrackFusion enables comprehensive local
caption generation, by leveraging vision experts to acquire cross-frame visual
prompts, coupled with a dual-stream structure; while CaptionBridge establishes
a local-global interaction, by using global context to guide local captioning,
and adaptively summarizing local captions into a coherent global caption.
Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark
featuring 5X more queries per video than existing benchmarks, covering diverse
visual dimensions to facilitate reliable evaluation. We further provide a
training dataset GLaVE-1.2M containing 16K high-quality fine-grained video
captions and 1.2M related question-answer pairs. Extensive experiments on four
benchmarks show that our GLaVE-Cap achieves state-of-the-art performance.
Besides, the ablation studies and student model analyses further validate the
effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the
video understanding community. The source code, model weights, benchmark, and
dataset will be open-sourced.

</details>


### [64] [In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing](https://arxiv.org/abs/2509.11385)
*Akhil Padmanabha,Arpit Agarwal,Catherine Li,Austin Williams,Dinesh K. Patel,Sankalp Chopkar,Achu Wilson,Ahmet Ozkan,Wenzhen Yuan,Sonal Choudhary,Arash Mostaghimi,Zackory Erickson,Carmel Majidi*

Main category: cs.CV

TL;DR: 提出一种便携手持的GelSight触觉成像3D皮肤重建探头，配合学习式算法，可在多部位实现微米级皱纹深度估计，并在受试者研究与保湿品干预中完成验证。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤三维重建缺乏便携、高分辨率、跨身体部位验证的设备，临床与美容评估需要客观量化皱纹/纹理的深度指标。

Method: 基于GelSight的定制弹性凝胶与手持力控探头获取高分辨率表面形貌，结合学习驱动的3D重建算法进行皱纹高度（深度）估计；在仿真皱纹物体上做标定评测，并在15名健康受试者多身体部位采集数据；对涂抹非处方保湿剂前后进行统计分析。

Result: 在皱纹样测试体上达到12.55微米平均绝对误差；首次在15名无皮肤病受试者的多身体部位给出经验证的皱纹深度指标；涂抹保湿剂后，在三个位置的皱纹高度显著降低（统计学显著）。

Conclusion: 该手持3D皮肤重建探头实现微米级皱纹深度测量并经实验与人群数据验证，可用于临床与日常护肤的诊断、疗效监测与产品评估，具有推广潜力。

Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for
objective and quantitative dermatological assessment, but no portable,
high-resolution device exists that has been validated and used for depth
reconstruction across various body locations. We present a compact 3-D skin
reconstruction probe based on GelSight tactile imaging with a custom elastic
gel and a learning-based reconstruction algorithm for micron-level wrinkle
height estimation. Our probe, integrated into a handheld probe with force
sensing for consistent contact, achieves a mean absolute error of 12.55 micron
on wrinkle-like test objects. In a study with 15 participants without skin
disorders, we provide the first validated wrinkle depth metrics across multiple
body regions. We further demonstrate statistically significant reductions in
wrinkle height at three locations following over-the-counter moisturizer
application. Our work offers a validated tool for clinical and cosmetic skin
analysis, with potential applications in diagnosis, treatment monitoring, and
skincare efficacy evaluation.

</details>


### [65] [MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation](https://arxiv.org/abs/2509.11394)
*Syed Talal Wasim,Hamid Suleman,Olga Zatsarynna,Muzammal Naseer,Juergen Gall*

Main category: cs.CV

TL;DR: 提出MixANT，一种用于人类活动长期密集预测的随机模型，通过对SSM遗忘门A矩阵采用输入依赖的专家混合选择，提升表示力与效率，并在50Salads、Breakfast、Assembly101上全面超SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SSM（如Mamba）虽能对部分参数进行输入依赖选择，但关键的时间记忆控制（遗忘门A矩阵）仍是静态，限制了对复杂、多样人类行为的长时建模与不确定性表征。

Method: 构建混合专家（MoE）框架：为SSM提供多组候选A矩阵，由输入特征驱动的门控网络动态选择/加权合成合适的A，从而实现输入依赖的遗忘机制；保持计算效率同时提升表示能力，用于随机的长期密集活动预判。

Result: 在50Salads、Breakfast、Assembly101三个数据集的各类评估设定中，MixANT稳定超过现有最优方法，验证了动态A矩阵的重要性与有效性。

Conclusion: 输入依赖的遗忘门（A矩阵）是提升人类活动长期密集预测可靠性的关键；MixANT通过MoE动态选择A矩阵，在不显著增加计算成本的情况下实现SOTA表现，适用于多样真实场景的人类行为预测。

Abstract: We present MixANT, a novel architecture for stochastic long-term dense
anticipation of human activities. While recent State Space Models (SSMs) like
Mamba have shown promise through input-dependent selectivity on three key
parameters, the critical forget-gate ($\textbf{A}$ matrix) controlling temporal
memory remains static. We address this limitation by introducing a mixture of
experts approach that dynamically selects contextually relevant $\textbf{A}$
matrices based on input features, enhancing representational capacity without
sacrificing computational efficiency. Extensive experiments on the 50Salads,
Breakfast, and Assembly101 datasets demonstrate that MixANT consistently
outperforms state-of-the-art methods across all evaluation settings. Our
results highlight the importance of input-dependent forget-gate mechanisms for
reliable prediction of human behavior in diverse real-world scenarios.

</details>


### [66] [No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data](https://arxiv.org/abs/2509.11406)
*Christoph Fürböck,Paul Weiser,Branko Mitic,Philipp Seeböck,Thomas Helbich,Georg Langs*

Main category: cs.CV

TL;DR: 提出一种基于超网络的多模态医学图像分类方法，能根据可用模态动态生成任务模型参数，从而在模态缺失时依然训练与推理；在高缺失率数据上较丢弃/填补/通道dropout等方法最高提升约8%准确率。


<details>
  <summary>Details</summary>
Motivation: 临床多模态影像常存在部分缺失，传统策略要么丢弃不完整样本、要么填补缺失、或借助dropout式鲁棒化，均影响鲁棒性与泛化；需要一种能在任意模态可用组合下稳定工作的统一模型。

Method: 训练一个超网络作为条件化参数生成器：输入为“可用模态集合”的指示（以及可能的模态特征摘要），输出为任务分类模型的权重/部分参数，使任务模型针对当前模态配置自适应。训练时对完整与不完整样本均可用，通过人为构造不完整数据系统评估鲁棒性；与仅用完整数据训练、通道dropout、以及基于插补的方案做对比。

Result: 在人工构造的模态缺失数据集上，所提方法对不同模态配置表现出更高适应性；在训练集仅25%样本完整（75%缺失）条件下，相比现有最佳方法，准确率最高提升约8%。

Conclusion: 超网络能根据可用模态动态生成分类模型，实现单一框架覆盖所有模态组合，无需丢弃或插补，显著提升在真实多模态临床环境中的效率、鲁棒性与泛化能力。

Abstract: In real world clinical environments, training and applying deep learning
models on multi-modal medical imaging data often struggles with partially
incomplete data. Standard approaches either discard missing samples, require
imputation or repurpose dropout learning schemes, limiting robustness and
generalizability. To address this, we propose a hypernetwork-based method that
dynamically generates task-specific classification models conditioned on the
set of available modalities. Instead of training a fixed model, a hypernetwork
learns to predict the parameters of a task model adapted to available
modalities, enabling training and inference on all samples, regardless of
completeness. We compare this approach with (1) models trained only on complete
data, (2) state of the art channel dropout methods, and (3) an imputation-based
method, using artificially incomplete datasets to systematically analyze
robustness to missing modalities. Results demonstrate superior adaptability of
our method, outperforming state of the art approaches with an absolute increase
in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of
training data with missing modalities). By enabling a single model to
generalize across all modality configurations, our approach provides an
efficient solution for real-world multi-modal medical data analysis.

</details>


### [67] [On the Skinning of Gaussian Avatars](https://arxiv.org/abs/2509.11411)
*Nikolaos Zioulis,Nikolaos Kotarelas,Georgios Albanis,Spyridon Thermos,Anargyros Chatzitofis*

Main category: cs.CV

TL;DR: 提出一种基于四元数加权旋转融合的高斯人物头像动画方法，替代线性混合蒙皮下对高斯非线性旋转的不当处理，实现更快训练/渲染与易集成的顶点高斯表示。


<details>
  <summary>Details</summary>
Motivation: 现有基于辐射场的人体重建（NeRF 等）渲染慢、从观测到规范空间的反向映射困难；高斯Splatting虽快且可用前向蒙皮，但线性混合蒙皮（LBS）在处理高斯的非线性旋转属性时产生伪影。已有方法依赖网格属性或学习校正偏移，复杂度高或不够稳健，需要更简单、通用、可高效动画的方案。

Method: 引入加权旋转融合：利用四元数平均对关节影响的旋转进行权重融合，替代LBS对旋转的线性插值；将Gaussians设为顶点级表示，通过修改传统LBS步骤（仅改变旋转组合方式），在任意Gaussian光栅器中进行前向蒙皮；无需额外网格导向旋转或学习偏移。

Result: 实现更稳定、物理一致的高斯旋转与变形，减少由LBS线性旋转引起的伪影；训练与渲染保持高斯Splatting的高效率，且动画更平滑；方法简化了实现并提高了兼容性与可移植性。

Conclusion: 基于四元数加权平均的旋转融合能有效替代LBS中的线性旋转处理，生成更可信的人体高斯动画，同时保持高效与易集成的特点，适用于任何Gaussian栅格化引擎，只需最小改动现有蒙皮流程。

Abstract: Radiance field-based methods have recently been used to reconstruct human
avatars, showing that we can significantly downscale the systems needed for
creating animated human avatars. Although this progress has been initiated by
neural radiance fields, their slow rendering and backward mapping from the
observation space to the canonical space have been the main challenges. With
Gaussian splatting overcoming both challenges, a new family of approaches has
emerged that are faster to train and render, while also straightforward to
implement using forward skinning from the canonical to the observation space.
However, the linear blend skinning required for the deformation of the
Gaussians does not provide valid results for their non-linear rotation
properties. To address such artifacts, recent works use mesh properties to
rotate the non-linear Gaussian properties or train models to predict corrective
offsets. Instead, we propose a weighted rotation blending approach that
leverages quaternion averaging. This leads to simpler vertex-based Gaussians
that can be efficiently animated and integrated in any engine by only modifying
the linear blend skinning technique, and using any Gaussian rasterizer.

</details>


### [68] [Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery](https://arxiv.org/abs/2509.11436)
*Jeanny Pan,Philipp Seeböck,Christoph Fürböck,Svitlana Pochepnia,Jennifer Straub,Lucian Beer,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 提出一种通过对潜在空间进行事后旋转来主动学习并消除成像域偏移的无监督解耦方法，使生物学与技术因素分离，提升跨中心医学影像聚类稳定性并改善生存预测。


<details>
  <summary>Details</summary>
Motivation: 多中心医学影像存在设备厂商、扫描/重建参数等带来的技术域偏移，导致表示学习与无监督聚类难以发现真正的生物学模式，影响诊断、预后与生物标志物发现。需要一种能从已学潜在表示中区分生物与技术因素、并在不同成像设置下保持聚类一致性的策略。

Method: 在已训练的潜在表征上进行事后（post-hoc）线性旋转，主动学习域偏移方向，使潜在空间的技术（域）维度与生物（组织）维度正交解耦。通过学习的旋转矩阵实现生物-技术因素的分离，并在异质临床数据上进行无监督聚类与评估。与四种SOTA和谐化方法比较。

Result: 在真实多中心数据上，解耦后得到的聚类可稳定对应不同采集设置下的一致组织类型。相较未解耦表征，聚类一致性提升：ARI +19.01%、NMI +16.85%、Dice +12.39%，并优于四种SOTA harmonization方法。将聚类用于量化特发性肺纤维化（IPF）患者的组织构成后，Cox生存预测性能提升。

Conclusion: 事后潜在空间旋转能有效学习并抵消成像域偏移，实现生物与技术因素解耦，显著提升跨中心聚类稳定性并增强下游生存预测，展示了在多中心常规影像中无标注促进生物标志物发现的潜力；代码已开源。

Abstract: Identifying new disease-related patterns in medical imaging data with the
help of machine learning enlarges the vocabulary of recognizable findings. This
supports diagnostic and prognostic assessment. However, image appearance varies
not only due to biological differences, but also due to imaging technology
linked to vendors, scanning- or re- construction parameters. The resulting
domain shifts impedes data representation learning strategies and the discovery
of biologically meaningful cluster appearances. To address these challenges, we
introduce an approach to actively learn the domain shift via post-hoc rotation
of the data latent space, enabling disentanglement of biological and technical
factors. Results on real-world heterogeneous clinical data showcase that the
learned disentangled representation leads to stable clusters representing
tissue-types across different acquisition settings. Cluster consistency is
improved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the
entangled representation, outperforming four state-of-the-art harmonization
methods. When using the clusters to quantify tissue composition on idiopathic
pulmonary fibrosis patients, the learned profiles enhance Cox survival
prediction. This indicates that the proposed label-free framework facilitates
biomarker discovery in multi-center routine imaging data. Code is available on
GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.

</details>


### [69] [MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder](https://arxiv.org/abs/2509.11442)
*Ayhan Can Erdur,Christian Beischl,Daniel Scholz,Jiazhen Pan,Benedikt Wiestler,Daniel Rueckert,Jan C Peeken*

Main category: cs.CV

TL;DR: 提出一种用于3D脑MRI的多模态多任务MAE预训练，能在缺失序列时通过跨序列推理重建并提升下游分割/分类表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像常有MRI序列缺失，传统深度模型依赖完整输入且泛化差；需要一种既能学习各序列表征又能在缺失输入时稳健推理的预训练框架，以提升下游任务鲁棒性与性能。

Method: 受MultiMAE启发，将每个MRI序列视为独立模态：使用晚期融合的Transformer编码器聚合多序列信息；为每个模态设置独立解码器分支进行重建，实现多任务重建监督。采用掩码自编码预训练，使模型学习每个模态的丰富表征，并通过跨序列信息推断缺失序列。预训练后得到可适配多下游任务的通用编码器。

Result: 在存在缺失输入的场景下，相比MAE-ViT基线，在分割和分类任务上分别取得+10.1绝对Dice与+0.46 MCC的提升，显示出更强的性能与鲁棒性。

Conclusion: 该多模态、多任务MAE预训练有效缓解MRI序列缺失问题，学得的通用编码器能跨序列推理并适配多种下游应用，显著优于基线，实现更稳健的医学影像分析。

Abstract: Missing input sequences are common in medical imaging data, posing a
challenge for deep learning models reliant on complete input data. In this
work, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm
for multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our
method treats each MRI sequence as a separate input modality, leveraging a
late-fusion-style transformer encoder to integrate multi-sequence information
(multi-modal) and individual decoder streams for each modality for multi-task
reconstruction. This pretraining strategy guides the model to learn rich
representations per modality while also equipping it to handle missing inputs
through cross-sequence reasoning. The result is a flexible and generalizable
encoder for brain MRIs that infers missing sequences from available inputs and
can be adapted to various downstream applications. We demonstrate the
performance and robustness of our method against an MAE-ViT baseline in
downstream segmentation and classification tasks, showing absolute improvement
of $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing
input sequences. Our experiments demonstrate the strength of this pretraining
strategy. The implementation is made available.

</details>


### [70] [Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking](https://arxiv.org/abs/2509.11453)
*BaiChen Fan,Sifan Zhou,Jian Li,Shibo Zhao,Muqing Cao,Qin Wang*

Main category: cs.CV

TL;DR: 提出TrajTrack：仅利用历史目标框轨迹进行隐式运动建模，结合快速显式运动提案，在不引入额外点云的前提下显著提升LiDAR 3D单目标跟踪精度与效率（+4.48% 精度，56 FPS）。


<details>
  <summary>Details</summary>
Motivation: 两帧方法高效但易在稀疏/遮挡场景失稳，序列方法鲁棒但计算代价高。需要一种兼顾长时序信息与计算效率的新范式。

Method: 引入“轨迹范式”：只用历史3D包围框轨迹学习连续运动。TrajTrack先基于两帧生成快速显式运动提案，再用隐式运动建模模块预测未来轨迹以校正/细化提案，无需额外点云输入；可作为轻量级插件增强多种基线跟踪器。

Result: 在NuScenes上取得SOTA，相比强基线精度提升4.48%，推理速度56 FPS；展现对不同基线跟踪器的良好可迁移性与泛化能力。

Conclusion: 通过利用历史轨迹进行隐式时序建模，TrajTrack在不增加点云计算成本的前提下兼顾鲁棒性与实时性，为3D SOT提供高效的新范式，并具备良好通用性。

Abstract: LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics
and autonomous systems. Existing methods typically follow frame-wise motion
estimation or a sequence-based paradigm. However, the two-frame methods are
efficient but lack long-term temporal context, making them vulnerable in sparse
or occluded scenes, while sequence-based methods that process multiple point
clouds gain robustness at a significant computational cost. To resolve this
dilemma, we propose a novel trajectory-based paradigm and its instantiation,
TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame
tracker by implicitly learning motion continuity from historical bounding box
trajectories alone-without requiring additional, costly point cloud inputs. It
first generates a fast, explicit motion proposal and then uses an implicit
motion modeling module to predict the future trajectory, which in turn refines
and corrects the initial proposal. Extensive experiments on the large-scale
NuScenes benchmark show that TrajTrack achieves new state-of-the-art
performance, dramatically improving tracking precision by 4.48% over a strong
baseline while running at 56 FPS. Besides, we also demonstrate the strong
generalizability of TrajTrack across different base trackers. Video is
available at https://www.bilibili.com/video/BV1ahYgzmEWP.

</details>


### [71] [Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision](https://arxiv.org/abs/2509.11476)
*Tianyao Sun,Dawei Xiang,Tianqi Ding,Xiang Fang,Yijiashun Qi,Zunduo Zhao*

Main category: cs.CV

TL;DR: 提出FusionNet用于红外与可见光图像融合，采用模态感知注意力、像素级Alpha融合与目标感知损失，在M3FD上实现更好的语义保留、感知质量与可解释性，并利于下游检测与理解。


<details>
  <summary>Details</summary>
Motivation: 现有IVIF方法难以同时建模跨模态互作用、突出任务关键区域且保持可解释性，导致语义目标（如行人、车辆）信息丢失或纹理/结构融合不充分。

Method: 1) 模态感知注意力：根据红外/可见特征的判别能力动态分配权重，建模跨模态交互；2) 像素级Alpha混合：学习空间可变的融合权重，实现细粒度、可解释的内容自适应融合；3) 目标感知损失：利用弱ROI监督，在包含重要物体的区域保持语义一致性；端到端训练。

Result: 在M3FD数据集上，生成的融合图像在语义保留、感知质量与可解释性方面优于对比方法（摘要未给出具体数值）。

Conclusion: FusionNet为语义感知的多模态融合提供了通用、可扩展方案，能提升下游任务如目标检测与场景理解的表现。

Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal
perception that aims to integrate complementary structural and textural cues
from different spectral domains. In this paper, we propose FusionNet, a novel
end-to-end fusion framework that explicitly models inter-modality interaction
and enhances task-critical regions. FusionNet introduces a modality-aware
attention mechanism that dynamically adjusts the contribution of infrared and
visible features based on their discriminative capacity. To achieve
fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha
blending module, which learns spatially-varying fusion weights in an adaptive
and content-aware manner. Moreover, we formulate a target-aware loss that
leverages weak ROI supervision to preserve semantic consistency in regions
containing important objects (e.g., pedestrians, vehicles). Experiments on the
public M3FD dataset demonstrate that FusionNet generates fused images with
enhanced semantic preservation, high perceptual quality, and clear
interpretability. Our framework provides a general and extensible solution for
semantic-aware multi-modal image fusion, with benefits for downstream tasks
such as object detection and scene understanding.

</details>


### [72] [Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis](https://arxiv.org/abs/2509.11526)
*Wenhao Tang,Sheng Huang,Heng Fang,Fengtao Zhou,Bo Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: 提出MHIM-MIL：通过教师-学生一致性与大规模掩蔽挖掘困难实例的多实例学习框架，在多种病理任务和12个基准上实现更优性能与效率。


<details>
  <summary>Details</summary>
Motivation: WSI中阳性组织只占很小比例，现有基于注意力的MIL偏向“容易”实例，忽视难样本，导致判别边界学习不足；难样本挖掘被证明对提升模型泛化关键，需在实例级引入有效机制。

Method: 构建带一致性约束的Siamese教师-学生MIL：以类感知的实例概率为依据，利用动量教师对显著（易）实例进行掩蔽，迫使学生从剩余区域学习难实例；采用大规模随机掩蔽获得多样且非冗余的难实例，并通过全局recycle网络缓解关键特征丢失；学生以EMA更新教师，迭代识别新的难实例并稳定训练。

Result: 在癌症诊断、亚型分型、生存分析等任务与12个公开基准上，MHIM-MIL在性能和效率上均优于最新方法。

Conclusion: 实例级困难样本挖掘结合动量教师-学生与大规模掩蔽，可有效克服注意力MIL的易样本偏置，提升CPath中WSI任务的准确性与训练稳定性与效率。

Abstract: Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has
opened new avenues for Computational Pathology (CPath). As positive tissue
comprises only a small fraction of gigapixel WSIs, existing Multiple Instance
Learning (MIL) methods typically focus on identifying salient instances via
attention mechanisms. However, this leads to a bias towards easy-to-classify
instances while neglecting challenging ones. Recent studies have shown that
hard examples are crucial for accurately modeling discriminative boundaries.
Applying such an idea at the instance level, we elaborate a novel MIL framework
with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure
with a consistency constraint to explore the hard instances. Using a
class-aware instance probability, MHIM-MIL employs a momentum teacher to mask
salient instances and implicitly mine hard instances for training the student
model. To obtain diverse, non-redundant hard instances, we adopt large-scale
random masking while utilizing a global recycle network to mitigate the risk of
losing key features. Furthermore, the student updates the teacher using an
exponential moving average, which identifies new hard instances for subsequent
training iterations and stabilizes optimization. Experimental results on cancer
diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate
that MHIM-MIL outperforms the latest methods in both performance and
efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.

</details>


### [73] [SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2509.11539)
*Dezhen Wang,Haixiang Zhao,Xiang Shen,Sheng Miao*

Main category: cs.CV

TL;DR: 提出SFGNet，用语义提示与频域特征联合引导COD，并通过MBFM与ISEB提升复杂背景与边界处理，三大基准显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测多忽视不同目标文本提示的语义差异与细粒度频率信息，导致在复杂背景、边界模糊场景下检测与分割能力不足。

Method: 构建Semantic and Frequency Guided Network (SFGNet)：(1) 引入语义提示融入视觉特征，利用不同目标的文本语义差异进行引导；(2) 设计Multi-Band Fourier Module (MBFM)，在频域分解多频带信息，增强对复杂背景与模糊边界的鲁棒性；(3) 设计Interactive Structure Enhancement Block (ISEB)，交互式增强结构一致性与边界细节。

Result: 在三个COD基准上进行大量实验，性能显著超过同类最新方法（具体数值未给出）。

Conclusion: 语义提示与频域特征的联合、配合MBFM与ISEB，可有效提升伪装目标检测的整体与边界感知能力；方法在多基准上验证有效并优于SOTA，代码已开源。

Abstract: Camouflaged object detection (COD) aims to segment objects that blend into
their surroundings. However, most existing studies overlook the semantic
differences among textual prompts of different targets as well as fine-grained
frequency features. In this work, we propose a novel Semantic and Frequency
Guided Network (SFGNet), which incorporates semantic prompts and
frequency-domain features to capture camouflaged objects and improve boundary
perception. We further design Multi-Band Fourier Module(MBFM) to enhance the
ability of the network in handling complex backgrounds and blurred boundaries.
In addition, we design an Interactive Structure Enhancement Block (ISEB) to
ensure structural integrity and boundary details in the predictions. Extensive
experiments conducted on three COD benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches. The core code of
the model is available at the following link:
https://github.com/winter794444/SFGNetICASSP2026.

</details>


### [74] [How Auxiliary Reasoning Unleashes GUI Grounding in VLMs](https://arxiv.org/abs/2509.11548)
*Weiming Li,Yan Shao,Jing Yang,Yujing Lu,Ling Zhong,Yuhan Wang,Manni Duan*

Main category: cs.CV

TL;DR: 他们发现通用VLM在GUI定位任务中“会看但不会报坐标”：隐式指向能力强，但显式坐标输出差；提出零样本的辅助空间推理方法（加坐标轴、网格、标注交点）来把隐式能力外化，跨四个基准与七个VLM验证，显著提升GUI定位效果。


<details>
  <summary>Details</summary>
Motivation: GUI代理需要精准地把文本目标映射到屏幕元素位置。现有VLM没有针对GUI定位优化，虽在Pointing Game上显示潜在对齐能力，但要求输出具体(x,y)时表现不佳；同时，微调需要大量标注成本高。

Method: 设计三种零样本辅助推理：在输入图像上叠加显式空间线索——(1) 坐标轴；(2) 规则网格；(3) 带编号/字母的网格交点标签。然后让VLM基于这些线索输出位置描述（如网格编号或交点标签），从而间接得到精确坐标。

Result: 在四个GUI grounding基准上，对七个开源与商用VLM评测，采用这些辅助线索后，显著提升显式坐标预测性能，相比原始零样本设置有大幅增益。

Conclusion: 无需昂贵数据与微调，只用输入层面的空间提示即可释放VLM的潜在定位能力，实现对GUI元素更可靠的零样本坐标预测，具有通用性和低成本优势。

Abstract: Graphical user interface (GUI) grounding is a fundamental task for building
GUI agents. However, general vision-language models (VLMs) struggle with this
task due to a lack of specific optimization. We identify a key gap in this
paper: while VLMs exhibit significant latent grounding potential, as
demonstrated by their performance measured by Pointing Game, they underperform
when tasked with outputting explicit coordinates. To address this discrepancy,
and bypass the high data and annotation costs of current fine-tuning
approaches, we propose three zero-shot auxiliary reasoning methods. By
providing explicit spatial cues such as axes, grids and labeled intersections
as part of the input image, these methods enable VLMs to articulate their
implicit spatial understanding capabilities. We evaluate these methods on four
GUI grounding benchmarks across seven open-source and proprietary VLMs. The
evaluation results demonstrate that the proposed methods substantially improve
the performance of GUI grounding.

</details>


### [75] [Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps](https://arxiv.org/abs/2509.11574)
*Zhexi Peng,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: 提出Gaussian-SDF混合表示（GPS-SLAM），用SDF承载平滑几何与基础外观，少量3D高斯补细节；由此把高斯数量减半、优化迭代减75%，在Azure Kinect真实序列上实现>150 fps，较现有高斯SLAM快一个数量级且质量相当。


<details>
  <summary>Details</summary>
Motivation: 纯高斯SLAM需大量高斯与迭代优化来拟合RGB-D，导致<20 fps，远慢于几何派（如KinectFusion数百fps）。若减少高斯或迭代会画质严重下降，计算成为瓶颈。

Method: 提出Gaussian-SDF混合：用RGB-D融合快速构建着色SDF（几何与基础外观的连续表示），并仅用3D高斯对SDF未能充分表达的细节进行迭代优化。通过“只优化该优化的”和“非全局高斯建模”实现高斯数量与迭代的大幅缩减。基于该表示实现实时系统GPS-SLAM。

Result: 在真实Azure Kinect序列上，系统达到>150 fps；较SOTA高斯SLAM快约一个数量级。同时在重建质量上保持可比。统计上，高斯数减少约50%，迭代次数减少约75%。

Conclusion: Gaussian-SDF混合能把几何高效性与高斯的细节能力结合，显著加速RGB-D重建而不损失质量；GPS-SLAM证明了在真实场景中的实时性与有效性，并承诺开源以促进后续研究。

Abstract: While recent Gaussian-based SLAM methods achieve photorealistic
reconstruction from RGB-D data, their computational performance remains a
critical bottleneck. State-of-the-art techniques operate at less than 20 fps,
significantly lagging behind geometry-centric approaches like KinectFusion
(hundreds of fps). This limitation stems from the heavy computational burden:
modeling scenes requires numerous Gaussians and complex iterative optimization
to fit RGB-D data, where insufficient Gaussian counts or optimization
iterations cause severe quality degradation. To address this, we propose a
Gaussian-SDF hybrid representation, combining a colorized Signed Distance Field
(SDF) for smooth geometry and appearance with 3D Gaussians to capture
underrepresented details. The SDF is efficiently constructed via RGB-D fusion
(as in geometry-centric methods), while Gaussians undergo iterative
optimization. Our representation enables drastic Gaussian reduction (50% fewer)
by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization
(75% fewer iterations) through targeted appearance refinement. Building upon
this representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time
3D reconstruction system achieving over 150 fps on real-world Azure Kinect
sequences -- delivering an order-of-magnitude speedup over state-of-the-art
techniques while maintaining comparable reconstruction quality. We will release
the source code and data to facilitate future research.

</details>


### [76] [Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2509.11587)
*Haonan Shi,Yubin Wang,De Cheng,Lingfeng He,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出用于无监督可见-红外行人重识别的分层身份学习框架，通过细粒度子聚类、多中心对比学习和双向反向选择传递机制提升跨模态匹配，实验在SYSU-MM01与RegDB上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有USVI-ReID多依赖聚类式对比学习，以单一聚类中心表示身份，强调簇内共性，忽视同簇样本之间的细粒度差异，且跨模态（可见-红外）域间差异大、无标注训练难，导致表示不足与跨模态匹配不稳。

Method: 1) 分层身份学习（HIL）：在粗粒度聚类基础上进行二次聚类，挖掘簇内子结构，为每个粗簇生成多个“记忆/中心”。2) 多中心对比学习（MCCL）：利用多个子中心进行对比学习，强化模态内聚类的判别性并缩小跨模态差异。3) 双向反向选择传递（BRST）：通过双向匹配伪标签筛选可靠的跨模态对应关系，从而提高跨模态监督信号质量。

Result: 在SYSU-MM01与RegDB两个标准USVI-ReID数据集上，所提方法在关键评价指标上优于现有方法（摘要未给出具体数值），显示出更强的跨模态检索性能与泛化能力。

Conclusion: 通过引入分层子聚类和多中心对比学习并结合双向伪标签筛选机制，可以更好地建模簇内细粒度差异并构建可靠的跨模态对应，从而显著提升USVI-ReID性能；代码已开源。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.

</details>


### [77] [Optimizing Class Distributions for Bias-Aware Multi-Class Learning](https://arxiv.org/abs/2509.11588)
*Mirco Felske,Stefan Stiene*

Main category: cs.CV

TL;DR: 提出BiCDO：一种针对多类图像分类的数据中心迭代框架，通过优化各类别样本数以实现帕累托最优的类别分布，支持按类优先级提升（如“人”优先于“狗”），在CIFAR-10与iNaturalist21上结合多种骨干模型验证，获得更平衡且更可靠的性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用常需对不同类别赋予不同安全与业务优先级；统一采样会导致偏差与方差不均衡，无法兼顾关键类别的可靠性。需要一种系统方法确定每类应采集/使用多少样本以在整体性能与关键类表现间取得帕累托最优。

Method: 提出Bias-Controlled Class Distribution Optimizer（BiCDO）：数据中心、可迭代搜索/优化流程，输入多类标注数据与类优先级，输出每类的最优样本配额。通过在训练管线中调整每类样本数量（重采样/配额选择），最小化目标函数的偏差与方差，并实现对特定类别的性能加权。可与现有训练（如ResNet、EfficientNet、ConvNeXt）无缝集成，代码改动小。

Result: 在CIFAR-10与iNaturalist21数据集上，结合EfficientNet、ResNet、ConvNeXt实验，优化后的类分布较均匀分布基线带来更平衡的多类性能，并提升关键类别的指标；整体表现更稳定，偏差与方差降低。

Conclusion: 通过对训练数据的类级配额进行帕累托优化，BiCDO在不改变模型架构的情况下实现按类优先级的可靠性能提升与更平衡的结果，适用于任意标注的多类数据集并可轻量集成到现有流水线。

Abstract: We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an
iterative, data-centric framework that identifies Pareto optimized class
distributions for multi-class image classification. BiCDO enables performance
prioritization for specific classes, which is useful in safety-critical
scenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions,
BiCDO determines the optimal number of images per class to enhance reliability
and minimize bias and variance in the objective function. BiCDO can be
incorporated into existing training pipelines with minimal code changes and
supports any labelled multi-class dataset. We have validated BiCDO using
EfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets,
demonstrating improved, balanced model performance through optimized data
distribution.

</details>


### [78] [MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment](https://arxiv.org/abs/2509.11589)
*Yanyun Pu,Kehan Li,Zeyi Huang,Zhijie Zhong,Kaixiang Yang*

Main category: cs.CV

TL;DR: 提出MVQA-68K，一个包含6.8万+视频、覆盖7个质量维度并带链式推理标注的多维视频质量评估数据集，显著提升MLLM在VQA上的表现与零样本泛化，并在多项基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VQA多给单一分数，难以全面刻画视频质量且缺乏可解释性；随着Sora等生成模型推动视频规模增长，需高质量、可解释、可泛化的评估基准与训练数据。

Method: 构建MVQA-68K数据集：为每段视频在7个维度（审美、机位运动、动态程度、纹理细节、构图、视觉质量、事实一致性）给出细粒度评分与链式推理说明；用该数据训练/微调多模态大模型，并在评测中显式引入推理过程。

Result: 在内部测试集与公共基准（LSVQ-test、LSVQ-1080p、LIVE-VQC）上取得SOTA；显式推理训练显著提升零样本泛化。

Conclusion: 多维度、带推理标注的数据能提升VQA的全面性与可解释性，并有效增强MLLM性能与泛化；MVQA-68K为未来生成视频评估提供强基准与训练资源，代码与数据将开源。

Abstract: With the rapid advancement of video generation models such as Sora, video
quality assessment (VQA) is becoming increasingly crucial for selecting
high-quality videos from large-scale datasets used in pre-training. Traditional
VQA methods, typically producing single numerical scores, often lack
comprehensiveness and interpretability. To address these challenges, we
introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over
68,000 carefully annotated videos, covering seven essential quality dimensions:
overall aesthetics, camera movement, dynamic degree, texture detail,
composition, visual quality, and factual consistency. Each annotation includes
detailed chain-of-thought reasoning to facilitate interpretability and
comprehensive understanding. Extensive experiments demonstrate that MVQA-68K
significantly enhances the performance of various multimodal large language
models (MLLMs) on the VQA task, achieving state-of-the-art results not only on
our internal test set (Fig.1) but also on public benchmarks including
LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning
process during VQA training substantially boosts the zero-shot generalization.
Code and dataset will be available at github:
https://github.com/Controller01-ai/MVQA-68K

</details>


### [79] [Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework](https://arxiv.org/abs/2509.11598)
*Siming Fu,Sijun Dong,Xiaoliang Meng*

Main category: cs.CV

TL;DR: 论文提出HyGDL，一个在自监督框架下实现显式内容-风格解耦的混合生成-判别学习方法，通过在输入端系统性改变偏置（风格）而保持监督不变，促使模型学习风格不变的本质表示，从而缓解捷径学习并提升跨域泛化。


<details>
  <summary>Details</summary>
Motivation: SSL在跨域泛化上受“捷径学习”限制，模型倾向依赖纹理等表层特征而非结构本质。现有方法多在特征对齐/分离层面缓解，但未改变导致捷径依赖的学习机制，生成式（如MAE）与判别式方法均受影响，需要从训练原则上根治。

Method: 提出HyGDL混合框架：单编码器设计；依据“不变性预训练原则”，在输入端系统性地变化风格偏置同时保持监督信号恒定；通过向量投影将表征分解为内容（风格不变）与与其正交的风格成分，实现显式内容-风格解耦，并联合生成/判别目标进行训练。

Result: 实证验证生成式SSL（如MAE）也存在捷径学习问题；HyGDL在此背景下实现对内容与风格的有效分离，并在未见域的泛化任务上优于现有方法（摘要未给出具体数值）。

Conclusion: 捷径学习是SSL泛化受限的系统性根因。通过不变性预训练原则与投影式显式解耦，HyGDL从学习机制层面减少对风格偏置的依赖，提升跨域鲁棒性，适用于生成与判别范式。

Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its
generalization is fundamentally hindered by Shortcut Learning, where models
exploit superficial features like texture instead of intrinsic structure. We
experimentally verify this flaw within the generative paradigm (e.g., MAE) and
argue it is a systemic issue also affecting discriminative methods, identifying
it as the root cause of their failure on unseen domains. While existing methods
often tackle this at a surface level by aligning or separating domain-specific
features, they fail to alter the underlying learning mechanism that fosters
shortcut dependency. To address this at its core, we propose HyGDL (Hybrid
Generative-Discriminative Learning Framework), a hybrid framework that achieves
explicit content-style disentanglement. Our approach is guided by the
Invariance Pre-training Principle: forcing a model to learn an invariant
essence by systematically varying a bias (e.g., style) at the input while
keeping the supervision signal constant. HyGDL operates on a single encoder and
analytically defines style as the component of a representation that is
orthogonal to its style-invariant content, derived via vector projection.

</details>


### [80] [DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection](https://arxiv.org/abs/2509.11605)
*Seoik Jung,Taekyung Song,Joshua Jordan Daniel,JinYoung Lee,SungJun Lee*

Main category: cs.CV

TL;DR: 提出软最大分配策略以在视频中优先采样异常密集片段并保持全局覆盖，据此构建帧级与视频级互补基准，并在UCF-Crime上取得改进。


<details>
  <summary>Details</summary>
Motivation: 现有VAD基准多局限于单一粒度（仅帧级或仅视频级），难以全面评估模型在时序尺度上的泛化与推理能力，因此需要一种既能关注异常密集区域又能覆盖全视频的采样与评测方案。

Method: 1) 软最大（softmax）驱动的帧分配策略：根据异常密度对时间段进行加权分配，优先采样高异常密度片段，同时确保全视频覆盖与跨尺度平衡采样。2) 基于该策略构建两套基准：a) 图像基准，用代表性帧评估帧级推理；b) 视频基准，扩展到时间定位片段并加入异常评分任务。3) 在UCF-Crime上进行实验与消融，对比均匀与随机采样。

Result: 在UCF-Crime数据集上，所提方法在帧级与视频级指标上均有提升；消融表明异常导向的采样显著优于均匀与随机基线。

Conclusion: 软最大帧分配与双基准设计可更全面评估与提升VAD性能，兼顾异常聚焦与全局覆盖，促进模型在多时间尺度上的泛化与定位能力。

Abstract: Video Anomaly Detection (VAD) is critical for surveillance and public safety.
However, existing benchmarks are limited to either frame-level or video-level
tasks, restricting a holistic view of model generalization. This work first
introduces a softmax-based frame allocation strategy that prioritizes
anomaly-dense segments while maintaining full-video coverage, enabling balanced
sampling across temporal scales. Building on this process, we construct two
complementary benchmarks. The image-based benchmark evaluates frame-level
reasoning with representative frames, while the video-based benchmark extends
to temporally localized segments and incorporates an abnormality scoring
task.Experiments on UCF-Crime demonstrate improvements at both the frame and
video levels, and ablation studies confirm clear advantages of anomaly-focused
sampling over uniform and random baselines.

</details>


### [81] [A Controllable 3D Deepfake Generation Framework with Gaussian Splatting](https://arxiv.org/abs/2509.11624)
*Wending Liu,Siyun Liang,Huy H. Nguyen,Isao Echizen*

Main category: cs.CV

TL;DR: 提出一种基于3D Gaussian Splatting的3D深度换脸/表情重演方法，实现身份保留、表情可控、视角一致的可控三维伪造，并在多视角与3D一致性上显著优于2D方法。


<details>
  <summary>Details</summary>
Motivation: 现有2D深度伪造在新视角下易出现几何不一致、透视失真与背景融合差，难以进行精准表情与姿态控制；需要能在三维空间中保持身份与表情一致、支持多视角一致渲染的生成框架。

Method: 将参数化头部模型与动态3D高斯表示结合：1) 将头部与背景高斯显式分离，便于编辑与融合；2) 利用预训练的2D指导信号对多视角人脸区域进行优化，提升身份与细节；3) 设计修复模块，在极端姿态/表情下增强视觉一致性；实现多视角一致渲染、精准表情控制及无缝背景集成。

Result: 在NeRSemble及其他视频上，身份保留、姿态与表情一致性与SOTA 2D方法相当，但在多视角渲染质量与3D一致性方面显著优于2D方法。

Conclusion: 该方法将3D建模与深度伪造连接，提供场景感知、可控、沉浸式的视觉伪造能力；同时揭示3D Gaussian Splatting为操纵攻击带来的新威胁。

Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.

</details>


### [82] [IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed](https://arxiv.org/abs/2509.11638)
*Yongzhe Lyu,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 提出IS-Diff：用未遮挡区域分布生成“和谐”初始噪声，并在采样过程中动态选择性地强化/减弱该先验，实现训练-free的高一致性图像修复，在CelebA-HQ、ImageNet、Places2及大掩膜任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统扩散修复通常用随机噪声初始化，被遮挡区域的语义可能与未遮挡区域不匹配，导致修复区域与上下文不一致、连贯性差。需要一种既不重新训练模型、又能提升语义一致性与数据协调性的机制。

Method: 1) 初始种子重构：从未遮挡区域采样（或估计）数据分布，生成分布上“和谐”的初始噪声/潜变量，引导被遮挡区域朝与上下文匹配的方向扩散；2) 动态选择性细化：在中间潜空间检测不和谐（严重失配）位置，动态调整初始化先验的影响强度，对问题区域更强约束，其他区域更弱；整体为训练-free、可插拔到现有扩散修复流程。

Result: 在标准与大掩膜修复上，于CelebA-HQ、ImageNet、Places2多指标全面优于现有SOTA，获得更高的一致性与真实感；对未遮挡区域保持高保真。

Conclusion: 通过以未遮挡区域分布构造初始种子并引入动态选择性细化，IS-Diff在无需再训练的条件下显著提升扩散修复的语义一致性与连贯性，具有通用、易集成的优势。

Abstract: Diffusion models have shown promising results in free-form inpainting. Recent
studies based on refined diffusion samplers or novel architectural designs led
to realistic results and high data consistency. However, random initialization
seed (noise) adopted in vanilla diffusion process may introduce mismatched
semantic information in masked regions, leading to biased inpainting results,
e.g., low consistency and low coherence with the other unmasked area. To
address this issue, we propose the Initial Seed refined Diffusion Model
(IS-Diff), a completely training-free approach incorporating distributional
harmonious seeds to produce harmonious results. Specifically, IS-Diff employs
initial seeds sampled from unmasked areas to imitate the masked data
distribution, thereby setting a promising direction for the diffusion
procedure. Moreover, a dynamic selective refinement mechanism is proposed to
detect severe unharmonious inpaintings in intermediate latent and adjust the
strength of our initialization prior dynamically. We validate our method on
both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,
and Places2 datasets, demonstrating its effectiveness across all metrics
compared to state-of-the-art inpainting methods.

</details>


### [83] [WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration](https://arxiv.org/abs/2509.11642)
*Qiyuan Guan,Qianfeng Yang,Xiang Chen,Tianyu Song,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 提出一个真实场景多天气一体化图像复原基准数据集（WeatherBench），含精确对齐的退化/清洁配对，覆盖雨/雪/雾等多天气与多场景光照，用于公平训练与评测；并在其上系统基准对比任务特定、通用与一体化方法。


<details>
  <summary>Details</summary>
Motivation: 现有一体化复原方法多依赖混合的单天气合成数据，分辨率、风格与域差异大，产生明显域间鸿沟，既限制模型泛化也影响公平评估；此外缺少大规模真实世界多天气一体化配对数据成为发展瓶颈。

Method: 构建并发布一个真实世界多天气图像复原数据集：在多样户外场景与光照下采集不同天气（雨、雪、雾）影像，并提供与清洁图像的精确配对对齐，形成监督学习可用的基准；随后在该数据集上对任务专用、任务通用及一体化复原模型进行全面基准评测。

Result: 得到一个可用于严格监督训练与评价的大规模真实配对数据集；基于该数据集完成了广泛的基准实验，对不同范式方法的性能进行系统比较，揭示其在真实多天气场景下的表现差异。

Conclusion: WeatherBench为鲁棒、实用的一体化图像复原研究提供了标准化、真实世界基础，缓解域偏移问题并促进公平评估；数据集已公开发布，便于社区复现与拓展。

Abstract: Existing all-in-one image restoration approaches, which aim to handle
multiple weather degradations within a single framework, are predominantly
trained and evaluated using mixed single-weather synthetic datasets. However,
these datasets often differ significantly in resolution, style, and domain
characteristics, leading to substantial domain gaps that hinder the development
and fair evaluation of unified models. Furthermore, the lack of a large-scale,
real-world all-in-one weather restoration dataset remains a critical bottleneck
in advancing this field. To address these limitations, we present a real-world
all-in-one adverse weather image restoration benchmark dataset, which contains
image pairs captured under various weather conditions, including rain, snow,
and haze, as well as diverse outdoor scenes and illumination settings. The
resulting dataset provides precisely aligned degraded and clean images,
enabling supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of task-specific, task-general, and
all-in-one restoration methods on our dataset. Our dataset offers a valuable
foundation for advancing robust and practical all-in-one image restoration in
real-world scenarios. The dataset has been publicly released and is available
at https://github.com/guanqiyuan/WeatherBench.

</details>


### [84] [Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba](https://arxiv.org/abs/2509.11649)
*Chuang Liu,Nan Guo*

Main category: cs.CV

TL;DR: 提出RVMamba/FAZMamba与统一的Joint-OCTAMamba，用Mamba状态空间模型增强OCTA视网膜血管与FAZ联合分割，在OCTA-500上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 2D方法对OCTA的视网膜血管分割精度不足；现有联合分割在不同任务（RV与FAZ）间性能失衡，需要一种兼顾两者、整体提升的框架。

Method: 设计RVMamba，将多特征提取模块与Mamba状态空间模型结合以提升RV分割；提出FAZMamba专注FAZ分割；构建统一的Joint-OCTAMamba，将两者整合用于OCTA的联合分割学习，缓解任务不平衡并提升整体性能。

Result: 在OCTA-500数据集上，Joint-OCTAMamba在各项评价指标上均超过当前方法。代码已开源。

Conclusion: Mamba驱动的统一联合分割框架能同时提升RV与FAZ分割并缓解任务不均衡问题，在标准数据集上实现SOTA表现。

Abstract: OCTA is a crucial non-invasive imaging technique for diagnosing and
monitoring retinal diseases like diabetic retinopathy, age-related macular
degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV)
segmentation offer insufficient accuracy. To address this, we propose RVMamba,
a novel architecture integrating multiple feature extraction modules with the
Mamba state-space model. Moreover, existing joint segmentation models for OCTA
data exhibit performance imbalance between different tasks. To simultaneously
improve the segmentation of the foveal avascular zone (FAZ) and mitigate this
imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework.
Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba
outperforms existing models across evaluation metrics.The code is available at
https://github.com/lc-sfis/Joint-OCTAMamba.

</details>


### [85] [DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition](https://arxiv.org/abs/2509.11661)
*Lifei Hao,Yue Cheng,Baoqi Huang,Bing Jia,Xuandong Zhao*

Main category: cs.CV

TL;DR: 提出DTGen：基于扩散模型的少样本数据增强方案，用于精细化污渍餐具识别；通过LoRA专化、结构化提示生成、CLIP筛选，合成高质多样数据，显著提升分类器性能，并可轻量化部署到嵌入式洗碗机以优化能耗与清洁剂用量。


<details>
  <summary>Details</summary>
Motivation: 现有餐具清洗视觉系统多为粗粒度分类，且工业场景少样本、标注稀缺，难以满足精细化污渍识别与实际落地需求。需要一种能在极少真实数据下扩充高质量样本、提升识别精度并具备可部署性的方案。

Method: 提出DTGen：1) 采用LoRA对扩散生成模型进行领域专化；2) 使用结构化提示词生成多样化“脏污餐具”图像；3) 利用CLIP进行跨模态一致性与质量过滤，保障合成数据有效性；4) 在极少真实样本下，通过合成数据增强训练下游细粒度分类器；5) 提供轻量化部署策略，便于嵌入式端集成。

Result: 在极少样本条件下，DTGen可几乎无限合成高质量样本，显著提升细粒度脏污识别分类器性能（定性结论，摘要未给出具体指标），验证生成式AI在工业少样本视觉中的有效性。

Conclusion: DTGen为自动化餐具清洗与食品安全监测提供了可行路径：既提高少样本精细识别效果，又具备嵌入式部署可行性，可与清洗程序联动以智能调节能耗与清洁剂使用。

Abstract: Intelligent tableware cleaning is a critical application in food safety and
smart homes, but existing methods are limited by coarse-grained classification
and scarcity of few-shot data, making it difficult to meet industrialization
requirements. We propose DTGen, a few-shot data augmentation scheme based on
generative diffusion models, specifically designed for fine-grained dirty
tableware recognition. DTGen achieves efficient domain specialization through
LoRA, generates diverse dirty images via structured prompts, and ensures data
quality through CLIP-based cross-modal filtering. Under extremely limited real
few-shot conditions, DTGen can synthesize virtually unlimited high-quality
samples, significantly improving classifier performance and supporting
fine-grained dirty tableware recognition. We further elaborate on lightweight
deployment strategies, promising to transfer DTGen's benefits to embedded
dishwashers and integrate with cleaning programs to intelligently regulate
energy consumption and detergent usage. Research results demonstrate that DTGen
not only validates the value of generative AI in few-shot industrial vision but
also provides a feasible deployment path for automated tableware cleaning and
food safety monitoring.

</details>


### [86] [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
*Feilong Chen,Yijiang Liu,Yi Huang,Hao Wang,Miren Tian,Ya-Qi Yu,Minghui Liao,Jihao Wu*

Main category: cs.CV

TL;DR: MindVL 是一款在华为 Ascend NPU 上训练的原生分辨率多模态大模型，借助定制的分布式训练框架和高效并行/数据打包策略，在仅使用约 1/10 数据量的情况下，达到与 Qwen2.5-VL 相当的通用与文档/表格理解表现，并在 OCR 上领先。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型常将图像重采样或切片至固定分辨率，导致细粒度细节与全局布局受损，尤其不利于复杂图表/文档理解；同时，Ascend NPU 上缺乏针对多模态大模型的高效稳定训练方案。作者希望在 Ascend 上实现高效、稳定训练，同时保留原生分辨率优势，提升在密集视觉场景和 OCR 的表现，并在更少数据下取得一线水平。

Method: 1) 架构：采用原生分辨率 ViT 作为视觉编码器，支持可变分辨率输入，避免固定分辨率切片带来的退化；整体类似 Qwen2.5-VL。2) 训练框架：提出 Mindspeed-MLLM，面向 Ascend NPU 的分布式多模态训练系统，对部分算子做等价替换以保证精度；引入多模态数据打包与混合并行以提升端到端吞吐。3) 训练流程：三阶段——预热（基础视觉与多模态预训练）、多任务大规模训练、监督指令微调。4) 推理增强：测试时分辨率搜索（TTRS）与权重平均（SWA/EMA 类）。

Result: 在通用多模态理解与文档/表格理解评测上与 Qwen2.5-VL 持平，且训练数据量仅约其 1/10；在 OCR 评测上取得领先。训练效率因数据打包与混合并行显著提升，在 Ascend 上训练稳定可靠。

Conclusion: 原生分辨率策略结合 Ascend 定制分布式框架与三阶段训练，使 MindVL 以更少数据达到一线水平，并在密集视觉/OCR 场景中表现突出；工程上证明了在 Ascend NPU 上可高效、等精度地训练大规模多模态模型。

Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

</details>


### [87] [RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps](https://arxiv.org/abs/2509.11674)
*Bjoern Kremser,Yusuke Matsui*

Main category: cs.CV

TL;DR: 提出从扫描纸质地图中自动提取可导航步道的端到端管线（配准+U-Net分割+图构建+借助路由引擎的迭代优化），实验证明能从多种风格地图稳健恢复步道网络并生成可用GPS路线。


<details>
  <summary>Details</summary>
Motivation: 纸质地图在徒步与观光中仍受欢迎，因为其包含策划的步道与本地标注，数字地图（如Google Maps）常缺失这些信息。需要将纸质地图中的步道数字化，使其可用于GPS导航。

Method: 1) 地理配准将扫描地图对齐到地理坐标；2) 采用U-Net进行二值分割检测步道像素；3) 基于分割结果构建图（节点/边）形成步道网络；4) 使用路由引擎进行迭代式细化，校正与完善网络。评估包括端到端与各组件表现。

Result: 方法能从多种地图风格稳健恢复步道网络，并生成可用于实际导航的GPS路线；组件级与整体评估均显示鲁棒性与实用性。

Conclusion: 提出的管线有效将纸质地图步道转换为可导航的GPS路线，具备跨风格鲁棒性与实际应用价值。

Abstract: Paper maps remain widely used for hiking and sightseeing because they contain
curated trails and locally relevant annotations that are often missing from
digital navigation applications such as Google Maps. We propose a pipeline to
extract navigable trails from scanned maps, enabling their use in GPS-based
navigation. Our method combines georeferencing, U-Net-based binary
segmentation, graph construction, and an iterative refinement procedure using a
routing engine. We evaluate the full end-to-end pipeline as well as individual
components, showing that the approach can robustly recover trail networks from
diverse map styles and generate GPS routes suitable for practical use.

</details>


### [88] [IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects](https://arxiv.org/abs/2509.11680)
*Ruimin Ma,Sebastian Zudaire,Zhen Li,Chi Zhang*

Main category: cs.CV

TL;DR: 提出IMD工业金属数据集与基准，包含45件真实比例金属零件，支持分割、6D位姿跟踪与一次性位姿估计任务，并评测现有SOTA，结果显示工业场景更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有6D位姿估计基准多为有纹理、低反射的日常物体，无法覆盖工业中常见的金属、无纹理、高反射目标，导致模型在工业场景泛化差。

Method: 构建Industrial Metallic Dataset（IMD）：用RGB-D在自然室内光照下采集45个工业组件，包含多种物体摆放与真实尺度；定义三项任务（视频目标分割、6D位姿跟踪、一次性6D位姿估计）；选择代表性SOTA模型（分割：XMem、SAM2；位姿：BundleTrack、BundleSDF）进行系统评测，形成基线。

Result: 在IMD上，现有SOTA在分割与位姿估计任务上的表现显著下降，相比家居类数据集更具挑战性，量化结果表明工业金属场景的高反射与缺纹理显著影响算法鲁棒性与精度。

Conclusion: IMD填补工业金属类6D位姿基准空白，提供了更贴近真实工业环境的评测平台与基线，能推动分割与6D位姿算法在工业机器人场景中的泛化与发展。

Abstract: Object 6DoF (6D) pose estimation is essential for robotic perception,
especially in industrial settings. It enables robots to interact with the
environment and manipulate objects. However, existing benchmarks on object 6D
pose estimation primarily use everyday objects with rich textures and
low-reflectivity, limiting model generalization to industrial scenarios where
objects are often metallic, texture-less, and highly reflective. To address
this gap, we propose a novel dataset and benchmark namely \textit{Industrial
Metallic Dataset (IMD)}, tailored for industrial applications. Our dataset
comprises 45 true-to-scale industrial components, captured with an RGB-D camera
under natural indoor lighting and varied object arrangements to replicate
real-world conditions. The benchmark supports three tasks, including video
object segmentation, 6D pose tracking, and one-shot 6D pose estimation. We
evaluate existing state-of-the-art models, including XMem and SAM2 for
segmentation, and BundleTrack and BundleSDF for pose estimation, to assess
model performance in industrial contexts. Evaluation results show that our
industrial dataset is more challenging than existing household object datasets.
This benchmark provides the baseline for developing and comparing segmentation
and pose estimation algorithms that better generalize to industrial robotics
scenarios.

</details>


### [89] [Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation](https://arxiv.org/abs/2509.11689)
*Jeremiah Fadugba,Petru Manescu,Bolanle Oladejo,Delmiro Fernandez-Reyes,Philipp Berens*

Main category: cs.CV

TL;DR: 提出用集成蒸馏把多模型不确定性与分割能力压缩到单模型，在视网膜血管分割上以更低计算成本达到与深度集成相当的性能与校准。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需可靠的不确定性估计；深度集成提升性能与不确定性质量但训练/推理成本随模型数线性增长，临床应用受限。

Method: 训练多个基模型形成深度集成，随后以其软标签/不确定性为教师信号，对单个学生模型进行蒸馏学习；在DRIVE与FIVES数据集上评估，使用分割与校准指标比较，同时统计计算复杂度。

Result: 在DRIVE与FIVES上，学生模型在分割精度与概率校准上与教师集成相当（可比指标），而训练与推理的计算成本显著降低。

Conclusion: 集成蒸馏为视网膜血管分割中的不确定性估计提供了一种高效且可靠的方案，兼顾性能与计算效率，适合在医疗成像应用中部署。

Abstract: Uncertainty estimation is critical for reliable medical image segmentation,
particularly in retinal vessel analysis, where accurate predictions are
essential for diagnostic applications. Deep Ensembles, where multiple networks
are trained individually, are widely used to improve medical image segmentation
performance. However, training and testing costs increase with the number of
ensembles. In this work, we propose Ensemble Distillation as a robust
alternative to commonly used uncertainty estimation techniques by distilling
the knowledge of multiple ensemble models into a single model. Through
extensive experiments on the DRIVE and FIVES datasets, we demonstrate that
Ensemble Distillation achieves comparable performance via calibration and
segmentation metrics, while significantly reducing computational complexity.
These findings suggest that Ensemble distillation provides an efficient and
reliable approach for uncertainty estimation in the segmentation of the retinal
vessels, making it a promising tool for medical imaging applications.

</details>


### [90] [The Quest for Universal Master Key Filters in DS-CNNs](https://arxiv.org/abs/2509.11711)
*Zahra Babaiee,Peyman M. Kiassari,Daniela Rus,Radu Grosu*

Main category: cs.CV

TL;DR: 作者声称在深度可分离卷积网络（DS-CNN）中，绝大多数学习到的深度卷积滤波器都等价于8个“通用滤波器”的线性仿射变换（ax+b），并且用这8个冻结滤波器即可在ImageNet上达80%+准确率，且在小数据集上超过常规模型。


<details>
  <summary>Details</summary>
Motivation: “主钥滤波器假说”认为CNN滤波器可能收敛到少数基础模式。现有DS-CNN使用成千上万滤波器，训练与部署成本大，且可解释性弱。作者希望验证并极限化该假说：是否存在极小且跨任务、架构通用的滤波器基，使得DS卷积天然趋向它，从而解释泛化、迁移并简化模型。

Method: 1) 在多种架构与数据集上，对深度卷积层的滤波器进行无监督系统搜索与聚类，寻找能解释已学滤波器的最小基。2) 观察并拟合已学滤波器为少数母滤波器的线性仿射变换（ax+b）与空间位移。3) 用发现的8个候选“主钥滤波器”进行网络初始化并冻结，只训练其后续层，评估性能。4) 将发现的滤波器与经典图像算子（高斯、差分高斯及其导数）和生物视觉感受野进行比对。

Result: - 大多数训练出的深度卷积滤波器可表示为8个通用滤波器的线性移位与缩放。- 用这8个固定滤波器初始化并冻结，ImageNet上仍能达到80%+准确率；在小数据集上甚至优于拥有大量可训练滤波器的模型。- 这8个滤波器形态与DoG、高斯及其导数高度吻合，并与哺乳动物早期视觉感受野相似。

Conclusion: DS卷积层在不同任务与架构下自然趋向一组基本空间算子（近似高斯/DoG及其导数）。这为理解CNN的泛化与迁移提供了统一视角，并提示可用极小的通用滤波器集构建高效、可解释的模型。

Abstract: A recent study has proposed the "Master Key Filters Hypothesis" for
convolutional neural network filters. This paper extends this hypothesis by
radically constraining its scope to a single set of just 8 universal filters
that depthwise separable convolutional networks inherently converge to. While
conventional DS-CNNs employ thousands of distinct trained filters, our analysis
reveals these filters are predominantly linear shifts (ax+b) of our discovered
universal set. Through systematic unsupervised search, we extracted these
fundamental patterns across different architectures and datasets. Remarkably,
networks initialized with these 8 unique frozen filters achieve over 80%
ImageNet accuracy, and even outperform models with thousands of trainable
parameters when applied to smaller datasets. The identified master key filters
closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,
structures that are not only fundamental to classical image processing but also
strikingly similar to receptive fields in mammalian visual systems. Our
findings provide compelling evidence that depthwise convolutional layers
naturally gravitate toward this fundamental set of spatial operators regardless
of task or architecture. This work offers new insights for understanding
generalization and transfer learning through the universal language of these
master key filters.

</details>


### [91] [Advanced Layout Analysis Models for Docling](https://arxiv.org/abs/2509.11720)
*Nikolaos Livathinos,Christoph Auer,Ahmed Nassar,Rafael Teixeira de Lima,Maksym Lysak,Brown Ebouky,Cesar Berrospi,Michele Dolfi,Panagiotis Vagenas,Matteo Omenetti,Kasper Dinkla,Yusik Kim,Valery Weber,Lucas Morin,Ingmar Meijer,Viktor Kuropiatnyk,Tim Strohmeyer,A. Said Gurbuz,Peter W. J. Staar*

Main category: cs.CV

TL;DR: 报告介绍在Docling文档转换流水线中集成的新版版面分析模型，基于RT-DETR/RT-DETRv2/DFINE，在15万份异构文档上训练。新模型经后处理优化，跨多基准与硬件评估，mAP较旧基线提升20.6%-23.9%，最佳“heron-101”达78% mAP、28ms/图（A100）。发布训练权重、代码与文档，并总结训练/评测/部署最佳实践。


<details>
  <summary>Details</summary>
Motivation: 提升文档转换任务中的版面分析准确性与速度，解决现有模型在异构文档场景下泛化与工程落地（后处理、跨硬件性能）的不足，为社区提供可复现、可部署的方案与经验。

Method: - 数据：15万份异构文档（公开+私有）。
- 模型：训练RT-DETR、RT-DETRv2、DFINE多种SOTA检测器；提出5个新布局模型。
- 工程：对原始检测结果进行面向转换任务的后处理。
- 评测：在多文档基准上用不同方法学评估精度；在CPU、Nvidia、Apple GPU上测运行时性能。

Result: 五个新模型在Docling基线之上带来20.6%-23.9% mAP提升；最佳模型“heron-101”在NVIDIA A100上达到78% mAP与28 ms/图的推理时间；整体运行时与基线相当或更优。

Conclusion: 新布局检测器在准确率与速度上均显著优于旧基线，并通过系统性实验给出训练、评估与部署的最佳实践，为文档转换社区提供可直接使用的开源模型与工具链（权重、代码、文档已在HuggingFace发布）。

Abstract: This technical report documents the development of novel Layout Analysis
models integrated into the Docling document-conversion pipeline. We trained
several state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and
DFINE architectures on a heterogeneous corpus of 150,000 documents (both openly
available and proprietary). Post-processing steps were applied to the raw
detections to make them more applicable to the document conversion task. We
evaluated the effectiveness of the layout analysis on various document
benchmarks using different methodologies while also measuring the runtime
performance across different environments (CPU, Nvidia and Apple GPUs). We
introduce five new document layout models achieving 20.6% - 23.9% mAP
improvement over Docling's previous baseline, with comparable or better
runtime. Our best model, "heron-101", attains 78% mAP with 28 ms/image
inference time on a single NVIDIA A100 GPU. Extensive quantitative and
qualitative experiments establish best practices for training, evaluating, and
deploying document-layout detectors, providing actionable guidance for the
document conversion community. All trained checkpoints, code, and documentation
are released under a permissive license on HuggingFace.

</details>


### [92] [Microsurgical Instrument Segmentation for Robot-Assisted Surgery](https://arxiv.org/abs/2509.11727)
*Tae Kyeong Jeong,Garam Kim,Juyoun Park*

Main category: cs.CV

TL;DR: 提出MISRA：通过RGB+亮度通道、跳跃注意力和迭代反馈模块提升显微外科细长器械分割，IoU提升5.37%，在接触/重叠处更稳定。并发布细粒度标注数据集MISAW-Seg。


<details>
  <summary>Details</summary>
Motivation: 显微外科场景中细长器械（细丝、尖端）分割困难：下采样导致分辨率损失，低对比度、类别不平衡使模型易漏检/断裂，影响机器人与计算机辅助手术的场景解析与安全。

Method: MISRA框架：1) 多模态输入——在RGB基础上加入亮度通道以增强弱对比边缘；2) Skip Attention：在编码器-解码器跳跃连接处引入注意力，强调细长、延展结构并抑制噪声；3) Iterative Feedback Module（IFM）：多次前向迭代，将上一轮预测反馈以修复连通性与细节断裂；4) 数据集——提供含细薄器械精细标注的显微外科分割数据集MISAW-Seg用于基准评测。

Result: 在新数据集与对比实验中，MISRA相较竞品方法平均类别IoU提升5.37%，在器械接触与重叠区域预测更稳定，减少断裂与误分割。

Conclusion: MISRA在显微外科器械分割上表现出色，尤其对细长结构的连续性与鲁棒性有所提升，并配套公开数据集，为机器人与计算机辅助手术的可靠场景解析奠定基础。

Abstract: Accurate segmentation of thin structures is critical for microsurgical scene
understanding but remains challenging due to resolution loss, low contrast, and
class imbalance. We propose Microsurgery Instrument Segmentation for Robotic
Assistance(MISRA), a segmentation framework that augments RGB input with
luminance channels, integrates skip attention to preserve elongated features,
and employs an Iterative Feedback Module(IFM) for continuity restoration across
multiple passes. In addition, we introduce a dedicated microsurgical dataset
with fine-grained annotations of surgical instruments including thin objects,
providing a benchmark for robust evaluation Dataset available at
https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate
that MISRA achieves competitive performance, improving the mean class IoU by
5.37% over competing methods, while delivering more stable predictions at
instrument contacts and overlaps. These results position MISRA as a promising
step toward reliable scene parsing for computer-assisted and robotic
microsurgery.

</details>


### [93] [Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference](https://arxiv.org/abs/2509.11731)
*Yudong Shen,Wenyu Wu,Jiali Mao,Yixiao Tong,Guoping Liu,Chaoya Wang*

Main category: cs.CV

TL;DR: 提出DGMap：具备全局上下文感知的双解码轨迹制图框架，缓解稀疏区断裂与密集区冗余/误连问题，在三数据集上APLS提升约5%。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据成本低、覆盖广且连续，但空间密度不均：稀疏区易道路断裂，密集区易冗余与误连接，现有方法难兼顾。需要结合全局语义与局部几何以稳健制图。

Method: DGMap双解码框架，包含：1) 多尺度网格编码（Multi-scale Grid Encoding）提取多尺度语义与几何特征；2) 掩码增强关键点提取（Mask-enhanced Keypoint Extraction）提升关键点检测鲁棒性，减少稀疏区断裂；3) 全局上下文关系预测（Global Context-aware Relation Prediction）建模长程轨迹模式，抑制密集区误连接与冗余。通过整合全局语义与局部几何进行联合解码。

Result: 在三个真实数据集上优于SOTA，APLS提升约5%；在滴滴出行轨迹数据上提升尤为显著。

Conclusion: 融合全局语义与局部几何的双解码设计有效缓解密度不均带来的碎片化与误连问题，提升路网重建质量；方法对真实世界多源轨迹具有良好泛化与稳健性。

Abstract: Trajectory data has become a key resource for automated map in-ference due to
its low cost, broad coverage, and continuous availability. However, uneven
trajectory density often leads to frag-mented roads in sparse areas and
redundant segments in dense regions, posing significant challenges for existing
methods. To address these issues, we propose DGMap, a dual-decoding framework
with global context awareness, featuring Multi-scale Grid Encoding,
Mask-enhanced Keypoint Extraction, and Global Context-aware Relation
Prediction. By integrating global semantic context with local geometric
features, DGMap improves keypoint detection accuracy to reduce road
fragmentation in sparse-trajectory areas. Additionally, the Global
Context-aware Relation Prediction module suppresses false connections in
dense-trajectory regions by modeling long-range trajectory patterns.
Experimental results on three real-world datasets show that DGMap outperforms
state-of-the-art methods by 5% in APLS, with notable performance gains on
trajectory data from the Didi Chuxing platform

</details>


### [94] [A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications](https://arxiv.org/abs/2509.11752)
*Hongyuan Zhang,Yuheng Wu,Mingyang Zhao,Zhiwei Chen,Rebecca Li,Fei Zhu,Haohan Zhao,Xiaohua Yuan,Meng Yang,Chunli Qiu,Xiang Cong,Haiyan Chen,Lina Luan,Randolph H. L. Wong,Huai Liao,Colin A Graham,Shi Chang,Guowei Tao,Dong Yi,Zhen Lei,Nassir Navab,Sebastien Ourselin,Jiebo Luo,Hongbin Liu,Gaofeng Meng*

Main category: cs.CV

TL;DR: EchoCare提出一个自监督训练的超声通用基础模型与大规模公开数据集EchoCareData（450万图像，跨23国、多设备、多中心、多族裔），通过层级分类器联合学习像素级与表征级特征，少量训练即可在10项超声任务上优于SOTA，并开放代码与权重以便微调与扩展。


<details>
  <summary>Details</summary>
Motivation: 现实临床中大规模标注超声数据匮乏，且任务特定模型泛化性差，限制了临床可用的通用AI模型发展。作者希望通过大规模、多源数据与自监督学习，构建一个能广泛适配多任务和多场景的超声基础模型。

Method: 1) 构建EchoCareData：包含450万张超声图像，覆盖23个国家、5大洲、众多设备与人群；2) 采用自监督学习进行预训练；3) 模型架构引入层级分类器，联合学习像素级（局部超声特征）与表征级（全局解剖语境）特征；4) 以最小额外训练对多个下游任务进行评测与微调。

Result: 在10个代表性超声基准（疾病诊断、病灶分割、器官检测、解剖标志点预测、定量回归、成像增强、报告生成等，涵盖不同诊断难度）上，EchoCare以较少训练量显著优于当前SOTA基线模型。

Conclusion: EchoCare作为完全开放、可微调且具普适性的超声基础模型，显示出良好的跨中心、跨设备与跨人群泛化能力，有望加速多样化临床超声AI应用的研发与落地。

Abstract: Artificial intelligence (AI) that can effectively learn ultrasound
representations by integrating multi-source data holds significant promise for
advancing clinical care. However, the scarcity of large labeled datasets in
real-world clinical environments and the limited generalizability of
task-specific models have hindered the development of generalizable clinical AI
models for ultrasound applications. In this study, we present EchoCare, a novel
ultrasound foundation model for generalist clinical use, developed via
self-supervised learning on our curated, publicly available, large-scale
dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,
sourced from over 23 countries across 5 continents and acquired via a diverse
range of distinct imaging devices, thus encompassing global cohorts that are
multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt
off-the-shelf vision foundation model architectures, we introduce a
hierarchical classifier into EchoCare to enable joint learning of pixel-level
and representation-level features, capturing both global anatomical contexts
and local ultrasound characteristics. With minimal training, EchoCare
outperforms state-of-the-art comparison models across 10 representative
ultrasound benchmarks of varying diagnostic difficulties, spanning disease
diagnosis, lesion segmentation, organ detection, landmark prediction,
quantitative regression, imaging enhancement and report generation. The code
and pretrained model are publicly released, rendering EchoCare accessible for
fine-tuning and local adaptation, supporting extensibility to additional
applications. EchoCare provides a fully open and generalizable foundation model
to boost the development of AI technologies for diverse clinical ultrasound
applications.

</details>


### [95] [MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images](https://arxiv.org/abs/2509.11763)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出MSMA框架，从单张无约束图像重建3D人脸，通过多尺度特征融合与多属性学习，并结合大核注意力，提升细节与鲁棒性，在MICC、FaceWarehouse和自采数据上达SOTA或可比表现。


<details>
  <summary>Details</summary>
Motivation: 单张无约束图像的3D人脸重建在姿态、光照、遮挡等复杂条件下仍难以捕捉细节与多尺度信息；现有学习方法依赖大量难以获取的3D标注数据，虽用投影损失缓解但细节与多属性鲁棒性不足。

Method: 提出MSMA（Multi-Scale Feature Fusion with Multi-Attribute）框架：1）多尺度特征融合，跨尺度聚合提升细节与全局形状一致性；2）多属性学习，显式建模多种面部属性以增强对多样条件的适应；3）大核注意力模块，扩大感受野并精确跨尺度特征选择；结合投影型监督以减轻对3D标注的依赖，从单张2D图像估计3D人脸参数。

Result: 在MICC Florence、FaceWarehouse与自建数据集上，整体表现与现有SOTA相当，部分困难场景超越SOTA；能够更好地恢复细节并在多属性、多条件下保持稳定。

Conclusion: MSMA通过多尺度融合+多属性学习+大核注意力，实现对单张无约束图像的高精度3D人脸重建，减少对3D标注数据依赖，并在多基准上达到或超越SOTA。

Abstract: Reconstructing 3D face from a single unconstrained image remains a
challenging problem due to diverse conditions in unconstrained environments.
Recently, learning-based methods have achieved notable results by effectively
capturing complex facial structures and details across varying conditions.
Consequently, many existing approaches employ projection-based losses between
generated and input images to constrain model training. However, learning-based
methods for 3D face reconstruction typically require substantial amounts of 3D
facial data, which is difficult and costly to obtain. Consequently, to reduce
reliance on labeled 3D face datasets, many existing approaches employ
projection-based losses between generated and input images to constrain model
training. Nonetheless, despite these advancements, existing approaches
frequently struggle to capture detailed and multi-scale features under diverse
facial attributes and conditions, leading to incomplete or less accurate
reconstructions. In this paper, we propose a Multi-Scale Feature Fusion with
Multi-Attribute (MSMA) framework for 3D face reconstruction from unconstrained
images. Our method integrates multi-scale feature fusion with a focus on
multi-attribute learning and leverages a large-kernel attention module to
enhance the precision of feature extraction across scales, enabling accurate 3D
facial parameter estimation from a single 2D image. Comprehensive experiments
on the MICC Florence, Facewarehouse and custom-collect datasets demonstrate
that our approach achieves results on par with current state-of-the-art
methods, and in some instances, surpasses SOTA performance across challenging
conditions.

</details>


### [96] [Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization](https://arxiv.org/abs/2509.11772)
*Diogo Mendonça,Tiago Barros,Cristiano Premebida,Urbano J. Nunes*

Main category: cs.CV

TL;DR: 提出Seg2Track-SAM2：将预训练检测器+SAM2+新Seg2Track模块结合，实现零样本多目标分割跟踪（MOTS），在KITTI MOT/MOTS上达SOTA（含AssA新高），并用滑动窗口内存将显存降至25%~50%（最高节省75%）且性能几乎不降。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如SAM2）在视频分割上零样本泛化强，但直接用于MOTS受限于身份管理不稳与内存开销大；需要一个无需微调、检测器无关且可高效维护ID与内存的方案，提升跟踪初始化、管理与强化。

Method: 整合：预训练目标检测器用于候选与初始化；SAM2负责高质量视频分割；Seg2Track模块处理轨迹初始化、身份维护（关联/重识别/强化）与轨迹管理。采用滑动窗口式记忆策略，限制历史帧记忆长度，降低内存占用；整体无需微调、对检测器保持无关。

Result: 在KITTI MOT与KITTI MOTS上达到SOTA：在MOTS中汽车与行人类别总体排名第4，同时在关联准确度（AssA）上建立新基准。滑动窗口记忆将内存占用最多降低约75%，性能下降可忽略。

Conclusion: Seg2Track-SAM2在无需微调的前提下，将检测器、SAM2与Seg2Track模块有机结合，实现稳健零样本MOTS，显著提升身份保持与关联准确度，并以滑动窗口记忆实现资源友好部署；代码已开源。

Abstract: Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to
operate reliably in dynamic environments. MOT ensures consistent object
identity assignment and precise spatial delineation. Recent advances in
foundation models, such as SAM2, have demonstrated strong zero-shot
generalization for video segmentation, but their direct application to MOTS
(MOT+Segmentation) remains limited by insufficient identity management and
memory efficiency. This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement. The
proposed approach requires no fine-tuning and remains detector-agnostic.
Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA). Furthermore, a sliding-window
memory strategy reduces memory usage by up to 75% with negligible performance
degradation, supporting deployment under resource constraints. These results
confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot
tracking, enhanced identity preservation, and efficient memory utilization. The
code is available at https://github.com/hcmr-lab/Seg2Track-SAM2

</details>


### [97] [SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.11774)
*Changlu Guo,Anders Nymark Christensen,Anders Bjorholm Dahl,Yugen Yi,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: 提出SA-UNetv2：在所有跳跃连接中注入跨尺度空间注意力，并用加权BCE+MCC联合损失应对前景-背景失衡；在DRIVE与STARE上以极小参数量达SOTA，适合CPU端部署。


<details>
  <summary>Details</summary>
Motivation: 现有SA-UNet仅在瓶颈层使用空间注意力，跳跃连接注意力不足；此外视网膜血管分割存在显著类别不平衡（前景血管稀少）问题，影响模型鲁棒性与精度。

Method: (1) 结构：在U-Net的所有skip连接注入跨尺度空间注意力，强化多尺度特征融合；(2) 损失：采用加权BCE与MCC的组合损失，提升对类别不平衡的鲁棒性；(3) 模型轻量化：参数约0.26M、内存1.2MB。

Result: 在DRIVE与STARE数据集取得SOTA；592×592×3输入在CPU上1秒推理；参数量和内存占用显著低于SA-UNet（小于其50%）。

Conclusion: SA-UNetv2在保持或超越精度的同时显著降低计算与存储成本，适合资源受限、仅CPU环境的实际部署，对视网膜血管分割任务具有实用价值。

Abstract: Retinal vessel segmentation is essential for early diagnosis of diseases such
as diabetic retinopathy, hypertension, and neurodegenerative disorders.
Although SA-UNet introduces spatial attention in the bottleneck, it underuses
attention in skip connections and does not address the severe
foreground-background imbalance. We propose SA-UNetv2, a lightweight model that
injects cross-scale spatial attention into all skip connections to strengthen
multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)
plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class
imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves
state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less
than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,
demonstrating strong efficiency and deployability in resource-constrained,
CPU-only settings.

</details>


### [98] [FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning](https://arxiv.org/abs/2509.11796)
*Haodong Chen,Haojian Huang,XinXiang Yin,Dian Shao*

Main category: cs.CV

TL;DR: 提出FineQuest：一个无需训练的体育视频问答框架，采用“反应式+深思式”双模推理，并结合跨九项运动的多模态知识图SSGraph；同时构建Gym-QA与Diving-QA基准，在新老数据集上达成SOTA且保持通用VideoQA能力。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在体育视频这一高复杂度、专业术语密集且细粒度动作繁多的领域表现不佳，现有VideoQA方法难以兼顾效率与复杂推理，且缺乏系统的体育领域知识与权威评测基准。作者希望在无需额外训练的前提下，提升体育VideoQA的知识覆盖与推理能力，并提供标准化评测。

Method: 1) 训练免框架FineQuest：根据问题难度选择两种推理模式——反应式（快速、直接）与深思式（多步、链式规划）。2) 引入SSGraph多模态体育知识场景图：覆盖九类运动，将视觉实例与领域术语、规则、动作层级等知识对齐，作为外部知识与检索增强。3) 构建Gym-QA与Diving-QA基准：源自FineGym与FineDiving，提供多样化细粒度问题以评测不同推理需求。4) 在SPORTU等现有数据集上进行评测与对比。

Result: FineQuest在新提出的Gym-QA与Diving-QA以及现有SPORTU数据集上取得SOTA，同时在通用VideoQA任务上保持强竞争力，显示其双模推理与SSGraph的有效性。

Conclusion: 通过双模推理与多模态体育知识图的结合，FineQuest在无需训练的前提下显著提升体育视频问答性能，并提供新的评测基准，验证了知识增强与推理策略对复杂视频理解的价值。

Abstract: Video Question Answering (VideoQA) based on Large Language Models (LLMs) has
shown potential in general video understanding but faces significant challenges
when applied to the inherently complex domain of sports videos. In this work,
we propose FineQuest, the first training-free framework that leverages
dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for
straightforward sports queries and ii) Deliberative Reasoning for more complex
ones. To bridge the knowledge gap between general-purpose models and
domain-specific sports understanding, FineQuest incorporates SSGraph, a
multimodal sports knowledge scene graph spanning nine sports, which encodes
both visual instances and domain-specific terminology to enhance reasoning
accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA
and Diving-QA, derived from the FineGym and FineDiving datasets, enabling
diverse and comprehensive evaluation. FineQuest achieves state-of-the-art
performance on these benchmarks as well as the existing SPORTU dataset, while
maintains strong general VideoQA capabilities.

</details>


### [99] [Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics](https://arxiv.org/abs/2509.11800)
*Ang Nan Gu,Michael Tsang,Hooman Vaseli,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: 提出一种将不确定性引入标签空间的训练框架：利用网络训练动态生成不确定性感知的伪标签，提升心超分类中的校准、选择性分类和多视图融合表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像存在噪声、遮挡与模糊，标注者间存在分歧；传统独热标签忽视不确定性，诱导模型过度自信，遇到不完整或含伪影输入时性能与可靠性下降。

Method: 在监督训练过程中利用神经网络训练动态（NNTD）：聚合并校准模型在训练期间对样本的逐步预测，衡量样本难度并生成反映歧义的不确定性伪标签（label augmentation）。该流程与模型架构无关，可直接插入任意监督学习管线以改进不确定性估计与鲁棒性。

Result: 在具有挑战性的超声心动图分类基准上，相比专用基线在校准（更低的过度自信/更好置信度对齐）、选择性分类（在同等覆盖率下更高准确或在同等风险下更高覆盖）与多视图融合任务上取得更优表现。

Conclusion: 将不确定性回注到标签空间、以训练动态驱动的伪标签增强，可在不改动模型架构的前提下显著提升医学影像诊断系统的可靠性与鲁棒性。

Abstract: Computer-aided diagnosis systems must make critical decisions from medical
images that are often noisy, ambiguous, or conflicting, yet today's models are
trained on overly simplistic labels that ignore diagnostic uncertainty. One-hot
labels erase inter-rater variability and force models to make overconfident
predictions, especially when faced with incomplete or artifact-laden inputs. We
address this gap by introducing a novel framework that brings uncertainty back
into the label space. Our method leverages neural network training dynamics
(NNTD) to assess the inherent difficulty of each training sample. By
aggregating and calibrating model predictions during training, we generate
uncertainty-aware pseudo-labels that reflect the ambiguity encountered during
learning. This label augmentation approach is architecture-agnostic and can be
applied to any supervised learning pipeline to enhance uncertainty estimation
and robustness. We validate our approach on a challenging echocardiography
classification benchmark, demonstrating superior performance over specialized
baselines in calibration, selective classification, and multi-view fusion.

</details>


### [100] [LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio](https://arxiv.org/abs/2509.11811)
*Mehwish Mehmood,Shahzaib Iqbal,Tariq Mahmood Khan,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: 提出LFRA-Net，一种轻量级视网膜血管分割网络，结合瓶颈处的focal modulation注意力与选择性跳连中的region-aware注意力，在DRIVE、STARE、CHASE_DB上以极少参数和计算量取得SOTA级精度。


<details>
  <summary>Details</summary>
Motivation: 临床资源受限环境中需要低算力且高精度的视网膜血管分割模型。现有深度学习方法对细小血管提取困难、计算成本高，限制了实时与边缘部署。

Method: 设计LFRA-Net：在编码器-解码器瓶颈引入focal modulation attention以高效建模局部与全局依赖；在选择性跳跃连接中引入region-aware attention以突出区域性关键信息与微细血管；整体结构轻量化（约0.17M参数），优化特征表达与区域聚焦。

Result: 在DRIVE、STARE、CHASE_DB三个公开数据集上取得更优指标：Dice分别84.28%、88.44%、85.50；Jaccard分别72.86%、79.31%、74.70；模型仅0.17M参数、0.66MB、约10.50 GFLOPs，较多SOTA方法更优或相当。

Conclusion: LFRA-Net在维持极低模型规模与计算量的同时，显著提升微细血管分割与整体精度，实现精度-开销的理想权衡，适合资源受限场景的实时临床应用；代码已开源。

Abstract: Retinal vessel segmentation is critical for the early diagnosis of
vision-threatening and systemic diseases, especially in real-world clinical
settings with limited computational resources. Although significant
improvements have been made in deep learning-based segmentation methods,
current models still face challenges in extracting tiny vessels and suffer from
high computational costs. In this study, we present LFRA-Net by incorporating
focal modulation attention at the encoder-decoder bottleneck and region-aware
attention in the selective skip connections. LFRA-Net is a lightweight network
optimized for precise and effective retinal vascular segmentation. It enhances
feature representation and regional focus by efficiently capturing local and
global dependencies. LFRA-Net outperformed many state-of-the-art models while
maintaining lightweight characteristics with only 0.17 million parameters, 0.66
MB memory size, and 10.50 GFLOPs. We validated it on three publicly available
datasets: DRIVE, STARE, and CHASE\_DB. It performed better in terms of Dice
score (84.28\%, 88.44\%, and 85.50\%) and Jaccard index (72.86\%, 79.31\%, and
74.70\%) on the DRIVE, STARE, and CHASE\_DB datasets, respectively. LFRA-Net
provides an ideal ratio between segmentation accuracy and computational cost
compared to existing deep learning methods, which makes it suitable for
real-time clinical applications in areas with limited resources. The code can
be found at https://github.com/Mehwish4593/LFRA-Net.

</details>


### [101] [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)
*Haiduo Huang,Fuwei Yang,Zhenhua Liu,Xuanwu Yin,Dong Li,Pengju Ren,Emad Barsoum*

Main category: cs.CV

TL;DR: SpecVLM将推测解码扩展到视觉语言模型，通过强基线EagleVLM与弹性视觉压缩器、在线logit蒸馏，实现在多数据集与分辨率下无损分布保持的2.5–2.9倍端到端加速。


<details>
  <summary>Details</summary>
Motivation: VLM在prefill阶段被大量视觉token主导，导致计算与KV缓存内存开销激增，直接套用LLM的推测解码受限；需要一种兼顾速度、内存和精度的实用系统方案。

Method: 1) 构建EAGLE-2风格的VLM推测解码基线EagleVLM；2) 设计弹性视觉压缩器，按输入自适应选择剪枝、池化、卷积、resampler等原语，权衡FLOPs/参数与精度；3) 提出在线logit蒸馏：在训练时从教师模型即时获取logits与倒数第二层特征，用交叉熵+Smooth L1联合目标训练草稿模型，无需离线语料、存储与预处理；4) 观察并利用训练时长扩展效应提升草稿模型的平均接受长度。

Result: EagleVLM相对全自回归推理带来1.5–2.3倍端到端加速；加入弹性视觉压缩与在线蒸馏后，SpecVLM在LLaVA与MMMU上于5个epoch内实现累计2.5–2.9倍端到端加速，覆盖不同分辨率与任务难度，同时保持目标模型输出分布（无损解码）。

Conclusion: SpecVLM为VLM提供了可落地的推测解码方案：通过自适应视觉压缩与在线蒸馏，在不牺牲输出分布一致性的前提下显著加速推理并降低KV与计算负担；训练更久可进一步提升推测效率。

Abstract: Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.

</details>


### [102] [MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation](https://arxiv.org/abs/2509.11817)
*Liying Wang,Xiaoli Zhang,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: 提出MAFS统一网络，将红外-可见图像融合与语义分割联合建模，通过异构特征融合与级联分割骨干、配合多阶段Transformer解码器及动态任务权重，实现更优视觉质量与下游分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义驱动融合方法只在像素融合中引入语义信息，缺乏从任务层面探索“像素级融合”与“跨模态特征级融合/感知任务（语义分割）”的互促潜力；需要一个统一框架让两任务相互增强并兼顾训练稳定性。

Method: 提出并行结构MAFS：包含融合子网与分割子网。1) 设计异构特征融合策略，提升融合过程中的语义感知能力；2) 通过将融合子网级联至分割骨干，把分割相关知识迁移到融合特征，促进基于融合特征的分割；3) 设计多阶段Transformer解码器，高效聚合细粒度多尺度融合特征；4) 基于max-min公平分配原理的动态因子，自适应分配两任务权重，保障多任务平滑训练。

Result: 在多项实验中，对比最新SOTA方法，MAFS在融合视觉质量和分割性能上取得有竞争力甚至更优的结果。

Conclusion: 统一建模图像融合与语义分割，通过异构融合、知识迁移、Transformer解码与动态任务加权，实现两任务的相互促进与稳定训练，带来领先的综合表现；代码已开源。

Abstract: Infrared-visible image fusion methods aim at generating fused images with
good visual quality and also facilitate the performance of high-level tasks.
Indeed, existing semantic-driven methods have considered semantic information
injection for downstream applications. However, none of them investigates the
potential for reciprocal promotion between pixel-wise image fusion and
cross-modal feature fusion perception tasks from a macroscopic task-level
perspective. To address this limitation, we propose a unified network for image
fusion and semantic segmentation. MAFS is a parallel structure, containing a
fusion sub-network and a segmentation sub-network. On the one hand, We devise a
heterogeneous feature fusion strategy to enhance semantic-aware capabilities
for image fusion. On the other hand, by cascading the fusion sub-network and a
segmentation backbone, segmentation-related knowledge is transferred to promote
feature-level fusion-based segmentation. Within the framework, we design a
novel multi-stage Transformer decoder to aggregate fine-grained multi-scale
fused features efficiently. Additionally, a dynamic factor based on the max-min
fairness allocation principle is introduced to generate adaptive weights of two
tasks and guarantee smooth training in a multi-task manner. Extensive
experiments demonstrate that our approach achieves competitive results compared
with state-of-the-art methods. The code is available at
https://github.com/Abraham-Einstein/MAFS/.

</details>


### [103] [Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network](https://arxiv.org/abs/2509.11838)
*Navid Hashemi,Samuel Sasaki,Diego Manzanas Lopez,Ipek Oguz,Meiyi Ma,Taylor T. Johnson*

Main category: cs.CV

TL;DR: 提出一种面向高维输出的语义分割网络概率验证框架，融合采样可达性分析与保形推断，显著减小保守性并提供可证明安全保证，跨多数据集与大型模型验证有效，并附工具箱。


<details>
  <summary>Details</summary>
Motivation: 现有针对语义分割网络的不确定性与安全性概率验证方法在高维与复杂任务上难以扩展，且给出的保证过于保守，不利于实践部署（如医疗、自动驾驶、环境监测等对安全敏感的场景）。

Method: 提出一个与架构无关、可扩展到高维输出的概率验证框架：将基于采样的可达性分析与保形推断（Conformal Inference, CI）相结合；并针对高维CI保守性强的问题，设计新的策略以降低保守性同时保持严格的统计保证。

Result: 在CamVid、OCTA-500、Lung Segmentation、Cityscapes等大型分割任务与模型上进行实证；相较SOTA，框架在提供可靠安全保证的同时显著收紧（更紧）概率界限。

Conclusion: 该框架实现了对高维语义分割的可扩展、可证明且不保守的概率安全验证，并配套开源工具箱，具有实际落地价值。

Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as
medical imaging, autonomous driving, and environmental monitoring, where safety
hinges on reliable model behavior under uncertainty. Yet, existing
probabilistic verification approaches struggle to scale with the complexity and
dimensionality of modern segmentation tasks, often yielding guarantees that are
too conservative to be practical. We introduce a probabilistic verification
framework that is both architecture-agnostic and scalable to high-dimensional
outputs. Our approach combines sampling-based reachability analysis with
conformal inference (CI) to deliver provable guarantees while avoiding the
excessive conservatism of prior methods. To counteract CI's limitations in
high-dimensional settings, we propose novel strategies that reduce conservatism
without compromising rigor. Empirical evaluation on large-scale segmentation
models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates
that our method provides reliable safety guarantees while substantially
tightening bounds compared to SOTA. We also provide a toolbox implementing this
technique, available on Github.

</details>


### [104] [Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation](https://arxiv.org/abs/2509.11840)
*Tim Lebailly,Vijay Veerabadran,Satwik Kottur,Karl Ridgeway,Michael Louis Iuzzolino*

Main category: cs.CV

TL;DR: 提出用VLM生成的合成文本描述来与图像进行稠密对齐，从而提升零样本开放词汇分割；简单高效且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成式VLM具备强语义理解，但视觉与语言在空间上缺乏稠密对齐；而另一方向的表示学习专注于稠密任务的零样本对齐。需要一种方法把生成式VLM的高层语义优势用于稠密对齐任务。

Method: 利用生成式VLM为图像生成可扩展、低成本的合成描述（caption），将这些高层语义文本信号用于训练/对齐视觉表示，实现图文的稠密对齐，从而支持零样本开放词汇分割。

Result: 在标准的零样本开放词汇分割基准/数据集上，方法优于以往工作，并在数据效率上更优。

Conclusion: 合成caption为稠密图文对齐提供了强大而廉价的高层语义监督，可有效弥合生成式VLM与稠密对齐学习之间的鸿沟，提升零样本分割性能与数据效率。

Abstract: Generative vision-language models (VLMs) exhibit strong high-level image
understanding but lack spatially dense alignment between vision and language
modalities, as our findings indicate. Orthogonal to advancements in generative
VLMs, another line of research has focused on representation learning for
vision-language alignment, targeting zero-shot inference for dense tasks like
segmentation. In this work, we bridge these two directions by densely aligning
images with synthetic descriptions generated by VLMs. Synthetic captions are
inexpensive, scalable, and easy to generate, making them an excellent source of
high-level semantic understanding for dense alignment methods. Empirically, our
approach outperforms prior work on standard zero-shot open-vocabulary
segmentation benchmarks/datasets, while also being more data-efficient.

</details>


### [105] [Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2509.11853)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Måarten Sjöström*

Main category: cs.CV

TL;DR: 提出SDI-GS：用分割引导的选择性初始化与下采样，减少稀疏视角3DGS中的高斯数量约50%，在PSNR/SSIM持平或更优、LPIPS略降的同时加速训练、降内存。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下几何与外观恢复困难；现有3DGS虽高效但依赖SfM求位姿，在极稀疏视角下易失败。替代的SfM-free方法多用MVS并把每个像素反投影为高斯，导致高内存与冗余点。需要一种既不依赖脆弱的SfM/MVS密集重建、又能控制高斯数量并保持质量的初始化策略。

Method: 提出SDI-GS：使用基于区域的图像分割确定结构重要区域，仅对这些区域保留与稀疏采样点，选择性下采样密集点云，从而初始化更紧凑的3D高斯集合；在训练中沿用3DGS渲染与优化流程。

Result: 在多种基准上，高斯数量最多减少约50%；PSNR与SSIM可达到相当或更优，LPIPS仅有轻微变差；训练更快、内存占用更低。

Conclusion: 分割驱动的高斯初始化能在稀疏视角场景中有效压缩高斯数量，同时维持甚至提升画质，提升3DGS在受限视角下的实用性。

Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of
recovering accurate geometry and appearance from limited observations. While
recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time
rendering with competitive quality, existing pipelines often rely on
Structure-from-Motion (SfM) for camera pose estimation, an approach that
struggles in genuinely sparse-view settings. Moreover, several SfM-free methods
replace SfM with multi-view stereo (MVS) models, but generate massive numbers
of 3D Gaussians by back-projecting every pixel into 3D space, leading to high
memory costs. We propose Segmentation-Driven Initialization for Gaussian
Splatting (SDI-GS), a method that mitigates inefficiency by leveraging
region-based segmentation to identify and retain only structurally significant
regions. This enables selective downsampling of the dense point cloud,
preserving scene fidelity while substantially reducing Gaussian count.
Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count
by up to 50% and achieves comparable or superior rendering quality in PSNR and
SSIM, with only marginal degradation in LPIPS. It further enables faster
training and lower memory footprint, advancing the practicality of 3DGS for
constrained-view scenarios.

</details>


### [106] [Bridging Vision Language Models and Symbolic Grounding for Video Question Answering](https://arxiv.org/abs/2509.11862)
*Haodi Ma,Vyom Pathak,Daisy Zhe Wang*

Main category: cs.CV

TL;DR: 以符号场景图作为中介信号，提出SG-VLM将冻结VLM与场景图定位结合，在多数据集上提升视频问答的因果与时序推理，但对最强VLM增益有限。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在视频问答中常依赖浅层相关性，时序定位弱、可解释性不足；需要更结构化的中间表示帮助因果与时序推理。

Method: 构建SG-VLM：以符号场景图(SG)作为中间表示，包含对象与关系的结构化信息；通过提示与视觉定位把SG与冻结的VLM集成，实现模块化的场景图对齐与推理；在多个VLM（如QwenVL、InternVL）与数据集上评估。

Result: 在NExT-QA、iVQA、ActivityNet-QA上，SG-VLM提升了因果与时序问题的表现，整体优于以往基线；但相较于最强VLM，提升幅度有限。

Conclusion: 符号场景图能为视频理解提供有价值的中间落地与可解释性，促进因果/时序推理；但仍受限于与强VLM的融合效果与上限，未来应探索更紧密的混合式VLM-符号方法与更好的场景图质量与对齐。

Abstract: Video Question Answering (VQA) requires models to reason over spatial,
temporal, and causal cues in videos. Recent vision language models (VLMs)
achieve strong results but often rely on shallow correlations, leading to weak
temporal grounding and limited interpretability. We study symbolic scene graphs
(SGs) as intermediate grounding signals for VQA. SGs provide structured
object-relation representations that complement VLMs holistic reasoning. We
introduce SG-VLM, a modular framework that integrates frozen VLMs with scene
graph grounding via prompting and visual localization. Across three benchmarks
(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM
improves causal and temporal reasoning and outperforms prior baselines, though
gains over strong VLMs are limited. These findings highlight both the promise
and current limitations of symbolic grounding, and offer guidance for future
hybrid VLM-symbolic approaches in video understanding.

</details>


### [107] [Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding](https://arxiv.org/abs/2509.11866)
*Meng Luo,Shengqiong Wu,Liqiang Jing,Tianjie Ju,Li Zheng,Jinxiang Lai,Tianlong Wu,Xinya Du,Jian Li,Siyuan Yan,Jiebo Luo,William Yang Wang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出Dr.V层级框架与配套数据集与代理，用细粒度时空定位诊断大视频模型的幻觉并提高可解释性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型视频模型虽在理解上进步显著，但常产生与输入视频不一致的“幻觉”。当前缺乏系统化、可解释、细粒度的手段来检测并分析这些幻觉来源与类型。

Method: 构建层级化诊断框架Dr.V：在感知、时间与认知三个层次，通过细粒度时空定位逐步校验视频内容；包含两部分：1) Dr.V-Bench——来自4,974个视频、约10k实例、多任务、附带精确时空标注的数据集；2) Dr.V-Agent——按层级流程先进行感知与时间层的细粒度时空对齐，再进行认知层推理，从而检测LVM的幻觉。

Result: 实验显示Dr.V-Agent能有效识别并诊断LVM的幻觉，提升解释性与可靠性；在多个任务与真实场景中表现稳健。

Conclusion: Dr.V提供了一条可行的、可解释的诊断管线和基准，促进更稳健的实际视频理解；数据与代码已开源。

Abstract: Recent advancements in large video models (LVMs) have significantly enhance
video understanding. However, these models continue to suffer from
hallucinations, producing content that conflicts with input videos. To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding. Dr.V comprises of two key components: a benchmark
dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes
10k instances drawn from 4,974 videos spanning diverse tasks, each enriched
with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in
LVMs by systematically applying fine-grained spatial-temporal grounding at the
perceptive and temporal levels, followed by cognitive level reasoning. This
step-by-step pipeline mirrors human-like video comprehension and effectively
identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is
effective in diagnosing hallucination while enhancing interpretability and
reliability, offering a practical blueprint for robust video understanding in
real-world scenarios. All our data and code are available at
https://github.com/Eurekaleo/Dr.V.

</details>


### [108] [Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods](https://arxiv.org/abs/2509.11873)
*Anne Marthe Sophie Ngo Bibinbe,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 论文基于10分钟猪群视频，对精密畜牧业中的长期多动物跟踪（MAT）与通用多目标跟踪（MOT）方法进行对比评测，发现近年来MOT算法在长期跟踪上整体优于传统MAT工具。


<details>
  <summary>Details</summary>
Motivation: 精密畜牧业需要长期、稳定、自动化的行为监测；现有易用的MAT工具在遮挡、外观相似、运动不规律等场景下精度不足，影响下游行为分析与健康评估。作者希望系统性比较MAT与SOTA MOT方法在猪只长期跟踪中的表现，以指导实际应用。

Method: 构建/选取10分钟猪只跟踪数据集；评测MAT工具（如DeepLabCut、idTracker）与多种MOT方法（ByteTrack、DeepSORT、Cross-Input Consistency、Track-Anything、PromptTrack），在相同数据与设置下进行长期跟踪性能对比。

Result: 在该数据集上，MOT方法整体性能优于传统MAT工具，能够在长期场景中提供更准确、可靠的跟踪结果。

Conclusion: 最新MOT技术可显著提升畜牧业自动化长期跟踪的准确性与可靠性，优于现有主流MAT工具，建议在实际生产中优先采用或融合MOT方法以改进下游行为与健康分析。

Abstract: Precision livestock farming requires advanced monitoring tools to meet the
increasing management needs of the industry. Computer vision systems capable of
long-term multi-animal tracking (MAT) are essential for continuous behavioral
monitoring in livestock production. MAT, a specialized subset of multi-object
tracking (MOT), shares many challenges with MOT, but also faces domain-specific
issues including frequent animal occlusion, highly similar appearances among
animals, erratic motion patterns, and a wide range of behavior types.
  While some existing MAT tools are user-friendly and widely adopted, they
often underperform compared to state-of-the-art MOT methods, which can result
in inaccurate downstream tasks such as behavior analysis, health state
estimation, and related applications. In this study, we benchmarked both MAT
and MOT approaches for long-term tracking of pigs. We compared tools such as
DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT,
cross-input consistency, and newer approaches like Track-Anything and
PromptTrack.
  All methods were evaluated on a 10-minute pig tracking dataset. Our results
demonstrate that, overall, MOT approaches outperform traditional MAT tools,
even for long-term tracking scenarios. These findings highlight the potential
of recent MOT techniques to enhance the accuracy and reliability of automated
livestock tracking.

</details>


### [109] [Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation](https://arxiv.org/abs/2509.11878)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,K J Joseph*

Main category: cs.CV

TL;DR: 提出加权提示操控（WPM）方法，在零样本场景下从诗歌生成并可交互改进图像，通过调节扩散模型中词级注意力权重与文本嵌入，强化或削弱特定词影响，从而得到更语义丰富、语境更准确的可视化；结合LLM与诗歌数据集，首个在诗歌意象增强中应用加权提示的方法。


<details>
  <summary>Details</summary>
Motivation: 诗歌意象多义且主观，现有文本到图像方法难以对诗句中关键意象与语境进行细粒度控制；用户希望零样本条件下按需修改生成结果，提升与诗歌语义的一致性与表现力。

Method: 提出Weighted Prompt Manipulation（WPM）：在扩散模型的提示处理管线中，动态调节词或短语的注意力权重并修改相应文本嵌入，放大/抑制其对图像生成的影响；结合LLM（如GPT）解析诗歌并提取意象、情绪、修辞等要素，利用现有诗歌数据集构建结构化输入以驱动扩散式生成与迭代改进，支持零样本用户交互调参。

Result: 在诗歌到图像生成任务中，WPM生成的图像更契合诗歌语义与上下文，意象更丰富且可控；能够根据用户需求对特定元素进行强化或弱化，实现更高的语义对齐与视觉准确性（摘要未给出具体数值指标）。

Conclusion: WPM作为一种对提示进行加权与嵌入操控的通用机制，可在零样本场景提升诗歌图像生成的语义控制与质量；这是首次将加权提示操控用于诗歌意象增强，展示了将扩散模型与LLM结合处理文学文本的可行性与潜力。

Abstract: Poetry is an expressive form of art that invites multiple interpretations, as
readers often bring their own emotions, experiences, and cultural backgrounds
into their understanding of a poem. Recognizing this, we aim to generate images
for poems and improve these images in a zero-shot setting, enabling audiences
to modify images as per their requirements. To achieve this, we introduce a
novel Weighted Prompt Manipulation (WPM) technique, which systematically
modifies attention weights and text embeddings within diffusion models. By
dynamically adjusting the importance of specific words, WPM enhances or
suppresses their influence in the final generated image, leading to
semantically richer and more contextually accurate visualizations. Our approach
exploits diffusion models and large language models (LLMs) such as GPT in
conjunction with existing poetry datasets, ensuring a comprehensive and
structured methodology for improved image generation in the literary domain. To
the best of our knowledge, this is the first attempt at integrating weighted
prompt manipulation for enhancing imagery in poetic language.

</details>


### [110] [SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection](https://arxiv.org/abs/2509.11884)
*Zhenni Yu,Li Zhao,Guobao Xiao,Xiaoqin Zhang*

Main category: cs.CV

TL;DR: 提出SAM-TTT：通过“反向参数配置”和测试时训练（TTT）双管齐下，抑制不利参数、强化有利参数，显著提升SAM在隐蔽目标检测（COD）上的语义理解与SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM的COD方法多在“强化有利特征/参数”，忽视了会损害下游语义理解的“不利参数”。这一缺口导致SAM在COD等细粒度、背景伪装严重的场景中表现受限。

Method: 1) 反向SAM参数配置模块：在无需训练的前提下，对SAM内部参数进行配置，显式削弱对下游语义理解有害的参数影响；2) T-Visioner模块：将原面向语言任务的测试时训练（TTT）序列建模层迁移到视觉任务，利用线性复杂度与具表现力的隐藏状态，在推理时强化有利参数；3) 二者结合形成SAM-TTT，同时抑制不利参数并增强有利参数，以提升COD语义理解。

Result: 在多项COD基准上达到SOTA，显著优于现有SAM增强方法；实验显示抑制不利参数+TTT增强的组合带来稳定增益。

Conclusion: 通过无训练的参数配置与测试时训练层的融合，SAM-TTT有效提升SAM在COD上的语义建模能力，树立新的性能基准；代码开源（链接提供）。

Abstract: This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT. While most
existing SAM-based COD models primarily focus on enhancing SAM by extracting
favorable features and amplifying its advantageous parameters, a crucial gap is
identified: insufficient attention to adverse parameters that impair SAM's
semantic understanding in downstream tasks. To tackle this issue, the Reverse
SAM Parameter Configuration Module is proposed to effectively mitigate the
influence of adverse parameters in a train-free manner by configuring SAM's
parameters. Building on this foundation, the T-Visioner Module is unveiled to
strengthen advantageous parameters by integrating Test-Time Training layers,
originally developed for language tasks, into vision tasks. Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state. By integrating two modules,
SAM-TTT simultaneously suppresses adverse parameters while reinforcing
advantageous ones, significantly improving SAM's semantic understanding in COD
task. Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field. The code will be available at
https://github.com/guobaoxiao/SAM-TTT.

</details>


### [111] [BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation](https://arxiv.org/abs/2509.11885)
*Francis Xiatian Zhang,Emile Mackute,Mohammadreza Kasaei,Kevin Dhaliwal,Robert Thomson,Mohsen Khadem*

Main category: cs.CV

TL;DR: 提出Brea-Depth，将支气管解剖几何先验融入单目深度基础模型自适应，通过深度感知的CycleGAN与气道结构感知损失提升在支气管镜中的深度一致性与结构保持，并引入结构评价指标，跨数据集验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度基础模型在支气管镜场景缺乏解剖认知，易受纹理/光照影响，难以在分叉复杂、线索模糊条件下保持全局气道结构与深度一致性，因此需要将解剖几何先验纳入以提升泛化与3D重建可靠性。

Method: 1) 以气道解剖数据为几何先验，设计深度感知CycleGAN，在真实支气管镜影像与合成气道几何渲染之间进行域翻译并保留深度信息；2) 设计气道结构感知损失，约束腔道内深度一致、平滑过渡与结构完整；3) 在基础深度模型微调/适配过程中融合上述先验与损失；4) 提出Airway Depth Structure Evaluation指标评估结构一致性。

Result: 在自建离体人体肺数据集与公开支气管镜数据集上，Brea-Depth在解剖深度保持、结构一致性与3D重建稳健性方面优于现有方法，表现出更强的泛化能力。

Conclusion: 引入气道特异的几何先验与结构感知训练范式可显著提升支气管镜单目深度估计的解剖一致性与鲁棒性，并通过新指标验证其结构真实感，适用于更可靠的实时导航与介入支持。

Abstract: Monocular depth estimation in bronchoscopy can significantly improve
real-time navigation accuracy and enhance the safety of interventions in
complex, branching airways. Recent advances in depth foundation models have
shown promise for endoscopic scenarios, yet these models often lack anatomical
awareness in bronchoscopy, overfitting to local textures rather than capturing
the global airway structure, particularly under ambiguous depth cues and poor
lighting. To address this, we propose Brea-Depth, a novel framework that
integrates airway-specific geometric priors into foundation model adaptation
for bronchoscopic depth estimation. Our method introduces a depth-aware
CycleGAN, refining the translation between real bronchoscopic images and airway
geometries from anatomical data, effectively bridging the domain gap. In
addition, we introduce an airway structure awareness loss to enforce depth
consistency within the airway lumen while preserving smooth transitions and
structural integrity. By incorporating anatomical priors, Brea-Depth enhances
model generalization and yields more robust, accurate 3D airway
reconstructions. To assess anatomical realism, we introduce Airway Depth
Structure Evaluation, a new metric for structural consistency. We validate
BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic
dataset, where it outperforms existing methods in anatomical depth
preservation.

</details>


### [112] [Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection](https://arxiv.org/abs/2509.11892)
*Akito Shinohara,Kohei Fukuda,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: 提出在logit空间进行线性插值的训练策略，通过将ID与OOD样本的logit混合并与输入空间混合保持一致性，从而平滑决边界、提升OOD检测，尤其针对靠近ID分布的细粒度OOD。


<details>
  <summary>Details</summary>
Motivation: 尽管OE/MOE通过引入外部异常样本训练提升了OOD检测，但模型仍难以充分学习类间关系、在接近决边界处稳定地区分ID与OOD。输入或特征空间的混合无法显式控制类间可分性，因此转而关注类间结构更清晰的logit空间。

Method: 1) 在logit空间对ID与OOD样本执行线性插值，得到混合logit，促使不同类别间的输出更平滑；2) 引入一致性约束：logit空间混合的结果需与输入空间混合（如Mixup）经模型得到的logit一致；3) 以该一致性损失联合常规分类与OE/MOE损失训练；4) 在细粒度OOD设置上进行评估。

Result: 实验显示：模型在决策边界附近的输出波动显著降低；ID与OOD的分离更稳定，OOD检测指标（如AUROC/FPR@95TPR等）提升，尤其对贴近ID分布的OOD与细粒度OOD任务更有效。

Conclusion: 在logit空间进行ID-OOD混合并与输入空间混合保持一致，可平滑类间过渡与决策边界，增强模型对近分布OOD的辨识，提升总体OOD检测性能，适用于细粒度场景。

Abstract: The ability to detect out-of-distribution data is essential not only for
ensuring robustness against unknown or unexpected input data but also for
improving the generalization performance of the model. Among various
out-of-distribution detection methods, Outlier Exposure and Mixture Outlier
Exposure are promising approaches that enhance out-of-distribution detection
performance by exposing the outlier data during training. However, even with
these sophisticated techniques, it remains challenging for models to learn the
relationships between classes effectively and to distinguish data sampling from
in-distribution and out-of-distribution clearly. Therefore, we focus on the
logit space, where the properties between class-wise distributions are
distinctly separated from those in the input or feature spaces. Specifically,
we propose a linear interpolation technique in the logit space that mixes
in-distribution and out-of-distribution data to facilitate smoothing logits
between classes and improve the out-of-distribution detection performance,
particularly for out-of-distribution data that lie close to the in-distribution
data. Additionally, we enforce consistency between the logits obtained through
mixing in the logit space and those generated via mixing in the input space.
Our experiments demonstrate that our logit-space mixing technique reduces the
abrupt fluctuations in the model outputs near the decision boundaries,
resulting in smoother and more reliable separation between in-distribution and
out-of-distribution data. Furthermore, we evaluate the effectiveness of the
proposed method on a fine-grained out-of-distribution detection task.

</details>


### [113] [Integrating Prior Observations for Incremental 3D Scene Graph Prediction](https://arxiv.org/abs/2509.11895)
*Marian Renz,Felix Igelbrink,Martin Atzmueller*

Main category: cs.CV

TL;DR: 提出一种异质图的增量式3D语义场景图预测模型，将多模态先验（如CLIP语义嵌入、历史观测）直接融入GNN消息传递，无需完整重建即可在局部与全局层面灵活推理，实验在3DSSG数据集上验证有效与可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有3DSSG方法多依赖传感器/几何数据，缺少对语义丰富环境中的多模态信息整合；同时普遍假设已有完整场景重建，不适用于现实中的增量建图与在线推理。

Method: 提出一种多层异质图神经网络，将对象、属性、关系及其跨模态先验（如CLIP语义嵌入、历史观测/先验）作为节点与边特征，设计在消息传递过程中融合这些信息；模型支持同时处理局部与全局场景表示，无需专用模块或完整重建即可进行增量预测。

Result: 在3DSSG数据集上评估，加入多模态信息（语义嵌入、先验观测）的GNN在可扩展性、泛化与复杂真实环境建模方面表现优于仅用传感器数据的方法。

Conclusion: 将多模态语义先验直接融入GNN消息传递，可在不依赖全场景重建的前提下实现可扩展、通用、适用于增量场景的3DSSG预测；代码将开源以促进复现与扩展。

Abstract: 3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.

</details>


### [114] [NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition](https://arxiv.org/abs/2509.11916)
*Zilin Li,Weiwei Xu,Xuanqi Zhao,Yiran Zhu*

Main category: cs.CV

TL;DR: 提出NeuroGaze-Distill：用EEG教师学习到的情感价度/唤醒(V/A)原型与抑郁启发几何先验，蒸馏到仅图像的FER学生模型，显著提升跨数据集泛化且无需部署时脑信号。


<details>
  <summary>Details</summary>
Motivation: 仅用像素训练的面部情感识别在跨数据集时泛化差，因面部外观是情感的间接且带偏代理。脑信号(如EEG)更直接反映情感维度(V/A)，若能把这种“脑启发先验”转移给视觉模型，或可提高稳健性与可解释性，同时保持部署简洁(无需多模态输入)。

Method: 构建跨模态蒸馏框架NeuroGaze-Distill：1) 教师：在DREAMER EEG地形图上训练模型，并用MAHNOB-HCI作无标注支撑，聚合出固定的5x5 V/A原型网格(静态原型)。2) 学生：在FERPlus上训练ResNet-18/50，仅用图像输入，损失包含常规CE/KD外两项轻量正则：i) Proto-KD(余弦)将学生特征对齐到静态原型；ii) D-Geo几何先验，软性约束嵌入空间呈现与抑郁研究一致的情感几何(如高价度区域的快感缺失式收缩)。3) 评估：域内(FERPlus验证)与跨域(AffectNet-mini，选配CK+)；同时报告8类精度及“仅现标签”的Macro-F1与均衡准确率以应对标签集不一致。并做消融比较不同网格密度。

Result: 原型与D-Geo均带来一致的性能提升；5x5原型网格较更密集网格更稳定；在域内与跨域测试上，相比基线(仅CE/KD)的学生网络获得更高的8类准确率、Macro-F1与均衡准确率。方法无需在部署阶段任何非视觉信号或额外结构复杂度。

Conclusion: 脑启发的V/A静态原型与抑郁相关几何先验可作为跨模态知识源，有效提升仅图像FER的稳健泛化与公平评估表现；方案简单可部署、对架构无侵入，推荐作为提升跨数据集鲁棒性的通用正则化组件。

Abstract: Facial emotion recognition (FER) models trained only on pixels often fail to
generalize across datasets because facial appearance is an indirect and biased
proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal
distillation framework that transfers brain-informed priors into an image-only
FER student via static Valence/Arousal (V/A) prototypes and a
depression-inspired geometric prior (D-Geo). A teacher trained on EEG
topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a
consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face
pairing and no non-visual signals at deployment are required. The student
(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two
lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the
static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with
affective findings often reported in depression research (e.g., anhedonia-like
contraction in high-valence regions). We evaluate both within-domain (FERPlus
validation) and cross-dataset protocols (AffectNet-mini; optional CK+),
reporting standard 8-way scores alongside present-only Macro-F1 and balanced
accuracy to fairly handle label-set mismatch. Ablations attribute consistent
gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.
The method is simple, deployable, and improves robustness without architectural
complexity.

</details>


### [115] [Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI](https://arxiv.org/abs/2509.11924)
*Bo Cao,Fan Yu,Mengmeng Feng,SenHao Zhang,Xin Meng,Yue Zhang,Zhen Qian,Jie Lu*

Main category: cs.CV

TL;DR: 提出VMD方法，将变分推断与多模态知识蒸馏结合，从有限标注和报告中学习跨模态先验，以自动诊断颈动脉斑块易损性，并在自建数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 单靠3D MRI直接判断斑块易损性困难，传统3D网络和放射科医师都面临特征提取与不确定性挑战；临床评估依赖多模态与专家知识，亟需将这种跨模态先验和文本报告知识注入自动化模型，尤其在标注稀缺场景。

Method: 提出Variation inference and Multimodal knowledge Distillation (VMD)：利用变分推断建模隐变量/先验，融合多模态（影像与放射学报告等）知识；通过跨模态知识蒸馏将从有限标注和报告中学到的先验迁移到仅有未标注3D MRI的诊断网络，从而提升对斑块易损性的判别。

Result: 在自建数据集上进行深入实验，VMD相较基线提高了对未标注3D MRI的诊断准确性，证明策略有效。（摘要未给出具体数值。）

Conclusion: VMD能有效利用有限标注与文本报告所含的跨模态先验，提升颈动脉斑块易损性自动诊断性能，显示出在标注受限医疗多模态学习中的应用潜力。

Abstract: Multimodal learning has attracted much attention in recent years due to its
ability to effectively utilize data features from a variety of different
modalities. Diagnosing the vulnerability of atherosclerotic plaques directly
from carotid 3D MRI images is relatively challenging for both radiologists and
conventional 3D vision networks. In clinical practice, radiologists assess
patient conditions using a multimodal approach that incorporates various
imaging modalities and domain-specific expertise, paving the way for the
creation of multimodal diagnostic networks. In this paper, we have developed an
effective strategy to leverage radiologists' domain knowledge to automate the
diagnosis of carotid plaque vulnerability through Variation inference and
Multimodal knowledge Distillation (VMD). This method excels in harnessing
cross-modality prior knowledge from limited image annotations and radiology
reports within training data, thereby enhancing the diagnostic network's
accuracy for unannotated 3D MRI images. We conducted in-depth experiments on
the dataset collected in-house and verified the effectiveness of the VMD
strategy we proposed.

</details>


### [116] [Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization](https://arxiv.org/abs/2509.11926)
*Xue Zhang,Bingshuo Hu,Gene Cheung*

Main category: cs.CV

TL;DR: 提出一种将图滤波与可解释轻量神经网络结合的图先验引导图像插值方法：以已知线性插值器映射为有向图滤波器初始化，再学习小扰动并通过展开的DR迭代实现端到端训练，达到SOTA性能且参数更少。


<details>
  <summary>Details</summary>
Motivation: 传统DNN随机初始化并用SGD优化，易陷入差局部最优，参数量大、可解释性弱。图像插值任务中，若能用有理论依据的结构化初始化与先验，可降低优化难度并提升稳定性与效率。

Method: 1) 利用定理将(伪)线性插值器Θ映射为带GSV正则的MAP问题解对应的有向图滤波器，构造初始有向图邻接矩阵A，获得可靠基线；2) 基于数据学习低秩/小范数的扰动矩阵P与P(2)，对A进行增强；3) 用Douglas-Rachford分裂法求解恢复过程，并将其迭代展开为浅层、可解释的神经网络，实现端到端训练与推理。

Result: 在图像插值实验中取得SOTA或接近SOTA的恢复质量，同时显著减少网络参数量；在同等或更少计算预算下获得更优或更稳健的重建效果。

Conclusion: 将可解释的图滤波与DR迭代展开融合，配合由已知插值器诱导的图结构初始化与小扰动学习，可在降低参数与优化风险的同时提升图像插值性能，提供了一条兼顾性能、效率与可解释性的替代路径。

Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random
and then optimize each one via stochastic gradient descent (SGD), resulting in
substantial risk of poor-performing local minima.Focusing on the image
interpolation problem and leveraging a recent theorem that maps a
(pseudo-)linear interpolator {\Theta} to a directed graph filter that is a
solution to a MAP problem regularized with a graph shift variation (GSV) prior,
we first initialize a directed graph adjacency matrix A based on a known
interpolator {\Theta}, establishing a baseline performance.Then, towards
further gain, we learn perturbation matrices P and P(2) from data to augment A,
whose restoration effects are implemented via Douglas-Rachford (DR) iterations,
which we unroll into a lightweight interpretable neural net.Experimental
results demonstrate state-of-the-art image interpolation results, while
drastically reducing network parameters.

</details>


### [117] [Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos](https://arxiv.org/abs/2509.11948)
*Mahmoud Z. A. Wahba,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 提出Sphere-GAN：面向360°视频的显著性检测模型，采用球面卷积的对抗网络，在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 沉浸式应用（VR/AR）兴起需要对360°图像/视频进行高效处理与传输。显著性估计能定位视觉关注区域，从而驱动编码、传输与渲染的自适应优化。现有显著性研究多针对2D内容，针对360°内容的方法匮乏、性能不足，亟需专门模型。

Method: 引入Sphere-GAN：基于生成对抗网络框架，并采用球面卷积以适配全景（无投影畸变）特性。生成器预测360°视频显著性图，判别器促进生成显著图与真实注视分布的一致性；在公开360°视频显著性数据集上进行训练与评估。

Result: 在公开360°视频显著性数据集上进行大量实验，Sphere-GAN在显著图预测精度上超越现有最先进模型。

Conclusion: 球面卷积+GAN的组合有效提升360°视频显著性估计性能，表明针对球面几何的专门建模是关键，并为后续在沉浸式媒体处理与传输中的自适应优化提供基础。

Abstract: The recent success of immersive applications is pushing the research
community to define new approaches to process 360{\deg} images and videos and
optimize their transmission. Among these, saliency estimation provides a
powerful tool that can be used to identify visually relevant areas and,
consequently, adapt processing algorithms. Although saliency estimation has
been widely investigated for 2D content, very few algorithms have been proposed
for 360{\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,
a saliency detection model for 360{\deg} videos that leverages a Generative
Adversarial Network with spherical convolutions. Extensive experiments were
conducted using a public 360{\deg} video saliency dataset, and the results
demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately
predicting saliency maps.

</details>


### [118] [CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation](https://arxiv.org/abs/2509.11952)
*Debopom Sutradhar,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sheikh Izzal Azid,Sami Azam*

Main category: cs.CV

TL;DR: 提出CLAIRE：一种用于多模态（光学+SAR）土地覆盖分割的双编码器与跨模态注意力融合架构，并结合不平衡感知损失RIFT与小模型生成解释模块，在多数据集上取得领先mIoU/OA与强鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 土地覆盖分类因自然景观复杂、类间视觉相似与类别不平衡而困难；单一模态信息不足且云遮挡等噪声影响大，需要能融合光学与SAR的模型，并在长尾类别上提升性能且增强可解释性。

Method: 1) 双编码器分别提取光学与SAR特征；2) 跨模态注意力融合模块CLAIRE强化互补的空间与纹理信息；3) 混合损失RIFT=加权Focal Loss+Tversky Loss以缓解类别不平衡并提升小样本类分割；4) 在多数据集上训练与评估；5) 引入由小语言模型(Phi-3)驱动的度量-引导型推理模块，生成样本级预测解释。

Result: 在WHU-OPT-SAR上mIoU 56.02%、OA 84.56%；在OpenEarthMap-SAR上mIoU 59.89%、OA 73.92%，表现出较强泛化；在PIE-RGB-SAR（云遮挡场景）上mIoU 86.86%、OA 94.58%，显示出显著鲁棒性。

Conclusion: 跨模态注意力融合与不平衡感知损失共同提升了多模态土地覆盖分割的精度、泛化与鲁棒性；借助小语言模型的度量驱动解释增强了透明性与可解释性。

Abstract: Accurate land cover classification from satellite imagery is crucial in
environmental monitoring and sustainable resource management. However, it
remains challenging due to the complexity of natural landscapes, the visual
similarity between classes, and the significant class imbalance in the
available datasets. To address these issues, we propose a dual encoder
architecture that independently extracts modality-specific features from
optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using
a cross-modality attention-fusion module named Cross-modality Land cover
segmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations
(CLAIRE). This fusion mechanism highlights complementary spatial and textural
features, enabling the network to better capture detailed and diverse land
cover patterns. We incorporate a hybrid loss function that utilizes Weighted
Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address
class imbalance and improve segmentation performance across underrepresented
categories. Our model achieves competitive performance across multiple
benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall
Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with
a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and
remarkable robustness under cloud-obstructed conditions, achieving an mIoU of
86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce
a metric-driven reasoning module generated by a Small Language Model (Phi-3),
which generates expert-level, sample-specific justifications for model
predictions, thereby enhancing transparency and interpretability.

</details>


### [119] [Learning to Generate 4D LiDAR Sequences](https://arxiv.org/abs/2509.11959)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: 提出LiDARCrafter：从自然语言生成并可编辑的4D LiDAR序列，兼顾可控性、时序稳定与评估基准；在nuScenes达SOTA。


<details>
  <summary>Details</summary>
Motivation: 视频与体素生成进展显著，但真实自动驾驶所需的LiDAR生成研究不足；扩展到4D（时序）LiDAR需解决可控生成、时间一致性与客观评估缺失的问题。

Method: 1) 将自由文本解析为自车视角场景图；2) 三分支扩散模型分别生成对象布局、轨迹与形状；3) 以range-image扩散生成首帧点云；4) 自回归模块扩展为时序一致序列；5) 显式布局支持对象级编辑（插入、移动等）；6) 提出EvalSuite覆盖场景/目标/序列级指标。

Result: 在nuScenes上，实现高保真度、强可控性与良好时间一致性，达到SOTA；可进行对象级编辑与可重复评估。

Conclusion: LiDARCrafter提供统一从文本到可编辑4D LiDAR的生成框架与评测基准，为仿真与数据增强奠定基础。

Abstract: While generative world models have advanced video and occupancy-based data
synthesis, LiDAR generation remains underexplored despite its importance for
accurate 3D perception. Extending generation to 4D LiDAR data introduces
challenges in controllability, temporal stability, and evaluation. We present
LiDARCrafter, a unified framework that converts free-form language into
editable LiDAR sequences. Instructions are parsed into ego-centric scene
graphs, which a tri-branch diffusion model transforms into object layouts,
trajectories, and shapes. A range-image diffusion model generates the initial
scan, and an autoregressive module extends it into a temporally coherent
sequence. The explicit layout design further supports object-level editing,
such as insertion or relocation. To enable fair assessment, we provide
EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On
nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and
temporal consistency, offering a foundation for LiDAR-based simulation and data
augmentation.

</details>


### [120] [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)
*Wenyan Li,Raphael Tang,Chengzu Li,Caiqi Zhang,Ivan Vulić,Anders Søgaard*

Main category: cs.CV

TL;DR: 论文研究VLM中视觉到语言嵌入映射（connector）造成的信息损失，并提出两种互补方法定量分析：kNN结构变化与从投影空间重建视觉嵌入，发现投影显著扭曲局部几何并与检索性能下降相关，且补丁级信息损失能预测VQA失败区域。


<details>
  <summary>Details</summary>
Motivation: VLM通常用预训练视觉编码器与语言模型融合，需要通过投影到语言嵌入空间。该步骤可能丢失关键信息，但其规模与对下游能力的直接影响缺乏系统研究。作者希望定量刻画这一信息损失并解释其对任务表现的影响。

Method: 1）语义保持性：比较投影前后图像表示的k近邻关系变化，量化局部几何扭曲；2）信息损失度量：从投影后的表示重建原始视觉嵌入，细化到图像patch级，定位丢失区域，并将其与模型在视觉问答中的行为关联分析。

Result: 发现connector显著改变视觉表示的局部结构，投影后kNN有40–60%偏离，且该偏离与检索性能下降相关；补丁级重建显示高信息损失区域与模型答题困难区域高度一致，提供可解释性证据。

Conclusion: VLM中的投影连接器会引入显著的信息损失与几何扭曲，直接影响检索与VQA等任务表现；补丁级损失度量可用作失败预测与诊断工具，提示需要设计更保真或可逆的连接器以减轻损失。

Abstract: Vision--language models (VLMs) often process visual inputs through a
pretrained vision encoder, followed by a projection into the language model's
embedding space via a connector component. While crucial for modality fusion,
the potential information loss induced by this projection step and its direct
impact on model capabilities remain understudied. We introduce two
complementary approaches to examine and quantify this loss by analyzing the
latent representation space. First, we evaluate semantic information
preservation by analyzing changes in k-nearest neighbor relationships between
image representations, before and after projection. Second, we directly measure
information loss by reconstructing visual embeddings from the projected
representation, localizing loss at an image patch level. Experiments reveal
that connectors substantially distort the local geometry of visual
representations, with k-nearest neighbors diverging by 40--60\%
post-projection, correlating with degradation in retrieval performance. The
patch-level embedding reconstruction provides interpretable insights for model
behavior on visually grounded question-answering tasks, finding that areas of
high information loss reliably predict instances where models struggle.

</details>


### [121] [Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness](https://arxiv.org/abs/2509.12024)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wen,Le Ku,Daheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 提出SCORE框架，将扩散模型中的敏感概念“擦除”表述为对抗式独立问题，通过最小化生成样本与目标概念之间的互信息，实现有理论保证的概念去除，并在多项基准上优于现有方法且保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的强大生成能力带来隐私、偏见与安全风险（如NSFW、名人肖像、私密实体、特定艺术风格的滥用）。现有“擦除”方法多为启发式，缺乏可证明的鲁棒性与泄漏上界，需要一个在保证生成能力的同时具备理论保证的概念移除方法。

Method: 将概念擦除建模为“对抗式独立”：通过一个判别器/概念分类器与生成过程对抗训练，直接最小化生成样本与目标概念之间的互信息（MI），配合三个关键技术——(1) 对抗优化确保难以分辨目标概念；(2) 轨迹一致性在扩散反推采样轨迹中保持分布稳定，避免过度失真；(3) 基于显著性的微调，仅对与目标概念相关的参数/区域进行更新，减少对无关能力的破坏。理论上给出收敛性质证明和残余概念泄漏的上界。

Result: 在Stable Diffusion与FLUX上，覆盖物体擦除、NSFW移除、名人脸抑制与艺术风格“反学习”四类难题。相较EraseAnything、ANT、MACE、ESD、UCE等SOTA方法，SCORE在擦除效果上最高提升约12.5%，同时图像质量相当或更优。

Conclusion: SCORE以互信息最小化与对抗式独立为核心，结合轨迹一致性与显著性微调，提供可证明、鲁棒的概念擦除方案，兼顾安全性与生成质量，为扩散模型的安全部署树立新基线。

Abstract: Diffusion models have achieved unprecedented success in image generation but
pose increasing risks in terms of privacy, fairness, and security. A growing
demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW
content, private individuals, artistic styles) from these models while
preserving their overall generative capabilities. We introduce \textbf{SCORE}
(Secure and Concept-Oriented Robust Erasure), a novel framework for robust
concept removal in diffusion models. SCORE formulates concept erasure as an
\emph{adversarial independence} problem, theoretically guaranteeing that the
model's outputs become statistically independent of the erased concept. Unlike
prior heuristic methods, SCORE minimizes the mutual information between a
target concept and generated outputs, yielding provable erasure guarantees. We
provide formal proofs establishing convergence properties and derive upper
bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable
Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW
removal, celebrity face suppression, and artistic style unlearning. SCORE
consistently outperforms state-of-the-art methods including EraseAnything, ANT,
MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy
while maintaining comparable or superior image quality. By integrating
adversarial optimization, trajectory consistency, and saliency-driven
fine-tuning, SCORE sets a new standard for secure and robust concept erasure in
diffusion models.

</details>


### [122] [RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration](https://arxiv.org/abs/2509.12039)
*Zilong Zhang,Chujie Qin,Chunle Guo,Yong Zhang,Chao Xue,Ming-Ming Cheng,Chongyi Li*

Main category: cs.CV

TL;DR: RAM++提出一种两阶段、内容导向的通用图像恢复框架，通过自适应语义掩码预训练、按贡献选择性微调和利用DINOv2稳健特征正则化，兼顾语义与纹理，针对已知/未知/极端/混合退化均实现稳健、均衡且SOTA的表现。


<details>
  <summary>Details</summary>
Motivation: 现有以退化类型为中心的方法在极端情形（退化与结构强耦合）易失效，且在多任务间表现不均衡、对已见退化过拟合、对未见退化泛化弱。需要一种能同时学习高层语义与低层纹理的内容导向表示学习方案，提升全能图像恢复的鲁棒性与泛化。

Method: 两阶段框架：1) 预训练阶段引入自适应语义感知掩码（AdaSAM），对语义丰富与纹理复杂区域进行像素级掩蔽，促使模型同时学习生成先验与内容先验；2) 微调阶段采用掩码属性导通（MAC），依据层对性能的贡献度进行选择性微调，弥合掩码预训与全图微调间的完整性差距并保留先验；同时提出鲁棒特征正则化（RFR），利用DINOv2的语义一致、退化不变表征并进行高效特征融合，约束重建保持语义一致与真实。

Result: 在已见、未见、极端与混合退化场景均取得稳健、均衡且SOTA水平的恢复性能；缓解了多任务不均衡、对已见退化过拟合与对未见退化泛化弱的问题。

Conclusion: 内容导向的RAM++通过AdaSAM、MAC与RFR三项设计，将高层语义与低层纹理有效融合，显著提升全能图像恢复的鲁棒性与泛化能力，并在多类退化下取得SOTA表现；代码与模型将开源。

Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++),
a two-stage framework for all-in-one image restoration. RAM++ integrates
high-level semantic understanding with low-level texture generation to achieve
content-oriented robust restoration. It addresses the limitations of existing
degradation-oriented methods in extreme scenarios (e.g., degradations strongly
coupled with image structures). RAM++ also mitigates common challenges such as
unbalanced performance across tasks, overfitting to seen degradations, and weak
generalization to unseen ones through three key designs: 1) Adaptive
Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level
masks to semantically rich and textured regions. This design enables the
network to learn both generative priors and image content priors from various
degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy
that leverages DINOv2's semantically consistent and degradation-invariant
representations, together with efficient feature fusion, to achieve faithful
and semantically coherent restoration. With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations. Our code and model will be released at
https://github.com/DragonisCV/RAM

</details>


### [123] [Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing](https://arxiv.org/abs/2509.12040)
*Bingyu Li,Haocheng Dong,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 提出并发布首个统一的开放词汇遥感语义分割评测基准OVRSISBench，并基于此系统评估现有OVS/OVRSIS方法；据此提出RSKT-Seg框架，通过多方向成本图聚合、轻量高效的融合Transformer与遥感知识迁移模块，显著提升跨域分割性能与速度（+3.8 mIoU、+5.9 mACC，推理2倍更快）。


<details>
  <summary>Details</summary>
Motivation: 开放词汇分割在自然图像上发展迅速，但遥感领域缺乏统一评测基准且存在明显域间差异，导致现有方法直接迁移表现欠佳。为解决评测不一致与域偏移问题，需建立标准基准并设计面向遥感特性的开词汇分割方法。

Method: 1) 构建OVRSISBench：基于常用遥感分割数据集，提供统一评测。2) 提出RSKT-Seg，由三部分组成：a) RS-CMA多方向成本图聚合，计算视觉-语言余弦相似度并在多方向上聚合以获得旋转不变特征；b) RS-Fusion高效成本图融合Transformer，同时建模空间与语义依赖，并通过降维实现轻量化；c) RS-Transfer遥感知识迁移模块，引入预训练知识并通过增强上采样促进域自适应。

Result: 在OVRSISBench上的大量实验显示，RSKT-Seg相较强力OVS基线平均提升+3.8 mIoU与+5.9 mACC，并因高效聚合在推理上实现约2倍加速。

Conclusion: 建立了首个统一的OVRSIS评测基准并证明现有OVS/OVRSIS模型在遥感场景存在局限；所提RSKT-Seg通过旋转不变表征、轻量融合与知识迁移有效弥合域差距，在精度与效率上均优于基线，具备实用价值。

Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task
that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)
domain, remains underexplored due to the absence of a unified evaluation
benchmark and the domain gap between natural and RS images. To bridge these
gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench})
based on widely-used RS segmentation datasets, enabling consistent evaluation
across methods. Using this benchmark, we comprehensively evaluate several
representative OVS/OVRSIS models and reveal their limitations when directly
applied to remote sensing scenarios. Building on these insights, we propose
\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for
remote sensing. RSKT-Seg integrates three key components: (1) a
Multi-Directional Cost Map Aggregation (RS-CMA) module that captures
rotation-invariant visual cues by computing vision-language cosine similarities
across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)
transformer, which jointly models spatial and semantic dependencies with a
lightweight dimensionality reduction strategy; and (3) a Remote Sensing
Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and
facilitates domain adaptation via enhanced upsampling. Extensive experiments on
the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines
by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through
efficient aggregation. Our code is
\href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.

</details>


### [124] [Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking](https://arxiv.org/abs/2509.12046)
*Zirui Zheng,Takashi Isobe,Tong Shen,Xu Jia,Jianbin Zhao,Xiaomin Li,Mengmeng Ge,Baolu Li,Qinghe Wang,Dong Li,Dong Zhou,Yunzhi Zhuge,Huchuan Lu,Emad Barsoum*

Main category: cs.CV

TL;DR: 提出SMARLI：通过结构化掩码与GRPO后训练，使自回归图像生成在布局条件下实现更强布局控制与高质量生成。


<details>
  <summary>Details</summary>
Motivation: AR图像生成强，但在布局条件下易因布局稀疏与特征纠缠导致区域-描述误配与控制弱；需要既保持AR效率又能精准注入空间约束的方法。

Method: 1) 结构化掩码：在注意力计算中对全局文本提示、布局token、图像token三者的交互进行分组与定向屏蔽/通道开放，防止跨区域误关联，同时充分注入布局约束；适配next-set式AR解码。2) GRPO后训练：设计面向布局准确度的奖励（如区域对齐、文本-区域一致性等），对AR模型进行群相对策略优化以提升质量与对齐。

Result: 在实验中，SMARLI可无缝融合布局、文本与图像token，不降低生成质量的同时显著提升布局可控性与准确度；相较基线具备更强的布局感知控制，并保持AR结构简单与推理高效。

Conclusion: 结构化掩码有效解决布局稀疏与特征纠缠导致的误配问题，配合GRPO奖励优化可进一步提升布局对齐与质量；SMARLI为AR布局到图像生成提供了高效且精确的控制方案。

Abstract: While autoregressive (AR) models have demonstrated remarkable success in
image generation, extending them to layout-conditioned generation remains
challenging due to the sparse nature of layout conditions and the risk of
feature entanglement. We present Structured Masking for AR-based
Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that
effectively integrates spatial layout constraints into AR-based image
generation. To equip AR model with layout control, a specially designed
structured masking strategy is applied to attention computation to govern the
interaction among the global prompt, layout, and image tokens. This design
prevents mis-association between different regions and their descriptions while
enabling sufficient injection of layout constraints into the generation
process. To further enhance generation quality and layout accuracy, we
incorporate Group Relative Policy Optimization (GRPO) based post-training
scheme with specially designed layout reward functions for next-set-based AR
models. Experimental results demonstrate that SMARLI is able to seamlessly
integrate layout tokens with text and image tokens without compromising
generation quality. It achieves superior layoutaware control while maintaining
the structural simplicity and generation efficiency of AR models.

</details>


### [125] [A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset](https://arxiv.org/abs/2509.12047)
*Haiyu Yang,Enhong Liu,Jennifer Sun,Sumit Sharma,Meike van Leerdam,Sebastien Franceschini,Puchun Niu,Miel Hostens*

Main category: cs.CV

TL;DR: 提出一个模块化开源计算机视觉管线，用于群养环境下的动物（以室内猪为例）行为识别，结合零样本检测、运动感知跟踪与分割、ViT特征提取与时间模型，在公开数据集上显著优于现有方法，并具备良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统人工行为观测耗时、主观且难以扩展；群养环境中遮挡多、目标密集，现有自动方法在识别与跟踪稳定性上不足。需要一个可复用、稳健且开源的自动化方案，提升动物福利监测与精准养殖效率。

Method: 构建模块化管线：1) 零样本目标检测定位个体；2) 运动感知的多目标跟踪与实例分割以缓解遮挡与身份切换；3) 基于视觉Transformer的高级特征提取；4) 时间建模进行行为分类与序列判别。以室内猪群为主要场景，面向多种行为任务进行训练和验证。

Result: 在Edinburgh Pig Behavior Video Dataset上，时间模型总体准确率94.2%，较现有方法提升21.2个百分点；跟踪身份保持分数93.3%；目标检测精度89.3%。在群养、遮挡条件下表现稳健。

Conclusion: 该开源、模块化管线在猪群行为分析中取得显著性能与鲁棒性，适用于精准养殖与福利评估的连续客观监测；具备迁移潜力，但跨物种与多场景仍需进一步验证。

Abstract: Animal behavior analysis plays a crucial role in understanding animal
welfare, health status, and productivity in agricultural settings. However,
traditional manual observation methods are time-consuming, subjective, and
limited in scalability. We present a modular pipeline that leverages
open-sourced state-of-the-art computer vision techniques to automate animal
behavior analysis in a group housing environment. Our approach combines
state-of-the-art models for zero-shot object detection, motion-aware tracking
and segmentation, and advanced feature extraction using vision transformers for
robust behavior recognition. The pipeline addresses challenges including animal
occlusions and group housing scenarios as demonstrated in indoor pig
monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset
for multiple behavioral tasks. Our temporal model achieved 94.2% overall
accuracy, representing a 21.2 percentage point improvement over existing
methods. The pipeline demonstrated robust tracking capabilities with 93.3%
identity preservation score and 89.3% object detection precision. The modular
design suggests potential for adaptation to other contexts, though further
validation across species would be required. The open-source implementation
provides a scalable solution for behavior monitoring, contributing to precision
pig farming and welfare assessment through automated, objective, and continuous
analysis.

</details>


### [126] [AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective](https://arxiv.org/abs/2509.12052)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Suiyang Zhang,Yi He,Yuxing Han*

Main category: cs.CV

TL;DR: AvatarSync是一种基于音素表征的自回归说话人头像动画框架，采用两阶段“分而治之”策略：先以音素驱动生成关键帧，再进行时序插帧，实现高保真、时序稳定且高效的单图驱动唇形与表情动画。


<details>
  <summary>Details</summary>
Motivation: 现有GAN或扩散式视频生成方法常见问题：跨帧闪烁、身份漂移、推理慢，限制实际应用。需要一种在保持视觉逼真与时序一致的同时，具备可控性与低延迟的方案。

Method: 提出AvatarSync：以音素为中间语义，采用自回归框架与两阶段生成。阶段1（FKG）：建立从文本/音频到音素的多对一映射，构造音素到视觉单元的映射，并引入定制的文本-帧因果注意力掩码，生成面部关键帧。阶段2（插帧）：基于带时间戳感知的选择性状态空间模型，进行双向上下文推理以增强时序连贯与平滑。并对推理流程做工程优化以降低时延。

Result: 在大量实验中，AvatarSync在视觉保真度、时间一致性以及计算效率上均优于现有说话人头像方法。

Conclusion: 音素驱动的两阶段自回归架构有效缓解了闪烁与身份漂移问题，在保证可控与可部署性的同时提升了速度与质量，适合作为可扩展的说话人头像生成解决方案。

Abstract: Existing talking-head animation approaches based on Generative Adversarial
Networks (GANs) or diffusion models often suffer from inter-frame flicker,
identity drift, and slow inference. These limitations inherent to their video
generation pipelines restrict their suitability for applications. To address
this, we introduce AvatarSync, an autoregressive framework on phoneme
representations that generates realistic and controllable talking-head
animations from a single reference image, driven directly text or audio input.
In addition, AvatarSync adopts a two-stage generation strategy, decoupling
semantic modeling from visual dynamics, which is a deliberate "Divide and
Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on
phoneme-level semantic representation by leveraging the many-to-one mapping
from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to
anchor abstract phonemes to character-level units. Combined with a customized
Text-Frame Causal Attention Mask, the keyframes are generated. The second
stage, inter-frame interpolation, emphasizes temporal coherence and visual
smoothness. We introduce a timestamp-aware adaptive strategy based on a
selective state space model, enabling efficient bidirectional context
reasoning. To support deployment, we optimize the inference pipeline to reduce
latency without compromising visual fidelity. Extensive experiments show that
AvatarSync outperforms existing talking-head animation methods in visual
fidelity, temporal consistency, and computational efficiency, providing a
scalable and controllable solution.

</details>


### [127] [Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation](https://arxiv.org/abs/2509.12062)
*Sebastian Diaz,Benjamin Billot,Neel Dey,Molin Zhang,Esra Abaci Turk,P. Ellen Grant,Polina Golland,Elfar Adalsteinsson*

Main category: cs.CV

TL;DR: 提出一种跨人群数据增强框架，使仅用晚孕周（GA）标注训练的3D EPI胎儿姿态估计模型能泛化到早孕周，显著降低方差并提升性能，促进早期临床检测与干预。


<details>
  <summary>Details</summary>
Motivation: 胎动反映神经发育与子宫内健康，但早孕周量化困难：现有基于3D EPI时间序列的关键点追踪方法多在三孕期训练，在早孕周因母胎解剖与成像环境差异、且缺少早孕周标注数据而泛化失败。需要在无早孕标注的情况下，提升模型对早孕周的稳健性。

Method: 提出跨人群（不同孕周群体）数据增强：设计胎儿特异的增强策略，模拟早孕周的子宫内环境与胎位特征，将晚孕周标注数据通过这些变换“合成”早孕周样本，用于训练姿态估计模型，从而提升跨孕周泛化。

Result: 跨人群增强在老孕周与具有挑战性的早孕周病例上均带来显著性能提升并降低结果波动（方差），相较无该增强的训练更稳健。

Conclusion: 该增强框架使姿态估计在全孕期更可靠，可望促进在困难的4D胎儿成像场景中开展更早期的临床检测与干预；代码已开源。

Abstract: Fetal motion is a critical indicator of neurological development and
intrauterine health, yet its quantification remains challenging, particularly
at earlier gestational ages (GA). Current methods track fetal motion by
predicting the location of annotated landmarks on 3D echo planar imaging (EPI)
time-series, primarily in third-trimester fetuses. The predicted landmarks
enable simplification of the fetal body for downstream analysis. While these
methods perform well within their training age distribution, they consistently
fail to generalize to early GAs due to significant anatomical changes in both
mother and fetus across gestation, as well as the difficulty of obtaining
annotated early GA EPI data. In this work, we develop a cross-population data
augmentation framework that enables pose estimation models to robustly
generalize to younger GA clinical cohorts using only annotated images from
older GA cohorts. Specifically, we introduce a fetal-specific augmentation
strategy that simulates the distinct intrauterine environment and fetal
positioning of early GAs. Our experiments find that cross-population
augmentation yields reduced variability and significant improvements across
both older GA and challenging early GA cases. By enabling more reliable pose
estimation across gestation, our work potentially facilitates early clinical
detection and intervention in challenging 4D fetal imaging settings. Code is
available at https://github.com/sebodiaz/cross-population-pose.

</details>


### [128] [End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data](https://arxiv.org/abs/2509.12068)
*Farahdiba Zarin,Nicolas Padoy,Jérémy Dana,Vinkle Srivastav*

Main category: cs.CV

TL;DR: 提出ImplMORe：一种基于隐式占据函数的端到端多器官三维表面重建方法，结合3D CNN局部特征与多尺度插值，在TotalSegmentator上实现比显式离散方法更精细、更高分辨率的器官表面。


<details>
  <summary>Details</summary>
Motivation: 医学3D影像器官表面重建受限于体素分辨率与内存/计算开销；通用视觉中的隐式表示具备紧凑可微优势，但架构与数据差异使其难以直接用于医学影像。需要一种能在医学场景下有效利用隐式表示并兼顾多器官与细节的方案。

Method: 提出ImplMORe：以3D CNN编码器提取局部体素特征；采用多尺度插值将离散特征映射到连续域；通过隐式占据函数学习器官表面；支持单器官与多器官重建，端到端训练；在推理时可在高于输入影像分辨率的采样上评估占据函数以生成细致表面。

Result: 在TotalSegmentator数据集上用于单/多器官重建，优于基于显式离散表示的表面重建方法，能重建出更细致的器官表面，并在高于输入图像分辨率的网格上表现更好。

Conclusion: 隐式占据函数结合3D CNN与多尺度插值可有效实现多器官高精度表面重建，突破输入分辨率限制，优于传统显式重建方法；代码将开源以便复现与拓展。

Abstract: The fine-grained surface reconstruction of different organs from 3D medical
imaging can provide advanced diagnostic support and improved surgical planning.
However, the representation of the organs is often limited by the resolution,
with a detailed higher resolution requiring more memory and computing
footprint. Implicit representations of objects have been proposed to alleviate
this problem in general computer vision by providing compact and differentiable
functions to represent the 3D object shapes. However, architectural and
data-related differences prevent the direct application of these methods to
medical images. This work introduces ImplMORe, an end-to-end deep learning
method using implicit surface representations for multi-organ reconstruction
from 3D medical images. ImplMORe incorporates local features using a 3D CNN
encoder and performs multi-scale interpolation to learn the features in the
continuous domain using occupancy functions. We apply our method for single and
multiple organ reconstructions using the totalsegmentator dataset. By
leveraging the continuous nature of occupancy functions, our approach
outperforms the discrete explicit representation based surface reconstruction
approaches, providing fine-grained surface details of the organ at a resolution
higher than the given input image. The source code will be made publicly
available at: https://github.com/CAMMA-public/ImplMORe

</details>


### [129] [U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT](https://arxiv.org/abs/2509.12069)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出U‑Mamba2，将Mamba2状态空间模型融入U‑Net，并结合交互点击提示、跨注意力、自监督预训练与牙科先验，实现CBCT多解剖结构高效分割，在ToothFairy3两项任务中名列前茅。


<details>
  <summary>Details</summary>
Motivation: CBCT在牙科广泛应用，但多解剖结构分割仍费时、困难，影响诊断与术前规划的效率与可靠性，需要更高效且准确的自动分割方法。

Method: 在U‑Net骨干中嵌入Mamba2状态空间模块以引入更强结构约束与高效长程建模；设计交互式点击提示并通过跨注意力与图像特征融合；进行自监督预训练以提升表征；融入牙科领域知识（面向牙颌多器官的结构化设计）；在ToothFairy3任务上训练与评测。

Result: 独立测试显示模型有效高效：任务1在保留测试集上Dice 0.792、HD95 93.19、推理时间待补；任务2Dice 0.852、HD95 7.39；整体取得比赛两项目前三名。代码公开。

Conclusion: U‑Mamba2在保持或提升性能的同时提高效率，能更好适配CBCT牙科多解剖分割与交互式场景，具有实际临床潜力与可复现实验基础。

Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in
dentistry, providing volumetric information about the anatomical structures of
jaws and teeth. Accurate segmentation of these anatomies is critical for
clinical applications such as diagnosis and surgical planning, but remains
time-consuming and challenging. In this paper, we present U-Mamba2, a new
neural network architecture designed for multi-anatomy CBCT segmentation in the
context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state
space models into the U-Net architecture, enforcing stronger structural
constraints for higher efficiency without compromising performance. In
addition, we integrate interactive click prompts with cross-attention blocks,
pre-train U-Mamba2 using self-supervised learning, and incorporate dental
domain knowledge into the model design to address key challenges of dental
anatomy segmentation in CBCT. Extensive experiments, including independent
tests, demonstrate that U-Mamba2 is both effective and efficient, securing top
3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2
achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with
an average inference time of XX (TBC during the ODIN workshop). In Task 2,
U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out
test data. The code is publicly available at
https://github.com/zhiqin1998/UMamba2.

</details>


### [130] [Progressive Flow-inspired Unfolding for Spectral Compressive Imaging](https://arxiv.org/abs/2509.12079)
*Xiaodong Wang,Ping Wang,Zijun He,Mengjie Qin,Xin Yuan*

Main category: cs.CV

TL;DR: 提出一种可控轨迹的展开式深度网络用于CASSI超光谱重建，通过平滑连续的优化路径与高效空间-光谱Transformer及频域融合模块，在仿真与真实数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CASSI的深度展开方法在重建过程中轨迹不可控，跨阶段出现突变与非渐进的质量提升，影响稳定性与性能；需要一种能控制从噪声初值到高质量结果的连续优化轨迹。

Method: 受扩散轨迹与flow matching启发，构建“轨迹可控”的展开式框架：在每一阶段显式数据保真更新+隐式深度去噪，同时施加平滑、连续的优化路径约束；设计高效的空间-光谱Transformer以契合HSI重建，并加入频域融合模块保证特征一致性与信息整合。

Result: 在仿真与真实CASSI数据上，相比最新SOTA，达到更高的重建质量与更高的效率（更快/更省算）；重建过程更平滑、无突跃。

Conclusion: 可控轨迹的展开框架与定制的空间-光谱Transformer、频域融合模块能有效提升CASSI重建的稳定性、质量与效率，表明平滑连续的优化路径是提升展开式HSI重建的重要方向。

Abstract: Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral
image (HSI) from a single 2D compressed measurement, which is a highly
challenging reconstruction task. Recent deep unfolding networks (DUNs),
empowered by explicit data-fidelity updates and implicit deep denoisers, have
achieved the state of the art in CASSI reconstruction. However, existing
unfolding approaches suffer from uncontrollable reconstruction trajectories,
leading to abrupt quality jumps and non-gradual refinement across stages.
Inspired by diffusion trajectories and flow matching, we propose a novel
trajectory-controllable unfolding framework that enforces smooth, continuous
optimization paths from noisy initial estimates to high-quality
reconstructions. To achieve computational efficiency, we design an efficient
spatial-spectral Transformer tailored for hyperspectral reconstruction, along
with a frequency-domain fusion module to gurantee feature consistency.
Experiments on simulation and real data demonstrate that our method achieves
better reconstruction quality and efficiency than prior state-of-the-art
approaches.

</details>


### [131] [End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI](https://arxiv.org/abs/2509.12090)
*Yihong Chen,Jiancheng Yang,Deniz Sayin Mercadier,Hieu Le,Juerg Schwitter,Pascal Fua*

Main category: cs.CV

TL;DR: 提出TetHeart：首个可在全栈与稀疏切片条件下统一重建4D多结构心脏网格的端到端方法，利用显隐式融合的可变形四面体表示、切片自适应2D-3D特征组装与知识蒸馏，以及仅需关键帧标注的两阶段弱监督运动学习，跨多数据集达到SOTA并具备强泛化。


<details>
  <summary>Details</summary>
Motivation: 临床介入过程中常只能获得稀疏CMR切片，现有方法多依赖完整堆栈，难以在稀疏观测下恢复全心脏形状与运动；同时需要统一处理离线全栈与在线稀疏情形，提升患者特异性建模与实时更新能力。

Method: 1) 显式-隐式混合的深度可变形四面体表示（deep deformable tetrahedra）在共享空间中同时建模多心脏结构形状与运动；由高质量全栈初始化得到细致的患者特异性网格，并可用任意数量/位置的切片进行在线更新（从全栈到单张切片）。2) 切片自适应的2D-3D注意力特征组装机制，动态融合任意切片信息；从全切片到稀疏切片的蒸馏策略，保证极端稀疏下的重建精度。3) 两阶段弱监督运动学习，仅需关键帧（如ED/ES）标注进行时序运动估计。

Result: 在三大全公共数据集上训练验证，并在额外的私有介入与公共CMR数据零样本外测，形状与运动重建达到或超过SOTA，且在预处理与介入两种场景均有强泛化与稳健性。

Conclusion: TetHeart实现了统一的4D多结构心脏网格重建框架，可从全栈或极稀疏切片可靠恢复患者特异的形状与运动；通过自适应特征融合、蒸馏与弱监督运动学习，兼顾精度与实用性，适用于离线评估与介入内更新。

Abstract: Reconstructing cardiac motion from cine CMR sequences is critical for
diagnosis, prediction, and intervention. Existing methods rely on complete CMR
stacks to infer full heart motion, limiting their utility in intra-procedural
scenarios where only sparse observations are available. We present TetHeart,
the first end-to-end framework that unifies full 4D multi-structure heart mesh
recovery from both offline full-stack acquisitions and intra-procedural
sparse-slice observations. Our method leverages deep deformable tetrahedra, an
explicit-implicit hybrid representation, to capture shape and motion in a
coherent space shared across cardiac structures. It is initialized from
high-quality pre-procedural or offline-acquired full stacks to build detailed,
patient-specific heart meshes, which can then be updated using whatever slices
are available, from full stacks down to a single slice. We further incorporate
several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D
feature assembly that dynamically integrates information from arbitrary numbers
of slices at any position, combined with a distillation strategy from
full-slice to sparse-slice settings to ensure accurate reconstruction under
extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme
requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on
three large public datasets and externally evaluated zero-shot on additional
private interventional and public CMR datasets, TetHeart achieves
state-of-the-art accuracy and strong generalization in both pre- and
intra-procedural settings.

</details>


### [132] [FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation](https://arxiv.org/abs/2509.12105)
*Bernardo Forni,Gabriele Lombardi,Federico Pozzi,Mirco Planamente*

Main category: cs.CV

TL;DR: 提出FS-SAM2：在SAM2基础上做小样本语义分割，复用其视频模块并用LoRA做轻量适配，少量可训练参数即可在多数据集上取得SOTA/强竞争性能与高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有小样本分割多在预训练模型上叠加新模块并需大量元训练数据才能达最优，训练成本高且泛化有限；而SAM2在零样本图像/视频分割上很强，具有可复用的模块化设计，亟需一种方法将其强视频时序能力转用于小样本任务，并解决从时序相邻帧到静态、分布更广图像域迁移的不匹配问题。

Method: 提出FS-SAM2：直接将SAM2的视频能力重用于few-shot分割，并在SAM2原始模块上施加LoRA进行低秩微调，以极少的可训练参数完成元训练与适配；方法支持任意K-shot配置，核心是以少量参数调整以适应标准数据集中多样、非时间相关的图像分布，同时继承SAM2强大的分割表示。

Result: 在PASCAL-5^i、COCO-20^i、FSS-1000上取得显著优异成绩（文中表述为remarkable）并在推理阶段展现出优秀的计算效率。

Conclusion: 通过将SAM2视频模块直转few-shot并用LoRA做轻量适配，仅需少量可训练参数即可高效适配多样图像域，支持任意K-shot并在多基准上表现出色，证明了基于基础模型的轻量元适配在小样本分割中的有效性。

Abstract: Few-shot semantic segmentation has recently attracted great attention. The
goal is to develop a model capable of segmenting unseen classes using only a
few annotated samples. Most existing approaches adapt a pre-trained model by
training from scratch an additional module. Achieving optimal performance with
these approaches requires extensive training on large-scale datasets. The
Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and
video segmentation with a modular design. In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank
Adaptation (LoRA) to the original modules in order to handle the diverse images
typically found in standard datasets, unlike the temporally connected frames
used in SAM2's pre-training. With this approach, only a small number of
parameters is meta-trained, which effectively adapts SAM2 while benefiting from
its impressive segmentation performance. Our method supports any K-shot
configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and
FSS-1000 datasets, achieving remarkable results and demonstrating excellent
computational efficiency during inference. Code is available at
https://github.com/fornib/FS-SAM2

</details>


### [133] [RailSafeNet: Visual Scene Understanding for Tram Safety](https://arxiv.org/abs/2509.12125)
*Ing. Ondrej Valach,Ing. Ivan Gruber*

Main category: cs.CV

TL;DR: 提出RailSafeNet：基于单目视频的实时轨道入侵风险检测框架，融合语义分割、目标检测与规则式距离评估，通过与标准1435mm轨距比较来判定风险；在RailSem19上取得SegFormer-B3 IoU 65%、YOLOv8 mAP@0.5 75.6%，可在低标注成本下为有轨电车安全预警。


<details>
  <summary>Details</summary>
Motivation: 城市有轨电车与行人/骑行者等在密集区域频繁近距离交互，碰撞风险高且后果严重；现有方案需要昂贵传感器或重标注成本高，缺乏高效、实时、可部署的视觉安全预警系统。

Method: 构建RailSafeNet：1) 使用语义分割（类过滤的SegFormer B3）检测轨道区域/钢轨；2) 使用目标检测（微调YOLOv8）定位行人、自行车、车辆、宠物等近轨目标；3) 规则式Distance Assessor将目标投影到轨面，基于标准1435mm轨距估计其与钢轨的相对位置与距离，按阈值进行风险分类；4) 仅用单目视频，实时融合分割与检测结果以高亮入侵。

Result: 在RailSem19数据集上：SegFormer B3取得65% IoU；YOLOv8在IoU阈值0.5下mAP为75.6%；系统展示了较准确、标注成本较低的场景理解能力。

Conclusion: RailSafeNet能在单目摄像头条件下实时识别轨道与近轨目标并评估入侵风险，为电车驾驶员提供提前预警，提升行人与乘客安全；代码已开源，利于复现与扩展。

Abstract: Tram-human interaction safety is an important challenge, given that trams
frequently operate in densely populated areas, where collisions can range from
minor injuries to fatal outcomes. This paper addresses the issue from the
perspective of designing a solution leveraging digital image processing, deep
learning, and artificial intelligence to improve the safety of pedestrians,
drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions. Using only
monocular video, the system identifies rails, localises nearby objects and
classifies their risk by comparing projected distances with the standard 1435mm
rail gauge. Experiments on the diverse RailSem19 dataset show that a
class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),
while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated
at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore
delivers accurate, annotation-light scene understanding that can warn drivers
before dangerous situations escalate. Code available at
https://github.com/oValach/RailSafeNet.

</details>


### [134] [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/abs/2509.12132)
*Pu Jian,Junhong Wu,Wei Sun,Chen Wang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: 提出Reflection-V，一种强调“视觉反思”的视觉推理模型，通过构造以视觉为中心的推理数据进行冷启动，并在强化学习中引入基于视觉注意力的奖励，显著提升多项视觉推理基准表现与对视觉信息的持续依赖。


<details>
  <summary>Details</summary>
Motivation: 现有将文本“慢思考”迁移到VLM的尝试受阻：随着回答变长，模型对图像的关注迅速衰减，导致视觉反思能力弱，影响视觉推理质量。

Method: 1) 数据构建：用一个在VLM与推理LLM之间交互的代理，生成以视觉为中心的推理数据，实现视觉反思模式的冷启动学习；2) 强化学习：设计基于视觉注意力的奖励，鼓励在推理过程中持续利用图像信息。

Result: Reflection-V在多个视觉推理基准上显著优于现有方法，并在长链式推理中保持更稳定、更强的视觉关注度。

Conclusion: 通过视觉中心的数据与视觉注意力奖励，Reflection-V有效提升了视觉反思能力，改善了长推理时对图像依赖度的衰减，带来整体视觉推理性能提升。

Abstract: Recent advances in text-only "slow-thinking" reasoning have prompted efforts
to transfer this capability to vision-language models (VLMs), for training
visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical
challenges: Effective "slow thinking" in VRMs requires \textbf{visual
reflection}, the ability to check the reasoning process based on visual
information. Through quantitative analysis, we observe that current VRMs
exhibit limited visual reflection, as their attention to visual information
diminishes rapidly with longer generated responses. To address this challenge,
we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection
based on reasoning data construction for cold-start and reward design for
reinforcement learning (RL). Firstly, we construct vision-centered reasoning
data by leveraging an agent that interacts between VLMs and reasoning LLMs,
enabling cold-start learning of visual reflection patterns. Secondly, a visual
attention based reward model is employed during RL to encourage reasoning based
on visual information. Therefore, \textbf{Reflection-V} demonstrates
significant improvements across multiple visual reasoning benchmarks.
Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent
reliance on visual information during visual reasoning, indicating effective
enhancement in visual reflection capabilities.

</details>


### [135] [3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data](https://arxiv.org/abs/2509.12143)
*Nojod M. Alotaibi,Areej M. Alhothali,Manar S. Ali*

Main category: cs.CV

TL;DR: 提出一套将3D ViT区域嵌入与GNN图分类相结合的sMRI自动抑郁症识别管线，基于REST-meta-MDD，在10折交叉验证下最佳准确率78.98%；基于脑图谱的区域定义优于均匀立方体划分。


<details>
  <summary>Details</summary>
Motivation: 现有MDD影像学自动检测多用体素级特征或依赖预定义图谱的手工区域特征，难以同时捕获复杂脑模式与区域间关系；需要一种既能学习更丰富区域表征，又能建模区域交互的端到端方法。

Method: 构建统一框架：1) 用3D Vision Transformer从sMRI中提取区域级嵌入；两种区域定义策略：a) 基于结构/功能脑图谱的atlas分区；b) 基于均匀3D patch的cube分区并让ViT学习区域。2) 以区域嵌入构建基于余弦相似度的图，表示脑区间关系。3) 采用GNN进行图级分类以检测MDD。4) 在REST-meta-MDD数据集上进行分层10折交叉验证评估。

Result: 最佳模型达到：准确率78.98%、敏感度76.54%、特异度81.58%、精确率81.58%、F1为78.98。Atlas策略整体优于cube策略。

Conclusion: ViT提取的区域嵌入结合GNN图分类可有效进行sMRI的MDD识别；引入解剖/功能先验的atlas分区比纯数据驱动的均匀划分更有效，强调领域先验的重要性。

Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that
negatively impacts both individual well-being and global public health.
Automated detection of MDD using structural magnetic resonance imaging (sMRI)
and deep learning (DL) methods holds increasing promise for improving
diagnostic accuracy and enabling early intervention. Most existing methods
employ either voxel-level features or handcrafted regional representations
built from predefined brain atlases, limiting their ability to capture complex
brain patterns. This paper develops a unified pipeline that utilizes Vision
Transformers (ViTs) for extracting 3D region embeddings from sMRI data and
Graph Neural Network (GNN) for classification. We explore two strategies for
defining regions: (1) an atlas-based approach using predefined structural and
functional brain atlases, and (2) an cube-based method by which ViTs are
trained directly to identify regions from uniformly extracted 3D patches.
Further, cosine similarity graphs are generated to model interregional
relationships, and guide GNN-based classification. Extensive experiments were
conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of
our model. With stratified 10-fold cross-validation, the best model obtained
78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and
78.98% F1-score. Further, atlas-based models consistently outperformed the
cube-based approach, highlighting the importance of using domain-specific
anatomical priors for MDD detection.

</details>


### [136] [Open-ended Hierarchical Streaming Video Understanding with Vision Language Models](https://arxiv.org/abs/2509.12145)
*Hyolim Kang,Yunsu Park,Youngbeom Yoo,Yeeun Choi,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出“分层流式视频理解”任务，结合在线时序动作定位与自由文本描述；构建OpenHOUSE系统，通过LLM进行层级事件聚合与专用流式模块优化边界检测，显著优于直接扩展的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏分层、细粒度的时序标注，限制了流式视频感知从单一分类走向更丰富的事件理解与描述。需要能在在线场景中同时定位动作并生成高层事件与文本描述的框架。

Method: 1) 利用LLM将原子动作聚合为高层事件，丰富现有数据集的层级标注；2) 提出OpenHOUSE：面向流式的分层理解系统，包含专门的流式模块，精确检测紧邻动作的边界；3) 将在线时序动作定位与自由形式描述生成结合，拓展流式感知的能力。

Result: 专用流式模块在紧邻动作边界识别上表现突出，相比直接把现有方法做流式扩展，性能近乎翻倍；实现了从分类到描述的能力提升。

Conclusion: OpenHOUSE验证了将强生成模型融入流式动作感知的可行性，分层与在线结合能显著提升边界检测与事件理解，为未来流式视频理解的发展奠定基础。

Abstract: We introduce Hierarchical Streaming Video Understanding, a task that combines
online temporal action localization with free-form description generation.
Given the scarcity of datasets with hierarchical and fine-grained temporal
annotations, we demonstrate that LLMs can effectively group atomic actions into
higher-level events, enriching existing datasets. We then propose OpenHOUSE
(Open-ended Hierarchical Online Understanding System for Events), which extends
streaming action perception beyond action classification. OpenHOUSE features a
specialized streaming module that accurately detects boundaries between closely
adjacent actions, nearly doubling the performance of direct extensions of
existing methods. We envision the future of streaming action perception in the
integration of powerful generative models, with OpenHOUSE representing a key
step in that direction.

</details>


### [137] [Multi Anatomy X-Ray Foundation Model](https://arxiv.org/abs/2509.12146)
*Nishank Singla,Krisztian Koos,Farzin Haddadpour,Amin Honarmandi Shandiz,Lovish Chum,Xiaojian Xu,Qing Jin,Erhan Bas*

Main category: cs.CV

TL;DR: 提出XR-0：一个面向多解剖部位的X射线基础模型，基于115万张多部位X片自监督预训练，并在12个数据集、20个任务上评测，跨分类/检索/分割/定位/视觉指引/报告生成等取得SOTA或竞争力表现，证明解剖多样性与监督对构建通用医学视觉模型至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有放射学AI基础模型几乎局限于胸部X光，泛化到其他解剖部位与更广临床任务能力不足，亟需一个能跨多解剖区域、可扩展且鲁棒的通用X射线视觉基础模型。

Method: 构建XR-0：在一个包含115万张、覆盖多种解剖部位的私有X射线数据集上进行自监督预训练；随后在12个公开/内部数据集上针对20个下游任务进行评估，任务涵盖分类、检索、分割、定位、视觉指引与报告生成；与胸部特定和通用基线进行比较。

Result: XR-0在大多数多解剖任务上达到SOTA，在胸部特定基准上保持有竞争力；显示跨解剖多样性训练能提升模型泛化与任务迁移能力。

Conclusion: 增加解剖多样性与适当监督是构建鲁棒、可扩展通用医学影像基础模型的关键；XR-0为放射学多任务、多解剖通用AI系统奠定基础。

Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation
models are limited to chest anatomy and fail to generalize across broader
clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray
foundation model using self-supervised learning on a large, private dataset of
1.15 million images spanning diverse anatomical regions and evaluated across 12
datasets and 20 downstream tasks, including classification, retrieval,
segmentation, localization, visual grounding, and report generation. XR-0
achieves state-of-the-art performance on most multi-anatomy tasks and remains
competitive on chest-specific benchmarks. Our results demonstrate that
anatomical diversity and supervision are critical for building robust,
general-purpose medical vision models, paving the way for scalable and
adaptable AI systems in radiology.

</details>


### [138] [LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury](https://arxiv.org/abs/2509.12155)
*M. Bolhassani,B. Veasey,E. Daugherty,S. Keltner,N. Kumar,N. Dunlap,A. Amini*

Main category: cs.CV

TL;DR: 评估在RILI诊断任务中，用LoRA微调大规模视觉模型（DinoV2、SwinV2）相较全参数微调与不微调的效果与效率；结果显示LoRA以更少可训练参数实现相当或更佳性能，并显著降低算力与训练时间。


<details>
  <summary>Details</summary>
Motivation: RILI在SBRT后诊断具有挑战，数据与计算资源有限使全参数微调成本高；探索参数高效微调（LoRA）能否在医学影像（CT）任务中保持或提升性能，同时降低训练成本与对3D数据适配难度。

Method: 使用DinoV2与SwinV2作为2D大视觉模型基底，比较三种策略：LoRA微调、全参数微调、推理仅模式；对以治疗等中心为中心的两种裁剪体素尺寸（50 mm^3与75 mm^3）进行实验，并测试多种将2D模型适配到3D体数据的方法，以评估对空间上下文的敏感性；度量性能、计算成本与训练时间。

Result: LoRA在RILI分类上取得与全参数微调相当或更优的性能；同时大幅减少可训练参数与训练时间，降低计算成本；模型对裁剪尺寸（空间上下文）与2D→3D适配策略存在敏感性。

Conclusion: 在RILI CT诊断中，LoRA为高效且有效的微调方案，能以更低计算代价达到甚至超过全参微调的表现；选择合适的空间上下文和2D到3D适配方式对性能有重要影响。

Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for
fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose
Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic
Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of
this approach, we compare LoRA with traditional full fine-tuning and
inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3
and 75 mm3), centered at the treatment isocenter, in addition to different
adaptation techniques for adapting the 2D LVMs for 3D data were used to
determine the sensitivity of the models to spatial context. Experimental
results show that LoRA achieves comparable or superior performance to
traditional fine-tuning while significantly reducing computational costs and
training times by requiring fewer trainable parameters.

</details>


### [139] [HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments](https://arxiv.org/abs/2509.12187)
*Johanna Karras,Yingwei Li,Yasamin Jafarian,Ira Kemelmacher-Shlizerman*

Main category: cs.CV

TL;DR: 提出HoloGarment：从1-3张图或视频生成服装在规范姿态下的360度新视角；通过共享服装嵌入与“服装图谱”微调，跨真实/合成域，达成SOTA、细节与一致性兼顾。


<details>
  <summary>Details</summary>
Motivation: 现有NVS方法多依赖合成、无遮挡、静态3D数据，难以泛化至真实服装场景，面对姿态复杂、遮挡、布料大变形与褶皱等挑战表现不佳。需要一种能利用真实视频、适配真实分布且对姿态/遮挡鲁棒的方案。

Method: 提出隐式训练范式：联合大规模真实视频与小规模合成3D数据，优化共享的服装嵌入空间；推理时对给定真实视频微调该服装嵌入，构建“服装图谱（atlas）”表示，解耦身体姿态/运动，统一表征服装在各视角的几何与纹理；支持从少量图像或连续视频重建规范姿态下的360°视图。

Result: 在真实服装NVS任务上达到SOTA；在褶皱、姿态变化、遮挡等困难情况下保持高逼真度、视角一致性、细节与几何准确性；同时支持从图像与视频输入进行360°生成与动态视频到360°的转换。

Conclusion: 通过共享嵌入+图谱微调打通真实与合成域，实现对野外服装的鲁棒NVS与高质量重建；方法通用于少样本图像与视频场景，显著提升泛化与细节保真度。

Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due
significant occlusions, complex human poses, and cloth deformations. Prior
methods rely on synthetic 3D training data consisting of mostly unoccluded and
static objects, leading to poor generalization on real-world clothing. In this
paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3
images or a continuous video of a person wearing a garment and generates
360{\deg} novel views of the garment in a canonical pose. Our key insight is to
bridge the domain gap between real and synthetic data with a novel implicit
training paradigm leveraging a combination of large-scale real video data and
small-scale synthetic 3D data to optimize a shared garment embedding space.
During inference, the shared embedding space further enables dynamic
video-to-360{\deg} NVS through the construction of a garment "atlas"
representation by finetuning a garment embedding on a specific real-world
video. The atlas captures garment-specific geometry and texture across all
viewpoints, independent of body pose or motion. Extensive experiments show that
HoloGarment achieves state-of-the-art performance on NVS of in-the-wild
garments from images and videos. Notably, our method robustly handles
challenging real-world artifacts -- such as wrinkling, pose variation, and
occlusion -- while maintaining photorealism, view consistency, fine texture
details, and accurate geometry. Visit our project page for additional results:
https://johannakarras.github.io/HoloGarment

</details>


### [140] [Domain-Adaptive Pretraining Improves Primate Behavior Recognition](https://arxiv.org/abs/2509.12193)
*Felix B. Mueller,Timo Lueddecke,Richard Vogg,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 利用自监督与领域自适应预训练提升灵长类行为识别，在两套大猩猩/黑猩猩数据集上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱视频带来大规模数据，但标注成本高，限制了动物行为识别研究的规模化与泛化能力，需要数据高效的学习方案。

Method: 采用自监督视觉表征模型V-JEPA作为起点，并进行领域自适应预训练（DAP），即在无标注的同域灵长类视频上继续预训练，再进行下游动作识别微调；与现有SOTA方法在PanAf与ChimpACT上对比评测。

Result: 在PanAf上提升准确率6.1个百分点，在ChimpACT上提升mAP 6.3个百分点；消融显示主要性能增益来自DAP过程而非仅使用通用自监督预训练。

Conclusion: 领域自适应自监督预训练能在无需标注的前提下显著提升动物行为识别效果，具有推广潜力；代码已开源以便复现与扩展。

Abstract: Computer vision for animal behavior offers promising tools to aid research in
ecology, cognition, and to support conservation efforts. Video camera traps
allow for large-scale data collection, but high labeling costs remain a
bottleneck to creating large-scale datasets. We thus need data-efficient
learning approaches. In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior. On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.
mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data. We show that most of the performance gain stems from the
DAP. Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples. Code is available at
https://github.com/ecker-lab/dap-behavior

</details>


### [141] [3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review](https://arxiv.org/abs/2509.12197)
*Salma Galaaoui,Eduardo Valle,David Picard,Nermin Samet*

Main category: cs.CV

TL;DR: 综述论文：系统回顾基于野外LiDAR点云的3D人体姿态估计与人体网格恢复，提出分类框架、方法对比、数据与评测梳理，并给出基准与未来方向。


<details>
  <summary>Details</summary>
Motivation: LiDAR在真实场景中具有鲁棒几何信息与远距离感知优势，但现有3D人体理解研究分散、术语不统一、评测不一致，缺乏系统综述与可比基准，阻碍方法公平比较与领域推进。

Method: 构建结构化分类法，从输入表示、人体先验、网络架构、回归/优化策略、监督信号与训练范式、时序与多传感器融合、效率与鲁棒性等维度梳理方法；系统分析各方法优劣与设计取舍；量化比较三大主流数据集（规模、标注类型、采集配置、场景多样性等）；统一评测指标定义；在各数据集上整理并建立两大任务（姿态、网格）的基准表；总结挑战与未来研究方向；提供持续更新的网页资源。

Result: 得到标准化的术语与评测口径、清晰的类别与方法版图；列出三大数据集的量化对比与各任务的基准成绩表；归纳现有方法的性能与局限，并给出可复现与公平比较的参考。

Conclusion: 该综述为LiDAR驱动的3D人体理解提供统一框架与评价基线，识别关键瓶颈（如稀疏性/自遮挡、跨域泛化、标注稀缺、时空一致性与多模态融合），并指向未来方向（大规模数据与弱/自监督、生成先验与物理约束、跨传感器校准与融合、实时与隐私友好开发），以促进社区协同进展。

Abstract: In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR

</details>


### [142] [OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](https://arxiv.org/abs/2509.12201)
*Yang Zhou,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Haoyu Guo,Zizun Li,Kaijing Ma,Xinyue Li,Yating Wang,Haoyi Zhu,Mingyu Liu,Dingning Liu,Jiange Yang,Zhoujie Fu,Junyi Chen,Chunhua Shen,Jiangmiao Pang,Kaipeng Zhang,Tong He*

Main category: cs.CV

TL;DR: 提出OmniWorld，一个大规模多域多模态数据集与基准，专为4D世界建模（空间+时间）设计，弥补现有数据在动态复杂性、跨域多样性与时空标注上的不足；在该数据上微调SOTA可显著提升4D重建与视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前4D世界模型的发展受限于高质量数据稀缺；现有数据集缺乏足够的动态复杂性、跨域覆盖和时空标注，难以支撑4D重建、未来预测、可控相机视频生成等关键任务。

Method: 构建OmniWorld数据资源：1) 新采集的OmniWorld-Game（更丰富模态、更大规模、更真实的动态交互）；2) 精选并整合多个公开数据集，覆盖多域多模态；据此建立更具挑战性的基准评测，系统评估现有SOTA方法。

Result: 基准实验揭示现有SOTA在复杂4D环境建模上的不足；在OmniWorld上对SOTA进行微调，在4D重建与视频生成等任务上取得显著性能提升。

Conclusion: OmniWorld为训练与评估通用4D世界模型提供关键数据基础和标准基准，有望加速推动机器对物理世界的整体性理解与建模。

Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry
and temporal dynamics - has witnessed remarkable progress in recent years,
driven by advances in large-scale generative models and multimodal learning.
However, the development of truly general 4D world models remains fundamentally
constrained by the availability of high-quality data. Existing datasets and
benchmarks often lack the dynamic complexity, multi-domain diversity, and
spatial-temporal annotations required to support key tasks such as 4D geometric
reconstruction, future prediction, and camera-control video generation. To
address this gap, we introduce OmniWorld, a large-scale, multi-domain,
multi-modal dataset specifically designed for 4D world modeling. OmniWorld
consists of a newly collected OmniWorld-Game dataset and several curated public
datasets spanning diverse domains. Compared with existing synthetic datasets,
OmniWorld-Game provides richer modality coverage, larger scale, and more
realistic dynamic interactions. Based on this dataset, we establish a
challenging benchmark that exposes the limitations of current state-of-the-art
(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning
existing SOTA methods on OmniWorld leads to significant performance gains
across 4D reconstruction and video generation tasks, strongly validating
OmniWorld as a powerful resource for training and evaluation. We envision
OmniWorld as a catalyst for accelerating the development of general-purpose 4D
world models, ultimately advancing machines' holistic understanding of the
physical world.

</details>


### [143] [LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence](https://arxiv.org/abs/2509.12203)
*Zixin Yin,Xili Dai,Duomin Wang,Xianfang Zeng,Lionel M. Ni,Gang Yu,Heung-Yeung Shum*

Main category: cs.CV

TL;DR: LazyDrag提出一种用于多模态扩散Transformer的拖拽式图像编辑方法，通过显式对应图替代注意力中的隐式点匹配，避免测试时优化，提升几何控制与文本引导的一致性与生成力，在DragBench上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽编辑依赖注意力中的隐式点匹配，导致反演能力减弱、需要昂贵的测试时优化（TTO），限制了高保真补洞与文本引导生成能力。需要一种稳定、强反演且无需TTO的方案。

Method: 基于用户拖拽输入，显式生成对应关系图（correspondence map），作为可靠参考增强注意力控制，实现稳定的全强度反演；在多模态扩散Transformer中将该参考融入，统一几何精确控制与文本条件；支持多轮交互、同时移动与缩放操作。

Result: 在DragBench基准上，LazyDrag在拖拽精度与感知质量上优于基线，VIEScore与人工评测均验证其优势；可完成以往困难任务：开狗嘴并补全内部、生成新物体（如网球）、对含糊拖拽进行语境感知调整（如把手移入口袋）。

Conclusion: LazyDrag去除了对隐式点匹配与TTO的依赖，实现稳定强反演与更强的生成编辑能力，统一几何与文本控制，达成SOTA并为拖拽编辑范式提供新方向。

Abstract: The reliance on implicit point matching via attention has become a core
bottleneck in drag-based editing, resulting in a fundamental compromise on
weakened inversion strength and costly test-time optimization (TTO). This
compromise severely limits the generative capabilities of diffusion models,
suppressing high-fidelity inpainting and text-guided creation. In this paper,
we introduce LazyDrag, the first drag-based image editing method for
Multi-Modal Diffusion Transformers, which directly eliminates the reliance on
implicit point matching. In concrete terms, our method generates an explicit
correspondence map from user drag inputs as a reliable reference to boost the
attention control. This reliable reference opens the potential for a stable
full-strength inversion process, which is the first in the drag-based editing
task. It obviates the necessity for TTO and unlocks the generative capability
of models. Therefore, LazyDrag naturally unifies precise geometric control with
text guidance, enabling complex edits that were previously out of reach:
opening the mouth of a dog and inpainting its interior, generating new objects
like a ``tennis ball'', or for ambiguous drags, making context-aware changes
like moving a hand into a pocket. Additionally, LazyDrag supports multi-round
workflows with simultaneous move and scale operations. Evaluated on the
DragBench, our method outperforms baselines in drag accuracy and perceptual
quality, as validated by VIEScore and human evaluation. LazyDrag not only
establishes new state-of-the-art performance, but also paves a new way to
editing paradigms.

</details>


### [144] [Character-Centric Understanding of Animated Movies](https://arxiv.org/abs/2509.12204)
*Zhongrui Gui,Junyu Xie,Tengda Han,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出一个音频-视觉联合管线，自动构建角色音视库，实现对动画电影角色的鲁棒识别，并用于无障碍音频描述与角色感知字幕；发布含75部动画电影的新数据集CMD-AM，效果显著超越基于人脸检测的方法。


<details>
  <summary>Details</summary>
Motivation: 动画角色外观、动作与形变高度多样，传统人脸识别依赖稳定视觉纹理，难以在动画中奏效；现有系统对长尾分布和跨模态线索利用不足，影响无障碍与叙事理解。

Method: 1) 从在线资源自动构建“角色音视库”，为每个角色收集视觉样例与语音样本；2) 设计音频-视觉联合识别管线，利用多模态匹配缓解外观长尾与形变问题；3) 基于稳健角色识别，开发两个下游：面向视障的音频描述生成与面向听障的角色感知字幕；4) 发布CMD-AM数据集并进行系统评测。

Result: 在角色识别与下游任务上，相比依赖人脸检测的基线方法取得显著提升；多模态融合在长尾与外观变化剧烈场景中表现尤佳。

Conclusion: 自动化构建的角色音视库结合音频-视觉融合，可有效提升动画电影中的角色识别，进而改善可及性与叙事理解；CMD-AM为该领域研究提供了基准与资源。

Abstract: Animated movies are captivating for their unique character designs and
imaginative storytelling, yet they pose significant challenges for existing
recognition systems. Unlike the consistent visual patterns detected by
conventional face recognition methods, animated characters exhibit extreme
diversity in their appearance, motion, and deformation. In this work, we
propose an audio-visual pipeline to enable automatic and robust animated
character recognition, and thereby enhance character-centric understanding of
animated movies. Central to our approach is the automatic construction of an
audio-visual character bank from online sources. This bank contains both visual
exemplars and voice (audio) samples for each character, enabling subsequent
multi-modal character recognition despite long-tailed appearance distributions.
Building on accurate character recognition, we explore two downstream
applications: Audio Description (AD) generation for visually impaired
audiences, and character-aware subtitling for the hearing impaired. To support
research in this domain, we introduce CMD-AM, a new dataset of 75 animated
movies with comprehensive annotations. Our character-centric pipeline
demonstrates significant improvements in both accessibility and narrative
comprehension for animated content over prior face-detection-based approaches.
For the code and dataset, visit
https://www.robots.ox.ac.uk/~vgg/research/animated_ad/.

</details>
