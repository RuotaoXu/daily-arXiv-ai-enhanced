<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SOSControl: Enhancing Human Motion Generation through Saliency-Aware Symbolic Orientation and Timing Control](https://arxiv.org/abs/2601.14258)
*Ho Yin Au,Junkun Jiang,Jie Chen*

Main category: cs.CV

TL;DR: 提出SOS脚本与SOSControl框架，实现对文本生成动作中关键帧的部位朝向与时间精确控制，并给出从动作数据自动提取符号脚本的管线；实验显示在质量、可控性与泛化上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作方法控制力不足；基于关键点位置的方法只能给出位置约束，难以直观指定身体部位朝向与动作时间，导致用户难以精确编辑与复现意图。

Method: 1) 提出可编程的Salient Orientation Symbolic (SOS)脚本，用符号在关键帧上标注身体部位朝向与时序；2) 设计自动SOS提取管线：使用带时间约束的凝聚聚类做帧显著性检测，并用基于显著性的掩码方案（SMS）从动作数据生成稀疏、可解释的SOS脚本；3) 提出SOSControl：把稀疏SOS中的朝向符号视为显著约束，优先满足；结合SMS数据增强与基于梯度的迭代优化提升与用户约束的对齐；并采用基于ControlNet的ACTOR-PAE解码器保证平滑自然的动作。

Result: 自动提取的SOS脚本在人类可解释性上良好，关键帧符号具有显著性；在动作质量、可控性与对时间和部位朝向控制的泛化能力上，SOSControl优于现有基线。

Conclusion: 以SOS脚本为核心的表示与控制范式可实现细粒度、直观的关键帧朝向与时序控制；与自动提取和控制框架结合后，能生成更平滑自然且与用户约束更一致的动作，优于当前主流方法。

Abstract: Traditional text-to-motion frameworks often lack precise control, and existing approaches based on joint keyframe locations provide only positional guidance, making it challenging and unintuitive to specify body part orientations and motion timing. To address these limitations, we introduce the Salient Orientation Symbolic (SOS) script, a programmable symbolic framework for specifying body part orientations and motion timing at keyframes. We further propose an automatic SOS extraction pipeline that employs temporally-constrained agglomerative clustering for frame saliency detection and a Saliency-based Masking Scheme (SMS) to generate sparse, interpretable SOS scripts directly from motion data. Moreover, we present the SOSControl framework, which treats the available orientation symbols in the sparse SOS script as salient and prioritizes satisfying these constraints during motion generation. By incorporating SMS-based data augmentation and gradient-based iterative optimization, the framework enhances alignment with user-specified constraints. Additionally, it employs a ControlNet-based ACTOR-PAE Decoder to ensure smooth and natural motion outputs. Extensive experiments demonstrate that the SOS extraction pipeline generates human-interpretable scripts with symbolic annotations at salient keyframes, while the SOSControl framework outperforms existing baselines in motion quality, controllability, and generalizability with respect to motion timing and body part orientation control.

</details>


### [2] [A Cloud-Based Cross-Modal Transformer for Emotion Recognition and Adaptive Human-Computer Interaction](https://arxiv.org/abs/2601.14259)
*Ziwen Zhong,Zhitao Shu,Yue Zhao*

Main category: cs.CV

TL;DR: 提出云端跨模态Transformer（CMT），融合视觉/语音/文本并在云上可扩展部署，实现实时、鲁棒的情感识别，SOTA，延迟低。


<details>
  <summary>Details</summary>
Motivation: 单一模态情感识别在真实场景鲁棒性与泛化性差；需要同时利用视觉、语音、文本并在大规模交互中保持低时延与可扩展性。

Method: 采用预训练编码器（ViT、Wav2Vec2、BERT）分别抽取视觉、语音、文本特征；通过跨模态注意力对齐并建模模态间依赖；在云端（Kubernetes + 分布式训练 + TensorFlow Serving）部署，实现可扩展与低时延推理。

Result: 在IEMOCAP、MELD、AffectNet上取得SOTA：F1提升3.0%，交叉熵下降12.9%；云端推理平均延迟128 ms，比传统Transformer融合方案降低35%。

Conclusion: CMT在精度与时延上均优于强基线，适用于客服、虚拟教学、情感接口等场景，推动云原生情感计算与情感智能交互系统落地。

Abstract: Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.

</details>


### [3] [Intelligent Power Grid Design Review via Active Perception-Enabled Multimodal Large Language Models](https://arxiv.org/abs/2601.14261)
*Taoliang Tan,Chengwei Ma,Zhen Tian,Zhao Lin,Dongdong Li,Si Shi*

Main category: cs.CV

TL;DR: 提出一个三阶段、由预训练多模态大模型（MLLM）通过提示工程驱动的电网工程设计图智能审图框架：先全局语义感知提出语义区域，再对候选区域做高分辨率细粒度识别，最后融合置信度进行错误诊断与可靠性评估；在真实数据上优于传统被动式MLLM推理。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率电网设计图难以被现有自动化系统高效准确处理：计算成本高、缩放导致信息丢失、缺乏全局语义理解以定位设计错误。需要一种既能把握整体语义又能在关键部位精细识别、并输出可置信判断的审图方法。

Method: 构建仿人专家流程的三阶段框架并用高级提示工程驱动MLLM：1）全局阶段：对低分辨率全图进行全局语义理解，自动提出电力领域相关的语义候选区域；2）局部阶段：对候选区域以高分辨率进行细粒度识别，产出结构化信息及置信度；3）决策阶段：融合带置信度的多区域结果，进行设计错误诊断并给出可靠性评估。

Result: 在真实电网设计图上，框架显著提升MLLM对宏观语义的把握与设计错误定位能力，缺陷发现准确率与审查结论的可靠性均优于传统被动式MLLM推理。

Conclusion: 基于提示驱动的三阶段MLLM审图范式可在处理超高分辨率电网设计图时实现高效、可靠的错误识别与评估，为电网工程智能审图提供了新的可行路径。

Abstract: The intelligent review of power grid engineering design drawings is crucial for power system safety. However, current automated systems struggle with ultra-high-resolution drawings due to high computational demands, information loss, and a lack of holistic semantic understanding for design error identification. This paper proposes a novel three-stage framework for intelligent power grid drawing review, driven by pre-trained Multimodal Large Language Models (MLLMs) through advanced prompt engineering. Mimicking the human expert review process, the first stage leverages an MLLM for global semantic understanding to intelligently propose domain-specific semantic regions from a low-resolution overview. The second stage then performs high-resolution, fine-grained recognition within these proposed regions, acquiring detailed information with associated confidence scores. In the final stage, a comprehensive decision-making module integrates these confidence-aware results to accurately diagnose design errors and provide a reliability assessment. Preliminary results on real-world power grid drawings demonstrate our approach significantly enhances MLLM's ability to grasp macroscopic semantic information and pinpoint design errors, showing improved defect discovery accuracy and greater reliability in review judgments compared to traditional passive MLLM inference. This research offers a novel, prompt-driven paradigm for intelligent and reliable power grid drawing review.

</details>


### [4] [LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models](https://arxiv.org/abs/2601.14330)
*Mengyu Sun,Ziyuan Yang,Andrew Beng Jin Teoh,Junxu Liu,Haibo Hu,Yi Zhang*

Main category: cs.CV

TL;DR: 论文提出LURE，通过在扩散模型的潜空间重建与采样引导，稳定“复活”被擦除的敏感概念，支持多概念高保真再现。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法被证明可被“重唤醒”，但多数攻击只做提示词层面的优化，忽视文本条件、模型参数、潜变量等多因素作用机制，缺乏系统理论理解与更强的重唤醒技术。

Method: 将生成过程视为隐式函数，理论分析表明对文本条件、模型参数、潜状态任一扰动均可重唤醒概念。据此提出LURE：1) 语义重绑定（对齐去噪预测与目标分布，重建潜空间以恢复文本-视觉关联）；2) 梯度场正交化（在多概念场景中强制特征/梯度正交，避免冲突与纠缠）；3) 基于潜语义识别的引导采样LSIS（通过后验密度校验稳定重唤醒轨迹）。

Result: 在多种擦除任务与方法上，LURE可同时、高保真地重唤醒多个被擦除概念，实验广泛验证其有效性与稳健性。

Conclusion: 多因素扰动均可导致概念重唤醒；通过潜空间重建与正交化+后验校验的采样引导，可稳定、并行地复现被擦除概念，揭示了现有擦除技术的脆弱性。

Abstract: Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.

</details>


### [5] [CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments](https://arxiv.org/abs/2601.14339)
*Haotian Xu,Yue Hu,Zhengqiu Zhu,Chen Gao,Ziyou Wang,Junreng Rao,Wenhao Lu,Weishi Li,Quanjun Yin,Yong Li*

Main category: cs.CV

TL;DR: 提出CityCube基准，用多平台多视角和四种视点动态评估VLM在城市开放空间的跨视角空间推理；评测33个VLM，发现与人类有显著差距，且小模型经微调可超60%准确率，显示基准的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角推理评测多集中在室内或街景，忽视开放式城市空间中语义丰富、几何复杂、视角变化大的挑战。需要一个系统性基准来检验VLM在真实城市多平台多视点条件下的空间理解与推理能力。

Method: 构建CityCube基准：涵盖车辆、无人机、卫星等多平台视角，模拟四种视点动态；设计五类认知维度与三种空间关系表达；标注5,022组多视图问答对；对33个VLM进行全面评测，并进行任务相关性与人机认知差异分析。

Result: 大型通用VLM在该基准上准确率最高约54.1%，比人类低34.2个百分点；相反，小规模且针对性微调的VLM可超过60.0%准确率；呈现不同任务之间的相关性与模型在基本认知上的不足。

Conclusion: CityCube揭示当前VLM在开放城市环境跨视角推理上的显著短板，并表明面向场景与任务的针对性微调可带来显著收益；该基准为研究弥合VLM与人类推理差距、推进具身智能的空间理解提供了关键测试平台。

Abstract: Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.

</details>


### [6] [Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data](https://arxiv.org/abs/2601.14406)
*Yixiong Chen,Zongwei Zhou,Wenxuan Li,Alan Yuille*

Main category: cs.CV

TL;DR: 提出SegAE，一个轻量级视觉-语言模型，可自动评估医学分割标签质量，相关系数0.902、单个3D掩膜评估耗时0.06秒，并在主动/半监督学习中显著降低标注与质检成本。


<details>
  <summary>Details</summary>
Motivation: 大型医学分割数据集中手工与伪标签质量参差不齐，劣质标签削弱训练与评测的可靠性与鲁棒性，亟需可扩展、自动化的标签质量控制手段。

Method: 构建SegAE：基于视觉-语言范式的轻量模型，使用超过四百万图像-标签对及其质量分数进行训练，学习从图像与对应分割掩膜预测质量分数（与Dice相一致）。模型可跨142个解剖结构进行质量评估，推理速度快（0.06秒/3D掩膜）。

Result: 在质量预测上与真实Dice的相关系数达0.902；对公共数据集分析发现低质量标注普遍存在；在主动学习与半监督学习中通过筛选高质量数据提升数据效率与训练表现，将整体标注成本降低约1/3，并将单标签质检时间缩短约70%。

Conclusion: SegAE为大规模医学分割数据集提供了简单高效的质量控制方案，可快速、可靠地评估多结构标签质量，并在实际训练流程中带来成本与性能的双重收益；数据与代码开源。

Abstract: Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.

</details>


### [7] [Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation](https://arxiv.org/abs/2601.14438)
*Danial Sadrian Zadeh,Otman A. Basir,Behzad Moshiri*

Main category: cs.CV

TL;DR: 提出从单目前视摄像头图像生成面向驾驶的自然语言场景描述的框架，结合混合注意力与新构建的BDD100K衍生数据集，在CIDEr、SPICE及人工评测上表现良好。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要既懂空间布局又懂语义关系与驾驶要点的场景理解。现有方法或侧重检测/分割，或通用图像描述，缺乏面向交通场景、可解释且与驾驶相关的语言描述；同时缺少合适的数据与评价标准。

Method: 提出单图像到文本的生成框架：利用混合注意力（空间+语义）增强特征提取与融合，生成包含布局、实体关系和驾驶相关线索的描述；并从BDD100K构建专用数据集，给出构建指南；系统讨论并选择适配任务的评价指标。

Result: 在新数据集上，通过CIDEr、SPICE等自动指标与人工主观评测，模型取得强性能，能有效生成上下文丰富且驾驶相关的描述。

Conclusion: 该混合注意力的图像到文本框架结合新数据集与合适评测，能准确、可解释地描述交通场景，满足自动驾驶感知与解释需求。

Abstract: Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.

</details>


### [8] [Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2601.14448)
*A. Enes Doruk*

Main category: cs.CV

TL;DR: 提出一种基于高斯表示的自适应相机-激光雷达多模态3D语义占据预测模型，兼顾语义与几何并以线性复杂度解码，包含四个模块：LDFA深度可变形采样、基于熵的特征平滑、自适应跨模态融合与Gauss-Mamba全局上下文头。


<details>
  <summary>Details</summary>
Motivation: 稀疏目标检测难以应对自动驾驶中的长尾与安全场景，需要转向稠密3D占据预测。然而现有体素化方法计算开销大、融合机制僵硬且在动态环境下易失效，亟需高效、鲁棒的多模态融合方案。

Method: 以内存友好的3D高斯模型为核心：1) LDFA使用按深度的可变形采样缓解LiDAR几何稀疏；2) 基于交叉熵的特征平滑抑制域噪声与不确定性；3) 自适应相机-激光雷达融合根据模型输出动态重标定传感器贡献；4) Gauss-Mamba头以选择性状态空间模型进行全局上下文解码，具线性复杂度。

Result: 在不牺牲精度的前提下降低计算与内存成本，并在动态场景与长尾安全案例中提升多模态占据预测的稳健性（摘要暗示的效果，具体数值未给出）。

Conclusion: 高斯表示+SSM解码的自适应多模态框架可高效而鲁棒地将相机语义与LiDAR几何结合，改善动态环境与长尾场景下的3D占据预测；建议未来在更大规模与真实车载设置中验证并量化收益。

Abstract: The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.

</details>


### [9] [Real-Time Wildfire Localization on the NASA Autonomous Modular Sensor using Deep Learning](https://arxiv.org/abs/2601.14475)
*Yajvan Ravan,Aref Malek,Chester Dolph,Nikhil Behari*

Main category: cs.CV

TL;DR: 提出并公开一个基于NASA AMS的12通道高空多光谱野火数据集，并结合分类与分割网络构建实时火线分割模型，在复杂条件（夜间、云烟遮挡、易混淆目标）下取得优于以往方法的精度。


<details>
  <summary>Details</summary>
Motivation: 高空多光谱航拍图像稀缺且昂贵，但对野火检测等高影响任务的机器学习至关重要；现有依赖卫星或可见光的方案在夜间/遮挡/误报上受限，亟需高质量标注数据与适配模型。

Method: 收集20次野火任务的AMS 12波段（含IR、SWIR、热红外）影像，采样成4000+带标注小图块；训练两个深度网络：图像级分类与像素级分割，并将二者组合为实时分割管线以在输入图像流上高效定位野火边界；进行通道重要性分析以评估各波段贡献。

Result: 在该多光谱数据集上，组合模型达到96%分类准确率、74% IoU、84%召回，超越卫星数据训练的模型及传统颜色规则法；可在夜间与云/烟遮挡下检测活跃火点并减少误报；发现SWIR、IR与热红外对划定火线最关键。

Conclusion: 多光谱（尤其SWIR/IR/热红外）与分类-分割组合的实时模型显著提升野火边界自动化提取能力，适用于高空任务场景；公开数据与代码为后续研究与部署提供基础。

Abstract: High-altitude, multi-spectral, aerial imagery is scarce and expensive to acquire, yet it is necessary for algorithmic advances and application of machine learning models to high-impact problems such as wildfire detection. We introduce a human-annotated dataset from the NASA Autonomous Modular Sensor (AMS) using 12-channel, medium to high altitude (3 - 50 km) aerial wildfire images similar to those used in current US wildfire missions. Our dataset combines spectral data from 12 different channels, including infrared (IR), short-wave IR (SWIR), and thermal. We take imagery from 20 wildfire missions and randomly sample small patches to generate over 4000 images with high variability, including occlusions by smoke/clouds, easily-confused false positives, and nighttime imagery.
  We demonstrate results from a deep-learning model to automate the human-intensive process of fire perimeter determination. We train two deep neural networks, one for image classification and the other for pixel-level segmentation. The networks are combined into a unique real-time segmentation model to efficiently localize active wildfire on an incoming image feed. Our model achieves 96% classification accuracy, 74% Intersection-over-Union(IoU), and 84% recall surpassing past methods, including models trained on satellite data and classical color-rule algorithms. By leveraging a multi-spectral dataset, our model is able to detect active wildfire at nighttime and behind clouds, while distinguishing between false positives. We find that data from the SWIR, IR, and thermal bands is the most important to distinguish fire perimeters. Our code and dataset can be found here: https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main and https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link

</details>


### [10] [XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping](https://arxiv.org/abs/2601.14477)
*Frank Bieder,Hendrik Königshof,Haohao Hu,Fabian Immel,Yinzhe Shen,Jan-Hendrik Pauls,Christoph Stiller*

Main category: cs.CV

TL;DR: 提出XD-MAP：用相机检测生成语义参数地图，为LiDAR无标注生成伪标签，实现跨传感器、无重叠、从前向到360°扩展的域迁移，并在2D/3D分割与全景分割上大幅超越基线。


<details>
  <summary>Details</summary>
Motivation: 深度模型性能依赖数据且需与目标类别与传感器模态匹配。现实部署常面临训练数据与传感器/场景不匹配，人工标注昂贵。希望把现有图像数据与模型能力迁移到LiDAR等异构传感器，减少标注并覆盖更广视角。

Method: 提出XD-MAP：先在相机图像上用已训练检测器得到目标/路面要素；将这些检测转化为语义参数化地图（结构化地图元素）；利用该地图在目标域（LiDAR）中生成伪标签，无需人工标注；设计使其不需相机与LiDAR时空重叠，并可从前向相机扩展到360°角度覆盖。

Result: 在大规模道路要素数据集上，较单次基线方法提升：2D语义分割+19.5 mIoU，2D全景分割+19.5 PQth，3D语义分割+32.3 mIoU，显示强泛化与显著性能增益。

Conclusion: XD-MAP能在无人工标注、无传感器重叠条件下，将相机知识有效迁移到LiDAR，扩展感知视场并在2D/3D任务上显著提升性能，验证跨模态无监督域迁移的可行性与实用价值。

Abstract: Until open-world foundation models match the performance of specialized approaches, the effectiveness of deep learning models remains heavily dependent on dataset availability. Training data must align not only with the target object categories but also with the sensor characteristics and modalities. To bridge the gap between available datasets and deployment domains, domain adaptation strategies are widely used. In this work, we propose a novel approach to transferring sensor-specific knowledge from an image dataset to LiDAR, an entirely different sensing domain. Our method XD-MAP leverages detections from a neural network on camera images to create a semantic parametric map. The map elements are modeled to produce pseudo labels in the target domain without any manual annotation effort. Unlike previous domain transfer approaches, our method does not require direct overlap between sensors and enables extending the angular perception range from a front-view camera to a full 360 view. On our large-scale road feature dataset, XD-MAP outperforms single shot baseline approaches by +19.5 mIoU for 2D semantic segmentation, +19.5 PQth for 2D panoptic segmentation, and +32.3 mIoU in 3D semantic segmentation. The results demonstrate the effectiveness of our approach achieving strong performance on LiDAR data without any manual labeling.

</details>


### [11] [GutenOCR: A Grounded Vision-Language Front-End for Documents](https://arxiv.org/abs/2601.14490)
*Hunter Heidenreich,Ben Elliott,Olivia Dinica,Yosheb Getachew*

Main category: cs.CV

TL;DR: GutenOCR 通过微调 Qwen2.5-VL 系列，提供统一提示词接口的一体化“读-检-定位”OCR前端，在业务/科研文档与合成数据上训练，显著提升带定位的OCR性能，但在页级线性化、颜色引导OCR与公式密集版面上存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在OCR任务上往往缺乏精确定位与结构化阅读能力，常把“读文本”“检测文本”“文本-区域对齐”分散于不同系统，评测也缺少统一的带定位指标。作者希望用单一检查点、统一提示词接口的VLM实现端到端的阅读、检测与 grounding，并建立相应评测协议。

Method: 以 Qwen2.5-VL-3B/7B 为骨干，通过在业务文档、科学论文与合成grounding数据上微调，得到 GutenOCR 家族；模型以统一的prompt支持整页或局部阅读，输出行/段级BBox，并支持条件式“where is x?”查询。提出一套带grounding的OCR评估协议，并在多基准上评测。

Result: 在10.5K留出页上，GutenOCR-7B 的综合 grounded OCR 分数由基线 Qwen2.5-VL-7B 的0.40提升到0.82（>2倍）。在 Fox 与 OmniDocBench v1.5 上，区域级与行级OCR、以及文本检测召回显著提升。

Conclusion: 统一的、基于提示的VLM微调可将阅读、检测与grounding整合到单模型中，显著提升带定位OCR表现；但在页级线性化、颜色引导OCR和公式密集版面存在性能权衡，提示未来需在复杂版面与特殊信号上进一步优化。

Abstract: GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.

</details>


### [12] [PAS-Mamba: Phase-Amplitude-Spatial State Space Model for MRI Reconstruction](https://arxiv.org/abs/2601.14530)
*Xiaoyan Kui,Zijie Fan,Zexin Ji,Qinsong Li,Hao Xu,Weixin Si,Haodong Xu,Beiji Zou*

Main category: cs.CV

TL;DR: 提出PAS-Mamba：在MRI重建中将频域幅度与相位解耦建模，并与图像域特征协同，通过环形频域扫描与双域融合实现更优重建，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有联合时空/频域方法将频域整体处理，忽视幅度与相位承载信息差异；统一建模会互相干扰，限制重建质量。

Method: 1) 图像域：采用LocalMamba保持局部空间依赖，增强细节；2) 频域：将幅度与相位解耦为两条专门分支，避免表示耦合；3) 提出环形频域扫描（CFDS），按同心环从低到高频序列化特征；4) 设计双域互补融合模块（DDCFM），自适应融合幅度/相位并实现频域与图像域双向信息交换；整体形成PAS-Mamba框架。

Result: 在IXI与fastMRI膝关节数据集上广泛实验，PAS-Mamba在重建质量上持续优于现有SOTA方法（具体指标未给出）。

Conclusion: 区分频域相位与幅度并与图像域互补融合，可减少表示干扰、提升结构与细节恢复；PAS-Mamba验证了该解耦与双域融合策略的有效性并取得SOTA表现。

Abstract: Joint feature modeling in both the spatial and frequency domains has become a mainstream approach in MRI reconstruction. However, existing methods generally treat the frequency domain as a whole, neglecting the differences in the information carried by its internal components. According to Fourier transform theory, phase and amplitude represent different types of information in the image. Our spectrum swapping experiments show that magnitude mainly reflects pixel-level intensity, while phase predominantly governs image structure. To prevent interference between phase and magnitude feature learning caused by unified frequency-domain modeling, we propose the Phase-Amplitude-Spatial State Space Model (PAS-Mamba) for MRI Reconstruction, a framework that decouples phase and magnitude modeling in the frequency domain and combines it with image-domain features for better reconstruction. In the image domain, LocalMamba preserves spatial locality to sharpen fine anatomical details. In frequency domain, we disentangle amplitude and phase into two specialized branches to avoid representational coupling. To respect the concentric geometry of frequency information, we propose Circular Frequency Domain Scanning (CFDS) to serialize features from low to high frequencies. Finally, a Dual-Domain Complementary Fusion Module (DDCFM) adaptively fuses amplitude phase representations and enables bidirectional exchange between frequency and image domains, delivering superior reconstruction. Extensive experiments on the IXI and fastMRI knee datasets show that PAS-Mamba consistently outperforms state of the art reconstruction methods.

</details>


### [13] [Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency](https://arxiv.org/abs/2601.14563)
*Thanh-Huy Nguyen,Hoang-Loc Cao,Dat T. Chung,Mai-Anh Vu,Thanh-Minh Nguyen,Minh Le,Phat K. Huynh,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出SDT-Net：在涂鸦监督的医学分割中，用双教师单学生、动态切换教师并结合像素挑选与层级一致性来提升伪标签质量与特征对齐，获得SOTA。


<details>
  <summary>Details</summary>
Motivation: 涂鸦（scribble）标注极其稀疏，导致伪标签传播含噪、边界学习不稳，限制弱监督分割性能。需要在低成本标注下最大化监督质量、减少歧义并获得更可靠的解剖边界。

Method: 构建双教师—单学生框架SDT-Net：1) 动态教师切换DTS，自适应选择当前更可靠的教师；2) 被选教师通过两路协同监督学生：a) 高置信伪标签监督，并用“挑选可靠像素”(PRP)筛除不确定像素；b) 分层特征对齐的一致性约束(HiCo)，在多层特征空间 enforcing 一致性，稳固结构表征。

Result: 在ACDC与MSCMRseg数据集上取得SOTA，分割更准确且形态更符合解剖学；相较现有涂鸦监督方法有显著提升。

Conclusion: 动态选择更可靠教师并结合可靠像素筛选与层级一致性，可在稀疏弱监督下显著提升伪标签品质与边界学习，最终实现更准确且解剖合理的医学图像分割。

Abstract: Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.

</details>


### [14] [Breaking the accuracy-resource dilemma: a lightweight adaptive video inference enhancement](https://arxiv.org/abs/2601.14568)
*Wei Ma,Shaowu Chen,Junjie Ye,Peichang Zhang,Lei Huang*

Main category: cs.CV

TL;DR: 提出一个基于模糊控制器的动态模型切换框架，利用相邻帧时空相关性，在视频推理中按设备实时资源自适应选择不同规模模型，实现性能与资源利用的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理增强方法多依赖更大模型和复杂架构，虽能提升精度但忽视资源效率与推理效果的权衡，导致算力浪费与实际推理性能不佳；需要一种能在不同资源条件下动态优化推理的机制。

Method: 设计基于关键系统参数与推理相关指标的模糊控制器（FC-r），在视频帧间利用目标的时空相关性，构建一个可根据设备实时资源状况在不同规模模型间动态切换的VI增强框架。

Result: 实验表明该方法能有效在资源利用与推理性能之间取得更佳平衡，相比固定大模型或单一架构方案更高效。

Conclusion: 通过FC-r引导的自适应模型切换与时空相关性利用，可在多变的资源环境中稳定提升视频推理的综合表现，兼顾效率与效果。

Abstract: Existing video inference (VI) enhancement methods typically aim to improve performance by scaling up model sizes and employing sophisticated network architectures. While these approaches demonstrated state-of-the-art performance, they often overlooked the trade-off of resource efficiency and inference effectiveness, leading to inefficient resource utilization and suboptimal inference performance. To address this problem, a fuzzy controller (FC-r) is developed based on key system parameters and inference-related metrics. Guided by the FC-r, a VI enhancement framework is proposed, where the spatiotemporal correlation of targets across adjacent video frames is leveraged. Given the real-time resource conditions of the target device, the framework can dynamically switch between models of varying scales during VI. Experimental results demonstrate that the proposed method effectively achieves a balance between resource utilization and inference performance.

</details>


### [15] [Anatomically Guided Latent Diffusion for Brain MRI Progression Modeling](https://arxiv.org/abs/2601.14584)
*Cheng Wan,Bahram Jafrasteh,Ehsan Adeli,Miaomiao Zhang,Qingyu Zhao*

Main category: cs.CV

TL;DR: 提出AG-LDM：一种由分割引导的潜变量扩散模型，在简化训练的同时保证解剖一致性，并在ADNI/OASIS-3上实现更高图像质量、体积误差降低15-20%、对时间与临床协变量更敏感，能生成符合阿尔茨海默病进程的反事实轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有纵向脑MRI进程建模（如BrLP）训练流程复杂、对临床协变量利用不足、且难以保证解剖一致性。需要一个既高效又能在结构层面保持一致性的模型，以更好预测个体化的脑结构变化并提升生物学合理性。

Method: 提出AG-LDM：在潜空间扩散中将基线解剖、噪声化的随访状态与临床协变量在输入层直接融合，避免多阶段/辅助控制网络，端到端学习解剖与进程；并引入轻量级3D组织分割模型WarpSeg，在自编码器微调与扩散训练阶段提供显式解剖监督，约束组织边界与形态学保真。

Result: 在31,713个ADNI纵向配对上训练，并在OASIS-3零样本评估；图像质量达SOTA，生成影像的体积误差降低15-20%；对时间与临床协变量的利用显著增强（对BrLP敏感度最高提升至31.5倍）；能生成生物学合理的反事实轨迹，准确再现阿尔茨海默病标志性变化（边缘系统萎缩、脑室扩张）。

Conclusion: AG-LDM以分割引导确保解剖一致性，同时简化训练与条件融合，较复杂扩散框架更高效、更可靠地建模脑MRI进程，具备良好泛化与临床协变量可控性，适用于神经退行性疾病的纵向预测与探索。

Abstract: Accurately modeling longitudinal brain MRI progression is crucial for understanding neurodegenerative diseases and predicting individualized structural changes. Existing state-of-the-art approaches, such as Brain Latent Progression (BrLP), often use multi-stage training pipelines with auxiliary conditioning modules but suffer from architectural complexity, suboptimal use of conditional clinical covariates, and limited guarantees of anatomical consistency. We propose Anatomically Guided Latent Diffusion Model (AG-LDM), a segmentation-guided framework that enforces anatomically consistent progression while substantially simplifying the training pipeline. AG-LDM conditions latent diffusion by directly fusing baseline anatomy, noisy follow-up states, and clinical covariates at the input level, a strategy that avoids auxiliary control networks by learning a unified, end-to-end model that represents both anatomy and progression. A lightweight 3D tissue segmentation model (WarpSeg) provides explicit anatomical supervision during both autoencoder fine-tuning and diffusion model training, ensuring consistent brain tissue boundaries and morphometric fidelity. Experiments on 31,713 ADNI longitudinal pairs and zero-shot evaluation on OASIS-3 demonstrate that AG-LDM matches or surpasses more complex diffusion models, achieving state-of-the-art image quality and 15-20\% reduction in volumetric errors in generated images. AG-LDM also exhibits markedly stronger utilization of temporal and clinical covariates (up to 31.5x higher sensitivity than BrLP) and generates biologically plausible counterfactual trajectories, accurately capturing hallmarks of Alzheimer's progression such as limbic atrophy and ventricular expansion. These results highlight AG-LDM as an efficient, anatomically grounded framework for reliable brain MRI progression modeling.

</details>


### [16] [From Volumes to Slices: Computationally Efficient Contrastive Learning for Sequential Abdominal CT Analysis](https://arxiv.org/abs/2601.14593)
*Po-Kai Chiu,Hung-Hsuan Chen*

Main category: cs.CV

TL;DR: 提出2D-VoCo：将3D体素级自监督VoCo简化为2D切片级对比学习，降低算力与显存开销，同时提升CT多器官损伤分类性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注昂贵稀缺，限制深度学习效果；现有3D自监督如VoCo虽有效但计算/内存成本高，难以在临床与常规硬件上落地。

Method: 把VoCo改造为2D切片级自监督：在未标注CT切片上进行对比学习以获取空间-语义表征；用预训练CNN作为骨干，接入CNN-LSTM以建模体轴序列信息，实现多器官损伤分类。

Result: 在RSNA 2023腹部创伤数据集上，2D-VoCo预训练相较从零训练显著提升mAP、Precision、Recall与RSNA分数；同时具备更低的计算与内存需求。

Conclusion: 2D-VoCo提供一种高效、可复现的切片级自监督预训练方案，在减少标注依赖的同时提升临床CT分析性能；代码已开源以促进应用与复现。

Abstract: The requirement for expert annotations limits the effectiveness of deep learning for medical image analysis. Although 3D self-supervised methods like volume contrast learning (VoCo) are powerful and partially address the labeling scarcity issue, their high computational cost and memory consumption are barriers. We propose 2D-VoCo, an efficient adaptation of the VoCo framework for slice-level self-supervised pre-training that learns spatial-semantic features from unlabeled 2D CT slices via contrastive learning. The pre-trained CNN backbone is then integrated into a CNN-LSTM architecture to classify multi-organ injuries. In the RSNA 2023 Abdominal Trauma dataset, 2D-VoCo pre-training significantly improves mAP, precision, recall, and RSNA score over training from scratch. Our framework provides a practical method to reduce the dependency on labeled data and enhance model performance in clinical CT analysis. We release the code for reproducibility. https://github.com/tkz05/2D-VoCo-CT-Classifier

</details>


### [17] [LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning](https://arxiv.org/abs/2601.14594)
*Lianying Chao,Linfeng Yin,Peiyu Ren,Yifan Jiang,Qiaoyu Ren,Dingcheng Shan,Jing-cheng Pang,Sijie Wu,Xubin Li,Kai Zhang*

Main category: cs.CV

TL;DR: 提出一个可学习的帧选择器（LFS），在不增加编码成本下，从视频中选出时间上多样且与事件相关的关键帧，用于提升视频字幕生成；并引入人类一致性基准ICH-CC。


<details>
  <summary>Details</summary>
Motivation: 均匀采样忽视视频中事件分布不均，既昂贵又可能遗漏关键信息；现有基准与人类对视频理解存在差距，需要更贴近人类认知的评测。

Method: 设计Learnable Frame Selector：显式建模时间重要性，在“事件相关性—时间多样性”间权衡；采用分层（stratified）策略保证时间覆盖、避免集中；利用冻结的视频LLM的字幕反馈作为学习信号，直接优化下游字幕质量；同时构建ICH-CC，通过人工精心设计问题衡量与人类一致的理解。

Result: 在两个社区基准与ICH-CC上，详细视频字幕任务稳定提升：VDC最高+2.0%，ICH-CC超过+4%；改进的字幕进一步提升视频问答性能。

Conclusion: LFS是一种易集成、成本友好的帧选择方案，能更好地捕捉关键事件并提升详细视频字幕效果；ICH-CC为更人类一致的评测提供了新基准。

Abstract: Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.

</details>


### [18] [3D Space as a Scratchpad for Editable Text-to-Image Generation](https://arxiv.org/abs/2601.14602)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Matheus Gadelha,Kevin Blackburn-Matzen*

Main category: cs.CV

TL;DR: 提出“空间草稿本（spatial scratchpad）”：在文本到图像生成中引入可编辑的3D中间表示（网格+场景规划+渲染回流），显著提升几何关系与身份一致性，GenAI-Bench文本对齐提升32%。


<details>
  <summary>Details</summary>
Motivation: LLM中的显式中间推理（如CoT、工具调用）显著增强推理，但VLM缺少对应的“空间推理”机制，导致生成图像在几何、身份、组合意图上不准确；现有2D布局方法难以进行可靠的三维编辑与一致传播。

Method: 给定文本：1) 解析主体与背景，实例化为可编辑3D网格；2) 通过“代理式”场景规划决策位置、朝向与视点；3) 渲染该3D布局回到图像域，提供保持身份的线索；4) 利用VLM据此生成最终图像。区别于2D布局，支持直观3D编辑并能稳定传导到最终结果。

Result: 在GenAI-Bench上文本对齐指标提升32%；生成效果在空间一致性、视觉连贯性与身份保持方面显著优于基线；展示了可靠的3D编辑到图像的传播。

Conclusion: 显式3D推理作为空间草稿本为VLM提供了语言—空间—图像的桥梁，带来更精确、可控的图像生成，提示VLM应在“语言+空间”双通道中思考。代码与可视化已开源。

Abstract: Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/

</details>


### [19] [U-Harmony: Enhancing Joint Training for Segmentation Models with Universal Harmonization](https://arxiv.org/abs/2601.14605)
*Weiwei Ma,Xiaobing Yu,Peijie Qiu,Jin Yang,Pan Xiao,Xiaoqi Zhao,Xiaofeng Liu,Tomo Miyazaki,Shinichiro Omachi,Yongsong Huang*

Main category: cs.CV

TL;DR: 提出U-Harmony联合训练方法，通过域门控头与“先归一化后去归一化”的特征对齐策略，在单一分割模型中同时学习多机构/多模态异质数据，兼顾泛化与域特异知识，并可无缝适配新模态与新解剖类别，在跨机构脑病灶数据上达新基准。


<details>
  <summary>Details</summary>
Motivation: 医疗分割数据跨机构常异质（模态、协议、解剖目标不同且样本少），现有模型难以在单模型中同时吸收多域知识，往往在泛化与域专长之间取舍，限制临床可用性与可扩展性。

Method: 在任意深度分割架构上加入域门控头，提出Universal Harmonization：对特征先进行域内归一化以削弱域差异，再进行去归一化以恢复并保留数据集特异信息；训练时联合多域数据，设计机制支持“通用模态适配”，可增量引入新模态与新解剖类别。

Result: 在跨机构脑病灶3D分割基准上进行大量实验，模型实现更强鲁棒性与适应性，优于现有方法并建立新的性能标杆。

Conclusion: U-Harmony能在单模型中统一学习异质医疗影像数据，同时保持泛化与域特异能力，并可顺畅扩展到新模态/新类别，适用于真实临床环境的稳健3D分割。

Abstract: In clinical practice, medical segmentation datasets are often limited and heterogeneous, with variations in modalities, protocols, and anatomical targets across institutions. Existing deep learning models struggle to jointly learn from such diverse data, often sacrificing either generalization or domain-specific knowledge. To overcome these challenges, we propose a joint training method called Universal Harmonization (U-Harmony), which can be integrated into deep learning-based architectures with a domain-gated head, enabling a single segmentation model to learn from heterogeneous datasets simultaneously. By integrating U-Harmony, our approach sequentially normalizes and then denormalizes feature distributions to mitigate domain-specific variations while preserving original dataset-specific knowledge. More appealingly, our framework also supports universal modality adaptation, allowing the seamless learning of new imaging modalities and anatomical classes. Extensive experiments on cross-institutional brain lesion datasets demonstrate the effectiveness of our approach, establishing a new benchmark for robust and adaptable 3D medical image segmentation models in real-world clinical settings.

</details>


### [20] [Learning Consistent Taxonomic Classification through Hierarchical Reasoning](https://arxiv.org/abs/2601.14610)
*Zhenghong Li,Kecheng Zheng,Haibin Ling*

Main category: cs.CV

TL;DR: 提出VL-Taxon两阶段层级推理框架，显著提升VLM在生物分类任务的叶节点准确率与层级一致性，在iNaturalist-2021上小数据微调即超越更大模型10%+。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型对层级知识理解薄弱，常出现叶级别预测正确但上层类别错误的问题；既有方法未建模层级推理，导致层级一致性差。

Method: 两阶段层级推理：阶段一自顶向下进行叶级分类以提升叶级准确；阶段二以上述叶级结果为锚点，向上约束并校正整条分类层级的一致性。每阶段先监督微调注入分类学知识，再用强化学习提升推理与泛化。基于Qwen2.5-VL-7B实现。

Result: 在iNaturalist-2021上，相比原始Qwen2.5-VL-72B，VL-Taxon（7B）平均在叶级准确率与层级一致性准确率上提升10%以上。

Conclusion: 显式建模层级推理并分阶段训练可在小数据下显著提升VLM的层级一致性与末端识别性能，甚至以更小模型超越更大基线，无需借助其他VLM生成样本。

Abstract: While Vision-Language Models (VLMs) excel at visual understanding, they often fail to grasp hierarchical knowledge. This leads to common errors where VLMs misclassify coarser taxonomic levels even when correctly identifying the most specific level (leaf level). Existing approaches largely overlook this issue by failing to model hierarchical reasoning. To address this gap, we propose VL-Taxon, a two-stage, hierarchy-based reasoning framework designed to improve both leaf-level accuracy and hierarchical consistency in taxonomic classification. The first stage employs a top-down process to enhance leaf-level classification accuracy. The second stage then leverages this accurate leaf-level output to ensure consistency throughout the entire taxonomic hierarchy. Each stage is initially trained with supervised fine-tuning to instill taxonomy knowledge, followed by reinforcement learning to refine the model's reasoning and generalization capabilities. Extensive experiments reveal a remarkable result: our VL-Taxon framework, implemented on the Qwen2.5-VL-7B model, outperforms its original 72B counterpart by over 10% in both leaf-level and hierarchical consistency accuracy on average on the iNaturalist-2021 dataset. Notably, this significant gain was achieved by fine-tuning on just a small subset of data, without relying on any examples generated by other VLMs.

</details>


### [21] [Diffusion Epistemic Uncertainty with Asymmetric Learning for Diffusion-Generated Image Detection](https://arxiv.org/abs/2601.14625)
*Yingsong Huang,Hui Guo,Jing Huang,Bing Bai,Qi Xiong*

Main category: cs.CV

TL;DR: 提出DEUA框架：用拉普拉斯近似估计扩散模型的认识不确定性(DEU)以区分生成图像，并配合不对称损失获得更稳健的一般化；在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有扩散图像检测器常用重构误差等扩散度量提升泛化，但未区分两类不确定性：偶然(aleatoric)与认识(epistemic)。前者来自数据固有噪声，会增加歧义并损害检测；后者反映模型对陌生模式的无知，有助于识别生成图像。需要一种显式估计认识不确定性并抑制偶然不确定性影响的方法。

Method: 提出DEUA框架：(1) 基于拉普拉斯近似的Diffusion Epistemic Uncertainty(DEU)估计，衡量样本与扩散生成样本流形的接近度，从而提取对“生成性”更敏感的信号；(2) 设计不对称损失，扩大分类边界并平衡真伪样本学习，提升跨分布泛化能力。

Result: 在大规模基准上进行广泛实验，DEUA在检测扩散生成图像任务上取得了SOTA性能，显示更好的泛化与鲁棒性。

Conclusion: 区分并显式建模认识不确定性是提升扩散生成图像检测的关键。通过拉普拉斯近似估计DEU并结合不对称学习，DEUA显著提升跨数据集与模型的检测效果。

Abstract: The rapid progress of diffusion models highlights the growing need for detecting generated images. Previous research demonstrates that incorporating diffusion-based measurements, such as reconstruction error, can enhance the generalizability of detectors. However, ignoring the differing impacts of aleatoric and epistemic uncertainty on reconstruction error can undermine detection performance. Aleatoric uncertainty, arising from inherent data noise, creates ambiguity that impedes accurate detection of generated images. As it reflects random variations within the data (e.g., noise in natural textures), it does not help distinguish generated images. In contrast, epistemic uncertainty, which represents the model's lack of knowledge about unfamiliar patterns, supports detection. In this paper, we propose a novel framework, Diffusion Epistemic Uncertainty with Asymmetric Learning~(DEUA), for detecting diffusion-generated images. We introduce Diffusion Epistemic Uncertainty~(DEU) estimation via the Laplace approximation to assess the proximity of data to the manifold of diffusion-generated samples. Additionally, an asymmetric loss function is introduced to train a balanced classifier with larger margins, further enhancing generalizability. Extensive experiments on large-scale benchmarks validate the state-of-the-art performance of our method.

</details>


### [22] [Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis](https://arxiv.org/abs/2601.14637)
*James Brock,Ce Zhang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 提出Forest-Chat：一个结合多层级视觉-语言骨干与LLM编排的交互式森林变化解读系统，并发布Forest-Change数据集，在森林遥感场景实现检测、描述、计数与推理等任务，实验在Forest-Change与LEVIR-MCI-Trees上表现强。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感与深度学习进步使森林监测更可行，但仍面临像素级变化检测与语义级变化解释两大难题。现有VLM/LLM在遥感变化解释，尤其非城市场景（森林），研究不足，需要能自然语言交互且可执行多任务的系统。

Method: 构建Forest-Chat框架：以多层级变化解释（MCI）视觉-语言骨干为基础，由LLM进行任务编排；融合基础变化检测模型实现零样本检测；提供交互式点提示以细粒度用户引导；支持多任务（变化检测、变化字幕生成、目标计数、毁林比例估计、变化推理）。同时发布Forest-Change数据集，包含双时相影像、像素级掩膜与多粒度语义变化描述（人标与规则生成结合）。

Result: 在Forest-Change与LEVIR-MCI-Trees数据上实现联合变化检测与字幕生成的强性能，展示出系统在森林场景的有效性与泛化潜力。

Conclusion: LLM驱动、可交互的RSICI系统能够提升森林变化分析的可达性、可解释性与效率；Forest-Chat与Forest-Change为该方向提供了方法与基准。

Abstract: The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.

</details>


### [23] [READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection](https://arxiv.org/abs/2601.14651)
*Chenglizhao Chen,Boze Li,Mengke Song,Dehao Feng,Xinyu Liu,Shanchen Pang,Jufeng Yang,Hui Yu*

Main category: cs.CV

TL;DR: 提出READ-Net，通过自适应特征重标定（AFR）缓解情绪歧义，使情绪特征中与抑郁相关的信号被强化、无关干扰被抑制，从而提升音视频抑郁检测；在3个数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有音视频抑郁检测要么忽视情绪线索而错过细微抑郁信号，要么引入情绪后将短暂情绪与稳定抑郁症状混淆（情绪歧义），导致误检，需要一种能区分并利用与抑郁相关情绪信息的方法。

Method: 提出READ-Net框架，核心为自适应特征重标定（AFR）：对情绪相关特征进行动态加权，识别并保留与抑郁相关的情绪线索，同时抑制无关情绪噪声；该模块可无缝集成至现有音视频框架以提升表示清晰度。

Result: 在三个公开数据集上取得平均+4.55%准确率、+1.26% F1 的提升，超过当前最先进方法，表现出对情绪干扰的鲁棒性。

Conclusion: 通过AFR解决情绪歧义问题，READ-Net能有效从情绪表达中提取抑郁相关信息并过滤干扰，显著提升音视频抑郁检测性能，且具有良好可集成性与泛化性。

Abstract: Depression is a severe global mental health issue that impairs daily functioning and overall quality of life. Although recent audio-visual approaches have improved automatic depression detection, methods that ignore emotional cues often fail to capture subtle depressive signals hidden within emotional expressions. Conversely, those incorporating emotions frequently confuse transient emotional expressions with stable depressive symptoms in feature representations, a phenomenon termed \emph{Emotional Ambiguity}, thereby leading to detection errors. To address this critical issue, we propose READ-Net, the first audio-visual depression detection framework explicitly designed to resolve Emotional Ambiguity through Adaptive Feature Recalibration (AFR). The core insight of AFR is to dynamically adjust the weights of emotional features to enhance depression-related signals. Rather than merely overlooking or naively combining emotional information, READ-Net innovatively identifies and preserves depressive-relevant cues within emotional features, while adaptively filtering out irrelevant emotional noise. This recalibration strategy significantly clarifies feature representations, and effectively mitigates the persistent challenge of emotional interference. Additionally, READ-Net can be easily integrated into existing frameworks for improved performance. Extensive evaluations on three publicly available datasets show that READ-Net outperforms state-of-the-art methods, with average gains of 4.55\% in accuracy and 1.26\% in F1-score, demonstrating its robustness to emotional disturbances and improving audio-visual depression detection.

</details>


### [24] [Mirai: Autoregressive Visual Generation Needs Foresight](https://arxiv.org/abs/2601.14671)
*Yonghao Yu,Lang Huang,Zerun Wang,Runyi Li,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 论文提出在自回归（AR）视觉生成训练中引入“前视”信号，用未来token的信息指导当前步，从而更快收敛并提升全局一致性。提出通用框架Mirai：Mirai-E注入来自多未来位置的显式前视；Mirai-I利用与双向表示对齐的隐式前视。无需改模型、推理无开销，实验证明收敛可加速至10倍，FID显著下降。


<details>
  <summary>Details</summary>
Motivation: AR图像生成将图像离散化为token并按因果方式训练，但每步仅由下一个token监督，导致长程依赖与全局一致性不足、收敛慢。作者探究：若将来自后续token的训练信号（前视）与AR内部2D网格表征对齐，能否改进因果建模与收敛？

Method: 提出Mirai框架，在训练期向AR模型注入与二维图像网格对齐的“未来信息”，不改架构也不增加推理耗时。两条实现：1）Mirai-E（Explicit）：从多个未来位置的单向表示显式注入前视信号；2）Mirai-I（Implicit）：通过与匹配的双向表示对齐，提供隐式前视信号。系统地在注入层级、布局与来源三个轴上做对照实验，找出最佳对齐策略。

Result: 在ImageNet类条件图像生成等基准上，Mirai显著提升质量并加速训练：以LlamaGen-B为例，收敛速度最高提升10倍，FID从5.34降至4.34。总体显示Mirai在不同设置下均稳健改进全局一致性与样本质量。

Conclusion: AR视觉生成受限于严格因果监督，缺乏对远期目标的优化信号。将与2D网格对齐的前视注入训练，能提升因果建模、加速收敛并提高生成质量。Mirai提供了无架构改动、无推理开销的通用做法，表明视觉自回归模型“需要前视”。

Abstract: Autoregressive (AR) visual generators model images as sequences of discrete tokens and are trained with next token likelihood. This strict causality supervision optimizes each step only by its immediate next token, which diminishes global coherence and slows convergence. We ask whether foresight, training signals that originate from later tokens, can help AR visual generation. We conduct a series of controlled diagnostics along the injection level, foresight layout, and foresight source axes, unveiling a key insight: aligning foresight to AR models' internal representation on the 2D image grids improves causality modeling. We formulate this insight with Mirai (meaning "future" in Japanese), a general framework that injects future information into AR training with no architecture change and no extra inference overhead: Mirai-E uses explicit foresight from multiple future positions of unidirectional representations, whereas Mirai-I leverages implicit foresight from matched bidirectional representations. Extensive experiments show that Mirai significantly accelerates convergence and improves generation quality. For instance, Mirai can speed up LlamaGen-B's convergence by up to 10$\times$ and reduce the generation FID from 5.34 to 4.34 on the ImageNet class-condition image generation benchmark. Our study highlights that visual autoregressive models need foresight.

</details>


### [25] [LaVR: Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models](https://arxiv.org/abs/2601.14674)
*Mingyang Xie,Numair Khan,Tianfu Wang,Naina Dhingra,Seonghyeon Nam,Haitao Yang,Zhuo Hui,Christopher Metzler,Andrea Vedaldi,Hamed Pirsiavash,Lei Luo*

Main category: cs.CV

TL;DR: 提出一种利用大型4D重建模型潜在空间作为隐式几何条件的单目视频重渲染方法，在无需显式重建的前提下提升新视角生成的稳定性与精度，并在基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法两难：不带几何条件的生成模型在视角变化下易漂移和形变；带几何条件的方法依赖深度/显式重建，对深度误差与标定噪声敏感。需要一种既具空间感知又能避免显式几何脆弱性的条件化生成方案。

Method: 利用大型4D重建模型的潜在表示作为隐式几何先验，这些latent连续地编码场景结构而无需显式网格/体素重建；在扩散视频生成中联合条件化于该latent与源相机位姿，让预训练扩散先验对误差进行正则；从单目视频输入，预测/提取4D latent，再沿新相机轨迹生成重渲染视频。

Result: 在视频重渲染任务上取得SOTA表现，相比基线更稳健，减少漂移与形变，对深度/标定误差不敏感；定性和定量评估均优于现有方法（细节未给出）。

Conclusion: 隐式几何latent作为条件可在不依赖显式重建的情况下提供空间约束，结合位姿联合条件化能显著提升新视角视频生成的稳定性与逼真度；方法通用且可扩展，项目页提供资源。

Abstract: Given a monocular video, the goal of video re-rendering is to generate views of the scene from a novel camera trajectory. Existing methods face two distinct challenges. Geometrically unconditioned models lack spatial awareness, leading to drift and deformation under viewpoint changes. On the other hand, geometrically-conditioned models depend on estimated depth and explicit reconstruction, making them susceptible to depth inaccuracies and calibration errors.
  We propose to address these challenges by using the implicit geometric knowledge embedded in the latent space of a large 4D reconstruction model to condition the video generation process. These latents capture scene structure in a continuous space without explicit reconstruction. Therefore, they provide a flexible representation that allows the pretrained diffusion prior to regularize errors more effectively. By jointly conditioning on these latents and source camera poses, we demonstrate that our model achieves state-of-the-art results on the video re-rendering task. Project webpage is https://lavr-4d-scene-rerender.github.io/

</details>


### [26] [A comprehensive overview of deep learning models for object detection from videos/images](https://arxiv.org/abs/2601.14677)
*Sukana Zulfqar,Sadia Saeed,M. Azam Zia,Anjum Ali,Faisal Mehmood,Abid Ali*

Main category: cs.CV

TL;DR: 该综述回顾了用于视频与图像监控的目标检测最新进展，按核心架构、数据处理与监控场景挑战分类，评估CNN、GAN与时序融合方法的有效性，并指出低时延与时空高效学习等未来趋势。


<details>
  <summary>Details</summary>
Motivation: 监控场景目标检测虽成熟但仍受动态环境、遮挡、光照变化与实时性等挑战；深度学习快速演进，需要系统梳理架构创新、生成模型融入与时序信息利用的现状与效果，为研究与应用提供参考。

Method: 以综述方式归类与比较：1) 架构维度（CNN检测器等）；2) 数据处理策略（预处理、特征提取、时序融合）；3) 监控特定挑战与对应技术；并纳入GAN等生成模型在补帧、去遮挡与光照归一化中的作用，结合数据集与基准对比评估。

Result: 总结了CNN检测器、GAN辅助与时序融合技术在鲁棒性与精度上的提升路径；明确预处理与特征提取的演进，给出各类数据集与基准下的对比见解；证实生成模型能缓解缺帧、遮挡与光照不稳等问题；梳理在低时延与高效推理方面的进展与差距。

Conclusion: 语义目标检测在监控中已取得显著进展，融合生成模型与时空特征可有效提升表现；未来研究应聚焦低延迟高效架构、端到端时空学习与更贴近真实场景的评测基准。

Abstract: Object detection in video and image surveillance is a well-established yet rapidly evolving task, strongly influenced by recent deep learning advancements. This review summarises modern techniques by examining architectural innovations, generative model integration, and the use of temporal information to enhance robustness and accuracy. Unlike earlier surveys, it classifies methods based on core architectures, data processing strategies, and surveillance specific challenges such as dynamic environments, occlusions, lighting variations, and real-time requirements. The primary goal is to evaluate the current effectiveness of semantic object detection, while secondary aims include analysing deep learning models and their practical applications. The review covers CNN-based detectors, GAN-assisted approaches, and temporal fusion methods, highlighting how generative models support tasks such as reconstructing missing frames, reducing occlusions, and normalising illumination. It also outlines preprocessing pipelines, feature extraction progress, benchmarking datasets, and comparative evaluations. Finally, emerging trends in low-latency, efficient, and spatiotemporal learning approaches are identified for future research.

</details>


### [27] [Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation](https://arxiv.org/abs/2601.14678)
*Justin Cheung,Samuel Savine,Calvin Nguyen,Lin Lu,Alhassan S. Yasin*

Main category: cs.CV

TL;DR: 研究评估在腺癌跨器官（肺、结肠、乳腺、肾）场景中的跨域分类：单域ResNet50在本域>98%但对其他域几乎不泛化；改造成DANN并用无标签目标域进行适配，显著提升跨域性能（如用有标签乳腺+结肠适配无标签肺达95.56%）。染色归一化对不同目标域影响不一：肺大降（95.56%→66.60%），乳腺与结肠显著提升。解释性分析（Integrated Gradients）显示模型关注生物学有意义区域（致密细胞核）。


<details>
  <summary>Details</summary>
Motivation: 医学病理图像标注稀缺且跨机构/器官存在显著域偏移，传统监督深度学习在训练分布外表现崩溃。腺癌跨器官存在形态学共性，但直接迁移仍困难，需要方法在保留判别能力的同时对齐域差异，以提升无标签目标域性能并确保临床相关性。

Method: 以ResNet50为基线，先在单域监督训练并评估跨域泛化与模型集成效果；再将ResNet50改造为域对抗神经网络（DANN），在源域有标签、目标域无标签设置下进行对抗式域对齐；系统评估染色归一化对不同目标域的影响；用Integrated Gradients进行归因以检验模型是否学习到生物学相关特征。

Result: 单域ResNet50在本域>98%准确率，但对其他腺癌域泛化极差；模型集成无济于事。DANN在无标签目标域显著提升准确率：例如在乳腺+结肠为源、肺为目标时达95.56%。染色归一化效果依目标域而异：肺从95.56%降至66.60%，而乳腺49.22%→81.29%，结肠78.48%→83.36%。IG显示模型关注致密细胞核等生物学意义区域。

Conclusion: 单纯监督学习难以跨器官泛化；域对抗适配能有效缓解域偏移并在无标签目标域获得高性能，但预处理（如染色归一化）并非通用增益，需按目标域定制。可解释性证据表明DANN学到的表征具有临床意义。

Abstract: Supervised deep learning models often achieve excellent performance within their training distribution but struggle to generalize beyond it. In cancer histopathology, for example, a convolutional neural network (CNN) may classify cancer severity accurately for cancer types represented in its training data, yet fail on related but unseen types. Although adenocarcinomas from different organs share morphological features that might support limited cross-domain generalization, addressing domain shift directly is necessary for robust performance. Domain adaptation offers a way to transfer knowledge from labeled data in one cancer type to unlabeled data in another, helping mitigate the scarcity of annotated medical images.
  This work evaluates cross-domain classification performance among lung, colon, breast, and kidney adenocarcinomas. A ResNet50 trained on any single adenocarcinoma achieves over 98% accuracy on its own domain but shows minimal generalization to others. Ensembling multiple supervised models does not resolve this limitation. In contrast, converting the ResNet50 into a domain adversarial neural network (DANN) substantially improves performance on unlabeled target domains. A DANN trained on labeled breast and colon data and adapted to unlabeled lung data reaches 95.56% accuracy.
  We also examine the impact of stain normalization on domain adaptation. Its effects vary by target domain: for lung, accuracy drops from 95.56% to 66.60%, while for breast and colon targets, stain normalization boosts accuracy from 49.22% to 81.29% and from 78.48% to 83.36%, respectively. Finally, using Integrated Gradients reveals that DANNs consistently attribute importance to biologically meaningful regions such as densely packed nuclei, indicating that the model learns clinically relevant features and can apply them to unlabeled cancer types.

</details>


### [28] [FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection](https://arxiv.org/abs/2601.14690)
*Yian Huang,Qing Qin,Aji Mao,Xiangyu Qiu,Liang Xu,Xian Zhang,Zhenming Peng*

Main category: cs.CV

TL;DR: 提出FeedbackSTS-Det：一种基于稀疏帧的时空语义反馈网络，通过闭环语义关联与结构化稀疏时序建模，在复杂背景下实现高效稳健的红外小目标检测。


<details>
  <summary>Details</summary>
Motivation: 红外小目标在复杂背景下信杂比低、干扰强、外观特征弱；多帧方法虽利用时间信息但难以高效建模长程依赖且鲁棒性不足，需要一种既能捕获长时依赖又能抑制虚警、并保持训练—推理一致性的方案。

Method: 构建FeedbackSTS-Det：在编码器-解码器间设计“前向/后向”成对细化模块，形成闭环的时空语义反馈；两模块内嵌稀疏语义模块（SSM），进行结构化稀疏的时间建模，以低计算量捕获长程依赖，并实现隐式帧间配准与连续语义细化；整个流程保持训练与推理的一致性。

Result: 在多个基准数据集上进行大量实验，显示所提方法在检测性能与鲁棒性上均优于现有方法，并有效降低虚警。

Conclusion: 闭环时空语义反馈结合结构化稀疏时序建模能在低成本下实现长程依赖捕获与鲁棒检测，适用于复杂背景下的红外小目标检测；公开代码与模型便于复现与推广。

Abstract: Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.

</details>


### [29] [RegFreeNet: A Registration-Free Network for CBCT-based 3D Dental Implant Planning](https://arxiv.org/abs/2601.14703)
*Xinquan Yang,Xuguang Li,Mianjie Zheng,Xuefen Liu,Kun Tang,Kian Ming Lim,He Meng,Jianfeng Ren,Linlin Shen*

Main category: cs.CV

TL;DR: 提出通过在术后CBCT中掩蔽种植体、免注册地获取种植位置信息，构建大规模多中心数据集，并结合坡度感知网络实现SOTA的种植体位点预测。


<details>
  <summary>Details</summary>
Motivation: 商业导板软件无法导出术前种植位置信息，现有方法需配准术后与术前CBCT获取标签，流程耗时且依赖配准精度；许多医院无成对CBCT，限制多中心数据构建。临床上牙医可依邻牙纹理判断种植位点，启发掩蔽种植体也不影响定位。

Method: 1) 掩蔽范式：在含种植体的术后CBCT中将种植体区域掩蔽，使任何含种植体的CBCT都可作为训练数据，无需术前-术后配准。2) 数据集：发布ImplantFairy，包含1622例CBCT，体素级3D标注。3) 模型RegFreeNet：基于牙齿空间结构的区域变化与种植体坡度信息；设计邻近距离感知（NDP）模块自适应提取牙区变化特征；增加种植体坡度预测分支提供额外监督，提升鲁棒性。

Result: 在ImplantFairy与两个公开数据集上进行大量实验，RegFreeNet在种植体位置预测任务上达到最新最优表现（SOTA）。

Conclusion: 掩蔽术后CBCT替代配准流程，显著简化标注与数据构建，支持大规模多中心数据集；结合坡度感知与邻近距离特征的RegFreeNet能有效提升种植体位点预测精度并具备良好泛化。

Abstract: As the commercial surgical guide design software usually does not support the export of implant position for pre-implantation data, existing methods have to scan the post-implantation data and map the implant to pre-implantation space to get the label of implant position for training. Such a process is time-consuming and heavily relies on the accuracy of registration algorithm. Moreover, not all hospitals have paired CBCT data, limitting the construction of multi-center dataset. Inspired by the way dentists determine the implant position based on the neighboring tooth texture, we found that even if the implant area is masked, it will not affect the determination of the implant position. Therefore, we propose to mask the implants in the post-implantation data so that any CBCT containing the implants can be used as training data. This paradigm enables us to discard the registration process and makes it possible to construct a large-scale multi-center implant dataset. On this basis, we proposes ImplantFairy, a comprehensive, publicly accessible dental implant dataset with voxel-level 3D annotations of 1622 CBCT data. Furthermore, according to the area variation characteristics of the tooth's spatial structure and the slope information of the implant, we designed a slope-aware implant position prediction network. Specifically, a neighboring distance perception (NDP) module is designed to adaptively extract tooth area variation features, and an implant slope prediction branch assists the network in learning more robust features through additional implant supervision information. Extensive experiments conducted on ImplantFairy and two public dataset demonstrate that the proposed RegFreeNet achieves the state-of-the-art performance.

</details>


### [30] [LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval](https://arxiv.org/abs/2601.14706)
*Chao Gao,Siqiao Xue,Yimin Peng,Jiwen Fu,Tingyi Gu,Shanshan Li,Fan Zhou*

Main category: cs.CV

TL;DR: LookBench 是一个面向真实电商场景的时尚图像检索基准，含真人商品图与AI生成图，覆盖单品与整套穿搭检索，按时间戳更新，提供公开数据、评测和模型；其难度高，强基线在R@1多低于60%，作者专有模型最佳，开源模型次之并在旧基准同样SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有时尚检索基准陈旧、易被数据污染、与真实电商需求（找同款/相似款/风格一致）脱节，缺少随时间演进和更贴近购物行为的评测；同时新出现的AI生成穿搭图在实际应用中增多，需要被纳入评测。

Method: 构建名为 LookBench 的活体基准：从在线电商抓取最新商品图并加入AI生成时尚图，使用细粒度属性分类体系标注，覆盖单品级与穿搭级检索；为每个样本打时间戳，按声明的训练截止时间进行“污染感知”评测；设定Recall@1等指标，建立公开排行榜、评测代码与训练模型，并计划半年度滚动更新与加难任务。

Result: 在 LookBench 上，多数强大基线模型的 Recall@1 低于60%，显示任务具有显著挑战性；作者的专有模型排名第一，开源对标模型排名第二；二者在旧的 Fashion200K 基准上也达到SOTA。

Conclusion: LookBench 提供了一个与真实电商更贴近、可持续更新且污染感知的时尚图像检索评测平台，能够更可靠地衡量进展；计划持续半年度更新并发布资源，推动该领域方法在更难、更现实的设置上提升。

Abstract: In this paper, we present LookBench (We use the term "look" to reflect retrieval that mirrors how people shop -- finding the exact item, a close substitute, or a visually consistent alternative.), a live, holistic and challenging benchmark for fashion image retrieval in real e-commerce settings. LookBench includes both recent product images sourced from live websites and AI-generated fashion images, reflecting contemporary trends and use cases. Each test sample is time-stamped and we intend to update the benchmark periodically, enabling contamination-aware evaluation aligned with declared training cutoffs. Grounded in our fine-grained attribute taxonomy, LookBench covers single-item and outfit-level retrieval across. Our experiments reveal that LookBench poses a significant challenge on strong baselines, with many models achieving below $60\%$ Recall@1. Our proprietary model achieves the best performance on LookBench, and we release an open-source counterpart that ranks second, with both models attaining state-of-the-art results on legacy Fashion200K evaluations. LookBench is designed to be updated semi-annually with new test samples and progressively harder task variants, providing a durable measure of progress. We publicly release our leaderboard, dataset, evaluation code, and trained models.

</details>


### [31] [Context Patch Fusion With Class Token Enhancement for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2601.14718)
*Yiyang Fu,Hui Li,Wangyu Wu*

Main category: cs.CV

TL;DR: 提出CPF-CTE框架，通过上下文补丁融合与类标记增强提升弱监督语义分割精度。核心用CF-BiLSTM捕获补丁间空间依赖，并引入可学习类token以强化类判别语义，在VOC2012与COCO2014上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有仅用图像级标签的WSSS多依赖区分度增强与数据增广，易忽视图像补丁间复杂上下文关系，导致局部表征不完整、伪激活与分割精度受限。需要一种能显式建模空间上下文并融合语义线索的方法。

Method: 提出CPF-CTE：1）Contextual-Fusion BiLSTM（CF-BiLSTM）在补丁序列上进行双向信息流建模，捕获长程与双向空间依赖以融合上下文；2）引入可学习类token，动态编码并细化类别语义，增强可分辨性；3）将空间与语义线索有效融合以得到更丰富的特征表示。

Result: 在PASCAL VOC 2012与MS COCO 2014上进行大量实验，CPF-CTE在WSSS任务上持续超越现有方法（文中称“consistent surpass”，具体数值未在摘要给出）。

Conclusion: 通过CF-BiLSTM建模补丁上下文与类token增强语义，CPF-CTE能显著提升弱监督语义分割的表征质量与鲁棒性，并在主流基准上取得领先表现。

Abstract: Weakly Supervised Semantic Segmentation (WSSS), which relies only on image-level labels, has attracted significant attention for its cost-effectiveness and scalability. Existing methods mainly enhance inter-class distinctions and employ data augmentation to mitigate semantic ambiguity and reduce spurious activations. However, they often neglect the complex contextual dependencies among image patches, resulting in incomplete local representations and limited segmentation accuracy. To address these issues, we propose the Context Patch Fusion with Class Token Enhancement (CPF-CTE) framework, which exploits contextual relations among patches to enrich feature representations and improve segmentation. At its core, the Contextual-Fusion Bidirectional Long Short-Term Memory (CF-BiLSTM) module captures spatial dependencies between patches and enables bidirectional information flow, yielding a more comprehensive understanding of spatial correlations. This strengthens feature learning and segmentation robustness. Moreover, we introduce learnable class tokens that dynamically encode and refine class-specific semantics, enhancing discriminative capability. By effectively integrating spatial and semantic cues, CPF-CTE produces richer and more accurate representations of image content. Extensive experiments on PASCAL VOC 2012 and MS COCO 2014 validate that CPF-CTE consistently surpasses prior WSSS methods.

</details>


### [32] [HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding](https://arxiv.org/abs/2601.14724)
*Haowei Zhang,Shudong Yang,Jinlan Fu,See-Kiong Ng,Xipeng Qiu*

Main category: cs.CV

TL;DR: 提出 HERMES：在不改动权重的前提下，实现流式视频实时理解的训练免架构，通过紧凑KV缓存复用，兼顾稳定性能、实时响应与低显存。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在离线视频理解表现强，但在流式视频场景难以同时做到：稳定理解、实时响应（低TTFT）、以及低GPU显存与计算开销。

Method: 基于对注意力机制与KV缓存的机理分析，将KV缓存视为多粒度的分层记忆。推理时复用并维护一个紧凑的KV缓存来编码并保留关键视频信息，无需在用户查询到达时进行额外计算，从而实现流式交互的实时响应与资源高效。该方法为训练免（training-free）架构。

Result: 在与统一采样相比减少多达68%视频token的情况下，仍在各基准上达到相当或更优的准确率；在流式数据集上最高提升11.4%。相较之前SOTA，TTFT提升至10×更快。

Conclusion: HERMES通过将KV缓存作为分层记忆并在推理时复用紧凑缓存，实现了资源受限下的流式视频实时准确理解，兼顾速度、内存与精度，且无需额外训练与在线重计算。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.

</details>


### [33] [DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling](https://arxiv.org/abs/2601.14732)
*Jing Lan,Hexiao Ding,Hongzhao Chen,Yufeng Jiang,Nga-Chun Ng,Gwing Kei Yip,Gerald W. Y. Cheng,Yunlin Mao,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: cs.CV

TL;DR: DeepMoLM 是一个将高分辨率分子图像与由构象几何不变量派生的离散指纹相结合的双视角分子语言模型，实现对3D立体化学一致的生成与理解，并在分子图像描述与数值属性预测上优于通用基线、接近或匹配专用最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有分子语言模型多依赖字符串或图结构，难以直接对接连续的3D几何；视觉-语言模型又常忽略立体化学细节，且难把连续3D结构映射为离散符号，导致化学上不一致或数值输出无效。

Method: 提出 DeepMoLM：1) 保留1024×1024分子图像的高频证据；2) 将分子构象邻域编码为离散的扩展三维指纹（Extended 3D Fingerprints, E3DFP），作为几何不变量；3) 通过跨注意力融合视觉流与几何流；4) 在无原子坐标输出的前提下进行物理约束的生成。

Result: 在PubChem图像到文本任务上，较最强通用基线的METEOR相对提升12.3%，对所有属性查询均能生成有效数值；在专用设定下，分子量MAE为13.64 g/mol、Complexity MAE为37.89；在ChEBI-20图像描述任务上，超过通用基线并匹配SOTA视觉-语言模型。

Conclusion: 将高分辨率视觉证据与构象派生的几何不变量离散化并通过跨注意力融合，可在不显式输出坐标的情况下实现立体化学一致、可数值推断的分子语言建模，在多个基准上验证了有效性与竞争力。

Abstract: AI models for drug discovery and chemical literature mining must interpret molecular images and generate outputs consistent with 3D geometry and stereochemistry. Most molecular language models rely on strings or graphs, while vision-language models often miss stereochemical details and struggle to map continuous 3D structures into discrete tokens. We propose DeepMoLM: Deep Molecular Language M odeling, a dual-view framework that grounds high-resolution molecular images in geometric invariants derived from molecular conformations. DeepMoLM preserves high-frequency evidence from 1024 $\times$ 1024 inputs, encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints, and fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates. DeepMoLM improves PubChem captioning with a 12.3% relative METEOR gain over the strongest generalist baseline while staying competitive with specialist methods. It produces valid numeric outputs for all property queries and attains MAE 13.64 g/mol on Molecular Weight and 37.89 on Complexity in the specialist setting. On ChEBI-20 description generation from images, it exceeds generalist baselines and matches state-of-the-art vision-language models. Code is available at https://github.com/1anj/DeepMoLM.

</details>


### [34] [Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption](https://arxiv.org/abs/2601.14738)
*Liqin Wang,Qianyue Hu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: VoidFace是一种针对扩散模型人脸替换的系统性防御，向关键管道注入对抗扰动，级联破坏身份建模与生成，同时保持高视觉质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推动人脸互换普及，但带来隐私与身份安全风险。现有主动防御多来自通用图像编辑场景，忽视了人脸替换中的结构韧性与静态条件引导机制，导致防御无效。

Method: 将人脸替换视作“耦合的身份通道”，在关键瓶颈处注入扰动以触发级联破坏：1) 感知/定位阶段的“定位破坏”和“身份抹除”，削弱物理回归与语义嵌入，损坏源脸建模；2) 生成阶段“注意力解耦”切断身份注入，并破坏扩散中间特征，阻止源身份重建；3) 在潜空间进行对抗搜索，引入感知自适应策略，平衡攻击强度与图像质量，确保难以察觉。

Result: 在多种基于扩散的人脸替换模型上进行大量实验，VoidFace在攻击成功率/防护效果上优于现有防御，同时生成的对抗人脸具有更好的视觉质量。

Conclusion: 面向扩散式人脸替换的系统性防御应针对身份通道的多环节联合破坏，并结合潜空间的感知自适应优化，以在保持可视质量的同时显著削弱身份迁移。

Abstract: The rapid evolution of diffusion models has democratized face swapping but also raises concerns about privacy and identity security. Existing proactive defenses, often adapted from image editing attacks, prove ineffective in this context. We attribute this failure to an oversight of the structural resilience and the unique static conditional guidance mechanism inherent in face swapping systems. To address this, we propose VoidFace, a systemic defense method that views face swapping as a coupled identity pathway. By injecting perturbations at critical bottlenecks, VoidFace induces cascading disruption throughout the pipeline. Specifically, we first introduce localization disruption and identity erasure to degrade physical regression and semantic embeddings, thereby impairing the accurate modeling of the source face. We then intervene in the generative domain by decoupling attention mechanisms to sever identity injection, and corrupting intermediate diffusion features to prevent the reconstruction of source identity. To ensure visual imperceptibility, we perform adversarial search in the latent manifold, guided by a perceptual adaptive strategy to balance attack potency with image quality. Extensive experiments show that VoidFace outperforms existing defenses across various diffusion-based swapping models, while producing adversarial faces with superior visual quality.

</details>


### [35] [Enhancing Text-to-Image Generation via End-Edge Collaborative Hybrid Super-Resolution](https://arxiv.org/abs/2601.14741)
*Chongbin Yi,Yuxin Liang,Ziqi Zhou,Peng Yang*

Main category: cs.CV

TL;DR: 提出端-边协同的生成-增强框架：边侧自适应低分辨率生成后按区域将图像切块，前景用扩散式SR、背景用轻量SR，再拼接为高分辨率；在保持画质的同时将服务时延降33%。


<details>
  <summary>Details</summary>
Motivation: 高分T2I生成在边缘计算场景中受算力与时延限制：轻量SR快但丢细节，扩散式SR细节好但计算重，存在保真度-时延权衡，需在QoE与效率间取得平衡。

Method: 1) 边侧根据任务自适应选择去噪步数与SR倍数生成低分辨率图；2) 将图像分割为块并进行区域识别（前景/背景）；3) 对前景块用扩散式SR恢复细节，对背景块用轻量学习式SR高效放大；4) 拼接增强块得到高分辨率结果。

Result: 在实验中，相比基线方案，在保持竞争性图像质量（高保真）下，总体服务时延降低约33%。

Conclusion: 端-边协同与区域感知的混合SR策略能有效缓解高分辨率T2I的保真度-时延矛盾，兼顾画质与效率，适用于资源受限的边缘AIGC部署。

Abstract: Artificial Intelligence-Generated Content (AIGC) has made significant strides, with high-resolution text-to-image (T2I) generation becoming increasingly critical for improving users' Quality of Experience (QoE). Although resource-constrained edge computing adequately supports fast low-resolution T2I generations, achieving high-resolution output still faces the challenge of ensuring image fidelity at the cost of latency. To address this, we first investigate the performance of super-resolution (SR) methods for image enhancement, confirming a fundamental trade-off that lightweight learning-based SR struggles to recover fine details, while diffusion-based SR achieves higher fidelity at a substantial computational cost. Motivated by these observations, we propose an end-edge collaborative generation-enhancement framework. Upon receiving a T2I generation task, the system first generates a low-resolution image based on adaptively selected denoising steps and super-resolution scales at the edge side, which is then partitioned into patches and processed by a region-aware hybrid SR policy. This policy applies a diffusion-based SR model to foreground patches for detail recovery and a lightweight learning-based SR model to background patches for efficient upscaling, ultimately stitching the enhanced ones into the high-resolution image. Experiments show that our system reduces service latency by 33% compared with baselines while maintaining competitive image quality.

</details>


### [36] [SimD3: A Synthetic drone Dataset with Payload and Bird Distractor Modeling for Robust Detection](https://arxiv.org/abs/2601.14742)
*Ami Pandat,Kanyala Muvva,Punna Rajasekhar,Gopika Vinod,Rohit Shukla*

Main category: cs.CV

TL;DR: SimD3提出高保真大规模合成数据集与改进的YOLOv5m+C3b，显著提升小目标无人机在复杂场景中的检测与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实无人机检测标注数据稀缺、外观变化大且易与鸟类等相混淆，现有合成数据集对载荷、多样干扰物与复杂环境建模不足，限制了模型鲁棒性与泛化。

Method: 构建SimD3：在UE5中生成多环境、多天气/光照、可控航迹的高保真场景；显式建模异构载荷无人机与多种鸟类作为干扰，使用360六机位采集；在YOLOv5框架评测，并提出用C3b注意力模块替换C3瓶颈的Yolov5m+C3b。进行纯合成、合成+真实及多未见真实基准的广泛实验。

Result: SimD3为小目标无人机检测提供有效监督；改进的Yolov5m+C3b在域内与跨数据集评测中稳定优于YOLOv5基线，展现更强鲁棒性与泛化。

Conclusion: 高保真合成数据与结构改进的结合能显著提升无人机检测在多样复杂条件下的表现；SimD3适用于训练与基准评测，缓解真实数据不足并提升对干扰物的区分能力。

Abstract: Reliable drone detection is challenging due to limited annotated real-world data, large appearance variability, and the presence of visually similar distractors such as birds. To address these challenges, this paper introduces SimD3, a large-scale high-fidelity synthetic dataset designed for robust drone detection in complex aerial environments. Unlike existing synthetic drone datasets, SimD3 explicitly models drones with heterogeneous payloads, incorporates multiple bird species as realistic distractors, and leverages diverse Unreal Engine 5 environments with controlled weather, lighting, and flight trajectories captured using a 360 six-camera rig. Using SimD3, we conduct an extensive experimental evaluation within the YOLOv5 detection framework, including an attention-enhanced variant termed Yolov5m+C3b, where standard bottleneck-based C3 blocks are replaced with C3b modules. Models are evaluated on synthetic data, combined synthetic and real data, and multiple unseen real-world benchmarks to assess robustness and generalization. Experimental results show that SimD3 provides effective supervision for small-object drone detection and that Yolov5m+C3b consistently outperforms the baseline across in-domain and cross-dataset evaluations. These findings highlight the utility of SimD3 for training and benchmarking robust drone detection models under diverse and challenging conditions.

</details>


### [37] [ReinPath: A Multimodal Reinforcement Learning Approach for Pathology](https://arxiv.org/abs/2601.14757)
*Kangcheng Zhou,Jun Jiang,Qing Zhang,Shuang Zheng,Qingli Li,Shugong Xu*

Main category: cs.CV

TL;DR: 提出一种具备强推理能力的病理多模态大语言模型，通过语义奖励+GRPO训练，并构建高质量病理VQA数据集；在新数据集上显著超越SOTA，用20%数据也胜出，对零样本图像分类与CLIP相当。


<details>
  <summary>Details</summary>
Motivation: 现有病理多模态方法可解释性不足，原因在于缺乏支持显式推理/推断的高质量数据集，以及方法的推理过程过于简单，难以生成准确、上下文相关的文本解释。

Method: 1) 设计并训练一个病理多模态LLM；2) 引入语义奖励策略并与Group Relative Policy Optimization结合，提升文本描述的准确性与上下文相关性；3) 构建面向复杂推理的高质量病理VQA数据集，用于训练与评测。

Result: 在所构建的数据集上，所提方法优于现有SOTA，即便只用20%的训练数据也能领先；在下游零样本图像分类任务上表现与CLIP相当。

Conclusion: 通过语义奖励+GRPO的训练范式与高质量病理VQA数据集，可显著提升病理多模态模型的推理与可解释性，并在数据高效性与跨任务泛化上取得强竞争力。

Abstract: Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.

</details>


### [38] [Using Multi-Instance Learning to Identify Unique Polyps in Colon Capsule Endoscopy Images](https://arxiv.org/abs/2601.14771)
*Puneet Sharma,Kristian Dalsbø Hindberg,Eibe Frank,Benedicte Schelde-Olesen,Ulrik Deding*

Main category: cs.CV

TL;DR: 将“唯一性息肉识别”建模为多实例学习的验证问题，结合注意力（VEMA、DBA）与自监督SimCLR预训练，在CCE数据上取得86.26%准确率与0.928 AUC，显示MIL+自监督有助于自动化内镜图像分析。


<details>
  <summary>Details</summary>
Motivation: CCE产生海量图像，医生标注与逐帧比对负担重，且帧级别是否为同一息肉存在歧义；需要一种能在不精确帧标注下判定“查询息肉”是否在目标图像包中出现的算法，提高效率与一致性。

Method: 将任务表述为多实例学习中的多实例验证（MIV）：输入为查询图像与目标图像包，通过注意力机制聚合实例表征。提出/采用两类注意力——方差激励多头注意力（VEMA）与基于距离的注意力（DBA），以突出判别性特征；同时使用SimCLR进行自监督预训练以获得稳健嵌入；主干网络采用ConvNeXt。

Result: 在含1912个息肉、754名患者的数据集上实验：注意力机制显著提升性能，其中DBA（L1距离）+ConvNeXt+SimCLR在测试集达到86.26%准确率、AUC 0.928。

Conclusion: 将CCE息肉“唯一性判定”转化为MIL问题并结合注意力与自监督预训练能有效提升识别效果；方法具有推广潜力，可拓展至其他医学影像的实例级检索与去重/匹配场景。

Abstract: Identifying unique polyps in colon capsule endoscopy (CCE) images is a critical yet challenging task for medical personnel due to the large volume of images, the cognitive load it creates for clinicians, and the ambiguity in labeling specific frames. This paper formulates this problem as a multi-instance learning (MIL) task, where a query polyp image is compared with a target bag of images to determine uniqueness. We employ a multi-instance verification (MIV) framework that incorporates attention mechanisms, such as variance-excited multi-head attention (VEMA) and distance-based attention (DBA), to enhance the model's ability to extract meaningful representations. Additionally, we investigate the impact of self-supervised learning using SimCLR to generate robust embeddings. Experimental results on a dataset of 1912 polyps from 754 patients demonstrate that attention mechanisms significantly improve performance, with DBA L1 achieving the highest test accuracy of 86.26\% and a test AUC of 0.928 using a ConvNeXt backbone with SimCLR pretraining. This study underscores the potential of MIL and self-supervised learning in advancing automated analysis of Colon Capsule Endoscopy images, with implications for broader medical imaging applications.

</details>


### [39] [Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis](https://arxiv.org/abs/2601.14774)
*Keita Takeda,Tomoya Sakai*

Main category: cs.CV

TL;DR: 研究分析开源医疗视觉-语言模型（VLM）的特征表征，比较医疗与通用VLM在多模态病灶分类上的特征分布与偏置，发现强化文本编码往往比大量医疗图像训练更关键，同时警示通用模型易受图像中的叠加文本干扰。


<details>
  <summary>Details</summary>
Motivation: 现有评估（如分类准确率）无法揭示模型是否真正学到病灶级、可判别的医学特征。理解特征分布有助于洞察医学影像结构并改进下游任务与模型选择。

Method: 选取若干具有代表性的医疗VLM与非医疗VLM（含具上下文增强的如LLM2CLIP），在多种影像模态与病灶分类数据集上提取图像特征；分析与比较其特征分布与判别性，并考察医疗专化训练与文本编码增强的影响；同时评估对图像中叠加文本偏置的脆弱性。

Result: 医疗VLM能提取对医学分类有效的判别性特征；近期通用VLM（具上下文/文本编码增强，如LLM2CLIP）可产生更精细的特征表征；非医疗模型对图像叠加文本偏置尤其敏感。

Conclusion: 开发医疗VLM时，提升文本编码器比单纯增加医疗图像训练更为关键；模型选择需结合具体下游任务，并警惕图像中文本等背景偏置对推理的风险。

Abstract: This study investigates the feature representations produced by publicly available open source medical vision-language models (VLMs). While medical VLMs are expected to capture diagnostically relevant features, their learned representations remain underexplored, and standard evaluations like classification accuracy do not fully reveal if they acquire truly discriminative, lesion-specific features. Understanding these representations is crucial for revealing medical image structures and improving downstream tasks in medical image analysis. This study aims to investigate the feature distributions learned by medical VLMs and evaluate the impact of medical specialization. We analyze the feature distribution of multiple image modalities extracted by some representative medical VLMs across lesion classification datasets on multiple modalities. These distributions were compared them with non-medical VLMs to assess the domain-specific medical training. Our experiments showed that medical VLMs can extract discriminative features that are effective for medical classification tasks. Moreover, it was found that non-medical VLMs with recent improvement with contextual enrichment such as LLM2CLIP produce more refined feature representations. Our results imply that enhancing text encoder is more crucial than training intensively on medical images when developing medical VLMs. Notably, non-medical models are particularly vulnerable to biases introduced by overlaied text strings on images. These findings underscore the need for careful consideration on model selection according to downstream tasks besides potential risks in inference due to background biases such as textual information in images.

</details>


### [40] [M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention](https://arxiv.org/abs/2601.14776)
*Xiaofan Yang,Yubin Liu,Wei Pan,Guoqing Chu,Junming Zhang,Jie Zhao,Zhuoqi Man,Xuanming Cao*

Main category: cs.CV

TL;DR: 提出M2I2HA：以超图为核心的多模态目标检测网络，通过捕获模态内高阶关系与模态间精准对齐融合，并配合自适应多级融合模块，在多数据集上达成SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检测虽提升了低光、过曝等场景表现，但仍面临：1) 模态内/跨模态任务相关信息提取不足；2) 跨模态精确对齐困难。方法层面：CNN受限于局部感受野与强归纳偏置，Transformer计算复杂度高且多建模于两两相关性；SSM/Mamba顺序扫描打平2D为1D，破坏拓扑并难以表达高阶依赖。需要一种能高效建模多对多高阶关系、兼顾全局上下文与跨模态对齐的架构。

Method: 提出基于超图理论的多模态感知网络M2I2HA：1) Intra-Hypergraph Enhancement（模态内超图增强）模块，刻画全局多对多高阶关系，强化每一模态的任务相关特征；2) Inter-Hypergraph Fusion（模态间超图融合）模块，通过超图桥接不同数据源在配置与空间上的差异，实现对齐、增强与融合；3) M2-FullPAD模块，实现自适应的多层级特征融合，并优化网络中的数据分布与信息流动。整体用于RGB、热成像、深度等多模态目标检测。

Result: 在多个公开多模态目标检测数据集上，相比强基线取得一致提升并达到SOTA。摘要未给出具体数值，但强调广泛实验验证其有效性与鲁棒性。

Conclusion: 基于超图的M2I2HA有效解决多模态内外信息建模与跨模态对齐难题，通过高阶关系建模与自适应多级融合实现更优检测表现，优于CNN/Transformer/SSM等既有范式在该任务上的局限。

Abstract: Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.

</details>


### [41] [FunCineForge: A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes](https://arxiv.org/abs/2601.14777)
*Jiaxuan Liu,Yang Xiang,Han Zhao,Xiangang Li,Zhenhua Ling*

Main category: cs.CV

TL;DR: 提出FunCineForge：一个用于大规模影视配音数据构建的端到端流水线与适配多样电影场景的MLLM配音模型，显著提升唇形同步、音质、音色迁移与情感表达，并在多场景上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有配音研究受两大瓶颈限制：1）高质量多模态配音数据稀缺、词错率高、标注稀疏且昂贵、常局限独白场景，难以有效训练；2）模型多仅依赖唇部区域做音视对齐，难适配复杂真人电影场景，导致唇形同步、音质与情感表达不佳。

Method: 提出FunCineForge，包括：a) 一个端到端的数据生产流水线，自动化构建大规模、高质量、富标注的影视配音数据（首个含丰富标注的中文电视剧配音数据集）；b) 一个面向多样电影场景的MLLM配音模型，利用更丰富的视觉线索与指令跟随能力实现唇形同步、音色迁移与情感建模。

Result: 在独白、旁白、对话和多说话人等多种场景中，该方法在音频质量、唇形同步、音色迁移与指令跟随方面均持续优于现有SOTA方法；并展示了所构建数据的高质量。

Conclusion: 通过数据流水线与MLLM配音模型的协同设计，FunCineForge有效缓解数据与建模两大瓶颈，显著提升复杂电影场景的配音效果，并为中文影视配音研究提供首个大规模、富标注的数据资源。

Abstract: Movie dubbing is the task of synthesizing speech from scripts conditioned on video scenes, requiring accurate lip sync, faithful timbre transfer, and proper modeling of character identity and emotion. However, existing methods face two major limitations: (1) high-quality multimodal dubbing datasets are limited in scale, suffer from high word error rates, contain sparse annotations, rely on costly manual labeling, and are restricted to monologue scenes, all of which hinder effective model training; (2) existing dubbing models rely solely on the lip region to learn audio-visual alignment, which limits their applicability to complex live-action cinematic scenes, and exhibit suboptimal performance in lip sync, speech quality, and emotional expressiveness. To address these issues, we propose FunCineForge, which comprises an end-to-end production pipeline for large-scale dubbing datasets and an MLLM-based dubbing model designed for diverse cinematic scenes. Using the pipeline, we construct the first Chinese television dubbing dataset with rich annotations, and demonstrate the high quality of these data. Experiments across monologue, narration, dialogue, and multi-speaker scenes show that our dubbing model consistently outperforms SOTA methods in audio quality, lip sync, timbre transfer, and instruction following. Code and demos are available at https://anonymous.4open.science/w/FunCineForge.

</details>


### [42] [Reconstruction-Anchored Diffusion Model for Text-to-Motion Generation](https://arxiv.org/abs/2601.14788)
*Yifei Liu,Changxing Ding,Ling Guo,Huaiguang Jiang,Qiong Cao*

Main category: cs.CV

TL;DR: 提出RAM：通过联合训练的运动重建分支与测试期重建误差引导，缩小文本-动作表示鸿沟并抑制扩散去噪误差传播，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动人体运动扩散模型存在两大问题：1) 预训练文本编码器缺少运动特定信息，导致文本到动作的表示错配；2) 扩散迭代去噪会累积并传播误差。作者希望构建更贴合运动语义的中间表示，并在推理阶段抑制误差传播。

Method: 提出重构锚定扩散模型（RAM）。训练期：引入“运动潜空间”作为中间监督，联合训练一个运动重建分支，采用两种损失——(a) 自正则化以提升运动空间判别性；(b) 以运动为中心的潜对齐，使文本语义精确映射到运动潜空间。测试期：提出重建误差引导（REG），在每个去噪步中用重建分支对上一步估计进行重构，复现先前误差模式，并放大当前预测与该重构之间的残差，以凸显当前预测的改进，从而减轻误差传播。

Result: 在大量实验中，相比现有方法显著提升多项指标并达到SOTA表现（具体数据未在摘要中给出）。

Conclusion: 通过以运动潜空间为锚的联合训练和测试期重建误差引导，RAM有效缩小文本-动作表示差距并缓解扩散误差传播，显著提升文本到动作生成质量。

Abstract: Diffusion models have seen widespread adoption for text-driven human motion generation and related tasks due to their impressive generative capabilities and flexibility. However, current motion diffusion models face two major limitations: a representational gap caused by pre-trained text encoders that lack motion-specific information, and error propagation during the iterative denoising process. This paper introduces Reconstruction-Anchored Diffusion Model (RAM) to address these challenges. First, RAM leverages a motion latent space as intermediate supervision for text-to-motion generation. To this end, RAM co-trains a motion reconstruction branch with two key objective functions: self-regularization to enhance the discrimination of the motion space and motion-centric latent alignment to enable accurate mapping from text to the motion latent space. Second, we propose Reconstructive Error Guidance (REG), a testing-stage guidance mechanism that exploits the diffusion model's inherent self-correction ability to mitigate error propagation. At each denoising step, REG uses the motion reconstruction branch to reconstruct the previous estimate, reproducing the prior error patterns. By amplifying the residual between the current prediction and the reconstructed estimate, REG highlights the improvements in the current prediction. Extensive experiments demonstrate that RAM achieves significant improvements and state-of-the-art performance. Our code will be released.

</details>


### [43] [Synthetic Data Augmentation for Multi-Task Chinese Porcelain Classification: A Stable Diffusion Approach](https://arxiv.org/abs/2601.14791)
*Ziyao Ling,Silvia Mirri,Paola Salomoni,Giovanni Delnevo*

Main category: cs.CV

TL;DR: 研究用Stable Diffusion+LoRA合成图像，来增强小样本中国瓷器数据，用MobileNetV3进行多任务分类，发现在类型任务提升最大，朝代与窑口提升有限，说明合成数据的有效性取决于与任务相关视觉特征的一致性。


<details>
  <summary>Details</summary>
Motivation: 考古领域尤其是稀有瓷器类别训练数据稀缺，深度学习分类效果受限。希望用生成模型产生高质量合成图像，缓解数据不足并探索其对不同考古分类任务的效果与边界。

Method: 基于Stable Diffusion结合LoRA微调生成瓷器合成图像；以MobileNetV3迁移学习为骨干，开展对照实验：纯真实数据 vs 真实-合成混合（95:5、90:10）两比例；评估四个任务：朝代、釉色、窑口、器型。以F1-macro等指标比较性能。

Result: 合成数据带来任务依赖的收益：器型识别提升最大（90:10时F1-macro提升约5.5%）；朝代与窑口有小幅提升（约3–4%）；显示生成特征与任务关键视觉签名的匹配度影响增益。

Conclusion: 在考古多任务瓷器分类中，LoRA-稳定扩散生成的合成图可在有限数据下提升性能，但增益具任务敏感性。提出在追求数据多样性的同时须保持考古真实性的实践指引，强调生成数据的潜力与局限并存。

Abstract: The scarcity of training data presents a fundamental challenge in applying deep learning to archaeological artifact classification, particularly for the rare types of Chinese porcelain. This study investigates whether synthetic images generated through Stable Diffusion with Low-Rank Adaptation (LoRA) can effectively augment limited real datasets for multi-task CNN-based porcelain classification. Using MobileNetV3 with transfer learning, we conducted controlled experiments comparing models trained on pure real data against those trained on mixed real-synthetic datasets (95:5 and 90:10 ratios) across four classification tasks: dynasty, glaze, kiln and type identification. Results demonstrate task-specific benefits: type classification showed the most substantial improvement (5.5\% F1-macro increase with 90:10 ratio), while dynasty and kiln tasks exhibited modest gains (3-4\%), suggesting that synthetic augmentation effectiveness depends on the alignment between generated features and task-relevant visual signatures. Our work contributes practical guidelines for deploying generative AI in archaeological research, demonstrating both the potential and limitations of synthetic data when archaeological authenticity must be balanced with data diversity.

</details>


### [44] [UniRoute: Unified Routing Mixture-of-Experts for Modality-Adaptive Remote Sensing Change Detection](https://arxiv.org/abs/2601.14797)
*Qingling Shu,Sibao Chen,Wei Lu,Zhihui You,Chengzhuang Liu*

Main category: cs.CV

TL;DR: 提出UniRoute统一框架，用条件路由将特征提取与差异融合自适应到不同遥感模态；通过AR2-MoE与MDR-MoE以及一致性自蒸馏，在多数据集上实现高精度且高效的统一部署。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测依赖专用模型与固定差分算子，难以同时适配同模态与跨模态场景。前者需要细粒度边界与局部交互，后者需要广域上下文抑制斑点与几何失配；固定减法在跨模态/错配时产生伪差异。缺乏既能自适应感受野又能自适应融合原语的统一方案。

Method: 将“特征提取”和“特征融合”统一表述为条件路由问题：1) AR2-MoE自适应感受野路由，将局部细节与全局语义解耦并按像素选择专家；2) MDR-MoE模态感知差异路由，按像素选择最合适的融合/差分原语而非固定减法；3) 一致性感知自蒸馏（CASD）在数据稀缺的异构场景中通过多层次一致性约束稳定统一训练。

Result: 在5个公开数据集上取得总体领先或竞争性的精度，并在统一部署下实现良好的准确率-效率折中。

Conclusion: 条件路由式统一框架能够自适应不同模态与对齐条件，提升同/异构变化检测的鲁棒性与精度，同时保持推理效率，适合面向大规模、模态自适应的地球观测应用。

Abstract: Current remote sensing change detection (CD) methods mainly rely on specialized models, which limits the scalability toward modality-adaptive Earth observation. For homogeneous CD, precise boundary delineation relies on fine-grained spatial cues and local pixel interactions, whereas heterogeneous CD instead requires broader contextual information to suppress speckle noise and geometric distortions. Moreover, difference operator (e.g., subtraction) works well for aligned homogeneous images but introduces artifacts in cross-modal or geometrically misaligned scenarios. Across different modality settings, specialized models based on static backbones or fixed difference operations often prove insufficient. To address this challenge, we propose UniRoute, a unified framework for modality-adaptive learning by reformulating feature extraction and fusion as conditional routing problems. We introduce an Adaptive Receptive Field Routing MoE (AR2-MoE) module to disentangle local spatial details from global semantic context, and a Modality-Aware Difference Routing MoE (MDR-MoE) module to adaptively select the most suitable fusion primitive at each pixel. In addition, we propose a Consistency-Aware Self-Distillation (CASD) strategy that stabilizes unified training under data-scarce heterogeneous settings by enforcing multi-level consistency. Extensive experiments on five public datasets demonstrate that UniRoute achieves strong overall performance, with a favorable accuracy-efficiency trade-off under a unified deployment setting.

</details>


### [45] [UBATrack: Spatio-Temporal State Space Model for General Multi-Modal Tracking](https://arxiv.org/abs/2601.14799)
*Qihua Liang,Liang Chen,Yaozong Zheng,Jian Nong,Zhiyi Mo,Bineng Zhong*

Main category: cs.CV

TL;DR: 提出UBATrack：基于Mamba风格状态空间模型的多模态目标跟踪框架，通过STMA与动态多模态特征混合器，显著提升跨模态时空建模与鲁棒性，且无需全参数微调；在多项RGB-T/D/E基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态跟踪多依赖提示学习统一任务，但忽视了有效的时空线索捕获与跨模态依赖建模，且训练成本高（常需全参数微调）。因此需要一种既能高效建模长序列时空信息与跨模态关系、又能降低训练开销的方法。

Method: 构建基于Mamba的多模态跟踪框架UBATrack，包含：1) 时空Mamba适配器（STMA），利用Mamba长序列建模能力，在适配器微调范式下联合建模跨模态依赖与时空视觉线索；2) 动态多模态特征混合器，在多特征维度上自适应融合不同模态，增强表示与鲁棒性；整体避免全参数微调以提升训练效率。

Result: 在RGB-T、RGB-D、RGB-E任务上优于现有SOTA，具体包括LasHeR、RGBT234、RGBT210、DepthTrack、VOT-RGBD22、VisEvent等基准上取得领先或卓越成绩。

Conclusion: UBATrack有效利用Mamba进行时空与跨模态联合建模，并通过动态融合增强鲁棒性，在无需全参数微调的前提下实现训练高效与性能领先，适用于多种多模态跟踪场景。

Abstract: Multi-modal object tracking has attracted considerable attention by integrating multiple complementary inputs (e.g., thermal, depth, and event data) to achieve outstanding performance. Although current general-purpose multi-modal trackers primarily unify various modal tracking tasks (i.e., RGB-Thermal infrared, RGB-Depth or RGB-Event tracking) through prompt learning, they still overlook the effective capture of spatio-temporal cues. In this work, we introduce a novel multi-modal tracking framework based on a mamba-style state space model, termed UBATrack. Our UBATrack comprises two simple yet effective modules: a Spatio-temporal Mamba Adapter (STMA) and a Dynamic Multi-modal Feature Mixer. The former leverages Mamba's long-sequence modeling capability to jointly model cross-modal dependencies and spatio-temporal visual cues in an adapter-tuning manner. The latter further enhances multi-modal representation capacity across multiple feature dimensions to improve tracking robustness. In this way, UBATrack eliminates the need for costly full-parameter fine-tuning, thereby improving the training efficiency of multi-modal tracking algorithms. Experiments show that UBATrack outperforms state-of-the-art methods on RGB-T, RGB-D, and RGB-E tracking benchmarks, achieving outstanding results on the LasHeR, RGBT234, RGBT210, DepthTrack, VOT-RGBD22, and VisEvent datasets.

</details>


### [46] [LocBAM: Advancing 3D Patch-Based Image Segmentation by Integrating Location Contex](https://arxiv.org/abs/2601.14802)
*Donnate Hooft,Stefan M. Fischer,Cosmin Bercea,Jan C. Peeken,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 提出LocBAM注意力，在3D补丁分割中显式利用位置信息，稳定训练并在低覆盖度下显著提升精度，优于CoordConv。


<details>
  <summary>Details</summary>
Motivation: 补丁式3D分割节省显存但丢失全局解剖上下文；现有方法对补丁在体数据中的位置处理不足，影响需要全局语义的器官/结构分割。

Method: 系统分析“位置上下文”在补丁分割中的作用，并提出LocBAM：一种显式处理空间坐标/位置信号的注意力模块。将位置信息与特征联合建模，通过注意力机制融合，从而在低patch-to-volume覆盖时补偿缺失的全局上下文。与CoordConv等坐标编码对比，集成到现有3D分割网络中进行训练/推理。

Result: 在BTCV、AMOS22、KiTS23数据集上，加入位置上下文可稳定训练并提升分割指标；在低覆盖度场景提升更显著。LocBAM在多项指标上持续优于经典的CoordConv基线。

Conclusion: 显式建模补丁的空间位置对3D医学图像补丁分割至关重要。LocBAM提供了有效而通用的注意力机制，能在缺乏全局上下文时提升性能并增强训练稳定性，优于传统坐标卷积方案。

Abstract: Patch-based methods are widely used in 3D medical image segmentation to address memory constraints in processing high-resolution volumetric data. However, these approaches often neglect the patch's location within the global volume, which can limit segmentation performance when anatomical context is important. In this paper, we investigate the role of location context in patch-based 3D segmentation and propose a novel attention mechanism, LocBAM, that explicitly processes spatial information. Experiments on BTCV, AMOS22, and KiTS23 demonstrate that incorporating location context stabilizes training and improves segmentation performance, particularly under low patch-to-volume coverage where global context is missing. Furthermore, LocBAM consistently outperforms classical coordinate encoding via CoordConv. Code is publicly available at https://github.com/compai-lab/2026-ISBI-hooft

</details>


### [47] [Symmetry Informative and Agnostic Feature Disentanglement for 3D Shapes](https://arxiv.org/abs/2601.14804)
*Tobias Weißberg,Weikang Wang,Paul Roetzer,Nafie El Amrani,Florian Bernard*

Main category: cs.CV

TL;DR: 论文提出一种同时包含“对称信息”和“对称无关”成分的形状描述子，通过特征解耦与后续精炼，缓解以往方法一维特征信息不足与噪声导致的小块误分类问题，在对称检测、左右分类与匹配等任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统几何手工描述子或精炼方法难以包含语义；利用图像大模型得到的语义描述子虽提升了多任务表现，但对“对称性”这一关键属性考虑不足。现有χ方法虽可从语义特征中提取对称信息，但仅一维且噪声大，产生误分类小斑块。需要一种既保留丰富语义、又显式编码对称性的稳健描述子。

Method: 提出特征解耦框架：从语义感知描述子中分离出“对称信息成分”和“对称无关成分”，使描述子在对称相关任务上判别性强，同时不丢失其他语义信息；并设计特征精炼策略（如平滑/一致性正则、区域一致性或图结构传播等思想）以抑制噪声，减少小块误分类，提升鲁棒性。

Result: 在大量实验中（内在对称检测、左右分类、形状匹配），所提方法在定性与定量指标上均优于多种SOTA，包括相较χ方法更稳定、误检更少、匹配更准确。

Conclusion: 通过对语义描述子的对称与非对称成分解耦并配合精炼，能够构建更稳健、信息更丰富的对称感知形状描述子，在多种形状分析任务上取得综合领先表现。

Abstract: Shape descriptors, i.e., per-vertex features of 3D meshes or point clouds, are fundamental to shape analysis. Historically, various handcrafted geometry-aware descriptors and feature refinement techniques have been proposed. Recently, several studies have initiated a new research direction by leveraging features from image foundation models to create semantics-aware descriptors, demonstrating advantages across tasks like shape matching, editing, and segmentation. Symmetry, another key concept in shape analysis, has also attracted increasing attention. Consequently, constructing symmetry-aware shape descriptors is a natural progression. Although the recent method $χ$ (Wang et al., 2025) successfully extracted symmetry-informative features from semantic-aware descriptors, its features are only one-dimensional, neglecting other valuable semantic information. Furthermore, the extracted symmetry-informative feature is usually noisy and yields small misclassified patches. To address these gaps, we propose a feature disentanglement approach which is simultaneously symmetry informative and symmetry agnostic. Further, we propose a feature refinement technique to improve the robustness of predicted symmetry informative features. Extensive experiments, including intrinsic symmetry detection, left/right classification, and shape matching, demonstrate the effectiveness of our proposed framework compared to various state-of-the-art methods, both qualitatively and quantitatively.

</details>


### [48] [POTR: Post-Training 3DGS Compression](https://arxiv.org/abs/2601.14821)
*Bert Ramlot,Martijn Courteaux,Peter Lambert,Glenn Van Wallendael*

Main category: cs.CV

TL;DR: POTR 是一个针对3D Gaussian Splatting（3DGS）的后训练压缩编解码器，核心在于高效全局剪枝与无训练的光照系数重计算，从而在基本无损画质的前提下显著减少存储、加速推理，并可用少量微调进一步提升率失真表现。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽训练/渲染更快于NeRF，但存储体积大、推理受大量splat数量与高维光照系数负担。需要后训练（无需再训练）的方法，在不牺牲质量的情况下减少splat数量与参数熵，从而降低存储并提升实时渲染速度。

Method: 1) 全局并行剪枝：修改3DGS栅格化器，能一次性评估每个splat被移除的影响，选择性删除影响小的splat，实现较以往后训练剪枝方法2-4倍的splat减少；2) 无训练光照系数重计算：重新估计SH等光照系数以显著降低熵，尤其提升AC系数稀疏度（由70%到97%），无需任何训练；3) 可选的轻量微调：在剪枝与重计算后做简单微调，进一步优化率失真与推理速度。

Result: 与其他后训练压缩方案相比，在相近或更好画质下：- splat数量大幅减少（2-4倍），- 推理速度提升1.5-2倍，- 光照AC系数稀疏度从约70%提升到约97%，- 整体率失真曲线与速度均取得一致领先，即便不进行微调也胜出。

Conclusion: POTR以高效全局剪枝和无训练光照系数重计算为核心，显著压缩3DGS并加速推理，且无需再训练即可优于现有后训练压缩方法；若加以简单微调，性能进一步提升，适合作为3DGS实用化部署的通用后处理压缩方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a promising contender to Neural Radiance Fields (NeRF) in 3D scene reconstruction and real-time novel view synthesis. 3DGS outperforms NeRF in training and inference speed but has substantially higher storage requirements. To remedy this downside, we propose POTR, a post-training 3DGS codec built on two novel techniques. First, POTR introduces a novel pruning approach that uses a modified 3DGS rasterizer to efficiently calculate every splat's individual removal effect simultaneously. This technique results in 2-4x fewer splats than other post-training pruning techniques and as a result also significantly accelerates inference with experiments demonstrating 1.5-2x faster inference than other compressed models. Second, we propose a novel method to recompute lighting coefficients, significantly reducing their entropy without using any form of training. Our fast and highly parallel approach especially increases AC lighting coefficient sparsity, with experiments demonstrating increases from 70% to 97%, with minimal loss in quality. Finally, we extend POTR with a simple fine-tuning scheme to further enhance pruning, inference, and rate-distortion performance. Experiments demonstrate that POTR, even without fine-tuning, consistently outperforms all other post-training compression techniques in both rate-distortion performance and inference speed.

</details>


### [49] [Multimodal system for skin cancer detection](https://arxiv.org/abs/2601.14822)
*Volodymyr Sydorskyi,Igor Krashenyi,Oleksii Yakubenko*

Main category: cs.CV

TL;DR: 提出一种基于普通照片+表格元数据的多模态黑色素瘤检测系统，采用两步/三阶段管线与提升算法，针对数据极度不平衡做特殊训练，取得较高部分ROC AUC与检索敏感度，证明多模态与分阶段设计可在无专用设备条件下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于皮镜图像的深度学习方法依赖专用设备，限制在基层与广泛临床环境的应用；普通照片更易获取但单模态性能受限，因此需要一种可用常规照片、并融合患者与病灶元数据、且能在不平衡数据下稳健学习的方案。

Method: 构建多模态神经网络，将常规皮肤照片与表格元数据（人口学信息、病灶特征）融合；设计有/无元数据的两步模型；引入三阶段预测管线：基模态模型→与提升（boosting）算法结合→进一步后处理增强；针对类别极度不平衡，采用特定损失函数、采样/重加权等稳健训练策略；开展消融研究比较不同视觉主干、提升算法与损失。

Result: 在高度不平衡数据上取得Partial ROC AUC 0.18068（上限0.2），top-15检索敏感度0.78371，多模态融合与三阶段管线较单模态/单阶段基线显著提升。

Conclusion: 多模态（照片+元数据）与分阶段、提升增强的框架能在无需皮镜设备的前提下实现可扩展的黑色素瘤检测，适配多样医疗环境，弥合专科与普通临床的应用差距。

Abstract: Melanoma detection is vital for early diagnosis and effective treatment. While deep learning models on dermoscopic images have shown promise, they require specialized equipment, limiting their use in broader clinical settings. This study introduces a multi-modal melanoma detection system using conventional photo images, making it more accessible and versatile. Our system integrates image data with tabular metadata, such as patient demographics and lesion characteristics, to improve detection accuracy. It employs a multi-modal neural network combining image and metadata processing and supports a two-step model for cases with or without metadata. A three-stage pipeline further refines predictions by boosting algorithms and enhancing performance. To address the challenges of a highly imbalanced dataset, specific techniques were implemented to ensure robust training. An ablation study evaluated recent vision architectures, boosting algorithms, and loss functions, achieving a peak Partial ROC AUC of 0.18068 (0.2 maximum) and top-15 retrieval sensitivity of 0.78371. Results demonstrate that integrating photo images with metadata in a structured, multi-stage pipeline yields significant performance improvements. This system advances melanoma detection by providing a scalable, equipment-independent solution suitable for diverse healthcare environments, bridging the gap between specialized and general clinical practices.

</details>


### [50] [MTFlow: Time-Conditioned Flow Matching for Microtubule Segmentation in Noisy Microscopy Images](https://arxiv.org/abs/2601.14841)
*Sidi Mohamed Sid El Moctar,Achraf Ait Laydi,Yousef El Mourabit,Hélène Bouvrais*

Main category: cs.CV

TL;DR: 提出MTFlow：一种时间条件的flow-matching模型，通过学习将噪声掩膜逐步“运输”到真值来分割微管网络，较U-Net单次预测具备可解释的轨迹式细化；在合成与真实微管及其他曲线结构数据上取得与SOTA相当的精度并具良好泛化与效率。


<details>
  <summary>Details</summary>
Motivation: 微管分割因弯曲、密集交叉与噪声而困难，影响对其组织与动态的研究；现有多为U-Net式一次性掩膜预测，细边界不确定性难以处理、缺乏可解释的细化过程。

Method: 提出MTFlow：以U-Net为骨干，引入时间/噪声步的嵌入，采用flow-matching学习向量场，使从噪声掩膜到标注的连续运输轨迹得以建模；迭代更新掩膜，实现不确定性沿边界的时序解析与细化；在合成与真实微管数据训练评估，并在视网膜血管、神经等公共弯曲结构数据上测试泛化。

Result: 在微管数据上取得与当前SOTA相当的分割精度；在跨域曲线结构数据上表现出良好泛化；推理过程时间高效，并能生成更精细、接近或优于人工/半自动标注的结果。

Conclusion: 时间条件的flow-matching分割范式能以可解释的迭代轨迹改善细丝状结构边界与交叉处的预测，提供高效、准确且可泛化的微管及其他曲线结构分割工具。

Abstract: Microtubules are cytoskeletal filaments that play essential roles in many cellular processes and are key therapeutic targets in several diseases. Accurate segmentation of microtubule networks is critical for studying their organization and dynamics but remains challenging due to filament curvature, dense crossings, and image noise. We present MTFlow, a novel time-conditioned flow-matching model for microtubule segmentation. Unlike conventional U-Net variants that predict masks in a single pass, MTFlow learns vector fields that iteratively transport noisy masks toward the ground truth, enabling interpretable, trajectory-based refinement. Our architecture combines a U-Net backbone with temporal embeddings, allowing the model to capture the dynamics of uncertainty resolution along filament boundaries. We trained and evaluated MTFlow on synthetic and real microtubule datasets and assessed its generalization capability on public biomedical datasets of curvilinear structures such as retinal blood vessels and nerves. MTFlow achieves competitive segmentation accuracy comparable to state-of-the-art models, offering a powerful and time-efficient tool for filamentous structure analysis with more precise annotations than manual or semi-automatic approaches.

</details>


### [51] [GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars](https://arxiv.org/abs/2601.14875)
*Zhe Chang,Haodong Jin,Ying Sun,Yan Song,Hui Yu*

Main category: cs.CV

TL;DR: 提出GAT‑NeRF：将轻量Transformer与坐标对齐MLP融合，并注入显式几何先验，实现从单目视频高保真4D动态人脸重建，显著提升高频细节（皱纹、细纹理）建模。


<details>
  <summary>Details</summary>
Motivation: 单目视频生成高保真、可控的4D动态人脸极具应用价值，但传统NeRF难以从信息受限的单目流中恢复高频细节（动态皱纹、细纹理）。需要一种能融合几何先验并加强局部复杂模式建模的表示。

Method: 设计混合NeRF框架GAT‑NeRF：在NeRF管线中引入几何感知轻量Transformer（GAT），与坐标对齐MLP协同。GAT融合多模态输入（3D空间坐标、3DMM表情参数、可学习潜码），利用Transformer的特征学习能力增强与细粒度几何相关的特征表示，从而更好刻画局部复杂人脸模式。

Result: 大量实验表明GAT‑NeRF在视觉保真度与高频细节恢复方面达到SOTA，能更好还原动态皱纹与痘印等局部细节。

Conclusion: 融合显式几何先验与Transformer的混合NeRF可有效提升单目4D人脸重建的细节与可控性，为逼真动态数字人类的多媒体应用开辟新路径。

Abstract: High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.

</details>


### [52] [SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval](https://arxiv.org/abs/2601.14895)
*Xinyi Zheng,Yunze Liu,Chi-Hao Wu,Fan Zhang,Hao Zheng,Wenqi Zhou,Walterio W. Mayol-Cuevas,Junxiao Shen*

Main category: cs.CV

TL;DR: SpatialMem 将3D几何、语义与语言统一到可查询记忆中，从随手拍的自视角视频重建室内、检测结构锚点并构建层级对象记忆，实现可解释的空间推理与语言引导导航/检索。


<details>
  <summary>Details</summary>
Motivation: 现有室内感知与导航方法往往分离几何、语义与语言表示，难以进行可解释的空间推理与跨任务共享；同时依赖专用传感器或缺乏对杂乱、遮挡环境的鲁棒性。

Method: 以普通RGB自视角视频为输入，进行度量尺度重建；提取结构性3D锚点（墙、门、窗）作为第一层支架；在其上构建层级记忆：为开放词汇物体建立节点，关联证据图块、视觉嵌入与两层文本描述，并绑定到3D坐标，实现紧凑存储与快速检索；该统一记忆支持对距离、方向、可见性等空间关系的查询与推理。

Result: 在三处真实室内场景上，系统在锚点/描述级导航完成率与分层检索准确率上保持稳健，即便在杂乱与遮挡增加时亦表现良好。

Conclusion: SpatialMem 提供了一种高效、可扩展且可解释的空间记忆框架，能在无需专用传感器条件下支持语言引导的导航与物体检索等下游任务。

Abstract: We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.

</details>


### [53] [Erosion Attack for Adversarial Training to Enhance Semantic Segmentation Robustness](https://arxiv.org/abs/2601.14950)
*Yufei Song,Ziqi Zhou,Menghao Deng,Yifan Hu,Shengshan Hu,Minghui Li,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 提出EroSeg-AT：利用EroSeg生成更具针对性的对抗样本，通过像素置信度驱动的扰动传播来打破语义一致性，从而提升分割模型在对抗训练下的鲁棒性与攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型易受对抗攻击。已有对抗训练多基于只考虑全局语义的攻击，忽略样本内部上下文语义关系，导致对抗训练难以充分暴露与修复模型脆弱性，需要一种能更细粒度、上下文感知的攻击来提升对抗训练效果。

Method: 提出EroSeg-AT框架：先用EroSeg生成对抗样本再进行对抗训练。EroSeg步骤：1) 依据像素级置信度筛选敏感像素；2) 将扰动从低置信度向高置信度像素逐步传播，刻意破坏语义一致性，从而构造更有效的分割对抗样本；3) 用这些样本进行鲁棒训练。

Result: 实验显示，相比现有方法，该方法在攻击有效性上更强，并且在采用对抗训练时显著提升模型鲁棒性。

Conclusion: 基于像素置信度与语义一致性破坏的EroSeg能生成更具威胁的分割对抗样本，配合EroSeg-AT能有效提升分割模型在对抗训练场景下的鲁棒性。

Abstract: Existing segmentation models exhibit significant vulnerability to adversarial attacks.To improve robustness, adversarial training incorporates adversarial examples into model training. However, existing attack methods consider only global semantic information and ignore contextual semantic relationships within the samples, limiting the effectiveness of adversarial training. To address this issue, we propose EroSeg-AT, a vulnerability-aware adversarial training framework that leverages EroSeg to generate adversarial examples. EroSeg first selects sensitive pixels based on pixel-level confidence and then progressively propagates perturbations to higher-confidence pixels, effectively disrupting the semantic consistency of the samples. Experimental results show that, compared to existing methods, our approach significantly improves attack effectiveness and enhances model robustness under adversarial training.

</details>


### [54] [TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models](https://arxiv.org/abs/2601.14951)
*Carolin Holtermann,Nina Krebs,Anne Lauscher*

Main category: cs.CV

TL;DR: 提出TempViz数据集，系统评估文本到图像模型的时间知识；现有模型和自动评测均表现不佳，亟需改进。


<details>
  <summary>Details</summary>
Motivation: 现实世界实体随时间变化（季节、昼夜、年龄、历史时期等），生成与语境匹配的图像需要时间知识与推理。但T2I模型在时间现象上的研究与评测缺位，相比NLP领域对时间知识的深入研究形成反差。

Method: 构建TempViz数据集：约7.9k条带时间线索的提示词与600+参考图像，覆盖五类时间知识；选取五个主流T2I模型进行系统评测，采用人工评估为主，并探索多种自动化评测方法与人评的一致性。

Result: 在人评下，五个模型在五类时间知识上总体表现较弱，无一模型在各类别上超过75%准确率；多种既有自动评测方法与人类判断相关性不足，无法可靠捕捉时间线索。

Conclusion: T2I模型当前缺乏稳健的时间知识与推理能力，现有自动评测也难以胜任。TempViz为系统研究提供基准，呼吁未来在模型与评测方法上加强对时间知识的建模与检测。

Abstract: Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.

</details>


### [55] [Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers](https://arxiv.org/abs/2601.14959)
*Xinyu Peng,Han Li,Yuyang Huang,Ziyang Zheng,Yaoming Wang,Xin Chen,Wenrui Dai,Chenglin Li,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: 提出LDF-VFI：基于自回归扩散Transformer的整体视频中心范式，实现长时序一致的插帧，并通过跳连采样、稀疏局部注意力与平铺VAE在长序列与超高分辨率上高效泛化，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有插帧多以帧/短片段为中心（如三帧）独立处理，导致跨帧时序不一致与运动伪影；且长序列/高分辨率处理效率与泛化受限。需要一种能端到端建模全视频、保证长程时序一致且可扩展到4K等任意分辨率的方法。

Method: 构建自回归扩散Transformer对整段视频序列建模；提出“skip-concatenate”采样策略缓解自回归误差累积并提升时序稳定；采用稀疏局部注意力与tiled VAE编码以高效处理长序列并支持任意分辨率推理；设计增强的条件VAE解码器，融合输入视频的多尺度特征以提升重建质量。

Result: 在长序列基准上取得SOTA，单帧质量与时序一致性优于现有方法，尤其在大运动场景中表现突出；可在推理时泛化到诸如4K分辨率且无需重新训练。

Conclusion: 整体视频中心的LDF-VFI通过自回归扩散+跳连采样+局部稀疏注意力与平铺VAE，实现长时序一致与高分辨率高效插帧，显著优于帧中心方法并具备强泛化能力。

Abstract: Existing video frame interpolation (VFI) methods often adopt a frame-centric approach, processing videos as independent short segments (e.g., triplets), which leads to temporal inconsistencies and motion artifacts. To overcome this, we propose a holistic, video-centric paradigm named \textbf{L}ocal \textbf{D}iffusion \textbf{F}orcing for \textbf{V}ideo \textbf{F}rame \textbf{I}nterpolation (LDF-VFI). Our framework is built upon an auto-regressive diffusion transformer that models the entire video sequence to ensure long-range temporal coherence. To mitigate error accumulation inherent in auto-regressive generation, we introduce a novel skip-concatenate sampling strategy that effectively maintains temporal stability. Furthermore, LDF-VFI incorporates sparse, local attention and tiled VAE encoding, a combination that not only enables efficient processing of long sequences but also allows generalization to arbitrary spatial resolutions (e.g., 4K) at inference without retraining. An enhanced conditional VAE decoder, which leverages multi-scale features from the input video, further improves reconstruction fidelity. Empirically, LDF-VFI achieves state-of-the-art performance on challenging long-sequence benchmarks, demonstrating superior per-frame quality and temporal consistency, especially in scenes with large motion. The source code is available at https://github.com/xypeng9903/LDF-VFI.

</details>


### [56] [Unified Multi-Dataset Training for TBPS](https://arxiv.org/abs/2601.14978)
*Nilanjana Chatterjee,Sidharatha Garg,A V Subramanyam,Brejesh Lall*

Main category: cs.CV

TL;DR: 提出Scale-TBPS：通过统一数据策划与可扩展身份判别学习，实现跨多数据集的单一文本检索行人模型，性能超越各自数据集微调与天真联合训练。


<details>
  <summary>Details</summary>
Motivation: TBPS虽受VLM推动但数据有限且VLM未对行人识别预训练，现有方法需针对每个数据集微调以适应分布偏移，造成多套模型与维护成本；合成数据虽增量但仍需域适配。作者提出能否训练一个跨多数据集的统一模型，并发现朴素联合训练受身份规模与噪声配对影响而欠佳。

Method: Scale-TBPS包含两部分：1) 噪声感知的统一数据集策划，将多源TBPS数据有机合并并缓解图文配对噪声；2) 可扩展的判别式身份学习框架，能在极大量唯一行人身份下稳定有效训练（如在损失设计、采样与分类/对比学习机制上做可扩展改进）。

Result: 在CUHK-PEDES、ICFG-PEDES、RSTPReid、IIITD-20K、UFine6926等上，单一Scale-TBPS模型优于针对各数据集独立优化的模型与朴素联合训练基线。

Conclusion: 通过统一策划与可扩展判别学习，可在多数据集上训练单一TBPS模型，解决分布偏移与身份规模/噪声问题，带来全面性能提升，表明跨数据集统一训练是可行且更优的路线。

Abstract: Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.

</details>


### [57] [LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding](https://arxiv.org/abs/2601.15016)
*Xiaodong Wang,Langling Huang,Zhirong Wu,Xu Zhao,Teng Xu,Xuhong Xia,Peixi Peng*

Main category: cs.CV

TL;DR: 提出LiViBench——首个面向互动直播视频的“全模态”评测基准，并配套两阶段指令微调与VCR模块，构建LiVi-LLM-7B，在LiViBench与通用视频基准上均显著领先同开源大模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频评测多针对非交互类视频（电影、录播），难以覆盖直播场景中的多模态、强交互、实时性挑战，需要新的基准与方法推动MLLM对互动直播的理解与推理。

Method: 1) 构建LiViBench：覆盖24项任务，聚焦感知、推理及直播特有挑战；所有视频含音频、语音与实时评论。2) 半自动标注流程：多阶段人机协同，“多代理MLLM”生成全面视频描述；以“种子问题驱动”生成高质量问答标注。3) 训练方案：面向互动直播的两阶段指令微调；提出Video-to-Comment Retrieval(VCR)模块以检索并利用实时评论。4) 实现LiVi-LLM-7B：在上述数据与策略上训练的7B多模态模型。

Result: LiVi-LLM-7B在LiViBench上超越多款参数高达72B的开源模型，与头部闭源模型差距缩小；并在VideoMME、LongVideoBench、MLVU、VideoEval-Pro等通用视频基准上表现提升。

Conclusion: 互动直播场景需要专门的评测与训练机制。LiViBench与配套的两阶段微调+VCR有效增强MLLM对直播中多模态与实时交互信息的理解，带来在专用与通用视频任务上的全面性能增益。

Abstract: The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.

</details>


### [58] [SpatialV2A: Visual-Guided High-fidelity Spatial Audio Generation](https://arxiv.org/abs/2601.15017)
*Yanan Wang,Linjie Ren,Zihao Li,Junyi Wang,Tian Gan*

Main category: cs.CV

TL;DR: 论文提出首个大规模视频-双耳音频数据集与端到端视觉引导空间音频生成框架，显著提升空间保真与沉浸感且不牺牲语义与时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频转音频方法主要关注语义与时序对齐，忽视空间感与沉浸度；根源在于普遍依赖单声道数据，缺乏双耳空间信息，无法学习视觉到空间音频的映射。

Method: 1) 构建BinauralVGGSound数据集，提供视频-双耳音频对以学习空间线索；2) 设计端到端视觉引导的空间音频生成框架，显式建模空间特征，包含视觉引导的音频空间化模块，使生成音频具备真实空间属性与分层空间深度，同时保持语义与时序对齐。

Result: 在空间保真度与沉浸体验指标上显著优于SOTA，同时保持时间与语义一致性；实验验证框架有效性。

Conclusion: 利用视觉线索与双耳数据显式建模空间特征，可在不牺牲语义/时序一致性的前提下显著提升视频到音频生成的空间感与沉浸度；数据与代码将公开，促进后续研究。

Abstract: While video-to-audio generation has achieved remarkable progress in semantic and temporal alignment, most existing studies focus solely on these aspects, paying limited attention to the spatial perception and immersive quality of the synthesized audio. This limitation stems largely from current models' reliance on mono audio datasets, which lack the binaural spatial information needed to learn visual-to-spatial audio mappings. To address this gap, we introduce two key contributions: we construct BinauralVGGSound, the first large-scale video-binaural audio dataset designed to support spatially aware video-to-audio generation; and we propose a end-to-end spatial audio generation framework guided by visual cues, which explicitly models spatial features. Our framework incorporates a visual-guided audio spatialization module that ensures the generated audio exhibits realistic spatial attributes and layered spatial depth while maintaining semantic and temporal alignment. Experiments show that our approach substantially outperforms state-of-the-art models in spatial fidelity and delivers a more immersive auditory experience, without sacrificing temporal or semantic consistency. All datasets, code, and model checkpoints will be publicly released to facilitate future research.

</details>


### [59] [Federated Transformer-GNN for Privacy-Preserving Brain Tumor Localization with Modality-Level Explainability](https://arxiv.org/abs/2601.15042)
*Andrea Protani,Riccardo Taiello,Marc Molina Van Den Bosch,Luigi Serio*

Main category: cs.CV

TL;DR: 该论文提出在不共享患者隐私数据的前提下，利用联邦学习进行脑肿瘤定位；在多机构分布式训练下，模型性能可达到集中式训练水平，并通过注意力机制解释不同MRI模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分析的深度模型需要大量且多样的数据，但医疗机构间因隐私限制难以集中数据；需要一种既能跨机构学习又能保护隐私的方案，并验证其在高维、复杂任务中的有效性与可解释性。

Method: 基于无解码器的超体素GNN，扩展为混合Transformer-GNN架构；在CERN面向医疗环境的联邦学习平台CAFEIN上部署。利用Transformer注意力进行可解释性分析，量化各MRI模态（如T2、FLAIR）对预测的影响，并在BraTS数据集上进行实验与统计检验（配对t检验+Bonferroni校正）。

Result: 单机构本地训练因数据受限出现提前停止，难以充分训练；联邦学习通过聚合多机构数据使训练持续改进，最终达到与集中式训练相当的性能。可解释性结果显示，网络深层对T2与FLAIR的注意显著增加（p<0.001，Cohen’s d=1.50），与临床经验一致。

Conclusion: 联邦学习在高维复杂医学影像任务上能显著缓解数据孤岛问题，提升模型学习能力并接近集中式性能；注意力分析证实T2与FLAIR在深层决策中更重要，支持其临床价值。

Abstract: Deep learning models for brain tumor analysis require large and diverse datasets that are often siloed across healthcare institutions due to privacy regulations. We present a federated learning framework for brain tumor localization that enables multi-institutional collaboration without sharing sensitive patient data. Our method extends a hybrid Transformer-Graph Neural Network architecture derived from prior decoder-free supervoxel GNNs and is deployed within CAFEIN\textsuperscript{\textregistered}, CERN's federated learning platform designed for healthcare environments. We provide an explainability analysis through Transformer attention mechanisms that reveals which MRI modalities drive the model predictions. Experiments on the BraTS dataset demonstrate a key finding: while isolated training on individual client data triggers early stopping well before reaching full training capacity, federated learning enables continued model improvement by leveraging distributed data, ultimately matching centralized performance. This result provides strong justification for federated learning when dealing with complex tasks and high-dimensional input data, as aggregating knowledge from multiple institutions significantly benefits the learning process. Our explainability analysis, validated through rigorous statistical testing on the full test set (paired t-tests with Bonferroni correction), reveals that deeper network layers significantly increase attention to T2 and FLAIR modalities ($p<0.001$, Cohen's $d$=1.50), aligning with clinical practice.

</details>


### [60] [Deep Leakage with Generative Flow Matching Denoiser](https://arxiv.org/abs/2601.15049)
*Isaac Baglin,Xiatian Zhu,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出一种将生成式Flow Matching先验融入重构优化的深度泄露攻击，在多数据集/模型上显著提升从联邦学习梯度重建私有数据的质量与稳健性，并对常见防御仍有效。


<details>
  <summary>Details</summary>
Motivation: 现有深度泄露攻击在真实FL场景中重构不稳定、保真度有限、对训练轮次/批量/防御鲁棒性差；需要一种无需知道私有数据分布、但能利用强生成先验提升重构质量与稳定性的方案。

Method: 在反演客户端梯度/更新的优化过程中，引入基于Flow Matching（流匹配）基础模型的生成式先验，作为引导项将重构解推向真实图像分布；无需访问私有数据。与标准DL优化结合，评估于多数据集与目标模型，并在不同epoch、较大batch以及添加噪声、裁剪、稀疏化等防御下测试。

Result: 相较SOTA攻击，在像素级、感知与特征相似度指标上持续领先；在不同训练阶段与较大客户端批量下仍能高质量重构；面对噪声注入、梯度裁剪、稀疏化等常见防御时仍保持有效。

Conclusion: 强生成先验（FM）显著提升DL攻击对FL的威胁与适用性，现有防御不足；需要设计针对具备强生成先验的对手的新型防御策略。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new DL attack that integrates a generative Flow Matching (FM) prior into the reconstruction process. By guiding optimization toward the distribution of realistic images (represented by a flow matching foundation model), our method enhances reconstruction fidelity without requiring knowledge of the private data. Extensive experiments on multiple datasets and target models demonstrate that our approach consistently outperforms state-of-the-art attacks across pixel-level, perceptual, and feature-based similarity metrics. Crucially, the method remains effective across different training epochs, larger client batch sizes, and under common defenses such as noise injection, clipping, and sparsification. Our findings call for the development of new defense strategies that explicitly account for adversaries equipped with powerful generative priors.

</details>


### [61] [Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD](https://arxiv.org/abs/2601.15061)
*Qiwei Ma,Jun Zhang*

Main category: cs.CV

TL;DR: 提出一种用于差分隐私数据生成的新框架，通过EFSGD、重建损失与噪声注入，在相同隐私预算下生成更高质量、可用性的合成图像，并在MNIST、Fashion-MNIST、CelebA上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统匿名化等脱敏手段难以兼顾隐私与效用；现有合成数据方法在隐私-效用之间反复权衡、效率低且质量受限，亟需在固定隐私预算下提升图像质量和下游可用性。

Method: 提出差分隐私生成框架：训练中采用带误差反馈的随机梯度下降(EFSGD)，并显式引入重建损失与噪声注入机制以控制隐私泄露同时稳定优化，适用于灰度与彩色图像数据。

Result: 在相同隐私预算下，生成的图像质量和下游任务可用性显著优于相关方法；在MNIST、Fashion-MNIST、CelebA三大基准上，多数指标达SOTA，实验显示方法有效且具泛化性。

Conclusion: 结合EFSGD、重建损失和噪声注入的DP生成框架能在固定隐私预算下提升合成图像质量与实用性，提供通用且强健的隐私保护合成数据方案。

Abstract: Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.

</details>


### [62] [Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background](https://arxiv.org/abs/2601.15065)
*Tianyu Li,Songyue Cai,Zongqian Wu,Ping Hu,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 提出一个即插即用框架FoBoR，通过自适应抑制背景和纠正易混前景补丁，显著提升基于CLIP的前景-背景分解方法在小样本OOD检测中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP式FG-BG分解在小样本OOD检测中有效，但：1）对背景补丁一刀切抑制，忽视不同补丁对预测的不同比重；2）对前景中与其他类别相似的易混补丁缺乏处理，导致训练被误导。

Method: 提出FoBoR三模块：1）沿用FG-BG分解得到前景与背景区域；2）自适应背景抑制（ABS），对每个背景补丁基于其分类熵进行自适应加权抑制；3）易混前景校正（CFR），识别并纠正与他类外观或语义相似的前景补丁，减少误导信号。该框架可无缝集成现有FG-BG方法。

Result: 在多项实验中，FoBoR作为插件显著提升已有FG-BG方法在few-shot OOD检测上的性能（定量细节未在摘要给出），代码已开源。

Conclusion: 针对背景与前景的细粒度差异化处理是关键：自适应抑制背景不确定补丁、识别并纠正易混前景补丁，可普遍增强CLIP式FG-BG方法在小样本OOD检测中的鲁棒性与准确性。

Abstract: CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contributions of different patches to the prediction. For foreground regions, existing methods fail to adequately consider that some local patches may exhibit appearance or semantic similarity to other classes, which may mislead the training process. To address these issues, we propose a new plug-and-play framework. This framework consists of three core components: (1) a Foreground-Background Decomposition module, which follows previous FG-BG methods to separate an image into foreground and background regions; (2) an Adaptive Background Suppression module, which adaptively weights patch classification entropy; and (3) a Confusable Foreground Rectification module, which identifies and rectifies confusable foreground patches. Extensive experimental results demonstrate that the proposed plug-and-play framework significantly improves the performance of existing FG-BG decomposition methods. Code is available at: https://github.com/lounwb/FoBoR.

</details>


### [63] [The Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling](https://arxiv.org/abs/2601.15071)
*Jingyang Huo,Yikai Wang,Yanwei Fu,Jianfeng Feng*

Main category: cs.CV

TL;DR: 提出零样本跨被试fMRI到图像重建：构建统一皮层表面数据集UniCortex-fMRI，并提出在通用皮层潜空间中进行可组合潜变量建模的PictorialCortex，利用因子分解与一致性正则及扩散生成，将多已见被试的替代潜变量聚合以重建未见被试视觉体验，显著提升重建效果。


<details>
  <summary>Details</summary>
Motivation: 跨个体与跨实验条件时，同一视觉刺激引发的皮层响应高度可变，导致fMRI→图像映射非单射；现有方法通常需被试特定训练，难以泛化。需要一个统一的数据基准与能处理主体/数据集/试次多源变异性的模型来实现“零样本跨被试重建”。

Method: 1) 数据：汇集多视觉刺激fMRI数据，标准化为统一皮层表面表示，形成UniCortex-fMRI以覆盖多被试、多刺激。2) 模型：提出PictorialCortex，在“通用皮层潜空间”中把fMRI表征分解为刺激驱动成分与主体/数据集/试次等变异因子；通过潜因子“分解-组合”模块实现，并加入“配对因子化”和“再因子化一致性”正则以稳定可辨性与组合性。3) 推理：在未见被试上，合成多种已见被试条件下的代理潜变量并聚合，指导扩散模型进行图像生成。

Result: 在零样本跨被试视觉重建任务上，相比基线方法取得更优的重建质量与语义一致性；实验证明可组合潜变量建模与多数据集联合训练带来显著收益。

Conclusion: 统一数据基准与可组合潜空间建模能够缓解跨被试变异，支持无需被试特定训练的fMRI→图像重建；PictorialCortex为跨主体神经表征解码提供了有效框架，并为多数据集整合与通用神经-视觉生成开辟新路径。

Abstract: Decoding visual experiences from human brain activity remains a central challenge at the intersection of neuroscience, neuroimaging, and artificial intelligence. A critical obstacle is the inherent variability of cortical responses: neural activity elicited by the same visual stimulus differs across individuals and trials due to anatomical, functional, cognitive, and experimental factors, making fMRI-to-image reconstruction non-injective. In this paper, we tackle a challenging yet practically meaningful problem: zero-shot cross-subject fMRI-to-image reconstruction, where the visual experience of a previously unseen individual must be reconstructed without subject-specific training. To enable principled evaluation, we present a unified cortical-surface dataset -- UniCortex-fMRI, assembled from multiple visual-stimulus fMRI datasets to provide broad coverage of subjects and stimuli. Our UniCortex-fMRI is particularly processed by standardized data formats to make it possible to explore this possibility in the zero-shot scenario of cross-subject fMRI-to-image reconstruction. To tackle the modeling challenge, we propose PictorialCortex, which models fMRI activity using a compositional latent formulation that structures stimulus-driven representations under subject-, dataset-, and trial-related variability. PictorialCortex operates in a universal cortical latent space and implements this formulation through a latent factorization-composition module, reinforced by paired factorization and re-factorizing consistency regularization. During inference, surrogate latents synthesized under multiple seen-subject conditions are aggregated to guide diffusion-based image synthesis for unseen subjects. Extensive experiments show that PictorialCortex improves zero-shot cross-subject visual reconstruction, highlighting the benefits of compositional latent modeling and multi-dataset training.

</details>


### [64] [Three-dimensional visualization of X-ray micro-CT with large-scale datasets: Efficiency and accuracy for real-time interaction](https://arxiv.org/abs/2601.15098)
*Yipeng Yin,Rao Yao,Qingying Li,Dazhong Wang,Hong Zhou,Zhijun Fang,Jianing Chen,Longjie Qian,Mingyue Wu*

Main category: cs.CV

TL;DR: 该综述聚焦Micro-CT在工业超精密检测中的高效且高精度3D重建与体绘制，从CT重建算法到体渲染与加速/数据压缩，梳理从医学影像到工业NDT的演进，并提出面向实时在线监测与数字孪生SHM的未来方向。


<details>
  <summary>Details</summary>
Motivation: 工业Micro-CT产生的超大规模数据使得缺陷三维表征面临精度与效率的权衡，需要系统性方法帮助研究者快速选择兼顾准确与高效的重建与渲染技术，以支撑超精密检测与在线监测。

Method: 选取具有代表性的CT重建（从解析法到深度学习）与体绘制方法进行对比评析，关注精度-效率平衡；讨论体绘制中的高级光照、加速策略与数据约简；并联系微结构表征技术的发展脉络进行综合分析。

Result: 形成对现有高效高精CT重建与体渲染技术的系统综述与对比框架，总结算法演进趋势（解析→迭代→深度学习），以及在渲染层面的光照模型与加速/压缩手段对精度与效率的影响。

Conclusion: Micro-CT在工业NDT中的3D可视化可通过深度学习重建、优化的体渲染与数据管理实现高精度与高效率的平衡；未来应面向实时在线监测与虚实交互，发展可用于数字孪生与结构健康监测的快速、精确方法与系统。

Abstract: As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection. This article provides a unique perspective on recent advances in accurate and efficient 3D visualization using Micro-CT, tracing its evolution from medical imaging to industrial non-destructive testing (NDT). Among the numerous CT reconstruction and volume rendering methods, this article selectively reviews and analyzes approaches that balance accuracy and efficiency, offering a comprehensive analysis to help researchers quickly grasp highly efficient and accurate 3D reconstruction methods for microscopic features. By comparing the principles of computed tomography with advancements in microstructural technology, this article examines the evolution of CT reconstruction algorithms from analytical methods to deep learning techniques, as well as improvements in volume rendering algorithms, acceleration, and data reduction. Additionally, it explores advanced lighting models for high-accuracy, photorealistic, and efficient volume rendering. Furthermore, this article envisions potential directions in CT reconstruction and volume rendering. It aims to guide future research in quickly selecting efficient and precise methods and developing new ideas and approaches for real-time online monitoring of internal material defects through virtual-physical interaction, for applying digital twin model to structural health monitoring (SHM).

</details>


### [65] [Pb4U-GNet: Resolution-Adaptive Garment Simulation via Propagation-before-Update Graph Network](https://arxiv.org/abs/2601.15110)
*Aoran Liu,Kun Hu,Clinton Ansun Mo,Qiuxia Wu,Wenxiong Kang,Zhiyong Wang*

Main category: cs.CV

TL;DR: 提出Pb4U-GNet，通过“先传播、后更新”的图网络，在不增加训练分辨率的情况下实现对高分辨率服装网格的强泛化。核心是动态传播深度与几何感知的更新缩放，从而解决GNN在跨分辨率上的退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的布料仿真精确但耗时，难以用于实时任务；现有GNN虽可加速，但在高于训练分辨率的网格上性能显著下降。原因在于固定的消息传递深度无法适应不同网格密度的信息聚合需求，且顶点位移量随分辨率变化而天然不同，导致预测失真。

Method: 提出Propagation-before-Update Graph Network (Pb4U-GNet)：将消息传播与特征更新解耦。1) 动态传播深度控制：根据网格分辨率自适应地调整消息传递迭代次数，使感受野覆盖与密度匹配；2) 几何感知的更新缩放：依据局部几何（如边长、面密度或曲率等）对位移/特征更新量进行尺度化，削弱分辨率依赖。训练主要在低分辨率网格上完成。

Result: 在仅用低分辨率网格训练的情况下，对多种更高分辨率网格仍保持显著稳定与准确的预测，优于现有GNN基线；跨分辨率泛化问题得到明显改善。

Conclusion: 通过“先传播、后更新”的结构与两项自适应机制，Pb4U-GNet有效消除GNN对网格分辨率的敏感性，实现高效且可泛化的神经服装仿真，适用于虚拟试衣与数字人等实时应用。

Abstract: Garment simulation is fundamental to various applications in computer vision and graphics, from virtual try-on to digital human modelling. However, conventional physics-based methods remain computationally expensive, hindering their application in time-sensitive scenarios. While graph neural networks (GNNs) offer promising acceleration, existing approaches exhibit poor cross-resolution generalisation, demonstrating significant performance degradation on higher-resolution meshes beyond the training distribution. This stems from two key factors: (1) existing GNNs employ fixed message-passing depth that fails to adapt information aggregation to mesh density variation, and (2) vertex-wise displacement magnitudes are inherently resolution-dependent in garment simulation. To address these issues, we introduce Propagation-before-Update Graph Network (Pb4U-GNet), a resolution-adaptive framework that decouples message propagation from feature updates. Pb4U-GNet incorporates two key mechanisms: (1) dynamic propagation depth control, adjusting message-passing iterations based on mesh resolution, and (2) geometry-aware update scaling, which scales predictions according to local mesh characteristics. Extensive experiments show that even trained solely on low-resolution meshes, Pb4U-GNet exhibits strong generalisability across diverse mesh resolutions, addressing a fundamental challenge in neural garment simulation.

</details>


### [66] [Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning](https://arxiv.org/abs/2601.15115)
*Shuonan Yang,Yuchen Zhang,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出MARS：一种无需训练的多阶段对抗式推理框架，用于可靠、可解释的仇恨视频检测，先客观描述，再分别构建证据支持的“仇恨”与“非仇恨”推理，最后综合裁决；在两套真实数据上优于其他无需训练方法并在一套上超过训练型SOTA，同时提供可审计的解释。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨视频检测方法受限于训练数据稀缺与可解释性不足；直接提示大型视觉语言模型在稳定性与准确性上不可靠。需要一种既可靠又可解释、且不依赖额外训练的数据高效方案。

Method: 提出MARS训练-free框架：1) 客观视频内容描述，奠定中立语境；2) 基于证据的“仇恨解读”推理；3) 并行“反证/非仇恨”推理；4) 综合两者形成最终、有解释的决策。框架可搭配不同骨干模型（LVLMs），以多阶段对抗式推理提升鲁棒性与可解释性。

Result: 在两个真实数据集上，MARS相较其他训练-free方法在特定骨干与设定下最高提升约10%；并在其中一个数据集上超过当前训练型SOTA。其输出含面向人的可理解论证，有助于合规审查与透明化内容审核流程。

Conclusion: 多阶段对抗式推理能在无需额外训练的条件下提高仇恨视频检测的可靠性与可解释性，具有实际部署价值与监管友好性。

Abstract: Hateful videos pose serious risks by amplifying discrimination, inciting violence, and undermining online safety. Existing training-based hateful video detection methods are constrained by limited training data and lack of interpretability, while directly prompting large vision-language models often struggle to deliver reliable hate detection. To address these challenges, this paper introduces MARS, a training-free Multi-stage Adversarial ReaSoning framework that enables reliable and interpretable hateful content detection. MARS begins with the objective description of video content, establishing a neutral foundation for subsequent analysis. Building on this, it develops evidence-based reasoning that supports potential hateful interpretations, while in parallel incorporating counter-evidence reasoning to capture plausible non-hateful perspectives. Finally, these perspectives are synthesized into a conclusive and explainable decision. Extensive evaluation on two real-world datasets shows that MARS achieves up to 10% improvement under certain backbones and settings compared to other training-free approaches and outperforms state-of-the-art training-based methods on one dataset. In addition, MARS produces human-understandable justifications, thereby supporting compliance oversight and enhancing the transparency of content moderation workflows. The code is available at https://github.com/Multimodal-Intelligence-Lab-MIL/MARS.

</details>


### [67] [BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation](https://arxiv.org/abs/2601.15123)
*Andrey Moskalenko,Danil Kuznetsov,Irina Dudko,Anastasiia Iasakova,Nikita Boldyrev,Denis Shepelev,Andrei Spiridonov,Andrey Kuznetsov,Vlad Shakhuro*

Main category: cs.CV

TL;DR: 论文研究了提示可控分割模型（如 SAM）对真实世界“边界框”提示扰动的鲁棒性，提出 BREPS 生成遵循自然性约束的对抗性边界框，用于最小化或最大化分割误差，并在 10 个数据集上基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有训练/评测多依赖用启发式生成的“合成”提示，缺乏对真实用户标注误差的刻画；而边界框虽高效但在真实操作中存在自然变动，可能显著影响分割质量，需要系统性鲁棒性评估与改进。

Method: 1) 进行受控用户研究，采集数千个真实边界框，量化不同用户在同一目标上的提示差异对分割的影响；2) 将鲁棒性评测表述为白盒优化，在边界框提示空间内搜索能最大化/最小化分割误差的提示；3) 提出 BREPS：在自然性约束（如位置/尺寸/抖动范围等）下生成对抗性边界框；4) 在 10 个跨域数据集（日常场景至医疗影像）上对主流模型进行基准测试。

Result: 发现 SAM 类模型对自然提示噪声高度敏感，不同用户在同一实例上可导致显著的分割质量差异；BREPS 能稳定找到使误差升高或降低的边界框，揭示模型在不同数据域的脆弱性与性能上限/下限。

Conclusion: 单纯依赖合成提示会高估模型鲁棒性；应将真实用户变动纳入评估。BREPS 提供系统化、可控的对抗性边界框生成与基准框架，可用于更公平的模型比较、鲁棒性诊断与后续训练/防御改进。

Abstract: Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.

</details>


### [68] [Graph Recognition via Subgraph Prediction](https://arxiv.org/abs/2601.15133)
*André Eberhard,Gerhard Neumann,Pascal Friederich*

Main category: cs.CV

TL;DR: 提出GraSP：通过子图预测实现通用的图结构视觉识别，在多种合成基准与一个真实应用上有效且可迁移。


<details>
  <summary>Details</summary>
Motivation: 视觉关系/图识别仍困难，现有方法多为任务定制、难以跨场景复用；缺乏统一、简单、可迁移的视觉图识别框架。

Method: 提出GraSP（Graph Recognition via Subgraph Prediction）：将图从图像中识别问题转化为子图级别的预测与组合，适配多种图类型与绘制方式；强调无需针对任务的特定修改即可迁移。

Result: 在多个合成数据集和一个真实世界任务上验证，GraSP对多样图结构与图绘制均表现良好，并可在任务间直接迁移。

Conclusion: GraSP为视觉图识别提供了更统一、简单且可迁移的框架，缓解了以往方法的任务依赖性与不可移植性。

Abstract: Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.

</details>


### [69] [Large-Scale Multidimensional Knowledge Profiling of Scientific Literature](https://arxiv.org/abs/2601.15170)
*Zhucun Xue,Jiangning Zhang,Juntao Jiang,Jinzhuo Liu,Haoyang He,Teng Hu,Xiaobin Hu,Guangming Yao,Yi Yuan,Yong Liu*

Main category: cs.CV

TL;DR: 构建超过10万篇AI论文的统一语料与多维画像管线，利用主题聚类、LLM解析与结构化检索，量化近年研究主题与方法的演化，揭示安全、多模态推理、智能体等兴起，机器翻译与图方法趋稳。


<details>
  <summary>Details</summary>
Motivation: 传统文献计量多依赖元数据，难以刻画论文语义与跨领域影响，无法清晰追踪主题生命周期与方法转变。需要基于文本内容的统一、可扩展分析框架来描绘2020–2025年AI研究的结构与演化。

Method: 收集22个顶会2020–2025年论文（>10万），构建多维画像管线：文本语义主题聚类、LLM辅助解析（提取任务/方法/数据/模型/机构等）、结构化检索与聚合，形成研究活动的综合表征，用于时间序列与交叉维度分析。

Result: 得到可用于研究主题生命周期、方法迁移、数据集与模型使用图谱、机构方向的结构化库。分析显示安全、多模态推理、智能体研究显著增长；神经机器翻译、图方法等领域增速放缓、进入相对稳定期。

Conclusion: 语义驱动的多维画像能比传统元数据更好地揭示AI研究演化，为洞察趋势、发现新方向提供证据与资源，并发布了代码与数据集以支持复现与后续研究。

Abstract: The rapid expansion of research across machine learning, vision, and language has produced a volume of publications that is increasingly difficult to synthesize. Traditional bibliometric tools rely mainly on metadata and offer limited visibility into the semantic content of papers, making it hard to track how research themes evolve over time or how different areas influence one another. To obtain a clearer picture of recent developments, we compile a unified corpus of more than 100,000 papers from 22 major conferences between 2020 and 2025 and construct a multidimensional profiling pipeline to organize and analyze their textual content. By combining topic clustering, LLM-assisted parsing, and structured retrieval, we derive a comprehensive representation of research activity that supports the study of topic lifecycles, methodological transitions, dataset and model usage patterns, and institutional research directions. Our analysis highlights several notable shifts, including the growth of safety, multimodal reasoning, and agent-oriented studies, as well as the gradual stabilization of areas such as neural machine translation and graph-based methods. These findings provide an evidence-based view of how AI research is evolving and offer a resource for understanding broader trends and identifying emerging directions. Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature

</details>


### [70] [BBoxMaskPose v2: Expanding Mutual Conditioning to 3D](https://arxiv.org/abs/2601.15200)
*Miroslav Purkrabek,Constantin Kolomiiets,Jiri Matas*

Main category: cs.CV

TL;DR: 提出PMPose与BMPv2，在拥挤场景显著提升2D姿态估计，并带动3D姿态提升；在COCO和OCHuman上创SOTA，OCHuman首次超过50 AP。


<details>
  <summary>Details</summary>
Motivation: 现有2D人体姿态基准多已接近饱和，但在多人拥挤场景仍表现不佳，需要能够在不损失常规场景性能的前提下，提升拥挤场景的姿态估计，并验证高质量2D对3D任务的促进作用。

Method: 1) PMPose：在top-down框架中引入概率化表述与mask条件（mask-conditioning），以更好地处理遮挡与重叠。2) BMPv2：将PMPose与改进的基于SAM的mask细化模块融合，形成BBoxMaskPose v2；同时将2D结果作为对3D模型的“提示”（prompting）以提升3D估计。

Result: 在COCO上提升1.5 AP，在OCHuman上提升6 AP，成为首个在OCHuman上超过50 AP的方法；在新建的OCHuman-Pose上表明多人人体性能更受姿态预测精度影响而非检测；2D质量提升显著改善拥挤场景下的3D姿态。

Conclusion: 概率化与mask条件的结合能显著改善拥挤场景的2D姿态；整合为BMPv2后达成新的SOTA，并验证高质量2D对3D的直接收益；提升多人人体系统应优先关注姿态预测精度。

Abstract: Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing performance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating PMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art by 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first method to exceed 50 AP on OCHuman. We demonstrate that BMP's 2D prompting of 3D model improves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit 3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is more affected by pose prediction accuracy than by detection. The code, models, and data are available on https://MiraPurkrabek.github.io/BBox-Mask-Pose/.

</details>


### [71] [A Computer Vision Hybrid Approach: CNN and Transformer Models for Accurate Alzheimer's Detection from Brain MRI Scans](https://arxiv.org/abs/2601.15202)
*Md Mahmudul Hoque,Shuvo Karmaker,Md. Hadi Al-Amin,Md Modabberul Islam,Jisun Junayed,Farha Ulfat Mahi*

Main category: cs.CV

TL;DR: 论文比较5个CNN、5个Transformer与混合模型Evan_V2在四分类AD MRI任务上的表现；CNN整体更稳，ResNet50达98.83%，ViT在Transformer中最佳95.38%；Evan_V2进行特征级融合，达到99.99%准确、0.9989 F1、0.9968 AUC，显著降低各分期误判，显示混合集成的临床潜力。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早诊对干预与预后至关重要；现有单一模型在不同分期上稳定性与泛化性受限，需要系统比较CNN与Transformer并探索能兼顾精度与稳健性的融合方案。

Method: 在MRI四分类（非痴呆、极轻度、轻度、中度）任务上，系统评估5个CNN（EfficientNetB0、ResNet50、DenseNet201、MobileNetV3、VGG16）与5个Transformer（ViT、ConvTransformer、PatchTransformer、MLP-Mixer、SimpleTransformer）；提出混合模型Evan_V2，通过10个基模型输出的特征级融合实现集成；采用准确率、F1、ROC AUC与混淆矩阵进行对比分析。

Result: CNN整体表现强，ResNet50准确率98.83%；Transformer具备竞争性泛化，其中ViT最高95.38%，但类别不稳定性更大；Evan_V2通过多模型特征融合取得最佳：准确率99.99%、F1=0.9989、AUC=0.9968，并在混淆矩阵上显著减少各分期误判。

Conclusion: 混合集成（特征级融合）优于单一CNN或Transformer，在AD MRI四分类上提供更高的精度与稳定性，具有成为临床可靠辅助诊断工具的潜力。

Abstract: Early and accurate classification of Alzheimers disease (AD) from brain MRI scans is essential for timely clinical intervention and improved patient outcomes. This study presents a comprehensive comparative analysis of five CNN architectures (EfficientNetB0, ResNet50, DenseNet201, MobileNetV3, VGG16), five Transformer-based models (ViT, ConvTransformer, PatchTransformer, MLP-Mixer, SimpleTransformer), and a proposed hybrid model named Evan_V2. All models were evaluated on a four-class AD classification task comprising Mild Dementia, Moderate Dementia, Non-Demented, and Very Mild Dementia categories. Experimental findings show that CNN architectures consistently achieved strong performance, with ResNet50 attaining 98.83% accuracy. Transformer models demonstrated competitive generalization capabilities, with ViT achieving the highest accuracy among them at 95.38%. However, individual Transformer variants exhibited greater class-specific instability. The proposed Evan_V2 hybrid model, which integrates outputs from ten CNN and Transformer architectures through feature-level fusion, achieved the best overall performance with 99.99% accuracy, 0.9989 F1-score, and 0.9968 ROC AUC. Confusion matrix analysis further confirmed that Evan_V2 substantially reduced misclassification across all dementia stages, outperforming every standalone model. These findings highlight the potential of hybrid ensemble strategies in producing highly reliable and clinically meaningful diagnostic tools for Alzheimers disease classification.

</details>


### [72] [ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation](https://arxiv.org/abs/2601.15221)
*Hanlei Guo,Jiahao Shao,Xinya Chen,Xiyang Tan,Sheng Miao,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: ScenDi融合3D与2D扩散：先用3D潜变量扩散生成可控的3D高斯场景，再以2D视频扩散细化外观，从而兼顾外观细节与相机可控性；在Waymo与KITTI‑360验证有效。


<details>
  <summary>Details</summary>
Motivation: 单纯3D扩散细节易劣化，单纯2D扩散难以保证相机控制与三维一致性；城市街景复杂且需遵循精确相机轨迹与语义结构，现有方法难两全。

Method: 两阶段：1) 训练3D潜变量扩散生成3D高斯(3DGS)以渲染低分辨率图像；可选条件包括3D包围盒、道路图、文本以实现可控布局。2) 训练2D视频扩散，在由3DGS渲染的序列条件下进行外观细化与一致性增强；以粗3D引导2D生成，从而保持轨迹与结构。

Result: 在Waymo与KITTI‑360上实现更真实的城市场景生成与精确相机轨迹遵循，兼顾结构控制与细节质量，实验证明优于仅3D或仅2D基线（摘要未给数值）。

Conclusion: 融合3D与2D扩散的级联框架能在城市场景中同时获得可控三维结构与高保真外观，是生成可控真实街景的有效方案。

Abstract: Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.

</details>


### [73] [PROGRESSLM: Towards Progress Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.15224)
*Jianshu Zhang,Chengxuan Qian,Haosen Sun,Haoran Lu,Dingcheng Wang,Letian Xue,Han Liu*

Main category: cs.CV

TL;DR: 提出Progress-Bench评估VLM对任务进度推理的能力，并给出训练免提示与小模型训练两条路径；多数现有VLM难以稳健估计进度，而专门训练的ProgressLM-3B在零重叠任务上获得稳定提升，并分析失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有VLM擅长识别可见静态内容，但难以从部分观察中推断“任务进行到哪一步”。缺乏系统评测与有效方法来检验和提升VLM的长程动态与进度推理能力。

Method: 1) 构建Progress-Bench，系统评测VLM在不同演示模态、视角变化与不可回答情形下的进度推理鲁棒性；2) 人类启发的两阶段进度推理范式：a) 训练免的结构化提示策略；b) 基于精心构建的ProgressLM-45K数据进行训练，得到小规模模型ProgressLM-3B；3) 在14个VLM上进行对比与消融，分析误差模式与成败条件。

Result: 大多数VLM对进度估计表现不佳：对演示模态/视角变化敏感，且处理不可回答案例能力弱。训练免的结构化提示提升有限且依赖模型；训练的ProgressLM-3B在与评测任务完全不重叠的设置下仍获得一致改进。

Conclusion: 进度推理是VLM的薄弱环节，单靠提示难以可靠提升；结合专门数据与两阶段范式的小模型可显著改进且具泛化性。基准与分析为何时/为何成功或失败提供了线索，指向面向长程动态理解的后续研究方向。

Abstract: Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.

</details>


### [74] [Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification](https://arxiv.org/abs/2601.15235)
*Fabi Nahian Madhurja,Rusab Sarmun,Muhammad E. H. Chowdhury,Adam Mushtak,Israa Al-Hashimi,Sohaib Bassam Zoghoul*

Main category: cs.CV

TL;DR: 提出一套以2D投影为核心的端到端流程，在CT中自动定位/分割颈椎并检测骨折，兼顾高效与准确：3视图YOLOv8定位(3D mIoU 94.45%)→2D投影驱动的DenseNet121-UNet多标签分割(Dice 87.86%)→由2D掩膜近似3D椎体体积→2.5D时空序列集成模型做骨折判别；在椎体/患者层面F1分别68.15/82.26，ROC-AUC分别91.62/83.04，并通过可解释性与医生一致性分析验证。


<details>
  <summary>Details</summary>
Motivation: 传统3D分割与检测计算量大、训练推理成本高，且颈椎解剖小而复杂、病灶离散，难以在有限资源下实现高质量的椎体级骨折检测。作者希望用2D投影的低复杂度方案，仍保持接近3D方法的定位分割性能，并在椎体与患者层面实现可靠的骨折判别。

Method: 1) 投影定位：将3D CT优化为轴/矢/冠三视图投影，用YOLOv8从各视图检出ROI并融合成3D颈椎区域(3D mIoU 94.45%)。2) 多标签分割：基于DenseNet121-UNet，利用方差/能量投影生成2D椎体分割，Dice 87.86%，再由2D掩膜重建近似3D椎体体积。3) 骨折检测：对每个椎体，组合原始切片与投影的2.5D时空序列特征，采用集成模型进行判别。4) 评估与解释：报告椎体/患者层面F1与AUC，并给出显著性图和与放射科医师一致性比较。

Result: - 定位：3D mIoU 94.45%。- 分割：Dice 87.86%。- 骨折检测：椎体层面F1 68.15、ROC-AUC 91.62；患者层面F1 82.26、ROC-AUC 83.04。- 可解释性显著图指向相关解剖；与专家比较具竞争力。

Conclusion: 2D投影驱动的端到端管线能在显著降低计算复杂度的同时，提供高质量颈椎定位/分割与有竞争力的椎体级骨折检测。投影融合与2.5D时空集成对性能关键，方法具备临床可行性并可扩展至其他脊柱部位。

Abstract: Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model's performance with expert radiologists, demonstrating competitive results.

</details>


### [75] [FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion](https://arxiv.org/abs/2601.15250)
*Zichen Xi,Hao-Xiang Chen,Nan Xue,Hongyu Yan,Qi-Yuan Feng,Levent Burak Kara,Joaquim Jorge,Qun-Ce Xu*

Main category: cs.CV

TL;DR: 提出FlowSSC：首个直接用于单目语义场景补全的生成式框架，基于捷径流匹配在三平面潜空间中一跳生成，高效且大幅提升现有方法在遮挡区域与空间关系的合理性，在SemanticKITTI上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目SSC需从单视角推断被遮挡的3D几何与语义，前馈方法在遮挡细节与物体空间关系上常失真，难以用于需要全空间一致性与可部署性的真实场景（如自动驾驶）。需要一种既能生成合理遮挡内容、又能实时推理的框架。

Method: 把SSC建模为条件生成问题：在紧凑的triplane潜空间中进行“Shortcut Flow-matching”，以流匹配/常微分方程生成替代多步扩散，利用捷径机制实现单步高保真生成；框架可无缝叠加到现有前馈SSC管线作为增强模块，实现端到端推理。

Result: 在SemanticKITTI上显著超越现有基线，报告达到SOTA；在遮挡区域细节与空间关系保持方面显著改进，同时实现近实时（单步生成）推理。

Conclusion: FlowSSC证明把SSC视作条件生成并在三平面潜空间用捷径流匹配可兼顾质量与速度，既提升遮挡推断与全局一致性，又具备实用部署价值，可作为通用增强模块提升现有单目SSC方法。

Abstract: Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.

</details>


### [76] [DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration](https://arxiv.org/abs/2601.15260)
*Dominik Rößle,Xujun Xie,Adithya Mohan,Venkatesh Thirugnana Sambandham,Daniel Cremers,Torsten Schön*

Main category: cs.CV

TL;DR: DrivIng 是一个含完整地理参照数字孪生的大规模多模态自动驾驶感知数据集，覆盖约18公里多场景路线，并提供高频三维标注与基准评测，支持真实到仿真一对一迁移与系统化验证。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶感知数据集缺乏高保真数字孪生，限制了系统化测试、极端场景仿真、传感器改型及仿真到现实的评估能力；因此需要一个兼具大规模、多模态、全时段覆盖且与数字孪生 tightly-coupled 的数据集。

Method: 构建约18公里城市/郊区/高速混合路线的完整地理参照数字孪生；同步采集六路RGB相机、单线束LiDAR与高精度ADMA定位，跨白天/黄昏/夜间；以10 Hz为所有序列提供12类目标的3D包围框与轨迹ID标注，总计约120万实例；提供可将真实交通一比一映射到仿真的管线与基准评测、HD地图及代码。

Result: 得到一个具备全栈可复现性的多模态数据集与数字孪生资源，能够在仿真中保留真实交互并灵活构造场景；并对最先进感知模型进行了基准评测（摘要未给出具体数值）。

Conclusion: DrivIng 填补了高保真数字孪生数据集的空白，提升了感知算法在真实—仿真之间的可迁移性与验证严谨性；公开数据集、数字孪生、HD地图与代码，促进可重复研究与稳健评估。

Abstract: Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.

</details>


### [77] [RayRoPE: Projective Ray Positional Encoding for Multi-view Attention](https://arxiv.org/abs/2601.15275)
*Yu Wu,Minsik Jeon,Jen-Hao Rick Chang,Oncel Tuzel,Shubham Tulsiani*

Main category: cs.CV

TL;DR: 提出RayRoPE，一种基于射线且几何自适应的多视角Transformer位置编码，实现SE(3)不变的多频相似度，并支持不确定性下的期望编码，显著提升新视角合成与双目深度估计表现。


<details>
  <summary>Details</summary>
Motivation: 现有多视角Transformer的绝对或相对位置编码无法同时满足：对每个图像patch的唯一标识、实现SE(3)不变的注意力比较、兼容多频相似度、并能适应场景几何与传感器模态（如RGB-D）。

Method: 1) 用每个patch对应的相机射线来表征位置；2) 不直接用射线方向，而是预测射线上一点形成几何感知的编码；3) 在查询视角坐标系中计算投影坐标以获得SE(3)不变的多频相似度（类似RoPE的复数/旋转频域相位调制思想但在射线-投影域中）；4) 对预测3D点的不确定性解析地计算位置编码的期望；5) 可无缝融合RGB-D，将深度作为对射线点的强约束。

Result: 在CO3D等基准的新视角合成和双目深度任务上，相比替代位置编码显著提升；例如在CO3D上LPIPS相对提升约15%，RGB-D场景下收益更大。

Conclusion: RayRoPE满足多视角注意力对几何的关键需求：唯一性、SE(3)不变与多频相似度，并通过解析不确定性建模与对深度的兼容，成为更鲁棒、可扩展的多视角位置编码方案。

Abstract: We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.

</details>


### [78] [StableWorld: Towards Stable and Consistent Long Interactive Video Generation](https://arxiv.org/abs/2601.15281)
*Ying Yang,Zhengyao Lv,Tianlin Pan,Haofan Wang,Binxin Yang,Hubery Yin,Chen Li,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出StableWorld：在交互式视频生成中通过动态帧淘汰机制过滤退化帧、保留几何一致帧，从源头抑制误差累积，显著提升稳定性与时间一致性，且对模型无关，在多种框架上验证有效。


<details>
  <summary>Details</summary>
Motivation: 交互式视频生成虽能通过相机运动、文本等交互合成可控视频，但在长时交互中易出现不稳定与时间退化（空间漂移、场景崩塌）。作者发现误差主要源自同一场景中生成帧逐渐偏离初始干净状态并向后传播，因此需要一种机制在生成过程中抑制误差积累。

Method: 提出StableWorld的动态帧淘汰机制：在生成过程中持续评估帧质量与几何一致性，过滤（剔除）退化的、会导致漂移的帧，仅保留与场景结构一致的帧进入后续生成，从而从源头阻断误差累积。方法简单、可插拔、与底层模型无关。

Result: 在Matrix-Game、Open-Oasis、Hunyuan-GameCraft等多种交互式视频模型与场景上，StableWorld显著提升稳定性、时间一致性与泛化能力，减少空间漂移与场景崩塌。

Conclusion: 通过动态淘汰退化帧、保留几何一致帧，可有效抑制交互式视频生成中的误差累积，带来更稳定、时间一致的生成效果；方法通用且易集成，跨多模型与场景验证有效。

Abstract: In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.

</details>


### [79] [Rethinking Video Generation Model for the Embodied World](https://arxiv.org/abs/2601.15282)
*Yufan Deng,Zilin Pan,Hongyu Zhang,Xiaojie Li,Ruoqing Hu,Yufei Ding,Yiming Zou,Yan Zeng,Daquan Zhou*

Main category: cs.CV

TL;DR: 提出RBench机器人视频生成基准与RoVid-X大规模数据集，系统评测并提升物理真实感与任务正确性的生成能力，显著对齐人类评估（ρ=0.96），旨在加速具身智能发展。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成虽能为具身智能提供多样机器人数据，但难以生成符合现实物理与交互规律的高质量视频，且缺乏统一标准评测，阻碍模型公平比较与进步。

Method: 1) 构建RBench：覆盖5类任务、4种形态，设计可复现实验的多维子指标（结构一致性、物理可行性、动作完整度等），同时评估任务正确性与视觉保真度；2) 系统评测25个代表性模型；3) 基于评测洞见，提出四阶段数据管线，构建RoVid-X开源数据集（400万标注视频片段、数千任务、包含丰富物理属性标注）。

Result: 现有模型在生成物理真实的机器人行为上存在显著不足；RBench自动评分与人类评价高度一致（Spearman ρ=0.96）；发布RoVid-X作为目前规模最大的机器人视频生成数据集。

Conclusion: RBench提供了可靠的评测框架，但要实现物理真实仍需高质量训练数据支持。通过RBench与RoVid-X的协同，形成评测—数据闭环，为严格评估与可扩展训练奠定基础，加速具身智能向通用智能演进。

Abstract: Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.

</details>


### [80] [LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes](https://arxiv.org/abs/2601.15283)
*Ruofan Liang,Norman Müller,Ethan Weber,Duncan Zauss,Nandita Vijaykumar,Peter Kontschieder,Christian Richardt*

Main category: cs.CV

TL;DR: 提出一种从单次多视角采集中实现室内场景交互式光照编辑的方法：先生成式分解多光源，再在多视角中一致传播，并融入可重光照的3D Gaussian Splatting，实现实时对单个光源的开关、色度与强度控制，效果在合成与真实数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于单次场景捕获的室内重光照难以将复杂照明分解为可独立编辑的光源，容易出现多视角不一致、编辑自由度低或效率不足的问题；需要一种既能物理一致地分解光源、又能在3D表示中实时交互编辑的方法。

Method: 1) 设计生成式图像级光照分解模型，将室内复杂照明因子化为若干可控光源（状态/色度/强度）。2) 提出多视角光照协调，使分解在所有视角间一致传播。3) 将分解结果集成到可重光照的3D Gaussian Splatting表示，实现实时交互控制。4) 在合成与真实数据上进行定量与定性评估，并与SOTA比较。

Result: 在多种室内场景中实现高保真光照分解与重光照；支持实时、可交互地对单个光源进行独立控制；在合成与真实数据上定量与定性均优于现有技术，展示了视频与交互式演示。

Conclusion: 生成式光照分解结合多视角一致性与3D Gaussian Splatting，可在单次多视角捕获下实现高质量、实时的室内光源级别编辑，提供了实用的交互式重光照框架并超越现有方法。

Abstract: We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.

</details>


### [81] [Walk through Paintings: Egocentric World Models from Internet Priors](https://arxiv.org/abs/2601.15284)
*Anurag Bagchi,Zhipeng Bao,Homanga Bharadhwaj,Yu-Xiong Wang,Pavel Tokmakov,Martial Hebert*

Main category: cs.CV

TL;DR: EgoWM 将任意预训练视频扩散模型“注入动作条件”，实现可控的、物理一致的未来视频预测，并在结构一致性与速度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型能生成“合理”但未必与给定动作匹配的未来；从头训练世界模型代价高且泛化差。作者希望复用互联网尺度视频先验，实现能准确随动作演化的自我中心（第一视角）世界模型，并独立于外观衡量其物理一致性。

Method: 提出 Egocentric World Model（EgoWM）：在冻结/部分冻结的预训练视频扩散模型上，加入轻量级动作条件层以注入马达/关节指令；通过少量微调实现动作对视频演化的精确控制。方法架构无关，适配多种具身体与动作空间（3-DoF 移动到 25-DoF 人形）。引入结构一致性评分（SCS），评估稳定场景元素是否按照动作一致演化，从而将物理正确性与视觉外观解耦。

Result: 在导航与操作任务中产生连贯的长 rollout，仅需少量微调；SCS 相比先前最优导航世界模型提升最高达80%；推理延迟最高降低6倍；对未知环境具鲁棒泛化，甚至可在“画作内导航”。

Conclusion: 通过将动作条件注入预训练视频扩散模型，EgoWM 在不牺牲真实感的前提下实现对未来的可控、物理一致预测，跨具身体与动作维度良好扩展，并在准确性、速度与泛化上取得显著优势；SCS为评估物理一致性提供了新的客观指标。

Abstract: What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.

</details>


### [82] [Iterative Refinement Improves Compositional Image Generation](https://arxiv.org/abs/2601.15286)
*Shantanu Jaiswal,Mihir Prabhudesai,Nikash Bhardwaj,Zheyang Qin,Amir Zadeh,Chuan Li,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.CV

TL;DR: 提出一种在推理时进行多步迭代自我纠错的T2I生成策略，用VLM作为评论家在回路中给反馈，逐步修正图像以更好满足复杂提示的多对象、关系与属性约束；在多个基准上相对并行采样显著提升对齐与偏好率。


<details>
  <summary>Details</summary>
Motivation: 现有T2I在复杂组合性提示（多对象、关系、属性）下对齐不足。并行采样+验证器或增加去噪步数虽有提升，但当需同时满足众多约束时仍乏力。受到LLM链式思维逐步推理成功的启发，作者希望把“分步纠错”带入图像生成以提升组合对齐。

Method: 提出一个推理时的迭代 refinement 框架：在每一轮，用基础T2I模型生成/更新图像；用一个VLM担任critic，对当前生成与文本提示的匹配度给出反馈；根据反馈对下一轮生成进行有针对性的修正（无需外部工具或先验）。该策略可与多种图像生成器与VLM灵活结合，通过多步“顺序纠错”来分解复杂提示。

Result: 在多项基准上优于计算量匹配的并行采样：ConceptMix (k=7) all-correct率提升16.9%；T2I-CompBench（3D-Spatial）提升13.8%；Visual Jenga场景分解提升12.5%；人类偏好评测中，方法被偏好58.7%（对比并行基线41.3%）。

Conclusion: 迭代自我纠错是一种普适且简洁的组合型图像生成原则：通过VLM引导的多步细化，能更忠实地满足复杂提示约束，兼具通用性与实证收益；为复杂T2I对齐提供了一条无需额外先验的有效路径。

Abstract: Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/

</details>


### [83] [Towards Understanding Best Practices for Quantization of Vision-Language Models](https://arxiv.org/abs/2601.15287)
*Gautom Das,Vincent La,Ethan Lau,Abhinav Shrivastava,Matthew Gwilliam*

Main category: cs.CV

TL;DR: 研究评估在多模态管线（视觉模型、语言模型及连接器）上应用多种量化方法（含GPTQ与AWQ）如何影响性能与效率，发现ViT与LLM对整体性能同等关键，且LLM可在更低比特下仍保持高精度，为高效部署MLLM提供实践指南。


<details>
  <summary>Details</summary>
Motivation: SOTA大模型推理需高端GPU与大显存，成本与延迟高；现有量化多集中于语言或视觉单模态/半精度，缺乏对多模态流水线各组件与任务的系统性评估与可操作建议。

Method: 构建多模态流水线（ViT + LLM + Connector），在图像描述、检索、视觉问答等任务上，系统比较不同位宽（含低于半精度）、不同量化策略（GPTQ、AWQ等），并分组件（ViT、LLM、连接器）逐段量化与组合量化，评估精度、bpw、延迟/显存占用。

Result: 1) ViT与LLM对最终性能的重要性相当，尽管参数量差异大；2) 对LLM进行更低位量化能在较低bpw下保持高准确率；3) 不同组件与任务对位宽与方法的敏感性不同；4) 给出在不同任务下的实用量化配置以平衡精度与效率。

Conclusion: 通过细粒度对比证明在MLLM中既要关注ViT也要关注LLM的量化策略；LLM低比特量化具有显著性价比。研究为实际部署提供指导，并强调探索组件敏感性以达成最佳效率-精度折中。

Abstract: Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.

</details>


### [84] [APPLE: Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping](https://arxiv.org/abs/2601.15288)
*Jiwon Kang,Yeji Choi,JoungBin Lee,Wooseok Jang,Jinhyeok Choi,Taekeun Kang,Yongjae Park,Myungin Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出APPLE：一种用于人脸换脸的属性保持伪标注扩散框架，通过教师-学生学习在无真实GT下同时强化身份迁移与目标属性保真，达成SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 扩散法基于掩膜修复会丢失目标外观线索，导致属性错配；缺少真实GT使得身份与属性难以兼顾，因此需要一种在无GT条件下仍能显式监督并保留细节属性的方法。

Method: 将换脸重构为“条件去模糊”问题：以带有目标外观线索的退化/模糊条件进行扩散生成，辅以属性感知的反演(inversion)以回收光照、肤色、妆容等细节；构建教师-学生框架，教师通过属性保持设计生成高质量伪三元组（源身份、目标图、合成结果），学生以此接受直接换脸监督，提升属性与身份的一致性。

Result: 在身份一致性与属性保真两方面达到SOTA，生成更真实、与目标更一致的视觉效果；相较基线，显著减少因掩膜导致的属性错配与伪影。

Conclusion: 通过属性感知的伪标注与条件去模糊的扩散建模，APPLE在无真实GT环境下有效兼顾身份迁移与目标属性保持，展示了实用且可扩展的换脸训练范式。

Abstract: Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup. However, since real ground truth for face swapping is unavailable, achieving both accurate identity transfer and high-quality attribute preservation remains challenging. In addition, recent diffusion-based approaches attempt to improve visual fidelity through conditional inpainting on masked target images, but the masked condition removes crucial appearance cues of target, resulting in plausible yet misaligned attributes. To address these limitations, we propose APPLE (Attribute-Preserving Pseudo-Labeling), a diffusion-based teacher-student framework that enhances attribute fidelity through attribute-aware pseudo-label supervision. We reformulate face swapping as a conditional deblurring task to more faithfully preserve target-specific attributes such as lighting, skin tone, and makeup. In addition, we introduce an attribute-aware inversion scheme to further improve detailed attribute preservation. Through an elaborate attribute-preserving design for teacher learning, APPLE produces high-quality pseudo triplets that explicitly provide the student with direct face-swapping supervision. Overall, APPLE achieves state-of-the-art performance in terms of attribute preservation and identity transfer, producing more photorealistic and target-faithful results.

</details>
