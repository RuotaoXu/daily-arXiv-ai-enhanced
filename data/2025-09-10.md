<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis](https://arxiv.org/abs/2509.06986)
*Cedric Caruzzo,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出CellPainTR，一种Transformer模型，通过源域上下文token缓解批次效应，实现对未见数据集的OOD泛化，在JUMP上优于ComBat/Harmony，并在Bray等未见数据上保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大规模细胞表型成像数据存在批次效应且现有模型缺乏可泛化能力，跨研究整合与生物信号保留受限，需要一种无需重训即可在未见数据上稳健表现的基础表征模型。

Method: 设计Transformer架构CellPainTR，引入“源特异上下文token”编码数据来源条件，学习对批次鲁棒的细胞形态表示；在JUMP数据上进行训练/验证，并与ComBat、Harmony比较；进行严苛的OOD测试（Bray等数据集），评估批次整合与生物信号保留。

Result: 在JUMP数据集上，CellPainTR在批次整合效果与生物信号保留指标上均优于ComBat和Harmony；在与训练分布差异显著的Bray等未见数据上仍保持较高性能，展现出强鲁棒性与泛化能力。

Conclusion: CellPainTR作为图像表型分析的基础模型候选，能够在无微调的情况下跨数据集泛化，有效应对批次效应，促进可靠且可扩展的跨研究生物学分析。

Abstract: Large-scale biological discovery requires integrating massive, heterogeneous
datasets like those from the JUMP Cell Painting consortium, but technical batch
effects and a lack of generalizable models remain critical roadblocks. To
address this, we introduce CellPainTR, a Transformer-based architecture
designed to learn foundational representations of cellular morphology that are
robust to batch effects. Unlike traditional methods that require retraining on
new data, CellPainTR's design, featuring source-specific context tokens, allows
for effective out-of-distribution (OOD) generalization to entirely unseen
datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP
dataset, where it outperforms established methods like ComBat and Harmony in
both batch integration and biological signal preservation. Critically, we
demonstrate its robustness through a challenging OOD task on the unseen Bray et
al. dataset, where it maintains high performance despite significant domain and
feature shifts. Our work represents a significant step towards creating truly
foundational models for image-based profiling, enabling more reliable and
scalable cross-study biological analysis.

</details>


### [2] [FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection](https://arxiv.org/abs/2509.06987)
*Alexey Zhukov,Jenny Benois-Pineau,Amira Youssef,Akka Zemmari,Mohamed Mosbah,Virginie Taillandier*

Main category: cs.CV

TL;DR: 提出基于YOLO与ViT及音频特征的多模态融合框架，用于铁路裂纹/表面缺陷检测，相比仅视觉方法精度与总体准确率提升约0.2，并经t检验显著。


<details>
  <summary>Details</summary>
Motivation: 单一视觉检测(如YOLO)在外观相似的正常结构与缺陷之间易误检，尤其在铁路场景中。实际场景常有麦克风采集的非语义音频信号，可能包含与缺陷相关的动态/接触信息，亟需利用音频与图像互补以降低过检并提升稳健性。

Method: 构建规则驱动的多模态融合架构：以YOLOv8n进行快速目标检测；以ViT提取第7/16/19层多尺度特征并与合成的音频表示进行融合；关注两个缺陷类别（钢轨断裂与表面缺陷）。融合机制在图像与音频模态之间进行，并以领域规则指导融合与判决。

Result: 在真实铁路数据集上，相比仅用视觉的基线，精度与整体准确率提升约0.2个百分点；采用非配对学生t检验验证平均准确率差异具有统计显著性。

Conclusion: 多模态图像-音频融合能在铁路缺陷检测中有效减少过检并小幅提升整体性能，证明音频特征对视觉检测的互补价值；方法在轻量YOLOv8n与ViT结合下实现，具备一定实用性。

Abstract: Multimodal fusion is a multimedia technique that has become popular in the
wide range of tasks where image information is accompanied by a signal/audio.
The latter may not convey highly semantic information, such as speech or music,
but some measures such as audio signal recorded by mics in the goal to detect
rail structure elements or defects. While classical detection approaches such
as You Only Look Once (YOLO) family detectors can be efficiently deployed for
defect detection on the image modality, the single modality approaches remain
limited. They yield an overdetection in case of the appearance similar to
normal structural elements. The paper proposes a new multimodal fusion
architecture built on the basis of domain rules with YOLO and Vision
transformer backbones. It integrates YOLOv8n for rapid object detection with a
Vision Transformer (ViT) to combine feature maps extracted from multiple layers
(7, 16, and 19) and synthesised audio representations for two defect classes:
rail Rupture and Surface defect. Fusion is performed between audio and image.
Experimental evaluation on a real-world railway dataset demonstrates that our
multimodal fusion improves precision and overall accuracy by 0.2 points
compared to the vision-only approach. Student's unpaired t-test also confirms
statistical significance of differences in the mean accuracy.

</details>


### [3] [Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection](https://arxiv.org/abs/2509.06988)
*Yingsheng Wang,Shuo Lu,Jian Liang,Aihua Zheng,Ran He*

Main category: cs.CV

TL;DR: 提出ClaFR：仅基于训练好分类器的权重、无需训练数据的后验（post-hoc）OOD检测方法，通过子空间投影与重构误差进行判别，性能达SOTA级别。


<details>
  <summary>Details</summary>
Motivation: 现有特征空间后验OOD方法常需访问训练数据，不适用于隐私受限场景；希望在不改动已训练模型、也不访问训练数据的前提下实现有效OOD检测。

Method: 将分类器权重做正交分解以提取“已知类别子空间”；把样本特征投影/映射到该子空间得到重构特征；以原始特征与重构特征之间的重构误差作为OOD分数，误差大者更可能OOD。整个流程不需训练数据，仅依赖已训练分类器权重与提取的特征。

Result: 在多个OOD基准上取得领先或竞争性的表现，同时避免了对训练数据的依赖；属于简单高效的post-hoc方法。

Conclusion: 基于分类器权重构造的子空间重构可有效区分ID与OOD样本，在数据隐私受限场景尤具实用性；提供代码以复现。

Abstract: Out-of-distribution (OOD) detection helps models identify data outside the
training categories, crucial for security applications. While feature-based
post-hoc methods address this by evaluating data differences in the feature
space without changing network parameters, they often require access to
training data, which may not be suitable for some data privacy scenarios. This
may not be suitable in scenarios where data privacy protection is a concern. In
this paper, we propose a simple yet effective post-hoc method, termed
Classifier-based Feature Reconstruction (ClaFR), from the perspective of
subspace projection. It first performs an orthogonal decomposition of the
classifier's weights to extract the class-known subspace, then maps the
original data features into this subspace to obtain new data representations.
Subsequently, the OOD score is determined by calculating the feature
reconstruction error of the data within the subspace. Compared to existing OOD
detection algorithms, our method does not require access to training data while
achieving leading performance on multiple OOD benchmarks. Our code is released
at https://github.com/Aie0923/ClaFR.

</details>


### [4] [DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining](https://arxiv.org/abs/2509.06990)
*Bryan Rodas,Natalie Montesino,Jakob Ambsdorf,David Klindt,Randall Balestriero*

Main category: cs.CV

TL;DR: 提出DIET-CP，一种极简的持续预训练策略，可在小数据目标域上无标签地将强大的基础模型（如DINOv3）对齐到新分布，仅需约1000张图像即可显著提升性能，且几乎不增加超参、对模态与骨干稳定。


<details>
  <summary>Details</summary>
Motivation: 专业领域数据稀缺，难以沿用大规模自监督预训练（SSL）的方法与超参搜索；同时开源预训练模型常仅提供骨干权重，缺乏继续预训练所需信息，导致迁移到新域受限。

Method: 提出DIET-CP：在目标域小样本上继续预训练，采用极简无标签目标（未详述细节，但强调无需新增超参，复杂度类似有监督微调），可直接作用于不同模态与不同骨干（如DINOv3）。

Result: 在仅1000张图像下，即可为SOTA基础模型（如DINOv3）带来显著性能提升；方法在多模态与多骨干上表现稳定。

Conclusion: DIET-CP以极低门槛和稳定性解决小数据目标域的持续预训练难题，成为在缺少标签与超参搜索条件下对齐基础模型至新分布的有效方案。

Abstract: Continued pretraining offers a promising solution for adapting foundation
models to a new target domain. However, in specialized domains, available
datasets are often very small, limiting the applicability of SSL methods
developed for large-scale pretraining and making hyperparameter search
infeasible. In addition, pretrained models are usually released as
backbone-weights only, lacking important information to continue pretraining.
We propose to bridge this gap with DIET-CP, a simple continued pretraining
strategy, where any strong foundation model can be steered towards the new data
distribution of interest. DIET-CP relies on a very simple objective, requires
no labels, and introduces no more hyperparameters than supervised finetuning.
It is stable across data modalities and backbone choices, while providing a
significant performance boost for state-of-the-art models such as DINOv3 using
only 1000 images.

</details>


### [5] [FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2509.06992)
*Kun Zhai,Siheng Chen,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出FedAPT，在联邦提示微调中提升对抗鲁棒性，通过全局标签嵌入引导的类感知提示生成器与跨层生成器共享，缓解非IID下的类信息鸿沟，显著提升对抗鲁棒性与跨域泛化。


<details>
  <summary>Details</summary>
Motivation: FPT高效但在对抗攻击下易被误导，尤其在联邦非IID场景中，本地客户端仅见局部标签，导致生成的对抗样本和提示与全局任务不匹配，形成“类信息鸿沟”，使全局模型难以抵御针对全局标签空间的攻击。

Method: 提出FedAPT：1) 类感知提示生成器：以文本提示为条件生成视觉提示，并由“全局标签嵌入”作为灯塔编码跨客户端标签信息，以对齐本地与全局标签分布；2) 跨层生成器共享：在模型多层共享生成器或其参数，强化多层提示的耦合性与一致性，提高鲁棒性与稳定性；3) 在联邦训练中用对抗样本（基于全局引导）进行提示微调。

Result: 在多种图像分类数据集上，FedAPT在对抗鲁棒性指标上大幅优于现有方法；同时在跨域与跨数据集迁移上显示出更好的泛化能力。

Conclusion: 通过引入全局标签知识与跨层共享机制，FedAPT有效弥合联邦非IID下的类信息鸿沟，显著提升VLM联邦提示微调的对抗鲁棒性与泛化，适用于真实世界场景。

Abstract: Federated Prompt Tuning (FPT) is an efficient method for cross-client
collaborative fine-tuning of large Vision-Language Models (VLMs). However,
models tuned using FPT are vulnerable to adversarial attacks, leading to
misclassification in downstream tasks. In this work, we introduce Federated
Adversarial Prompt Tuning (\textbf{FedAPT}), a novel method designed to enhance
the adversarial robustness of FPT. We identify a key issue in FedAPT under
non-independent and identically distributed (non-IID) settings: a \textit{class
information gap} between clients and the global model. Clients rely solely on
limited local label information to generate adversarial samples for training,
while the global model must defend against adversarial attacks from global
labels. To address this issue, we propose a \textbf{class-aware prompt
generator} that generates visual prompts from text prompts. This generator is
guided by a \emph{Global Label Embedding} (serving as a ``beacon") which
encodes cross-client label information to create more globally-aligned visual
prompts. Additionally, we propose a \textbf{cross-layer generator sharing}
strategy to enhance prompt coupling across different layers of the model,
further boosting adversarial robustness. Extensive experiments on multiple
image classification datasets demonstrate the superiority of FedAPT in
improving adversarial robustness, outperforming existing methods by a large
margin. FedAPT also exhibits exceptional generalization in cross-domain and
cross-dataset scenarios, indicating its effectiveness in real-world
applications.

</details>


### [6] [Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)](https://arxiv.org/abs/2509.06993)
*Zirui Xu,Raphael Tang,Mike Bianco,Qi Zhang,Rishi Madhok,Nikolaos Karianakis,Fuxun Yu*

Main category: cs.CV

TL;DR: 该报告介绍在CVPR 2025 EarthVision Embed2Scale 挑战中获得Top-1的方案，面向将SSL4EO-S12高光谱地理数据立方体嵌入为通用向量以服务多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有地理遥感大模型对高光谱/多维时空数据的通用表征不足，缺乏可泛化的嵌入以支持分类、回归等多任务；比赛推动构建能高效、可扩展、可迁移的地理基础嵌入模型。

Method: 提出一种将SSL4EO-S12高光谱数据立方体编码为低维嵌入的方案（细节未给出），应包括数据预处理、时空/光谱特征建模与对比式或自监督学习策略，以及适配多任务的投影头或检索友好损失。

Result: 该方法在Embed2Scale挑战中获得Top-1成绩，优于其他参赛方法。

Conclusion: 通过所提通用嵌入模型，可有效支持多种下游任务，展示了构建地理基础模型的可行性与优越性。

Abstract: EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational
geospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into
embedding vectors that faciliatetes various downstream tasks, e.g.,
classification, regression, etc. In this technical report, we introduce our
proposed method for the Top-1 winning solution on the Embed2Scale Challenge.

</details>


### [7] [VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality](https://arxiv.org/abs/2509.06994)
*Srihari Bandraupalli,Anupam Purwar*

Main category: cs.CV

TL;DR: ViLD提出一个面向企业实际应用的VLM评测框架与数据集，覆盖10类业务关键任务，并引入BlockWeaver算法解决VLM OCR输出无序分组对齐难题；在7,500条真实分布样本上，结合多种匹配与度量，对多款开源VLM与专有基线进行行业化对比，给出可部署洞见。


<details>
  <summary>Details</summary>
Motivation: 学术评测与企业落地之间存在鸿沟：现有基准偏重选择题与合成数据，难以反映真实业务（如社媒内容分析）的复杂性与合规需求。企业需要覆盖多任务、可操作的评估与可解释指标，以指导模型选型与部署。

Method: 1) 提出ViLD框架：定义10类企业关键视觉语言任务；2) 构建真实分布基准：从百万级图像/视频中分层抽取7,500样本；3) 评估设计：结合语义匹配（向量/LLM判别）、传统检测指标与新颖的描述完整性与忠实度度量；4) 提出BlockWeaver：无需嵌入或LLM，即可高效稳健地对齐VLM产生的无序、可变分组的OCR片段；5) 在Qwen、MIMO、InternVL与强力专有基线上进行对比实验。

Result: BlockWeaver在对齐OCR输出上实现高速度与高可靠性；ViLD在多任务、多维度指标下对开源VLM与专有模型给出差异化表现与可执行洞见，揭示开源模型在部分企业任务上的优势与不足。

Conclusion: ViLD为企业场景提供了贴近实操的VLM评测范式与数据资源，结合BlockWeaver与多维指标体系，能更准确衡量和比较模型在真实应用中的有效性，为模型选型、风险控制与部署优化提供依据。

Abstract: Open-source Vision-Language Models show immense promise for enterprise
applications, yet a critical disconnect exists between academic evaluation and
enterprise deployment requirements. Current benchmarks rely heavily on
multiple-choice questions and synthetic data, failing to capture the complexity
of real-world business applications like social media content analysis. This
paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge
this gap by evaluating VLMs on operational enterprise requirements. We define
ten business-critical tasks: logo detection, OCR, object detection, human
presence and demographic analysis, human activity and appearance analysis,
scene detection, camera perspective and media quality assessment, dominant
colors, comprehensive description, and NSFW detection. To this framework, we
bring an innovative BlockWeaver Algorithm that solves the challenging problem
of comparing unordered, variably-grouped OCR outputs from VLMs without relying
on embeddings or LLMs, achieving remarkable speed and reliability. To
demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500
diverse samples, carefully stratified from a corpus of one million real-world
images and videos. ViLD provides actionable insights by combining semantic
matching (both embedding-based and LLM-as-a-judge approaches), traditional
metrics, and novel methods to measure the completeness and faithfulness of
descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and
InternVL) against a powerful proprietary baseline as per ViLD framework, we
provide one of the first industry-grounded, task-driven assessment of VLMs
capabilities, offering actionable insights for their deployment in enterprise
environments.

</details>


### [8] [The Protocol Genome A Self Supervised Learning Framework from DICOM Headers](https://arxiv.org/abs/2509.06995)
*Jimmy Joseph*

Main category: cs.CV

TL;DR: 提出“Protocol Genome”自监督框架，将结构化DICOM头信息与影像联合建模，通过对比学习、掩码协议预测和协议翻译，学习对协议敏感但临床稳健的表征；在多中心外部验证上显著提升AUROC与校准（ECE），跨模态与厂商具备更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学影像在不同机构/设备/协议下存在显著潜在混杂（如机型、序列、重建核、kVp、TR/TE、层厚），导致仅用图像训练的模型跨域泛化差、校准差。DICOM头虽常被忽略，但包含这些协议变量；若将其作为“弱标签/上下文”，可帮助模型学习协议感知的稳健特征，从而减少跨站点性能下降与边界假阳性。

Method: 利用1.26M多机构多厂商多模态影像，提取脱敏的DICOM头字段进行分词嵌入，并与图像特征联合训练，包含三种自监督目标：1）协议-图像对比学习，拉近对应配对；2）掩码协议预测，从部分头信息恢复缺失字段；3）协议-协议翻译，建模不同头字段间关系。与SimCLR、MAE及ImageNet迁移等强基线比较，并在三项下游任务（胸部CT肺栓塞分诊、脑MRI胶质瘤分级、胸片心脏增大检测）评估。

Result: 在完全独立外部验证上AUROC达0.901（基线0.847），ECE降至0.036（基线0.058），跨CT/MRI/CXR与多厂商更稳健。三个任务外部AUROC分别提升约+0.046、+0.058、+0.041；校准改善25–37%（DeLong检验p<0.01）。在仅10–20%标注下仍保持收益，并减少协议边界的假阳性。

Conclusion: 将DICOM头作为结构化“协议标签”与影像联合自监督，可显著提升跨域泛化与校准，适配临床PACS工作流（C-FIND/C-MOVE、QIDO/WADO），并配套模型卡、部署指南、去标识与偏倚审计；方法有效但任务依赖性存在，需要进一步拓展验证。

Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning
system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs
0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.
Our method also improves calibration and robustness across modalities (CT, MRI,
CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where
procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice
thickness) have consequences for contrast, noise, and artifact. These latent
confounders impede the generalization of image-only networks across sites. We
consider structured DICOM headers as a label and learn protocol-aware but
clinically robust image representations. Protocol Genome obtains tokenized
embeddings of de-identified header fields and models them along with image
features using: (1) protocol-image contrastive learning, (2) masked protocol
prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health
systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT
triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph
cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well
as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:
cardiomegaly) is associated with higher external AUROC; 25-37% calibration
improvements are obtained (p < 0.01, DeLong tests). While the gains may be
task-dependent, they are preserved with 10-20% of labeled data. From a clinical
point of view, the technique reduces false positives at protocol borders and is
applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a
model card and deployment guide, complete with both de-identification and bias
audits.

</details>


### [9] [Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems](https://arxiv.org/abs/2509.06996)
*Jie Zhang,Ting Xu,Gelei Deng,Runyi Hu,Han Qiu,Tianwei Zhang,Qing Guo,Ivor Tsang*

Main category: cs.CV

TL;DR: 研究评估先进视觉-语言模型在“可见但不可读”扰动文本上的鲁棒性，发现其在中文与英文书写系统下显著失稳，暴露出对组合性先验与符号绑定的不足。


<details>
  <summary>Details</summary>
Motivation: 人类对破碎、融合、遮挡字符仍能读懂文字，显示强鲁棒性；想了解当前VLM是否具备类似能力，并为多模态在教育、无障碍、文博与安全等场景的可靠部署提供依据。

Method: 构建心理物理学风格基准：针对中文表意文字与英文字母词，通过切分、重组、叠加字形生成对人类可读而对模型“可见但不可读”的刺激；提供统一的生成代码、提示词与评测协议，比较模型在干净文本与扰动文本上的表现。

Result: 尽管在干净文本上表现强，现有VLM在扰动条件下性能大幅下降，输出常与输入无关或不连贯；表现模式显示模型更多依赖通用视觉不变性，而不足以利用文字的组合性与结构先验。

Conclusion: 当前VLM缺乏稳健读写所需的符号分割、组成与绑定机制；需在架构与训练策略中显式编码这些能力。研究发布可复现实验资源，并指出在教育、无障碍、文化遗产与安全应用中的具体挑战。

Abstract: Writing is a universal cultural technology that reuses vision for symbolic
communication. Humans display striking resilience: we readily recognize words
even when characters are fragmented, fused, or partially occluded. This paper
investigates whether advanced vision language models (VLMs) share this
resilience. We construct two psychophysics inspired benchmarks across distinct
writing systems, Chinese logographs and English alphabetic words, by splicing,
recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli
for models while remaining legible to humans. Despite strong performance on
clean text, contemporary VLMs show a severe drop under these perturbations,
frequently producing unrelated or incoherent outputs. The pattern suggests a
structural limitation: models heavily leverage generic visual invariances but
under rely on compositional priors needed for robust literacy. We release
stimuli generation code, prompts, and evaluation protocols to facilitate
transparent replication and follow up work. Our findings motivate architectures
and training strategies that encode symbol segmentation, composition, and
binding across scripts, and they delineate concrete challenges for deploying
multimodal systems in education, accessibility, cultural heritage, and
security.

</details>


### [10] [K-Syn: K-space Data Synthesis in Ultra Low-data Regimes](https://arxiv.org/abs/2509.06997)
*Guan Yu,Zhang Jianhua,Liang Dong,Liu Qiegen*

Main category: cs.CV

TL;DR: 在超低数据场景下，提出在频域（k-space）进行特征级学习并结合时间融合策略作为生成式引导，合成高质量、多样的心脏动态MRI k-space 数据，以改善重建鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动态CMR受限于获取高质量且多样的k-space数据，数据稀缺导致重建不稳健。需要一种能在小样本下仍能有效学习与生成的数据增强/合成方法。

Method: 将频域视为全局特征空间：直接在k-space进行特征级建模与学习，避免传统图像域像素级卷积的局部性限制；利用傅里叶变换的全局表征能力进行稳定、丰富的生成。在此基础上，提出多种跨时间帧的k-space融合策略（时间融合）作为生成式引导，优化生成轨迹并合成时序一致的k-space数据。

Result: 实验表明该方法在低数据量条件下仍具有强生成能力，能够生成高质量且多样的k-space数据，并提升动态MRI重建的鲁棒性。

Conclusion: 频域特征级建模结合时间融合可有效缓解动态CMR数据稀缺问题，为小样本条件下的动态MRI重建提供实用的生成式数据合成方案。

Abstract: Owing to the inherently dynamic and complex characteristics of cardiac
magnetic resonance (CMR) imaging, high-quality and diverse k-space data are
rarely available in practice, which in turn hampers robust reconstruction of
dynamic cardiac MRI. To address this challenge, we perform feature-level
learning directly in the frequency domain and employ a temporal-fusion strategy
as the generative guidance to synthesize k-space data. Specifically, leveraging
the global representation capacity of the Fourier transform, the frequency
domain can be considered a natural global feature space. Therefore, unlike
traditional methods that use pixel-level convolution for feature learning and
modeling in the image domain, this letter focuses on feature-level modeling in
the frequency domain, enabling stable and rich generation even with ultra
low-data regimes. Moreover, leveraging the advantages of feature-level modeling
in the frequency domain, we integrate k-space data across time frames with
multiple fusion strategies to steer and further optimize the generative
trajectory. Experimental results demonstrate that the proposed method possesses
strong generative ability in low-data regimes, indicating practical potential
to alleviate data scarcity in dynamic MRI reconstruction.

</details>


### [11] [Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories](https://arxiv.org/abs/2509.06998)
*Liviu Nicolae Fircă,Antonio Bărbălau,Dan Oneata,Elena Burceanu*

Main category: cs.CV

TL;DR: 提出在语义与感知均不相似的类别间测试“属性”可迁移性的评估框架；通过多种去相关化的训练/测试划分策略，发现当前模型对划分极其敏感，性能随相关性降低而急剧下滑；聚类式划分在去相关与可学性间最均衡。


<details>
  <summary>Details</summary>
Motivation: 以往属性预测大多局限在同一门类或视觉相近的对象（如同一纲目），难以判断模型是否真正学到抽象属性而非记忆或利用类间相关性。作者动机是构建一个能检验模型在“概念上遥远”的类别之间抽象并迁移属性知识（如“有四条腿”同时适用于狗和椅子）的基准与评估方法。

Method: 提出一组系统的训练-测试划分策略以逐步降低类间相关性：1）基于LLM的语义分组；2）基于嵌入相似度阈值的过滤；3）基于嵌入的无监督聚类划分；4）利用真值超类标签的分区。然后在这些不同相关性水平下评测属性预测模型的鲁棒性与泛化能力。

Result: 当训练与测试类别相关性降低时，模型性能显著下滑，显示出对划分设计的强敏感性。在多种策略中，基于嵌入的聚类划分在“减少隐含相关性”与“保持可学习性”之间取得最佳折中。

Conclusion: 当前表示与模型在跨语义/感知不相似类别的属性抽象与迁移上存在明显局限；合理的划分策略对评测至关重要。聚类划分值得作为构建更严谨属性推理基准的优先选择，并为未来改进模型表征与基准设计提供方向。

Abstract: Can models generalize attribute knowledge across semantically and
perceptually dissimilar categories? While prior work has addressed attribute
prediction within narrow taxonomic or visually similar domains, it remains
unclear whether current models can abstract attributes and apply them to
conceptually distant categories. This work presents the first explicit
evaluation for the robustness of the attribute prediction task under such
conditions, testing whether models can correctly infer shared attributes
between unrelated object types: e.g., identifying that the attribute "has four
legs" is common to both "dogs" and "chairs". To enable this evaluation, we
introduce train-test split strategies that progressively reduce correlation
between training and test sets, based on: LLM-driven semantic grouping,
embedding similarity thresholding, embedding-based clustering, and
supercategory-based partitioning using ground-truth labels. Results show a
sharp drop in performance as the correlation between training and test
categories decreases, indicating strong sensitivity to split design. Among the
evaluated methods, clustering yields the most effective trade-off, reducing
hidden correlations while preserving learnability. These findings offer new
insights into the limitations of current representations and inform future
benchmark construction for attribute reasoning.

</details>


### [12] [Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models](https://arxiv.org/abs/2509.07010)
*Ahmed R. Sadik,Mariusz Bujny*

Main category: cs.CV

TL;DR: 提出一个人类参与的量化评估框架，用于衡量LLM生成的3D CAD模型相对于真值的几何与结构一致性；通过多种相似度与复杂度指标及多模态输入实验，发现语义信息越丰富，重建越精确，代码级提示可实现全指标完美重建。


<details>
  <summary>Details</summary>
Motivation: 当前LLM可从多模态输入生成复杂3D形状，但缺乏稳健的量化评估方法；实践需求（CAD普惠化、遗留设计逆向、快速原型）迫切需要可比可复现的测度来评价与加速迭代。

Method: 构建“人类在环”评估框架，提出并组合体积准确度、表面对齐、尺寸保真、拓扑复杂度等指标，将生成模型与真值CAD比对；以L形支架为基准任务，比较四种输入模态（2D正投影、轴测草图、几何结构树、基于代码的纠错提示）的生成表现。

Result: 随着输入语义丰富度增加，生成保真度提升；在代码级纠错提示下，四类指标上实现与真值的一致（“完美重建”）；量化评估相较仅依赖目测直觉，可显著加速向真值收敛。

Conclusion: 该框架为AI辅助形状合成提供了可扩展、可量化的验证方法；通过多指标与人类在环策略，可更快更可靠地逼近真值，并适用于多样CAD应用与后续生成模型的迭代优化。

Abstract: Large Language Models are increasingly capable of interpreting multimodal
inputs to generate complex 3D shapes, yet robust methods to evaluate geometric
and structural fidelity remain underdeveloped. This paper introduces a human in
the loop framework for the quantitative evaluation of LLM generated 3D models,
supporting applications such as democratization of CAD design, reverse
engineering of legacy designs, and rapid prototyping. We propose a
comprehensive suite of similarity and complexity metrics, including volumetric
accuracy, surface alignment, dimensional fidelity, and topological intricacy,
to benchmark generated models against ground truth CAD references. Using an L
bracket component as a case study, we systematically compare LLM performance
across four input modalities: 2D orthographic views, isometric sketches,
geometric structure trees, and code based correction prompts. Our findings
demonstrate improved generation fidelity with increased semantic richness, with
code level prompts achieving perfect reconstruction across all metrics. A key
contribution of this work is demonstrating that our proposed quantitative
evaluation approach enables significantly faster convergence toward the ground
truth, especially compared to traditional qualitative methods based solely on
visual inspection and human intuition. This work not only advances the
understanding of AI assisted shape synthesis but also provides a scalable
methodology to validate and refine generative models for diverse CAD
applications.

</details>


### [13] [MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning](https://arxiv.org/abs/2509.07021)
*Jiarui Chen,Yikeng Chen,Yingshuang Zou,Ye Huang,Peng Wang,Yuan Liu,Yujing Sun,Wenping Wang*

Main category: cs.CV

TL;DR: 提出MEGS²：通过同时减少3DGS原语数量与每个原语参数量，实现渲染与存储双重内存压缩；以面向任意方向的球面高斯色彩替代球谐，并用统一的软剪枝把原语数与“瓣”（lobe）数剪枝表述为同一受约束优化问题；在保持画质下，静态VRAM降50%，渲染VRAM降40%。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩多聚焦于存储压缩，忽视渲染阶段的显存瓶颈；球谐系数占用大，阻碍在边缘设备部署。需要一种同时面向原语规模与参数规模的内存高效方案。

Method: 1) 用轻量的任意取向球面高斯lobe替代内存占用高的球谐作为颜色表示；2) 提出统一软剪枝框架，将原语数量剪枝与lobe数量剪枝建模为同一受约束优化问题，联合训练与剪枝以压缩内存；3) 端到端优化确保在压缩同时保持画质。

Result: 在与现有方法对比中，MEGS²实现静态显存减少约50%、渲染显存减少约40%，同时保持可比的渲染质量。

Conclusion: 联合优化原语数与每原语参数、并以球面高斯lobe替代球谐，可在不显著损失质量的前提下显著降低3DGS的静态与渲染显存，提升在边缘设备上的实用性。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

</details>


### [14] [Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models](https://arxiv.org/abs/2509.07027)
*Jisung Hwang,Jaihoon Kim,Minhyuk Sung*

Main category: cs.CV

TL;DR: 提出一种统一的“标准高斯性”正则化损失，将高维样本视为一维标准高斯变量，通过空间域的矩（均值、方差、偏度、峰度等）匹配与频域功率谱匹配联合约束，使潜空间更接近标准高斯；随机置换输入以保证置换不变。该框架涵盖并改进既有高斯性正则（矩法、协方差匹配），在文本到图像模型的潜空间优化中提升审美与对齐，避免奖励黑客并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型的潜空间优化（如测试时基于奖励的对齐）常假设潜变量接近标准高斯；若偏离，会导致优化不稳、奖励黑客和泛化差。现有高斯性正则要么只约束低阶矩、要么用空间域协方差匹配（计算昂贵）。需要一种兼具准确性、效率与置换不变性的统一方法。

Method: - 将高维样本各元素视为独立一维标准高斯目标；
- 设计复合损失：
  1) 空间域矩正则：匹配已知解析期望的各阶矩（至少到四阶，亦可扩展）。
  2) 频域功率谱正则：匹配标准高斯应满足的功率谱分布（等价于在空间域做协方差匹配，但更高效）。
- 随机置换输入后再施加损失，确保置换不变；
- 证明既有高斯性正则是该框架的特例（某些阶矩损失或协方差匹配≈频域损失）。
- 将该正则用于测试时的奖励对齐优化（审美与文本对齐）。

Result: 在文本到图像潜空间的测试时优化中：
- 美学评分与文本对齐指标优于以往高斯正则；
- 显著抑制奖励黑客；
- 收敛更快、计算更高效（频域代替空间域协方差匹配）。

Conclusion: 统一的高斯性正则框架通过空间矩与频域功率谱联合，既提升了表达能力又降低了计算成本，适合用于潜空间优化等下游任务，较现有方法更稳健并能避免奖励黑客。

Abstract: We propose a novel regularization loss that enforces standard Gaussianity,
encouraging samples to align with a standard Gaussian distribution. This
facilitates a range of downstream tasks involving optimization in the latent
space of text-to-image models. We treat elements of a high-dimensional sample
as one-dimensional standard Gaussian variables and define a composite loss that
combines moment-based regularization in the spatial domain with power
spectrum-based regularization in the spectral domain. Since the expected values
of moments and power spectrum distributions are analytically known, the loss
promotes conformity to these properties. To ensure permutation invariance, the
losses are applied to randomly permuted inputs. Notably, existing
Gaussianity-based regularizations fall within our unified framework: some
correspond to moment losses of specific orders, while the previous
covariance-matching loss is equivalent to our spectral loss but incurs higher
time complexity due to its spatial-domain computation. We showcase the
application of our regularization in generative modeling for test-time reward
alignment with a text-to-image model, specifically to enhance aesthetics and
text alignment. Our regularization outperforms previous Gaussianity
regularization, effectively prevents reward hacking and accelerates
convergence.

</details>


### [15] [SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards](https://arxiv.org/abs/2509.07047)
*Kamyar Barakati,Utkarsh Pratiush,Sheryl L. Sanchez,Aditya Raghavan,Delia J. Milliron,Mahshid Ahmadi,Philip D. Rack,Sergei V. Kalinin*

Main category: cs.CV

TL;DR: 提出一种基于奖励函数的优化策略来微调基础分割模型（以 SAM 为例），得到面向实时流数据的改进版本 SAM*，在显微成像中实现更精确和可适配的分割。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如 SAM）虽具通用性，但存在大量不透明的调参项，需人工反复试错，难以满足显微流式实时分析与任务特异需求；需要一种能利用成像物理先验（粒径分布、几何约束等）自动优化的机制。

Method: 构建能够编码成像系统物理先验与任务指标的奖励函数（如粒子尺寸与形状、边界几何等），在 SAM 框架上进行奖励驱动的自动优化/微调，使模型参数或推理超参朝着高奖励方向调整，形成适配不同任务的 SAM*。

Result: 通过在显微成像场景中应用该优化框架，SAM 的适配性与分割性能提升，能够满足多样化任务需求，并支持对实时流式数据的分割。

Conclusion: 奖励函数驱动的优化可将领域知识嵌入基础分割模型，实现无需繁琐人工调参的快速适配；改进后的 SAM* 更适合显微图像多任务与实时处理。

Abstract: Image segmentation is a critical task in microscopy, essential for accurately
analyzing and interpreting complex visual data. This task can be performed
using custom models trained on domain-specific datasets, transfer learning from
pre-trained models, or foundational models that offer broad applicability.
However, foundational models often present a considerable number of
non-transparent tuning parameters that require extensive manual optimization,
limiting their usability for real-time streaming data analysis. Here, we
introduce a reward function-based optimization to fine-tune foundational models
and illustrate this approach for SAM (Segment Anything Model) framework by
Meta. The reward functions can be constructed to represent the physics of the
imaged system, including particle size distributions, geometries, and other
criteria. By integrating a reward-driven optimization framework, we enhance
SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,
that better aligns with the requirements of diverse segmentation tasks and
particularly allows for real-time streaming data segmentation. We demonstrate
the effectiveness of this approach in microscopy imaging, where precise
segmentation is crucial for analyzing cellular structures, material interfaces,
and nanoscale features.

</details>


### [16] [Enhancing Classification of Streaming Data with Image Distillation](https://arxiv.org/abs/2509.07049)
*Rwad Khatib,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 提出一种面向资源受限环境的流式图像数据分类方法，通过数据蒸馏提取关键特征，实现更高的准确率与效率；在基准对比（Hoeffding Trees、Adaptive Random Forest 及基于水库采样的RBC）中，DBC达73.1%准确率，表现最佳。


<details>
  <summary>Details</summary>
Motivation: 流数据分类在内存与计算受限条件下常受精度与效率权衡的限制；传统流式分类器（如HT与ARF）对图像数据适配度有限，难以兼顾特征表达与在线更新，因此需要一种能在资源约束下提取关键信息、提升分类精度的新方法。

Method: 提出Distillation Based Classification (DBC)：对连续到达的图像流进行数据蒸馏，从原始流中提炼“必要特征/样本”或其紧凑表示（经嵌入）以构建/更新分类器；与HT、ARF（经图像嵌入适配）以及基于水库采样的RBC进行对比评估，关注内存与计算效率。

Result: DBC在实验中达到73.1%的准确率，优于HT、ARF以及RBC基线；在资源受限设置下仍保持较高精度，显示更好的准确-效率权衡。

Conclusion: 数据蒸馏可有效提升流式图像数据分类性能，在受限资源场景中较传统方法更准确与高效；DBC为流数据复杂场景提供了新的可行标准。

Abstract: This study tackles the challenge of efficiently classifying streaming data in
envi-ronments with limited memory and computational resources. It delves into
the application of data distillation as an innovative approach to improve the
precision of streaming image data classification. By focusing on distilling
essential features from data streams, our method aims to minimize computational
demands while preserving crucial information for accurate classification. Our
investigation com-pares this approach against traditional algorithms like
Hoeffding Trees and Adap-tive Random Forest, adapted through embeddings for
image data. The Distillation Based Classification (DBC) demonstrated superior
performance, achieving a 73.1% accuracy rate, surpassing both traditional
methods and Reservoir Sam-pling Based Classification (RBC) technique. This
marks a significant advance-ment in streaming data classification, showcasing
the effectiveness of our method in processing complex data streams and setting
a new standard for accuracy and efficiency.

</details>


### [17] [Automated Evaluation of Gender Bias Across 13 Large Multimodal Models](https://arxiv.org/abs/2509.07050)
*Juan Manuel Contreras*

Main category: cs.CV

TL;DR: 提出Aymara图像公平评测基准，跨13个商用大模型用性别中性提示评估文本到图像中的性别偏见；发现模型系统性放大职业性别刻板印象并存在默认男性偏置，且模型间差异显著，说明偏见与设计选择相关。


<details>
  <summary>Details</summary>
Motivation: 现有对LMM性别偏见的研究缺乏大规模、可比、跨模型的统一评估方法；需要标准化、自动化的评测基准来衡量与推动公平性与问责。

Method: 构建Aymara Image Fairness Evaluation基准；使用75个程序生成的性别中性提示，覆盖男性刻板、女性刻板及非刻板职业；让13个商用LMM生成图像；用经验证的LLM-as-a-judge对965张图像进行性别呈现打分；与现实劳动力数据对比并统计显著性。

Result: 显著性均p<.001：1) 相比现实数据模型放大职业性别刻板印象：男性刻板职业图像93.0%生成男性、女性刻板职业仅22.5%生成男性；2) 非刻板职业存在强默认男性偏置：生成男性达68.3%；3) 模型间整体男性占比差异大：46.7%—73.3%；最佳模型能减弱刻板印象并接近性别均衡。

Conclusion: LMM的性别偏见普遍且可被量化，且并非不可避免，取决于设计选择；Aymara基准提供了至今最全面的跨模型性别偏见评测，强调建立标准化、自动化工具以促进公平与问责。

Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation,
but they risk perpetuating the harmful social biases in their training data.
Prior work has identified gender bias in these models, but methodological
limitations prevented large-scale, comparable, cross-model analysis. To address
this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for
assessing social bias in AI-generated images. We test 13 commercially available
LMMs using 75 procedurally-generated, gender-neutral prompts to generate people
in stereotypically-male, stereotypically-female, and non-stereotypical
professions. We then use a validated LLM-as-a-judge system to score the 965
resulting images for gender representation. Our results reveal (p < .001 for
all): 1) LMMs systematically not only reproduce but actually amplify
occupational gender stereotypes relative to real-world labor data, generating
men in 93.0% of images for male-stereotyped professions but only 22.5% for
female-stereotyped professions; 2) Models exhibit a strong default-male bias,
generating men in 68.3% of the time for non-stereotyped professions; and 3) The
extent of bias varies dramatically across models, with overall male
representation ranging from 46.7% to 73.3%. Notably, the top-performing model
de-amplified gender stereotypes and approached gender parity, achieving the
highest fairness scores. This variation suggests high bias is not an inevitable
outcome but a consequence of design choices. Our work provides the most
comprehensive cross-model benchmark of gender bias to date and underscores the
necessity of standardized, automated evaluation tools for promoting
accountability and fairness in AI development.

</details>


### [18] [Faster VGGT with Block-Sparse Global Attention](https://arxiv.org/abs/2509.07120)
*Chung-Shien Brian Wang,Christian Schmidt,Jens Piekenbrinck,Bastian Leibe*

Main category: cs.CV

TL;DR: 提出用高效块稀疏注意力替换多视图重建模型中的全局注意力，在无需重训的前提下，将推理加速至多4×且性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有如 VGGT、π^3 的前馈多视图重建模型虽准确简洁，但全局注意力的二次复杂度在大规模图像集上造成运行时瓶颈；作者观察到注意力权重实际上集中在少量跨视几何匹配上，存在可利用的稀疏结构。

Method: 实证分析全局注意力矩阵，发现概率质量集中于少量patch间交互；据此以结构化注意力思想，采用高度优化的块稀疏kernel替换密集全局注意力层；作为后装（retrofit）模块，无需重训主干，适配VGGT与π^3，支持大规模图像集合。

Result: 在综合多视图基准上，保持与原模型可比的任务性能，同时推理速度最高提升约4倍；可扩展到更大图像集。

Conclusion: 利用注意力稀疏性的工程化替换能显著缓解Transformer多视图重建的计算瓶颈，在不牺牲精度的情况下提升可扩展性与效率，并且通用于现有模型架构。

Abstract: Efficient and accurate feed-forward multi-view reconstruction has long been
an important task in computer vision. Recent transformer-based models like VGGT
and $\pi^3$ have achieved impressive results with simple architectures, yet
they face an inherent runtime bottleneck, due to the quadratic complexity of
the global attention layers, that limits the scalability to large image sets.
In this paper, we empirically analyze the global attention matrix of these
models and observe that probability mass concentrates on a small subset of
patch-patch interactions that correspond to cross-view geometric matches.
Motivated by the structured attention and inspired by recent advancement in
large language models, we propose a replacement for the dense global attention
operation based on highly optimized block-sparse kernels, yielding up to
$4\times$ faster inference with comparable task performance. Our retrofit
requires no retraining of the backbone, extends to both VGGT and $\pi^3$, and
supports large image collections. Evaluations on a comprehensive suite of
multi-view benchmarks demonstrate the effectiveness of our approach.

</details>


### [19] [Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry](https://arxiv.org/abs/2509.07130)
*Soruya Saha,Md Nurul Absur,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 提出一种针对边缘服务器上VIO被细微姿态欺骗导致累计漂移的无监督检测与恢复方法，在ILLIXR离线/离载测试床上验证，可显著降低轨迹与姿态误差。


<details>
  <summary>Details</summary>
Motivation: VIO常被卸载到边缘服务器以支撑VR实时姿态估计，但服务器侧攻击面增大，细微的姿态伪造可在规避启发式检查的同时累计造成显著漂移，影响VR沉浸与安全，亟需无需标注、可泛化的在线检测与纠正机制。

Method: 在仅含无攻击数据的会话上训练无监督、无标签模型，学习用户运动的时间规律；运行时监测时序偏离以判定伪造并触发恢复逻辑，重建/校正姿态以恢复一致性。方法在真实的VIO卸载环境（ILLIXR测试床）下，对多种欺骗强度进行评估。

Result: 相较无防御基线，在多种攻击强度下，常用评测指标（轨迹误差与姿态误差）显著下降，表明检测与恢复均有效。

Conclusion: 无监督、无标签的时序规律学习可在边缘卸载VIO场景中有效检测并缓解细微姿态欺骗导致的累计漂移，提升VR姿态鲁棒性与一致性。

Abstract: Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by
fusing camera and Inertial Measurement Unit (IMU) data for real-time pose.
However, current trend of offloading VIO to edge servers can lead server-side
threat surface where subtle pose spoofing can accumulate into substantial
drift, while evading heuristic checks. In this paper, we study this threat and
present an unsupervised, label-free detection and recovery mechanism. The
proposed model is trained on attack-free sessions to learn temporal
regularities of motion to detect runtime deviations and initiate recovery to
restore pose consistency. We evaluate the approach in a realistic offloaded-VIO
environment using ILLIXR testbed across multiple spoofing intensities.
Experimental results in terms of well-known performance metrics show
substantial reductions in trajectory and pose error compared to a no-defense
baseline.

</details>


### [20] [Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement](https://arxiv.org/abs/2509.07178)
*Muhammad Saad Saeed,Ijaz Ul Haq,Khalid Malik*

Main category: cs.CV

TL;DR: 研究评估“人脸增强”对深伪检测的破坏作用：常见增强（传统滤波与GAN增强）均可显著降低多类检测器（朴素/空间域/频域）的准确率；基本滤波ASR最高64.63%，GAN增强最高75.12%；对抗训练可提升一定鲁棒性但问题仍在。


<details>
  <summary>Details</summary>
Motivation: 人脸增强广泛用于提升视觉质量，但可能扭曲生物特征并掩盖深伪痕迹，导致取证系统失效。作者希望量化这种“增强即反取证”的风险，推动更稳健、可适应的深伪取证方法。

Method: 系统性基准：选取传统图像增强（如平滑/锐化/对比度/去噪等）与GAN型增强，对三类检测器（朴素特征、空间域、频域）进行对比评测；在FaceForensics++、DeepFakeDetection、CelebDF-v2上测试；并进行对抗训练（将增强样本纳入训练）以评估鲁棒性改善。

Result: 即使基础增强也能显著降低检测准确率（ASR至64.63%），GAN增强进一步放大脆弱性（ASR至75.12%）；不同检测范式均受影响；对抗训练有帮助但不足以完全抵消退化。

Conclusion: 人脸增强可充当有效的反取证工具，现有深伪检测对感知增强敏感；需要更具鲁棒性与自适应性的取证方法（例如对增强不变、跨域/多模态/频谱与空间联合、或包含增强分布的训练策略）。

Abstract: Face enhancement techniques are widely used to enhance facial appearance.
However, they can inadvertently distort biometric features, leading to
significant decrease in the accuracy of deepfake detectors. This study
hypothesizes that these techniques, while improving perceptual quality, can
degrade the performance of deepfake detectors. To investigate this, we
systematically evaluate whether commonly used face enhancement methods can
serve an anti-forensic role by reducing detection accuracy. We use both
traditional image processing methods and advanced GAN-based enhancements to
evaluate the robustness of deepfake detectors. We provide a comprehensive
analysis of the effectiveness of these enhancement techniques, focusing on
their impact on Na\"ive, Spatial, and Frequency-based detection methods.
Furthermore, we conduct adversarial training experiments to assess whether
exposure to face enhancement transformations improves model robustness.
Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2
datasets indicate that even basic enhancement filters can significantly reduce
detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based
techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%.
Our results demonstrate that face enhancement methods can effectively function
as anti-forensic tools, emphasizing the need for more resilient and adaptive
forensic methods.

</details>


### [21] [Dimensionally Reduced Open-World Clustering: DROWCULA](https://arxiv.org/abs/2509.07184)
*Erencem Ozbey,Dimitrios I. Diochnos*

Main category: cs.CV

TL;DR: 提出一种完全无监督的方法，通过ViT嵌入与流形学习，自动估计簇数并进行图像聚类/新类发现，在多数据集上达成SOTA，已知与未知簇数场景均适用。


<details>
  <summary>Details</summary>
Motivation: 标注成本高且开放世界场景中会出现未见类别，现有方法多为半监督；需要能在无标签下识别并确定新类别数量与分配。

Method: 使用Vision Transformer生成注意力驱动的向量嵌入；估计数据内在簇数；结合流形学习（利用数据内在几何结构）对嵌入进行细化，从而提升聚类与新类发现性能；适用于已知或未知簇数设置。

Result: 在CIFAR-10/100、ImageNet-100、Tiny ImageNet上取得单模态聚类与新类发现任务的最新SOTA；同时在簇数已知与未知的设置下均表现出色；代码已开源。

Conclusion: 无监督的ViT嵌入+流形细化方案能有效自动确定并发现新类别，显著提升图像聚类与NCD性能，适用于开放世界学习场景。

Abstract: Working with annotated data is the cornerstone of supervised learning.
Nevertheless, providing labels to instances is a task that requires significant
human effort. Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future. Not unsurprisingly, prior work in this,
so-called, `open-world' context has focused a lot on semi-supervised
approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset. Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings. Furthermore, we incorporate manifold learning techniques to
refine these embeddings by exploiting the intrinsic geometry of the data,
thereby enhancing the overall image clustering performance. Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do
so, both when the number of clusters is known or unknown ahead of time. The
code is available at: https://github.com/DROWCULA/DROWCULA.

</details>


### [22] [XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning](https://arxiv.org/abs/2509.07213)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: 提出XBusNet：双提示、双分支多模态BUS分割模型，结合CLIP全局语义与U-Net局部边界，并用从结构化元数据自动生成的临床文本提示进行调制；在BLU上以Dice 0.8765/IoU 0.8149达SOTA，尤其提升小、低对比病灶分割。


<details>
  <summary>Details</summary>
Motivation: BUS分割对定量分析与诊断关键，但小/低对比、边界模糊且含散斑噪声的病灶难以精确分割。仅用弱定位文本-图像线索（如CAM/CLIP）会边界粗糙、呈“团块状”。亟需能融合临床语义并恢复精细边缘的方案。

Method: 设计XBusNet：双提示（全局提示：病灶尺寸与位置；局部提示：形状、边缘、BI-RADS术语）+双分支（全局分支：CLIP ViT编码整图语义；局部分支：U-Net强调精细边界）。文本提示由结构化元数据自动组装，无需人工交互。五折交叉验证于BLU，主指标Dice/IoU；含按尺寸分层评估与消融，检验全局/局部路径及文本调制作用。

Result: 在BLU上达SOTA：平均Dice 0.8765、IoU 0.8149，优于6个强基线。对小病灶提升最大，减少漏检与伪激活。消融表明全局上下文、局部边界建模及提示调制具有互补贡献。

Conclusion: 融合全局语义与局部精度的双提示、双分支多模态设计可生成更准确稳健的BUS分割，尤其改善小、低对比病灶的表现。

Abstract: Background: Precise breast ultrasound (BUS) segmentation supports reliable
measurement, quantitative analysis, and downstream classification, yet remains
difficult for small or low-contrast lesions with fuzzy margins and speckle
noise. Text prompts can add clinical context, but directly applying weakly
localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce
coarse, blob-like responses that smear boundaries unless additional mechanisms
recover fine edges. Methods: We propose XBusNet, a novel dual-prompt,
dual-branch multimodal model that combines image features with clinically
grounded text. A global pathway based on a CLIP Vision Transformer encodes
whole-image semantics conditioned on lesion size and location, while a local
U-Net pathway emphasizes precise boundaries and is modulated by prompts that
describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS)
terms. Prompts are assembled automatically from structured metadata, requiring
no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using
five-fold cross-validation. Primary metrics are Dice and Intersection over
Union (IoU); we also conduct size-stratified analyses and ablations to assess
the roles of the global and local paths and the text-driven modulation.
Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice
of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions
show the largest gains, with fewer missed regions and fewer spurious
activations. Ablation studies show complementary contributions of global
context, local boundary modeling, and prompt-based modulation. Conclusions: A
dual-prompt, dual-branch multimodal design that merges global semantics with
local precision yields accurate BUS segmentation masks and improves robustness
for small, low-contrast lesions.

</details>


### [23] [Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion](https://arxiv.org/abs/2509.07277)
*Sepehr Salem,M. Moein Esfahani,Jingyu Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 利用扩散概率模型（DPM）进行数据增广，并将ResNet-50深特征与基于U-Net分割肿瘤得到的非线性手工特征融合，通过XGBoost实现乳腺热成像高精度分类（98%+），且消融与统计检验证明DPM增广与特征融合均为关键。


<details>
  <summary>Details</summary>
Motivation: 医疗影像样本稀缺制约深度学习性能，尤其在热成像乳腺癌分类中。需要一种既能缓解数据不足、又能结合可解释特征与深度特征的方法，以提升诊断准确性与稳健性。

Method: 1) 用扩散概率模型生成高质量增广样本，对比传统增广与ProGAN基线；2) 用U-Net对肿瘤区域分割，基于分割结果提取非线性手工特征（如分形维数等）；3) 从预训练ResNet-50提取深度特征；4) 将深度特征与手工特征进行特征级融合；5) 使用XGBoost进行分类；6) 通过消融和统计显著性检验评估各组件贡献。

Result: 在乳腺热成像分类上取得98.0%准确率、98.1%敏感性；DPM增广优于传统增广与ProGAN；消融与统计检验显示DPM增广与非线性特征融合带来显著性能提升。

Conclusion: 先进生成模型（DPM）与可解释的非线性手工特征的协同，可在小样本医疗影像中构建高准确度的诊断系统；数据增广质量与多源特征融合是达到SOTA性能的关键。

Abstract: Data scarcity hinders deep learning for medical imaging. We propose a
framework for breast cancer classification in thermograms that addresses this
using a Diffusion Probabilistic Model (DPM) for data augmentation. Our
DPM-based augmentation is shown to be superior to both traditional methods and
a ProGAN baseline. The framework fuses deep features from a pre-trained
ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived
from U-Net segmented tumors. An XGBoost classifier trained on these fused
features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and
statistical tests confirm that both the DPM augmentation and the nonlinear
feature fusion are critical, statistically significant components of this
success. This work validates the synergy between advanced generative models and
interpretable features for creating highly accurate medical diagnostic tools.

</details>


### [24] [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)
*Ji Xie,Trevor Darrell,Luke Zettlemoyer,XuDong Wang*

Main category: cs.CV

TL;DR: 提出RecA：用视觉理解编码嵌入作为密集“文本提示”来对齐统一多模态模型（UMM）的理解与生成，通过自监督重建损失在极少GPU时间内显著提升生成与编辑 fidelity，并适用于多种UMM架构。


<details>
  <summary>Details</summary>
Motivation: 传统UMM训练依赖图文对，但文本描述稀疏且缺失细粒度视觉信息，导致理解与生成子模块不对齐、生成细节欠佳；需要一种无需人工字幕、成本低且可泛化的对齐方法。

Method: 提出Reconstruction Alignment（RecA）：用UMM自身的视觉理解编码器输出作为密集条件（“文本提示”），让生成器在该条件下重建输入图像，采用自监督重建损失进行后训练，从而将理解表征与生成过程对齐。方法可直接套用于自回归、掩码自回归与扩散等UMM。

Result: 在仅27 GPU小时的后训练下，生成与编辑显著提升：GenEval 0.73→0.90、DPGBench 80.93→88.15；编辑基准ImgEdit 3.38→3.75、GEdit 6.94→7.25。结果表明RecA在资源高效前提下超过更大开源模型，并跨架构普适。

Conclusion: RecA是一种高效、通用的UMM后训练对齐策略，通过自监督重建将理解与生成紧密结合，能显著提升多类UMM的生成与编辑保真度，具备良好扩展性与实用价值。

Abstract: Unified multimodal models (UMMs) unify visual understanding and generation
within a single architecture. However, conventional training relies on
image-text pairs (or sequences) whose captions are typically sparse and miss
fine-grained visual details--even when they use hundreds of words to describe a
simple image. We introduce Reconstruction Alignment (RecA), a
resource-efficient post-training method that leverages visual understanding
encoder embeddings as dense "text prompts," providing rich supervision without
captions. Concretely, RecA conditions a UMM on its own visual understanding
embeddings and optimizes it to reconstruct the input image with a
self-supervised reconstruction loss, thereby realigning understanding and
generation. Despite its simplicity, RecA is broadly applicable: across
autoregressive, masked-autoregressive, and diffusion-based UMMs, it
consistently improves generation and editing fidelity. With only 27 GPU-hours,
post-training with RecA substantially improves image generation performance on
GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while
also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit
6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models
and applies broadly across diverse UMM architectures, establishing it as an
efficient and general post-training alignment strategy for UMMs

</details>


### [25] [DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion](https://arxiv.org/abs/2509.07327)
*Shucong Li,Zhenyu Liu,Zijie Hong,Zhiheng Zhou,Xianghai Cao*

Main category: cs.CV

TL;DR: 提出DEPF：基于线性复杂度Mamba的无人机多光谱目标检测框架，通过双域增强与优先引导融合，提升低光图像质量、抑制冗余信息并适配轻量部署，在DroneVehicle与VEDAI上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: UAV多光谱检测受三难题制约：低照度导致模态互补性下降；融合阶段冗余信息干扰小目标建模；Transformer类方法二次复杂度难以在机载端部署。

Method: 1) 双域增强模块DDE：包含CSWM与FDR。CSWM对小波低频分量进行跨尺度Mamba扫描，提升全局亮度与结构；FDR以频域谱恢复网络增强高频纹理细节。2) 优先引导Mamba融合PGMF：依据模态差异得到优先级分数，从局部目标特征出发进行优先扫描与融合，减少冗余干扰，强化小目标表征。3) 以Mamba为骨干的线性复杂度设计，适配UAV算力。

Result: 在DroneVehicle与VEDAI数据集上，DEPF在目标检测性能上优于现有SOTA方法（摘要未给出具体数值），并展示出良好的低光增强与轻量部署能力。

Conclusion: DEPF通过双域增强和优先引导的Mamba融合，有效缓解低光、冗余干扰和复杂度瓶颈，提升多光谱UAV检测效果，具备实用部署潜力。

Abstract: Multispectral remote sensing object detection is one of the important
application of unmanned aerial vehicle (UAV). However, it faces three
challenges. Firstly, the low-light remote sensing images reduce the
complementarity during multi-modality fusion. Secondly, the local small target
modeling is interfered with redundant information in the fusion stage easily.
Thirdly, due to the quadratic computational complexity, it is hard to apply the
transformer-based methods on the UAV platform. To address these limitations,
motivated by Mamba with linear complexity, a UAV multispectral object detector
with dual-domain enhancement and priority-guided mamba fusion (DEPF) is
proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain
Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba
(CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba
scanning for the low-frequency components to enhance the global brightness of
images, while FDR constructs spectrum recovery network to enhance the frequency
spectra features for recovering the texture-details. Secondly, to enhance local
target modeling and reduce the impact of redundant information during fusion,
Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the
concept of priority scanning, which starts from local targets features
according to the priority scores obtained from modality difference. Experiments
on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on
object detection, comparing with state-of-the-art methods. Our code is
available in the supplementary material.

</details>


### [26] [G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2509.07335)
*Haiqing Ren,Zhongkai Luo,Heng Fan,Xiaohui Yuan,Guanchen Wang,Libo Zhang*

Main category: cs.CV

TL;DR: 提出G³CN：在GCN中引入高斯拓扑滤波与门控传播，以更好区分骨架动作中的易混类别，并在多个数据集上显著提升识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统骨架动作识别的GCN虽能利用图拓扑聚合特征，但在区分姿态相近、动态细微差别的“模糊动作”时表现欠佳，说明其学习到的拓扑与空间特征表达存在不足，需要更精细的拓扑建模与信息传播机制来提升判别力与泛化能力。

Method: 提出Gaussian Topology Refinement Gated Graph Convolution（G³CN）。(1) 以高斯滤波器对骨架图拓扑进行细化/加权，平滑噪声并突出关键邻接关系，从而提升对易混动作的结构表征。(2) 在GCN中融入GRU式门控机制，增强关节点间时空信息传递与记忆/遗忘能力，缓解信息衰减与过平滑。(3) 方法作为通用模块，可无缝集成到多种GCN主干中。

Result: 在NTU RGB+D、NTU RGB+D 120、NW-UCLA等基准上，集成G³CN后均取得优于原有GCN的性能提升，尤其在模糊样本的识别准确率上提升显著，显示出良好的泛化与稳健性。

Conclusion: 通过高斯拓扑细化与门控传播的协同，G³CN强化了骨架图的判别性表示，有效缓解GCN对模糊动作区分能力弱的问题，具备对多种GCN架构的普适增益。

Abstract: Graph Convolutional Networks (GCNs) have proven to be highly effective for
skeleton-based action recognition, primarily due to their ability to leverage
graph topology for feature aggregation, a key factor in extracting meaningful
representations. However, despite their success, GCNs often struggle to
effectively distinguish between ambiguous actions, revealing limitations in the
representation of learned topological and spatial features. To address this
challenge, we propose a novel approach, Gaussian Topology Refinement Gated
Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing
ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates
a Gaussian filter to refine the skeleton topology graph, improving the
representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs)
are integrated into the GCN framework to enhance information propagation
between skeleton points. Our method shows strong generalization across various
GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA
benchmarks demonstrate that G$^{3}$CN effectively improves action recognition,
particularly for ambiguous samples.

</details>


### [27] [Parse Graph-Based Visual-Language Interaction for Human Pose Estimation](https://arxiv.org/abs/2509.07385)
*Shibang Liu,Xuemei Xie,Guangming Shi*

Main category: cs.CV

TL;DR: 提出PGVL：用解析图进行视觉-语言交互以提升遮挡场景的人体姿态估计，通过分层节点与引导模块实现细粒度局部保持与全局推理的有效融合，并在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有人体姿态估计多依赖单模态或简单的视觉-语言全局融合，导致遮挡区域响应被削弱、跨模态对齐与定位失败。语言中蕴含空间关系等强先验，如何以结构化方式与视觉层级信息有效交互，是提升遮挡/不可见关键点估计的关键。

Method: 提出基于解析图的视觉-语言交互框架PGVL：1) 为视觉与语言分别构建分层解析图，低层节点聚焦局部细节，高层节点表征全局语义；2) 设计Guided Module（GM），让已跨注意的高语义节点引导低语义节点的特征更新，实现多源信息的有序融合与净化；3) 采用自顶向下分解与自底向上组合：先构建模态特定解析图，再进行递归的双向跨注意并由GM纯化信息，最终整合得到关键点估计；4) 基于PGVL设计端到端网络实现。

Result: 在主流人体姿态估计数据集上验证，PGVL与其网络均取得优于现有方法的性能（特别在遮挡/不可见关键点场景下），显示更好的定位与对齐能力。代码将开源。

Conclusion: 结构化的解析图与GM引导的跨模态交互能有效融合视觉局部与语言全局先验，显著提升遮挡环境下的人体姿态估计；PGVL提供了通用、可扩展的多模态融合思路，并在基准上验证有效。

Abstract: Parse graphs boost human pose estimation (HPE) by integrating context and
hierarchies, yet prior work mostly focuses on single modality modeling,
ignoring the potential of multimodal fusion. Notably, language offers rich HPE
priors like spatial relations for occluded scenes, but existing visual-language
fusion via global feature integration weakens occluded region responses and
causes alignment and location failures. To address this issue, we propose Parse
Graph-based Visual-Language interaction (PGVL) with a core novel Guided Module
(GM). In PGVL, low-level nodes focus on local features, maximizing the
maintenance of responses in occluded areas and high-level nodes integrate
global features to infer occluded or invisible parts. GM enables high semantic
nodes to guide the feature update of low semantic nodes that have undergone
cross attention. It ensuring effective fusion of diverse information. PGVL
includes top-down decomposition and bottom-up composition. In the first stage,
modality specific parse graphs are constructed. Next stage. recursive
bidirectional cross-attention is used, purified by GM. We also design network
based on PGVL. The PGVL and our network is validated on major pose estimation
datasets. We will release the code soon.

</details>


### [28] [DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation](https://arxiv.org/abs/2509.07435)
*Ze-Xin Yin,Jiaxiong Qiu,Liu Liu,Xinjie Wang,Wei Sui,Zhizhong Su,Jian Yang,Jin Xie*

Main category: cs.CV

TL;DR: 提出LGAA：统一几何与PBR材质的端到端3D资产生成框架，基于多视图扩散先验，采用Wrapper/Switcher/Decoder三模块并配合后处理提取高质量可重光网格；在仅69k多视图数据上高效收敛，优于现有方法，代码与数据将开源。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成多聚焦几何，纹理常被简化或依赖后处理，难以直接得到可重光的PBR资产；需要一种能同时建模几何与PBR材质、数据高效、易融合多种扩散先验的端到端方案。

Method: - 利用多视图扩散模型的先验，从“适配”角度提出模块化框架LGAA：
  1) LGAA Wrapper：重用并适配MV扩散模型的网络层，迁移大规模图像知识以提升收敛与数据效率。
  2) LGAA Switcher：对齐并融合多个携带不同知识的Wrapper层，引入多种扩散先验分别服务几何与PBR合成。
  3) LGAA Decoder：一个“驯化”的VAE，直接预测带PBR通道的2D Gaussian Splatting（2DGS）。
- 后处理：从2DGS稳健地提取高质量、可重光的网格资产（含PBR材质）。
- 适用于文本或图像条件的MV扩散模型。

Result: 定量与定性实验均显示，在几何与PBR质量上优于现有方法；模块化设计可灵活整合多种扩散先验；知识保留策略使其在仅69k多视图实例上即可高效收敛。

Conclusion: LGAA实现了端到端、PBR就绪的3D资产生成，统一了几何与材质建模，具备数据效率与灵活扩展性；提供代码、预训练权重与数据集，促进复现与应用。

Abstract: The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.

</details>


### [29] [In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting](https://arxiv.org/abs/2509.07447)
*Taiying Peng,Jiacheng Hua,Miao Liu,Feng Lu*

Main category: cs.CV

TL;DR: 提出EgoGazeVQA基准与方法，利用凝视(gaze)信号引导MLLM在自我视角长视频上的问答，展示现有模型难以把握用户意图，而凝视引导的意图提示与微调显著提升表现，并分析凝视估计精度对效果的影响。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在自我视角视频上缺乏对用户意图的精准把握，尤其忽视了“凝视”这一强指示信号；基准与评测亦未系统纳入凝视因素，限制了个性化、主动式AI助手的发展。

Method: 构建EgoGazeVQA：从日常长时 egocentric 视频中，结合MLLM生成并经人工精修的凝视相关问答；提出“凝视引导的意图提示”策略，将空间、时间与意图线索编码进提示，比较不同提示与凝视精度；并进行含凝视信息的微调实验。

Result: 现有MLLM在意图理解与QA上表现不足；加入凝视引导提示后显著提升性能；微调进一步带来收益；凝视估计越准确，提示增益越大。

Conclusion: 凝视是理解自我视角用户意图的关键信号。在EgoGazeVQA上，凝视引导的提示与微调能显著提升MLLM对长时日常视频的理解与个性化交互能力，强调了在egocentric场景中纳入凝视信息的必要性。

Abstract: The emergence of advanced multimodal large language models (MLLMs) has
significantly enhanced AI assistants' ability to process complex information
across modalities. Recently, egocentric videos, by directly capturing user
focus, actions, and context in an unified coordinate, offer an exciting
opportunity to enable proactive and personalized AI user experiences with
MLLMs. However, existing benchmarks overlook the crucial role of gaze as an
indicator of user intent. To address this gap, we introduce EgoGazeVQA, an
egocentric gaze-guided video question answering benchmark that leverages gaze
information to improve the understanding of longer daily-life videos.
EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by
human annotators. Our experiments reveal that existing MLLMs struggle to
accurately interpret user intentions. In contrast, our gaze-guided intent
prompting methods significantly enhance performance by integrating spatial,
temporal, and intent-related cues. We further conduct experiments on
gaze-related fine-tuning and analyze how gaze estimation accuracy impacts
prompting effectiveness. These results underscore the value of gaze for more
personalized and effective AI assistants in egocentric settings.

</details>


### [30] [GLEAM: Learning to Match and Explain in Cross-View Geo-Localization](https://arxiv.org/abs/2509.07450)
*Xudong Lu,Zhi Zheng,Yi Wan,Yongxiang Yao,Annan Wang,Renrui Zhang,Panwang Xia,Qiong Wu,Qingyun Li,Weifeng Lin,Xiangyu Zhao,Xue Yang,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出GLEAM-C与GLEAM-X：前者作为统一多视角多模态并以卫星图为枢纽对齐的CVGL基础模型；后者将匹配预测与可解释推理结合，构建双语基准，提升地理定位的透明性与可扩展性。两阶段训练达成与专用方法相当的精度，并通过MLLM生成与人工校对数据实现可解释评测。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角地理定位方法多限于单一视角/模态，且仅做“是否匹配”的黑盒判定，缺乏解释性与可扩展统一框架。需要一种能统一多模态多视角、训练高效且具可解释推理能力的CVGL方案。

Method: 1) GLEAM-C：以卫星图像为对齐枢纽，将无人机、街区地图、全景与地面照片统一到同一嵌入空间；通过优化实现与两阶段训练提高效率与精度。2) GLEAM-X：引入可解释任务，将跨视角匹配与MLLM驱动的推理结合；利用GPT-4o与豆包思考多模态模型生成双语训练/测试数据，并对测试集进行人工精修，形成系统化评测基准。

Result: 在不牺牲训练效率的情况下，GLEAM-C达到与以往针对单一模态优化的CVGL模型相当的精度。构建了一个双语（中英）可解释跨视角推理基准，可系统评估模型的匹配与解释能力。

Conclusion: GLEAM-C与GLEAM-X共同提供了一个从多模态多视角对齐到可解释对应分析的完整管线，兼顾准确性与可解释性，推动CVGL在透明性与可扩展性上的发展；代码与数据将开源。

Abstract: Cross-View Geo-Localization (CVGL) focuses on identifying correspondences
between images captured from distinct perspectives of the same geographical
location. However, existing CVGL approaches are typically restricted to a
single view or modality, and their direct visual matching strategy lacks
interpretability: they merely predict whether two images correspond, without
explaining the rationale behind the match. In this paper, we present GLEAM-C, a
foundational CVGL model that unifies multiple views and modalities-including
UAV imagery, street maps, panoramic views, and ground photographs-by aligning
them exclusively with satellite imagery. Our framework enhances training
efficiency through optimized implementation while achieving accuracy comparable
to prior modality-specific CVGL models through a two-phase training strategy.
Moreover, to address the lack of interpretability in traditional CVGL methods,
we leverage the reasoning capabilities of multimodal large language models
(MLLMs) to propose a new task, GLEAM-X, which combines cross-view
correspondence prediction with explainable reasoning. To support this task, we
construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro
to generate training and testing data. The test set is further refined through
detailed human revision, enabling systematic evaluation of explainable
cross-view reasoning and advancing transparency and scalability in
geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL
pipeline that integrates multi-modal, multi-view alignment with interpretable
correspondence analysis, unifying accurate cross-view matching with explainable
reasoning and advancing Geo-Localization by enabling models to better Explain
And Match. Code and datasets used in this work will be made publicly accessible
at https://github.com/Lucky-Lance/GLEAM.

</details>


### [31] [XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning](https://arxiv.org/abs/2509.07455)
*Pooya Khosravi,Kun Han,Anthony T. Wu,Arghavan Rezvani,Zexin Feng,Xiaohui Xie*

Main category: cs.CV

TL;DR: 提出XOCT框架，通过跨维度监督和多尺度特征融合，实现OCT到OCTA的层感知血管重建，提升en-face投影质量与临床可用性。


<details>
  <summary>Details</summary>
Motivation: OCTA对视网膜/脉络膜血管可视化与疾病诊断关键，但受运动敏感与设备/软件改造成本限制，难以获得高质量图像。现有深度学习OCT→OCTA方法忽视不同视网膜层的血管差异，且难以重建精细、致密的血管细节，影响诊断可靠性。

Method: 提出XOCT：1) 跨维度监督（CDS）：通过分割加权的z轴平均生成逐层2D en-face投影，作为监督信号，强制网络学习各视网膜层的区分性表示；2) 多尺度特征融合（MSFF）：多尺度特征提取结合通道重加权，强化不同空间尺度的血管边界与细节表征；整体实现层感知的OCT→OCTA重建，特别优化en-face投影。

Result: 在OCTA-500数据集上实验表明，XOCT在总体和尤其是en-face投影评价上优于现有方法，显著提升血管细节重构与临床评估相关指标。

Conclusion: XOCT通过CDS与MSFF实现对层结构敏感的高保真血管重建，提高OCTA的可及性、可靠性和诊断价值，适用于眼科疾病检测与随访监测。

Abstract: Optical Coherence Tomography Angiography (OCTA) and its derived en-face
projections provide high-resolution visualization of the retinal and choroidal
vasculature, which is critical for the rapid and accurate diagnosis of retinal
diseases. However, acquiring high-quality OCTA images is challenging due to
motion sensitivity and the high costs associated with software modifications
for conventional OCT devices. Moreover, current deep learning methods for
OCT-to-OCTA translation often overlook the vascular differences across retinal
layers and struggle to reconstruct the intricate, dense vascular details
necessary for reliable diagnosis. To overcome these limitations, we propose
XOCT, a novel deep learning framework that integrates Cross-Dimensional
Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for
layer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise
en-face projections, generated via segmentation-weighted z-axis averaging, as
supervisory signals to compel the network to learn distinct representations for
each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF
module enhances vessel delineation through multi-scale feature extraction
combined with a channel reweighting strategy, effectively capturing vascular
details at multiple spatial scales. Our experiments on the OCTA-500 dataset
demonstrate XOCT's improvements, especially for the en-face projections which
are significant for clinical evaluation of retinal pathologies, underscoring
its potential to enhance OCTA accessibility, reliability, and diagnostic value
for ophthalmic disease detection and monitoring. The code is available at
https://github.com/uci-cbcl/XOCT.

</details>


### [32] [Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting](https://arxiv.org/abs/2509.07456)
*Sai Siddhartha Chary Aylapuram,Veeraraju Elluru,Shivang Agarwal*

Main category: cs.CV

TL;DR: 提出“偏置感知的机器遗忘（Bias-Aware Machine Unlearning, BAMU）”，用后验的选择性遗忘来缓解视觉模型中的多种偏置，在三大数据集上显著降低子群不公平，同时几乎不牺牲准确率。


<details>
  <summary>Details</summary>
Motivation: 深度模型常依赖训练数据中的虚假相关（如姿态、贴片、性别），在医疗、自动驾驶等高风险场景会造成不公平与安全隐患。传统去偏要重训或重构数据流程，成本高且不适用于已部署模型。机器遗忘提供了无需全量重训的后处理路径，值得系统评估用于公平性提升。

Method: 基于隐私导向的机器遗忘框架，选择性地遗忘“带偏”的样本或特征表示；比较多种策略：梯度上升（对偏置信号进行反向优化以削弱其影响）、LoRA低秩适配微调、教师-学生蒸馏（用去偏教师引导学生遗忘偏置）；在CUB-200-2011（姿态偏置）、CIFAR-10（合成贴片偏置）、CelebA（性别偏置于微笑检测）上进行实证。

Result: 在不显著损失总体准确率的前提下显著降低子群差异：CUB-200的人口平等性提升94.86%，CIFAR-10提升30.28%，CelebA提升97.37%；在效用、公平、质量、隐私的联合指标上三种设定平均得分0.62。

Conclusion: 机器遗忘是实用的后验去偏框架，可在无需完全重训的情况下改进已部署视觉系统的公平性；选择性移除偏置样本/表征与现成遗忘技术结合，能够在维持性能的同时大幅降低不公平。

Abstract: Deep neural networks often rely on spurious correlations in training data,
leading to biased or unfair predictions in safety-critical domains such as
medicine and autonomous driving. While conventional bias mitigation typically
requires retraining from scratch or redesigning data pipelines, recent advances
in machine unlearning provide a promising alternative for post-hoc model
correction. In this work, we investigate \textit{Bias-Aware Machine
Unlearning}, a paradigm that selectively removes biased samples or feature
representations to mitigate diverse forms of bias in vision models. Building on
privacy-preserving unlearning techniques, we evaluate various strategies
including Gradient Ascent, LoRA, and Teacher-Student distillation. Through
empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias),
CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection),
we demonstrate that post-hoc unlearning can substantially reduce subgroup
disparities, with improvements in demographic parity of up to \textbf{94.86\%}
on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These
gains are achieved with minimal accuracy loss and with methods scoring an
average of 0.62 across the 3 settings on the joint evaluation of utility,
fairness, quality, and privacy. Our findings establish machine unlearning as a
practical framework for enhancing fairness in deployed vision systems without
necessitating full retraining.

</details>


### [33] [ANYPORTAL: Zero-Shot Consistent Video Background Replacement](https://arxiv.org/abs/2509.07472)
*Wenshuo Gao,Xicheng Lan,Shuai Yang*

Main category: cs.CV

TL;DR: 提出ANYPORTAL：零样本、免训练的视频背景替换框架，融合视频扩散的时序先验与图像扩散的重光照能力，并通过“精炼投影算法”实现前景像素级一致性与时序一致重光照，能在消费级GPU上产出高质量结果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成/编辑方法难以精细控制细节，尤其在背景替换中常出现前景形变、边界污染与跨帧不一致，限制了实用性。作者希望在不再训练的前提下，提升前景保持与时序重光照一致性，实现高质量、可控的视频编辑。

Method: 提出ANYPORTAL：1) 零样本、训练免疫，直接利用预训练扩散模型；2) 协同使用视频扩散模型（提供时序先验）与图像扩散模型（提供重光照/外观控制）；3) 设计“Refinement Projection Algorithm（精炼投影算法）”，在像素级对前景进行细节操控与约束，将生成结果投影以保持前景一致与干净的边界；4) 实现跨帧一致的重光照与背景替换流程，适配消费级GPU。

Result: 在实验中，ANYPORTAL实现了高质量的视频背景替换与稳定的时序重光照，在不额外训练的情况下即可运行，并能在消费级GPU上高效推理，优于现有方法在前景一致性与可控性方面的表现。

Conclusion: ANYPORTAL在零样本与免训练设定下，解决了前景一致性与时序重光照难题，提供实用、高效的高清视频背景替换方案，具有较强的落地潜力。

Abstract: Despite the rapid advancements in video generation technology, creating
high-quality videos that precisely align with user intentions remains a
significant challenge. Existing methods often fail to achieve fine-grained
control over video details, limiting their practical applicability. We
introduce ANYPORTAL, a novel zero-shot framework for video background
replacement that leverages pre-trained diffusion models. Our framework
collaboratively integrates the temporal prior of video diffusion models with
the relighting capabilities of image diffusion models in a zero-shot setting.
To address the critical challenge of foreground consistency, we propose a
Refinement Projection Algorithm, which enables pixel-level detail manipulation
to ensure precise foreground preservation. ANYPORTAL is training-free and
overcomes the challenges of achieving foreground consistency and temporally
coherent relighting. Experimental results demonstrate that ANYPORTAL achieves
high-quality results on consumer-grade GPUs, offering a practical and efficient
solution for video content creation and editing.

</details>


### [34] [MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification](https://arxiv.org/abs/2509.07477)
*Patrick Wienholt,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 提出MedicalPatchNet：将胸片划分为不重叠小块，逐块独立分类并聚合预测，实现内生可解释并与SOTA性能相当（AUROC≈0.907）。在CheXlocalize上显著提升病灶定位（命中率0.485优于Grad-CAM的0.376）。


<details>
  <summary>Details</summary>
Motivation: 当前放射影像深度模型分类强但解释性弱，临床不信任，且易出现捷径学习。需要一种无需事后解释、能直接指向证据区域的模型，以提升可信度与定位能力。

Method: 将输入胸片切分为不重叠patch；为每个patch训练独立分类器（或共享权重的局部分类头）；再通过聚合器整合patch级预测为图像级判断。由此每个patch的贡献可直接可视化，无需Grad-CAM等事后方法。模型在CheXpert上训练，评估包括AUROC与在CheXlocalize上的病灶定位命中率。

Result: 在CheXpert上，MedicalPatchNet与EfficientNet-B0性能持平（AUROC 0.907 vs 0.908）。在CheXlocalize上显著提升定位可解释性（平均命中率0.485 vs 0.376，后者为EfficientNet+Grad-CAM）。提供完整可复现实验与代码。

Conclusion: MedicalPatchNet在保持分类性能的同时，提供原生、可靠的区域级解释，降低捷径学习风险，增强临床信任，并具备良好可复现性与可推广性。

Abstract: Deep neural networks excel in radiological image classification but
frequently suffer from poor interpretability, limiting clinical acceptance. We
present MedicalPatchNet, an inherently self-explainable architecture for chest
X-ray classification that transparently attributes decisions to distinct image
regions. MedicalPatchNet splits images into non-overlapping patches,
independently classifies each patch, and aggregates predictions, enabling
intuitive visualization of each patch's diagnostic contribution without
post-hoc techniques. Trained on the CheXpert dataset (223,414 images),
MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908)
of EfficientNet-B0, while substantially improving interpretability:
MedicalPatchNet demonstrates substantially improved interpretability with
higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with
Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable
explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks
associated with shortcut learning, thus improving clinical trust. Our model is
publicly available with reproducible training and inference scripts and
contributes to safer, explainable AI-assisted diagnostics across medical
imaging domains. We make the code publicly available:
https://github.com/TruhnLab/MedicalPatchNet

</details>


### [35] [LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors](https://arxiv.org/abs/2509.07484)
*Wenshuo Gao,Xicheng Lan,Luyao Zhang,Shuai Yang*

Main category: cs.CV

TL;DR: 提出一种将隐式神经表征与文本到视频扩散模型结合，用于自动为矢量图形生成自然动画的方法。通过分层隐式表征重建矢量图并保持其无损特性，利用视频得分蒸馏进行运动优化，最后对矢量图进行形变以得到平滑动画，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 矢量图具有可缩放、可编辑等优势，但对其进行高质量动画通常需要大量手工工作。现有基于扩散或像素域的方法与矢量图在表示上存在巨大鸿沟，难以同时保证分辨率无损、形状/颜色约束与自然运动。因此需要一种既保留矢量固有特性又能自动化生成生动动画的办法。

Method: 1) 用分层隐式神经表征（layered implicit neural representations）重建输入矢量图，显式保留无限分辨率、精确颜色与形状约束；2) 通过视频得分蒸馏采样（Video SDS）从预训练文本到视频扩散模型引入运动先验，对隐式表征进行优化；3) 将优化后的隐式表示转化为对原始矢量图的形变/扭曲（warping），得到平滑连贯的矢量动画。

Result: 实验显示该方法能生成生动、自然的矢量图动画，相比现有方法在灵活性和动画质量上显著提升，减少伪影与分辨率损失，并更好地保持几何与颜色约束。

Conclusion: 结合隐式神经表征与文本到视频扩散的运动先验，可在保持矢量图固有优势的同时，自动产生高质量动画；方法在灵活性和质量上优于当前技术。

Abstract: Vector graphics, known for their scalability and user-friendliness, provide a
unique approach to visual content compared to traditional pixel-based images.
Animation of these graphics, driven by the motion of their elements, offers
enhanced comprehensibility and controllability but often requires substantial
manual effort. To automate this process, we propose a novel method that
integrates implicit neural representations with text-to-video diffusion models
for vector graphic animation. Our approach employs layered implicit neural
representations to reconstruct vector graphics, preserving their inherent
properties such as infinite resolution and precise color and shape constraints,
which effectively bridges the large domain gap between vector graphics and
diffusion models. The neural representations are then optimized using video
score distillation sampling, which leverages motion priors from pretrained
text-to-video diffusion models. Finally, the vector graphics are warped to
match the representations resulting in smooth animation. Experimental results
validate the effectiveness of our method in generating vivid and natural vector
graphic animations, demonstrating significant improvement over existing
techniques that suffer from limitations in flexibility and animation quality.

</details>


### [36] [Fine-Tuning Vision-Language Models for Visual Navigation Assistance](https://arxiv.org/abs/2509.07488)
*Xiao Li,Bharat Gandhi,Ming Zhan,Mohit Nehra,Zhicheng Zhang,Yuchen Sun,Meijia Song,Naisheng Zhang,Xi Wang*

Main category: cs.CV

TL;DR: 论文提出一种结合视觉与语言模型的室内导航方法，针对视障人士，通过微调BLIP-2（使用LoRA）在人工标注数据上生成逐步导航指令，并提出强调方向与顺序要素的改进评测指标；结果显示方向性指令生成显著提升。


<details>
  <summary>Details</summary>
Motivation: 室内环境缺乏精确定位（如GPS）使传统导航无效，视障人士难以获得可靠的方向与步骤级引导；需要一种能从图像与自然语言理解并输出可执行、方向明确的导航指令的系统，同时需要更能反映导航可执行性的评测标准。

Method: 将BLIP-2作为基础视觉-语言框架，引入LoRA进行参数高效微调；使用人工标注的室内导航数据训练模型以生成逐步（step-by-step）的方向性指令；提出对BERT F1进行改进的评估指标，增加对方向（如左/右/直行、转弯）与顺序（步骤次序、先后关系）词项的加权与约束。

Result: 应用LoRA微调后，模型在生成方向性与顺序明确的导航指令方面显著优于原始BLIP-2；新评测指标能更全面地衡量导航质量，尤其在方向与步骤一致性上体现改进。

Conclusion: 参数高效微调使BLIP-2在室内导航任务上得到有效增强；结合强调方向/顺序的评测，有助于更真实地反映导航可用性。该方法提升了为视障人士提供可执行导航指令的能力。

Abstract: We address vision-language-driven indoor navigation to assist visually
impaired individuals in reaching a target location using images and natural
language guidance. Traditional navigation systems are ineffective indoors due
to the lack of precise location data. Our approach integrates vision and
language models to generate step-by-step navigational instructions, enhancing
accessibility and independence. We fine-tune the BLIP-2 model with Low Rank
Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose
an evaluation metric that refines the BERT F1 score by emphasizing directional
and sequential variables, providing a more comprehensive measure of
navigational performance. After applying LoRA, the model significantly improved
in generating directional instructions, overcoming limitations in the original
BLIP-2 model.

</details>


### [37] [DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning](https://arxiv.org/abs/2509.07493)
*Wenzhi Guo,Bing Wang*

Main category: cs.CV

TL;DR: 提出DiGS，将SDF学习嵌入3D高斯点渲染(3DGS)，通过可学习SDF与几何引导的多尺度网格生长，使高质量渲染同时获得更准确、完整的几何重建。


<details>
  <summary>Details</summary>
Motivation: 3DGS在新视角渲染上表现出色，但因表示无结构、缺少显式几何监督，难以获得精准且完整的表面重建。需要在不牺牲渲染质量的前提下引入强几何先验与跨视角一致性。

Method: 在3DGS中为每个高斯关联一个可学习的SDF值，使高斯与底层几何显式对齐并强化跨视角一致性；提出几何引导的网格生长策略，在多尺度层级下自适应地沿几何一致区域分布高斯，实现稠密、连贯的覆盖。

Result: 在DTU、Mip-NeRF 360、Tanks&Temples基准上，相比基线，DiGS在重建精度与完整性上持续提升，同时保持高保真渲染质量。

Conclusion: 将SDF直接融入3DGS并配合几何引导的多尺度高斯分布，可兼顾高质量渲染与可靠的表面重建，提升几何对齐与跨视角一致性。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for
photorealistic view synthesis, representing scenes with spatially distributed
Gaussian primitives. While highly effective for rendering, achieving accurate
and complete surface reconstruction remains challenging due to the unstructured
nature of the representation and the absence of explicit geometric supervision.
In this work, we propose DiGS, a unified framework that embeds Signed Distance
Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong
and interpretable surface priors. By associating each Gaussian with a learnable
SDF value, DiGS explicitly aligns primitives with underlying geometry and
improves cross-view consistency. To further ensure dense and coherent coverage,
we design a geometry-guided grid growth strategy that adaptively distributes
Gaussians along geometry-consistent regions under a multi-scale hierarchy.
Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and
Tanks& Temples, demonstrate that DiGS consistently improves reconstruction
accuracy and completeness while retaining high rendering fidelity.

</details>


### [38] [Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition](https://arxiv.org/abs/2509.07495)
*Chun Liu,Hailong Wang,Bingqian Zhu,Panpan Ding,Zheng Zheng,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 论文提出一种面向遥感场景的非定向对抗攻击框架，通过局部混合与logits优化提升黑盒可迁移性，并结合平滑约束抑制高频噪声；在FGSCR-42与MTARSI上对6个替代模型、对比12种SOTA方法，平均提升显著（如在MTARSI上以ResNet为替代模型，黑盒成功率平均提高17.28%）。


<details>
  <summary>Details</summary>
Motivation: 现有混合类增强策略（如全局MixUp或区域替换MixCut）要么破坏全局语义，要么误导对抗优化；同时普遍依赖交叉熵优化，迭代中梯度易衰减，导致扰动质量与可迁移性不足。遥感应用对模型安全性要求高，需更稳健且可迁移的攻击以暴露脆弱点并促进防御研究。

Method: 提出三点创新：1) 局部混合策略：只对图像的局部区域进行加权融合，保持全局语义一致性，同时产生多样输入，提升迁移性；区别于MixUp（全局线性混合）与MixCut（拼接）。2) 非定向logit损失：将原用于定向攻击的logit损失改造用于非定向场景，缓解交叉熵带来的梯度消失问题，稳定迭代优化。3) 扰动平滑损失：抑制高频噪声，鼓励低频、可迁移扰动。整体在多替代模型上进行迭代生成，实现黑盒攻击。

Result: 在FGSCR-42与MTARSI数据集上，相比12种SOTA攻方法、在6个替代模型设定下均取得更高黑盒成功率；特别是在MTARSI上以ResNet为替代模型，黑盒攻击平均成功率提升17.28%。

Conclusion: 局部混合+logit优化+平滑正则的组合有效提升非定向对抗样本的可迁移性与质量，避免语义破坏和梯度衰减，在遥感分类任务中达到SOTA表现，为后续鲁棒性研究与防御设计提供参考。

Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing
significant security threats to their deployment in remote sensing
applications. Research on adversarial attacks not only reveals model
vulnerabilities but also provides critical insights for enhancing robustness.
Although current mixing-based strategies have been proposed to increase the
transferability of adversarial examples, they either perform global blending or
directly exchange a region in the images, which may destroy global semantic
features and mislead the optimization of adversarial examples. Furthermore,
their reliance on cross-entropy loss for perturbation optimization leads to
gradient diminishing during iterative updates, compromising adversarial example
quality. To address these limitations, we focus on non-targeted attacks and
propose a novel framework via local mixing and logits optimization. First, we
present a local mixing strategy to generate diverse yet semantically consistent
inputs. Different from MixUp, which globally blends two images, and MixCut,
which stitches images together, our method merely blends local regions to
preserve global semantic information. Second, we adapt the logit loss from
targeted attacks to non-targeted scenarios, mitigating the gradient vanishing
problem of cross-entropy loss. Third, a perturbation smoothing loss is applied
to suppress high-frequency noise and enhance transferability. Extensive
experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance
over 12 state-of-the-art methods across 6 surrogate models. Notably, with
ResNet as the surrogate on MTARSI, our method achieves a 17.28% average
improvement in black-box attack success rate.

</details>


### [39] [MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection](https://arxiv.org/abs/2509.07507)
*Saad Lahlali,Alexandre Fournier Montgieux,Nicolas Granger,Hervé Le Borgne,Quoc Cuong Pham*

Main category: cs.CV

TL;DR: 提出MVAT，通过时间多视角聚合点云与蒸馏学习，在仅用2D框标注的弱监督条件下实现高质量3D目标检测，逼近全监督性能。


<details>
  <summary>Details</summary>
Motivation: 3D框标注昂贵且费时，弱监督方法若仅依赖单视角2D框会遭遇投影歧义与遮挡导致的可见性不足，难以准确恢复3D姿态与尺寸。

Method: 利用序列数据的时间多视角信息：跨时间对同一目标的点云进行对象级配准与聚合，形成更稠密完整的3D表示；采用Teacher-Student蒸馏框架——Teacher在单视角输入下学习，但其训练目标来自对静止目标的时间聚合结果；Teacher随后为静止与运动目标生成高质量伪标签，Student学习从单视角预测；并引入多视角2D投影一致性损失，约束预测的3D框与所有可用2D标注一致。

Result: 在nuScenes与Waymo Open数据集上取得弱监督3D检测的SOTA，显著缩小与全监督方法的性能差距，且无需任何3D框标注。

Conclusion: 通过时间多视角聚合与蒸馏学习，可在仅用2D标注条件下有效解决投影歧义与可见性不足，显著提升弱监督3D检测性能，方法通用且实用性强。

Abstract: Annotating 3D data remains a costly bottleneck for 3D object detection,
motivating the development of weakly supervised annotation methods that rely on
more accessible 2D box annotations. However, relying solely on 2D boxes
introduces projection ambiguities since a single 2D box can correspond to
multiple valid 3D poses. Furthermore, partial object visibility under a single
viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,
a novel framework that leverages temporal multi-view present in sequential data
to address these challenges. Our approach aggregates object-centric point
clouds across time to build 3D object representations as dense and complete as
possible. A Teacher-Student distillation paradigm is employed: The Teacher
network learns from single viewpoints but targets are derived from temporally
aggregated static objects. Then the Teacher generates high quality
pseudo-labels that the Student learns to predict from a single viewpoint for
both static and moving objects. The whole framework incorporates a multi-view
2D projection loss to enforce consistency between predicted 3D boxes and all
available 2D annotations. Experiments on the nuScenes and Waymo Open datasets
demonstrate that MVAT achieves state-of-the-art performance for weakly
supervised 3D object detection, significantly narrowing the gap with fully
supervised methods without requiring any 3D box annotations. % \footnote{Code
available upon acceptance} Our code is available in our public repository
(\href{https://github.com/CEA-LIST/MVAT}{code}).

</details>


### [40] [EHWGesture -- A dataset for multimodal understanding of clinical gestures](https://arxiv.org/abs/2509.07525)
*Gianluca Amprimo,Alberto Ancilotto,Alessandro Savino,Fabio Quazzolo,Claudia Ferraris,Gabriella Olmo,Elisabetta Farella,Stefano Di Carlo*

Main category: cs.CV

TL;DR: 提出EHWGesture多模态手势数据集：5种临床相关手势、25名受试者、1100+段（约6小时），含双RGB-D与事件相机、动作捕捉提供精确手部关键点，设备已标定与同步，并提供按执行速度分级的动作质量标签；给出分类、触发检测与质量评估基线。


<details>
  <summary>Details</summary>
Motivation: 动态手势理解受复杂时空变化影响仍具挑战；现有数据集在多模态/多视角多样性、精确地面真值跟踪以及手势内嵌的动作质量维度方面不足，限制了临床手部灵巧度评估等应用。

Method: 构建EHWGesture数据集：采集25名健康被试执行5类临床相关手势，使用两台高分辨率RGB-D相机与一台事件相机同步采集；借助动作捕捉系统获取精确手部关键点轨迹；对所有设备进行空间标定与时间同步以实现跨模态对齐；将录制按执行速度划分以反映临床质量评估；并进行基线实验（分类、触发检测、质量评估）。

Result: 获得>1100段、约6小时的多模态、多视角数据，含精确手部关键点地面真值与速度等级标签；基线结果验证数据集对手势分类、手势触发检测与动作质量评估任务的有效性与潜力。

Conclusion: EHWGesture为动态手势理解，尤其是临床场景中的多模态研究，提供了高质量、对齐良好的基准，能够推动手势分类、触发检测与动作质量评估等任务的进展。

Abstract: Hand gesture understanding is essential for several applications in
human-computer interaction, including automatic clinical assessment of hand
dexterity. While deep learning has advanced static gesture recognition, dynamic
gesture understanding remains challenging due to complex spatiotemporal
variations. Moreover, existing datasets often lack multimodal and multi-view
diversity, precise ground-truth tracking, and an action quality component
embedded within gestures. This paper introduces EHWGesture, a multimodal video
dataset for gesture understanding featuring five clinically relevant gestures.
It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects
using two high-resolution RGB-Depth cameras and an event camera. A motion
capture system provides precise ground-truth hand landmark tracking, and all
devices are spatially calibrated and synchronized to ensure cross-modal
alignment. Moreover, to embed an action quality task within gesture
understanding, collected recordings are organized in classes of execution speed
that mirror clinical evaluations of hand dexterity. Baseline experiments
highlight the dataset's potential for gesture classification, gesture trigger
detection, and action quality assessment. Thus, EHWGesture can serve as a
comprehensive benchmark for advancing multimodal clinical gesture
understanding.

</details>


### [41] [Universal Few-Shot Spatial Control for Diffusion Models](https://arxiv.org/abs/2509.07530)
*Kiet T. Nguyen,Chanhuyk Lee,Donggyun Kim,Dong Hoon Lee,Seunghoon Hong*

Main category: cs.CV

TL;DR: 提出UFC，一种面向预训练文生图扩散模型的通用少样本空间控制适配器，能用极少标注样本在新颖空间条件下实现精细结构控制，并在多骨干（UNet、DiT）上取得接近全监督的方法表现。


<details>
  <summary>Details</summary>
Motivation: 现有空间控制适配器在面对与训练任务差异较大的新空间条件时，适应性差、训练成本高，缺乏对未见任务的快速泛化能力。

Method: 提出Universal Few-Shot Control (UFC)：给定未见任务的少量（支持集）图像-条件对和查询条件，利用查询与支持条件的类比匹配机制，构造任务特定的控制特征；并通过少量任务特定参数的更新实现快速适配。该适配器可无关地插接于不同扩散骨干（UNet、DiT）。

Result: 在六个新颖空间控制任务上，只用30个标注样本微调即可实现与条件一致的精细结构控制；在仅使用全量训练数据0.1%的情况下，性能可与全监督基线竞争。

Conclusion: UFC在少样本场景下显著提升对新空间条件的泛化与控制精度，且具有骨干无关性与低训练开销，适用于多种扩散模型结构。

Abstract: Spatial conditioning in pretrained text-to-image diffusion models has
significantly improved fine-grained control over the structure of generated
images. However, existing control adapters exhibit limited adaptability and
incur high training costs when encountering novel spatial control conditions
that differ substantially from the training tasks. To address this limitation,
we propose Universal Few-Shot Control (UFC), a versatile few-shot control
adapter capable of generalizing to novel spatial conditions. Given a few
image-condition pairs of an unseen task and a query condition, UFC leverages
the analogy between query and support conditions to construct task-specific
control features, instantiated by a matching mechanism and an update on a small
set of task-specific parameters. Experiments on six novel spatial control tasks
show that UFC, fine-tuned with only 30 annotated examples of novel tasks,
achieves fine-grained control consistent with the spatial conditions. Notably,
when fine-tuned with 0.1% of the full training data, UFC achieves competitive
performance with the fully supervised baselines in various control tasks. We
also show that UFC is applicable agnostically to various diffusion backbones
and demonstrate its effectiveness on both UNet and DiT architectures. Code is
available at https://github.com/kietngt00/UFC.

</details>


### [42] [HU-based Foreground Masking for 3D Medical Masked Image Modeling](https://arxiv.org/abs/2509.07534)
*Jin Lee,Vu Dang,Gwang-Hyun Yu,Anh Le,Zahid Rahman,Jin-Ho Jang,Heonzoo Lee,Kun-Yung Kim,Jin-Sul Kim,Jin-Young Kim*

Main category: cs.CV

TL;DR: 提出一种基于HU值的前景遮罩策略，改进3D医学图像的MIM预训练，聚焦器官组织、排除无诊断意义区域；在五个数据集上显著提升分割性能（Dice提升，列出具体成绩）。


<details>
  <summary>Details</summary>
Motivation: 随机遮挡的MIM未考虑解剖目标的空间/密度分布，3D医学图像中大量空气/液体区域无信息，导致预训练信号稀释、学习低效。需要域知识引导的遮罩，让模型专注于有诊断价值的前景组织。

Method: 利用CT的Hounsfield Unit（HU）范围界定前景：根据HU强度分布选择内脏器官对应的强度区间，生成前景遮罩，排除空气与液体等非组织区域；在此前景上进行遮挡并进行重建式MIM预训练。与标准随机遮挡MIM仅改变遮罩策略，其余训练框架保持一致。

Result: 在五个公共3D医学影像分割数据集上，采用该遮罩的一致性增益：BTCV 84.64%，Flare22 92.43%，MM-WHS 90.67%，AMOS22 88.64%，BraTS 78.55%（Dice）。相较随机遮挡基线，质量与Dice均有提升。

Conclusion: 域知识驱动的MIM比纯随机遮挡更适配医学分割任务；HU前景遮罩是简单有效的改动，能稳定提升表示学习与下游分割性能，提示未来应在MIM中融入医学成像先验。

Abstract: While Masked Image Modeling (MIM) has revolutionized fields of computer
vision, its adoption in 3D medical image computing has been limited by the use
of random masking, which overlooks the density of anatomical objects. To
address this limitation, we enhance the pretext task with a simple yet
effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we
implement an HU-based Foreground Masking, which focuses on the intensity
distribution of visceral organs and excludes non-tissue regions, such as air
and fluid, that lack diagnostically meaningful features. Extensive experiments
on five public 3D medical imaging datasets demonstrate that our masking
consistently improves performance, both in quality of segmentation and Dice
score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%,
BraTS:~78.55\%). These results underscore the importance of domain-centric MIM
and suggest a promising direction for representation learning in medical image
segmentation. Implementation is available at github.com/AISeedHub/SubFore/.

</details>


### [43] [TextlessRAG: End-to-End Visual Document RAG by Speech Without Text](https://arxiv.org/abs/2509.07538)
*Peijin Xie,Shun Qian,Bingquan Liu,Dexin Wang,Lin Sun,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: 提出TextlessRAG：首个面向文档图像的端到端“语音→检索→作答”系统，去除ASR/TTS/OCR，直接以语音检索视觉知识并生成答案，并加入版面感知重排序；同时发布中英双语语音-文档RAG数据集，实验显示效率与准确性显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有知识问答多依赖文本（ASR、OCR、TTS）链路，带来误差累积、延迟和跨语种/噪声环境适应性差的问题。文档图像含丰富结构化/版面信息，但缺乏直接面向“语音查询+文档图像”的端到端方案与数据集。

Method: 构建全“无文本”管线：直接编码语音查询；在大规模文档图像库上进行多模态检索；采用版面感知重排序（考虑版面/布局结构提升相关性）；再在检索结果上以多模态生成器直接输出答案，无需ASR、OCR、TTS。并发布中英双语语音-文档RAG数据集以训练/评测。

Result: 在效率与准确率上均显著优于含ASR/OCR等基线；版面感知重排序进一步提升检索质量与最终问答性能。

Conclusion: TextlessRAG验证了“语音直达视觉知识”的可行性与优势，减少误差链路、加速推理，并通过双语数据集推动该方向研究；代码与数据集将开源。

Abstract: Document images encapsulate a wealth of knowledge, while the portability of
spoken queries enables broader and flexible application scenarios. Yet, no
prior work has explored knowledge base question answering over visual document
images with queries provided directly in speech. We propose TextlessRAG, the
first end-to-end framework for speech-based question answering over large-scale
document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR,
directly interpreting speech, retrieving relevant visual knowledge, and
generating answers in a fully textless pipeline. To further boost performance,
we integrate a layout-aware reranking mechanism to refine retrieval.
Experiments demonstrate substantial improvements in both efficiency and
accuracy. To advance research in this direction, we also release the first
bilingual speech--document RAG dataset, featuring Chinese and English voice
queries paired with multimodal document content. Both the dataset and our
pipeline will be made available at
repository:https://github.com/xiepeijinhit-hue/textlessrag

</details>


### [44] [PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image](https://arxiv.org/abs/2509.07552)
*Peng Li,Yisheng He,Yingdong Hu,Yuan Dong,Weihao Yuan,Yuan Liu,Zilong Dong,Yike Guo*

Main category: cs.CV

TL;DR: 提出一种从单张未配准图像直接前向推理重建高斯全头模型的框架，无需GAN反演或测试时优化；利用3D GAN合成数据训练，采用粗到细流程与双分支特征聚合（球面triplane+点特征）实现快速高保真重建与渲染。


<details>
  <summary>Details</summary>
Motivation: 现有全头重建依赖耗时的GAN反演和测试时优化，推理慢且数据稀缺（缺少大规模3D头部资产）。需要一种既快又高保真的从单图重建方案，并能在缺少真实3D数据时充分利用预训练3D GAN先验。

Method: 1) 数据：从训练好的3D GAN合成大规模头部数据，仅用合成数据训练。2) 架构：纯前向的高斯全头合成框架。3) 粗到细：用FLAME稀疏点与图像特征通过Transformer交互，提取特征并进行粗形状重建，再进行点/高斯密化实现高保真。4) 双分支：融合结构化的球面triplane特征与非结构化点特征，充分利用3D GAN先验以提升重建效果。

Result: 实验显示在速度与重建质量上优于或不逊于现有方法，实现单次前向快速重建与渲染；在单图输入下获得高保真全头高斯表示。

Conclusion: 通过合成数据训练、粗到细密化与双分支特征聚合，框架在无需测试时优化的情况下实现快速高质量的全头高斯重建，验证了预训练3D GAN先验与点/平面特征融合的有效性。

Abstract: We present a feed-forward framework for Gaussian full-head synthesis from a
single unposed image. Unlike previous work that relies on time-consuming GAN
inversion and test-time optimization, our framework can reconstruct the
Gaussian full-head model given a single unposed image in a single forward pass.
This enables fast reconstruction and rendering during inference. To mitigate
the lack of large-scale 3D head assets, we propose a large-scale synthetic
dataset from trained 3D GANs and train our framework using only synthetic data.
For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian
head generation pipeline, where sparse points from the FLAME model interact
with the image features by transformer blocks for feature extraction and coarse
shape reconstruction, which are then densified for high-fidelity
reconstruction. To fully leverage the prior knowledge residing in pretrained 3D
GANs for effective reconstruction, we propose a dual-branch framework that
effectively aggregates the structured spherical triplane feature and
unstructured point-based features for more effective Gaussian head
reconstruction. Experimental results show the effectiveness of our framework
towards existing work.

</details>


### [45] [Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks](https://arxiv.org/abs/2509.07581)
*Barkin Buyukcakir,Rocharles Cavalcante Fontenele,Reinhilde Jacobs,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 提出一种面向3D形状识别的“类节点图注意力网络（CGAT）”，在第三磨牙Demirjian发育分期任务上实现可解释与可用性能兼顾；通过引入全局CLS类节点与注意力rollout可视化，结合曲率与到质心距离等节点特征，获得0.76加权F1并产生更直观的注意力图。


<details>
  <summary>Details</summary>
Motivation: 高风险场景（医学、法庭）需要可解释且可审计的深度学习。3D形状识别中的黑盒性限制了临床落地，故需要一种既具竞争性能又能提供人类可理解解释的模型。

Method: 提出CGAT：在图上进行注意力卷积，引入全局CLS类节点与有向边；使用注意力rollout对决策进行可视化解释。节点特征考察了局部平均曲率、到质心距离及其组合，并比较不同网络深度与是否使用CLS节点对性能与可解释性的影响。

Result: 使用带有向边连接至全局CLS节点的模型产生更直观的注意力图并保持良好分类表现；曲率+到质心距离的特征组合略提升性能至加权F1=0.76，同时提供更全面的注意力可视化。

Conclusion: CGAT在牙科CBCT第三磨牙分期上实现了可解释与性能的平衡，生成的人类可理解注意力图可增强信任并便于专家验证；方法对一般图分类/回归任务具有可迁移性，有望促进透明、可竞争深度学习在高风险领域的应用。

Abstract: Deep learning offers a promising avenue for automating many recognition tasks
in fields such as medicine and forensics. However, the black-box nature of
these models hinders their adoption in high-stakes applications where trust and
accountability are required. For 3D shape recognition tasks in particular, this
paper introduces the Class Node Graph Attention Network (CGAT) architecture to
address this need. Applied to 3D meshes of third molars derived from CBCT
images, for Demirjian stage allocation, CGAT utilizes graph attention
convolutions and an inherent attention mechanism, visualized via attention
rollout, to explain its decision-making process. We evaluated the local mean
curvature and distance to centroid node features, both individually and in
combination, as well as model depth, finding that models incorporating directed
edges to a global CLS node produced more intuitive attention maps, while also
yielding desirable classification performance. We analyzed the attention-based
explanations of the models, and their predictive performances to propose
optimal settings for the CGAT. The combination of local mean curvature and
distance to centroid as node features yielded a slight performance increase
with 0.76 weighted F1 score, and more comprehensive attention visualizations.
The CGAT architecture's ability to generate human-understandable attention maps
can enhance trust and facilitate expert validation of model decisions. While
demonstrated on dental data, CGAT is broadly applicable to graph-based
classification and regression tasks, promoting wider adoption of transparent
and competitive deep learning models in high-stakes environments.

</details>


### [46] [Temporal Image Forensics: A Review and Critical Evaluation](https://arxiv.org/abs/2509.07591)
*Robert Jöchl,Andreas Uhl*

Main category: cs.CV

TL;DR: 综述“时间性图像取证”基于成像链路产生的随时间变化痕迹（如传感器在场缺陷、灰尘），系统梳理其特性与方法，强调内容偏置风险与可解释AI的重要性；提出更现实的评测设定，复核关键假设，复现实验并发现部分方法并非利用年龄痕迹而是被内容干扰；并分析神经网络在掌纹数据上的学习特征与其易受分心的现象。


<details>
  <summary>Details</summary>
Motivation: 图像拍摄时间是关键取证线索，但现有方法依赖的“年龄痕迹”可靠性、可复现性与抗内容偏置性存疑；领域缺乏系统评估和更现实设定，也缺少对深度模型到底学了什么的可解释验证。

Method: 文献回顾+再实现与复现实验：详述两类年龄痕迹（在场传感器缺陷、传感器灰尘）的增长率与空间分布；构建更贴近实务的新取证设定；对利用在场缺陷估龄的方法进行再评估；训练并剖析一个用于掌纹图像定年的神经网络，采用可解释AI手段分析其特征来源与偏置；设计干扰实验验证网络易被非年龄因素吸引。

Result: 验证了在场传感器缺陷的关键属性（增长速率、空间分布）；发现某估龄方法主要受内容偏置驱动而非真实年龄痕迹；解释性分析揭示掌纹定年网络学习到的特征与年龄无关成分；证明神经网络很容易被非目标痕迹分散注意。

Conclusion: 时间性图像取证需警惕内容偏置并引入可解释AI做可靠性核验；当前部分方法对年龄痕迹的依赖被高估。更现实的评测设定与严格复现至关重要，以避免将内容相关信号误判为时间线索，并推动稳健可泛化的估龄技术。

Abstract: Temporal image forensics is the science of estimating the age of a digital
image. Usually, time-dependent traces (age traces) introduced by the image
acquisition pipeline are exploited for this purpose. In this review, a
comprehensive overview of the field of temporal image forensics based on
time-dependent traces from the image acquisition pipeline is given. This
includes a detailed insight into the properties of known age traces (i.e.,
in-field sensor defects and sensor dust) and temporal image forensics
techniques. Another key aspect of this work is to highlight the problem of
content bias and to illustrate how important eXplainable Artificial
Intelligence methods are to verify the reliability of temporal image forensics
techniques. Apart from reviewing material presented in previous works, in this
review: (i) a new (probably more realistic) forensic setting is proposed; (ii)
the main properties (growth rate and spatial distribution) of in-field sensor
defects are verified; (iii) it is shown that a method proposed to utilize
in-field sensor defects for image age approximation actually exploits other
traces (most likely content bias); (iv) the features learned by a neural
network dating palmprint images are further investigated; (v) it is shown how
easily a neural network can be distracted from learning age traces. For this
purpose, previous work is analyzed, re-implemented if required and experiments
are conducted.

</details>


### [47] [Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation](https://arxiv.org/abs/2509.07596)
*Yusuke Hirota,Ryo Hachiuma,Boyi Li,Ximing Lu,Michael Ross Boone,Boris Ivanovic,Yejin Choi,Marco Pavone,Yu-Chiang Frank Wang,Noa Garcia,Yuta Nakashima,Chao-Han Huck Yang*

Main category: cs.CV

TL;DR: 论文揭示：VLM性别偏见评测被“非性别特征”严重干扰；对对象/背景做极小扰动就会显著改变偏见分数，现有基准的可靠性存疑。建议在偏见指标外同时报告对特征扰动的敏感性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM性别偏见评估常用带性别标注的真实图像基准，但这些数据中性别与物体、背景等非性别特征存在偶然相关。作者质疑：这些“伪相关特征”是否会扭曲偏见评估，从而误把对非性别特征的反应当成性别偏见。

Method: 在四个常用基准（COCO-gender、FACET、MIAP、PHASE）上，系统性地对非性别特征施加轻量扰动（如遮挡少量对象、弱化背景模糊），并在多种VLM（包含生成式与CLIP变体）上测量偏见指标变化，以量化偏见评估对伪相关特征的敏感度。

Result: 即便是极小扰动（例如仅遮挡10%的对象或轻度模糊背景），也会显著改变偏见分数：生成式VLM的偏见指标最高可变化175%，CLIP变体最高达43%。说明偏见评估很大程度反映了模型对非性别特征的反应。

Conclusion: 由于从根本上很难构建完全无伪相关的基准，单独报告偏见分数不可靠。应在发布偏见评估时同时报告对非性别特征扰动的敏感性指标，以获得更稳健、可信的偏见诊断。

Abstract: Gender bias in vision-language foundation models (VLMs) raises concerns about
their safe deployment and is typically evaluated using benchmarks with gender
annotations on real-world images. However, as these benchmarks often contain
spurious correlations between gender and non-gender features, such as objects
and backgrounds, we identify a critical oversight in gender bias evaluation: Do
spurious features distort gender bias evaluation? To address this question, we
systematically perturb non-gender features across four widely used benchmarks
(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact
on bias evaluation. Our findings reveal that even minimal perturbations, such
as masking just 10% of objects or weakly blurring backgrounds, can dramatically
alter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in
CLIP variants. This suggests that current bias evaluations often reflect model
responses to spurious features rather than gender bias, undermining their
reliability. Since creating spurious feature-free benchmarks is fundamentally
challenging, we recommend reporting bias metrics alongside feature-sensitivity
measurements to enable a more reliable bias assessment.

</details>


### [48] [Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2509.07613)
*Fangqi Cheng,Surajit Ray,Xiaochen Yang*

Main category: cs.CV

TL;DR: 提出一种数据高效的微调流程，将基于3D CT的医学视觉-语言模型迁移到3D MRI，用于阿尔茨海默病诊断；通过将结构化元数据转为合成报告并加入预测MMSE的辅助token，结合轻量级prompt tuning，在仅1500张训练图像下达到SOTA，超越使用1万张图像微调的方法。


<details>
  <summary>Details</summary>
Motivation: 现有Med-VLM存在：1) 对患者元数据利用不足、缺乏临床诊断知识融合；2) 多在大规模2D图文上从头训练或微调，计算成本高；3) 对3D医学影像效果有限，缺乏结构信息。需要一种数据与计算更高效、能融合临床与元数据的3D适配方案。

Method: 提出数据高效微调管线，将3D CT训练的Med-VLM适配到3D MRI并用于AD：- 将结构化患者元数据转为合成文本报告，增强图文对齐；- 在模型中加入辅助token，训练其预测MMSE分数，提供额外监督；- 对图像与文本模态采用轻量级prompt tuning以减少参数更新与计算成本。

Result: 在两个AD数据集上，以仅1500张训练图像实现SOTA表现，优于需要1万张图像微调的现有方法。

Conclusion: 通过元数据到合成报告的文本增强与MMSE辅助预测监督，配合轻量级prompt tuning，可高效地将3D CT基础的Med-VLM迁移到3D MRI并在AD诊断上取得领先性能，显示出数据与计算效率的优势。

Abstract: Medical vision-language models (Med-VLMs) have shown impressive results in
tasks such as report generation and visual question answering, but they still
face several limitations. Most notably, they underutilize patient metadata and
lack integration of clinical diagnostic knowledge. Moreover, most existing
models are typically trained from scratch or fine-tuned on large-scale 2D
image-text pairs, requiring extensive computational resources, and their
effectiveness on 3D medical imaging is often limited due to the absence of
structural information. To address these gaps, we propose a data-efficient
fine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate
its application in Alzheimer's disease (AD) diagnosis. Our system introduces
two key innovations. First, we convert structured metadata into synthetic
reports, enriching textual input for improved image-text alignment. Second, we
add an auxiliary token trained to predict the mini-mental state examination
(MMSE) score, a widely used clinical measure of cognitive function that
correlates with AD severity. This provides additional supervision for
fine-tuning. Applying lightweight prompt tuning to both image and text
modalities, our approach achieves state-of-the-art performance on two AD
datasets using 1,500 training images, outperforming existing methods fine-tuned
on 10,000 images. Code will be released upon publication.

</details>


### [49] [Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2509.07623)
*Fangqi Cheng,Yingying Zhao,Xiaochen Yang*

Main category: cs.CV

TL;DR: 提出一种自监督跨编码器用于纵向MRI的时序连续性学习，将表征拆分为静态与动态两部分，在ADNI上取得更高分类精度与可解释性，并在OASIS与PPMI上展现良好零样本与跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习诊断神经退行性疾病依赖大量标注且可解释性差；纵向MRI蕴含时间连续性信息但未被充分利用，需要一种既减轻标注需求又提升可解释性的表征学习方法。

Method: 构建自监督跨编码器，利用纵向MRI的时序连续性进行监督；将表征解耦为静态与动态两部分：静态表征通过对比学习约束，捕获稳定解剖结构；动态表征通过输入梯度正则化引导，刻画随时间变化信号，并可高效微调用于下游分类。

Result: 在ADNI数据集上获得更高的分类准确率与更好的可解释性；在OASIS上表现出强零样本泛化；在PPMI上实现跨任务泛化。

Conclusion: 通过时序自监督与表征解耦，方法在减少标注依赖的同时提升诊断性能与可解释性，并具备良好的跨数据集与跨任务泛化潜力；代码将公开。

Abstract: Deep learning has shown significant potential in diagnosing neurodegenerative
diseases from MRI data. However, most existing methods rely heavily on large
volumes of labeled data and often yield representations that lack
interpretability. To address both challenges, we propose a novel
self-supervised cross-encoder framework that leverages the temporal continuity
in longitudinal MRI scans for supervision. This framework disentangles learned
representations into two components: a static representation, constrained by
contrastive learning, which captures stable anatomical features; and a dynamic
representation, guided by input-gradient regularization, which reflects
temporal changes and can be effectively fine-tuned for downstream
classification tasks. Experimental results on the Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset demonstrate that our method achieves
superior classification accuracy and improved interpretability. Furthermore,
the learned representations exhibit strong zero-shot generalization on the Open
Access Series of Imaging Studies (OASIS) dataset and cross-task generalization
on the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the
proposed method will be made publicly available.

</details>


### [50] [Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity](https://arxiv.org/abs/2509.07647)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出一种用于潜语义扩散模型(LDM)的频域语义水印方法SFW，通过强制厄米对称保持频谱完整性，并配合中心感知嵌入以抵御裁剪；在多种攻击下实现SOTA的检测/识别，且图像保真度更优。


<details>
  <summary>Details</summary>
Motivation: 现有语义水印能抗重生成，但因频率完整性受损导致检测准确度下降，且对裁剪等空间攻击脆弱，存在鲁棒性与图像保真之间的权衡。

Method: 1) Hermitian Symmetric Fourier Watermarking：在频域嵌入并强制厄米对称，确保实图像的频谱一致性，提升可检性与稳健性；2) 中心感知嵌入：在更中心、信息密集区域进行鲁棒嵌入，降低裁剪丢失；3) 将上述策略集成到现有语义水印方案，优化频域结构；4) 进行大规模实验与消融。

Result: 在多种攻击场景（重生成、裁剪等）下实现SOTA的验证与身份识别性能；消融显示SFW显著提升检测能力、中心感知嵌入有效抗裁剪、消息容量影响识别精度；同时FID与CLIP指标显示图像保真度优于既有方法。

Conclusion: SFW为语义水印提供了兼顾鲁棒性与保真的统一框架：通过厄米对称维持频率完整性并用中心感知策略抗裁剪，整体检测与识别性能领先且图像质量更佳；代码已开源。

Abstract: Semantic watermarking techniques for latent diffusion models (LDMs) are
robust against regeneration attacks, but often suffer from detection
performance degradation due to the loss of frequency integrity. To tackle this
problem, we propose a novel embedding method called Hermitian Symmetric Fourier
Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian
symmetry. Additionally, we introduce a center-aware embedding strategy that
reduces the vulnerability of semantic watermarking due to cropping attacks by
ensuring robust information retention. To validate our approach, we apply these
techniques to existing semantic watermarking schemes, enhancing their
frequency-domain structures for better robustness and retrieval accuracy.
Extensive experiments demonstrate that our methods achieve state-of-the-art
verification and identification performance, surpassing previous approaches
across various attack scenarios. Ablation studies confirm the impact of SFW on
detection capabilities, the effectiveness of the center-aware embedding against
cropping, and how message capacity influences identification accuracy. Notably,
our method achieves the highest detection accuracy while maintaining superior
image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed
SFW is shown to be an effective framework for balancing robustness and image
fidelity, addressing the inherent trade-offs in semantic watermarking. Code
available at https://github.com/thomas11809/SFWMark

</details>


### [51] [Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection](https://arxiv.org/abs/2509.07654)
*Guoyi Zhang,Siyang Chen,Guangsheng Xu,Zhihua Shen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 提出TenRPCANet：以张量低秩+稀疏分解思想为先验，通过自注意力实现多阶低秩背景建模、结合稀疏更新式特征精炼，提升小目标检测鲁棒性；在红外小目标与空间目标两类任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 小目标在低信噪比、弱纹理、背景杂波下难检测，现有方法多依赖目标外观或简单运动线索，泛化与鲁棒性不足。观察到视频背景常具强低秩结构，可作为稳定先验与目标（稀疏）形成可分性，因此希望以低秩-稀疏分解视角统一建模，提高复杂场景鲁棒性与跨任务泛化。

Method: 将小目标检测重构为张量低秩+稀疏分解问题：对背景建模为多阶低秩，对目标为稀疏，对噪声显式区分；据此设计TenRPCANet。核心包括：1）基于tokenization的自注意力模块，隐式施加多阶张量低秩先验，捕获局部与非局部自相似性，无需显式迭代优化；2）受张量RPCA稀疏项更新启发的特征精炼模块，突出稀疏小目标显著性；3）端到端训练，最小化对目标外观假设的依赖。

Result: 在两类差异显著且具有挑战性的任务上取得SOTA：多帧红外小目标检测与空间目标检测，显示在复杂背景、低信噪比条件下具有更高检测精度与鲁棒性。

Conclusion: 将低秩背景与稀疏目标的耦合关系纳入深度架构设计，通过自注意力实现低秩先验与特征精炼增强稀疏目标，可在不依赖目标先验的情况下实现更强泛化与SOTA表现，验证了张量RPCA思想与深度学习结合的有效性与通用性。

Abstract: Small moving target detection is crucial for many defense applications but
remains highly challenging due to low signal-to-noise ratios, ambiguous visual
cues, and cluttered backgrounds. In this work, we propose a novel deep learning
framework that differs fundamentally from existing approaches, which often rely
on target-specific features or motion cues and tend to lack robustness in
complex environments. Our key insight is that small target detection and
background discrimination are inherently coupled, even cluttered video
backgrounds often exhibit strong low-rank structures that can serve as stable
priors for detection. We reformulate the task as a tensor-based low-rank and
sparse decomposition problem and conduct a theoretical analysis of the
background, target, and noise components to guide model design. Building on
these insights, we introduce TenRPCANet, a deep neural network that requires
minimal assumptions about target characteristics. Specifically, we propose a
tokenization strategy that implicitly enforces multi-order tensor low-rank
priors through a self-attention mechanism. This mechanism captures both local
and non-local self-similarity to model the low-rank background without relying
on explicit iterative optimization. In addition, inspired by the sparse
component update in tensor RPCA, we design a feature refinement module to
enhance target saliency. The proposed method achieves state-of-the-art
performance on two highly distinct and challenging tasks: multi-frame infrared
small target detection and space object detection. These results demonstrate
both the effectiveness and the generalizability of our approach.

</details>


### [52] [EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image Registration](https://arxiv.org/abs/2509.07662)
*Haokai Zhu,Bo Qu,Si-Yuan Cao,Runmin Zhang,Shujie Chen,Bailin Yang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: 提出EDFFDNet，一种基于指数衰减自由形变和稀疏运动聚合的新框架，结合渐进相关细化，在深度差异场景下实现更高效更准确的图像配准，并以更少参数/内存/时间超过SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像配准（单/多重单应、薄板样条）在真实场景的视差与深度不一致下表现欠佳，存在全局性强、自由度分配不当、参数量大与效率低的问题，需要一种既具局部性、又高效且可泛化的方法。

Method: 1) 设计EDFFDNet：采用自由形变（FFD）但以指数衰减基函数作为局部形变核，天然具局部性与高效性；2) 提出ASMA自适应稀疏运动聚合器：将以往的MLP式密集交互改为自适应稀疏交互，减少参数并提升精度；3) 渐进式相关细化：结合全局-局部相关进行由粗到细的运动估计；4) 变体EDFFDNet-2加入额外本地精修阶段。

Result: 相较SOTA：参数减少约70.5%，内存降低32.6%，总运行时间减少33.7%，PSNR提升0.5 dB；加入本地精修（EDFFDNet-2）后PSNR再升1.06 dB，同时计算开销仍低于SOTA；在跨数据集泛化上优于既有深度方法。

Conclusion: 指数衰减FFD结合稀疏运动聚合与渐进相关细化可在深度差异场景下实现更高效、准确且具泛化性的图像配准，新方法以更低成本超越当前SOTA，并可通过附加精修进一步提升。

Abstract: Previous deep image registration methods that employ single homography,
multi-grid homography, or thin-plate spline often struggle with real scenes
containing depth disparities due to their inherent limitations. To address
this, we propose an Exponential-Decay Free-Form Deformation Network (EDFFDNet),
which employs free-form deformation with an exponential-decay basis function.
This design achieves higher efficiency and performs well in scenes with depth
disparities, benefiting from its inherent locality. We also introduce an
Adaptive Sparse Motion Aggregator (ASMA), which replaces the MLP motion
aggregator used in previous methods. By transforming dense interactions into
sparse ones, ASMA reduces parameters and improves accuracy. Additionally, we
propose a progressive correlation refinement strategy that leverages
global-local correlation patterns for coarse-to-fine motion estimation, further
enhancing efficiency and accuracy. Experiments demonstrate that EDFFDNet
reduces parameters, memory, and total runtime by 70.5%, 32.6%, and 33.7%,
respectively, while achieving a 0.5 dB PSNR gain over the state-of-the-art
method. With an additional local refinement stage,EDFFDNet-2 further improves
PSNR by 1.06 dB while maintaining lower computational costs. Our method also
demonstrates strong generalization ability across datasets, outperforming
previous deep learning methods.

</details>


### [53] [Nearest Neighbor Projection Removal Adversarial Training](https://arxiv.org/abs/2509.07673)
*Himanshu Singh,A. V. Subramanyam,Shivank Rajput,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: 提出一种通过在特征空间投影消除类间依赖来提升对抗鲁棒性的对抗训练框架；通过最近异类邻居投影剔除与logits校正，降低网络Lipschitz常数与Rademacher复杂度，实验在CIFAR-10/100与SVHN上在干净与鲁棒精度均达到领先或竞争水平。


<details>
  <summary>Details</summary>
Motivation: 标准对抗训练未显式处理类间特征重叠（inter-class proximity），这会导致对抗样本更容易跨类混淆，降低鲁棒性与泛化。需要一种能直接拉开类间特征距离、减少互相干扰的机制。

Method: 在对抗训练过程中：1) 对每个对抗样本在特征空间中寻找最近的异类邻居；2) 将样本特征在这些异类邻居方向上的分量投影并移除，从而减少类间依赖、增强可分性；3) 结合logits校正以控制模型的Lipschitz常数。方法同时作用于干净与对抗样本。

Result: 理论上证明logits校正可降低模型Lipschitz常数与Rademacher复杂度，从而提升泛化与鲁棒；实证上在CIFAR-10/100、SVHN上，相较主流对抗训练方法，取得具有竞争力的鲁棒精度，并在干净精度上也有显著优势。

Conclusion: 显式缓解类间特征接近是提升DNN对抗鲁棒性的关键；通过对特征进行“异类投影剔除”与logits校正，可同时提升鲁棒与干净性能，验证了该方向的有效性。

Abstract: Deep neural networks have exhibited impressive performance in image
classification tasks but remain vulnerable to adversarial examples. Standard
adversarial training enhances robustness but typically fails to explicitly
address inter-class feature overlap, a significant contributor to adversarial
susceptibility. In this work, we introduce a novel adversarial training
framework that actively mitigates inter-class proximity by projecting out
inter-class dependencies from adversarial and clean samples in the feature
space. Specifically, our approach first identifies the nearest inter-class
neighbors for each adversarial sample and subsequently removes projections onto
these neighbors to enforce stronger feature separability. Theoretically, we
demonstrate that our proposed logits correction reduces the Lipschitz constant
of neural networks, thereby lowering the Rademacher complexity, which directly
contributes to improved generalization and robustness. Extensive experiments
across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that
our method demonstrates strong performance that is competitive with leading
adversarial training techniques, highlighting significant achievements in both
robust and clean accuracy. Our findings reveal the importance of addressing
inter-class feature proximity explicitly to bolster adversarial robustness in
DNNs.

</details>


### [54] [CAViAR: Critic-Augmented Video Agentic Reasoning](https://arxiv.org/abs/2509.07680)
*Sachit Menon,Ahmet Iscen,Arsha Nagrani,Tobias Weyand,Carl Vondrick,Cordelia Schmid*

Main category: cs.CV

TL;DR: 提出一个基于LLM的“代理+评论家(critic)”框架，用可调用的视频感知模块作为工具，动态规划多步推理流程，在长视频与复杂查询基准上取得强绩效。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解在短片感知上进步显著，但在长视频与复杂推理任务（LVBench、Neptune、ActivityNet-RTL等）上性能下滑。作者希望利用已经很强的感知能力，通过更好的推理与决策机制，将其转化为复杂视频推理的有效方案。

Method: 构建一个LLM代理，可调用多种视频模块（检测、检索、OCR、ASR、动作识别等）作为子代理/工具。不同于固定流水线（如Visual Programming、ViperGPT、MoReVQA），该代理依据每次模块输出自适应决定下一步调用与计划。同时引入一个critic组件，学习区分成功与失败的推理序列，用于指导或筛选代理的步骤与结果，提高鲁棒性与成功率。

Result: 在LVBench、Neptune、ActivityNet-RTL等复杂视频推理与长视频基准上，代理+critic的组合达到强竞争或SOTA级别的表现（摘要宣称优异成绩）。

Conclusion: 动态工具调用的LLM代理结合critic评估能有效将现有感知能力转换为复杂视频推理能力，显著提升长视频与复杂查询任务的表现，相较固定流程方法更灵活、效果更强。

Abstract: Video understanding has seen significant progress in recent years, with
models' performance on perception from short clips continuing to rise. Yet,
multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show
performance wanes for tasks requiring complex reasoning on videos as queries
grow more complex and videos grow longer. In this work, we ask: can existing
perception capabilities be leveraged to successfully perform more complex video
reasoning? In particular, we develop a large language model agent given access
to video modules as subagents or tools. Rather than following a fixed procedure
to solve queries as in previous work such as Visual Programming, ViperGPT, and
MoReVQA, the agent uses the results of each call to a module to determine
subsequent steps. Inspired by work in the textual reasoning domain, we
introduce a critic to distinguish between instances of successful and
unsuccessful sequences from the agent. We show that the combination of our
agent and critic achieve strong performance on the previously-mentioned
datasets.

</details>


### [55] [SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression](https://arxiv.org/abs/2509.07704)
*Chunhang Zheng,Zichang Ren,Dou Li*

Main category: cs.CV

TL;DR: 提出SEEC：用语义分割引导多熵模型的无损图像压缩，针对不同语义区域分配专用熵模型并用多通道离散Logistic混合似然建模像素分布，达成SOTA压缩率且时延增加很小，并支持基于分割掩码的ROI编码。


<details>
  <summary>Details</summary>
Motivation: 单一熵模型对整幅图像建模，难以兼顾不同语义区域的统计差异，限制无损压缩性能；需要一种能按语义自适应概率建模的方法以提升压缩率并保持低延迟。

Method: 1) 提取图像特征；2) 执行语义分割得到区域/掩码；3) 为不同语义区域选择/适配专用熵模型（多熵模型架构）；4) 采用多通道离散Logistic混合似然对像素值分布建模；5) 在编码/解码时根据分割指导使用对应熵模型，可选支持ROI优先。

Result: 在基准数据集上获得SOTA的无损压缩比，同时仅带来极小的编解码时延；在提供分割掩码条件下可实现ROI编码。

Conclusion: 语义分割辅助的多熵模型能更精确地拟合不同语义区域统计，显著提升无损图像压缩效率且保持低延迟，具备ROI编码扩展性。

Abstract: Recently, learned image compression has attracted considerable attention due
to its superior performance over traditional methods. However, most existing
approaches employ a single entropy model to estimate the probability
distribution of pixel values across the entire image, which limits their
ability to capture the diverse statistical characteristics of different
semantic regions. To overcome this limitation, we propose Segmentation-Assisted
Multi-Entropy Models for Lossless Image Compression (SEEC). Our framework
utilizes semantic segmentation to guide the selection and adaptation of
multiple entropy models, enabling more accurate probability distribution
estimation for distinct semantic regions. Specifically, SEEC first extracts
image features and then applies semantic segmentation to identify different
regions, each assigned a specialized entropy model to better capture its unique
statistical properties. Finally, a multi-channel discrete logistic mixture
likelihood is employed to model the pixel value distributions effectively.
Experimental results on benchmark datasets demonstrate that SEEC achieves
state-of-the-art compression ratios while introducing only minimal encoding and
decoding latency. With superior performance, the proposed model also supports
Regions of Interest (ROIs) coding condition on the provided segmentation mask.
Our code is available at https://github.com/chunbaobao/SEEC.

</details>


### [56] [XSRD-Net: EXplainable Stroke Relapse Detection](https://arxiv.org/abs/2509.07772)
*Christian Gapp,Elias Tappeiner,Martin Welk,Karl Fritscher,Stephanie Mangesius,Constantin Eisenschink,Philipp Deisl,Michael Knoflach,Astrid E. Grams,Elke R. Gizewski,Rainer Schubert*

Main category: cs.CV

TL;DR: 研究通过多模态深度学习（CTA 3D影像+表格临床信息）早期预测卒中复发与无复发生存期（RFS）。二分类用表格数据AUC=0.84；生存回归的多模态模型c-index=0.68、AUC=0.71。解释性分析显示心脏疾病与颈动脉影像特征共同指示复发风险。


<details>
  <summary>Details</summary>
Motivation: 卒中复发率高且复发死亡率高（约40%），需要尽早识别高风险患者以制定个体化治疗和随访策略，降低复发与死亡。

Method: 收集2010–2024年卒中患者的3D颅内CTA、合并心脏病、年龄、性别等。训练单模态与多模态深度学习：任务1为复发二分类；任务2为RFS时间回归并进行后续分类。提出多模态XSRD-net，融合影像与表格（贡献度比约0.68:0.32），并进行可解释性分析以量化模态贡献和关键特征。

Result: 任务1：仅用表格数据即可达测试集AUC=0.84。任务2：多模态模型在测试集上c-index=0.68，AUC=0.71；模态贡献显示影像>表格（0.68 vs 0.32）。

Conclusion: 多模态深度学习可在卒中复发风险与RFS预测上取得中等性能；心脏疾病（表格）与颈动脉（影像）特征对复发预测至关重要。持续扩充数据与再训练有望提升模型表现并验证这些生物学关联。

Abstract: Stroke is the second most frequent cause of death world wide with an annual
mortality of around 5.5 million. Recurrence rates of stroke are between 5 and
25% in the first year. As mortality rates for relapses are extraordinarily high
(40%) it is of utmost importance to reduce the recurrence rates. We address
this issue by detecting patients at risk of stroke recurrence at an early stage
in order to enable appropriate therapy planning. To this end we collected 3D
intracranial CTA image data and recorded concomitant heart diseases, the age
and the gender of stroke patients between 2010 and 2024. We trained single- and
multimodal deep learning based neural networks for binary relapse detection
(Task 1) and for relapse free survival (RFS) time prediction together with a
subsequent classification (Task 2). The separation of relapse from non-relapse
patients (Task 1) could be solved with tabular data (AUC on test dataset:
0.84). However, for the main task, the regression (Task 2), our multimodal
XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to
modality contribution measures. The c-index with respect to relapses for the
multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final,
deeper interpretability analysis results could highlight a link between both
heart diseases (tabular) and carotid arteries (vision) for the detection of
relapses and the prediction of the RFS time. This is a central outcome that we
strive to strengthen with ongoing data collection and model retraining.

</details>


### [57] [HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.07774)
*Yimin Pan,Matthias Nießner,Tobias Kirschstein*

Main category: cs.CV

TL;DR: 提出基于3D高斯泼洒(3DGS)的多阶段管线，实现从多视图图像重建发丝级几何，并引入评估拓扑连通性的指标；可在约1小时内稳健重建多种发型。


<details>
  <summary>Details</summary>
Motivation: 现有头发重建多关注几何精度，忽略发丝连通性/拓扑，且需要高成本或效率低。3DGS与发丝片段天然契合，期望在保持效率的同时获得更真实的“可梳理”发丝结构。

Method: 扩展3DGS为可微高斯栅格化与分段-合并的发丝建模：1) 以可微高斯渲染重建细节几何；2) 通过新提出的合并策略将独立高斯段聚合为连贯发丝；3) 在光度监督下对发丝进行细化与生长。并提出一个面向拓扑连通性的代理评估指标。

Result: 在合成和真实数据上，方法对多样发型表现稳健、重建高效，通常一小时内完成；在几何与新拓扑代理指标上优于或竞争于现有方法。

Conclusion: 3DGS可被有效用于发丝级重建，通过分段合并与光度细化，实现高效且拓扑更正确的头发模型；新指标弥补了评估空白，为后续方法提供对连通性敏感的度量。

Abstract: Human hair reconstruction is a challenging problem in computer vision, with
growing importance for applications in virtual reality and digital human
modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient
and explicit scene representations that naturally align with the structure of
hair strands. In this work, we extend the 3DGS framework to enable strand-level
hair geometry reconstruction from multi-view images. Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.
  While existing methods typically evaluate reconstruction quality at the
geometric level, they often neglect the connectivity and topology of hair
strands. To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction. Extensive
experiments on both synthetic and real-world datasets demonstrate that our
method robustly handles a wide range of hairstyles and achieves efficient
reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/

</details>


### [58] [RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis](https://arxiv.org/abs/2509.07782)
*Hugo Blanc,Jean-Emmanuel Deschaud,Alexis Paljic*

Main category: cs.CV

TL;DR: RayGaussX在RayGauss基础上引入空洞跳跃、自适应采样、射线一致性增强与尺度正则化，并提出新密度增厚策略，使训练加速5–12倍、推理加速50–80倍，同时PSNR最高提升0.56 dB，能在真实场景实现实时/高帧率渲染。


<details>
  <summary>Details</summary>
Motivation: RayGauss虽在合成与室内场景达到SOTA质量，但真实世界渲染成本高、无法实时，且远距离区域密度分布与射线–基元交互存在低效与误检问题，需要系统性加速与质量稳健性改进。

Method: 在RayGauss的椭圆基函数+BVH体渲染框架上：1) 引入空域加速（空洞跳跃、基于内容的自适应采样）；2) 增强射线相干性以提高缓存/向量化效率；3) 尺度正则化以减少错误交点（假阳性）；4) 新的密度增厚（densification）准则，特别改善远距离区域的密度分布。

Result: 在真实世界数据集上，训练速度提升5–12倍，渲染帧率提升50–80倍；视觉质量保持或提升，PSNR最高+0.56 dB；并提供项目页面与代码。

Conclusion: 通过一系列体渲染加速与稳健性改进，RayGaussX在不牺牲甚至提升质量的前提下，将RayGauss推广到真实场景的高效训练与实时渲染。

Abstract: RayGauss has achieved state-of-the-art rendering quality for novel-view
synthesis on synthetic and indoor scenes by representing radiance and density
fields with irregularly distributed elliptical basis functions, rendered via
volume ray casting using a Bounding Volume Hierarchy (BVH). However, its
computational cost prevents real-time rendering on real-world scenes. Our
approach, RayGaussX, builds on RayGauss by introducing key contributions that
accelerate both training and inference. Specifically, we incorporate volumetric
rendering acceleration strategies such as empty-space skipping and adaptive
sampling, enhance ray coherence, and introduce scale regularization to reduce
false-positive intersections. Additionally, we propose a new densification
criterion that improves density distribution in distant regions, leading to
enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x
to 12x faster training and 50x to 80x higher rendering speeds (FPS) on
real-world datasets while improving visual quality by up to +0.56 dB in PSNR.
Project page with videos and code: https://raygaussx.github.io/.

</details>


### [59] [Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss](https://arxiv.org/abs/2509.07798)
*Maja Schlereth,Moritz Schillinger,Katharina Breininger*

Main category: cs.CV

TL;DR: 提出一种自监督多视角网络，将两组正交各向异性低分辨率MR图像融合，重建统一的高细节表示；通过稀疏坐标式损失支持任意缩放，并结合离线通用与在线患者特异阶段实现最高10倍提速，同时SR质量达至或优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 临床MR扫描需要在分辨率、时间与患者舒适度间权衡，常用两组不同方向的各向异性低分辨率扫描，但逐一阅片耗时且可能误判；缺乏相应高分辨率标注数据，限制了有监督SR方法的应用。

Method: 设计自监督多视角神经网络，输入两组正交各向异性LR图像，在无需配对HR的情况下进行训练；提出稀疏坐标驱动的损失函数，可对任意缩放比的LR进行对齐与融合；训练与推理由两阶段组成：患者无关的离线阶段学习通用先验，患者特异的在线阶段进行快速适配以重建统一高细节体素表示。

Result: 在两个独立队列MR数据上评估，不同上采样比例下达到与或优于SOTA自监督SR的定量与定性表现；通过离线+在线策略，在保持相当或更好SR质量的同时，实现患者特异重建最高约10倍的速度提升。

Conclusion: 多视角自监督融合两组正交各向异性LR MR可在无HR标注下实现高质量SR重建；稀疏坐标损失和两阶段训练兼顾灵活性与效率，具备临床潜力与良好泛化。

Abstract: Acquiring images in high resolution is often a challenging task. Especially
in the medical sector, image quality has to be balanced with acquisition time
and patient comfort. To strike a compromise between scan time and quality for
Magnetic Resonance (MR) imaging, two anisotropic scans with different
low-resolution (LR) orientations can be acquired. Typically, LR scans are
analyzed individually by radiologists, which is time consuming and can lead to
inaccurate interpretation. To tackle this, we propose a novel approach for
fusing two orthogonal anisotropic LR MR images to reconstruct anatomical
details in a unified representation. Our multi-view neural network is trained
in a self-supervised manner, without requiring corresponding high-resolution
(HR) data. To optimize the model, we introduce a sparse coordinate-based loss,
enabling the integration of LR images with arbitrary scaling. We evaluate our
method on MR images from two independent cohorts. Our results demonstrate
comparable or even improved super-resolution (SR) performance compared to
state-of-the-art (SOTA) self-supervised SR methods for different upsampling
scales. By combining a patient-agnostic offline and a patient-specific online
phase, we achieve a substantial speed-up of up to ten times for
patient-specific reconstruction while achieving similar or better SR quality.
Code is available at https://github.com/MajaSchle/tripleSR.

</details>


### [60] [SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting](https://arxiv.org/abs/2509.07809)
*Mahtab Dahaghin,Milind G. Padalkar,Matteo Toso,Alessio Del Bue*

Main category: cs.CV

TL;DR: SplatFill是一种面向3D Gaussian Splatting场景的深度引导填补方法，通过联合深度/对象监督与一致性自适应细化，实现更清晰、更一致的3D补全，并显著提速。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽能重建高保真场景，但遮挡或编辑导致的缺失区域填补常出现模糊、伪影与几何不一致；现有NeRF/3DGS补全在细节与效率上仍不足。

Method: (1) 深度与对象联合监督：利用深度约束和对象级语义/区域信息，指导新高斯在3D空间的准确放置并与邻域几何对齐；(2) 一致性感知细化：检测跨视角/几何不一致的局部区域，选择性地优化这些区域的高斯参数，避免破坏已正确区域，从而高效稳定地提升质量。

Result: 在SPIn-NeRF数据集上，SplatFill在感知质量上优于现有NeRF与3DGS补全方法，并将训练时间减少24.5%；定性上呈现更锐利细节、更少伪影与跨视角一致性更强。

Conclusion: 深度引导+对象级监督结合一致性自适应细化，可在3DGS场景补全中兼顾几何准确与感知质量，同时提升效率，适用于复杂视角与编辑场景的稳健3D修复。

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D
scene representations from sets of multi-view images. However, inpainting
missing regions, whether due to occlusion or scene editing, remains a
challenging task, often leading to blurry details, artifacts, and inconsistent
geometry. In this work, we introduce SplatFill, a novel depth-guided approach
for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and
improved efficiency. Our method combines two key ideas: (1) joint depth-based
and object-based supervision to ensure inpainted Gaussians are accurately
placed in 3D space and aligned with surrounding geometry, and (2) we propose a
consistency-aware refinement scheme that selectively identifies and corrects
inconsistent regions without disrupting the rest of the scene. Evaluations on
the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing
NeRF-based and 3DGS-based inpainting methods in visual fidelity but also
reduces training time by 24.5%. Qualitative results show our method delivers
sharper details, fewer artifacts, and greater coherence across challenging
viewpoints.

</details>


### [61] [Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model](https://arxiv.org/abs/2509.07825)
*Zhuoxu Huang,Mingqi Gao,Jungong Han*

Main category: cs.CV

TL;DR: 提出Point Linguist Model（PLM），通过对象中心判别表示（OcDR）与几何再激活解码器（GRD）弥合LLM语义与3D稠密点云几何的表示错配，无需大规模3D-文本/图像预对齐，显著提升3D指代分割与多任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的3D分割存在表示错配：LLM处理高层语义token，而点云仅提供稠密几何，导致输入端需重预对齐、语义弱化且易被干扰物混淆；输出端仅依赖稠密特征、缺乏显式几何线索，细粒度精度受损。需要一种在不依赖大规模3D-文本/图像预对齐的前提下，桥接语义token与点云几何的通用框架。

Method: 1) OcDR：学习对象中心token，捕获目标语义与场景关系；引入hard negative-aware训练目标，提升对干扰物的判别并缓解LLM token与3D点的错配，使LLM可进行语义级推理。2) GRD：在解码时将承载LLM推断几何的OcDR token与对应稠密特征融合，输出精确掩码，同时在管线中保留全面稠密几何信息。整体无需大规模3D-文本/图像预对齐。

Result: 在ScanNetv2与Multi3DRefer的3D指代分割上分别提升+7.3 mIoU与+6.0 mIoU；在4类任务覆盖的7个基准上均取得一致增益，验证了对象中心推理对稳健3D理解的有效性。

Conclusion: PLM通过对象中心表示与几何再激活解码，有效对齐LLM语义与点云几何，在无需大规模跨模态预对齐的情况下带来显著、广泛的一致性能提升，改善抗干扰与细粒度分割精度。

Abstract: 3D object segmentation with Large Language Models (LLMs) has become a
prevailing paradigm due to its broad semantics, task flexibility, and strong
generalization. However, this paradigm is hindered by representation
misalignment: LLMs process high-level semantic tokens, whereas 3D point clouds
convey only dense geometric structures. In prior methods, misalignment limits
both input and output. At the input stage, dense point patches require heavy
pre-alignment, weakening object-level semantics and confusing similar
distractors. At the output stage, predictions depend only on dense features
without explicit geometric cues, leading to a loss of fine-grained accuracy. To
address these limitations, we present the Point Linguist Model (PLM), a general
framework that bridges the representation gap between LLMs and dense 3D point
clouds without requiring large-scale pre-alignment between 3D-text or
3D-images. Specifically, we introduce Object-centric Discriminative
Representation (OcDR), which learns object-centric tokens that capture target
semantics and scene relations under a hard negative-aware training objective.
This mitigates the misalignment between LLM tokens and 3D points, enhances
resilience to distractors, and facilitates semantic-level reasoning within
LLMs. For accurate segmentation, we introduce the Geometric Reactivation
Decoder (GRD), which predicts masks by combining OcDR tokens carrying
LLM-inferred geometry with corresponding dense features, preserving
comprehensive dense features throughout the pipeline. Extensive experiments
show that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and
+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains
across 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness
of comprehensive object-centric reasoning for robust 3D understanding.

</details>


### [62] [Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets](https://arxiv.org/abs/2509.07852)
*Seyd Teymoor Seydi*

Main category: cs.CV

TL;DR: 提出用AlphaEArth数据集结合Siamese U-Net与集成策略，实现自动烧毁区制图，跨区域测试达OA 95%、IoU 0.60、F1 0.74，具备良好迁移与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有烧毁区制图在时效性与精度、跨地区泛化方面不足；需要借助高分辨率多模态遥感数据与更强的深度模型，实现快速、鲁棒的全球火烧迹地监测与评估，以支撑环境监测、灾害管理与气候影响评估。

Method: 构建基于AlphaEArth高分辨率光学+热红外影像与完备标注的数据训练框架，采用Siamese U-Net进行特征对齐与变化/烧毁判别，并使用集成策略提升稳健性。以美国MTBS数据训练，在欧洲17个区域进行跨域评估，衡量OA、IoU与F1等指标。

Result: 在测试集上取得总体精度95%、IoU 0.60、F1 0.74。模型能在复杂背景与多样生态系统中识别烧毁区，尤其对部分烧毁植被与火烧边界检测表现突出，显示良好的可迁移性与泛化能力。

Conclusion: 该方法提升了自动化火灾损害评估的准确性与可扩展性，表明利用AlphaEArth数据与Siamese U-Net的方案可用于全球尺度的烧毁区监测。

Abstract: Accurate and timely mapping of burned areas is crucial for environmental
monitoring, disaster management, and assessment of climate change. This study
presents a novel approach to automated burned area mapping using the AlphaEArth
dataset combined with the Siamese U-Net deep learning architecture. The
AlphaEArth Dataset, comprising high-resolution optical and thermal infrared
imagery with comprehensive ground-truth annotations, provides an unprecedented
resource for training robust burned area detection models. We trained our model
with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US
and evaluated it with 17 regions cross in Europe. Our experimental results
demonstrate that the proposed ensemble approach achieves superior performance
with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test
dataset. The model successfully identifies burned areas across diverse
ecosystems with complex background, showing particular strength in detecting
partially burned vegetation and fire boundaries and its transferability and
high generalization in burned area mapping. This research contributes to the
advancement of automated fire damage assessment and provides a scalable
solution for global burn area monitoring using the AlphaEarth dataset.

</details>


### [63] [D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics](https://arxiv.org/abs/2509.07864)
*Tiancheng Yang,Lin Zhang,Jiaye Lin,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.CV

TL;DR: 提出诊断与校正MLLM视觉幻觉的新方法：用LIAE定位层级异常、IAF甄别头部，并据此在推理时动态调整注意力（D-LEAF），在描述与VQA基准上显著抑制幻觉且几乎零开销。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在图像字幕、VQA上强，但常产生与图像不符的幻觉。已有注意力相关方法通常对各层各头一刀切地调节，无法定位误差来源，导致检测与缓解效果受限。

Method: 1) 指出现有基于注意力的检测/缓解不能准确定位问题层；2) 提出两种诊断指标：LIAE（层级图像注意力熵）用于标记异常层；IAF（图像注意力聚焦度）用于在这些层内对注意力头排名；3) 基于诊断信号提出D-LEAF：在推理时动态、分层地融合与重加权图像注意力，对高风险层与头进行有针对性的校正，几乎不引入额外开销、任务无关。

Result: 在标准图像字幕基准上相对提升53%；在VQA上准确率与F1均提升约4%；在抑制幻觉的同时保持效率（推理开销可忽略）。

Conclusion: 细粒度、层—头级的诊断与动态校正可有效缓解MLLM幻觉，优于统一式调整；D-LEAF在多任务上显著提升可靠性且高效，可作为通用推理时改进方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance on tasks
like image captioning and visual question answering, but remain prone to
hallucinations, where generated text conflicts with the visual input. Prior
work links this partly to insufficient visual attention, but existing
attention-based detectors and mitigation typically apply uniform adjustments
across layers and heads, obscuring where errors originate. In this paper, we
first show these methods fail to accurately localize problematic layers. Then,
we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags
anomalous layers, and Image Attention Focus (IAF) which scores attention heads
within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF
reliably ranks heads that warrant correction. Guided by these signals, we
propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a
task-agnostic, attention-guided method that dynamically localizes and corrects
errors during inference with negligible overhead. Results show our D-LEAF
delivers a 53% relative improvement on standard captioning benchmarks, and on
VQA both accuracy and F1-score improve by approximately 4%, substantially
suppressing hallucinations while preserving efficiency.

</details>


### [64] [Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning](https://arxiv.org/abs/2509.07879)
*Daniel DeAlcala,Aythami Morales,Julian Fierrez,Gonzalo Mancera,Ruben Tolosana,Javier Ortega-Garcia*

Main category: cs.CV

TL;DR: aMINT提出一种主动型成员推断测试，通过多任务联合训练“被审计模型”和用于识别训练样本的“MINT模型”，利用中间激活作为输入来学习可审计性，从而在多种架构与数据集上以>80%准确率判断某样本是否参与训练，优于既有方法。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断多为被动、后验分析，效果受限且难以在训练期直接优化可审计性。需要一种在模型训练中内生化审计能力的机制，以提升透明度，强化安全、隐私与版权合规。

Method: 提出Active MINT：将可审计性作为联合优化目标，端到端多任务训练两套网络——被审计模型负责主任务（如分类），MINT模型接收被审计模型的中间激活图作为输入，学习判别样本是否为训练集；通过在训练期让MINT层适配不同架构（MobileNet到ViT），提升成员识别信号。

Result: 在5个公共基准上评估，覆盖轻量CNN到Vision Transformer，多数设置下aMINT在成员识别任务上达到80%以上准确率，显著超过文献基线。

Conclusion: 将审计能力内嵌到训练过程可显著提升成员推断性能，增强模型透明度与合规性，为安全、隐私与版权保护提供实践路径。

Abstract: Active Membership Inference Test (aMINT) is a method designed to detect
whether given data were used during the training of machine learning models. In
Active MINT, we propose a novel multitask learning process that involves
training simultaneously two models: the original or Audited Model, and a
secondary model, referred to as the MINT Model, responsible for identifying the
data used for training the Audited Model. This novel multi-task learning
approach has been designed to incorporate the auditability of the model as an
optimization objective during the training process of neural networks. The
proposed approach incorporates intermediate activation maps as inputs to the
MINT layers, which are trained to enhance the detection of training data. We
present results using a wide range of neural networks, from lighter
architectures such as MobileNet to more complex ones such as Vision
Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT
achieves over 80% accuracy in detecting if given data was used for training,
significantly outperforming previous approaches in the literature. Our aMINT
and related methodological developments contribute to increasing transparency
in AI models, facilitating stronger safeguards in AI deployments to achieve
proper security, privacy, and copyright protection.

</details>


### [65] [Object-level Correlation for Few-Shot Segmentation](https://arxiv.org/abs/2509.07917)
*Chunlin Wen,Yu Zhang,Jie Fan,Hongyuan Zhu,Xiu-Shen Wei,Yijun Wang,Zhiqiang Kou,Shuzhou Sun*

Main category: cs.CV

TL;DR: 提出OCNet，用对象级相关性替代传统图像级相关性，利用GOMM挖掘通用对象、CCM进行目标原型匹配，从而抑制背景噪声，在PASCAL-5^i与COCO-20^i上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有FSS多在支持图像目标与整个查询图像之间建立图像级相关性，但包含难抑制的硬像素噪声（无关背景），导致背景过拟合。仿生视觉表明在对象层面识别新目标更有效，尤其在小样本下，因此需要对象级相关性建模以更好地区分目标与背景。

Method: 提出对象级相关网络OCNet，包括两模块：1) GOMM（通用对象挖掘）：通过显著性与高层相似性线索学习，构建查询的通用对象特征（包含无关背景对象与目标前景对象）；2) CCM（相关性构建）：将从支持集提取的目标原型分配/匹配到查询通用对象特征上，建立对象级相关性，进而挖掘查询中的目标特征并抑制硬像素噪声。

Result: 在PASCAL-5^i与COCO-20^i上进行大量实验，性能达到当前最优（SOTA），表明对象级相关性有效降低背景过拟合、提升分割精度。

Conclusion: 对象级而非图像级的相关性建模能更好地在少样本情形下识别目标并抑制背景噪声。OCNet通过GOMM与CCM实现该思路，实验验证其在两个基准数据集上的SOTA表现。

Abstract: Few-shot semantic segmentation (FSS) aims to segment objects of novel
categories in the query images given only a few annotated support samples.
Existing methods primarily build the image-level correlation between the
support target object and the entire query image. However, this correlation
contains the hard pixel noise, \textit{i.e.}, irrelevant background objects,
that is intractable to trace and suppress, leading to the overfitting of the
background. To address the limitation of this correlation, we imitate the
biological vision process to identify novel objects in the object-level
information. Target identification in the general objects is more valid than in
the entire image, especially in the low-data regime. Inspired by this, we
design an Object-level Correlation Network (OCNet) by establishing the
object-level correlation between the support target object and query general
objects, which is mainly composed of the General Object Mining Module (GOMM)
and Correlation Construction Module (CCM). Specifically, GOMM constructs the
query general object feature by learning saliency and high-level similarity
cues, where the general objects include the irrelevant background objects and
the target foreground object. Then, CCM establishes the object-level
correlation by allocating the target prototypes to match the general object
feature. The generated object-level correlation can mine the query target
feature and suppress the hard pixel noise for the final prediction. Extensive
experiments on PASCAL-${5}^{i}$ and COCO-${20}^{i}$ show that our model
achieves the state-of-the-art performance.

</details>


### [66] [ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion](https://arxiv.org/abs/2509.07920)
*Ao Li,Jinpeng Liu,Yixuan Zhu,Yansong Tang*

Main category: cs.CV

TL;DR: 提出ScoreHOI：利用扩散模型与得分引导采样的先验，联合重建人-物交互，显著提升物理可行性与精度，并通过接触驱动的迭代细化进一步优化接触与姿态。


<details>
  <summary>Details</summary>
Motivation: 以往基于优化的人-物交互联合重建缺乏交互先验，易出现不物理、接触不合理与姿态/物体姿势不稳定的问题；需要一种能把数据驱动先验与物理约束结合的方法以提升重建的逼真度与鲁棒性。

Method: 引入扩散先验作为优化器（ScoreHOI）：1）用得分引导采样重建给定图像观测与物体特征条件下的人体与物体姿态的条件分布；2）在推理中将物理约束（如接触、穿透、动力学一致性等）融入引导项，指导去噪过程朝物理可行解收敛；3）提出接触驱动的迭代细化策略，专注提升接触可行性与重建精度。

Result: 在标准基准上优于现有SOTA，能更精确、鲁棒地重建人-物交互，尤其在物理可行性与接触质量方面显著提升。

Conclusion: 扩散先验结合物理引导与接触细化能有效解决以往方法的物理不合理与不稳定问题，实现高精度、可物理解释的人-物交互联合重建。

Abstract: Joint reconstruction of human-object interaction marks a significant
milestone in comprehending the intricate interrelations between humans and
their surrounding environment. Nevertheless, previous optimization methods
often struggle to achieve physically plausible reconstruction results due to
the lack of prior knowledge about human-object interactions. In this paper, we
introduce ScoreHOI, an effective diffusion-based optimizer that introduces
diffusion priors for the precise recovery of human-object interactions. By
harnessing the controllability within score-guided sampling, the diffusion
model can reconstruct a conditional distribution of human and object pose given
the image observation and object feature. During inference, the ScoreHOI
effectively improves the reconstruction results by guiding the denoising
process with specific physical constraints. Furthermore, we propose a
contact-driven iterative refinement approach to enhance the contact
plausibility and improve the reconstruction accuracy. Extensive evaluations on
standard benchmarks demonstrate ScoreHOI's superior performance over
state-of-the-art methods, highlighting its ability to achieve a precise and
robust improvement in joint human-object interaction reconstruction.

</details>


### [67] [Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation](https://arxiv.org/abs/2509.07923)
*Moo Hyun Son,Juyoung Bae,Zelin Qiu,Jiale Peng,Kai Xin Li,Yifan Lin,Hao Chen*

Main category: cs.CV

TL;DR: 提出ToothMCL：一种利用CBCT体数据与IOS表面数据进行多模态对比学习预训练的牙齿分割框架，并发布3,867例配对数据集CBCT-IOS3.8K；在多数据集评测中显著提升分割性能（CBCT DSC提升12%，IOS提升8%），对FDI编号与多类分割表现出强泛化性。


<details>
  <summary>Details</summary>
Motivation: 数字牙科需要精确的数字化牙列表示，而现有牙齿分割方法验证不足、临床适用性有限、对不同模态泛化弱。作者希望通过统一地学习CBCT与IOS的模态不变表示，提升细粒度解剖结构建模能力，从而改进分割与FDI编号准确性，并满足多中心多条件的鲁棒应用需求。

Method: 提出ToothMCL多模态对比学习预训练框架：将体素级CBCT与表面级IOS作为成对样本，通过对比学习获取模态不变特征；在此基础上进行多类牙齿分割与FDI编号。并构建大规模配对数据集CBCT-IOS3.8K（3,867例）。在多套独立数据上进行内部与外部评测。

Result: 在最大规模与多样性的评测集合中取得SOTA：相较现有方法，CBCT分割DSC提升约12%，IOS分割DSC提升约8%；在不同牙齿分组上持续领先，并在不同成像条件与临床场景下保持稳健的泛化性能。

Conclusion: 多模态对比预训练能有效对齐CBCT与IOS的共享语义，捕获细粒度解剖特征，显著提升牙齿分割与FDI编号的准确性与泛化性；ToothMCL与CBCT-IOS3.8K为数字牙科提供了具备临床潜力的基础模型与数据资源。

Abstract: Digital dentistry represents a transformative shift in modern dental
practice. The foundational step in this transformation is the accurate digital
representation of the patient's dentition, which is obtained from segmented
Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the
growing interest in digital dental technologies, existing segmentation
methodologies frequently lack rigorous validation and demonstrate limited
performance and clinical applicability. To the best of our knowledge, this is
the first work to introduce a multimodal pretraining framework for tooth
segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for
pretraining that integrates volumetric (CBCT) and surface-based (IOS)
modalities. By capturing modality-invariant representations through multimodal
contrastive learning, our approach effectively models fine-grained anatomical
features, enabling precise multi-class segmentation and accurate identification
of F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with the
framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to
date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive
collection of independent datasets, representing the largest and most diverse
evaluation to date. Our method achieves state-of-the-art performance in both
internal and external testing, with an increase of 12\% for CBCT segmentation
and 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC).
Furthermore, ToothMCL consistently surpasses existing approaches in tooth
groups and demonstrates robust generalizability across varying imaging
conditions and clinical scenarios.

</details>


### [68] [Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s](https://arxiv.org/abs/2509.07928)
*Mahmudul Islam Masum,Miad Islam,Arif I. Sarwat*

Main category: cs.CV

TL;DR: 提出一种面向消费级硬件的“两阶段自适应推理”策略：先用低分辨率快速检测，仅在置信度低时再用高分辨率复核；在COCO 5k上较PyTorch早退基线实现1.85倍加速，mAP仅下降5.51%。


<details>
  <summary>Details</summary>
Motivation: 现实中的本地AI部署常受系统级瓶颈（而非算力）限制，尤其在笔记本等资源受限设备上，导致在台式机高端GPU上报告的实时指标难以复现。需要硬件感知的推理策略，而非仅靠模型结构优化。

Method: 提出与模型无关的Two-Pass Adaptive Inference：第一遍低分辨率、快速模型推理；当检测置信度低时触发第二遍高分辨率推理。并在统一框架下对比建筑式早退(early-exit)与分辨率自适应路由，分析权衡。通过“瓶颈测试”证明性能受系统级瓶颈主导。

Result: 在COCO 5k数据上，相比PyTorch Early-Exit基线实现1.85×速度提升，mAP仅下降5.51%。展示了在RTX 4060笔记本等硬件上可复现的实时加速。

Conclusion: 把优化重心从纯模型改造转向硬件感知的自适应推理，可在消费级设备上获得高吞吐、可复现的实时目标检测。Two-Pass方案在保持可用精度的同时显著提速，提供一套实用蓝图。

Abstract: As local AI grows in popularity, there is a critical gap between the
benchmark performance of object detectors and their practical viability on
consumer-grade hardware. While models like YOLOv10s promise real-time speeds,
these metrics are typically achieved on high-power, desktop-class GPUs. This
paper reveals that on resource-constrained systems, such as laptops with RTX
4060 GPUs, performance is not compute-bound but is instead dominated by
system-level bottlenecks, as illustrated by a simple bottleneck test. To
overcome this hardware-level constraint, we introduce a Two-Pass Adaptive
Inference algorithm, a model-independent approach that requires no
architectural changes. This study mainly focuses on adaptive inference
strategies and undertakes a comparative analysis of architectural early-exit
and resolution-adaptive routing, highlighting their respective trade-offs
within a unified evaluation framework. The system uses a fast, low-resolution
pass and only escalates to a high-resolution model pass when detection
confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x
speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.
This work provides a practical and reproducible blueprint for deploying
high-performance, real-time AI on consumer-grade devices by shifting the focus
from pure model optimization to hardware-aware inference strategies that
maximize throughput.

</details>


### [69] [Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space Object](https://arxiv.org/abs/2509.07932)
*Bala Prenith Reddy Gopu,Timothy Jacob Huber,George M. Nehma,Patrick Quinn,Madhur Tiwari,Matt Ueckermann,David Hinckley,Christopher McKenna*

Main category: cs.CV

TL;DR: 评估用于动态场景的最先进3D重建算法在“翻滚”非协作航天器上的适用性；基于Isaac Sim生成物理真实的影像序列，静态场景用Neuralangelo重建与CAD高度吻合，为后续动态评测奠定基线。


<details>
  <summary>Details</summary>
Motivation: OOS与ADR任务需要对非协作RSO的几何与运动进行高精度表征，但翻滚目标的3D重建难度大且缺乏系统评测与高保真数据支撑。

Method: 搭建Isaac Sim仿真环境，生成在真实轨道光照条件下的卫星翻滚2D图像序列；选取SOTA动态场景3D重建算法进行评测；先在静态场景以Neuralangelo进行基线重建，并用CloudCompare与原始CAD进行几何误差对比。

Result: 在静态场景中，Neuralangelo产生的3D网格与CAD高度一致，误差与伪影较小，能够还原关键细节，满足任务规划需求。

Conclusion: 静态评测显示Neuralangelo具备高保真重建能力，建立了可信基线；将据此继续推进对动态翻滚场景重建算法的系统评估。

Abstract: Characterization of uncooperative Resident Space Objects (RSO) play a crucial
role in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions to
assess the geometry and motion properties. To address the challenges of
reconstructing tumbling uncooperative targets, this study evaluates the
performance of existing state-of-the-art 3D reconstruction algorithms for
dynamic scenes, focusing on their ability to generate geometrically accurate
models with high-fidelity. To support our evaluation, we developed a simulation
environment using Isaac Sim to generate physics-accurate 2D image sequences of
tumbling satellite under realistic orbital lighting conditions. Our preliminary
results on static scenes using Neuralangelo demonstrate promising
reconstruction quality. The generated 3D meshes closely match the original CAD
models with minimal errors and artifacts when compared using Cloud Compare
(CC). The reconstructed models were able to capture critical fine details for
mission planning. This provides a baseline for our ongoing evaluation of
dynamic scene reconstruction.

</details>


### [70] [Feature Space Analysis by Guided Diffusion Model](https://arxiv.org/abs/2509.07936)
*Kimiaki Shirahama,Miki Yanobu,Kaduki Yamashita,Miho Ohsaki*

Main category: cs.CV

TL;DR: 提出一种带有特征匹配保证的解码器：在扩散模型反向生成过程中逐步最小化与目标特征的欧氏距离，从而合成与用户指定特征高度一致的图像，以解释DNN特征空间。无需额外训练、可跨模型分析、单张消费级GPU可运行。


<details>
  <summary>Details</summary>
Motivation: DNN特征提取过程不透明，过去的可视化/反演方法常缺乏“生成结果的特征确与指定特征接近”的严格保证，难以明确哪些图像属性被编码。作者希望以强保证的生成方法来揭示特征空间含义。

Method: 构建“引导扩散”解码器：在预训练扩散模型的逆过程每一步，估计当前干净图像的特征（针对目标编码器，如CLIP、ResNet-50、ViT），并将该特征与用户给定目标特征的欧氏距离纳入引导信号，迭代最小化；无需对目标DNN或扩散模型进行再训练。

Result: 在CLIP图像编码器、ResNet-50与ViT上，生成图像的特征与指定特征高度相似（定量上欧氏距离显著降低），并能直观揭示特征空间所编码的属性；方法稳定、通用，跨模型适用。

Conclusion: 该解码器为DNN特征空间提供了可控、可靠的可视化通道，提升了可解释性与诊断能力；具有零额外训练、硬件友好和模型通用等实际优势。

Abstract: One of the key issues in Deep Neural Networks (DNNs) is the black-box nature
of their internal feature extraction process. Targeting vision-related domains,
this paper focuses on analysing the feature space of a DNN by proposing a
decoder that can generate images whose features are guaranteed to closely match
a user-specified feature. Owing to this guarantee that is missed in past
studies, our decoder allows us to evidence which of various attributes in an
image are encoded into a feature by the DNN, by generating images whose
features are in proximity to that feature. Our decoder is implemented as a
guided diffusion model that guides the reverse image generation of a
pre-trained diffusion model to minimise the Euclidean distance between the
feature of a clean image estimated at each step and the user-specified feature.
One practical advantage of our decoder is that it can analyse feature spaces of
different DNNs with no additional training and run on a single COTS GPU. The
experimental results targeting CLIP's image encoder, ResNet-50 and vision
transformer demonstrate that images generated by our decoder have features
remarkably similar to the user-specified ones and reveal valuable insights into
these DNNs' feature spaces.

</details>


### [71] [Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images](https://arxiv.org/abs/2509.07966)
*Boammani Aser Lompo,Marc Haraoui*

Main category: cs.CV

TL;DR: 提出Visual-TableQA：一个大规模、开放域、面向渲染表格图像的多模态视觉表格推理数据集与自动化生成管线，成本低（< $100）、高多样性与高推理难度，并验证微调后能稳健泛化、超越部分专有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在表格图像上的推理评测数据稀缺，规模小、领域与布局单一、推理深度不足，限制了对复杂表格视觉推理能力的训练与评测。

Method: 提出一个模块化、可扩展、全自动的数据生成与过滤管线：多LLM分工协作（生成、验证、启发），利用跨模型提示（强模型提供布局与主题，弱模型细化与扩展），并通过“LLM陪审团”过滤；产出LaTeX渲染的复杂表格与对应推理型问答对。

Result: 构建了包含约2.5k复杂LaTeX渲染表格和6k高强度推理QA对的数据集；在该数据上微调的模型在外部基准上具备良好泛化，尽管数据为合成，仍优于若干专有模型；生成成本低于100美元。

Conclusion: Visual-TableQA与其自动化管线为视觉表格推理提供了高性价比、高多样性的数据来源；通过多模型协作与过滤实现复杂推理模式的蒸馏，推动VLM在表格图像推理上的训练与评测，并具备良好的跨基准泛化能力。

Abstract: Visual reasoning over structured data such as tables is a critical capability
for modern vision-language models (VLMs), yet current benchmarks remain limited
in scale, diversity, or reasoning depth, especially when it comes to rendered
table images. Addressing this gap, we introduce Visual-TableQA, a large-scale,
open-domain multimodal dataset specifically designed to evaluate and enhance
visual reasoning over complex tabular data. Our generation pipeline is modular,
scalable, and fully autonomous, involving multiple reasoning LLMs collaborating
across distinct roles: generation, validation, and inspiration. Visual-TableQA
comprises 2.5k richly structured LaTeX-rendered tables and 6k
reasoning-intensive QA pairs, all produced at a cost of under USD 100. To
promote diversity and creativity, our pipeline performs multi-model
collaborative data generation via cross-model prompting ('inspiration') and
LLM-jury filtering. Stronger models seed layouts and topics that weaker models
elaborate, collectively distilling diverse reasoning patterns and visual
structures into the dataset. Empirical results show that models fine-tuned on
Visual-TableQA generalize robustly to external benchmarks, outperforming
several proprietary models despite the dataset's synthetic nature. The full
pipeline and resources are publicly available at
https://github.com/AI-4-Everyone/Visual-TableQA.

</details>


### [72] [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search](https://arxiv.org/abs/2509.07969)
*Xin Lai,Junyi Li,Wei Li,Tao Liu,Tianjian Li,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 提出Mini-o3：通过可扩展的多回合工具交互与强化学习，实现深层（数十步）视觉搜索推理并达SOTA。关键：新的视觉探测数据集、迭代式多样推理轨迹收集、RL中的over-turn遮蔽以支持推理步数在推理时自然扩展。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态工具+RL方法推理模式单一、交互轮数受限，难以处理需反复试探的困难视觉任务；亟需一种既能高效训练又能在测试时进行深度探索的系统。

Method: 1) 构建Visual Probe Dataset：包含大量需探索性推理的困难视觉搜索题。2) 迭代数据收集：生成冷启动轨迹，覆盖DFS、试错、目标保持等多样推理范式。3) 提出over-turn masking：对达到最大轮数而未完成的响应在RL训练中不予惩罚，兼顾训练效率与测试可扩展性。尽管训练上限为6轮，推理时可自然扩展到数十轮。

Result: Mini-o3在挑战性视觉搜索任务上取得SOTA；生成更丰富的推理模式与更深的思维路径；随着回合数增加，准确率持续提升，展示出良好的可扩展性。

Conclusion: 通过数据集、轨迹多样化与RL遮蔽策略的结合，可在有限训练轮数下获得可在推理时自适应延长多轮深度推理能力，显著提升复杂视觉搜索任务表现。

Abstract: Recent advances in large multimodal models have leveraged image-based tools
with reinforcement learning to tackle visual problems. However, existing
open-source approaches often exhibit monotonous reasoning patterns and allow
only a limited number of interaction turns, making them inadequate for
difficult tasks that require trial-and-error exploration. In this work, we
address this limitation by scaling up tool-based interactions and introduce
Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of
steps -- and achieves state-of-the-art performance on challenging visual search
tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key
components. First, we construct the Visual Probe Dataset, a collection of
thousands of challenging visual search problems designed for exploratory
reasoning. Second, we develop an iterative data collection pipeline to obtain
cold-start trajectories that exhibit diverse reasoning patterns, including
depth-first search, trial-and-error, and goal maintenance. Third, we propose an
over-turn masking strategy that prevents penalization of over-turn responses
(those that hit the maximum number of turns) during reinforcement learning,
thereby balancing training-time efficiency with test-time scalability. Despite
training with an upper bound of only six interaction turns, our model generates
trajectories that naturally scale to tens of turns at inference time, with
accuracy improving as the number of turns increases. Extensive experiments
demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking
paths, effectively solving challenging visual search problems.

</details>


### [73] [One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation](https://arxiv.org/abs/2509.07978)
*Zheng Geng,Nan Wang,Shaocong Xu,Chongjie Ye,Bohan Li,Zhaoxi Chen,Sida Peng,Hao Zhao*

Main category: cs.CV

TL;DR: 提出OnePoseViaGen：基于单幅参考图的任意未见物体6D位姿估计，通过联合尺度与位姿的粗到细对齐，以及文本引导的生成式域随机化来增强合成数据，从而实现高保真单视图3D生成辅助的一次性位姿估计；在多基准上达SOTA并验证真实机器人抓取。


<details>
  <summary>Details</summary>
Motivation: 现实机器人需要在长尾物体上进行操作，而多数物体无CAD模型、单视图重建无绝对尺度、生成模型到真实图像存在域差，导致一次性(oneshot)6D位姿估计难以可靠。需要一种不依赖现成3D模型、能解决尺度与域间隙的问题的通用方法。

Method: 1) 粗到细的对齐模块：结合多视图特征匹配与“渲染-比较”的优化，联合细化物体的尺度与位姿，实现从单视图生成的3D到真实图像的精确对齐。2) 文本引导的生成式域随机化：利用文本条件生成多样纹理，合成大量风格多变的数据以微调位姿估计器，缩小合成-真实域差。两者协同：用高保真单视图3D生成作为基础，依对齐与合成增强提升鲁棒的一次性6D位姿估计。

Result: 在YCBInEOAT、Toyota-Light、LM-O等具有挑战的基准上显著超越以往方法（达SOTA）。并在真实机器人手上完成稳健的灵巧抓取实验，显示方法在实际操作中的有效性。

Conclusion: 通过联合尺度-位姿对齐与文本引导的域随机化，OnePoseViaGen将高保真单视图3D生成与一次性6D位姿估计有机结合，显著提高在无CAD、域差大的真实场景中的鲁棒性与精度，并具备实际机器人操作可用性。

Abstract: Estimating the 6D pose of arbitrary unseen objects from a single reference
image is critical for robotics operating in the long-tail of real-world
instances. However, this setting is notoriously challenging: 3D models are
rarely available, single-view reconstructions lack metric scale, and domain
gaps between generated models and real-world images undermine robustness. We
propose OnePoseViaGen, a pipeline that tackles these challenges through two key
components. First, a coarse-to-fine alignment module jointly refines scale and
pose by combining multi-view feature matching with render-and-compare
refinement. Second, a text-guided generative domain randomization strategy
diversifies textures, enabling effective fine-tuning of pose estimators with
synthetic data. Together, these steps allow high-fidelity single-view 3D
generation to support reliable one-shot 6D pose estimation. On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches. We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation. Project page:
https://gzwsama.github.io/OnePoseviaGen.github.io/

</details>


### [74] [Visual Representation Alignment for Multimodal Large Language Models](https://arxiv.org/abs/2509.07979)
*Heeji Yoon,Jaewoo Jung,Junwan Kim,Hyungyu Choi,Heeseong Shin,Sangbeom Lim,Honggyu An,Chaehyun Kim,Jisang Han,Donghyun Kim,Chanho Eom,Sunghwan Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出VIRAL：通过将MLLM内部视觉表征与预训练视觉基础模型对齐，缓解仅文本监督导致的细粒度视觉信息丢失，显著提升计数、空间推理等视觉中心任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多依赖文本指令微调，视觉通路仅获间接监督，易忽略细节，导致在计数、空间关系等需要精细视觉的任务上表现欠佳。需要一种方法在训练中显式保留并强化视觉信息。

Method: 在视觉指令微调过程中加入正则化项，将MLLM中间视觉表示与预训练视觉基础模型（VFM）的表示进行显式对齐（表示级别的监督）。这种表示对齐既保留来自输入视觉编码器的关键细节，又引入VFM的额外视觉知识，从而改善复杂视觉推理。包含消融实验以验证关键设计。

Result: 在主流多模态基准的所有任务上稳定提升，尤其在物体计数、空间推理等视觉中心任务上效果显著；消融显示对齐设计为提升的关键因素。

Conclusion: 简单有效的表示对齐正则可显著增强MLLM对视觉信息的利用与推理能力，为在训练中更好整合视觉知识提供了重要方向。

Abstract: Multimodal large language models (MLLMs) trained with visual instruction
tuning have achieved strong performance across diverse tasks, yet they remain
limited in vision-centric tasks such as object counting or spatial reasoning.
We attribute this gap to the prevailing text-only supervision paradigm, which
provides only indirect guidance for the visual pathway and often leads MLLMs to
discard fine-grained visual details during training. In this paper, we present
VIsual Representation ALignment (VIRAL), a simple yet effective regularization
strategy that aligns the internal visual representations of MLLMs with those of
pre-trained vision foundation models (VFMs). By explicitly enforcing this
alignment, VIRAL enables the model not only to retain critical visual details
from the input vision encoder but also to complement additional visual
knowledge from VFMs, thereby enhancing its ability to reason over complex
visual inputs. Our experiments demonstrate consistent improvements across all
tasks on widely adopted multimodal benchmarks. Furthermore, we conduct
comprehensive ablation studies to validate the key design choices underlying
our framework. We believe this simple finding opens up an important direction
for the effective integration of visual information in training MLLMs.

</details>
