<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 117]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?](https://arxiv.org/abs/2511.20710)
*David Amebley,Sayanton Dibbo*

Main category: cs.CV

TL;DR: 论文研究多模态视觉-语言模型在黑盒成员推断攻击下的隐私泄露，并提出神经科学启发的拓扑正则化(tau)来提升抗攻击性；在不明显牺牲生成质量的前提下，BLIP等模型的MIA成功率显著下降，结论在多模型多数据集上得到验证。


<details>
  <summary>Details</summary>
Motivation: 多模态模型广泛部署带来新的隐私攻击面，尤其是成员推断攻击可泄露训练数据；现有防御与研究主要集中在单模态，且生物启发表示被证明能提升对抗鲁棒性，但其对隐私攻击的作用尚未被验证。

Method: 提出系统性的神经科学启发拓扑正则化框架tau，将拓扑结构约束融入VLM训练/微调，使表示空间更稳健；对比baseline与加入tau(>0)的NEURO变体，在BLIP、PaliGemma 2、ViT-GPT2上，使用COCO、CC3M、NoCaps数据集；评价隐私鲁棒性用MIA的ROC-AUC，效用用MPNet相似度与ROUGE-2等指标。

Result: 在BLIP+COCO上，加入tau的NEURO模型使MIA的平均ROC-AUC降低24%，同时保持与baseline相近的生成-参考标题相似度（MPNet、ROUGE-2变化不显著）。在PaliGemma 2与ViT-GPT2上、CC3M与NoCaps数据集上得到一致趋势，表明方法通用有效。

Conclusion: 神经科学启发的拓扑正则化能提升多模态VLM对成员推断隐私攻击的抵抗力，而不显著损伤任务效用；为MM隐私风险研究提供了实证证据与可行防御思路。

Abstract: In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.

</details>


### [2] [Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation](https://arxiv.org/abs/2511.20714)
*Inferix Team,Tianyu Feng,Yizeng Han,Jiahao He,Yuanyu He,Xi Lin,Teng Liu,Hanfeng Lu,Jiasheng Tang,Wei Wang,Zhiyuan Wang,Jichao Wu,Mingyang Yang,Yinghao Yu,Zeyu Zhang,Bohan Zhuang*

Main category: cs.CV

TL;DR: Inferix提出面向“世界模型”视频生成的下一代推理引擎，围绕半自回归（块式扩散）解码优化，支持长时长、交互式、可流式的视频生成与评测集成（LV-Bench），旨在推动高逼真、可交互的世界仿真。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散或高并发LLM推理系统难以同时满足：长时长、物理一致性、交互性、可变长度与高质量的视频生成。半自回归解码带来KV Cache式管理等优势，但缺乏专门的推理引擎与评测工具。

Method: 以半自回归（块式扩散）为核心：按块生成视频token，每块内部用扩散，跨块自回归并缓存上下文（KV Cache），提升连贯性与效率。Inferix在系统层面优化该解码过程，提供交互式视频流式生成与性能画像；并无缝集成新基准LV-Bench用于分钟级视频细粒度评测与基准测试。

Result: 实现更稳定连贯、可变长度和高质量的长视频生成；支持实时交互与流式播放；提供系统级分析与基准测试能力，便于对世界模型进行性能与质量评估。

Conclusion: 面向世界模型应用场景，Inferix将半自回归块式扩散与工程化推理解码、流式交互和细粒度评测集成在一起，为超越LLM中心范式的视觉基础模型与世界仿真提供基础设施，并期待社区共建与扩展。

Abstract: World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.
  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.

</details>


### [3] [Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?](https://arxiv.org/abs/2511.20716)
*Kun Guo,Yun Shen,Xijun Wang,Chaoqun You,Yun Rui,Tony Q. S. Quek*

Main category: cs.CV

TL;DR: 论文提出一种在资源受限设备上结合本地跟踪与边缘检测的自适应策略，用深度强化学习在帧率、精度与时延约束下动态选择操作，并在多设备场景引入联邦学习以提升泛化与协同效果；在树莓派与边缘服务器实测中取得优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 视频目标识别在低功耗设备上难以同时满足高精度与低时延。仅本地检测精度受限，完全依赖边缘又受网络条件波动影响。现有“检测+跟踪”混合方案缺乏在动态网络与多样帧率/需求下的长期最优决策机制，需要能权衡精度、时延与算力的自适应策略。

Method: 将“何时边缘检测、何时本地跟踪”的决策建模为长期优化问题，分别针对单设备与多设备情形。提出基于深度强化学习的LTED-Ada策略网络，以帧率、历史相关性、精度与时延需求等为状态，动作为选择边缘检测或本地跟踪，奖励综合精度-时延权衡。多设备场景引入联邦学习聚合各设备策略以提升在不同帧率与需求上的泛化。

Result: 在硬件闭环实验中（多台树莓派4B+PC边缘服务器），LTED-Ada相较基线在识别准确率、端到端时延与总体吞吐上更优，且对不同帧率与网络条件具有更强鲁棒性与泛化能力。

Conclusion: 通过DRL驱动的自适应检测/跟踪切换，并结合联邦学习的跨设备协同，能够在动态网络与资源受限场景中实现更优的长期精度-时延权衡，适用于实际边缘视频分析部署。

Abstract: Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.

</details>


### [4] [DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving](https://arxiv.org/abs/2511.20720)
*Haibo HU,Lianming Huang,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: DeeAD提出一种无需训练、基于动作的早退框架，通过物理可行性校验与自适应层跳跃，加速自动驾驶VLA模型推理，在不牺牲规划质量与安全的前提下减少延迟。


<details>
  <summary>Details</summary>
Motivation: VLA模型统一感知-推理-轨迹生成，但深层Transformer导致推理时延高；传统早退依赖置信度不可靠，亟需能保证规划可行性且无需重训的加速方法。

Method: 在推理过程中对中间轨迹进行轻量级物理可行性与先验（导航/低精规划）一致性校验，当轨迹与先验在<2m偏差内即触发早退；并引入基于分数变化率的multi-hop控制器，自适应跳过冗余层。方法可无缝集成至现有VLA（如ORION）且免训练。

Result: 在Bench2Drive上实现最高28% Transformer层稀疏化、29%时延降低，同时保持规划质量与安全性。

Conclusion: 基于可行性的一致性早退与自适应层跳跃能有效加速VLA规划且不降质，可作为现有模型的免训练插件以提升实时性。

Abstract: Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.

</details>


### [5] [Foundry: Distilling 3D Foundation Models for the Edge](https://arxiv.org/abs/2511.20721)
*Guillaume Letellier,Siddharth Srivastava,Frédéric Jurie,Gaurav Sharma*

Main category: cs.CV

TL;DR: 提出Foundation Model Distillation（FMD），通过学习压缩的SuperTokens来重构教师模型的token级表示，将大型自监督基础模型压缩为小而高效、仍具通用迁移能力的代理；在3D点云上实现为Foundry，接近教师性能但显著减少tokens与FLOPs。


<details>
  <summary>Details</summary>
Motivation: 基础模型在下游通用性强，但体积与算力需求高，难以上边缘设备；传统蒸馏多产出面向单一任务的“专才”，丧失基础模型的下游无关通用性。需要一种既压缩又保留通用表示力的方法。

Method: 提出FMD范式：训练学生模型学习一组压缩的SuperTokens，能重构教师的token级表征，从而捕获教师潜空间的紧致基；具体实现为Foundry（面向3D点云），以重构损失对齐学生与教师表示，减少token数量与计算开销，同时不局限于某一任务。

Result: 单一蒸馏学生在多种下游任务（分类、部件分割、少样本）上保持强迁移能力，性能接近完整基础模型；同时显著降低tokens数量与FLOPs，使资源受限硬件部署更可行。

Conclusion: FMD可将大型SSL基础模型压缩为紧凑且忠实的代理，保留通用表示与迁移性；Foundry展示了在3D点云领域的有效性，为在边缘设备部署基础模型提供了实用路径。

Abstract: Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.

</details>


### [6] [DinoLizer: Learning from the Best for Generative Inpainting Localization](https://arxiv.org/abs/2511.20722)
*Minh Thong Doi,Jan Butora,Vincent Itier,Jérémie Boulanger,Patrick Bas*

Main category: cs.CV

TL;DR: DinoLizer基于DINOv2与线性头，在ViT补丁层面定位生成式修补的篡改区域；通过滑窗与后处理生成精细掩膜，跨多数据集显著超越SOTA，鲁棒于常见后处理，平均IoU提升约12%。


<details>
  <summary>Details</summary>
Motivation: 现有本地篡改/修补区域定位在不同生成模型与后处理下鲁棒性不足；需要一种能在更大图像上稳定定位语义性篡改并具备跨数据集泛化的方案。

Method: 以在B-Free上预训练为“合成图像检测”的DINOv2为骨干，在其patch嵌入上加线性分类头，输出14×14分辨率的篡改概率；训练时仅将语义改变视为篡改；由于ViT定长输入，采用滑窗对大图汇聚预测，生成热力图并经后处理（阈值/形态等）得到二值掩膜；进行大量消融，含DINOv2与DINOv3对比。

Result: 在多种来源的修补数据集上超越现有本地操纵检测方法；对缩放、加噪、JPEG及双重压缩等后处理保持鲁棒；平均IoU较次优方法高约12%，后处理后优势更大。

Conclusion: ViT（DINO系）对本地篡改定位有强表示力；DinoLizer在生成式修补定位上达SOTA且鲁棒性强，优于DINOv3替代；代码将开源。

Abstract: We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.

</details>


### [7] [CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design](https://arxiv.org/abs/2511.20737)
*Daeheon Jeong,Seoyeon Byun,Kihoon Son,Dae Hyun Kim,Juho Kim*

Main category: cs.CV

TL;DR: 提出CANVAS基准，用工具调用评估VLM在UI设计中的迭代编辑能力，含复制与修改两类任务，598个任务源自3.3K移动UI；发现领先模型更有策略地调用工具但仍有常见错误。


<details>
  <summary>Details</summary>
Motivation: 现有VLM具备调用设计工具（如Figma/Sketch）的潜力，可在常规软件中与设计师协作，但缺乏针对“基于工具的UI设计”能力的评测，导致其真实能力与改进方向未知。

Method: 构建CANVAS基准：从30个功能类目（如引导、消息）中采样3.3K移动UI，标注并生成598个与设计软件联动的工具调用任务；设定两类任务——(i) 设计复刻：复现整屏UI；(ii) 设计修改：对现有屏幕的特定部分进行更改。每个任务要求VLM通过上下文驱动的序列化工具调用（如创建矩形作为按钮背景）逐步更新设计，并以地面真值参考评估。

Result: 实验显示，领先模型在操作策略上更合理，能通过更有效的工具调用提升设计质量；同时归纳出模型常见错误模式。

Conclusion: CANVAS为评测与推动VLM在基于工具的UI设计中的迭代编辑能力提供了标准基准与诊断视角，可指导未来改进策略与系统设计。

Abstract: User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.

</details>


### [8] [Text-Guided Semantic Image Encoder](https://arxiv.org/abs/2511.20770)
*Raghuveer Thirukovalluru,Xiaochuang Han,Bhuwan Dhingra,Emily Dinan,Maha Elbayad*

Main category: cs.CV

TL;DR: 提出一种文本引导的语义图像编码器（TIE），让图像特征直接受输入文本查询条件化，从而在多项视觉问答与图文任务上提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 传统VLM先独立预训练图像编码器，再与语言模型对齐，导致编码器对下游任务与具体文本查询不敏感，生成与查询无关的通用视觉表征，限制了性能与可解释性与效率。

Method: 设计TIE，使图像编码在前向时直接以文本查询为条件进行特征提取与注意力聚焦；在多任务图像到文本基准上训练/评估，并比较不同模型规模（1B/3B）与不同图像切片（token/tiles）数量下的表现；进行定性可视化以检验对查询相关区域的关注。

Result: 在九个image-to-text基准上，TIE版VLM平均提升：1B提升+1.5分、3B提升+1.3分，DocVQA与InfoVQA上最高可达+6分；在仅使用一半图像tiles的情况下仍优于常规模型，推理效率显著提高；对通用查询也具备良好泛化。

Conclusion: 让图像编码器受文本条件化可更好对齐任务需求，既提升性能又减少视觉token，且带来更强的可解释性与查询定位能力，显示这种端到端文本引导编码策略是提升VLM的有效途径。

Abstract: Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.

</details>


### [9] [One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues](https://arxiv.org/abs/2511.20784)
*Sindhuja Penchala,Gavin Money,Gabriel Marques,Samuel Wood,Jessica Kirschman,Travis Atkison,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 提出SMARC：从单幅图像中仅10%连续可见区域，重建整张RGB表面并分类材质，基于部分卷积U-Net与分类头，在Touch and Go数据集上取得SOTA（PSNR 17.55 dB，分类准确率85.10%）。


<details>
  <summary>Details</summary>
Motivation: 现实场景中常存在视野受限或观测稀疏，传统方法依赖密集或全景观测，难以胜任。需要一种能在极少视觉信息下同时完成表面重建与材质识别的方法，以服务机器人、仿真与材料感知。

Method: 构建统一模型SMARC：采用Partial Convolutional U-Net执行空间补全与重建，并并联分类头进行材质类别判别。输入仅为图像中单个连续10%补丁及其掩码；通过部分卷积处理缺失区域、U-Net解码重建RGB表面，同时进行语义分类。与CAE、ViT、MAE、Swin、DETR在Touch and Go数据集上对比评测。

Result: 在真实表面纹理数据集Touch and Go上，SMARC在极端稀疏观测条件下实现SOTA：重建PSNR 17.55 dB，材质分类准确率85.10%，优于对比的五种基线模型。

Conclusion: 部分卷积能在缺失数据下增强空间推理，SMARC在最小视觉输入下同时实现高质量重建与分类，奠定了稀疏视觉下表面理解的有效范式。

Abstract: Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.

</details>


### [10] [LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling](https://arxiv.org/abs/2511.20785)
*Zuhao Yang,Sudong Wang,Kaichen Zhang,Keming Wu,Sicong Leng,Yifan Zhang,Chengwei Qin,Shijian Lu,Xingxuan Li,Lidong Bing*

Main category: cs.CV

TL;DR: LongVT 提出一个面向长视频理解的“全局先扫、局部深挖”的代理式框架，通过多模态工具链思维在视频中反复定位相关片段、细粒度重采样并推理，从而降低幻觉并提升问答表现；并发布配套数据集与三阶段训练流程，全面领先现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有大多模态模型在长视频推理中易产生幻觉：证据稀疏、时间分散，纯文本CoT难以在长时间轴上可靠对齐视觉证据。人类观看长视频常先全局浏览、再聚焦细节，启发以“工具化的循序定位”增强证据落地与时序对齐。

Method: 提出 LongVT：将LMM的时间定位能力当作“原生视频裁剪工具”，在全局浏览后迭代地：1) 选取候选时间段；2) 对该片段更细粒度抽帧；3) 交替进行多模态CoT与工具调用，直至答案被可视证据支撑。配套数据套件 VideoSIAH：247.9K 冷启动工具一体SFT样本、1.6K 代理式RL样本、15.4K 代理式RFT样本；评测端含1280条经半自动+人工校验的QA。训练采用三阶段（SFT → RL → RFT），系统化优化代理式决策与证据对齐。

Result: 在四个具有挑战性的长视频理解与推理基准上，LongVT 稳定且显著优于强基线。提供公开代码、数据与模型权重以复现。

Conclusion: 通过将“时间定位/裁剪”内化为可调用工具，并在全局到局部的循环中进行多模态思维链推理，LongVT 有效缓解长视频幻觉、提升证据对齐与问答效果；VideoSIAH 与三阶段训练为此类代理式长视频推理提供了可复用范式。

Abstract: Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .

</details>


### [11] [Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2511.20795)
*Souradeep Dutta,Keshav Bulia,Neena S Nair*

Main category: cs.CV

TL;DR: 作者对KRISP视觉-语言推理模型进行轻量化复现，参数大幅减少，性能约达原作的75%，并通过消融与小规模数据实验揭示原方法的若干设计与实现问题；模型在受限知识图谱域内推理，减少幻觉并可在边缘设备上运行。


<details>
  <summary>Details</summary>
Motivation: 原版KRISP依赖工业级算力与大体量骨干网络，不利于资源受限场景和边缘部署；作者希望验证在低参数与受限知识域条件下，知识增强型VQA是否仍具有效性，并系统暴露原论文中未充分讨论的设计缺陷与实践陷阱。

Method: 重构KRISP为小参数版本：集成外部结构化知识（知识图谱）到VQA管线；在受限知识域内进行推理；开展系统性消融实验，包括合成VQA与DAQUAR数据集评测；分析可扩展性、鲁棒性和资源-性能权衡，并观察模型在边缘设备可部署性。

Result: 轻量化模型以显著更少参数达到原模型约75%的性能；在领域受限的知识图谱约束下，有效抑制模型幻觉；在DAQUAR和合成数据上验证有效；显著降低计算与存储成本，具备在智能手机与AR/VR等边缘设备离线运行的可行性。

Conclusion: 知识增强型VQA在资源受限设置下仍具竞争力；通过领域受限的外部知识约束可减少幻觉并提升可控性；尽管性能低于原模型，但带来部署友好与稳定性的现实优势，并暴露出原KRISP设计上的若干隐性问题，为后续在低资源条件下的VQA研究与工程提供指导。

Abstract: Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.

</details>


### [12] [Intriguing Properties of Dynamic Sampling Networks](https://arxiv.org/abs/2511.20800)
*Dario Morle,Reid Zaffino*

Main category: cs.CV

TL;DR: 提出并统一“动态采样”类视觉模块的通用算子“warping”，给出其统计与优化性质，并据此给出稳定训练条件与可视化方法。


<details>
  <summary>Details</summary>
Motivation: 动态可变采样（如可变形卷积、ACU、STN）广泛有效，但缺乏统一理论框架与可分析的最小实现；需要理解其统计性质、前后向差异、与卷积不变性关系，以及训练稳定性与离散化影响。

Method: 提出可泛化现有动态采样方法的“warping”算子作为最小实现；在输入分别为独立同分布与齐次随机场的假设下进行统计分析；对比前向与反向传播性质并揭示非对称性；证明其与卷积这类平移不变算子形成不同的正交算子类；分析离散化误差；提出基于梯度更新信息的损失景观可视化方法；并辅以实证验证与稳定训练条件总结。

Result: 显示warping能重构多种动态采样架构；给出在IID与齐次随机场下的方差/偏差等统计特性；发现训练时前后向传播存在固有不对称；证明其算子族与传统卷积族正交；总结确保稳定训练的条件（如采样偏移/插值尺度、正则与初始化范围等）；量化离散化带来的统计影响；新可视化方法直观揭示学习动力学。

Conclusion: Warping为动态采样网络提供统一且可分析的框架，揭示其与卷积的本质差异与训练非对称性，并给出稳定训练准则与离散化影响评估；所提基于梯度的可视化有助于理解与诊断此类模型的学习行为。

Abstract: Dynamic sampling mechanisms in deep learning architectures have demonstrated utility across many computer vision models, though the theoretical analysis of these structures has not yet been unified. In this paper we connect the various dynamic sampling methods by developing and analyzing a novel operator which generalizes existing methods, which we term "warping". Warping provides a minimal implementation of dynamic sampling which is amenable to analysis, and can be used to reconstruct existing architectures including deformable convolutions, active convolutional units, and spatial transformer networks. Using our formalism, we provide statistical analysis of the operator by modeling the inputs as both IID variables and homogeneous random fields. Extending this analysis, we discover a unique asymmetry between the forward and backward pass of the model training. We demonstrate that these mechanisms represent an entirely different class of orthogonal operators to the traditional translationally invariant operators defined by convolutions. With a combination of theoretical analysis and empirical investigation, we find the conditions necessary to ensure stable training of dynamic sampling networks. In addition, statistical analysis of discretization effects are studied. Finally, we introduce a novel loss landscape visualization which utilizes gradient update information directly, to better understand learning behavior.

</details>


### [13] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: 提出Δ-NeRF：一种为增量场景设计的模块化残差NeRF，可在不访问历史数据的情况下对冻结基模型进行增量精炼；通过不确定性门控与高效视角选择，训练更快、数据更少且避免遗忘，并可蒸馏为紧凑学生模型。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF在增量到来新视角时需完全重训，难以适应卫星地形等时序采集场景；朴素增量微调会灾难性遗忘，且训练成本高、数据冗余大。

Method: 1) 冻结基NeRF，新增“残差控制器”在各层注入校正；2) 不确定性感知门控，自适应融合基模型与残差输出，避免过校正；3) 视角选择策略，在保证性能下最多减少约47%训练视图；4) 知识蒸馏，将增强后的模型压缩为原始大小20%的学生网络。

Result: 在卫星影像实验中，较联合训练达到可比性能且训练时间降低30–42%；较朴素微调PSNR最高提升43.5%，在部分指标上超越联合训练。

Conclusion: Δ-NeRF实现对NeRF的高效增量精炼：无需历史数据、缓解遗忘、降低数据与计算开销，并可压缩部署，适合连续观测的遥感/卫星场景。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [14] [Layer-Aware Video Composition via Split-then-Merge](https://arxiv.org/abs/2511.20809)
*Ozgur Kara,Yujia Chen,Ming-Hsuan Yang,James M. Rehg,Wen-Sheng Chu,Du Tran*

Main category: cs.CV

TL;DR: 提出Split-then-Merge（StM）框架：先将无标注视频拆分为前景/背景层，再自组合学习主体与场景的交互，用变换感知训练、多层融合与增强及身份保持损失，实现更真实的生成式视频合成，并在定量与人评/大模型评上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 生成式视频合成既需要对主体与场景的可组合性、可控性与交互动态进行建模，又面临标注数据稀缺与手工规则脆弱的问题。作者希望在不依赖人工标注的前提下，从大规模无标注视频中学习“主体-场景-交互”的可组合规律，提升可控性与真实感。

Method: 1) 数据层：将无标注视频自动切分为动态前景与静/动背景两层，并进行自我组合（不同视频的前景与背景跨源融合）。2) 训练管线：引入“变换感知”训练，通过多层融合与数据增强（包括几何/光照/运动变换）实现可供性（affordance）感知的合成学习。3) 损失设计：身份保持损失，约束合成时前景主体的外观与身份一致性，同时学习前景与背景的物理/语义交互。

Result: 在定量基准与人类评测/多模态大模型（VLLM）评测中均优于现有SOTA方法，生成视频在真实感、合成质量与可控性方面更佳。

Conclusion: StM通过“先拆分后合并”的自监督式视频组合学习，在无标注条件下掌握主体-场景的复杂交互，提升视频生成的可控性与真实性，并以新颖的变换感知训练与身份保持损失实现更稳定的合成效果。

Abstract: We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io

</details>


### [15] [SPHINX: A Synthetic Environment for Visual Perception and Reasoning](https://arxiv.org/abs/2511.20814)
*Md Tanvirul Alam,Saksham Aggarwal,Justin Yang Chae,Nidhi Rastogi*

Main category: cs.CV

TL;DR: Sphinx 是一个用于视觉感知与推理的合成评测与数据生成环境，覆盖25类任务并带可验证答案；现有LVLM（含最强GPT-5）远未达成人类水平，RLVR能显著提升效果并泛化到外部基准。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在系统化的视觉推理与认知基元上缺乏可验证、可扩展、细粒度的评测与数据来源。需要一个能大规模生成、覆盖多种推理类型且带严格可验证真值的环境与基准，以诊断模型短板并推动方法进步。

Method: 构建名为 Sphinx 的程序化生成框架：通过图案、瓷砖、图表、图标和几何原语组合生成谜题，并为每个实例提供可验证的标准答案；设计25类任务，涵盖对称检测、几何变换、空间推理、图表理解与序列预测；对主流LVLM进行评测；提出基于可验证奖励的强化学习（RLVR），利用Sphinx的真值进行训练与优化，并评估其在外部基准的迁移效果。

Result: 在Sphinx基准上，最先进的GPT-5准确率仅51.1%，显著低于人类；采用RLVR后，模型在Sphinx上的准确率大幅提升，并在其他视觉推理基准上也取得增益。

Conclusion: Sphinx提供了高可控、可扩展、带真值的多模态推理评测与数据生成平台，揭示现有LVLM在核心认知任务上的明显不足；基于可验证奖励的强化学习是提升多模态推理能力并实现对外部任务泛化的有效途径。

Abstract: We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.

</details>


### [16] [Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion](https://arxiv.org/abs/2511.20821)
*Samuele Dell'Erba,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 提出一种无需训练的数据/训练自由的优化式视觉反演(OVI)替代扩散先验：用随机伪token初始化视觉潜表示，通过最大化与文本嵌入的余弦相似度迭代优化，并引入马氏距离与近邻损失以逼近真实图像分布；在Kandinsky 2.2上，OVI可替代传统先验，且揭示T2I-CompBench++等评测对“直接用文本嵌入作先验”存在偏好；加约束后视觉质量显著提升，近邻法最有效，量化成绩可比或优于数据高效先验。


<details>
  <summary>Details</summary>
Motivation: 当前文生图扩散模型常依赖昂贵、需大数据训练的扩散先验，将文本嵌入映射到视觉流形；作者质疑先验是否必要，并希望用无需训练的优化方法替代，同时检验与改进评测基准的可靠性。

Method: Optimization-based Visual Inversion (OVI)：从随机伪token初始化潜视觉表示，优化其与文本嵌入的余弦相似度；为避免偏离真实图像分布，引入两种正则：1) 基于马氏距离的约束，使表示靠近训练分布协方差结构；2) 最近邻损失，使表示靠近最近的真实样本/库中的视觉嵌入。以Kandinsky 2.2为后端进行生成。

Result: 无训练的OVI可作为扩散先验的替代；在T2I-CompBench++等基准上，即便直接用文本嵌入作先验也能得到很高分，但感知质量差；加入马氏/近邻约束后，视觉保真度提升，其中近邻法效果最佳，定量指标达到或超过最新的数据高效先验。

Conclusion: 训练先验并非必需，优化式、无数据的OVI能替代之；现有评测对“语义一致性”高而忽视感知质量，存在缺陷；所提约束尤其近邻损失能提高质量，方法值得进一步研究与开放代码验证。

Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.

</details>


### [17] [RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs](https://arxiv.org/abs/2511.20823)
*Roman Naeem,David Hagerman,Jennifer Alvén,Fredrik Kahl*

Main category: cs.CV

TL;DR: RefTr提出一种面向3D医学影像的“图生成”中心线方法：用Transformer解码器的Producer-Refiner架构生成并递归细化汇流轨迹，保证树形拓扑、提升召回并减少参数与推理时间。


<details>
  <summary>Details</summary>
Motivation: 临床任务（诊断、规划、导航）需要完整、正确拓扑的血管/气道中心线。漏检细小分支会造成严重后果，因此需要在保证拓扑正确的前提下尽可能提高召回，同时兼顾精度与效率。

Method: 提出RefTr：基于Transformer解码器的Producer-Refiner架构。Producer先提出一组初始“汇流轨迹”（可同时表示分叉并显式约束树拓扑）；Refiner以递归方式对这些轨迹进行多步细化，并参数共享以降低模型规模。配套提出针对空间树图的高效NMS以合并重复分支、提升精度。

Result: 在多个人体血管中心线公共数据集上，RefTr相较SOTA具有更高召回、可比精度，同时推理更快、解码器参数减少约2.4倍。

Conclusion: RefTr通过汇流轨迹表示与递归细化，实现高召回、可靠树拓扑与高效率，显示为3D血管树中心线分析的有力新SOTA候选方案。

Abstract: Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.

</details>


### [18] [MODEST: Multi-Optics Depth-of-Field Stereo Dataset](https://arxiv.org/abs/2511.20853)
*Nisarg K. Trivedi,Vinayak A. Belludi,Li-Yun Wang,Pardis Taghavi,Dante Lok*

Main category: cs.CV

TL;DR: 提出一个首个高分辨率（5472×3648）真实双目DSLR数据集（18000张），覆盖广范围焦距与光圈，并随数据、标定与评测代码发布，用于推动在真实光学条件下的深度估计与景深相关任务的研究与评测。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计与景深研究受限于缺乏大规模、高保真、真实双目单反数据集，导致在真实相机光学条件下泛化与评估不足；合成数据与现实之间存在显著“光学真实感鸿沟”。

Method: 构建并采集一个系统化的双目DSLR数据集：9个复杂真实场景；两套相同相机同步双目；10个焦距（28–70mm）×5个光圈（f/2.8–f/22）共50种光学配置；每场景约2000张、总计18000张，高分辨率5472×3648。每个焦距配置提供专门的内外参标定集与校准文件。场景包含反射、透明、镜面、多尺度细节、光照变化等挑战元素。

Result: 数据集展示了当前SOTA单目/双目深度与景深方法在真实光学复杂性下的挑战与不足；提供可复现实验的评测代码与标定，支持广泛任务（深度估计、浅景深渲染、去模糊、三维重建、视角合成）上的受控分析。

Conclusion: 该数据集弥合了合成训练与真实相机光学之间的差距，成为评测与提升模型真实环境泛化能力的重要基准与资源，并促进对光学与几何效应的系统研究。

Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.

</details>


### [19] [Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries](https://arxiv.org/abs/2511.20854)
*Sree Bhattacharyya,Yaman Kumar Singla,Sudhir Yarram,Somesh Kumar Singh,Harini S,James Z. Wang*

Main category: cs.CV

TL;DR: 提出首个基于网络ToT（“舌尖现象”）查询构建的、含8.2万视频与开放式回忆描述的大规模无监督数据集，用于建模视觉可记忆性；在回忆生成与ToT检索上提供强信号，并使微调的多模态大模型在开放式记忆描述生成上超过GPT-4o，且通过对比学习实现首个多模态ToT检索模型。


<details>
  <summary>Details</summary>
Motivation: 人工收集可记忆性标注昂贵且通常仅给出汇总分数，缺乏自然、开放式回忆中的细粒度记忆线索，限制了数据多样性与可扩展性，阻碍对视觉可记忆性的建模与应用。

Method: 从Reddit等平台抓取用户的ToT检索帖，构建视频与对应开放式回忆描述的配对，无需人工监督；在该数据上微调大规模视觉-语言模型用于回忆描述生成；并采用对比学习策略训练多模态检索模型，实现以文本ToT线索检索相应视频。

Result: 微调后的大模型在开放式记忆描述生成上优于SOTA（如GPT-4o）；基于对比学习的模型实现并首示多模态ToT检索任务的有效性。

Conclusion: 基于ToT的无监督大规模数据集与相应模型为视觉可记忆性研究提供新方向，既能提升开放式回忆生成，也开启多模态ToT检索能力，促进更丰富与可扩展的记忆性建模。

Abstract: Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.

</details>


### [20] [Estimating Fog Parameters from a Sequence of Stereo Images](https://arxiv.org/abs/2511.20865)
*Yining Ding,João F. C. Mota,Andrew M. Wallace,Sen Wang*

Main category: cs.CV

TL;DR: 提出一种针对连续双目雾天图像的联合参数估计方法，通过一次性优化同时估计雾模型参数并动态更新，避免顺序估计的误差传播；并发布含标定与晴天对照的真实雾天双目数据集SDIRF与代码。


<details>
  <summary>Details</summary>
Motivation: 现有雾模型参数多采用逐步估计，易受误差累积影响，且假设全局均匀雾与真实场景不符；SLAM/里程计在雾天感知受限，亟需鲁棒的在线雾参数估计以提升视觉感知与定位。

Method: - 以连续双目雾天图像为输入，基于大气散射模型构建新的联合优化问题，同时估计全部雾参数（如大气光、散射/消光系数、透射等）并随时间动态更新。
- 采用“局部均匀、全局可变”的雾假设，分区建模以适应真实不均匀雾。
- 设计为可插拔模块，可无缝集成至现有视觉SLAM/里程计管线。
- 构建SDIRF数据集：高质量连续双目真实雾天道路场景（>40分钟，3.4万帧），含实验室标定的相机光度参数，以及相同路线的阴天清晰对照序列；并提供合成雾数据以评测。

Result: 在合成雾数据上取得最准确的参数估计；在SDIRF真实雾序列上较现有方法适应性更好、稳健性更高，验证联合优化和局部均匀假设的有效性。

Conclusion: 联合、动态的雾参数估计能显著降低误差传播并更契合真实不均匀雾，可作为SLAM/里程计的增强模块；公开的代码与SDIRF数据集为雾天视觉感知研究提供了标准基准与资源。

Abstract: We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically. In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem. By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous. The proposed algorithm can be easily used as an add-on module in existing visual Simultaneous Localisation and Mapping (SLAM) or odometry systems in the presence of fog. In order to assess our method, we also created a new dataset, the Stereo Driving In Real Fog (SDIRF), consisting of high-quality, consecutive stereo frames of real, foggy road scenes under a variety of visibility conditions, totalling over 40 minutes and 34k frames. As a first-of-its-kind, SDIRF contains the camera's photometric parameters calibrated in a lab environment, which is a prerequisite for correctly applying the atmospheric scattering model to foggy images. The dataset also includes the counterpart clear data of the same routes recorded in overcast weather, which is useful for companion work in image defogging and depth reconstruction. We conducted extensive experiments using both synthetic foggy data and real foggy sequences from SDIRF to demonstrate the superiority of the proposed algorithm over prior methods. Our method not only produces the most accurate estimates on synthetic data, but also adapts better to real fog. We make our code and SDIRF publicly available\footnote{https://github.com/SenseRoboticsLab/estimating-fog-parameters} to the community with the aim of advancing the research on visual perception in fog.

</details>


### [21] [V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence](https://arxiv.org/abs/2511.20886)
*Jiancheng Pan,Runze Wang,Tianwen Qian,Mohammad Mahdi,Yanwei Fu,Xiangyang Xue,Xiaomeng Huang,Luc Van Gool,Danda Pani Paudel,Yuqian Fu*

Main category: cs.CV

TL;DR: V^2-SAM将单视角分割模型SAM2扩展为跨视角目标对应：用几何锚点与视觉提示两类生成器提供跨视信息，并通过多专家与循环一致性选择器自适应选最可靠结果，在多数据集取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有分割/跟踪模型在自/他视（ego/exo）巨大视角与外观差异下难以直接建立同一物体的跨视对应；尤其SAM2依赖单视提示，缺乏可泛化的跨视几何与外观对齐机制。

Method: 提出统一框架V^2-SAM：1) V^2-Anchor基于DINOv3特征构建几何感知的跨视对应，并首次实现跨视情形下对SAM2的坐标式提示（anchor prompts）；2) V^2-Visual通过新颖的视觉提示匹配器，从特征与结构两层面对齐ego/exo表征，提供外观引导的视觉提示；3) 多专家架构融合两类提示的不同强项；4) 提出后验循环一致性选择器（PCCS）按循环一致性自适应选择最可靠专家输出。

Result: 在Ego-Exo4D的自-他视目标对应、DAVIS-2017视频目标跟踪、以及HANDAL-X跨视机器人数据集上均达成新的SOTA，验证框架有效性与泛化性。

Conclusion: 通过将几何锚点与外观视觉提示结合，并配合多专家与循环一致性选择，V^2-SAM成功把SAM2从单视分割扩展到稳健的跨视目标对应，在多任务/数据集上表现优异。

Abstract: Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).

</details>


### [22] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 提出 Null-TTA：在扩散模型推理时，通过优化无条件文本嵌入来进行测试时对齐，避免奖励黑客并取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试时对齐方法要么优化不足，要么过度优化导致奖励黑客，往往通过操控噪声/潜变量，容易偏离语义。需要一种既能有效提升目标奖励、又保持语义一致与泛化能力的方法。

Method: 在无分类器引导（CFG）框架下，不更新模型参数，直接优化无条件文本嵌入（null-text embedding），利用文本嵌入空间的结构化语义，使对齐在语义流形上进行；因无条件嵌入是生成分布的锚点，优化它可直接偏转生成分布以最大化目标奖励，同时抑制对非语义噪声的利用。

Result: 在多个目标奖励上实现最先进的测试时对齐性能，并在跨奖励迁移/泛化上保持强劲表现，避免奖励黑客现象。

Conclusion: 语义空间优化（优化无条件嵌入）是一种有效且有原则的TTA新范式：无需更新模型权重即可稳健对齐，兼顾性能与泛化并减少奖励黑客。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


### [23] [GaINeR: Geometry-Aware Implicit Network Representation](https://arxiv.org/abs/2511.20924)
*Weronika Jakubowska,Mikołaj Zieliński,Rafał Tobiasz,Krzysztof Byrski,Maciej Zięba,Dominik Belter,Przemysław Spurek*

Main category: cs.CV

TL;DR: GaINeR 将可训练高斯分布与隐式神经表示结合，通过局部高斯检索与加权嵌入，实现连续图像表示、几何可解释性与可编辑性。


<details>
  <summary>Details</summary>
Motivation: 传统 INR（如 SIREN/WIRE/FINER）能高保真重建与超分，但缺乏显式几何结构，难以局部编辑，也难与物理/几何过程耦合，不利于动态或交互式应用。

Method: 在图像平面上学习一组带参数的高斯（位置、尺度、方向、权重/嵌入）。给定坐标，检索最近 K 个高斯，按距离加权聚合其嵌入，再输入小型神经网络预测 RGB。整体端到端训练，既保留连续隐式表示，又引入可解释的几何先验与可控局部支撑。

Result: 在高保真连续重建的同时，提供显式局部结构，支持对局部区域的灵活编辑与交互；相较传统 INR，在可编辑性与物理/几何集成方面具优势（摘要未给具体数值）。

Conclusion: GaINeR 为 2D 图像提供几何感知的隐式表示，兼顾重建质量与局部可编辑性，为物理感知与交互式图像处理奠定基础；代码已开源。

Abstract: Implicit Neural Representations (INRs) have become an essential tool for modeling continuous 2D images, enabling high-fidelity reconstruction, super-resolution, and compression. Popular architectures such as SIREN, WIRE, and FINER demonstrate the potential of INR for capturing fine-grained image details. However, traditional INRs often lack explicit geometric structure and have limited capabilities for local editing or integration with physical simulation, restricting their applicability in dynamic or interactive settings. To address these limitations, we propose GaINeR: Geometry-Aware Implicit Network Representation, a novel framework for 2D images that combines trainable Gaussian distributions with a neural network-based INR. For a given image coordinate, the model retrieves the K nearest Gaussians, aggregates distance-weighted embeddings, and predicts the RGB value via a neural network. This design enables continuous image representation, interpretable geometric structure, and flexible local editing, providing a foundation for physically aware and interactive image manipulation. The official implementation of our method is publicly available at https://github.com/WJakubowska/GaINeR.

</details>


### [24] [A deep learning model to reduce agent dose for contrast-enhanced MRI of the cerebellopontine angle cistern](https://arxiv.org/abs/2511.20926)
*Yunjie Chen,Rianne A. Weber,Olaf M. Neve,Stephan R. Romeijn,Erik F. Hensen,Jelmer M. Wolterink,Qian Tao,Marius Staring,Berit M. Verbist*

Main category: cs.CV

TL;DR: 该研究利用深度学习从低剂量对比剂MRI重建接近标准剂量质量的T1ce影像，在10%–30%对比剂剂量下仍可实现可靠诊断与分割。


<details>
  <summary>Details</summary>
Motivation: 对比增强MRI在CPA池及前庭神经鞘瘤评估中至关重要，但对比剂存在成本与潜在风险；若能在显著降低剂量的同时保持图像质量与可诊断性，将有临床价值。

Method: 回顾性多中心数据：VS患者的T1与标准剂量T1ce用于模拟不同低剂量T1ce；训练DL模型将低剂量输入恢复为标准剂量外观。通过客观图像质量指标（SSIM、PSNR）、肿瘤分割性能（Dice、95%HD、ASD）及放射科医师主观评分评估DL重建效果。

Result: 纳入72例VS患者203次MRI。输入剂量越高，重建后SSIM由0.639±0.113升至0.993±0.009，PSNR由21.6±3.73 dB升至41.4±4.84 dB。以10%剂量作为输入，DL重建用于分割时Dice由0.673增至0.734，95%HD由2.38 mm降至2.07 mm，ASD由1.00 mm降至0.59 mm。10%与30%剂量重建图像均获优秀主观评分，30%信息量更高。

Conclusion: 深度学习可显著提升低剂量对比增强T1的图像质量，使在仅10%–30%标准剂量下仍具备病灶检出与诊断表征的可行性。

Abstract: Objectives: To evaluate a deep learning (DL) model for reducing the agent dose of contrast-enhanced T1-weighted MRI (T1ce) of the cerebellopontine angle (CPA) cistern. Materials and methods: In this multi-center retrospective study, T1 and T1ce of vestibular schwannoma (VS) patients were used to simulate low-dose T1ce with varying reductions of contrast agent dose. DL models were trained to restore standard-dose T1ce from the low-dose simulation. The image quality and segmentation performance of the DL-restored T1ce were evaluated. A head and neck radiologist was asked to rate DL-restored images in multiple aspects, including image quality and diagnostic characterization. Results: 203 MRI studies from 72 VS patients (mean age, 58.51 \pm 14.73, 39 men) were evaluated. As the input dose increased, the structural similarity index measure of the restored T1ce increased from 0.639 \pm 0.113 to 0.993 \pm 0.009, and the peak signal-to-noise ratio increased from 21.6 \pm 3.73 dB to 41.4 \pm 4.84 dB. At 10% input dose, using DL-restored T1ce for segmentation improved the Dice from 0.673 to 0.734, the 95% Hausdorff distance from 2.38 mm to 2.07 mm, and the average surface distance from 1.00 mm to 0.59 mm. Both DL-restored T1ce from 10% and 30% input doses showed excellent images, with the latter being considered more informative. Conclusion: The DL model improved the image quality of low-dose MRI of the CPA cistern, which makes lesion detection and diagnostic characterization possible with 10% - 30% of the standard dose.

</details>


### [25] [Smooth regularization for efficient video recognition](https://arxiv.org/abs/2511.20928)
*Gil Goldman,Raja Giryes,Mahadev Satyanarayanan*

Main category: cs.CV

TL;DR: 提出一种针对视频识别的平滑正则化方法，把相邻帧的中间层表示建模为高斯随机游走，从而抑制表征的突变，显著提升轻量模型在Kinetics-600上的准确率（+3.8%~6.4%），并在给定FLOPs/内存约束下刷新多项SOTA（MoViNets、MobileNetV3、MoViNets-Stream）。


<details>
  <summary>Details</summary>
Motivation: 视频具有天然的时间连续性，而轻量化模型难以稳定捕捉复杂时序动态，易出现表示在相邻帧间剧烈波动，导致识别鲁棒性与精度下降。需要一种无需显著增加计算开销、能向模型注入强时间先验的正则化，以提升小模型的时序建模能力与泛化性能。

Method: 在训练中对相邻帧的中间层嵌入施加平滑约束：将嵌入随时间的变化建模为高斯随机游走（Gaussian Random Walk, GRW），通过正则项惩罚高“加速度”（表示变化的二阶差分/急剧跳变），鼓励低加速度、时间连贯的表示。该正则化可插拔、对多种轻量视频模型（如MoViNets、MobileNetV3、MoViNets-Stream）适用，开销低。

Result: 在Kinetics-600上，轻量模型使用该平滑正则后准确率提升3.8%~6.4%。MoViNets家族在各自FLOPs约束下超越先前SOTA 3.8%~6.1%；MobileNetV3与MoViNets-Stream在相近内存占用下超过先前SOTA 4.9%~6.4%。

Conclusion: 将相邻帧中间表示约束为GRW的平滑正则能有效注入强时间归纳偏置，显著增强轻量视频识别模型的时序建模与泛化，在不显著增加资源的前提下刷新SOTA；方法通用、实现简单，具备较强实用性。

Abstract: We propose a smooth regularization technique that instills a strong temporal inductive bias in video recognition models, particularly benefiting lightweight architectures. Our method encourages smoothness in the intermediate-layer embeddings of consecutive frames by modeling their changes as a Gaussian Random Walk (GRW). This penalizes abrupt representational shifts, thereby promoting low-acceleration solutions that better align with the natural temporal coherence inherent in videos. By leveraging this enforced smoothness, lightweight models can more effectively capture complex temporal dynamics. Applied to such models, our technique yields a 3.8% to 6.4% accuracy improvement on Kinetics-600. Notably, the MoViNets model family trained with our smooth regularization improves the current state of the art by 3.8% to 6.1% within their respective FLOP constraints, while MobileNetV3 and the MoViNets-Stream family achieve gains of 4.9% to 6.4% over prior state-of-the-art models with comparable memory footprints. Our code and models are available at https://github.com/gilgoldm/grw-smoothing.

</details>


### [26] [Open Vocabulary Compositional Explanations for Neuron Alignment](https://arxiv.org/abs/2511.20931)
*Biagio La Rosa,Leilani H. Gilpin*

Main category: cs.CV

TL;DR: 提出一种用于视觉领域的开放词汇组合解释框架，利用开放词汇语义分割生成掩码，从而在任意数据集与概念上探测神经元并生成可组合的解释，兼顾定量指标与人类可解释性并展现更高灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于组合逻辑的神经元解释依赖人工标注的数据集，仅覆盖特定领域与预定义概念，限制了可扩展性与通用性。作者希望在不依赖人工标注的前提下，让用户针对任意概念与数据集检索和解释神经元。

Method: 构建三步框架：1) 用户以开放词汇指定任意概念；2) 使用开放词汇语义分割模型为这些概念生成像素级掩码；3) 基于掩码与神经元激活的空间对齐，计算开放词汇的组合解释（通过逻辑关系组合不同概念的掩码与激活）。并与既有方法在定量和人评上进行对比。

Result: 与依赖人工标注的方法相比，框架在定量指标与人类可解释性上具有竞争力或更优；分析了采用模型标注（而非人工标注）时解释差异的来源；展示了对任务与属性的灵活适配能力（可跨数据集与概念进行探测与解释）。

Conclusion: 开放词汇语义分割使组合式神经元解释从受限的人工标注场景推广到任意概念与数据集，显著提升了可扩展性与实用性，同时保持良好解释质量，并提供更灵活的分析能力。

Abstract: Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.

</details>


### [27] [UruDendro4: A Benchmark Dataset for Automatic Tree-Ring Detection in Cross-Section Images of Pinus taeda L](https://arxiv.org/abs/2511.20935)
*Henry Marichal,Joaquin Blanco,Diego Passarella,Gregory Randall*

Main category: cs.CV

TL;DR: 提出UruDendro4树轮图像数据集（102张Pinus taeda横截面，含人工标注年轮），并给出自动年轮检测基线；最佳方法DeepCS-TRD在mAP 0.838、mAR 0.782、ARE 0.084；加入该数据集训练可提升树轮检测泛化。


<details>
  <summary>Details</summary>
Motivation: 树轮年增长量衡量林业经营与生长模式，但手工沿少量半径测量既耗时又不精确。公开数据稀缺、且多为单一高度样本，限制了自动分割与体积生长建模的发展。

Method: 构建UruDendro4：收集并人工标注102个南方松（Pinus taeda）横截面图像，覆盖树干多个高度，支持体积年增长建模；利用多种SOTA算法进行自动年轮检测评测与消融，确定参数设置，并检验将该数据集纳入训练对泛化的影响。

Result: DeepCS-TRD在该数据集上取得最佳：mAP 0.838、mAR 0.782、Adapted Rand Error 0.084。消融实验证实最终参数选择的有效性。将UruDendro4加入训练可提升树轮检测任务的泛化性能。

Conclusion: UruDendro4填补多高度树轮横切图像与标注的缺口，使基于横截面图像的年木材体积估计成为可能；并提供可靠基线与证据，表明该数据集能提升自动树轮检测模型的精度与泛化。

Abstract: Tree-ring growth represents the annual wood increment for a tree, and quantifying it allows researchers to assess which silvicultural practices are best suited for each species. Manual measurement of this growth is time-consuming and often imprecise, as it is typically performed along 4 to 8 radial directions on a cross-sectional disc. In recent years, automated algorithms and datasets have emerged to enhance accuracy and automate the delineation of annual rings in cross-sectional images.
  To address the scarcity of wood cross-section data, we introduce the UruDendro4 dataset, a collection of 102 image samples of Pinus taeda L., each manually annotated with annual growth rings. Unlike existing public datasets, UruDendro4 includes samples extracted at multiple heights along the stem, allowing for the volumetric modeling of annual growth using manually delineated rings. This dataset (images and annotations) allows the development of volumetric models for annual wood estimation based on cross-sectional imagery.
  Additionally, we provide a performance baseline for automatic ring detection on this dataset using state-of-the-art methods. The highest performance was achieved by the DeepCS-TRD method, with a mean Average Precision of 0.838, a mean Average Recall of 0.782, and an Adapted Rand Error score of 0.084. A series of ablation experiments were conducted to empirically validate the final parameter configuration. Furthermore, we empirically demonstrate that training a learning model including this dataset improves the model's generalization in the tree-ring detection task.

</details>


### [28] [BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model](https://arxiv.org/abs/2511.20956)
*Rawa Mohammed,Mina Attin,Bryar Shareef*

Main category: cs.CV

TL;DR: 提出BUSTR，一种无需成对图像-报告监督的多任务视觉-语言框架，用于乳腺超声报告生成；通过结构化描述符与放射组学特征训练描述符感知的视觉表示，并以词元级CE与表示对齐损失联合优化，在两数据集上提升NLG与临床效能指标（特别是BI-RADS与病理）。


<details>
  <summary>Details</summary>
Motivation: RRG在乳腺超声场景受限于缺乏成对图像-报告数据以及大语言模型幻觉风险；需要一种既能减少监督依赖、又能提高临床关键要素正确性的方案。

Method: 1) 用结构化描述符（BI-RADS、病理、组织学等）与放射组学特征构造文本报告目标；2) 多头Swin编码器学习“描述符感知”的视觉表示，基于多任务损失在不同数据集各自的描述符集合上训练；3) 通过双层对齐目标将视觉与文本词元对齐：词元级交叉熵损失+输入/输出表示的余弦相似度对齐损失；4) 在无需成对图像-报告的设定下实现报告生成。

Result: 在BrEaST与BUS-BRA两公开BUS数据集上，BUSTR在通用NLG指标与临床效能指标上均取得一致提升，尤其在BI-RADS分级与病理等关键目标上表现更佳。

Conclusion: 面向BUS的描述符感知视觉模型，结合词元级与表示对齐损失，无需成对监督即可改进报告质量与临床关键要素的准确性；方法具有跨数据集的稳健性并降低LLM幻觉风险。

Abstract: Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR

</details>


### [29] [Beyond Realism: Learning the Art of Expressive Composition with StickerNet](https://arxiv.org/abs/2511.20957)
*Haoming Lu,David Kocharian,Humphrey Shi*

Main category: cs.CV

TL;DR: 提出“表达性合成”任务与StickerNet框架，从真实线上编辑行为中学习贴纸/元素放置，优先表达与用户意图而非写实；在用户研究与量化评测中优于基线并接近人类放置。


<details>
  <summary>Details</summary>
Motivation: 现实中许多图像合成并不追求写实，而是追求艺术性、趣味性和社交互动。传统研究主要关注视觉逼真与语义合理，和现代创作平台上的实际编辑动机存在偏差，缺乏对“表达性”“松弛布局逻辑”的系统建模。

Method: 提出“表达性合成”任务；设计两阶段模型StickerNet：先判定合成类型，再预测放置参数（不透明度、遮罩、位置、尺度等）。与以往利用合成数据不同，构建基于匿名线上平台收集的180万次真实编辑动作的数据集，作为用户与社区认可的放置监督信号；在此上进行训练与评估。

Result: StickerNet在用户研究与定量评测中优于常见基线，放置行为与人类更为一致；即使任务存在固有歧义，来自真实编辑模式的学习仍能有效提升性能与对齐度。

Conclusion: 以用户意图与表达性为中心重新定义图像合成问题，并用真实编辑轨迹监督的两阶段模型取得显著效果；该方向为视觉理解打开强调表达性而非写实性的研究路径。

Abstract: As a widely used operation in image editing workflows, image composition has traditionally been studied with a focus on achieving visual realism and semantic plausibility. However, in practical editing scenarios of the modern content creation landscape, many compositions are not intended to preserve realism. Instead, users of online platforms motivated by gaining community recognition often aim to create content that is more artistic, playful, or socially engaging. Taking inspiration from this observation, we define the expressive composition task, a new formulation of image composition that embraces stylistic diversity and looser placement logic, reflecting how users edit images on real-world creative platforms. To address this underexplored problem, we present StickerNet, a two-stage framework that first determines the composition type, then predicts placement parameters such as opacity, mask, location, and scale accordingly. Unlike prior work that constructs datasets by simulating object placements on real images, we directly build our dataset from 1.8 million editing actions collected on an anonymous online visual creation and editing platform, each reflecting user-community validated placement decisions. This grounding in authentic editing behavior ensures strong alignment between task definition and training supervision. User studies and quantitative evaluations show that StickerNet outperforms common baselines and closely matches human placement behavior, demonstrating the effectiveness of learning from real-world editing patterns despite the inherent ambiguity of the task. This work introduces a new direction in visual understanding that emphasizes expressiveness and user intent over realism.

</details>


### [30] [TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs](https://arxiv.org/abs/2511.20965)
*Md Adnan Arefeen,Biplob Debnath,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 提出TrafficLens：在多摄像头路口场景中，通过顺序利用相机重叠视域、分级VLM token预算和对象级相似度跳过冗余调用，实现更快的视频转文本，同时保持信息准确，最高提速约4倍。


<details>
  <summary>Details</summary>
Motivation: 多路口摄像头产生海量视频，现有基于LLM+RAG的方案需先用VLM将视频转文本，过程耗时，难以及时用于执法取证与交通管理；需要一种既快又准的多摄像头视频描述生成方法。

Method: 设计TrafficLens：1) 顺序处理多摄像头，利用相邻摄像头视域重叠，将前一摄像头的输出作为后续摄像头的提示，逐步细化描述；2) 采用分层VLM策略，按不同token上限递进调用，兼顾速度与细节；3) 引入对象级相似度检测，若与已有描述高度相似则跳过VLM调用以避免冗余。

Result: 在真实数据集上，TrafficLens将视频到文本的转换时间最多降低约4倍，同时保持信息准确性（未报告显著准确率损失）。

Conclusion: 通过顺序融合多摄像头信息、分级token预算与相似度跳过机制，可大幅加速路口多摄像头视频转文本流程，为交通管理与事件研判提供更及时的文本化洞见。

Abstract: Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\times$ while maintaining information accuracy.

</details>


### [31] [Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI](https://arxiv.org/abs/2511.20983)
*Al Amin,Kamrul Hasan,Liang Hong,Sharif Ullah*

Main category: cs.CV

TL;DR: 提出一种结合ViT与同态加密（CKKS）的隐私保护联邦学习框架，用CLS token加密聚合，显著降低通信量并抵御梯度重建攻击，在多机构肺癌病理分类上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习虽不共享原始数据，但梯度/更新可被重建攻击还原敏感图像；医疗合规（如HIPAA）限制数据共享，需在保证隐私下跨机构提升诊断性能，并减少通信与计算成本。

Method: 以ViT的CLS token（768维）作为紧凑特征表示进行安全聚合；在客户端使用CKKS同态加密对CLS token加密后上传，服务器端在密文域执行聚合与推理；与传统梯度加密对比，强调通信与隐私权衡；在三客户端的肺癌病理联邦设置中实验评估。

Result: 实证显示常规梯度在模型反演下易被近乎完美重建（PSNR 52.26 dB、SSIM 0.999、NMI 0.741）；所提CLS+HE方案可阻止此类攻击，并能在密文上进行推理，每轮聚合仅需约326 KB传输；相较梯度加密通信量降低约30倍；全局分类准确率：明文域96.12%，密文域90.02%。

Conclusion: 利用ViT的CLS token作为加密聚合单元配合同态加密，可在多机构病理分类中兼顾隐私与效率，显著降低通信开销并抵御重建攻击，代价是一定准确率下降，但整体可用性强。

Abstract: Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.

</details>


### [32] [Inversion-Free Style Transfer with Dual Rectified Flows](https://arxiv.org/abs/2511.20986)
*Yingying Deng,Xiangyu He,Fan Tang,Weiming Dong,Xucheng Yin*

Main category: cs.CV

TL;DR: 提出一种无需反演的双重Rectified Flow风格迁移框架，仅用前向推理即可将内容与风格融合，速度快、失真少，并通过动态中点插值与注意力注入提升保真与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的训练免风格迁移多依赖反演（如DDIM/ODE反演），计算昂贵且反演不准时会引入伪影与结构失真；需要一种既高效又稳定、能在无需反演的前提下实现内容与风格可靠融合的方法。

Method: 基于双Rectified Flows：分别为内容图与风格图预测各自的连续流动轨迹（速度场），在生成过程中以动态中点插值融合两条轨迹，按当前生成状态自适应整合两侧速度；联合建模内容分布、风格分布与目标风格化分布以获得鲁棒融合，而非简单叠加；并通过注意力注入在关键层引导风格特征进入，强化风格表达同时保持内容结构。全流程仅需前向传播，无反演。

Result: 在多种内容/风格组合上实现了更好的视觉保真（内容保持）与风格一致性，同时显著降低计算成本与时延；对不同风格类型具有良好泛化，较基线的训练免扩散方法减少失真与伪影。

Conclusion: 双Rectified Flow与动态中点插值的无反演框架可在仅用前向推理的条件下实现高效、稳健的风格迁移；注意力注入进一步提升风格融合质量，是一种兼顾质量与效率的通用管线。

Abstract: Style transfer, a pivotal task in image processing, synthesizes visually compelling images by seamlessly blending realistic content with artistic styles, enabling applications in photo editing and creative design. While mainstream training-free diffusion-based methods have greatly advanced style transfer in recent years, their reliance on computationally inversion processes compromises efficiency and introduces visual distortions when inversion is inaccurate. To address these limitations, we propose a novel \textit{inversion-free} style transfer framework based on dual rectified flows, which tackles the challenge of finding an unknown stylized distribution from two distinct inputs (content and style images), \textit{only with forward pass}. Our approach predicts content and style trajectories in parallel, then fuses them through a dynamic midpoint interpolation that integrates velocities from both paths while adapting to the evolving stylized image. By jointly modeling the content, style, and stylized distributions, our velocity field design achieves robust fusion and avoids the shortcomings of naive overlays. Attention injection further guides style integration, enhancing visual fidelity, content preservation, and computational efficiency. Extensive experiments demonstrate generalization across diverse styles and content, providing an effective and efficient pipeline for style transfer.

</details>


### [33] [RefOnce: Distilling References into a Prototype Memory for Referring Camouflaged Object Detection](https://arxiv.org/abs/2511.20989)
*Yu-Huan Wu,Zi-Xuan Zhu,Yan Wang,Liangli Zhen,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出一种无需测试时参考图像的Ref-COD方法：用类别原型内存蒸馏参考信息，并在推理时由查询自适应混合原型生成指导向量；配合双向注意力对齐提升表征匹配，R2C7K上达SOTA或竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有Ref-COD依赖双分支并在测试时需要参考图像，带来部署不便、延迟增大和数据收集负担；需一种既能利用参考信息又不在推理期依赖参考的方案。

Method: 训练期：维护按类别的EMA更新原型库，将参考信息蒸馏为类原型记忆；推理期：仅给定查询图像，由查询网络预测对各类原型的混合权重，合成“参考向量”提供引导。为缩小参考统计与伪装查询特征的分布差异，引入双向注意力对齐模块，同时适配查询特征与类表示。最终在单分支路径上完成Ref-COD分割。

Result: 在大规模R2C7K基准上进行广泛实验，整体性能与最新方法相比具有竞争性或更优，且在无需测试时参考的情况下实现更高效的推理。

Conclusion: 通过原型记忆蒸馏与查询条件混合、配合双向注意力对齐，可在不使用测试参考图的前提下有效完成Ref-COD，简化部署并提升效率，同时保持或提升精度。

Abstract: Referring Camouflaged Object Detection (Ref-COD) segments specified camouflaged objects in a scene by leveraging a small set of referring images. Though effective, current systems adopt a dual-branch design that requires reference images at test time, which limits deployability and adds latency and data-collection burden. We introduce a Ref-COD framework that distills references into a class-prototype memory during training and synthesizes a reference vector at inference via a query-conditioned mixture of prototypes. Concretely, we maintain an EMA-updated prototype per category and predict mixture weights from the query to produce a guidance vector without any test-time references. To bridge the representation gap between reference statistics and camouflaged query features, we propose a bidirectional attention alignment module that adapts both the query features and the class representation. Thus, our approach yields a simple, efficient path to Ref-COD without mandatory references. We evaluate the proposed method on the large-scale R2C7K benchmark. Extensive experiments demonstrate competitive or superior performance of the proposed method compared with recent state-of-the-arts. Code is available at https://github.com/yuhuan-wu/RefOnce.

</details>


### [34] [Wavefront-Constrained Passive Obscured Object Detection](https://arxiv.org/abs/2511.20991)
*Zhiwen Zheng,Yiwei Ouyang,Zhao Huang,Tao Zhang,Xiaoshuai Zhang,Huiyu Zhou,Wenwen Tang,Shaowei Jiang,Jin Liu,Xingru Huang*

Main category: cs.CV

TL;DR: 提出WavePCNet，用物理驱动的复杂波前传播建模与补偿机制，在低信噪多散射环境下更稳健地定位/分割视场外被遮挡目标，实验在四个实采数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用实值或局部卷积，难以刻画相干光的复振幅传播；在低信噪和介质扰动下易陷入不符合物理的解，导致不稳定与不可靠。需要把波动光学物理（复域传播、相位）纳入网络，并提升对扰动/高频细节的稳健感知。

Method: 提出物理驱动的WavePCNet：1) TriWCP（三相位波前复传播重投影）模块，引入复振幅传递算子，对相干传播进行精确约束；2) 动量记忆机制，抑制扰动累积、稳定优化；3) 高频跨层补偿增强（HCCE），构建频率选择与多尺度通路，动态建模跨层结构一致性，提升鲁棒性与可解释性。

Result: 在四个物理采集数据集上进行广泛实验，WavePCNet在准确性与鲁棒性上持续优于现有最先进方法。

Conclusion: 通过将物理一致的复域波前传播、动量记忆与高频跨层补偿融入网络，WavePCNet能在多散射、低信噪复杂环境下稳定恢复被遮挡目标的位置信息与分割，展现出更强鲁棒性与解释性。

Abstract: Accurately localizing and segmenting obscured objects from faint light patterns beyond the field of view is highly challenging due to multiple scattering and medium-induced perturbations. Most existing methods, based on real-valued modeling or local convolutional operations, are inadequate for capturing the underlying physics of coherent light propagation. Moreover, under low signal-to-noise conditions, these methods often converge to non-physical solutions, severely compromising the stability and reliability of the observation. To address these challenges, we propose a novel physics-driven Wavefront Propagating Compensation Network (WavePCNet) to simulate wavefront propagation and enhance the perception of obscured objects. This WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP) to incorporate complex amplitude transfer operators to precisely constrain coherent propagation behavior, along with a momentum memory mechanism to effectively suppress the accumulation of perturbations. Additionally, a High-frequency Cross-layer Compensation Enhancement is introduced to construct frequency-selective pathways with multi-scale receptive fields and dynamically model structural consistency across layers, further boosting the model's robustness and interpretability under complex environmental conditions. Extensive experiments conducted on four physically collected datasets demonstrate that WavePCNet consistently outperforms state-of-the-art methods across both accuracy and robustness.

</details>


### [35] [GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision](https://arxiv.org/abs/2511.20994)
*Yuxiao Xiang,Junchi Chen,Zhenchao Jin,Changtao Miao,Haojie Yuan,Qi Chu,Tao Gong,Nenghai Yu*

Main category: cs.CV

TL;DR: 提出GuardTrace-VL：一个面向视觉-语言推理链(QTA：问题-思考-答案)的安全审计器，能在中间推理阶段检测不安全内容；并构建GuardTrace数据集与三阶段渐进式训练，显著提升不安全推理检测F1至93.1%，比现有最强基线高13.5%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态安全防护多只检查输入问题与最终答案，忽略显式推理链中的中间想法/解释，导致偏见、违规利用图像信息等在推理过程中出现而未被发现，带来部署风险。需要一个能够面向图文联合、覆盖完整QTA流程、在推理产生时就识别风险的审计机制。

Method: 1) 设计GuardTrace-VL：联合图像与文本，对QTA全流程进行安全审计，特别聚焦中间推理内容的风险识别；2) 构建GuardTrace数据集：通过多样化提示生成、再经由MLRM与人工投票核验进行精炼；3) 提出三阶段渐进式训练策略，并与数据精炼结合，让模型学习细粒度、情境依赖的安全偏好与不同风险等级的判定。

Result: 在作者提出的测试集（含域内与域外场景）上，GuardTrace-VL在不安全推理检测任务上取得F1=93.1%，较此前最强多模态安全防御方法提升13.5% F1。

Conclusion: 监控并审计QTA全链路、尤其是中间推理，对多模态模型安全至关重要。GuardTrace-VL与配套数据与训练方案显著提升了对不安全推理的检测能力，具有实际部署价值；代码将开源。

Abstract: Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use of visual context, to emerge during reasoning. We introduce GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline via joint image-text analysis, enabling detection of unsafe content as it emerges in the reasoning stage. To support training and evaluation, we construct the GuardTrace dataset, which is generated through diverse prompting strategies and refined via a MLRM- and human-based voting and verification pipeline. Furthermore, we propose a three-stage progressive training scheme combined with the data refinement process, enabling the model to learn nuanced and context-dependent safety preferences according to different risk levels. On our proposed test set covering both in-domain and out-of-domain scenarios, GuardTrace-VL model achieves an F1 score of 93.1% on unsafe reasoning detection tasks, representing a 13.5% improvement in F1 score compared to the previous strongest multimodal safety defense methods. The codes will be made publicly available.

</details>


### [36] [From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition](https://arxiv.org/abs/2511.20996)
*Jingxi Chen,Yixiao Zhang,Xiaoye Qian,Zongxia Li,Cornelia Fermuller,Caren Chen,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 通过轻量微调扩散式修复模型，把单幅图像分解为前景/背景分层；并提出线性复杂度的多模态上下文融合模块以更好保细节；用纯合成数据训练，在对象移除与遮挡恢复上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单图像分层能带来独立编辑与更灵活的创作，但现有方法和数据匮乏，导致难以精准分离前景/背景并恢复被遮挡内容。作者观察到分层分解与图像修复/外扩有强关联，期望借助强大的扩散式修复模型能力解决分层难题。

Method: 将扩散式inpainting模型适配为层分解器：对单图像进行轻量微调，使其能够同时预测可见前景层与被遮挡的背景层；为提升潜空间细节保真，引入线性复杂度的多模态上下文融合模块（融合图像/掩码/文本等条件）；全流程在由开源素材构建的合成数据上训练。

Result: 在对象移除和遮挡恢复任务上取得更佳定量与定性表现；能够更完整地重建被遮挡区域，并保持细节与一致性；展示多种下游编辑与创意应用的新能力。

Conclusion: 扩散式修复模型经轻量微调即可胜任图像层分解；结合线性复杂度的多模态融合可提升细节与效率；纯合成数据足以训练出高质量的分层分解模型，为灵活的图像编辑与创作解锁新可能。

Abstract: Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.

</details>


### [37] [Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning](https://arxiv.org/abs/2511.21002)
*Xiaoxing You,Qiang Huang,Lingyu Li,Chi Zhang,Xiaopeng Liu,Min Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 提出MERGE：一个面向新闻图像描述的多模态实体感知检索增强生成框架，通过实体中心的多模态知识库、分阶段假设-描述策略与动态图像引导检索，显著提升信息覆盖、跨模态对齐与视觉-实体落地，在GoodNews、NYTimes800k与跨域Visual News上均大幅超SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有新闻图像描述面临三大痛点：描述未覆盖关键信息、图文对齐弱、视觉实体落地差；需要一种可融合文章上下文、视觉线索与外部知识的统一方法来提升新闻语境下的准确性与可解释性。

Method: 提出MERGE框架：1）构建实体中心的多模态知识库（EMKB），融合文本、图像与结构化知识以支持背景检索；2）采用多阶段“假设-描述”策略加强跨模态对齐；3）利用由图像内容引导的动态检索提升视觉-实体匹配；整体为检索增强生成范式。

Result: 在GoodNews与NYTimes800k上，CIDEr分别提升+6.84与+1.16，命名实体识别F1分别提升+4.14与+2.64；在未见过的Visual News上也获得+20.17 CIDEr与+6.22 F1的跨域增益。

Conclusion: MERGE通过实体感知的检索增强与分阶段对齐，显著改进新闻图像描述的完整性、对齐性与实体落地，并展现出强鲁棒性与域泛化能力。

Abstract: News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.

</details>


### [38] [MetaRank: Task-Aware Metric Selection for Model Transferability Estimation](https://arxiv.org/abs/2511.21007)
*Yuhang Liu,Wenjie Zhao,Yunhui Guo*

Main category: cs.CV

TL;DR: 论文提出MetaRank：一种自动、任务感知的MTE指标选择框架，用文本描述编码数据集与指标，学习到“哪个任务该用哪个MTE指标”，在新任务上无需微调即可优先推荐最合适的指标，实验显示在多模型多数据集上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有迁移学习需先挑选合适的源模型，MTE可用作无需全面微调的代理打分。但不同MTE指标在不同目标任务上效果差异大，缺乏通用最佳指标；现实中常凭经验或平均历史表现选指标，易失配。需要一种能依据具体任务自动挑选最合适MTE指标的方法。

Method: 将“选择MTE指标”建模为Learning-to-Rank问题。使用预训练语言模型将目标数据集的文本描述与各MTE指标的文本机制描述编码到同一语义空间；离线阶段在多样化meta-tasks上训练一个meta-predictor，用listwise排序目标优化，强调正确排列顶尖指标；在线阶段对新数据集仅依据其文本描述，对候选MTE指标进行排序推荐。

Result: 在包含11个预训练模型与11个目标数据集的实验中，MetaRank在为任务选择MTE指标的排序质量上显著优于基线与简单平均历史表现的做法，能更稳定地挑出表现更优的指标。

Conclusion: MTE指标的有效性强依赖任务，单一指标难以通吃。MetaRank通过文本语义编码与排序式meta-learning，能在新任务上先验地选出更合适的指标，提升迁移学习前期选择效率与效果。

Abstract: Selecting an appropriate pre-trained source model is a critical, yet computationally expensive, task in transfer learning. Model Transferability Estimation (MTE) methods address this by providing efficient proxy metrics to rank models without full fine-tuning. In practice, the choice of which MTE metric to use is often ad hoc or guided simply by a metric's average historical performance. However, we observe that the effectiveness of MTE metrics is highly task-dependent and no single metric is universally optimal across all target datasets. To address this gap, we introduce MetaRank, a meta-learning framework for automatic, task-aware MTE metric selection. We formulate metric selection as a learning-to-rank problem. Rather than relying on conventional meta-features, MetaRank encodes textual descriptions of both datasets and MTE metrics using a pretrained language model, embedding them into a shared semantic space. A meta-predictor is then trained offline on diverse meta-tasks to learn the intricate relationship between dataset characteristics and metric mechanisms, optimized with a listwise objective that prioritizes correctly ranking the top-performing metrics. During the subsequent online phase, MetaRank efficiently ranks the candidate MTE metrics for a new, unseen target dataset based on its textual description, enabling practitioners to select the most appropriate metric a priori. Extensive experiments across 11 pretrained models and 11 target datasets demonstrate the strong effectiveness of our approach.

</details>


### [39] [Structure-Aware Prototype Guided Trusted Multi-View Classification](https://arxiv.org/abs/2511.21021)
*Haojian Huang,Jiahao Shi,Zhe Liu,Harold Haodong Chen,Han Fang,Hao Sun,Zhongjiang He*

Main category: cs.CV

TL;DR: 提出一种基于原型的可信多视图分类框架，用原型刻画各视图的邻域结构，联合对齐视内与视间关系，以更高效一致地挖掘跨视共识，实验表明在多数据集上性能与鲁棒性具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有TMVC依赖全局稠密邻接来建模视内依赖，计算开销大，且难以直接保证视间关系一致性；跨视证据聚合多靠手工权重，缺乏在类别空间中邻域结构一致性的保证，削弱分类可信度。

Method: 引入“原型”表示每个视图的邻域结构：1) 以少量原型简化视内邻域关系学习，替代全局稠密邻接；2) 设计动态对齐机制，将视内原型结构与跨视关系共同约束，使不同视图的结构在类别空间中对齐；3) 基于原型对齐进行跨视共识发现与证据聚合，实现端到端训练、自动学习权重。

Result: 在多个公共多视图数据集上，所提方法在下游分类性能与鲁棒性方面优于或达到主流TMVC方法，显示出竞争力。

Conclusion: 用原型化邻域结构与动态跨视对齐替代稠密邻接与手工权重，可在保证一致性的同时降低计算成本，提升TMVC的可靠性与效率。

Abstract: Trustworthy multi-view classification (TMVC) addresses the challenge of achieving reliable decision-making in complex scenarios where multi-source information is heterogeneous, inconsistent, or even conflicting. Existing TMVC approaches predominantly rely on globally dense neighbor relationships to model intra-view dependencies, leading to high computational costs and an inability to directly ensure consistency across inter-view relationships. Furthermore, these methods typically aggregate evidence from different views through manually assigned weights, lacking guarantees that the learned multi-view neighbor structures are consistent within the class space, thus undermining the trustworthiness of classification outcomes. To overcome these limitations, we propose a novel TMVC framework that introduces prototypes to represent the neighbor structures of each view. By simplifying the learning of intra-view neighbor relations and enabling dynamic alignment of intra- and inter-view structure, our approach facilitates more efficient and consistent discovery of cross-view consensus. Extensive experiments on multiple public multi-view datasets demonstrate that our method achieves competitive downstream performance and robustness compared to prevalent TMVC methods.

</details>


### [40] [CameraMaster: Unified Camera Semantic-Parameter Control for Photography Retouching](https://arxiv.org/abs/2511.21024)
*Qirui Yang,Yang Yang,Ying Zeng,Xiaobin Hu,Bo Li,Huanjing Yue,Jingyu Yang,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: CameraMaster提出一种统一的相机感知图像润饰框架，通过显式解耦“摄影指令”（语义意图）与“相机参数”并在扩散模型中协同调制，实现对曝光、白平衡、变焦等可物理解释参数的精确、可组合、近线性控制，显著优于仅依赖文本或多头训练的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导扩散在图像润饰中难以实现物理一致且可精确控制的相机参数调整：文本提示模糊且耦合，难以对曝光/白平衡/变焦等进行稳健控制；为每个参数训练独立头或权重则牺牲可扩展性、参数组合能力与对细微变化的敏感度。

Method: 提出CameraMaster：1) 显式解耦两路信息——摄影指令表示（意图）与相机参数嵌入（精确设置）。2) 用相机参数嵌入同时调制摄影指令与内容语义；将调制后的指令通过跨注意力注入内容特征，形成强相机敏感的语义上下文。3) 将指令与相机嵌入作为条件与门控信号注入时间嵌入，实现统一、逐层的去噪调制，强化语义-参数对齐。4) 构建含7.8万图像-文本对并带相机参数标注的数据集以训练与评测。

Result: 模型对参数变化呈单调、近线性响应；支持无缝的多参数组合；在多项实验中显著优于现有方法，表现出更强的相机敏感性与可控性。

Conclusion: 通过解耦摄影意图与相机参数并在扩散去噪过程中进行统一层级调制，CameraMaster实现了可物理解释、精确且可组合的图像润饰控制，为相机参数可控的文本引导编辑提供了可扩展的统一框架。

Abstract: Text-guided diffusion models have greatly advanced image editing and generation. However, achieving physically consistent image retouching with precise parameter control (e.g., exposure, white balance, zoom) remains challenging. Existing methods either rely solely on ambiguous and entangled text prompts, which hinders precise camera control, or train separate heads/weights for parameter adjustment, which compromises scalability, multi-parameter composition, and sensitivity to subtle variations. To address these limitations, we propose CameraMaster, a unified camera-aware framework for image retouching. The key idea is to explicitly decouple the camera directive and then coherently integrate two critical information streams: a directive representation that captures the photographer's intent, and a parameter embedding that encodes precise camera settings. CameraMaster first uses the camera parameter embedding to modulate both the camera directive and the content semantics. The modulated directive is then injected into the content features via cross-attention, yielding a strongly camera-sensitive semantic context. In addition, the directive and camera embeddings are injected as conditioning and gating signals into the time embedding, enabling unified, layer-wise modulation throughout the denoising process and enforcing tight semantic-parameter alignment. To train and evaluate CameraMaster, we construct a large-scale dataset of 78K image-prompt pairs annotated with camera parameters. Extensive experiments show that CameraMaster produces monotonic and near-linear responses to parameter variations, supports seamless multi-parameter composition, and significantly outperforms existing methods.

</details>


### [41] [CaptionQA: Is Your Caption as Useful as the Image Itself?](https://arxiv.org/abs/2511.21025)
*Shijia Yang,Yunong Liu,Bohan Zhai,Ximeng Sun,Zicheng Liu,Emad Barsoum,Manling Li,Chenfeng Xu*

Main category: cs.CV

TL;DR: CaptionQA提出以“下游任务效用”为核心的图片字幕评测基准：用仅基于字幕回答细粒度多选题的表现，来衡量字幕能否真正替代图像。跨4大领域构建3.3万题的密集标注问答，显示现有MLLM生成的字幕与图像本身效用存在显著差距（在字幕场景可掉至-32%）。


<details>
  <summary>Details</summary>
Motivation: 现有字幕评估多依赖词汇或句子层相似度（如BLEU、CIDEr）或人主观偏好，难以回答“字幕能否在真实多模态应用中替代图像”这一关键问题。需要一个直接以下游任务完成度衡量字幕价值的评测框架，覆盖多领域、多粒度信息需求，并可扩展。

Method: 提出CaptionQA：构建4个领域（自然、文档、电商、具身智能）及25个一级、69个二级细粒度标签体系；针对每张图生成高密度、需视觉证据的多选题（平均每图50.3题，共33,027题）。评测时，使用LLM在“只给字幕不看图”的条件下回答问题，以回答准确率作为字幕的任务效用指标；并与直接看图的表现比较，量化“图→字幕”的信息损失。提供开源管线以扩展新领域。

Result: 在SOTA多模态大模型上测试发现：即便在传统Image-QA基准上表现相近的模型，其生成字幕在CaptionQA上的任务效用可相差显著，最高较看图降至-32%。不同领域与细粒度类别下也呈系统性差距，显示现有字幕常遗漏对任务关键的视觉细节。

Conclusion: 字幕质量应以“可支撑下游任务”的效用来衡量，而非仅靠表面相似度或笼统主观分。CaptionQA揭示当前MLLM字幕与真实图像效用间存在显著鸿沟，并提供可扩展评测与数据生成管线，促进更注重任务关键信息保真的字幕生成与评估研究。

Abstract: Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.

</details>


### [42] [FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation](https://arxiv.org/abs/2511.21029)
*Kaixing Yang,Xulong Tang,Ziqiao Peng,Xiangyue Zhang,Puwei Wang,Jun He,Hongyan Liu*

Main category: cs.CV

TL;DR: 提出FlowerDance，一种高效的music-to-dance生成方法，结合MeanFlow与物理一致性约束，用少量采样步生成高质量、可编辑的舞蹈动作，并以BiMamba骨干与通道级跨模态融合实现非自回归推理，在AIST++与FineDance上达成SOTA质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有音乐到舞蹈生成方法在推理速度与内存效率上不足，难以为实时或高保真3D渲染留出计算余量，限制了实际应用中角色的表现力与交互性；同时需要保证生成动作的物理合理性与艺术表现。

Method: 1) 将MeanFlow引入动作生成，以流式/最小均方运输思想在少量采样步中逼近高质量分布；2) 物理一致性约束（如关节动力学与接触稳定等）融入训练/采样，提升物理可行性；3) 采用BiMamba为骨干网络以高效时序建模；4) 设计通道级跨模态融合，将音乐特征与动作通道高效对齐；5) 非自回归生成框架提升并行度；6) 支持用户驱动的动作编辑。

Result: 在AIST++与FineDance数据集上，获得SOTA的动作质量指标与显著更快的推理速度/更低内存占用（文中宣称较基线以更少采样步实现更高或相当指标），并展示了交互式编辑能力。

Conclusion: FlowerDance在保证艺术表现与物理可信的同时大幅提升生成效率，适用于需要实时或高保真渲染的应用场景，并提供可编辑性以增强实用性。

Abstract: Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.

</details>


### [43] [LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules](https://arxiv.org/abs/2511.21042)
*Cheng Yang,Hui Jin,Xinlei Yu,Zhipeng Wang,Yaoqun Liu,Fenglei Fan,Dajiang Lei,Gangyong Jia,Changmiao Wang,Ruiquan Ge*

Main category: cs.CV

TL;DR: 提出 LungNoduleAgent：一个用于肺结节CT分析的协作多智能体系统，通过“找结节—写报告—推断恶性”三阶段流水线，将检测、描述与病理知识推理结合，在多数据集上优于现有VLM与专家模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在肺CT上难以精准描述结节形态、融入医学知识，影响临床可靠性；协作多智能体在医学中的潜力未充分验证。

Method: 构建三模块协作流水线：1) Nodule Spotter 协调临床级检测模型实现高精度结节定位；2) Radiologist 结合局部化图像描述生成结构化CT报告；3) Doctor Agent System 以报告与影像为输入，结合病理知识库与多智能体框架进行恶性度推理与分级；强调区域级语义对齐与专家知识注入。

Result: 在两套私有数据与公开LIDC-IDRI上进行大量实验，整体性能超过主流视觉-语言模型、通用代理系统及先进专家模型，报告与恶性分级更准确、鲁棒。

Conclusion: 区域级语义对齐与多智能体协作可显著提升肺结节诊断的可用性与准确性；LungNoduleAgent为临床肺结节分析提供了有前景的基础工具。

Abstract: Diagnosing lung cancer typically involves physicians identifying lung nodules in Computed tomography (CT) scans and generating diagnostic reports based on their morphological features and medical expertise. Although advancements have been made in using multimodal large language models for analyzing lung CT scans, challenges remain in accurately describing nodule morphology and incorporating medical expertise. These limitations affect the reliability and effectiveness of these models in clinical settings. Collaborative multi-agent systems offer a promising strategy for achieving a balance between generality and precision in medical applications, yet their potential in pathology has not been thoroughly explored. To bridge these gaps, we introduce LungNoduleAgent, an innovative collaborative multi-agent system specifically designed for analyzing lung CT scans. LungNoduleAgent streamlines the diagnostic process into sequential components, improving precision in describing nodules and grading malignancy through three primary modules. The first module, the Nodule Spotter, coordinates clinical detection models to accurately identify nodules. The second module, the Radiologist, integrates localized image description techniques to produce comprehensive CT reports. Finally, the Doctor Agent System performs malignancy reasoning by using images and CT reports, supported by a pathology knowledge base and a multi-agent system framework. Extensive testing on two private datasets and the public LIDC-IDRI dataset indicates that LungNoduleAgent surpasses mainstream vision-language models, agent systems, and advanced expert models. These results highlight the importance of region-level semantic alignment and multi-agent collaboration in diagnosing nodules. LungNoduleAgent stands out as a promising foundational tool for supporting clinical analyses of lung nodules.

</details>


### [44] [PG-ControlNet: A Physics-Guided ControlNet for Generative Spatially Varying Image Deblurring](https://arxiv.org/abs/2511.21043)
*Hakki Motorcu,Mujdat Cetin*

Main category: cs.CV

TL;DR: 提出一种将生成式先验与显式致密物理约束相结合的空间可变去模糊框架，用高维压缩核连续场刻画退化并以其条件控制扩散模型，实现物理准确与感知质量的统一并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有两类方法各有缺陷：基于模型的深度展开能嵌入物理约束但纹理过平且有伪影；生成式模型感知质量好却因物理约束弱而易幻觉。复杂混合退化与强噪声下，这一物理-感知鸿沟尤为突出，亟需兼顾二者的新范式。

Method: 不再简化退化场，而是将其表示为致密的高维压缩核连续场，捕捉细微的运动与多样退化变化；将该丰富的退化描述字段用于条件化一个ControlNet架构，从而强力引导扩散采样，使生成先验受到严格物理约束。

Result: 在大量严苛、强模糊数据上，所提方法在物理一致性与感知真实感上均显著优于最先进的模型驱动方法与生成式基线，尤其在严重模糊场景中表现突出。

Conclusion: 通过以致密物理约束“驯化”强生成先验，并用高维压缩核场精细建模空间可变退化，可有效弥合物理准确性与感知质量之间的差距，提供一种更稳健的去模糊范式。

Abstract: Spatially varying image deblurring remains a fundamentally ill-posed problem, especially when degradations arise from complex mixtures of motion and other forms of blur under significant noise. State-of-the-art learning-based approaches generally fall into two paradigms: model-based deep unrolling methods that enforce physical constraints by modeling the degradations, but often produce over-smoothed, artifact-laden textures, and generative models that achieve superior perceptual quality yet hallucinate details due to weak physical constraints. In this paper, we propose a novel framework that uniquely reconciles these paradigms by taming a powerful generative prior with explicit, dense physical constraints. Rather than oversimplifying the degradation field, we model it as a dense continuum of high-dimensional compressed kernels, ensuring that minute variations in motion and other degradation patterns are captured. We leverage this rich descriptor field to condition a ControlNet architecture, strongly guiding the diffusion sampling process. Extensive experiments demonstrate that our method effectively bridges the gap between physical accuracy and perceptual realism, outperforming state-of-the-art model-based methods as well as generative baselines in challenging, severely blurred scenarios.

</details>


### [45] [MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization](https://arxiv.org/abs/2511.21051)
*Yingjie Xia,Xi Wang,Jinglei Shi,Vicky Kalogeiton,Jian Yang*

Main category: cs.CV

TL;DR: 提出MUSE：一个无需重新训练、统一支持情感图像生成与编辑的框架，通过测试时优化策略，利用现成情感分类器引导，解决如何、何时、以及引导哪种情感三大问题，在情感准确性、语义多样性与内容一致性间达到更优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有图像情感合成方法将“生成”和“编辑”割裂处理，带来效率低下与适用性受限；许多场景（如治疗、叙事）需要二者自然交织的工作流，且缺乏无需额外训练与专门数据集的统一方案。

Method: 提出MUSE，以测试时扩展/缩放（类TTS）思路：不更新扩散模型参数，利用现成情感分类器的梯度对“情感token”进行优化；设计三项机制：1) HOW：用分类器梯度稳定地优化情感引导；2) WHEN：以语义相似度为监督信号，自适应确定引导时机；3) WHICH：多情感损失，抑制内在或相近情感的干扰，提升目标情感的可控性。

Result: 在情感生成与编辑两任务上均优于现有方法，提升情感准确性与语义多样性，同时在内容保持、文本遵从与情感真实度之间取得更佳平衡。

Conclusion: MUSE建立了统一的情感合成范式：通过测试时无训练的情感引导，实现稳定、可控、通用的图像情感生成与编辑。

Abstract: Images evoke emotions that profoundly influence perception, often prioritized over content. Current Image Emotional Synthesis (IES) approaches artificially separate generation and editing tasks, creating inefficiencies and limiting applications where these tasks naturally intertwine, such as therapeutic interventions or storytelling. In this work, we introduce MUSE, the first unified framework capable of both emotional generation and editing. By adopting a strategy conceptually aligned with Test-Time Scaling (TTS) that widely used in both LLM and diffusion model communities, it avoids the requirement for additional updating diffusion model and specialized emotional synthesis datasets. More specifically, MUSE addresses three key questions in emotional synthesis: (1) HOW to stably guide synthesis by leveraging an off-the-shelf emotion classifier with gradient-based optimization of emotional tokens; (2) WHEN to introduce emotional guidance by identifying the optimal timing using semantic similarity as a supervisory signal; and (3) WHICH emotion to guide synthesis through a multi-emotion loss that reduces interference from inherent and similar emotions. Experimental results show that MUSE performs favorably against all methods for both generation and editing, improving emotional accuracy and semantic diversity while maintaining an optimal balance between desired content, adherence to text prompts, and realistic emotional expression. It establishes a new paradigm for emotion synthesis.

</details>


### [46] [Long-Term Alzheimers Disease Prediction: A Novel Image Generation Method Using Temporal Parameter Estimation with Normal Inverse Gamma Distribution on Uneven Time Series](https://arxiv.org/abs/2511.21057)
*Xin Hong,Xinze Sun,Yinhao Li,Yen-Wei Chen*

Main category: cs.CV

TL;DR: 提出T-NIG模型，在不规则时间间隔下利用正态-逆伽马分布的时间参数生成中间/未来脑影像并预测AD进展，兼顾不确定性估计，短长期任务均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于影像生成的AD长期预测在处理不规则时间序列时易丢失疾病相关特征；时间分布特性可反映病灶演化，因此需要能显式建模时间对特征分布影响并控制不确定性的生成预测模型。

Method: 在正态-逆伽马分布（NIG）中注入时间参数，形成T-NIG；利用两个时间点的脑影像通过坐标邻域提取特征，建模其随时间演化以生成中间/未来影像；通过NIG的参数化同时量化并分离认知不确定性（epistemic）与内在不确定性（aleatoric），缓解因时间数据稀疏导致的不确定；在不规则时间间隔序列上进行训练与推断。

Result: 在给定数据集上的短期与长期预测任务均达成SOTA表现；生成图像能更好保留AD相关特征，并在不规则时间分布条件下提升预测准确度与稳定性。

Conclusion: 显式时间参数化的NIG分布与不确定性建模可有效支持不规则时间间隔下的脑影像生成与AD进展预测，既提高长期预测性能，也保持疾病相关特征的可追踪性。

Abstract: Image generation can provide physicians with an imaging diagnosis basis in the prediction of Alzheimer's Disease (AD). Recent research has shown that long-term AD predictions by image generation often face difficulties maintaining disease-related characteristics when dealing with irregular time intervals in sequential data. Considering that the time-related aspects of the distribution can reflect changes in disease-related characteristics when images are distributed unevenly, this research proposes a model to estimate the temporal parameter within the Normal Inverse Gamma Distribution (T-NIG) to assist in generating images over the long term. The T-NIG model employs brain images from two different time points to create intermediate brain images, forecast future images, and predict the disease. T-NIG is designed by identifying features using coordinate neighborhoods. It incorporates a time parameter into the normal inverse gamma distribution to understand how features change in brain imaging sequences that have varying time intervals. Additionally, T-NIG utilizes uncertainty estimation to reduce both epistemic and aleatoric uncertainties in the model, which arise from insufficient temporal data. In particular, the T-NIG model demonstrates state-of-the-art performance in both short-term and long-term prediction tasks within the dataset. Experimental results indicate that T-NIG is proficient in forecasting disease progression while maintaining disease-related characteristics, even when faced with an irregular temporal data distribution.

</details>


### [47] [MIRA: Multimodal Iterative Reasoning Agent for Image Editing](https://arxiv.org/abs/2511.21087)
*Ziyun Zeng,Hang Hua,Jiebo Luo*

Main category: cs.CV

TL;DR: 提出MIRA：一个可插拔的多模态“感知-推理-行动”迭代代理，通过逐步原子化编辑指令与视觉反馈闭环，显著提升复杂自然语言指令下的图像编辑语义一致性与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 扩散式图像编辑在复杂语言指令（组合关系、上下文线索、指代表达）下常出现语义漂移或无法落地编辑，缺乏对多步推理与交互式反馈的机制。

Method: 构建MIRA多模态迭代代理：在每一步根据图像与文本状态产生原子编辑指令，执行后读取视觉反馈再推理下一步；提供15万条多模态工具使用数据集MIRA-Editing；采用两阶段训练（SFT监督微调 + GRPO强化偏好优化），并与开源编辑器（如Flux.1-Kontext、Step1X-Edit、Qwen-Image-Edit）对接。

Result: 在复杂编辑任务上，相比仅一次性Prompt或静态计划的扩散编辑模型，MIRA显著提升语义一致性与感知质量；与GPT-Image、Nano-Banana等专有系统相比达到相当或更优表现。

Conclusion: 通过“迭代感知—推理—行动”的代理化范式与数据+训练管线，MIRA使开源图像编辑器在复杂语言编辑中实现强鲁棒与高质量输出，可作为轻量、即插即用的增强模块。

Abstract: Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.

</details>


### [48] [CLRecogEye : Curriculum Learning towards exploiting convolution features for Dynamic Iris Recognition](https://arxiv.org/abs/2511.21097)
*Geetanjali Sharma,Gaurav Jaswal,Aditya Nigam,Raghavendra Ramachandra*

Main category: cs.CV

TL;DR: 提出一种将虹膜图像沿一维切分为序列并用3D-CNN学习时空-空间特征的通用匹配管线，结合课程学习与Triplet+ArcFace损失，提升对旋转、尺度、反射和虚焦的鲁棒性与可判别性。


<details>
  <summary>Details</summary>
Motivation: 现有虹膜识别虽性能优，但在旋转、尺度变化、镜面反射、虚焦等扰动下鲁棒性不足；多数方法依赖点对点的余弦/L2比对，未有效利用虹膜图案的空间与“时空”（序列化后）的结构信息。

Method: 将每张虹膜图像沿一维切分成按序子图，输入3D-CNN以同时建模空间与“时空”依赖；采用课程学习逐步增强时空特征动态的建模难度；以端到端方式训练，使用Triplet损失与ArcFace损失约束嵌入，强化深度度量空间的判别性与鲁棒性。

Result: 在各种扰动（旋转、尺度、反射、虚焦）下得到更鲁棒、泛化性更好的嵌入表示与匹配性能（摘要未给具体数值，但宣称显著提升）；提供开源代码仓库以复现。

Conclusion: 通过序列化输入+3D-CNN+课程学习+度量学习损失的统一框架，可显式嵌入“时间”依赖以丰富特征表达，从而在多种真实场景扰动下实现更稳健、可泛化的虹膜认证。

Abstract: Iris authentication algorithms have achieved impressive recognition performance, making them highly promising for real-world applications such as border control, citizen identification, and both criminal investigations and commercial systems. However, their robustness is still challenged by variations in rotation, scale, specular reflections, and defocus blur. In addition, most existing approaches rely on straightforward point-to-point comparisons, typically using cosine or L2 distance, without effectively leveraging the spatio-spatial-temporal structure of iris patterns. To address these limitations, we propose a novel and generalized matching pipeline that learns rich spatio-spatial-temporal representations of iris features. Our approach first splits each iris image along one dimension, generating a sequence of sub-images that serve as input to a 3D-CNN, enabling the network to capture both spatial and spatio-spatial-temporal cues. To further enhance the modeling of spatio-spatial-temporal feature dynamics, we train the model in curriculum manner. This design allows the network to embed temporal dependencies directly into the feature space, improving discriminability in the deep metric domain. The framework is trained end-to-end with triplet and ArcFace loss in a curriculum manner, enforcing highly discriminative embeddings despite challenges like rotation, scale, reflections, and blur. This design yields a robust and generalizable solution for iris authentication.Github code: https://github.com/GeetanjaliGTZ/CLRecogEye

</details>


### [49] [Pygmalion Effect in Vision: Image-to-Clay Translation for Reflective Geometry Reconstruction](https://arxiv.org/abs/2511.21098)
*Gayoung Lee,Junho Kim,Jin-Hwa Kim,Junmo Kim*

Main category: cs.CV

TL;DR: 提出“视觉中的皮格马利翁效应”：把反射物体的图像翻译成“陶土风格”无反射图像，以稳定几何并提升3D重建的法线和网格完整度。核心是一个反射BRDF分支与陶土引导分支的双分支网络，利用合成陶土图像共同训练，抑制高光而保留固有几何一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 反射会把外界辐射场与物体几何纠缠在一起，导致多视角重建难以从视角相关高光中分离真实几何。现有反射处理方法在几何稳定性与法线精度上仍不足，需要一种能“去高光、保几何”的归纳偏置。

Method: 提出“图像到陶土”翻译思想：生成无反射、色调中性的陶土风格监督图像。设计双分支网络：1) 基于BRDF建模的反射分支学习处理视角相关反射；2) 陶土引导分支用中性图像稳定几何与细化法线。两分支联合训练，以合成陶土图像作为无反射监督信号，彼此互补。

Result: 在合成与真实数据集上，相比现有反射处理重建方法，显著提升法线准确度与网格完整度。

Conclusion: 将“去光泽、向中性翻译”的归纳偏置引入重建，可有效解耦反射与几何。用陶土风格监督训练的双分支网络能稳健重建高反射物体，提示“以无光见物”的思路是学习反射物体几何的有力先验。

Abstract: Understanding reflection remains a long-standing challenge in 3D reconstruction due to the entanglement of appearance and geometry under view-dependent reflections. In this work, we present the Pygmalion Effect in Vision, a novel framework that metaphorically "sculpts" reflective objects into clay-like forms through image-to-clay translation. Inspired by the myth of Pygmalion, our method learns to suppress specular cues while preserving intrinsic geometric consistency, enabling robust reconstruction from multi-view images containing complex reflections. Specifically, we introduce a dual-branch network in which a BRDF-based reflective branch is complemented by a clay-guided branch that stabilizes geometry and refines surface normals. The two branches are trained jointly using the synthesized clay-like images, which provide a neutral, reflection-free supervision signal that complements the reflective views. Experiments on both synthetic and real datasets demonstrate substantial improvement in normal accuracy and mesh completeness over existing reflection-handling methods. Beyond technical gains, our framework reveals that seeing by unshining, translating radiance into neutrality, can serve as a powerful inductive bias for reflective object geometry learning.

</details>


### [50] [Scaling Foundation Models for Radar Scene Understanding](https://arxiv.org/abs/2511.21105)
*Pushkal Mishra,Kshitiz Bansal,Dinesh Bharadia*

Main category: cs.CV

TL;DR: 引入RadarFM，一个以结构化空间语言监督学习场景级表示的雷达基础模型，提出结构化字幕与哈希感知对比学习以实现可迁移、细粒度空间推理，并在CARLA上生成大规模数据与提出定位感知评测指标。


<details>
  <summary>Details</summary>
Motivation: 现有雷达感知方法分散、任务特定、难以跨任务迁移；视觉与语言的基础模型已成功，但与雷达融合不足，需要统一的、可泛化的雷达场景级表示学习框架。

Method: 1) 结构化字幕框架：在雷达原生坐标中编码车辆分布与空间关系；2) 哈希感知对比学习：用连续相似度而非二元匹配度量场景相似性，支持细粒度空间推理；3) 基于CARLA生成多场景、大规模、精标注的雷达数据；4) 设计定位感知指标，评估超越传统检测的空间精度。

Result: 在合成数据上训练的RadarFM学得统一表征，能在多下游任务间迁移，并在新提出的定位感知指标上表现优异，同时提升细粒度空间推理能力。

Conclusion: 结构化空间语言监督结合哈希感知对比学习可为雷达感知构建通用表征，改善跨任务迁移与空间精度评估，为雷达领域的基础模型研究提供了可行路径。

Abstract: Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.

</details>


### [51] [EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens](https://arxiv.org/abs/2511.21106)
*Ze Feng,Sen Yang,Boqiang Duan,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出EM-KD蒸馏范式，通过匹配并对齐师生模型的视觉token后，在视觉-语言亲和与视觉语义两层面进行知识蒸馏，显著提升高效多模态大模型的准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 高效MLLM通过压缩视觉token节省资源，但因视觉信息缺失导致细粒度理解能力下降；现有蒸馏忽视师生在视觉token数量与空间对应关系的不平衡，导致蒸馏信号失真。

Method: 1) 先计算师生视觉logits的曼哈顿距离，并用匈牙利算法在空间维度进行一对一匹配与对齐；2) 在对齐后进行两种蒸馏：a) VLAD：计算文本token与对齐视觉token的亲和矩阵，最小化师生亲和矩阵的smooth L1距离；b) VSD：利用最终层视觉logits在词表空间的离散分布，采用反向KL散度进行蒸馏。

Result: 在多种基准上，采用EM-KD训练的模型在准确率与效率上均显著优于以往高效MLLM；在为公平比较而统一使用提出的视觉token匹配策略后，EM-KD仍优于其他蒸馏方法。

Conclusion: 通过显式对齐师生视觉token并联合进行亲和与语义层次的蒸馏，可在保持高效率的同时显著提升高效MLLM的多模态理解表现。

Abstract: Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.

</details>


### [52] [FaithFusion: Harmonizing Reconstruction and Generation via Pixel-wise Information Gain](https://arxiv.org/abs/2511.21113)
*YuAn Wang,Xiaofan Li,Chi Huang,Wenhao Zhang,Hao Li,Bosheng Wang,Xun Sun,Jun Wang*

Main category: cs.CV

TL;DR: FaithFusion 将3D高斯泼洒(3DGS)与扩散模型通过逐像素的期望信息增益(EIG)结合，作为统一决策来在大视角偏移下既保几何又提外观。EIG指导扩散只在高不确定区域编辑，并把像素级权重蒸馏回3DGS，无需额外先验或结构改动，在Waymo上取得SOTA（NTA/NTL-IoU与FID优）。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重建/生成在大视角变换下要兼顾几何一致性与逼真外观很难。几何导向的3DGS与外观导向的扩散模型难以有效融合，缺乏逐像素、3D一致的编辑准则，导致过度修复与几何漂移。

Method: 提出FaithFusion：以逐像素期望信息增益(EIG)为核心。EIG作为空间先验引导扩散在高不确定区域进行修复；同时将像素级权重用于将扩散编辑蒸馏回3DGS，实现跨时间与空间一致的融合。该方法为即插即用，无需额外先验条件与结构修改。

Result: 在Waymo数据集上，方法在NTA-IoU、NTL-IoU、FID均达到SOTA；即使在6米车道横移条件下，FID仍为107.47，显示强鲁棒性与一致性。

Conclusion: EIG驱动的像素级策略可在3DGS与扩散模型间建立统一、稳定的融合机制，避免过修与几何漂移，显著提升大视角位姿变化下的几何保真与外观质量，且具有即插即用与通用性。

Abstract: In controllable driving-scene reconstruction and 3D scene generation, maintaining geometric fidelity while synthesizing visually plausible appearance under large viewpoint shifts is crucial. However, effective fusion of geometry-based 3DGS and appearance-driven diffusion models faces inherent challenges, as the absence of pixel-wise, 3D-consistent editing criteria often leads to over-restoration and geometric drift. To address these issues, we introduce \textbf{FaithFusion}, a 3DGS-diffusion fusion framework driven by pixel-wise Expected Information Gain (EIG). EIG acts as a unified policy for coherent spatio-temporal synthesis: it guides diffusion as a spatial prior to refine high-uncertainty regions, while its pixel-level weighting distills the edits back into 3DGS. The resulting plug-and-play system is free from extra prior conditions and structural modifications.Extensive experiments on the Waymo dataset demonstrate that our approach attains SOTA performance across NTA-IoU, NTL-IoU, and FID, maintaining an FID of 107.47 even at 6 meters lane shift. Our code is available at https://github.com/wangyuanbiubiubiu/FaithFusion.

</details>


### [53] [Deformation-aware Temporal Generation for Early Prediction of Alzheimers Disease](https://arxiv.org/abs/2511.21114)
*Xin Honga,Jie Lin,Minghui Wang*

Main category: cs.CV

TL;DR: 提出一种名为DATGN的变形感知时序生成网络，自动学习AD进程中的脑形变，先插补缺失MRI序列，再在双向时序变形约束下生成未来MRI，既提升合成图像质量（PSNR/MMSE），又作为数据增强显著提高多种分类器在AD相关分类任务上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有AD早期预测多依赖人工提取形态特征，难以捕捉进程性脑萎缩的时序形变，且临床MRI时序常有缺失，影响建模与预测效果。需要一种能自动学习疾病进展相关形变、处理缺失序列并生成符合病程趋势的未来影像的方法，以辅助早期诊断与分类。

Method: 提出DATGN：1) 对不完整的MRI时间序列进行插补；2) 设计双向时序变形感知模块，利用前后时序信息与形变场约束，生成符合AD进展（脑萎缩趋势）的未来MRI；3) 以生成的序列作为数据增强，配合SVM、CNN、3DCNN进行分类；以PSNR、MMSE评估生成质量。

Result: 在ADNI数据集上，DATGN在PSNR和MMSE等指标上竞争力强；将合成数据加入后，分类性能显著提升：AD vs. NC提升约6.21%–16%；AD vs. MCI vs. NC提升约7.34%–21.25%。可视化显示生成图像与AD脑萎缩趋势一致。

Conclusion: DATGN能在缺失时序下学习并生成符合疾病进展的未来MRI，用于早期预测与数据增强，提升生成质量与下游分类准确率，具备临床辅助潜力。

Abstract: Alzheimer's disease (AD), a degenerative brain condition, can benefit from early prediction to slow its progression. As the disease progresses, patients typically undergo brain atrophy. Current prediction methods for Alzheimers disease largely involve analyzing morphological changes in brain images through manual feature extraction. This paper proposes a novel method, the Deformation-Aware Temporal Generative Network (DATGN), to automate the learning of morphological changes in brain images about disease progression for early prediction. Given the common occurrence of missing data in the temporal sequences of MRI images, DATGN initially interpolates incomplete sequences. Subsequently, a bidirectional temporal deformation-aware module guides the network in generating future MRI images that adhere to the disease's progression, facilitating early prediction of Alzheimer's disease. DATGN was tested for the generation of temporal sequences of future MRI images using the ADNI dataset, and the experimental results are competitive in terms of PSNR and MMSE image quality metrics. Furthermore, when DATGN-generated synthetic data was integrated into the SVM vs. CNN vs. 3DCNN-based classification methods, significant improvements were achieved from 6. 21\% to 16\% in AD vs. NC classification accuracy and from 7. 34\% to 21. 25\% in AD vs. MCI vs. NC classification accuracy. The qualitative visualization results indicate that DATGN produces MRI images consistent with the brain atrophy trend in Alzheimer's disease, enabling early disease prediction.

</details>


### [54] [Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models](https://arxiv.org/abs/2511.21122)
*Changlin Li,Jiawei Zhang,Zeyi Shi,Zongxin Yang,Zhihui Li,Xiaojun Chang*

Main category: cs.CV

TL;DR: 提出EntPruner：一种面向扩散与流模型的熵引导、自动化、渐进式剪枝框架，在保持生成质量的同时最高实现2.22×推理加速。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉生成模型迁移到下游任务时存在显著参数冗余；生成模型需兼顾多样性与条件一致性，传统判别式剪枝指标不适用，且一次性剪枝易导致模式崩塌与性能退化。

Method: 1) 熵引导剪枝：提出基于数据依赖的条件熵偏差(CED)在块级评估重要性，优先剪除对条件分布偏离小的模块；2) 零样本自适应渐进剪枝：在训练过程中动态决定“何时/剪多少”，避免一刀切剪枝带来的不稳定与塌缩；在DiT、SiT等生成骨干上应用。

Result: 在ImageNet及三个下游数据集上验证，相比未剪模型在基本保持生成质量(多样性与条件忠实度)的前提下，实现最高2.22×的推理加速。

Conclusion: EntPruner能在保持生成分布质量的同时显著减少冗余并加速推理；CED为生成模型提供了有效的块级重要性度量，动态零样本剪枝策略优于一-shot剪枝，适用于扩散与流模型的迁移场景。

Abstract: Large-scale vision generative models, including diffusion and flow models, have demonstrated remarkable performance in visual generation tasks. However, transferring these pre-trained models to downstream tasks often results in significant parameter redundancy. In this paper, we propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. First, we introduce entropy-guided pruning, a block-level importance assessment strategy specifically designed for generative models. Unlike discriminative models, generative models require preserving the diversity and condition-fidelity of the output distribution. As the importance of each module can vary significantly across downstream tasks, EntPruner prioritizes pruning of less important blocks using data-dependent Conditional Entropy Deviation (CED) as a guiding metric. CED quantifies how much the distribution diverges from the learned conditional data distribution after removing a block. Second, we propose a zero-shot adaptive pruning framework to automatically determine when and how much to prune during training. This dynamic strategy avoids the pitfalls of one-shot pruning, mitigating mode collapse, and preserving model performance. Extensive experiments on DiT and SiT models demonstrate the effectiveness of EntPruner, achieving up to 2.22$\times$ inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.

</details>


### [55] [CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion](https://arxiv.org/abs/2511.21129)
*Dianbing Xi,Jiepeng Wang,Yuanzhi Liang,Xi Qiu,Jialun Liu,Hao Pan,Yuchi Huo,Rui Wang,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出一个统一扩散框架CtrlVDiff，结合多种图形学与几何模态（深度、法线、边缘、分割、内在属性如反照率/粗糙度/金属度）来同时实现视频理解与可控视频生成；并配套构建多模态、时序对齐的数据集MMVideo。模型能在任意模态子集输入下保持时序一致，支持重光照、材质调整、对象插入等分层编辑，性能优于SOTA且对缺失模态鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖几何线索（深度、边缘）的控制方法只能约束布局，难以明确外观、材质与光照，导致物理不一致的编辑和时间漂移；需要引入更丰富、互补的图形学模态来消除歧义并实现精确可控的生成。同时，一个模型要兼容多模态子集并保持时序一致，且缺乏大规模、时序对齐的真实视频与逐像素多模态标注数据，是两大瓶颈。

Method: 提出CtrlVDiff：统一视频扩散模型，采用混合模态控制策略（HMCS）对多模态特征进行路由与融合（深度、法线、分割、边缘、内在属性：反照率/粗糙度/金属度），在缺失任意模态时仍能稳健注入控制信号并维持时间一致性。为训练构建MMVideo数据集，将真实与合成视频在像素级多模态与文本描述上对齐，用于监督理解与生成任务。

Result: 在视频理解与生成基准上取得更高的可控性与保真度，能够进行分层编辑（重光照、材质调节、对象插入等），并在部分模态缺失时保持鲁棒与强时序一致性，整体优于现有SOTA基线。

Conclusion: 多模态（含图形学内在属性）的联合控制比仅几何线索更有效，可在统一扩散框架下实现精确、可预测、物理一致的可控视频生成与理解；HMCS与MMVideo共同解决了架构与数据两大难题，使模型在任意模态子集下稳定工作并提供高质量时序一致的编辑。

Abstract: We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.
  However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.
  We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.

</details>


### [56] [DeepRFTv2: Kernel-level Learning for Image Deblurring](https://arxiv.org/abs/2511.21132)
*Xintian Mao,Haofei Song,Yin-Nian Liu,Qingli Li,Yan Wang*

Main category: cs.CV

TL;DR: 作者提出在傅里叶域估计模糊核（FKE），把空间卷积转为频域乘法，与去模糊网络联合训练，在无需额外监督、低复杂度下学习核级模糊过程；并以特征而非原始图像参与“卷积”，结合可逆的解耦多尺度子UNet架构，实现高效特征提取与SOTA运动去模糊；估计的核具有物理可解释性，方法可推广到其他核相关任务。


<details>
  <summary>Details</summary>
Motivation: 像素级端到端或阶段式伪核学习难以让模型真正理解模糊本质（由清晰图像与核的卷积导致）。若能在核级别直接学习模糊过程，去模糊性能可显著提升，同时需控制计算复杂度并避免额外监督。

Method: 提出Fourier Kernel Estimator：在傅里叶空间进行激活，将空间域的卷积转为频域乘法；与去模糊主干联合优化，无需核真值监督。将“卷积对象”从原图改为网络提取的特征，以利用更丰富的语义与结构信息学习模糊核。为高效特征提取，设计可逆的解耦多尺度架构，包含多层级子UNet，实现更好多尺度编码解码并降低训练显存。

Result: 在单幅运动去模糊任务上达到SOTA表现；核估计器学习到物理上有意义的核形状；展现对其他与核相关问题的潜在适用性。

Conclusion: 频域核估计与特征级卷积的联合框架能以低复杂度、无额外监督地学习核级模糊过程，提升去模糊效果；可逆多尺度设计进一步提高效率与效果，方法具备物理可解释性与可推广性。

Abstract: It is well-known that if a network aims to learn how to deblur, it should understand the blur process. Blurring is naturally caused by the convolution of the sharp image with the blur kernel. Thus, allowing the network to learn the blur process in the kernel-level can significantly improve the image deblurring performance. But, current deep networks are still at the pixel-level learning stage, either performing end-to-end pixel-level restoration or stage-wise pseudo kernel-level restoration, failing to enable the deblur model to understand the essence of the blur. To this end, we propose Fourier Kernel Estimator (FKE), which considers the activation operation in Fourier space and converts the convolution problem in the spatial domain to a multiplication problem in Fourier space. Our FKE, jointly optimized with the deblur model, enables the network to learn the kernel-level blur process with low complexity and without any additional supervision. Furthermore, we change the convolution object of the kernel from ``image" to network extracted ``feature", whose rich semantic and structural information is more suitable to blur process learning. With the convolution of the feature and the estimated kernel, our model can learn the essence of blur in kernel-level. To further improve the efficiency of feature extraction, we design a decoupled multi-scale architecture with multiple hierarchical sub-unets with a reversible strategy, which allows better multi-scale encoding and decoding in low training memory. Extensive experiments indicate that our method achieves state-of-the-art motion deblurring results and show potential for handling other kernel-related problems. Analysis also shows our kernel estimator is able to learn physically meaningful kernels. The code will be available at https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur.

</details>


### [57] [Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning](https://arxiv.org/abs/2511.21136)
*Changlin Li,Jiawei Zhang,Shuhao Liu,Sihao Lin,Zeyi Shi,Zhihui Li,Xiaojun Chang*

Main category: cs.CV

TL;DR: 提出Ent-Prog训练框架，通过基于条件熵的重要性评估与自适应渐进式日程，优先训练关键组件、逐步提升复杂度，在不降生成质量下，把人类视频扩散模型的训练加速至2.2×、显存降至2.4×。


<details>
  <summary>Details</summary>
Motivation: 人类视频生成需在高分辨率、多帧条件下训练扩散模型，训练代价与显存开销极高，阻碍大规模与快速迭代；需要一种能在保证性能下显著降低成本的训练策略。

Method: 1) 条件熵膨胀(CEI)：度量模型各组件在目标条件生成任务中的重要性，以此排序并优先训练关键模块；2) 自适应渐进式日程：根据“收敛效率”动态提升训练复杂度（如分辨率/时长/网络深度或噪声步数），将计算集中在最有效阶段。整体为一种优先级+渐进的高效训练流程。

Result: 在三个数据集上，Ent-Prog在不牺牲生成质量的前提下，实现最高2.2×的训练加速与2.4×的GPU显存节省。

Conclusion: 通过熵引导的优先训练与自适应渐进策略，可显著降低人类视频扩散模型训练的时间与显存成本，同时保持生成性能，对大规模与资源受限场景具有实用价值。

Abstract: Human video generation has advanced rapidly with the development of diffusion models, but the high computational cost and substantial memory consumption associated with training these models on high-resolution, multi-frame data pose significant challenges. In this paper, we propose Entropy-Guided Prioritized Progressive Learning (Ent-Prog), an efficient training framework tailored for diffusion models on human video generation. First, we introduce Conditional Entropy Inflation (CEI) to assess the importance of different model components on the target conditional generation task, enabling prioritized training of the most critical components. Second, we introduce an adaptive progressive schedule that adaptively increases computational complexity during training by measuring the convergence efficiency. Ent-Prog reduces both training time and GPU memory consumption while maintaining model performance. Extensive experiments across three datasets, demonstrate the effectiveness of Ent-Prog, achieving up to 2.2$\times$ training speedup and 2.4$\times$ GPU memory reduction without compromising generative performance.

</details>


### [58] [Referring Video Object Segmentation with Cross-Modality Proxy Queries](https://arxiv.org/abs/2511.21139)
*Baoli Sun,Xinzhu Ma,Ning Wang,Zhihui Wang,Zhiyong Wang*

Main category: cs.CV

TL;DR: 提出ProxyFormer用于指代引导的视频目标分割，通过“代理查询”跨模态对齐并逐阶段传播以稳健跟踪目标，解耦时空交互并配合联合语义一致性训练，在四个基准上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件查询的Transformer方法存在两点不足：缺乏跨帧依赖与外观变化建模，导致大幅帧间变化时跟踪不稳；文本约束介入过晚，视频特征易偏向非指代目标。因此需要一种更早更紧密的跨模态融合与能随时间演化的机制。

Method: 提出ProxyFormer：1) 引入一组“代理查询”（proxy queries）作为视觉-文本语义的桥梁，在视频特征编码器的多阶段中逐步更新与传播，持续将注意力引向被指代对象并建立跨帧依赖；2) 将跨模态交互解耦为时间和空间两个维度以降低计算开销；3) 设计联合语义一致性（JSC）训练策略，使代理查询与视频-文本对的语义达成一致。

Result: 在四个主流RVOS基准上进行大量实验，结果显示ProxyFormer在性能上全面优于现有最先进方法。

Conclusion: 通过代理查询的逐阶段演化与时空解耦的跨模态交互，再配合JSC训练，模型实现了更准确连贯的跨帧指代分割，并以较低成本取得SOTA表现。

Abstract: Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.

</details>


### [59] [TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models](https://arxiv.org/abs/2511.21145)
*Jiaming He,Guanyu Hou,Hongwei Li,Zhicong Huang,Kangjie Chen,Yi Yu,Wenbo Jiang,Guowen Xu,Tianwei Zhang*

Main category: cs.CV

TL;DR: TEAR 提出一种面向时间维度的自动化红队框架，用“无害”文本提示利用视频时序动态诱发T2V模型违规输出，攻破率超80%。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估多针对静态图像/文本，无法捕捉视频生成中的时序复杂性，导致T2V模型在动态序列层面存在未被发现的安全风险。

Method: 构建TEAR框架：1) 设计时间感知测试生成器，采用两阶段优化——先离线训练生成器，再进行时间感知在线偏好学习；2) 生成看似无害但在时序组合上具诱导性的提示词，引发策略违规视频；3) 引入循环式refine模型提高提示的隐蔽性与对抗效果。

Result: 在多种开源与商用T2V系统上取得>80%的攻击成功率，较既有最佳57%显著提升。

Conclusion: 面向时序的自动化红队方法能有效揭示T2V模型的动态安全漏洞，提示需在视频生成安全防护中纳入时序维度与在线偏好对抗学习等机制。

Abstract: Text-to-Video (T2V) models are capable of synthesizing high-quality, temporally coherent dynamic video content, but the diverse generation also inherently introduces critical safety challenges. Existing safety evaluation methods,which focus on static image and text generation, are insufficient to capture the complex temporal dynamics in video generation. To address this, we propose a TEmporal-aware Automated Red-teaming framework, named TEAR, an automated framework designed to uncover safety risks specifically linked to the dynamic temporal sequencing of T2V models. TEAR employs a temporal-aware test generator optimized via a two-stage approach: initial generator training and temporal-aware online preference learning, to craft textually innocuous prompts that exploit temporal dynamics to elicit policy-violating video output. And a refine model is adopted to improve the prompt stealthiness and adversarial effectiveness cyclically. Extensive experimental evaluation demonstrates the effectiveness of TEAR across open-source and commercial T2V systems with over 80% attack success rate, a significant boost from prior best result of 57%.

</details>


### [60] [LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs](https://arxiv.org/abs/2511.21150)
*Shichu Sun,Yichen Zhang,Haolin Song,Zonghao Guo,Chi Chen,Yidan Zhang,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.CV

TL;DR: 提出LLaVA-UHD v3，通过在ViT中引入渐进式视觉压缩（PVC），在保持原生高分辨率优势的同时显著降低计算与TTFT；在同等MLLM框架下，与MoonViT/Qwen2-VL性能相当但更快。


<details>
  <summary>Details</summary>
Motivation: 全局原生分辨率编码在多模态大模型中效果更好，但计算开销高、TTFT长；需要一种既保留全局高分辨率理解力又高效的视觉编码方案。

Method: 提出PVC，可无缝集成到标准ViT：1) 精炼Patch Embedding，支持可变patch尺寸以实现细粒度建模与自适应下采样；2) 窗口化Token压缩，在ViT层间分层部署，逐步聚合局部token表征。将大规模预训练ViT重构为高效的ViT-UHD，并在同一MLLM骨架下评估。

Result: 在广泛基准上，ViT-UHD在与MoonViT相当的性能下，将TTFT降低2.4倍；基于ViT-UHD构建的LLaVA-UHD v3达到与Qwen2-VL相当的表现，同时进一步将TTFT降低1.9倍。

Conclusion: PVC使原生分辨率全局编码更高效，在不显著牺牲泛化能力的情况下大幅提升推理启动速度；LLaVA-UHD v3验证了该路径的有效性，代码与模型将开源以促进高效MLLM研究。

Abstract: Visual encoding followed by token condensing has become the standard architectural paradigm in multi-modal large language models (MLLMs). Many recent MLLMs increasingly favor global native- resolution visual encoding over slice-based methods. To investigate this trend, we systematically compare their behavior on vision-language understanding and attention patterns, revealing that global encoding enhances overall capability but at the expense of greater computational overhead. To address this issue, we present LLaVA-UHD v3, an MLLM centered upon our proposed Progressive Visual Compression (PVC) method, which can be seamlessly integrated into standard Vision Transformer (ViT) to enable efficient native-resolution encoding. The PVC approach consists of two key modules: (i) refined patch embedding, which supports flexible patch-size scaling for fine-grained visual model- ing, (ii) windowed token compression, hierarchically deployed across ViT layers to progressively aggregate local token representations. Jointly modulated by these two modules, a widely pretrained ViT can be reconfigured into an efficient architecture while largely preserving generality. Evaluated across extensive benchmarks, the transformed ViT, termed ViT-UHD, demonstrates competitive performance with MoonViT while reducing TTFT (time-to-first-token) by 2.4x, when developed within an identical MLLM architecture. Building upon ViT-UHD, LLaVA-UHD v3 also achieves competitive performance to Qwen2-VL, while further reducing TTFT by 1.9x. We will release all code and checkpoints to support future research on efficient MLLMs.

</details>


### [61] [Progress by Pieces: Test-Time Scaling for Autoregressive Image Generation](https://arxiv.org/abs/2511.21185)
*Joonhyung Park,Hyeongwon Jang,Joowon Kim,Eunho Yang*

Main category: cs.CV

TL;DR: GridAR是一种用于视觉自回归(T2I)模型的测试时扩展框架，通过网格化渐进生成与布局引导，在较小N下取得优于Best-of-N的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM式测试时计算扩展(如Best-of-N)直接用于视觉AR存在两难：1) 计算浪费在错误轨迹上，2) 光栅扫描缺乏全局画布蓝图，导致仅少量候选与提示对齐、扩展收益有限。需要一种能早停劣解、提供全局指导并高效利用预算的方法。

Method: 提出GridAR：1) 网格划分的渐进式生成。对同一位置并行生成多个部分候选，早期剪枝不可行候选，将可行者固定为“锚点”以引导后续解码；2) 布局指定的提示重写。通过检查部分视图推断满足文本的可行布局，重写提示以弥补蓝图缺失，指导后续生成。

Result: 在有限测试时扩展下获得更高质量：N=4时在T2I-CompBench++上比Best-of-N(N=8)高14.4%，同时成本降25.6%；在自回归图像编辑上，编辑质量可比并在PIE-Bench上语义保真度相对更高(+13.9%)。

Conclusion: GridAR通过网格化多候选-早剪枝-锚点引导与布局感知提示重写，有效克服视觉AR测试时扩展的低效与缺蓝图问题，在生成与编辑任务中以更低预算实现更优或相当性能。

Abstract: Recent visual autoregressive (AR) models have shown promising capabilities in text-to-image generation, operating in a manner similar to large language models. While test-time computation scaling has brought remarkable success in enabling reasoning-enhanced outputs for challenging natural language tasks, its adaptation to visual AR models remains unexplored and poses unique challenges. Naively applying test-time scaling strategies such as Best-of-N can be suboptimal: they consume full-length computation on erroneous generation trajectories, while the raster-scan decoding scheme lacks a blueprint of the entire canvas, limiting scaling benefits as only a few prompt-aligned candidates are generated. To address these, we introduce GridAR, a test-time scaling framework designed to elicit the best possible results from visual AR models. GridAR employs a grid-partitioned progressive generation scheme in which multiple partial candidates for the same position are generated within a canvas, infeasible ones are pruned early, and viable ones are fixed as anchors to guide subsequent decoding. Coupled with this, we present a layout-specified prompt reformulation strategy that inspects partial views to infer a feasible layout for satisfying the prompt. The reformulated prompt then guides subsequent image generation to mitigate the blueprint deficiency. Together, GridAR achieves higher-quality results under limited test-time scaling: with N=4, it even outperforms Best-of-N (N=8) by 14.4% on T2I-CompBench++ while reducing cost by 25.6%. It also generalizes to autoregressive image editing, showing comparable edit quality and a 13.9% gain in semantic preservation on PIE-Bench over larger-N baselines.

</details>


### [62] [AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning](https://arxiv.org/abs/2511.21188)
*Zheng Li,Yibing Song,Xin Zhang,Lei Luo,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: 提出AnchorOPT：一种为CLIP提示学习引入“动态锚”的方法，通过可学习锚值与可学习位置矩阵，自适应不同任务与训练阶段，从而提升零/少样本泛化；两阶段训练，效果优于或匹配更复杂方法，且可无缝集成现有框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的提示学习多用静态文本单词（如“shape”“color”）作为锚点来引导软提示，但这些锚在取值与相对位置上都是固定的，缺乏跨任务、跨阶段的自适应性，限制了泛化与性能。

Method: 提出AnchorOPT：1) 动态锚值：不再手工指定显式文本词，而是从任务数据中学习锚向量；2) 动态位置：引入可学习的位置矩阵，学习锚与软提示之间的相对位置关系，并对训练阶段与任务上下文自适应；3) 两阶段训练：先学习锚向量，再冻结锚，优化软提示与位置矩阵；4) 以即插即用的模块融入现有CLIP提示学习框架。

Result: 在多数据集上，使用仅由可学习锚和位置矩阵组成的简单模块即可达到或超过包含额外可学习模块或正则化技巧的方法；在多种框架中带来稳定增益。

Conclusion: 动态锚与动态位置建模能在不增加复杂结构的情况下显著提升CLIP提示学习的适应性与泛化，AnchorOPT作为可插拔模块具有良好实用性与兼容性。

Abstract: Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., "shape", "color"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.

</details>


### [63] [Scenes as Tokens: Multi-Scale Normal Distributions Transform Tokenizer for General 3D Vision-Language Understanding](https://arxiv.org/abs/2511.21191)
*Yutao Tang,Cheng Zhao,Gaurav Mittal,Rohith Kukkala,Rama Chellappa,Cheng Peng,Mei Chen*

Main category: cs.CV

TL;DR: 提出NDTokenizer3D：以多尺度NDT表示和解码器为核心的通用3D视觉语言模型，能将点云压缩为可供LLM消费的“场景token”，并统一支持交互式提示与多种3D理解任务，显著提升3D指代分割、3D VQA与3D致密描述性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D VLM虽具潜力，但缺乏有效的“场景token化”方法，难以在保持全局语义与局部几何细节的同时，为多任务与人机交互提供统一接口。需要一种既能高效压缩高分辨率点云，又能被LLM直接利用、并自然支持交互提示的通用表示。

Method: 提出三阶段场景token化流水线：1) 从高分辨率点云构建多尺度NDT表示，保存全局上下文与细粒度几何；2) 通过多尺度NDT解码器（MSDec）逐级融合跨尺度特征，生成可消费的“整体场景token”；3) 复用MSDec作为通用接口，支持人机交互提示（点/框/掩码）与分割掩码解码，实现统一的多任务框架。

Result: 在多项3D场景理解基准上取得显著提升，尤其是在3D指代分割、3D视觉问答与3D致密描述任务上表现突出，展示出更细粒度与更通用的能力。

Conclusion: 多尺度NDT表示+MSDec实现了紧凑统一的3D场景token化与任务接口，顺畅连接语言层推理与3D空间理解，为通用3D VLM提供有效范式并带来显著性能提升。

Abstract: Recent advances in 3D vision-language models (VLMs) highlight a strong potential for 3D scene understanding and reasoning. However, effectively tokenizing 3D scenes into holistic scene tokens, and leveraging these tokens across diverse 3D understanding tasks, remain highly challenging. We present NDTokenizer3D, a generalist 3D VLM that performs a wide range of 3D scene understanding tasks while naturally supporting human interactions, thereby bridging language-level reasoning with 3D spatial understanding. The core of our approach is a novel three-stage scene tokenization pipeline built upon a Multi-Scale Normal Distributions Transform (NDT) representation, paired with a Multi-Scale NDT Decoder (MSDec). Specifically, NDTokenizer3D first constructs a multi-scale NDT representation from raw high-resolution point clouds, preserving both global context and fine-grained geometric details. Next, the MSDec progressively fuses cross-scale NDT features, producing holistic scene tokens consumable by LLM endpoints. Beyond tokenization, MSDec is repurposed as a general interface for human-interactive prompting (points, boxes, masks) and segmentation-mask decoding, unifying diverse 3D scene understanding tasks within a single architecture. With this compact and unified design, NDTokenizer3D offers a fine-grained, general-purpose 3D VLM, achieving remarkable improvements in 3D Referring Segmentation, 3D Visual Question Answering, and 3D Dense Captioning.

</details>


### [64] [When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21192)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Qixin Zhang,Bingquan Shen,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 提出UPA-RFAS，一种可物理打印的通用对抗贴片方法，能在不同VLA架构、微调变体与仿真到现实迁移中实现强可迁移黑箱攻击；其通过特征、注意力与语义三方面联合优化，显著提升跨模型与跨任务攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有对VLA的对抗补丁多对单一模型过拟合，黑箱与跨模型/跨域可迁移性差，尤其在未知架构、微调差异与仿真-现实落差下失效；缺乏系统性的通用物理攻击框架与评测基线。

Method: 提出UPA-RFAS统一框架，学习单一物理贴片于共享特征空间中优化以增强迁移：1) 特征空间目标：L1偏移先验+排斥式InfoNCE，推动可迁移表示偏移；2) 鲁棒性增强的两阶段min-max：内环对每样本学习不可见微扰以“加硬”邻域，外环在此困难分布上优化通用贴片；3) 面向VLA的两类损失：贴片注意力主导(Patch Attention Dominance)劫持文本→视觉注意力；贴片语义失配(Patch Semantic Misalignment)在无标签下诱导图文错配。

Result: 在多种VLA模型、操作任务套件与真实物理平台上，单一贴片在跨模型、跨任务与多视角条件下稳定迁移，黑箱设定下依然显著降低任务成功率，揭示贴片攻击的现实威胁面。

Conclusion: UPA-RFAS为VLA系统提供了强有力的通用可迁移物理攻击基线，显示了从特征、注意力与语义联合优化的有效性，并为未来防御研究提供了清晰对照与挑战。

Abstract: Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.

</details>


### [65] [You Can Trust Your Clustering Model: A Parameter-free Self-Boosting Plug-in for Deep Clustering](https://arxiv.org/abs/2511.21193)
*Hanyang Li,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出DCBoost，一个无参数的插件，通过利用局部结构的高置信样本，提升深度聚类模型的全局特征分离度；在多数据集显著提升SOTA表现与轮廓系数。


<details>
  <summary>Details</summary>
Motivation: 现有深度聚类局部结构（类内紧致、一致）与全局结构（类间界限缠绕、分离差）不匹配，导致聚类边界不清晰、性能受限；需要一种方法把可靠的局部结构线索转化为改进全局特征布局的监督信号。

Method: 提出DCBoost作为无参数可插拔模块：1) 自适应k近邻一致性过滤，选出高置信样本作为锚点（具备高标签可靠性）；2) 基于这些样本构造判别性损失，联合鼓励类内紧凑与类间可分；3) 将该损失作为自监督信号，优化原有深度聚类网络的全局特征结构。

Result: 在多项基准上，DCBoost为多种现有深度聚类模型带来显著增益；相较当前SOTA（如ProPos）提升超过3%，轮廓系数提升超过7倍。代码已开源。

Conclusion: 利用局部结构的高置信样本作为自监督锚，能有效修正全局特征的缠绕与分离不足；DCBoost简单、无参数、易集成，能普遍提升深度聚类模型的性能与可分性。

Abstract: Recent deep clustering models have produced impressive clustering performance. However, a common issue with existing methods is the disparity between global and local feature structures. While local structures typically show strong consistency and compactness within class samples, global features often present intertwined boundaries and poorly separated clusters. Motivated by this observation, we propose DCBoost, a parameter-free plug-in designed to enhance the global feature structures of current deep clustering models. By harnessing reliable local structural cues, our method aims to elevate clustering performance effectively. Specifically, we first identify high-confidence samples through adaptive $k$-nearest neighbors-based consistency filtering, aiming to select a sufficient number of samples with high label reliability to serve as trustworthy anchors for self-supervision. Subsequently, these samples are utilized to compute a discriminative loss, which promotes both intra-class compactness and inter-class separability, to guide network optimization. Extensive experiments across various benchmark datasets showcase that our DCBoost significantly improves the clustering performance of diverse existing deep clustering models. Notably, our method improves the performance of current state-of-the-art baselines (e.g., ProPos) by more than 3% and amplifies the silhouette coefficient by over $7\times$. Code is available at <https://github.com/l-h-y168/DCBoost>.

</details>


### [66] [BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data](https://arxiv.org/abs/2511.21194)
*Selene Cerna,Sara Si-Moussi,Wilfried Thuiller,Hadrien Hendrikx,Vincent Miele*

Main category: cs.CV

TL;DR: 提出BotaCLIP：在不重训的前提下，用轻量对比学习将遥感基础模型（DOFA）的影像表征与植物样方记录对齐，作为可迁移表征用于生态任务，显著优于原模型与监督基线。


<details>
  <summary>Details</summary>
Motivation: 基础模型表征强但缺少特定领域知识；生态与生物多样性任务常数据稀缺、标注昂贵，需在低计算开销下将专家/领域结构注入预训练模型以提升下游表现。

Method: 构建轻量多模态对比学习框架BotaCLIP：以高分辨率航片与植物relevé作为正负对比对；在DOFA预训练表征上进行适配；加入正则化以缓解灾难性遗忘，使模型在学习生态结构时保留原有通用能力；训练后输出的嵌入作为下游预测器输入。

Result: 在三个生态任务（植物出现预测、蝴蝶出现建模、土壤营养级群丰度估计）上，BotaCLIP产生的表征相较DOFA与监督学习基线均有一致提升。

Conclusion: 领域感知的适配可在数据稀缺场景中以低成本将专家知识注入基础模型，得到更具生态结构的可迁移表征，从而实现“节俭”的表示学习。

Abstract: Foundation models have demonstrated a remarkable ability to learn rich, transferable representations across diverse modalities such as images, text, and audio. In modern machine learning pipelines, these representations often replace raw data as the primary input for downstream tasks. In this paper, we address the challenge of adapting a pre-trained foundation model to inject domain-specific knowledge, without retraining from scratch or incurring significant computational costs. To this end, we introduce BotaCLIP, a lightweight multimodal contrastive framework that adapts a pre-trained Earth Observation foundation model (DOFA) by aligning high-resolution aerial imagery with botanical relevés. Unlike generic embeddings, BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting. Once trained, the resulting embeddings serve as transferable representations for downstream predictors. Motivated by real-world applications in biodiversity modeling, we evaluated BotaCLIP representations in three ecological tasks: plant presence prediction, butterfly occurrence modeling, and soil trophic group abundance estimation. The results showed consistent improvements over those derived from DOFA and supervised baselines. More broadly, this work illustrates how domain-aware adaptation of foundation models can inject expert knowledge into data-scarce settings, enabling frugal representation learning.

</details>


### [67] [Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition](https://arxiv.org/abs/2511.21202)
*Baoli Sun,Yihan Wang,Xinzhu Ma,Zhihui Wang,Kun Lu,Zhiyong Wang*

Main category: cs.CV

TL;DR: 论文提出ART框架，通过查询-响应机制在视频中发现并跟踪区分性的细粒度动作局部区域，显著提升细粒度动作识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法多捕捉粗粒度的整体运动模式，难以定位并利用随时间演化的关键局部细节，导致相似动作难以区分。

Method: 引入Action-Region Tracking（ART）：1）区域语义激活模块以文本约束的判别语义作为查询，在每帧视频中捕获最相关的动作区域响应，并与时空视频特征交互；2）将区域响应组织为“动作轨迹片段”（tracklets），在帧间关联相应区域以表征区域级动作动态；3）多层次轨迹对比约束，同时在空间（同帧区分）与时间（相邻帧关联）层面优化区域响应；4）任务特定微调机制对VLM语言分支提取的标签文本语义进行细化，既保留预训练语义又贴合任务需求。

Result: 在多项主流动作识别基准上，相比现有SOTA取得全面优于的性能，证明ART在识别细粒度动作方面有效。

Conclusion: 通过文本引导的区域查询与跨帧跟踪，并辅以多层次对比学习和任务化语义微调，ART能捕获并利用细粒度的局部动态，从而有效区分相似动作、提升FGAR表现。

Abstract: Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories. However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time. In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions. Specifically, we propose a region-specific semantic activation module that employs discriminative and text-constrained semantics as queries to capture the most action-related region responses in each video frame, facilitating interaction among spatial and temporal dimensions with corresponding video features. The captured region responses are organized into action tracklets, which characterize region-based action dynamics by linking related responses across video frames in a coherent sequence. The text-constrained queries encode nuanced semantic representations derived from textual descriptions of action labels extracted by language branches within Visual Language Models (VLMs). To optimize the action tracklets, we design a multi-level tracklet contrastive constraint among region responses at spatial and temporal levels, enabling effective discrimination within each frame and correlation between adjacent frames. Additionally, a task-specific fine-tuning mechanism refines textual semantics such that semantic representations encoded by VLMs are preserved while optimized for task preferences. Comprehensive experiments on widely used action recognition benchmarks demonstrate the superiority to previous state-of-the-art baselines.

</details>


### [68] [From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting](https://arxiv.org/abs/2511.21215)
*Umang Agarwal,Rudraksh Sangore,Sumit Laddha*

Main category: cs.CV

TL;DR: 对DDPM、CFM与MeanFlow三种生成范式在统一TinyUNet上做对比：CFM以50步达FID 24.15，显著优于DDPM的FID 402.98；MeanFlow单步生成FID 29.15，推理提速约50倍。并将CFM扩展到图像修复，加入掩膜引导与专门微调，PSNR与SSIM大幅提升。


<details>
  <summary>Details</summary>
Motivation: 在生成模型中，扩散与流匹配方法通常需多步采样，速度与质量权衡明显；而新近的一步式方法（如MeanFlow）可极大提速但质量未知。作者希望在统一、轻量的架构与数据设置下，系统比较三者的效率与质量，并探索在下游任务（图像修复）上的可迁移性与训练策略。

Method: - 统一实现：在CIFAR-10上，用<1.5M参数的TinyUNet统一实现DDPM、CFM与MeanFlow。
- 采样与评测：对DDPM与CFM进行多步采样比较，MeanFlow采用单步。主要用FID评估生成质量与速度权衡。
- Inpainting扩展：将CFM扩展到掩膜引导采样，设计四类掩膜（中心、随机框、非规则、半图），并进行面向修复的微调，评估PSNR/SSIM。

Result: - 生成对比：CFM在50步达到FID 24.15，显著优于DDPM的FID 402.98；MeanFlow单步生成FID 29.15，实现约50倍推理提速且保持接近CFM的质量。
- 修复任务：在中心掩膜上，微调后PSNR从4.95提升至8.57（+73%），SSIM从0.289提升至0.418（+45%），其他掩膜类型也显示改进。

Conclusion: CFM在相同步数下优于DDPM，兼顾质量与效率；MeanFlow可在单步生成中取得接近CFM的FID，显著加速推理。将CFM用于掩膜引导图像修复并进行针对性微调，能显著提升重建质量，说明任务感知训练对于条件生成至关重要。

Abstract: We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (<1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.

</details>


### [69] [3-Tracer: A Tri-level Temporal-Aware Framework for Audio Forgery Detection and Localization](https://arxiv.org/abs/2511.21237)
*Shuhan Xia,Xuannan Liu,Xing Cui,Peipei Li*

Main category: cs.CV

TL;DR: 提出T3-Tracer，联合帧/段/整段音频三级分析，利用FA-FAM与SMDAM检测并定位部分音频篡改，在三数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 部分音频篡改只改动少量但语义关键帧，整体听感仍真实，导致检测困难。现有方法多做逐帧独立判别，缺乏跨时间层级的结构来同时捕捉瞬态与持续异常。

Method: 构建三级检测框架T3-Tracer：1) FA-FAM聚合帧级与整段级时序信息，判断每帧真伪，兼顾帧内伪造线索与全局语义不一致；2) SMDAM在段级检测边界，采用双分支多尺度设计，同时建模帧特征与帧间差分，在多时间窗下捕捉突变异常，用于校正与细化帧级结果；联合输出框架、段和整段级的伪造痕迹。

Result: 在三个具有挑战性数据集上进行大量实验，整体性能达到或超过现有最优方法（SOTA）。

Conclusion: 多层级联合建模能更全面地发现部分音频篡改，特别是边界突变与全局不一致；T3-Tracer验证了该思路的有效性并实现SOTA。

Abstract: Recently, partial audio forgery has emerged as a new form of audio manipulation. Attackers selectively modify partial but semantically critical frames while preserving the overall perceptual authenticity, making such forgeries particularly difficult to detect. Existing methods focus on independently detecting whether a single frame is forged, lacking the hierarchical structure to capture both transient and sustained anomalies across different temporal levels. To address these limitations, We identify three key levels relevant to partial audio forgery detection and present T3-Tracer, the first framework that jointly analyzes audio at the frame, segment, and audio levels to comprehensively detect forgery traces. T3-Tracer consists of two complementary core modules: the Frame-Audio Feature Aggregation Module (FA-FAM) and the Segment-level Multi-Scale Discrepancy-Aware Module (SMDAM). FA-FAM is designed to detect the authenticity of each audio frame. It combines both frame-level and audio-level temporal information to detect intra-frame forgery cues and global semantic inconsistencies. To further refine and correct frame detection, we introduce SMDAM to detect forgery boundaries at the segment level. It adopts a dual-branch architecture that jointly models frame features and inter-frame differences across multi-scale temporal windows, effectively identifying abrupt anomalies that appeared on the forged boundaries. Extensive experiments conducted on three challenging datasets demonstrate that our approach achieves state-of-the-art performance.

</details>


### [70] [FIELDS: Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision](https://arxiv.org/abs/2511.21245)
*Chen Ling,Henglin Shi,Hedvig Kjellström*

Main category: cs.CV

TL;DR: 提出FIELDS方法，通过在自监督2D一致性基础上加入真实3D表情参数监督和情感识别辅助分支，缓解2D/3D域差与表情强度偏差，从单张图像重建保留细微情绪线索的高保真3D人脸，并提升野外表情识别表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D人脸重建依赖2D监督且缺少3D真值，难以准确捕捉细微的情感/表情细节，易产生表情强度夸张或偏差，影响情绪相关应用。

Method: 在自监督2D图像一致性框架上，增加两种直接监督：1) 使用来自自发表情的4D面部扫描提取的“真实”3D表情参数对编码器进行引导；2) 设计强度感知的情感损失，通过辅助情感识别分支，约束重建的3D表情参数既能表达情绪又不过度夸张；联合训练以缩小2D/3D域差并抑制强度偏差。

Result: 单张图像即可重建包含丰富情绪线索、表情自然的高保真3D人脸；在不牺牲自然度的情况下，显著提升野外场景的表情识别性能。

Conclusion: 双重监督（真实3D表情参数+强度感知情感分支）有效弥合2D/3D域差并降低强度偏差，使3D重建能保留微妙情感细节，兼顾真实感与识别性能。

Abstract: Facial expressions convey the bulk of emotional information in human communication, yet existing 3D face reconstruction methods often miss subtle affective details due to reliance on 2D supervision and lack of 3D ground truth. We propose FIELDS (Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision) to address these limitations by extending self-supervised 2D image consistency cues with direct 3D expression parameter supervision and an auxiliary emotion recognition branch. Our encoder is guided by authentic expression parameters from spontaneous 4D facial scans, while an intensity-aware emotion loss encourages the 3D expression parameters to capture genuine emotion content without exaggeration. This dual-supervision strategy bridges the 2D/3D domain gap and mitigates expression-intensity bias, yielding high-fidelity 3D reconstructions that preserve subtle emotional cues. From a single image, FIELDS produces emotion-rich face models with highly realistic expressions, significantly improving in-the-wild facial expression recognition performance without sacrificing naturalness.

</details>


### [71] [Shift-Equivariant Complex-Valued Convolutional Neural Networks](https://arxiv.org/abs/2511.21250)
*Quentin Gabot,Teck-Yian Lim,Jérémy Fix,Joana Frontera-Pons,Chengfang Ren,Jean-Philippe Ovarlez*

Main category: cs.CV

TL;DR: 该论文将“可学习多相位上/下采样（LPS）”从实值网络推广到复值网络，并引入从复到实的投影层结合Gumbel-Softmax，以在分类、重建、语义分割任务中实现/保持平移不变性与等变性，特别在极化SAR图像上验证。


<details>
  <summary>Details</summary>
Motivation: 传统CNN因下/上采样破坏了平移等变与不变性，只能靠数据增广弥补，缺乏理论保证。已有APS与LPS在实值网络中提供了构造性保证，但对于复值网络（在SAR等领域常见）尚缺系统化方法。

Method: 在LPS框架基础上扩展到复值神经网络：提出在Gumbel-Softmax之前的C→R投影层，使复值特征可进行可学习的多相位选择与上/下采样；从理论上证明扩展后层对平移的等变/不变性质；将该构件嵌入分类、重建、语义分割模型。

Result: 在极化SAR数据集上评估：分类任务中更稳定的平移不变性与更高准确率；重建与语义分割中保持/提升平移等变性并带来更好或相当的性能（定量指标未在摘要给出）。

Conclusion: 复值LPS与新引入的C→R投影层为实现理论可保证的平移等变/不变性提供通用积木，在涉及复值信号（如极化SAR）的视觉任务中有效，优于仅靠数据增广的经验方法。

Abstract: Convolutional neural networks have shown remarkable performance in recent years on various computer vision problems. However, the traditional convolutional neural network architecture lacks a critical property: shift equivariance and invariance, broken by downsampling and upsampling operations. Although data augmentation techniques can help the model learn the latter property empirically, a consistent and systematic way to achieve this goal is by designing downsampling and upsampling layers that theoretically guarantee these properties by construction. Adaptive Polyphase Sampling (APS) introduced the cornerstone for shift invariance, later extended to shift equivariance with Learnable Polyphase up/downsampling (LPS) applied to real-valued neural networks. In this paper, we extend the work on LPS to complex-valued neural networks both from a theoretical perspective and with a novel building block of a projection layer from $\mathbb{C}$ to $\mathbb{R}$ before the Gumbel Softmax. We finally evaluate this extension on several computer vision problems, specifically for either the invariance property in classification tasks or the equivariance property in both reconstruction and semantic segmentation problems, using polarimetric Synthetic Aperture Radar images.

</details>


### [72] [AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs](https://arxiv.org/abs/2511.21251)
*Shuhan Xia,Peipei Li,Xuannan Liu,Dongsen Zhang,Xinyu Guo,Zekun Li*

Main category: cs.CV

TL;DR: AVFakeBench提出首个覆盖人类与通用场景的音视频造假检测综合基准，含12K问答、7类伪造与4层标注，并建立多任务评测。通过多阶段混合造假框架生成高质量伪造，评测13种模型显示AV-LMM具潜力但在细粒度感知与推理上仍薄弱。


<details>
  <summary>Details</summary>
Motivation: 现实中的音视频伪造已超越以人脸为中心的DeepFake，扩展到更复杂、更多样的自然场景与语义，但现有基准局限于单一类型与单粒度标注，难以反映真实威胁与方法性能。

Method: 构建AVFakeBench基准：1) 多阶段混合伪造生成框架，结合专有任务规划模型与专家级生成模型实现精确操控与多样化伪造；2) 汇集12K音视频问答，覆盖7种伪造类型与4层次标注；3) 设计多任务评测，包括二分类判断、伪造类型分类、伪造细节选择与可解释推理；4) 在基准上系统评测11个AV-LMM与2个主流检测方法。

Result: 在AVFakeBench上，AV-LMM展现成为新型伪造检测器的潜力，但在细粒度感知与推理能力方面存在显著短板；基准有效揭示当前方法对复杂、多语义伪造的适应性不足。

Conclusion: AVFakeBench更全面地刻画真实场景中的音视频伪造版图，提供分层标注与多任务评测以推动检测研究；尽管AV-LMM有前景，但需提升细粒度感知与多模态因果推理能力。

Abstract: The threat of Audio-Video (AV) forgery is rapidly evolving beyond human-centric deepfakes to include more diverse manipulations across complex natural scenes. However, existing benchmarks are still confined to DeepFake-based forgeries and single-granularity annotations, thus failing to capture the diversity and complexity of real-world forgery scenarios. To address this, we introduce AVFakeBench, the first comprehensive audio-video forgery detection benchmark that spans rich forgery semantics across both human subject and general subject. AVFakeBench comprises 12K carefully curated audio-video questions, covering seven forgery types and four levels of annotations. To ensure high-quality and diverse forgeries, we propose a multi-stage hybrid forgery framework that integrates proprietary models for task planning with expert generative models for precise manipulation. The benchmark establishes a multi-task evaluation framework covering binary judgment, forgery types classification, forgery detail selection, and explanatory reasoning. We evaluate 11 Audio-Video Large Language Models (AV-LMMs) and 2 prevalent detection methods on AVFakeBench, demonstrating the potential of AV-LMMs as emerging forgery detectors while revealing their notable weaknesses in fine-grained perception and reasoning.

</details>


### [73] [LaGen: Towards Autoregressive LiDAR Scene Generation](https://arxiv.org/abs/2511.21256)
*Sizhuo Zhou,Xiaosong Jia,Fanrui Zhang,Junjie Li,Juyong Zhang,Yukang Feng,Jianwen Sun,Songbur Wong,Junqi You,Junchi Yan*

Main category: cs.CV

TL;DR: 提出LaGen：首个可逐帧自回归生成长时域LiDAR场景的生成式世界模型，单帧起步、条件为3D框，生成高保真4D点云，并通过场景解耦估计与噪声调制缓解交互与累计误差，在nuScenes协议上优于SOTA，尤其在后期帧。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR生成仅能单帧静态生成；预测方法需多帧历史且一次性确定性输出多帧，缺乏交互与灵活控制，无法支持长时域交互式生成。需要一种既可长时序、又可交互、还能以最少条件启动的LiDAR生成框架。

Method: 提出LaGen：1) 逐帧自回归生成器，以单帧点云为起点；2) 引入以3D检测框为条件的条件生成，以控制对象级内容并生成4D场景点云；3) 场景解耦估计模块，显式建模和操纵对象级与背景级成分以提升交互性；4) 噪声调制模块，缓解长时序自回归的误差累积；5) 基于nuScenes构建长时域LiDAR生成评测协议。

Result: 在构建的nuScenes长时域评测上，相比最先进的LiDAR生成与预测模型，LaGen在整体和尤其后期帧指标上显著领先，生成质量与时序一致性更好；具备对象级可控与交互式生成能力。

Conclusion: LaGen实现了首个面向LiDAR的长时域逐帧自回归生成框架，兼顾单帧启动、对象级可控与误差抑制，显著提升长时序场景生成效果，为AD世界模型的交互式LiDAR生成提供新范式。

Abstract: Generative world models for autonomous driving (AD) have become a trending topic. Unlike the widely studied image modality, in this work we explore generative world models for LiDAR data. Existing generation methods for LiDAR data only support single frame generation, while existing prediction approaches require multiple frames of historical input and can only deterministically predict multiple frames at once, lacking interactivity. Both paradigms fail to support long-horizon interactive generation. To this end, we introduce LaGen, which to the best of our knowledge is the first framework capable of frame-by-frame autoregressive generation of long-horizon LiDAR scenes. LaGen is able to take a single-frame LiDAR input as a starting point and effectively utilize bounding box information as conditions to generate high-fidelity 4D scene point clouds. In addition, we introduce a scene decoupling estimation module to enhance the model's interactive generation capability for object-level content, as well as a noise modulation module to mitigate error accumulation during long-horizon generation. We construct a protocol based on nuScenes for evaluating long-horizon LiDAR scene generation. Experimental results comprehensively demonstrate LaGen outperforms state-of-the-art LiDAR generation and prediction models, especially on the later frames.

</details>


### [74] [Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting](https://arxiv.org/abs/2511.21265)
*Juncheng Chen,Chao Xu,Yanjun Cao*

Main category: cs.CV

TL;DR: MatchGS通过纠正3D高斯泼洒(3DGS)的几何偏差，生成高精度对应关系并将3D显式知识注入2D匹配器，使在零样本条件下显著提升图像匹配性能。


<details>
  <summary>Details</summary>
Motivation: 学习型图像匹配需要大规模、几何准确且多样的数据。3DGS虽然渲染真实感强、适合合成视角数据，但其几何不准与深度偏置会破坏对应标注，限制其作为训练数据源的价值。

Method: 提出MatchGS，包括两部分：(1) 几何忠实的数据生成流水线：对3DGS几何进行系统性校正/细化，从而可在保持高保真渲染下，从大量和极端视角合成图像并获得高精度对应标签；(2) 2D-3D表征对齐：将3DGS的显式3D属性（如高斯属性）注入2D半稠密匹配器，使其学习视角不变的3D感知表征，并提供自监督信号。

Result: 相较现有数据集，所生成的真值对应将极线误差降低至最多40倍；在极端视角变化下也能稳定监督；基于其数据单独训练的SOTA匹配器在公开基准上零样本提升显著，最高可达17.7%。

Conclusion: 经过几何精炼的3DGS可作为可扩展、高保真且结构丰富的数据源，推动新一代强健的零样本图像匹配方法。

Abstract: Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.

</details>


### [75] [Co-Training Vision Language Models for Remote Sensing Multi-task Learning](https://arxiv.org/abs/2511.21272)
*Qingyun Li,Shuran Ma,Junwei Luo,Yi Yu,Yue Zhou,Fengxiang Wang,Xudong Lu,Xiaoxing Wang,Xin He,Yushi Chen,Xue Yang,Junchi Yan*

Main category: cs.CV

TL;DR: 提出RSCoVLM，一种用于遥感多任务学习的统一视觉语言模型基线，通过数据策展引擎、动态分辨率与Zoom-in链式机制处理多尺度/UHR影像，并加强检测与公平评测，在多任务上达SOTA，资源全开源。


<details>
  <summary>Details</summary>
Motivation: Transformer和VLM在单一RS任务与文本接口上的成功显示出构建跨任务统一模型的潜力；RS数据多源异构、尺度跨度大、UHR推理昂贵，现有方法在泛化、可扩展性与统一接口上仍有限。

Method: 1) 数据策展引擎：涵盖采集、离线处理与整合、在线加载与加权，生成灵活的图文对话以适配多任务；2) 统一动态分辨率策略：针对RS多尺度，按需调整输入分辨率；3) UHR处理的Zoom-in Chain机制并配套LRS-VQA-Zoom数据集，逐步放大细节降低算力开销；4) 增强目标检测能力并提出VLM与传统检测模型的公平评测协议。

Result: 在多种RS任务上取得SOTA，优于现有RS VLM并可与专门专家模型竞争；验证了动态分辨率与Zoom-in在UHR上的有效性与计算效率；公开训练/评测工具、权重与数据集。

Conclusion: RSCoVLM作为简单灵活的基线，有效统一RS多任务，兼顾性能与效率，推动朝通用RS模型发展，并通过开源促进可复现与后续研究。

Abstract: With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.

</details>


### [76] [PathMamba: A Hybrid Mamba-Transformer for Topologically Coherent Road Segmentation in Satellite Imagery](https://arxiv.org/abs/2511.21298)
*Jules Decaestecker,Nicolas Vigne*

Main category: cs.CV

TL;DR: 提出PathMamba：将线性高效的State Space Model（Mamba）与Transformer结合，用于卫星图像道路分割，在保持计算效率的同时显著提升拓扑连续性，取得SOTA并在APLS上刷新基准。


<details>
  <summary>Details</summary>
Motivation: 现有道路分割需兼顾像素精度与道路拓扑连通性。Transformer能捕获全局语义但计算复杂度为二次，难以在资源受限设备部署；而Mamba具线性时间、擅长建模长序列与连续结构，但可能欠缺全局推理。作者认为两者优势互补，亟需一种既保拓扑又高效的混合架构。

Method: 提出PathMamba混合网络：在编码/解码中以Mamba模块沿空间序列建模，追踪道路的连续与拓扑结构；以Transformer模块注入全局上下文以细化特征。整体设计兼顾线性效率与全局推理能力，旨在生成拓扑更优的分割图，同时避免纯注意力的规模化成本。

Result: 在DeepGlobe Road Extraction与Massachusetts Roads数据集上达到新的SOTA。特别是APLS（评估道路网络拓扑连通性）显著提升，设定新的基准，同时保持与或优于现有方法的计算效率与竞争力。

Conclusion: Mamba与Transformer的互补性在道路分割中有效：用Mamba保连续与拓扑，用Transformer供全局语义，能在不牺牲效率的情况下提升拓扑质量。PathMamba为资源受限场景提供了高效且拓扑友好的方案，并可推广到需长结构建模的其他遥感/图像任务。

Abstract: Achieving both high accuracy and topological continuity in road segmentation from satellite imagery is a critical goal for applications ranging from urban planning to disaster response. State-of-the-art methods often rely on Vision Transformers, which excel at capturing global context, yet their quadratic complexity is a significant barrier to efficient deployment, particularly for on-board processing in resource-constrained platforms. In contrast, emerging State Space Models like Mamba offer linear-time efficiency and are inherently suited to modeling long, continuous structures. We posit that these architectures have complementary strengths. To this end, we introduce PathMamba, a novel hybrid architecture that integrates Mamba's sequential modeling with the Transformer's global reasoning. Our design strategically uses Mamba blocks to trace the continuous nature of road networks, preserving topological structure, while integrating Transformer blocks to refine features with global context. This approach yields topologically superior segmentation maps without the prohibitive scaling costs of pure attention-based models. Our experiments on the DeepGlobe Road Extraction and Massachusetts Roads datasets demonstrate that PathMamba sets a new state-of-the-art. Notably, it significantly improves topological continuity, as measured by the APLS metric, setting a new benchmark while remaining computationally competitive.

</details>


### [77] [CaliTex: Geometry-Calibrated Attention for View-Coherent 3D Texture Generation](https://arxiv.org/abs/2511.21309)
*Chenyu Liu,Hongze Chen,Jingzhi Bao,Lingting Zhu,Runze Zhang,Weikai Chen,Zeyu Hu,Yingda Yin,Keyang Luo,Xin Wang*

Main category: cs.CV

TL;DR: 提出CaliTex，通过几何校准注意力解决3D纹理跨视角不一致，显著提升视角一致与无缝纹理质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推动了3D纹理生成，但跨视角不一致普遍存在，其根源在于未结构化的全局注意力导致几何与外观耦合不稳、注意力歧义。需要一种能与3D几何对齐的注意力机制。

Method: 提出几何校准注意力框架CaliTex：1) 部件对齐注意力（Part-Aligned Attention），在语义匹配部件间强制空间对齐；2) 条件路由注意力（Condition-Routed Attention），将外观信息通过由几何条件化的通道路由以保持空间保真。配合两阶段扩散Transformer，使几何一致性成为网络内生属性。

Result: 在实证中生成无缝、视角一致的3D纹理，超越开源与商业基线。

Conclusion: 通过显式将注意力与3D结构对齐，CaliTex缓解注意力歧义，使几何一致性成为模型固有能力，显著提升3D纹理跨视角一致与质量。

Abstract: Despite major advances brought by diffusion-based models, current 3D texture generation systems remain hindered by cross-view inconsistency -- textures that appear convincing from one viewpoint often fail to align across others. We find that this issue arises from attention ambiguity, where unstructured full attention is applied indiscriminately across tokens and modalities, causing geometric confusion and unstable appearance-structure coupling. To address this, we introduce CaliTex, a framework of geometry-calibrated attention that explicitly aligns attention with 3D structure. It introduces two modules: Part-Aligned Attention that enforces spatial alignment across semantically matched parts, and Condition-Routed Attention which routes appearance information through geometry-conditioned pathways to maintain spatial fidelity. Coupled with a two-stage diffusion transformer, CaliTex makes geometric coherence an inherent behavior of the network rather than a byproduct of optimization. Empirically, CaliTex produces seamless and view-consistent textures and outperforms both open-source and commercial baselines.

</details>


### [78] [HTTM: Head-wise Temporal Token Merging for Faster VGGT](https://arxiv.org/abs/2511.21317)
*Weitian Wang,Lukas Meiner,Rai Shubham,Cecilia De La Parra,Akash Kumar*

Main category: cs.CV

TL;DR: 提出HTTM，一种针对VGGT的训练-free 3D token合并方法，在多头粒度进行时序合并，减少全局注意计算，GPU推理最高加速7倍且性能几乎不降。


<details>
  <summary>Details</summary>
Motivation: VGGT一次性联合推断相机位姿、深度和稠密几何，需跨视角全局注意，长序列/大场景重建时带来显著延迟瓶颈；现有token合并方法跨头统一合并，输出各头特征同质化，削弱表示力。

Method: 提出Head-wise Temporal Token Merging (HTTM)：在多头粒度执行时空附近性驱动的token合并，保留各注意头的特异性。利用头级空间局部性与时间对应关系，支持更高合并率、较低合并成本；训练-free地插入VGGT推理流程中，避免修改或再训练模型。

Result: 在GPU推理中，相比原始VGGT实现最高约7倍加速，同时精度/重建质量基本无损（“可忽略”性能下降）。

Conclusion: 通过头级时序合并缓解VGGT全局注意的计算瓶颈，在不牺牲重建质量的前提下显著提升大场景与长序列重建的推理速度，优于统一合并的既有方法。

Abstract: The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.

</details>


### [79] [The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment](https://arxiv.org/abs/2511.21331)
*Stefanos Koutoupis,Michaela Areti Zervou,Konstantinos Kontras,Maarten De Vos,Panagiotis Tsakalides,Grigorios Tsagatakis*

Main category: cs.CV

TL;DR: ConFu提出在统一表示空间中同时对单模态与其融合表示进行对比学习，既保持强配对关系又捕获高阶跨模态依赖，从而在检索与分类任务上取得竞争性表现，并支持一对一与二对一检索。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法多为两两对齐，难以捕获需要多模态共同作用的高阶关系（如XOR），且有些高阶方法反而削弱了基础的成对关系，导致单模态任务表现受限。

Method: 提出Contrastive Fusion（ConFu）：在统一嵌入空间中，将单个模态及其融合（两两或更多模态的组合）共同进行对比学习。在传统成对对比损失上加入“融合模态对比项”，要求融合表示既与其组成模态对齐，又与第三模态形成联合嵌入，从而显式建模高阶依赖并保持成对一致性。框架支持统一的一对一与二对一检索。

Result: 在合成与真实多模态基准上，ConFu能更好利用跨模态互补、捕获高阶依赖，并随模态数增加可扩展；在检索与分类任务上取得有竞争力的结果，并在统一框架下支持一对一与二对一检索。

Conclusion: 通过在对比学习中同时对齐单模态与其融合表示，ConFu兼顾成对关系与高阶依赖，提升多模态表示学习的泛化与实用性，在多种任务与复杂度条件下表现稳健。

Abstract: Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.

</details>


### [80] [Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure](https://arxiv.org/abs/2511.21337)
*Munish Rathee,Boris Bačić,Maryam Doborjeh*

Main category: cs.CV

TL;DR: SIFT-SNN提出一种结合SIFT特征与脉冲神经网络的低时延结构异常检测管线，在6000帧奥克兰海港大桥数据上达92.3%准确率、单帧9.5ms、8.1%稀疏放电，适合低功耗边缘部署，但对未见场景的泛化仍待验证。


<details>
  <summary>Details</summary>
Motivation: 交通基础设施（如可移动混凝土护栏）需要在多变环境下进行实时、低功耗、可解释的结构安全监测，传统CNN虽准确但在时延、能耗与可解释性上受限，亟需一种能在嵌入式设备上高效运行且透明决策的方案。

Method: 构建SIFT-SNN混合管线：用SIFT进行空间特征编码；通过延迟驱动的脉冲转换层将特征转为时序脉冲；采用LIF脉冲神经网络进行分类。使用包含真实与合成不安全样本、覆盖多天气与光照的奥克兰海港大桥数据集（6000标注帧）进行训练与验证。

Result: 在该数据集上获得92.3%±0.8%的分类准确率；单帧推理时延9.5毫秒；网络脉冲活动稀疏度8.1%，满足亚10毫秒实时需求并降低能耗；原型系统在消费级设备上成功部署。

Conclusion: SIFT-SNN在保持空间特征可解释性的同时，实现实时、低功耗边缘检测，优于传统CNN在可解释与硬件效率上的表现；合成增强提升了鲁棒性，但对未见现场条件的泛化需进一步验证。该框架作为可推广的结构安全监测案例，适用于部署在多城市的可移动混凝土护栏。

Abstract: This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.

</details>


### [81] [SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding](https://arxiv.org/abs/2511.21339)
*Tae-Min Choi,Tae Kyeong Jeong,Garam Kim,Jaemin Lee,Yeongyoon Koh,In Cheul Choi,Jae-Ho Chung,Jong Woong Park,Juyoun Park*

Main category: cs.CV

TL;DR: 提出SurgMLLMBench：统一外科多模态LLM基准，含像素级分割与结构化VQA，覆盖腹腔镜、机器人与显微外科，并新增MAVIS数据集；单模统一评测与泛化表现良好，促进可复现实验与交互式外科推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有外科多模态数据多为VQA且标签体系不一致，缺乏像素级分割支持，难以进行一致、全面的评测，也限制了模型在交互推理与场景理解中的应用。

Method: 构建统一分类体系，整合多领域外科数据；新采集显微血管吻合（MAVIS）数据；为图像提供器械像素级分割掩码并配套结构化VQA标注；在统一框架下评测多模态LLM的VQA、分割与对话交互能力，并进行跨域与零样本泛化实验。

Result: 基线实验显示：在SurgMLLMBench上训练的单一模型可在腹腔镜、机器人、显微外科三域取得一致表现，并能有效泛化到未见数据集；相比传统仅VQA基准，评测更全面、稳定。

Conclusion: SurgMLLMBench为外科多模态LLM提供统一、可复现的评测与开发平台，推动交互式外科场景理解与推理模型的发展，并将公开发布以促进社区研究。

Abstract: Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.

</details>


### [82] [PFF-Net: Patch Feature Fitting for Point Cloud Normal Estimation](https://arxiv.org/abs/2511.21365)
*Qing Li,Huifang Feng,Kanle Shi,Yue Gao,Yi Fang,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: 提出一种基于多尺度特征融合与补偿的点云法向估计方法，通过聚合不同邻域尺度的补丁特征并进行跨尺度补偿，实现对局部几何的自适应表征，达到更高精度、更少参数与更快速度。


<details>
  <summary>Details</summary>
Motivation: 点云法向估计依赖局部补丁，但不同数据/几何需不同邻域尺度，现有方法参数重、泛化差、难以兼顾精度与效率，亟需一种既能自适应尺度又高效稳健的特征提取策略。

Method: 构建“补丁特征拟合”(PFF)框架：1) 多尺度邻域采样以获取不同尺度的补丁特征；2) 特征聚合模块将各尺度特征逐级向中心聚合，并通过移除远离中心的点缩小补丁，兼顾大范围结构与细节；3) 跨尺度特征补偿模块复用大尺度早期层特征，显式关联不同补丁尺寸的信息；4) 通过多尺度聚合与补偿实现对“最优几何描述”的近似以进行法向预测。

Result: 在合成与真实数据集上达到SOTA法向估计精度，同时显著减少网络参数与推理时间，显示出强鲁棒性与效率。

Conclusion: 多尺度特征聚合与跨尺度补偿能有效实现尺度自适应的局部几何表征，提升点云法向估计的准确性与效率，为轻量化、鲁棒的点云几何学习提供了可行范式。

Abstract: Estimating the normal of a point requires constructing a local patch to provide center-surrounding context, but determining the appropriate neighborhood size is difficult when dealing with different data or geometries. Existing methods commonly employ various parameter-heavy strategies to extract a full feature description from the input patch. However, they still have difficulties in accurately and efficiently predicting normals for various point clouds. In this work, we present a new idea of feature extraction for robust normal estimation of point clouds. We use the fusion of multi-scale features from different neighborhood sizes to address the issue of selecting reasonable patch sizes for various data or geometries. We seek to model a patch feature fitting (PFF) based on multi-scale features to approximate the optimal geometric description for normal estimation and implement the approximation process via multi-scale feature aggregation and cross-scale feature compensation. The feature aggregation module progressively aggregates the patch features of different scales to the center of the patch and shrinks the patch size by removing points far from the center. It not only enables the network to precisely capture the structure characteristic in a wide range, but also describes highly detailed geometries. The feature compensation module ensures the reusability of features from earlier layers of large scales and reveals associated information in different patch sizes. Our approximation strategy based on aggregating the features of multiple scales enables the model to achieve scale adaptation of varying local patches and deliver the optimal feature description. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets with fewer network parameters and running time.

</details>


### [83] [Endo-G$^{2}$T: Geometry-Guided & Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes](https://arxiv.org/abs/2511.21367)
*Yangle Liu,Fengze Li,Kan Liu,Jieming Ma*

Main category: cs.CV

TL;DR: 提出 Endo-G²T：在内镜动态场景中为 4D 高斯泼洒提供几何引导与时间一致性，结合深度先验蒸馏、时间嵌入与关键帧约束训练，在 EndoNeRF 与 StereoMIS-P1 上达 SOTA。


<details>
  <summary>Details</summary>
Motivation: 内镜视频存在强视角相关效应（高光、湿反射、遮挡），纯光度监督与真实几何不一致，易导致早期几何漂移并在密化中被放大，难以纠正。需要一种既能早期锚定几何又保持时序一致与训练效率的方法。

Method: 1) 几何引导的先验蒸馏：将基于置信度门控的单目深度转为监督，采用尺度不变深度与深度梯度损失，并以“热身→封顶”日程逐步注入先验避免过拟合；2) 时间嵌入的高斯场：在 XYZT 中建模动态，采用类似转子的旋转参数化，辅以轻量正则以获得平滑运动与清晰的不透明边界；3) 关键帧约束的流式训练：在最大点数预算下优先优化关键帧，非关键帧轻量更新，提升效率与长时稳定性。

Result: 在 EndoNeRF 与 StereoMIS-P1 数据集上，相较单目重建基线取得最先进性能（SOTA），实现更一致的时序与更稳定的几何。

Conclusion: 通过几何先验蒸馏、时间嵌入与关键帧流式优化，Endo-G²T 能在动态内镜场景中早期锚定几何并维持时序一致与效率，缓解纯光度监督导致的几何漂移并取得 SOTA。

Abstract: Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.

</details>


### [84] [Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning](https://arxiv.org/abs/2511.21375)
*Xin Gu,Haoji Zhang,Qihang Fan,Jingxuan Niu,Zhipeng Zhang,Libo Zhang,Guang Chen,Fan Chen,Longyin Wen,Sijie Zhu*

Main category: cs.CV

TL;DR: 提出STVG-o1：在不改动架构的前提下，让现成的多模态大模型通过“边界框思维链+强化学习奖励”实现时空视频指代新SOTA。


<details>
  <summary>Details</summary>
Motivation: MLLM虽具强语言理解，但在STVG上表现不佳，原因在于训练目标与任务不匹配，以及视觉编码器缺乏细粒度区域-词对齐，导致时空定位不准。

Method: 1) 设计“边界框思维链”（bounding-box chain-of-thought）：先在中间步骤显式推理目标在时间与空间上的位置序列，再输出最终预测；2) 提出多维度强化奖励：包括格式、一致性、时间、空间、以及“思考”奖励，为几何感知的强化微调提供监督；3) 无需改动MLLM架构，作为即插即用框架。

Result: 在HCSTVG-v1/v2与VidSTG评测：HCSTVG-v1的m_tIoU较最佳专用方法提升7.3%，在VidSTG与专用模型持平，并大幅超过所有现有基于MLLM的方法；显示出跨数据集的开放词汇泛化能力。

Conclusion: 通过显式的边界框推理与多维强化奖励，通用MLLM可成为精确时空指代的强大骨干，在无需架构改动的条件下达到或超越专用方法，并具备良好的泛化。

Abstract: Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\% m\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.

</details>


### [85] [Monet: Reasoning in Latent Visual Space Beyond Images and Language](https://arxiv.org/abs/2511.21395)
*Qixun Wang,Yang Shi,Yifei Wang,Yuanxing Zhang,Pengfei Wan,Kun Gai,Xianghua Ying,Yisen Wang*

Main category: cs.CV

TL;DR: 提出Monet框架，使MLLM在潜在视觉空间中直接“以图思考”，通过生成连续视觉嵌入作为中间思维；结合三阶段蒸馏式SFT与新RL算法VLPO，配合125K多类型图文CoT数据集，显著提升感知与抽象视觉推理泛化。


<details>
  <summary>Details</summary>
Motivation: 现有“以图思考”多依赖外部工具或显式图像编辑，难以像人类那样灵活进行抽象的视觉思维；同时，训练MLLM进行潜在视觉推理面临对齐计算昂贵与潜在表示监督不足两大瓶颈。

Method: 1) 设计Monet，使模型在潜在视觉空间生成连续嵌入，作为中间视觉思维；2) 三阶段蒸馏式SFT：降低潜在视觉对齐成本并增强潜在嵌入监督；3) 指出GRPO对潜在推理增益有限，提出VLPO，在策略梯度中显式纳入潜在嵌入优化；4) 构建Monet-SFT-125K图文交错CoT数据，覆盖真实、图表、OCR与几何场景。

Result: 在多项真实世界感知与推理基准上取得一致提升；在抽象视觉推理的OOD任务上表现强，显示良好的泛化；并通过消融与失败案例分析验证各组件贡献。

Conclusion: 让MLLM直接在潜在视觉空间进行中间推理是有效路径；配合三阶段蒸馏SFT与VLPO可高效对齐并强化潜在推理能力；Monet-7B和Monet-SFT-125K为未来视觉潜在推理研究提供可复用基线与资源。

Abstract: "Thinking with images" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.

</details>


### [86] [Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis](https://arxiv.org/abs/2511.21397)
*Jiyun Bae,Hyunjong Ok,Sangwoo Mo,Jaeho Lee*

Main category: cs.CV

TL;DR: 研究VLM在测试时加入干扰信息的扩展效应：视觉干扰会造成逆向扩展，准确率下降但推理长度不变；提出Idis数据集并用属性计数跟踪解释机制，结果在Waterbirds等偏置基准上泛化，并给出简单提示策略缓解偏置。


<details>
  <summary>Details</summary>
Motivation: 语言模型中已观察到文本干扰会让推理更长但效果更差（逆向扩展）。尚不清楚多模态环境下，尤其视觉干扰（语义/数量/空间维度）是否产生同类或不同模式的影响，需要系统数据与分析来揭示VLM的脆弱性与偏置机制，并寻找缓解策略。

Method: 构建Idis视觉问答数据集，系统控制并注入三类视觉干扰（语义、数量、空间）；在多种VLM上评测测试时扩展（更多思维步/推理长度）与性能关系；分析模型推理轨迹中属性计数（如对象/属性/位置计数）以刻画干扰、推理长度与准确率的相互作用；在Waterbirds等偏置基准上验证外部效度；设计并评估一种简单的提示策略以降低偏置驱动的预测。

Result: 与语言模型的文本干扰不同，视觉干扰同样导致逆向扩展效应但不会增加推理长度；随着干扰增多，准确率显著下降；属性计数轨迹能揭示模型在干扰存在时的系统性计数/归因错误；这些现象在Waterbirds等视觉偏置基准上重现；所提简单提示在推理式模型中能缓解偏置预测。

Conclusion: 视觉干扰与文本干扰的效应机制不同：前者引发准确率下降但不拉长推理；属性计数是洞察干扰影响的关键信号；发现具有跨数据集的普适性；简单的提示工程可部分对抗偏置，提示在多模态推理中需要更细粒度的干扰鲁棒性设计。

Abstract: How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.

</details>


### [87] [DiverseVAR: Balancing Diversity and Quality of Next-Scale Visual Autoregressive Models](https://arxiv.org/abs/2511.21415)
*Mingue Park,Prin Phunyaphibarn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: DiverseVAR在不需重训/微调的前提下，通过测试时策略显著提升文本条件VAR图像生成的多样性，并用“尺度穿越”保持画质，达成更优的多样性-质量帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: VAR模型在生成质量上接近或竞争于扩散/流模型，但在多样性上表现差：对同一提示往往输出近乎相同的图像。现有研究多聚焦画质而忽视多样性；需要一种无需改动训练的测试时方法来提升多样性且尽量不牺牲质量。

Method: 两阶段测试时方案：1) 文本嵌入加噪，借鉴扩散模型的多样性增强思路，但会引入多样性-质量权衡；2) 提出“scale-travel（尺度穿越）”的潜空间细化方法：使用多尺度自动编码器提取粗尺度token，使生成可在中间阶段恢复/继续，从而在引入多样性后进行质量修复与细化。

Result: 将文本嵌入加噪与scale-travel结合，在多组实验中显著提升多样性，同时将画质下降降至最小，相比现有方法形成新的多样性-质量帕累托前沿。

Conclusion: 通过纯测试时的轻量策略，DiverseVAR有效弥补VAR在多样性上的短板，并以尺度穿越在不重训的条件下维持高画质，为文本到图像VAR提供实用的多样性—质量折中新基线。

Abstract: We introduce DiverseVAR, a framework that enhances the diversity of text-conditioned visual autoregressive models (VAR) at test time without requiring retraining, fine-tuning, or substantial computational overhead. While VAR models have recently emerged as strong competitors to diffusion and flow models for image generation, they suffer from a critical limitation in diversity, often producing nearly identical images even for simple prompts. This issue has largely gone unnoticed amid the predominant focus on image quality. We address this limitation at test time in two stages. First, inspired by diversity enhancement techniques in diffusion models, we propose injecting noise into the text embedding. This introduces a trade-off between diversity and image quality: as diversity increases, the image quality sharply declines. To preserve quality, we propose scale-travel: a novel latent refinement technique inspired by time-travel strategies in diffusion models. Specifically, we use a multi-scale autoencoder to extract coarse-scale tokens that enable us to resume generation at intermediate stages. Extensive experiments show that combining text-embedding noise injection with our scale-travel refinement significantly enhances diversity while minimizing image-quality degradation, achieving a new Pareto frontier in the diversity-quality trade-off.

</details>


### [88] [SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning](https://arxiv.org/abs/2511.21420)
*Futian Wang,Mengqi Wang,Xiao Wang,Haowen Wang,Jin Tang*

Main category: cs.CV

TL;DR: 提出一种结合SAM的遥感变化描述方法：用CNN/Transformer提取全局特征，SAM划出语义与运动变化区域，知识图谱提供对象先验，跨注意力融合，多数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法区域感知弱、时序对齐有限，难以精确定位变化并将其转为自然语言描述。作者希望引入更强的区域级表示与显式先验，改善变化定位与语义生成质量。

Method: 多源信息融合框架：1) CNN/Transformer抽取双时相影像的全局视觉特征；2) 采用SAM分割得到语义/运动层面的变化区域，实现区域级表示与ROI注入；3) 构建知识图谱，提供关注对象的语义与关系先验；4) 通过跨注意力融合全局特征、SAM区域特征与知识图谱；5) 使用Transformer解码器生成变化描述。

Result: 在多个主流遥感变化描述基准上取得最新最优结果（SOTA），实验广泛且验证了各模块有效。

Conclusion: 将SAM的区域级分割与知识图谱先验融入变化描述，可显著提升区域感知与时序对齐，从而改进描述质量；方法通用且可复现（代码将开源）。

Abstract: Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning

</details>


### [89] [E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework](https://arxiv.org/abs/2511.21422)
*Adeela Islam,Stefano Fiorini,Manuel Lecha,Theodore Tsesmelis,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: E-M3RF 是一个等变多模态3D重组框架，融合点云几何与颜色特征，并通过SE(3) flow matching 预测碎片位姿，实现无重叠物理一致的装配；在多数据集上优于现有方法，尤其在RePAIR上显著降低旋转、平移与CD误差。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习3D拼合方法主要依赖几何特征，面对小型、对称、侵蚀碎片时几何信息不足且易歧义；同时缺乏显式物理约束（如防重叠）。因此需要一个能够利用多模态线索（几何+颜色）并具备群等变性与物理一致性的框架，以提升匹配与装配的稳健性与精度。

Method: 提出E-M3RF：1) 输入为包含位置与颜色的碎片点云；2) 几何分支采用旋转等变编码器，将3D点位置编码为与旋转一致的几何特征；3) 颜色分支用Transformer对点颜色序列建模；4) 融合两分支形成多模态表示；5) 通过SE(3) flow matching 学习从碎片到装配的变换场，预测每个碎片的位姿；6) 通过设计避免重叠与不物理一致的装配（文中暗示引入相应约束/损失）。

Result: 在四个数据集（Breaking Bad、Fantastic Breaks、RePAIR、Presious）上评测，相比竞品在RePAIR数据集上旋转误差降23.1%、平移误差降13.2%、Chamfer Distance降18.4%，整体表现优于现有方法，特别在几何歧义场景表现更稳健。

Conclusion: 融合几何与颜色的等变多模态表示结合SE(3)流匹配能有效提升3D碎片重组的精度与稳健性，并缓解几何歧义与物理不一致问题；方法在合成与真实文物数据上取得显著改进，显示其在文化遗产重建等应用中的潜力。

Abstract: 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.

</details>


### [90] [From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings](https://arxiv.org/abs/2511.21428)
*Jiajie Zhang,Sören Schwertfeger,Alexander Kleiner*

Main category: cs.CV

TL;DR: 提出一种无监督流水线，从工业连续视频中自动提取可用于VLA预训练的“片段化视频+潜在动作序列”，核心包括轻量运动分词器与基于“潜在动作能量”的无监督动作切分；在公开与电机装配数据上验证能发现语义一致的动作原语，实现端到端可扩展的数据组织。


<details>
  <summary>Details</summary>
Motivation: 工业场景中大量人类操作视频未标注，VLA预训练缺乏结构化、语义一致且可规模化获取的训练样本。现有方法多需人工标注或弱监督，成本高、难泛化。因此需要一种无需标注、能从长时连续视频中自动发现/切分动作，并直接输出适配VLA的数据管线。

Method: 1) 训练轻量级运动tokenizer，将时序运动动力学编码为潜在运动token；2) 提出“潜在动作能量”（LAE）度量，在潜在空间中进行变化检测与边界定位，实现无监督动作切分，得到语义一致的动作原语；3) 输出：已切分的视频片段及对应的潜在动作序列；4) 对发现的原语进行聚类，并用VLM做语义一致性定量评估。

Result: 在公开基准与自有电机装配数据集上，方法有效切分出工位任务关键步骤；通过后续聚类与VLM打分，证明发现的动作原语具有较高语义一致性，适合作为VLA预训练数据。

Conclusion: 首次实现从非结构化工业视频到VLA预训练数据的全自动端到端抽取与组织，具备可扩展性与实际制造场景价值，为具身智能在制造业落地提供数据基础。

Abstract: We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.

</details>


### [91] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang,Fan Zhang,Xiao Wang,Mengqi Wang,Dexing Huang,Jin Tang*

Main category: cs.CV

TL;DR: 提出一种超图引导的时空事件流补全机制，通过跨时空超图连接事件与RGB标记并进行信息传递与自注意融合，缓解事件相机空间稀疏导致的欠采样，显著提升单/多标签事件分类表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据时间密、空间稀疏，主流用帧/体素/张量表示仍受稀疏致欠采样影响，导致特征不完整、识别受限；需要一种能跨时空聚合上下文并可融合RGB的机制补全稀疏事件信息。

Method: 构建跨时间与空间的超图，将事件tokens（可扩展含RGB tokens）作为节点，通过超边建立多元关系，进行上下文消息传递以补全稀疏事件；随后对不同时刻的超图节点进行自注意力聚合，实现多模态特征的学习与融合。

Result: 在单标签与多标签事件分类任务上进行广泛实验，所提框架优于现有方法（文中示意显著提升），证明补全与融合机制有效。

Conclusion: 超图引导的时空补全结合自注意力的多时刻、多模态融合能有效缓解事件流空间稀疏与欠采样问题，提升分类性能；框架灵活且可无缝引入RGB信息，源码将开源。

Abstract: Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.

</details>


### [92] [MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices](https://arxiv.org/abs/2511.21475)
*Shuai Zhang,Bao Tang,Siyuan Yu,Yueting Zhu,Jingfeng Yao,Ya Zou,Shanglin Yuan,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: MobileI2V 是一个约2.7亿参数的轻量扩散模型，实现移动端实时图像生成视频（I2V），最高支持720p、单步每帧<100ms，质量接近现有模型。核心贡献包括混合线性注意力去噪器、两步采样的时间步蒸馏，以及移动端注意力优化（2倍加速）。总体实现10倍以上生成速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在I2V生成上效果好但计算量大、速度慢，难以在算力受限的移动设备上实现高分辨率、实时生成。需要在效率与质量之间找到平衡并针对移动端进行系统级优化。

Method: 1) 架构：分析线性注意力与softmax注意力在移动端的表现，提出线性混合架构的去噪器，实现效率与质量的折中。2) 采样：提出时间步蒸馏，将>20步采样压缩到2步（亦支持1步），显著加速。3) 推理优化：面向移动端的注意力算子与实现细化，使注意力计算获得2倍加速；整体管线适配移动端。

Result: 首次在移动端实现快速720p I2V 生成；在单步条件下，720p每帧<100ms；两步采样在质量几乎不降的情况下带来约10倍速度提升；注意力部分在端侧获得2倍加速；整体模型约270M参数，质量与现有方法相当。

Conclusion: MobileI2V通过网络架构、采样策略蒸馏和端侧算子优化的协同设计，打通移动端高分辨率I2V的实时生成通路，在保持画质的同时显著降低时延与算力需求，并提供开源实现以促进后续研究与应用。

Abstract: Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.

</details>


### [93] [Frequency-Aware Token Reduction for Efficient Vision Transformer](https://arxiv.org/abs/2511.21477)
*Dong-Jae Lee,Jiwan Hur,Jaehyun Choi,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: 提出一种频率感知的Token精简策略：保留高频Token，将低频Token汇聚为一个直流(DC)Token，减算力、提精度，并缓解自注意力的秩塌缩与过平滑。


<details>
  <summary>Details</summary>
Motivation: ViT在多任务上表现优异，但注意力计算随Token数二次增长，现有Token压缩方法忽视注意力的频域特性（如秩塌缩、过平滑），导致效率-性能权衡受限。

Method: 基于频域分析将序列划分为高频与低频部分：选择性保留高频Token以维持细节与区分性；将低频部分聚合为一个紧凑的直流(DC)Token以保留全局/低频信息，从而在不丢失关键信号的前提下降低计算量。并对既有方法进行频率角度的重释与比较。

Result: 在广泛实验与分析中，该策略在降低计算开销的同时显著提升准确率，并明确缓解了自注意力中的秩塌缩与过平滑现象。

Conclusion: 频率感知的Token压缩为ViT在效率与性能间提供更优折中；通过保留高频与汇聚低频为DC Token，可稳定训练、提升精度并减少计算量，也解释了先前方法的频域局限。

Abstract: Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.

</details>


### [94] [Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning](https://arxiv.org/abs/2511.21490)
*Taehoon Kim,Donghwan Jang,Bohyung Han*

Main category: cs.CV

TL;DR: 提出Merge-and-Bound (M&B) 的类增量学习训练法，直接在参数空间做“跨任务/内任务”权重合并，并配合有界更新，既学新任务又减遗忘，且可无缝接入现有CIL方法，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: CIL中多阶段持续学习会产生灾难性遗忘；现有方法多依赖蒸馏、重放或结构改动，训练复杂且难以同时保持旧知识与吸收新知识。作者希望通过直接操控模型权重，低成本地在不改架构和目标的前提下缓解遗忘并提升当前任务学习。

Method: 在参数空间进行两类权重合并：1) 跨任务合并：对历次阶段模型权重做平均，得到统一参考模型；2) 内任务合并：在当前阶段训练过程中合并模型参数（如不同优化步/快照）以促进稳定学习。同时引入有界更新（bounded update），约束累计权重变化，尽量在旧模型附近优化，减少对旧知识的破坏。整体作为插件式策略融入现有CIL流程，无需改网络或损失。

Result: 在标准CIL基准上广泛评测，相比多种SOTA方法取得更优的平均精度/较低遗忘（摘要未给具体数值）。

Conclusion: 直接在权重空间做合并并施加有界更新，可以在不改变架构与目标的情况下有效缓解灾难性遗忘并提升CIL性能，方法通用且易集成。

Abstract: We present a novel training approach, named Merge-and-Bound (M&B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.

</details>


### [95] [CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation](https://arxiv.org/abs/2511.21503)
*Shizhe Sun,Wataru Ohyama*

Main category: cs.CV

TL;DR: 提出CanKD：用跨注意力做非局部特征蒸馏，仅加一个损失项，在检测与分割上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的蒸馏多为自注意对齐，通常在教师与学生各自特征图内部建模，难以充分捕获像素级跨特征图关系，导致知识转移不充分。因此需要一种能让学生像素直接“看”到教师全局像素关系的机制，以强化表征学习。

Method: 设计跨注意力的非局部蒸馏：对学生特征图的每个像素，与教师特征图所有像素建立注意力关系，实现像素级非局部关联建模；训练时仅引入一个额外的损失项来引导这种跨注意力匹配，无需改动主干结构。

Result: 在目标检测与图像分割上广泛实验，CanKD在性能上超过当前特征蒸馏与混合蒸馏SOTA方法。

Conclusion: 跨注意力驱动的非局部知识转移能更充分地捕获像素关系，以极小改动带来显著增益，具备作为注意力引导蒸馏新范式的潜力。

Abstract: We propose Cross-Attention-based Non-local Knowledge Distillation (CanKD), a novel feature-based knowledge distillation framework that leverages cross-attention mechanisms to enhance the knowledge transfer process. Unlike traditional self-attention-based distillation methods that align teacher and student feature maps independently, CanKD enables each pixel in the student feature map to dynamically consider all pixels in the teacher feature map. This non-local knowledge transfer more thoroughly captures pixel-wise relationships, improving feature representation learning. Our method introduces only an additional loss function to achieve superior performance compared with existing attention-guided distillation methods. Extensive experiments on object detection and image segmentation tasks demonstrate that CanKD outperforms state-of-the-art feature and hybrid distillation methods. These experimental results highlight CanKD's potential as a new paradigm for attention-guided distillation in computer vision tasks. Code is available at https://github.com/tori-hotaru/CanKD

</details>


### [96] [Generalized Design Choices for Deepfake Detectors](https://arxiv.org/abs/2511.21507)
*Lorenzo Pellegrini,Serafino Pandolfini,Davide Maltoni,Matteo Ferrara,Marco Prati,Marco Ramilli*

Main category: cs.CV

TL;DR: 论文指出深伪检测性能很大程度受实现细节（预处理、增强、优化等）影响，系统性拆解这些因素，提出与架构无关的最佳实践，并在AI-GenBench上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前检测器之间难以公平比较，原因在于训练/推理/增量更新中的实现选择差异大，且不清楚哪些因素真正贡献性能。需要建立可复现、可泛化的设计准则。

Method: 对训练、推理、与增量更新相关的多种设计选择进行系统消融：数据预处理、数据增强、优化策略、推理流程等。通过隔离变量的实验，评估各因素对准确率与泛化能力的影响，提炼架构无关的实践。

Result: 识别出一组在不同模型与设置中都能稳定提升的设计选择，并在AI-GenBench基准上取得了最先进的性能。

Conclusion: 深伪检测的关键在于细致且标准化的实现细节。遵循文中的最佳实践可提升准确率与跨域泛化，并为未来检测系统提供架构无关的设计指南。

Abstract: The effectiveness of deepfake detection methods often depends less on their core design and more on implementation details such as data preprocessing, augmentation strategies, and optimization techniques. These factors make it difficult to fairly compare detectors and to understand which factors truly contribute to their performance. To address this, we systematically investigate how different design choices influence the accuracy and generalization capabilities of deepfake detection models, focusing on aspects related to training, inference, and incremental updates. By isolating the impact of individual factors, we aim to establish robust, architecture-agnostic best practices for the design and development of future deepfake detection systems. Our experiments identify a set of design choices that consistently improve deepfake detection and enable state-of-the-art performance on the AI-GenBench benchmark.

</details>


### [97] [Self-Paced Learning for Images of Antinuclear Antibodies](https://arxiv.org/abs/2511.21519)
*Yiyang Jiang,Guangwu Qian,Jiaxin Wu,Qi Huang,Qing Li,Yongkang Wu,Xiao-Yong Wei*

Main category: cs.CV

TL;DR: 提出一个面向临床实际的多实例多标签（MIML）ANA荧光图像自动判读框架，无需手工预处理，在一个ANA数据集和三个公共医学MIML基准上取得SOTA/近SOTA表现。


<details>
  <summary>Details</summary>
Motivation: ANA检测对自身免疫病诊断关键，但人工判读耗时耗力、主观性强；ANA图像存在上百种抗体共存、图样复杂且需MIML标注，现有自动化方法难以在真实临床场景稳定有效。

Method: 受人工标注逻辑启发，端到端框架直接处理原始显微镜图像，包含三组件：1) 实例采样器：建模图样置信度并抑制低置信实例；2) 概率式伪标签分派器：依据实例可分辨性自适应聚合与分派标签；3) 自步学习权重/学习率系数：根据经验标签观测调整训练节奏与样本权重。整体支持端到端优化并处理MIML复杂性。

Result: 在自建ANA数据集上，相比最佳先前方法F1-Macro最高提升+7.0%，mAP提升+12.6%，达成SOTA；在三个公共医学MIML数据集上关键指标稳居前二，Hamming loss与one-error最高分别降低18.2%与26.9%。

Conclusion: 该框架有效解决临床场景下ANA多实例多标签判读难题，提升性能与鲁棒性，并具备端到端、无需预处理的实用优势；代码开源，便于复现与扩展。

Abstract: Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sjögren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.

</details>


### [98] [EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?](https://arxiv.org/abs/2511.21523)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 提出一种高效替代大规模遥感基础模型的方案：由多个轻量任务专家的ConvNeXtV2组成的专家集成框架，能复用、冻结、易扩展，并适配联邦训练、剪枝与持续集成，降低资源与碳足迹。


<details>
  <summary>Details</summary>
Motivation: 当前遥感基础模型多依赖“更大模型+更大数据”的扩展路线，训练与数据成本高、仅少数机构可承受，且与可持续、环保AI原则相悖，需要一种资源友好、可协作且可扩展的新范式。

Method: 将训练分解为多个轻量、任务特定的ConvNeXtV2“专家”网络；专家训练后可冻结与复用，通过模块化集成形成整体RSFM；框架天然支持联邦训练、剪枝与持续加入新专家，提升效率与可维护性。

Result: 在无需单一超大模型与巨量数据/算力的情况下，实现可跨任务泛化的遥感基础模型构建路径，显著提升效率、可解释性与可扩展性（摘要层面未给出具体数值）。

Conclusion: 专家集成式、模块化RSFM为遥感领域提供了可扩展且资源高效的基础模型新方向，契合协作与资源受限场景，并兼顾可持续性与工程可落地性。

Abstract: Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.

</details>


### [99] [The Age-specific Alzheimer 's Disease Prediction with Characteristic Constraints in Nonuniform Time Span](https://arxiv.org/abs/2511.21530)
*Xin Hong,Kaifeng Huang*

Main category: cs.CV

TL;DR: 提出一种由量化指标引导的序列MRI图像生成方法，并加入年龄缩放因子与年龄加权像素损失，用于阿尔茨海默病进展预测；在不规则时间间隔序列中保持疾病关键特征，获得较高结构相似度（SSIM=0.882）。


<details>
  <summary>Details</summary>
Motivation: 面对AD早期与个体化治疗需求，传统利用生成图像进行预测在不规则时间序列下难以准确保留进展特征，影响长期预后建模与阶段预测精度。

Method: 设计一个顺序图像生成框架：以定量指标（量化度量）作为生成约束与指导，实现跨时间步的特征保持；引入年龄缩放因子生成年龄特定的MRI；采用年龄缩放的像素损失，迭代生成更贴近真实进展的图像；通过消融实验评估各组件贡献。

Result: 加入定量指标显著提升MRI合成准确性；年龄缩放像素损失提高迭代生成质量；长期预后场景下合成图像的SSIM最高达到0.882。

Conclusion: 量化指标引导与年龄缩放策略可在不规则时间序列下稳定保留AD进展特征，提升MRI合成质量与阶段预测潜力，为个体化长期预后提供可行路径。

Abstract: Alzheimer's disease is a debilitating disorder marked by a decline in cognitive function. Timely identification of the disease is essential for the development of personalized treatment strategies that aim to mitigate its progression. The application of generated images for the prediction of Alzheimer's disease poses challenges, particularly in accurately representing the disease's characteristics when input sequences are captured at irregular time intervals. This study presents an innovative methodology for sequential image generation, guided by quantitative metrics, to maintain the essential features indicative of disease progression. Furthermore, an age-scaling factor is integrated into the process to produce age-specific MRI images, facilitating the prediction of advanced stages of the disease. The results obtained from the ablation study suggest that the inclusion of quantitative metrics significantly improves the accuracy of MRI image synthesis. Furthermore, the application of age-scaled pixel loss contributed to the enhanced iterative generation of MRI images. In terms of long-term disease prognosis, the Structural Similarity Index reached a peak value of 0.882, indicating a substantial degree of similarity in the synthesized images.

</details>


### [100] [Video Generation Models Are Good Latent Reward Models](https://arxiv.org/abs/2511.21541)
*Xiaoyue Mi,Wenqing Yu,Jiesong Lian,Shibo Jie,Ruizhe Zhong,Zijun Liu,Guozhen Zhang,Zixiang Zhou,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: 提出PRFL：在视频生成的噪声潜空间进行全程偏好优化，避免像素空间解码与后期优化限制，兼顾效率与对时序/结构的早期监督。


<details>
  <summary>Details</summary>
Motivation: 现有视频ReFL依赖像素空间的视觉语言奖励模型，只能在VAE解码后的后期去噪步骤优化，导致显存与训练时间开销大，且监督晚、更多改善画质而非运动与结构一致性。

Method: 观察到预训练视频扩散/生成模型天然处理任意时间步的噪声潜表示并保留时序信息，因此在潜空间构建“过程奖励”模型与偏好优化框架。PRFL在不进行VAE解码的情况下，对整个去噪链路进行端到端的奖赏反馈学习，支持对早期与中期步骤进行监督并反传梯度。

Result: 在多项实验中，PRFL比RGB（像素空间）ReFL显著提升与人类偏好的对齐度，同时显著降低显存占用与训练时间；还带来更好的运动动态与结构连贯性。

Conclusion: 在噪声潜空间进行过程级的奖励反馈学习可高效、全面地对齐视频生成与人类偏好，优于像素空间的后期优化范式，并提供实用的效率与质量双重收益。

Abstract: Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.

</details>


### [101] [UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes](https://arxiv.org/abs/2511.21565)
*Kang Du,Xue Liao,Junpeng Xia,Chaozheng Guo,Yi Gu,Yirui Guan,Duotun Wang,ShengHuang,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出UAVLight数据集，用可重复无人机航线在不同固定时刻采集同一场景，提供真实但受控的光照变化，以评测和促进对光照鲁棒的3D重建方法。


<details>
  <summary>Details</summary>
Motivation: 多视角3D重建在户外受日照方向、云层与阴影变化影响严重，破坏恒定光照假设，导致几何漂移、颜色不一致与阴影烙印。现有数据集要么时间窗口很短、光照多样性不足，要么跨季节跨月，几何/语义变化混入，难以单独研究光照鲁棒性。

Method: 构建UAVLight基准：同一场景沿可复现、带地理参考的无人机航线，在一天中多个固定时刻重复拍摄；保持几何、标定与视角一致，仅自然光照发生变化；并提供跨光照条件的标准化评测协议。

Result: 获得一套在真实户外环境下、具有受控光照变化且几何一致的数据与评测基准，可系统检验MVS/SfM与神经重建方法在光照变化下的稳定性。

Conclusion: UAVLight为开发和比较在户外光照变化中仍具一致性、真实性与可重光照（relightable）的3D重建方法提供可靠基础。

Abstract: Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction. Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting. This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable. However, existing datasets either restrict capture to short time windows, thus lacking meaningful illumination diversity, or span months and seasons, where geometric and semantic changes confound the isolated study of lighting robustness. We introduce UAVLight, a controlled-yet-real benchmark for illumination-robust 3D reconstruction. Each scene is captured along repeatable, geo-referenced flight paths at multiple fixed times of day, producing natural lighting variation under consistent geometry, calibration, and viewpoints. With standardized evaluation protocols across lighting conditions, UAVLight provides a reliable foundation for developing and benchmarking reconstruction methods that are consistent, faithful, and relightable in real outdoor environments.

</details>


### [102] [Multimodal Robust Prompt Distillation for 3D Point Cloud Models](https://arxiv.org/abs/2511.21574)
*Xiang Gu,Liming Lu,Xu Zheng,Anan Du,Yongbin Zhou,Shuchao Pang*

Main category: cs.CV

TL;DR: 提出MRPD：基于教师-学生与多模态提示蒸馏的高效3D点云鲁棒防御，训练期蒸馏、推理零额外开销，显著提升对白盒/黑盒攻击与干净样本表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云模型易受对抗攻击，防御方法常计算开销大、且对不同攻击泛化差，影响在安防等敏感场景的可靠性，需要一种高效且通用的鲁棒化方案。

Method: 构建教师-学生框架MRPD：学生为轻量点云模型；三位教师分别为处理深度投影的视觉模型、强性能3D模型、以及文本编码器。通过多模态对齐学习轻量“提示”（prompts），以教师鲁棒嵌入引导学生特征。引入置信门控机制，动态调节各模态在蒸馏中的权重。蒸馏仅在训练阶段进行，推理无需额外计算。

Result: 在多种白盒与黑盒攻击下，MRPD较现有防御方法取得显著优势，同时在干净数据上也有更好性能，具备更强的泛化与效率。

Conclusion: 多模态鲁棒提示蒸馏为3D点云鲁棒学习提供了实用新范式：以低开销训练换取推理阶段零开销、跨攻击类型的稳健性与性能提升。

Abstract: Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.

</details>


### [103] [Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss](https://arxiv.org/abs/2511.21575)
*Chou Mo,Yehyun Suh,J. Ryan Martin,Daniel Moyer*

Main category: cs.CV

TL;DR: 提出将2D/3D地标配准融入U‑Net训练，以提升在非标准骨盆透视姿态下的地标检测精度；比较基线U‑Net、加入姿态估计损失的训练与微调方案在变位工况中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有骨盆透视地标检测多假设固定前后位(AP)视角，实际术中常因C臂或患者体位变化而偏离，导致检测鲁棒性下降。需要一种能适应姿态变化并利用术前/术中成像几何与3D信息约束的学习框架。

Method: 在U‑Net地标回归框架中引入2D/3D地标配准：通过将预测的2D地标与已知3D解剖点进行投影-重投影一致性，定义“姿态估计损失”(Pose Estimation Loss)，用于端到端训练或在基线模型上微调；与仅用像素/热力图监督的基线进行对比评估。

Result: 在模拟真实术中、患者体位可变的条件下，加入姿态估计损失的模型在地标检测精度上优于基线；微调版本也较基线提升，具体数值未在摘要中给出。

Conclusion: 把2D/3D配准作为训练约束能显著提高在非标准视角下的鲁棒地标检测；该框架适用于术中成像中存在姿态变化的场景，优于仅依赖2D监督的U‑Net。

Abstract: Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.

</details>


### [104] [Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy](https://arxiv.org/abs/2511.21579)
*Teng Hu,Zhentao Yu,Guozhen Zhang,Zihan Su,Zhengguang Zhou,Youliang Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: 提出Harmony框架，解决生成式AI中音视频同步难题，通过跨任务协同训练、全局-局部解耦交互和同步增强CFG，实现更稳健的对齐与SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 开源音视频生成模型在音画对齐上不稳，源于联合扩散中的三大问题：噪声潜变量并行演化引发对应漂移、全局注意力难以捕捉细粒度时序线索、常规CFG偏重单模态条件而非跨模态同步。

Method: 1) 跨任务协同（音驱视频、视驱音）训练，利用强监督减少对应漂移；2) 设计全局-局部解耦交互模块，提升高效且精确的时序-风格对齐；3) 提出同步增强CFG（SyncCFG），在推理中显式分离并放大对齐信号。

Result: 大量实验显示在生成质量与细粒度音视频同步上显著超越现有方法，达成新的SOTA。

Conclusion: 通过机制性地强化跨模态同步，Harmony有效缓解联合扩散的三大瓶颈，提升对齐与保真度，为稳健的音视频生成提供通用范式。

Abstract: The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.

</details>


### [105] [Deep Learning-Based Multiclass Classification of Oral Lesions with Stratified Augmentation](https://arxiv.org/abs/2511.21582)
*Joy Naoum,Revana Salama,Ali Hamdi*

Main category: cs.CV

TL;DR: 该研究构建一个针对16类口腔病变的深度学习多分类器，通过分层划分、数据增强与过采样缓解小样本与类别不平衡，取得83.33%准确率、89.12%精确率、77.31%召回率，优于现有方法，显示其在早期口腔癌CAD中的潜力。


<details>
  <summary>Details</summary>
Motivation: 口腔癌常在晚期才被确诊，因其外观与良性或癌前病变相似；临床需要早期、可靠且可扩展的计算机辅助诊断工具以提升检测与预后。

Method: 采用深度学习多分类框架（16类）。为应对数据量少与类别不平衡：使用分层数据划分确保各类分布一致；结合高级数据增强与过采样以提升少数类表示与鲁棒性；与现有方法进行比较评估。

Result: 模型在实验中达到83.33%准确率、89.12%精确率、77.31%召回率，整体性能优于当前对比的先进方法，少数类分类表现显著改善。

Conclusion: 所提框架验证了在不平衡小数据场景中，过采样与增强策略的有效性，作为构建可靠的早期口腔癌CAD系统的初步方案具有应用前景。

Abstract: Oral cancer is highly common across the globe and is mostly diagnosed during the later stages due to the close visual similarity to benign, precancerous, and malignant lesions in the oral cavity. Implementing computer aided diagnosis systems early on has the potential to greatly improve clinical outcomes. This research intends to use deep learning to build a multiclass classifier for sixteen different oral lesions. To overcome the challenges of limited and imbalanced datasets, the proposed technique combines stratified data splitting and advanced data augmentation and oversampling to perform the classification. The experimental results, which achieved 83.33 percent accuracy, 89.12 percent precision, and 77.31 percent recall, demonstrate the superiority of the suggested model over state of the art methods now in use. The suggested model effectively conveys the effectiveness of oversampling and augmentation strategies in situations where the minority class classification performance is noteworthy. As a first step toward trustworthy computer aided diagnostic systems for the early detection of oral cancer in clinical settings, the suggested framework shows promise.

</details>


### [106] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN 是一种面向“运动质量”的后训练框架，在不依赖奖励模型或人类偏好数据的前提下，显著提升视频扩散模型的时序一致性与运动真实感，同时保持图像保真与生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽有较高单帧保真度，但常出现抖动、重影、物理不合理等运动问题。根因在于标准去噪 MSE 目标函数缺乏对时序一致性的直接监督，模型可在低损失下仍生成差的运动。需要一种直接针对“运动”的训练信号与机制，以纠正运动质量而不牺牲视觉质量或推理效率。

Method: 在一个3步蒸馏的视频扩散基座上进行后训练：1) 训练一个基于 DiT 的光流判别器，输入真实与生成视频的光流（或与运动相关表征），学习区分真实与生成的运动分布，从而为生成模型提供针对“运动”的对抗性监督；2) 结合分布匹配正则项，约束生成分布与教师/数据分布接近，以保持美学和图像细节保真；整体形成无需奖励模型/人偏的运动中心化对抗后训练框架。

Result: 在 Wan2.1-T2V-1.3B 上，MoGAN 在多基准上显著提升运动质量：VBench 上对比 50 步教师提升 +7.3%，对比 3 步 DMD 提升 +13.3%；VideoJAM-Bench 上相对教师 +7.4%、相对 DMD +8.8%；同时美学与图像质量维持相当或更好。人评中，MoGAN 在运动质量偏好上优于教师（52% vs 38%）与 DMD（56% vs 29%）。

Conclusion: MoGAN 通过引入光流判别器的对抗监督与分布匹配正则，在保持三步高效生成的同时显著提升运动真实感与时序一致性，提供了一条无需人偏或奖励模型的实用路径，朝向快速且高质量的视频生成。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [107] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 提出一种自提示、仅点监督的方法将SAM适配遥感图像：通过Refine-Requery-Reinforce循环，从稀疏点生成伪掩码、用自构框提示改进、并进行跨迭代嵌入对齐以减弱偏置；无需完整掩码，逐步提升分割质量与域鲁棒性，在WHU、HRSID、NWPU VHR-10上优于预训练SAM与近期点监督方法。


<details>
  <summary>Details</summary>
Motivation: SAM在自然图像上泛化强，但在遥感图像上因域偏移与稠密标注稀缺而表现欠佳；需要一种低成本监督方式在遥感域高效适配基础分割模型。

Method: 提出自提示的点监督框架：1) Refine：用初始点生成粗伪掩码；2) Requery：基于伪掩码自构框提示再次查询以细化结果；3) Reinforce：跨迭代对齐/约束嵌入，降低确认偏差，逐步提升；全流程不依赖完整掩码，仅用点标注与自生成提示完成适配。

Result: 在三项遥感基准（WHU、HRSID、NWPU VHR-10）上，方法持续超越预训练SAM和最新点监督分割算法（文中未给出具体数值，但强调一致领先）。

Conclusion: 自提示与语义对齐可在仅点级监督下有效适配基础分割模型到遥感域，提升分割质量与域鲁棒性，具备可扩展性与低标注成本。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [108] [Active Learning for GCN-based Action Recognition](https://arxiv.org/abs/2511.21625)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出一种标签高效的GCN用于骨架动作识别：用对抗式主动学习选取小而信息丰富的样本，并设计双向且稳定的GCN以更好地连接原始与潜在空间；在两大基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GCN在骨架动作识别上效果好但依赖大量标注，现实中标注稀缺且昂贵，需要在少标注条件下仍能取得高性能的方法。

Method: (1) 设计对抗式获取函数用于主动选择样本，兼顾代表性、多样性与不确定性，从未标注池中挑出紧凑而信息量高的样本进行标注；(2) 提出双向且稳定的GCN结构，实现原始空间与潜在空间之间的更有效映射，从而更好建模并解释被选样本的分布。

Result: 在两个具有挑战性的骨架动作识别基准上进行广泛评测，所提的标签高效GCN取得显著优于以往方法的性能提升。

Conclusion: 通过对抗式样本选择与双向稳定GCN架构的结合，可在有限标注条件下显著提升骨架动作识别性能，验证了所提框架的有效性。

Abstract: Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.

</details>


### [109] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL 是一款多模态大模型，支持最长 256K 交错上下文，在文本、图像与视频任务上取得领先成绩，并提供多规模密集与MoE变体以平衡质量与延迟。其三大能力支柱：更强纯文本理解、稳健的长上下文多模态理解、先进的多模态推理。架构上引入增强的交错-MRoPE、DeepStack 跨层视觉特征融合、以及文本化时间对齐以实现更精准的视频时序定位。综合而言，在相同算力/延迟预算下优于同类模型，面向图像推理、智能体决策与多模态代码智能等实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在三个方面存在瓶颈：1) 纯文本理解往往弱于专用文本基座；2) 长上下文尤其是跨文本/图像/视频的交错输入保真度与检索/引用能力不足；3) 跨图像与视频的空间-时间建模与复杂数学/工具化任务推理仍有差距。作者希望在同等token预算与延迟下，实现更强的泛化、多模态对齐与时序 grounding，使其可作为通用多模态引擎服务实际工作流。

Method: 提出 Qwen3-VL 模型族：包含密集(2B/4B/8B/32B)与MoE(30B-A3B/235B-A22B)版本；原生支持256K交错上下文。关键技术：1) 增强的 interleaved-MRoPE，用于更强的跨图像/视频空间-时间相对位置建模；2) DeepStack，将多层ViT特征有效整合以加强视觉-语言对齐；3) 文本化时间对齐，从T-RoPE演进到显式文本时间戳对齐，提升视频时序定位精度。并在相同token/延迟约束下优化训练与推理。

Result: 在多模态综合评测（如 MMMU、MathVista、MathVision 等视觉数学基准）上达到领先；在纯文本理解上在若干场景超过同规模文本-only基座；长上下文（256K）下能更好地保持、检索和跨引用文档与视频内容。密集与MoE架构在相同预算与延迟下均优于对比模型。

Conclusion: Qwen3-VL 实现了强纯文本、长上下文与高级多模态推理三大支柱能力，得益于改进的位置编码、跨层视觉融合与文本化时序对齐。其作为基础引擎，可用于图像驱动推理、智能体决策与多模态代码智能等真实世界场景。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [110] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出一套“服务器蒸馏 + 端侧原型分类”的少样本纠错系统，在移动设备上通过更新原型而非重训模型，实现对AI误判的高效修正；一/少样本即可纠错，计算与存储开销极低。


<details>
  <summary>Details</summary>
Motivation: 端侧AI广泛部署但误判会严重影响体验；现有方法多做错误检测而缺乏高效纠错，且移动设备资源受限，难以频繁重训或大规模更新模型。需要一种既能快速纠错、又兼顾资源受限与遗忘问题的方案。

Method: 两段式架构：1) 服务器端以基础模型为教师，通过知识蒸馏训练轻量、设备可用的特征抽取器，获得鲁棒表示；2) 设备端采用基于原型的分类/检测，发生误判时由用户提供少量标注样本，在线更新类别原型（而非重训网络）以实现超轻量纠错；支持图像分类与目标检测。

Result: 在Food-101与Flowers-102上，一次性（一样本）即可纠正50%以上错误；遗忘极低（<0.02%）；端侧计算与存储开销可忽略；提供Android演示App验证实际可用性。

Conclusion: 通过“蒸馏得到强表征 + 原型自适应”的组合，可在资源受限设备上以极低成本实现对误分类的快速纠错，兼顾精度提升与稳定性，具备现实部署价值。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [111] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: 提出CaFlow用于长时序动作质量评估，通过因果反事实正则与双向时间条件流建模，缓解混杂与单向时序缺陷，在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 长时程AQA需建模长期动态并对上下文混杂鲁棒；现有方法或依赖高成本标注，或仅单向时间建模，易受伪相关影响且表征不稳定。

Method: 统一框架CaFlow包含两部分：1) Causal Counterfactual Regularization（CCR）自监督地解耦因果与混杂特征，并通过反事实干预施加因果鲁棒性；2) BiT-Flow（双向时间条件流）同时建模前向与后向动态，并用循环一致性约束保证表征平滑与连贯。

Result: 在多个长时AQA基准上取得最新最优性能（SOTA），验证了对长时序与混杂鲁棒性的改进。

Conclusion: 结合因果反事实去混杂与双向时间流的CaFlow能稳定捕获长期动作动态、降低伪相关影响，适用于体育、康复、技能评估等长时序AQA场景。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [112] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: 提出Multi-Crit基准，系统评估大规模多模态模型作为“评审”在多元细粒度评价准则下的遵循能力，并给出新指标与广泛实证分析，发现专有与开源模型在多准则一致性与灵活性上仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态评审依赖LMMs，但其能否准确、稳定地遵循多元且细粒度的评价准则（而非单一“整体好坏”）尚不清楚；需要一个标准化数据与指标体系来检验与比较这类“多准则遵循”能力。

Method: 构建Multi-Crit基准：1) 通过严格的数据管线收集具挑战性的响应对，并由人工按多维准则标注；2) 覆盖开放式生成与可验证推理两类任务；3) 设计三项新指标，分别衡量多元准则遵循度、准则切换灵活性、以及识别准则层面偏好冲突的能力；4) 在25个LMM上进行系统评测，并开展关于critic微调、推理微调、测试时扩展与开源/闭源边界一致性的附加分析。

Result: 大规模实验显示：(1) 专有模型在开放式评审场景中仍难以保持对多元准则的稳定一致遵循；(2) 开源模型在多准则灵活遵循方面落后更明显；(3) 使用整体性评审信号进行critic微调可提升视觉对齐/落地，但无法迁移到细粒度多准则判断。

Conclusion: Multi-Crit为构建可靠、可调度的多模态评审奠定基础，揭示当前LMM评审在多准则一致性、灵活切换与冲突识别方面的局限，并提供衡量与改进路径。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [113] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: 提出ADVLA：在VLA模型中直接对视觉编码器投射到文本空间的特征进行扰动，在L∞=4/255且Top-K掩码下，仅修改<10%补丁即可接近100%攻击成功，迭代约0.06s，几乎不可见，优于补丁式攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对VLA的对抗攻击多需端到端昂贵训练、且扰动显眼（补丁），难以在低幅度和稀疏条件下有效破坏下游动作决策，缺乏针对VLA跨模态特征空间的高效隐蔽攻击。

Method: 提出ADVLA：在视觉编码器输出并投射到文本特征空间的表征上施加对抗扰动；引入注意力引导使扰动集中且稀疏；设计三种策略分别提升敏感性、施加稀疏约束、聚焦关键区域；结合Top-K掩码在关键patch上注入低幅度扰动；采用快速单步迭代以提升效率。

Result: 在L∞=4/255下，ADVLA+Top-K仅修改<10%图像patch即可实现近100%攻击成功率；扰动集中在关键区域、全局几乎不可感知；单步迭代约0.06秒，显著快于传统补丁攻击。

Conclusion: ADVLA在低幅度、局部稀疏条件下有效削弱VLA下游动作预测，避免端到端训练和显眼补丁，展示了针对VLA特征空间攻击的独特有效性与应用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [114] [Revolutionizing Glioma Segmentation & Grading Using 3D MRI - Guided Hybrid Deep Learning Models](https://arxiv.org/abs/2511.21673)
*Pandiyaraju V,Sreya Mynampati,Abishek Karthik,Poovarasan L,D. Saraswathi*

Main category: cs.CV

TL;DR: 提出一个混合深度学习框架：3D MRI 上用U‑Net分割肿瘤，再用带多头与通道-空间注意力的DenseNet+VGG双分支做分类；报告分割Dice 98%、分类准确率99%，优于传统CNN与无注意力方法。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤致死率高，临床需要早期、准确的定位与分级；现有方法在3D数据利用、肿瘤区域关注和可解释性方面不足，需要端到端地同时提升分割精度、分类准确度与可解释性。

Method: 1) 预处理3D MRI：归一化、重采样、数据增强。2) 分割：U‑Net基于空间与上下文信息精确勾画肿瘤体素。3) 分类：以分割掩膜引导的ROI作为输入，采用DenseNet与VGG双分支特征提取，融合多头注意力与通道-空间注意力强化临床相关特征，最终输出分级/分类。4) 评估：分割用Dice、mIoU；分类用Accuracy、Precision、Recall、F1。

Result: 物理实验显示：分割Dice达98%，分类准确率达99%，均优于传统CNN与无注意力基线；注意力机制提升对临床关键区域的关注度和可解释性。

Conclusion: 该混合框架在胶质瘤分割与分级上表现出色，具有临床辅助诊断潜力，可支持更及时、可靠的决策与治疗规划；未来需在更大、多中心数据集上验证其泛化与稳健性。

Abstract: Gliomas are brain tumor types that have a high mortality rate which means early and accurate diagnosis is important for therapeutic intervention for the tumors. To address this difficulty, the proposed research will develop a hybrid deep learning model which integrates U-Net based segmentation and a hybrid DenseNet-VGG classification network with multihead attention and spatial-channel attention capabilities. The segmentation model will precisely demarcate the tumors in a 3D volume of MRI data guided by spatial and contextual information. The classification network which combines a branch of both DenseNet and VGG, will incorporate the demarcated tumor on which features with attention mechanisms would be focused on clinically relevant features. High-dimensional 3D MRI data could successfully be utilized in the model through preprocessing steps which are normalization, resampling, and data augmentation. Through a variety of measures the framework is evaluated: measures of performance in segmentation are Dice coefficient and Mean Intersection over Union (IoU) and measures of performance in classification are accuracy precision, recall, and F1-score. The hybrid framework that has been proposed has demonstrated through physical testing that it has the capability of obtaining a Dice coefficient of 98% in tumor segmentation, and 99% on classification accuracy, outperforming traditional CNN models and attention-free methods. Utilizing multi-head attention mechanisms enhances notions of priority in aspects of the tumor that are clinically significant, and enhances interpretability and accuracy. The results suggest a great potential of the framework in facilitating the timely and reliable diagnosis and grading of glioma by clinicians is promising, allowing for better planning of patient treatment.

</details>


### [115] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 提出CamFormer：将相机位姿轨迹编码并与自然语言对齐，发现仅凭“如何移动”的相机轨迹即可强有力地推断视频“在做什么/在看什么”，并在多任务上验证其跨模态、分类与时间分析能力且对不同位姿估计方法鲁棒。


<details>
  <summary>Details</summary>
Motivation: 检验一个看似不可能的问题：不看像素，仅从相机运动轨迹能否感知视频内容？同时希望将这种轻量信号作为稳健的感知模态，补充或替代传统基于像素的方法。

Method: 构建对比学习框架，训练专用编码器CamFormer，将相机位姿轨迹映射到与自然语言共享的嵌入空间。利用不同来源的相机位姿（多传感器高精度与仅RGB估计）进行训练/评估，并在多种下游任务上测试（跨模态对齐、分类、时间分析）。

Result: CamFormer学到的轨迹嵌入能有效揭示视频语义：相机“如何移动”可对应“在做什么/观察什么”。在多种下游任务上表现良好，并且对不同位姿估计管线具有鲁棒性。

Conclusion: 相机轨迹是轻量、鲁棒且通用的感知模态；通过CamFormer与语言对齐，可无需像素也能感知视频内容，并能广泛用于跨模态与时序理解任务。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [116] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出G^2VLM：将3D几何重建与VLM空间理解统一，利用学习到的3D几何特征提升3D属性预测与空间推理，达SOTA级重建与理解表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在空间智能上不稳健，主要缺乏从2D到3D的几何学习过程，难以进行可靠的3D理解与推理；需要一种兼具语义与几何能力的统一框架。

Method: 构建几何扎根的VLM（G^2VLM）：在多视图图像与视频上训练，使模型“原生”学习3D视觉几何特征；这些几何特征用于直接预测3D属性，并通过in-context learning与交错式推理提升空间理解。设计同时引入来自传统3D视觉先验（通常依赖难收集标注）的好处，实现统一与可扩展的训练。

Result: 在两类任务上均取得强绩效：3D重建方面达到与SOTA前馈重建模型可比的结果；在空间理解与推理基准上优于或具竞争力于现有方法。

Conclusion: 将语义强的VLM与低层3D视觉任务统一，可作为社区强基线，增强空间推理与3D属性预测，并为3D场景编辑等应用铺路。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [117] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出Canvas-to-Image：把文本、参照主体、空间布局、姿态、标注等多种控制信号统一编码成一张“画布”，并通过多任务联合训练，让扩散模型在单一框架中进行综合视觉-空间推理，显著提升身份保持与控制遵从度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在单一或少量控制下表现好，但在同时满足文本、身份、姿态、布局等多模态/多约束时易冲突或失真，常依赖任务专用启发式，泛化差。需要一个统一接口与训练范式，实现高保真组合控制与多控制泛化。

Method: 将异构控制（文本提示、参考图像/身份、空间/布局、姿态、标注等）编码为一张复合“画布”图像，作为模型可直接解释的输入；构建多任务数据集并提出Multi-Task Canvas Training，使扩散模型在统一学习框架下联合理解并融合多种控制信号，从而进行端到端的文本到图像生成与多控制推理。

Result: 在多人物合成、姿态控制合成、布局约束生成及多控制组合等基准上，相比SOTA显著提升身份保持与控制遵从度；在推理阶段对多控制场景具有良好泛化。

Conclusion: 统一的画布表示与多任务联合训练可有效整合异构控制，增强扩散模型的综合视觉-空间推理能力，实现高保真、多约束一致的图像生成，并优于现有方法。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>
