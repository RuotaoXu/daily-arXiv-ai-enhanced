<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: 论文针对涵洞/下水道缺陷分割在标注数据稀缺情况下的表现，提出数据增强与标签注入预处理、轻量化新架构FORTRESS，以及少样本分割方法三条路径，在IoU与F1上显著提升并降参降算。


<details>
  <summary>Details</summary>
Motivation: 排水系统管涵失效风险高，但该领域数据采集与标注昂贵且需专业知识，难以获得大规模数据，现有分割方法在小数据场景下表现受限，需要提升在数据稀缺条件下的缺陷分割精度与效率。

Method: 三方面：1) 训练前的数据层面：传统数据增强+动态标签注入以扩充有效样本与监督信号；2) 模型层面：提出FORTRESS，将深度可分离卷积、可自适应KAN（Kolmogorov-Arnold Networks）与多尺度注意力融合，兼顾表达力与轻量化；3) 学习范式：研究少样本语义分割，引入带注意力的双向原型网络以获得更判别的特征原型。

Result: 在涵洞下水道缺陷数据集上，预处理策略显著提升IoU与F1；FORTRESS在保持或提升SOTA精度的同时大幅减少可训练参数与计算量；少样本分割方案在多项指标上达到令人满意的表现，验证小样本可行性。

Conclusion: 通过数据增强/标签注入、FORTRESS轻量架构与少样本原型网络三管齐下，有效缓解数据稀缺导致的性能瓶颈，实现高精度、低计算成本的缺陷分割；方法具有实际工程应用潜力。

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 论文系统评估多模态大语言模型（MLLMs）在异质人脸识别（HFR：跨VIS/NIR/SWIR/热成像）上的能力，发现与传统人脸识别方法相比存在显著性能差距，尤其在跨光谱条件下，提示当前MLLMs尚不适合直接用于严苛的生物识别应用。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉-语言任务上表现强，但其在高要求的生物识别应用（特别是跨模态/跨光谱的人脸识别）中的实用性尚不清楚。为避免盲目部署，需要用标准生物识别协议和指标对其进行系统性、可比性的评估。

Method: 选取多种开源MLLMs，构建多个跨模态HFR场景（VIS-NIR、VIS-SWIR、VIS-THERMAL），采用生物识别评测协议，计算Acquire Rate、EER、TAR等指标，对比其识别性能；并与经典人脸识别系统进行基准对照。

Result: 在多个跨模态设置下，MLLMs的识别性能明显落后于传统方法，尤其在跨光谱（如VIS-THERMAL、VIS-SWIR）条件下表现不佳；多项指标（EER较高、TAR较低）显示其稳定性与可靠性不足。

Conclusion: 现阶段MLLMs并不适合直接用于异质人脸识别等严苛的生物识别任务；部署前应进行严格的生物识别评估，并继续研究提升其跨光谱鲁棒性的方法或与传统专用模型结合的混合方案。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE 是一种错误感知课程学习框架，用于微调医学视觉-语言模型，在不增加数据的前提下提升放射学报告的可视化定位与事实一致性；通过动态采样与多种基于解剖/短语的定位任务，显著提高定位 IoU、报告质量并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型生成的放射学报告常出现文本与图像证据错配，空间定位不准、事实不一致与幻觉问题，削弱临床可靠性；需要一种无需额外数据、能系统提升视觉对齐与报告可信度的方法。

Method: 提出 CURE：在多模态指令模型上进行三类任务的微调——短语定位（phrase grounding）、有证据支撑的报告生成（grounded report generation）以及基于解剖结构约束的报告生成（anatomy-grounded report）。采用错误感知的课程学习：根据模型在训练样本上的表现动态调整采样权重，逐步聚焦困难样本与错误类型，以强化空间-文本对齐与事实一致性；使用公共数据集，无需新增标注。

Result: 与基线相比：定位精度提升 +0.37 IoU；报告质量指标 CXRFEScore 提升 +0.188；幻觉率降低 18.6%。代码与权重已开源。

Conclusion: CURE 在不增加数据的情况下，通过错误感知的课程学习与多粒度的定位/解剖约束训练，显著提升医学 VLM 的可视化定位与报告可靠性，具备数据效率高、可复用和实际部署潜力。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 提出DuFal框架，通过频域+空域双路径处理与高频增强FNO，实现稀疏视角CBCT高频细节重建并显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角CBCT因投影不足导致高频（精细解剖）信息欠采样，传统CNN偏向低频学习，难以恢复细节。需要能同时捕获全局频率模式与局部空间细节的方法。

Method: 提出DuFal双频感知学习：频域与空域双路径。核心为高-局部因子化Fourier Neural Operator（FNO），包含两支：1) 全局高频增强FNO捕获全局频率模式；2) 局部高频增强FNO在分块patch上操作保留空间局部性。引入谱-通道因子化以降参；设计跨注意力频率融合模块整合空间与频率特征；经特征解码器得到投影表征，再经强度场解码重建最终CT体数据。

Result: 在LUNA16与ToothFairy数据集上，尤其在极端稀疏视角条件下，DuFal在高频解剖结构保真与整体重建质量上显著优于现有SOTA。

Conclusion: 双路径频-空融合与高频增强FNO能有效弥补稀疏视角CBCT的高频细节缺失，兼顾全局频率与局部空间特性，参数更高效并在多数据集上验证了优越性。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 提出一种偏差引导的提示学习框架，将可学习提示与统计偏差评分结合，在少样本正常数据下实现更强的异常分割与定位。


<details>
  <summary>Details</summary>
Motivation: FNSAD仅有少量正常样本，异常形态多样且监督不足；现有基于CLIP的提示法对正常/异常提示区分度弱，且缺乏稳健的patch级异常评分。

Method: 1) 将固定前缀提示替换为共享的可学习上下文向量，并为异常类别引入专属后缀token以实现类感知对齐；2) 设计偏差损失与Top-K多实例学习：把patch特征视为相对正常分布的高斯偏差，对统计显著偏离的patch赋予更高异常分数；3) 端到端联合优化提示与偏差评分。

Result: 在MVTecAD与VISA上，像素级检测优于PromptAD等基线；消融显示可学习提示、偏差评分与Top-K MIL均带来稳定提升。

Conclusion: 语义对齐与统计偏差建模的结合提升了少正常样本场景下的异常可分性与定位性，具备可解释的patch级评分并在主流基准上达到SOTA。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 提出一个基于传感器物理的统一NeRF框架，从单次曝光的模糊LDR图像与事件数据重建清晰HDR的三维场景并进行新视角合成。通过在HDR域直接建模场景辐射，并引入像素级RGB映射与事件映射场，将物理辐射与传感器输出对齐，联合优化以提升去模糊与HDR效果，达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有从模糊LDR图像进行新视角合成的方法在强光对比与运动模糊下难以获得清晰HDR的3D表征。虽然引入事件相机可缓解，但忽视了相机输出与真实场景辐射之间的物理失配，导致HDR重建与去模糊效果欠佳。

Method: 1) 用NeRF在HDR域直接表示3D场景真实辐射，并按物理成像过程建模射线到像素的曝光累积；2) 设计像素级RGB映射场，将渲染的HDR像素映射到传感器记录的LDR值，实现相机响应/压缩对齐；3) 设计事件映射场，将物理场景的时空强度变化与实际事件传感器输出对应起来；4) 将上述两个映射场与NeRF联合优化，利用事件的高时间分辨率增强时空动态与清晰HDR表示学习。

Result: 在自采与公开数据集上，方法在去模糊的HDR新视角合成上取得SOTA，显示出对极端光照与运动模糊的鲁棒性，提升HDR重建质量与几何/外观清晰度。

Conclusion: 将传感器物理引入NeRF并联合学习RGB与事件的映射，可有效弥合物理辐射与传感器输出的鸿沟，利用事件的时序信息获得清晰HDR的3D表示与新视角合成，优于以往忽视物理失配的方案。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 该研究用Vision Transformer替换属性中和框架中的U-Net编码器，以减少胸片中性别/年龄泄漏，同时保持诊断性能；在中等编辑强度下显著降低性别识别AUC至≈0.80，疾病ROC AUC基本不降（≤5%），提示自注意力模型能更公平且实用。


<details>
  <summary>Details</summary>
Motivation: 胸片分类器常利用与性别、年龄相关的“捷径”，导致少数群体系统性漏诊。现有基于卷积编码器的像素空间属性中和方法在临床可用的编辑强度下仍有属性泄漏。作者想探索更强的全局表征（自注意力）是否能更有效地抑制人口属性泄漏，同时不牺牲诊断准确性。

Method: 在Attribute-Neutral Framework中用Vision Transformer（DeiT-S）替换U-Net的卷积编码器，训练于ChestX-ray14数据集；对图像按11个编辑强度生成中和图；用独立AI评审器评估人口属性（性别）泄漏AUC，并用ConvNet评估15种疾病的预测性能（宏ROC AUC与最差子组AUC）。对比同框架下卷积U-Net版本与未编辑基线。

Result: 在中等编辑强度alpha=0.5时：ViT中和器将性别识别AUC降至≈0.80，比卷积U-Net版本低约10个百分点，且训练轮次仅为其一半；同时15项发现的宏ROC AUC与未编辑基线相比下降不超过5个百分点，最差子组AUC仍约0.70。

Conclusion: 替换为具全局自注意力的ViT骨干可在保持临床诊断效用的同时进一步压制人口属性泄漏，为构建更公平的胸片AI提供了可行路径。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 提出LASAGNA框架：在一次生成中同时输出可合成的分层结果（写实背景+带透明度且含物理效果的前景），支持多种条件控制，并配套新数据集LASAGNA-48K与评测基准LASAGNABENCH。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成/编辑在对已有图像的局部元素可控编辑、跨层一致性与合成关系上不足，且对象层缺乏真实阴影/反射等物理视觉效果，限制了真实场景应用与后期编辑。

Method: 构建统一分层生成框架LASAGNA：一次性学习与输出背景层与RGBA前景层（包含阴影、反射等物理效果），可在多条件（文本、前景、背景、位置掩码）下高效学习正确的图像合成关系；同时发布LASAGNA-48K数据集（干净背景+具物理效果的前景）与层编辑基准LASAGNABENCH。

Result: 在多层同时生成与编辑任务中，实现更高的一致性与连贯性，能保持身份与视觉效果，并展现更强的可控性与可编辑性，相比既有方法更稳定可靠。

Conclusion: LASAGNA实现了可控、连贯的分层图像生成与后期编辑能力，填补了真实视觉效果与合成关系学习的空白，并通过数据集与基准促进开放研究；项目与资源将公开。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 提出利用手背皮肤形变信号与双流“差分编码器”学习手指姿态，在遮挡严重的第一视角场景显著降低关节角误差并缩小模型规模。


<details>
  <summary>Details</summary>
Motivation: XR设备普及下，第一视角手势估计常受手指自遮挡影响，传统方法依赖完整几何或大模型，难以鲁棒且代价高。作者注意到手背皮肤的细微形变蕴含姿态与用力信息，但以往未充分利用。

Method: 仅使用裁剪的手背图像，构建双流delta encoder：一支提取动态手当前特征，另一支提取“放松基线”特征，通过对比二者的差分来回归三维关节角；借助密集视觉表征（如预训练特征）提升对皮肤纹理与形变的敏感度。

Result: 在手指≥50%被遮挡的自遮挡场景中，较依赖全手几何与大骨干网络的SOTA方法，MPJAE降低18%。在下游任务（食指捏合、点击估计）中可靠性提升；模型更小。

Conclusion: 手背皮肤形变结合差分学习可在严重遮挡下提升自中心手势估计精度并降低计算开销，同时支持新交互，如无可见运动的等长用力“点击”。

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出VIOLA，一种在极少标注下将多模态大模型适配到新视频域的ICL框架：用密度-不确定性加权采样挑选少量高价值标注样本，并以置信度感知的检索与提示融合大量未标注数据与伪标签，跨9个基准与4种MLLM显著优于低资源基线。


<details>
  <summary>Details</summary>
Motivation: 现实应用（工业/手术等）中视频标注昂贵且稀缺，常规ICL需要大规模标注池，不切实际；需在极低标注预算下，仍能让MLLM对新视频域可靠泛化，同时避免伪标签噪声放大。

Method: 1) 密度-不确定性加权采样：结合样本密度估计与不确定性/多样性，优先选择既具代表性又信息量高且非离群的样本供专家标注。2) 构建“混合池”（含少量真标注与大量伪标签/未标注），引入置信度感知检索与提示：以相似度×置信度的复合分数检索演示样本，并在提示中显式区分“已验证真值”与“可能含噪伪标签”，引导MLLM自适应权衡可信度。

Result: 在9个不同基准、4种MLLM上，在低资源设定下显著超过多种强基线；在极小标注成本下实现稳健适配（定量指标未给出，但描述为显著提升与鲁棒性更强）。

Conclusion: 通过将少量高价值人类标注与大规模未标注数据有效融合，并用置信度建模抑制噪声传播，VIOLA在视频域迁移的训练免调适配场景中大幅提升MLLM表现，适合标注成本高的专用场景。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [11] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: 论文针对类条件DDPM在单一细粒度人脸域（K-pop偶像，32×32）中的语义可控性评估难题，提出相对分类准确率RCA指标，并发现视觉质量高但身份一致性差的显著权衡（FID 8.93 vs. RCA 0.27），揭示分辨率与性别内相似导致的模式坍塌。


<details>
  <summary>Details</summary>
Motivation: 现有生成质量指标（FID、IS）在高相似度、细粒度身份生成任务中难以反映“身份对齐/可控性”，容易忽视语义模式坍塌。需要一个能与域内分类基线对齐、可解释且可比较的度量来评估条件生成模型的身份一致性。

Method: 在K-pop偶像面孔（32×32，类条件）上训练DDPM，并提出校准指标RCA：将生成样本的分类准确率相对“神谕”分类器的基线进行归一化。通过混淆矩阵分析失败模式，并从分辨率限制与性别内高相似度维度解释错误来源。

Result: 模型在视觉质量上表现良好（FID 8.93），但在语义可控性上表现不佳（RCA 0.27），显示严重的语义模式坍塌，尤其在视觉上易混淆的身份间。混淆矩阵进一步表明错误集中在跨相近外观类别与性别内模糊处。

Conclusion: RCA作为校准度量能有效揭示条件生成中的身份一致性问题；仅靠FID/IS会掩盖细粒度语义失真。低分辨率与类内相似是失效主因，框架为后续提升分辨率、改进条件信号与辨别器辅助评估提供了标准。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [12] [Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2601.15615)
*Weiwei Wu,Yueyang Li,Yuhu Shi,Weiming Zeng,Lang Qin,Yang Yang,Ke Zhou,Zhiguo Zhang,Wai Ting Siok,Nizhuan Wang*

Main category: cs.CV

TL;DR: 提出RSM-CoDG框架，以脑区先验的区域级空间表示+多尺度时间建模+协同域泛化来缓解跨被试EEG情绪识别中的分布偏移与主体偏差，实验在SEED系列显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG情绪识别受强烈个体差异与复杂时空神经动态影响，导致分布偏移与难以对齐的表示。现有方法多仅在空间、时间或泛化某一方面改进，缺乏统一框架来同时对齐跨被试表示、捕获多尺度动态并抑制主体特异性偏差。

Method: 构建RSM-CoDG框架：1) 融合神经科学先验的功能脑区划分，生成区域级空间表示以提升跨被试可比性；2) 多尺度时间建模表征情绪诱发神经活动的动态演化；3) 协同域泛化（CoDG）策略，在完全未见目标被试设定下引入多维度约束，协同降低主体特异性偏差、提升到未知个体的泛化。

Result: 在SEED系列数据集上进行大量实验，RSM-CoDG在跨被试情绪识别任务中持续优于现有竞争方法，表现出更强鲁棒性与泛化能力。

Conclusion: 结合脑区先验、时空多尺度建模与协同域泛化，可有效缓解跨被试EEG的分布偏移与主体偏差，提升情绪识别鲁棒性；RSM-CoDG为跨被试EER提供了有效统一方案，代码已开源。

Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.

</details>


### [13] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: 提出一种基于自混合图像的自动化链式思维数据生成与强化学习增强的多模态大模型深伪检测框架，在降低标注成本的同时提升跨数据集泛化，达到与SOTA相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测方法缺乏可解释输出；MLLM 有望提供可解释性但受限于高质量细粒度伪造归因文本标注稀缺和昂贵；同时，RL 在视觉任务上被证明可提升跨域泛化。作者希望在低标注成本下，将主流 MLLM 引入可解释深伪检测并检验 RL 的增益。

Method: 1) 设计基于 Self-Blended Images (SBI) 的自动化 Chain-of-Thought (CoT) 数据生成框架：通过自混合生成可控伪造样本，并自动产出逐步推理与伪造归因文本；2) 构建强化学习增强的检测框架：为 MLLM 设定定制化奖励（正确判别、定位/归因、语言质量等），并采用反馈驱动的合成数据生成与策略更新；3) 以主流 MLLM 为骨干，结合上述合成 CoT 数据和 RL 训练。

Result: 在多项跨数据集基准上取得与当前 SOTA 可比的性能；消融证明 CoT 数据构建流程、定制奖励机制以及反馈驱动的合成数据生成均有效；在解释性与跨域泛化方面表现突出。

Conclusion: 自动化 CoT 数据 + RL 能有效降低标注成本并提升 MLLM 在深伪检测中的可解释性与泛化；所提框架为在缺乏细粒度标注场景下将 MLLM 应用于可解释深伪检测提供了可行路径，实现在多个基准上的竞争性成绩。

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [14] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: 提出“持续全景感知”（CPP），将多模态+多任务的持续学习统一，包含跨模态编码、可塑知识继承、跨模态一致性与无样本回放伪标签机制，在多数据集与多种CL任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习多集中于单任务，难以处理多任务与多模态场景；除了遗忘，还存在跨模态语义对齐混淆，导致增量训练性能严重下降。需要一种统一框架在像素、实例、图像层面进行联合理解并稳健增量更新。

Method: 提出CPP端到端框架：1) 协作式跨模态编码器（CCE）做多模态嵌入；2) 可塑知识继承模块，结合对比式特征蒸馏与实例蒸馏，从任务交互角度缓解遗忘；3) 跨模态一致性约束，扩展为CPP+以在多任务增量下维持语义对齐；4) 非对称伪标签策略，在无样本回放下实现模型演进。

Result: 在多模态数据集和多种CL设定（尤其细粒度CL任务）中，模型取得优于现有方法的性能，表现出更强的稳定-可塑权衡与对齐鲁棒性。

Conclusion: CPP将多模态与多任务CL统一到全景感知上，通过跨模态编码、蒸馏与一致性约束以及无回放伪标签，有效缓解遗忘与语义混淆，并在广泛实验中验证了其优越性。

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [15] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: SuperOcc提出基于超二次体的稀疏3D占用预测框架，结合时序建模、多超二次体解码和高效体素投影，兼顾精度与效率并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有占用预测多用稠密体素表示，忽视道路场景的稀疏性；新兴的超二次体表示虽稀疏但存在时序建模不足、查询稀疏性与几何表达力的权衡困难、以及从超二次体到体素投影效率低的问题。

Method: 提出SuperOcc：1) 联合视角中心与目标中心的时序建模机制；2) 多超二次体解码以在不增加查询密度的前提下提升几何表达；3) 高效的超二次体到体素的splatting方案，降低计算开销。

Result: 在SurroundOcc与Occ3D基准上取得SOTA，同时保持更高计算效率。

Conclusion: 稀疏的超二次体表示配合有效时序、解码与投影设计，可在3D占用预测中实现更佳的精度-效率权衡；SuperOcc验证了该路线的有效性并提供代码复现。

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [16] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: 提出Event-VStream：把连续视频流转化为离散且语义一致的“事件”，仅在事件边界进行解码与生成，并把事件嵌入写入持久记忆，从而在长时程实时理解中兼顾低延迟与长程推理。


<details>
  <summary>Details</summary>
Motivation: 现有VLM实时视频理解需处理大量冗余帧且易遗忘长程上下文。主流方法用固定间隔解码或剪枝缓存：前者输出重复、忽略语义变化；后者丢失关键时序信息。需要一种既能捕捉语义状态转变、又能长期保留关键信息且低延迟的流式框架。

Method: 构建事件感知的流式框架Event-VStream：融合运动、语义与预测信号检测“有意义的状态转换”，把连续视频切分为离散事件；仅在事件边界触发语言生成；为每个事件生成嵌入并写入持久记忆库（memory bank），用于跨事件的长程推理与检索，同时保持在线低延迟。

Result: 在OVOBench-Realtime与长时程Ego4D评测中，较VideoLLM-Online-8B提升+10.4分；在仅使用通用LLaMA-3-8B文本骨干下，性能接近Flash-VStream-7B；在2小时Ego4D流上约70% GPT-5胜率。

Conclusion: 事件级流式表征与持久记忆有效减少冗余、缓解遗忘，在不增加大模型规模的情况下，实现低延迟的长时程视频理解，并取得强竞争力表现。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [17] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: 提出Skywork UniPic 3.0：统一单图编辑与多图合成，聚焦HOI场景；以序列建模统一条件生成，配合高质量数据管线与后训练加速，实现SOTA效果与8步高保真采样。


<details>
  <summary>Details</summary>
Motivation: 多图合成（尤其HOI）需求旺盛但方法细节匮乏，现有模型在一致性与质量上存在明显不足，需要统一框架与可复现的方法论。

Method: 1) 统一多模态框架，支持1-6张任意分辨率输入与任意输出分辨率（总像素≤1024×1024）；2) 数据层面：构建采集-过滤-合成一体的高质量数据管线，仅用70万样本；3) 训练范式：将多图合成表述为序列建模，将条件生成转为统一的序列合成；4) 推理加速：在后训练阶段引入轨迹映射与分布匹配，使采样步数降至8步并获得12.5×加速。

Result: 在单图编辑基准上达SOTA；在多图合成基准上超越Nano-Banana与Seedream 4.0；实现高保真、快速推理。

Conclusion: 通过高质量数据管线与序列化训练范式，UniPic 3.0在HOI导向的多图合成上实现一致性与质量的双提升，并以高效采样验证方法有效性，代码与模型已开源。

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [18] [Consistency-Regularized GAN for Few-Shot SAR Target Recognition](https://arxiv.org/abs/2601.15681)
*Yikui Zhai,Shikuang Liu,Wenlve Zhou,Hongsheng Zhang,Zhiheng Zhou,Xiaolin Tian,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 提出Cr-GAN，在极少样本SAR条件下通过一致性正则和双分支判别器合成高质量数据，结合SSL预训练+少样本微调，MSTAR与SRSDD上8-shot显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 少样本SAR识别因数据稀缺受限，常用“GAN合成数据+SSL预训练+微调”但传统GAN本身需大量数据训练稳定，产生悖论，需能在小数据下仍可生成多样高保真样本的方法。

Method: 提出一致性正则化的Cr-GAN：1) 双分支判别器，将对抗判别与表征学习解耦；2) 通道级特征插值，生成新颖潜在特征；3) 图像域与特征域的双域循环一致性，保证语义一致性；框架可适配多种GAN，并为多种SSL算法提供有效合成数据。

Result: 在MSTAR与SRSDD数据集8-shot设置下，分别达到71.21%与51.64%准确率，显著优于主流基线；同时参数量仅为SOTA扩散模型的约1/5。

Conclusion: Cr-GAN在极少样本下稳定生成高质量、多样合成SAR数据，强化SSL预训练并提升下游识别表现，具备通用性与高效性，适合资源受限的真实应用场景。

Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.

</details>


### [19] [Performance-guided Reinforced Active Learning for Object Detection](https://arxiv.org/abs/2601.15688)
*Zhixuan Liang,Xingyu Zeng,Rui Zhao,Ping Luo*

Main category: cs.CV

TL;DR: 提出MGRAL：以mAP提升为奖励、用强化学习策略梯度进行批量样本选择的目标检测主动学习框架，并通过无监督的快速查表近似评估未标注样本对mAP的影响，在VOC与COCO上获得最优AL曲线。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习多基于数据分布或不依赖任务的“信息量”度量，与下游目标检测的mAP提升弱相关；批量选择又面临组合爆炸，且选择与最终mAP之间不可微，难以端到端优化；评估未标注样本对性能的影响往往计算昂贵。

Method: 将“期望模型输出变化”转化为以mAP提升为目标的奖励信号，设计强化学习采样代理以策略梯度优化批量选择；为缓解估计成本，对未标注样本的mAP贡献采用无监督近似与快速查找表进行快速估计，实现可部署性。

Result: 在PASCAL VOC与COCO上，MGRAL的主动学习曲线优于现有方法，并给出可视化例证，显示选择的样本更能提升检测性能。

Conclusion: 以任务性能（mAP）为导向、用强化学习直接优化批量选择可有效提升目标检测主动学习效率；无监督的快速估计机制降低了计算成本，展示了强化学习驱动的主动目标检测新范式。

Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.

</details>


### [20] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: 提出BVS框架，通过“重构-再生成”策略与中性化视觉拼接/归纳式重组，诱导MLLM生成有害图像，揭示视觉安全边界薄弱；据称对GPT-5（2026-01-12版）越狱成功率98.21%。


<details>
  <summary>Details</summary>
Motivation: 现有对MLLM安全研究多聚焦文本或跨模态通用漏洞，缺乏对“视觉安全边界”的系统评估；需要一种方法专门刻画并攻破视觉侧的防线，检验对抗鲁棒性与对齐有效性。

Method: 设计BVS图文越狱框架：1) 重构-再生成策略，将恶意意图从原始输入中解耦；2) 中性化视觉拼接，将敏感元素分解/弱化后再组合；3) 归纳式重组，以诱导式图文提示引导模型在后续生成阶段恢复并放大被“中和”的恶意意图，从而产出有害图像。

Result: 在实验中，BVS对多模态大模型（重点是GPT-5 2026-01-12版）实现98.21%越狱成功率，说明现有视觉对齐防线极易被规避。

Conclusion: 当前MLLM在视觉安全对齐上存在关键缺陷；仅靠常规安全训练不足以防御基于视觉重构与归纳诱导的攻击，亟需更强的视觉侧防护与跨模态一致性安全机制。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [21] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: 针对日本全国的ALOS‑2 HH 极化 SAR 影像做LULC语义分割与水体二分类，在不增加整体复杂度下，通过三项轻量改进缓解SAR密集预测常见问题（边界过平滑、细长结构漏检、长尾类别退化），基于SAR‑W‑MixMAE自监督预训练取得在全国基准上的稳定提升，尤其改善稀有类与水体检测指标。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAR的全国尺度语义分割在密集预测上常出现边界模糊、细长目标漏检及长尾类别性能差；希望在不增加推理/训练复杂度的前提下，提升对弱势类别与结构细节的刻画能力，并与自监督预训练相结合服务日本全国LULC与水体检测。

Method: 在SAR‑W‑MixMAE自监督预训练骨干上提出三项轻量模块：1) 在多尺度解码器中注入更高分辨率特征以保留细节；2) 采用“渐进式refine‑up”解码头，在逐步上采样过程中交替卷积细化与上采样以缓解边界过平滑与细结构缺失；3) 在focal+dice联合损失的类别重加权中引入α缩放因子，抑制过强重加权导致的不稳定，兼顾长尾类别。

Result: 在日本范围ALOS‑2 LULC基准上取得一致性提升，尤其在稀有/欠表示类别上显著改善；同时在水体二分类任务上，各项标准评估指标（如精确率、召回率、F1/IoU等）均有提升。

Conclusion: 轻量级结构细化与适度的损失重加权即可在保持管线简单的同时，显著缓解SAR密集预测的典型失效模式，并在全国尺度场景中稳定提升LULC与水体检测性能。

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [22] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 论文提出一个三层评测框架，系统评估视觉-语言模型在时尚多属性任务中的表现，尤其处理“属性不适用/不可见（NA）”问题；在DeepFashion-MultiModal上基准测试9个VLM与基于Fashion-CLIP的分类器，发现VLM零样本整体显著优于传统嵌入+逻辑回归，但在“适用性检测”上是主要瓶颈；高效模型以更低成本达到旗舰模型90%以上性能。


<details>
  <summary>Details</summary>
Motivation: 时尚应用（商品目录补全、视觉搜索、推荐）需要细粒度多属性预测；许多属性是条件性的（如无外套则“外层面料”不适用），现有评测常忽略“属性是否适用”的先决判断，导致对模型能力的误估与部署困难。

Method: 提出三层评测：Tier1总体任务（含NA类）；Tier2检测属性适用性（NA识别）；Tier3在可判定样本上进行细粒度分类。使用含NA定义的DeepFashion-MultiModal数据集，在18个属性、5000张图上，对9个VLM（旗舰/高效/超高效）与基于Fashion-CLIP嵌入的监督分类器进行对比，报告宏F1、NA-F1与细粒度F1。

Result: 零样本VLM宏F1达64.0%，约为基于Fashion-CLIP的逻辑回归的3倍；Tier3细粒度分类F1为70.8%，而Tier2适用性（NA）检测仅34.1% F1，成为主要短板；高效模型以更低成本达旗舰90%+性能。

Conclusion: 应将多属性任务拆解为“适用性检测+细粒度分类”，以诊断瓶颈并指导改进；VLM在分类方面强但需提升可见性/适用性判断；高效VLM具备性价比优势，框架可为生产系统提供针对性优化路径。

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [23] [VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning](https://arxiv.org/abs/2601.15724)
*Chenglin Li,Qianglong Chen,Feng Han,Yikun Wang,Xingxi Yin,Yan Gong,Ruilin Li,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: VideoThinker 通过在“字幕空间”合成多步工具交互轨迹，并再对齐回视频，训练出具备动态检索与缩放能力的代理式视频大模型，在长视频理解上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中，均匀采样+静态推理会弱化时间定位并造成信息丢失。虽然时间检索、空间/时间缩放等代理工具能自适应聚焦关键片段，但要构造带有这些工具使用的训练数据，又需要已有强长视频理解模型，形成“先有鸡还是先有蛋”的依赖。

Method: 提出 VideoThinker：先将视频转为丰富字幕描述；用强大的代理式语言模型在字幕空间生成多步工具使用轨迹（如检索与缩放的序列）；再将这些字幕级轨迹锚定回对应视频帧/片段，得到规模化、交错的视频—工具推理数据；用该合成数据训练视频 LLM，使其习得动态推理与多步工具使用。

Result: 在多项长视频基准上，VideoThinker 显著优于仅用字幕的语言代理与强视频模型基线，展现出更强的自适应时间探索与多步工具推理能力。

Conclusion: 基于工具增强的合成数据与“字幕→视频”对齐的训练范式，能有效打破长视频代理数据匮乏的瓶颈，为长视频理解带来可扩展的动态检索与缩放式推理。

Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.

</details>


### [24] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: 提出FAIR-ESI框架，通过多视角自适应特征重要性精炼（频谱、时间、空间patch）提升电生理源成像（ESI）精度，并在仿真与临床数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: ESI对脑疾病诊断关键，但现有基于模型优化或深度学习方法在特征选择与精炼上仍不足，影响定位与判别精度。需要一种能在不同特征视角下自适应衡量与优化特征重要性的机制。

Method: 提出FAIR-ESI，多视角特征重要性自适应精炼：1) 频域：基于FFT的谱特征精炼，强调诊断相关频带；2) 时域：加权的时间特征精炼，动态分配时间片重要性；3) 空间/局部：基于自注意力的patch级特征精炼，突出关键空间片段。整体形成端到端框架以提升ESI。

Result: 在两组多配置仿真数据与两组真实临床数据上进行大量实验，结果显示FAIR-ESI优于现有方法，提升ESI精度与稳健性。

Conclusion: 多视角的自适应特征重要性精炼可显著提升ESI性能，为脑疾病诊断提供更可靠工具，并为脑功能研究带来新见解。

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [25] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 提出一种将基础模型适配到多模态医学影像的新框架，通过“亚区感知的模态注意力”和“自适应提示工程”提升脑肿瘤分割性能，在BraTS 2020上显著优于基线，尤其改善坏死核心分割。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在医学多模态（如MRI多序列）场景下难以充分融合不同模态信息，且肿瘤组织异质性强，导致对各子区域（如增强区、浸润区、坏死区）分割不稳；需要一种既能细粒度地按亚区选择模态贡献、又能利用基础模型先验能力的适配方法。

Method: 提出两项技术：1）亚区感知的模态注意力（sub-region-aware modality attention），使模型对每个肿瘤子区域动态学习最优模态组合与权重，实现细粒度多模态融合；2）自适应提示工程（adaptive prompt engineering），为基础模型设计与更新任务相关提示，引导其在分割过程中利用先验知识与上下文以精炼预测。整体形成一个可与基础模型对接的适配框架。

Result: 在BraTS 2020脑肿瘤分割数据集上进行验证，相比基线方法整体性能显著提升，尤其在最具挑战的坏死核心（necrotic core）子区域取得明显增益。

Conclusion: 该框架为多模态融合与提示策略提供了系统化方案，能有效将基础模型适配到医学影像分割任务，提升准确性与鲁棒性，并为未来基于基础模型的医学影像应用奠定基础。

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [26] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出ARDIS：首个任意分辨率深度图像隐写框架，实现在固定分辨率封面中隐藏并无盲先验恢复任意原分辨率的秘密图像，显著提升不可感知性与跨分辨率还原质量。


<details>
  <summary>Details</summary>
Motivation: 现有DIS要求秘密图与封面同分辨率，导致：(1) 分辨率不一致需重采样，恢复细节丢失；(2) 不知道原分辨率时无法按原分辨率恢复。亟需一种能在固定封面分辨率下仍保真恢复任意原分辨率秘密图的方法。

Method: 1) 频率解耦隐藏：将秘密图分解为分辨率对齐的全局基（低频/结构）与分辨率无关的高频潜码；在固定分辨率封面中嵌入。2) 潜码引导的隐式重建器：用恢复出的细节潜码调制连续隐式函数，对任意坐标查询并渲染高频残差叠加到全局基，实现确定性细节恢复。3) 隐式分辨率编码：把离散分辨率值映射为稠密特征图，嵌入到特征域冗余空间，使重建器能从隐写表示中盲解码秘密图的原始分辨率。

Result: 在不可感知性和跨分辨率恢复保真度上显著优于SOTA；在未知分辨率场景下可正确解码并按原分辨率高保真重建秘密图。

Conclusion: 通过“频率解耦+隐式函数重建+隐式分辨率编码”，ARDIS将DIS从离散映射升级为参考引导的连续信号重建，实现任意分辨率的盲恢复与更强的隐形与保真性能。

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [27] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: 提出ES-mHC：一种在高光谱图像分类中显式建模电磁谱段相互作用的“白盒”超连接框架，分离表示与交互结构，用方向矩阵刻画谱段互动，提升可解释性、减少冗余，并展示可视化的内部信息流与非对称交互模式。


<details>
  <summary>Details</summary>
Motivation: 现有HSIC深度模型多通过不透明的谱-空混合获得特征，难以解释内部决策机理；需要一种既保留性能又能揭示谱段间相互作用路径与结构的可解释方法。

Method: 提出ES-mHC（物理谱感知的白盒mHC）：将特征表示与交互结构解耦；用结构化、具方向性的超连接矩阵显式建模不同电磁谱分组之间的残差交互流；通过可视化这些矩阵来分析空间一致性与交互的非对称性；研究扩展率对结构化模式涌现的影响。

Result: 学习到的超连接矩阵呈现连贯的空间图样与非对称交互行为；随着扩展率增大，结构化的交互模式更快、更明显地涌现；模型展现出较低冗余与更清晰的信息流路径。

Conclusion: ES-mHC将HSIC从纯黑盒预测转变为结构透明、部分白盒的学习过程，促进谱段专门化与可解释分析，为理解模型内部动力学提供机制性洞见。

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [28] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: FeTal-SAM 将多图谱提示与 SAM 结合，实现对胎脑 MRI 的灵活、可泛化分割，在不重新训练的情况下适配不同标签定义，并在多数据集上取得与特定训练基线相当的表现。


<details>
  <summary>Details</summary>
Motivation: 现有胎脑 MRI 分割方法需要大量标注并针对固定标签训练，难以适配临床/科研中动态变化的结构定义；同时难以判断分割是基于真实影像对比还是仅依赖空间先验。

Method: 以多图谱配准生成空间对齐的标签模板，作为致密（dense）提示与边界框提示共同输入 SAM 的分割解码器；按结构进行二值分割，再融合生成完整三维分割体。采用 dHCP 与自有数据集跨孕周评估。

Result: 对对比度良好的结构（如皮质板、小脑）取得与针对各数据集与标签专门训练的 SOTA 基线相当的 Dice；对低对比、小体积结构（如海马、杏仁核）略低。表现跨孕周稳健并可按需分割任意指定解剖结构。

Conclusion: FeTal-SAM 在无需耗时重新训练的前提下提供临床可适配、通用化的胎脑 MRI 分割方案，虽对低对比结构仍有提升空间，但为可扩展的临床工具迈出重要一步。

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [29] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 提出LL-GaussianMap：首个将2D Gaussian Splatting引入低照度增强的无监督框架，用结构先验指导增益图生成，兼顾边缘保真与抑制伪影，效果好且存储开销小。


<details>
  <summary>Details</summary>
Motivation: 现有低照度增强多在像素域或隐式特征域操作，忽视图像固有几何结构先验；2DGS具备显式结构拟合与高效渲染优势，但尚未用于低层视觉。需要一种方法将2DGS的结构感知引入增强过程，且摆脱成对数据依赖。

Method: 将增强重构为由2DGS原语指导的增益图生成。两阶段：1) 用2DGS进行高保真结构重建；2) 通过高斯splatting的栅格化机制，在统一增强模块中渲染数据驱动的增强字典系数，得到增益图。整体以无监督学习训练，显式结构感知嵌入增益图生成，保边去伪。

Result: 在实验中取得优于现有方法的增强性能，同时模型存储占用极低，验证显式高斯表示在图像增强任务中的有效性。

Conclusion: 显式2DGS结构先验可有效指导低照度增强的增益图生成，带来更好边缘保真与伪影抑制；无监督框架避免对成对数据的依赖，并以极低存储实现高性能增强。

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [30] [LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting](https://arxiv.org/abs/2601.15772)
*Yuhan Chen,Wenxuan Yu,Guofa Li,Yijun Xu,Ying Fang,Yicui Shi,Long Cao,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 提出LL-GaussianImage：在2D Gaussian Splatting压缩域内直接做低照度增强的零样本无监督方法，避免解压-增强-重压流程，兼顾画质、压缩率与效率。


<details>
  <summary>Details</summary>
Motivation: 像素域低照度增强需先解压再重压，效率低且带来二次失真；2DGS作为高保真高压缩的显式表示若能在其压缩域直接处理，可显著提升效率并减少失真，但缺乏相应方法。

Method: 1) 语义引导的专家混合（MoE）增强：以渲染图像为引导，对2DGS稀疏属性空间进行动态自适应变换，实现“压缩即增强”；2) 多目标协同损失：同时约束平滑性与保真度，抑制伪影并提升视觉质量；3) 两阶段优化：先单尺度重建确保基底表示精度，再进行鲁棒网络优化，实现“重建即增强”。

Result: 在无需完全解压到像素网格的前提下，实现对低照度图像的高质量增强，同时保持高压缩率与较高效率；实验验证该压缩域直接处理范式的可行性与优越性。

Conclusion: LL-GaussianImage证明了在2DGS压缩表示域内进行零样本、无监督的低照度增强是可行且有效的，可在减少管线复杂度与失真的同时提升画质，并为压缩域图像增强提供新范式。

Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.

</details>


### [31] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: 提出一种用于EM神经元分割的扩散式数据增广框架，能从3D掩膜合成多样且结构可信的图像-标签对，并在低标注条件下显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割依赖大量手工标注，传统几何/光照增广与原图高度相关、缺乏结构多样性，限制了泛化与性能；需要能生成结构更丰富且生物学合理的训练样本。

Method: 构建分辨率感知的条件扩散模型：利用多尺度条件与EM分辨率先验，从3D掩膜进行体素级图像合成；引入生物学引导的掩膜重塑模块，生成结构更真实的掩膜；由此形成可同时产出图像-标签对的数据增广流水线，可与后处理流程结合。

Result: 在AC3与AC4数据集的低标注场景下，与两种不同后处理方法结合时，ARAND分别提升32.1%与30.7%，显示显著改进。

Conclusion: 扩散式、分辨率感知且受生物学先验约束的生成增广能有效丰富训练分布，减少对大规模标注的依赖，并显著提升EM神经元分割质量；代码开源便于复现与推广。

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [32] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: 该工作提出一个合成视频基准，用最小成对视频测试VLM在情境与空间意识上的薄弱环节，涵盖暴力与良性行为区分、跨视角的施暴者角色绑定、以及精细轨迹对齐判断。结果显示多款现有VLM几乎接近随机，仅有稳定颜色提示能部分缓解角色混淆。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在需要细微时空与几何线索的语义任务上表现脆弱，现有大规模预训练缺乏显式空间先验与可诊断评测。作者希望用可控、可复现的数据诊断具体失败模式，并推动引入轻量空间先验。

Method: 构建合成视频数据集与基准，设计“最小视频对”以隔离关键线索；评测两类能力：情境意识（区分有害/无害互动）与空间意识（跟踪主体、关系与相对运动）。设三项挑战：1) 暴力vs良性活动；2) 跨视角施暴者角色绑定；3) 细粒度轨迹对齐。对近期VLM进行零训练评估，并测试一种简单辅助——为角色施加稳定颜色线索。

Result: 多项任务上模型表现仅略高于随机；稳定颜色线索能一定程度降低角色绑定错误，但未从根本提升空间推理。

Conclusion: 当前VLM缺乏稳健的时空与几何推理。该基准与代码数据的公开为可复现实验与诊断提供工具，并呼吁在大规模预训练之外引入轻量空间先验以弥补短板。

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [33] [A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks](https://arxiv.org/abs/2601.15810)
*Mustafa Yurdakul,Enes Ayan,Fahrettin Horasan,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 研究开发了一个基于CNN的移动应用，用于花卉种类识别；在MobileNet、DenseNet121、Xception与7种优化器对比中，使用SGD的DenseNet121表现最佳，准确率95.84%，精确率、召回率、F1均为96%。


<details>
  <summary>Details</summary>
Motivation: 花卉在生活中应用广泛，但识别花卉需要专家且不易随时获得；随着视觉数据量增长，传统算法难以胜任，需要高效的深度学习方案为非专业用户快速提供花卉信息，适配移动端使用场景。

Method: 构建移动端花卉识别应用，选取三种CNN架构（MobileNet、DenseNet121、Xception），并分别配合七种优化算法训练与评估，比较分类性能指标（准确率、精确率、召回率、F1）。

Result: DenseNet121+SGD取得最佳结果：准确率95.84%，精确率96.00%，召回率96.00%，F1 96.00%；其他模型/优化器组合表现次之。

Conclusion: CNN适用于移动端花卉分类任务，其中DenseNet121配合SGD在该研究中最优，验证了将深度学习集成到移动应用以实现便捷花卉识别的可行性。

Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.

</details>


### [34] [Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data](https://arxiv.org/abs/2601.15813)
*Clare Chemery,Hendrik Edelhoff,Ludwig Bothmann*

Main category: cs.CV

TL;DR: 提出一个轻量级机器学习实验管线，帮助生态学者用本地数据自行训练图像分类器；在红鹿年龄与性别分类任务上取得高准确率，证明小数据也可实现可靠的人口统计识别。


<details>
  <summary>Details</summary>
Motivation: 生态学相机陷阱影像激增，但通用“现成模型”难以适配本地物种与特定问题；生态研究人员缺乏便捷工具与ML专长来迭代构建任务定制的分类器。

Method: 构建一体化工具：命令行执行预处理、训练、评估；图形界面用于标注、误差分析与模型比较。以德国Veldenstein森林红鹿为案例，基于4352张裁剪个体图像，系统地探索多种主干网络、超参数与数据增强策略，训练年龄与性别分类器。

Result: 最佳模型在年龄分类上达到90.77%准确率，在性别分类上达到96.15%准确率；在仅数千张样本的小规模数据集上实现稳健的人口统计分类。

Conclusion: 该轻量化管线使生态学者无需深厚ML背景即可构建并迭代任务定制模型；对特定、狭义生态问题可用有限数据获得可靠结果，推动ML在野生动物监测与人口统计分析中的更广泛应用。

Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.

</details>


### [35] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: 论文将“数据集蒸馏”首次引入遥感图像解译：用文本到图像扩散模型把大规模数据压缩为少量高代表性的合成样本，并通过分类器一致性引导、潜空间聚类的视觉原型与视觉语言描述提升判别性与多样性，在三大遥感场景分类基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感解译依赖海量标注数据，但带来高存储/算力成本与敏感类别泄露风险。需要能在保持任务性能的同时显著减少数据量、并便于共享与隐私保护的技术。

Method: 1) 训练文本到图像扩散模型，将大规模遥感数据“蒸馏”为小规模代表性数据；2) 在扩散训练中加入来自预训练分类器的分类一致性损失，作为 classifier-driven guidance 提升合成样本的可分辨性；3) 对训练样本做潜空间聚类，选取多样且代表性的聚类中心作为视觉风格原型；4) 借助视觉语言模型生成聚合文本描述，作为扩散条件，从而同时注入视觉原型与语义引导。

Result: 在三个高分辨率遥感场景分类数据集上，合成的蒸馏样本逼真且多样，用其训练下游模型可获得优良性能；提供代码与预训练模型以复现。

Conclusion: 基于扩散模型的遥感数据集蒸馏可在显著压缩数据规模的同时保持甚至提升分类性能，并降低存储/计算负担与数据泄露风险；方法通用且可复用。

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [36] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 提出一种低成本（$45.20）的物联网植物监测与智能灌溉系统，基于ESP32采集多传感器数据，上云（ThingSpeak）分析与告警，实现土壤墒情精准维持（92%准确度）并节水约40%，适配家庭与商业农业。


<details>
  <summary>Details</summary>
Motivation: 传统农业依赖人工巡检与定时灌溉，存在水资源浪费、植株生长不一致、对环境变化响应滞后；需要一种可持续、智能、可远程监控的解决方案以优化资源利用与植株健康管理。

Method: 以ESP32为核心，集成DHT22温湿度、HC-SR04水位、土壤湿度传感器，配套OLED显示与蜂鸣器告警；无线传输数据至ThingSpeak云端，实现远程监控、历史分析与自动告警；基于阈值/规则驱动的自动灌溉控制；提供可视化Web仪表盘。

Result: 实验显示系统能维持最佳土壤湿度，监测准确度达92%；相较传统灌溉节水约40%；实现实时环境监测与历史数据可视化与预警。

Conclusion: 该方案在成本低、可扩展、部署简易的前提下，有效提升用水效率与作物管理水平，适用于小规模园艺到商业农业的精细化与智能化应用。

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [37] [TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing](https://arxiv.org/abs/2601.15838)
*Toan Gian,Dung T. Tran,Viet Quoc Pham,Francesco Restuccia,Van-Dinh Nguyen*

Main category: cs.CV

TL;DR: TinySense提出用VQGAN+自适应码本量化与Transformer纠错，实现Wi‑Fi CSI的人体姿态估计高效压缩；在同等压缩率下PCK20最高提升1.5倍，并把时延与网络开销分别降至5×与2.5×。


<details>
  <summary>Details</summary>
Motivation: 现有Wi‑Fi人体姿态估计直接传输/处理大量CSI，导致带宽占用高、端边协同难扩展且在不稳定网络下效果下降；亟需在保证HPE精度的前提下显著压缩CSI并提升鲁棒性与端到端效率。

Method: 构建基于VQGAN的压缩框架TinySense：1) 用VQGAN学习CSI的离散码本，通过矢量量化将CSI映射为短码串；2) 采用K‑means对大规模预训练码本进行聚类，按网络条件/目标码率动态选择更小子码本以调整比特率；3) 引入Transformer对压缩表示进行建模/纠错，缓解低比特率或丢包带来的信息损失；4) 在Jetson Nano与Raspberry Pi上实现原型并评估时延与带宽。

Result: 与先进压缩方案相比，在相同压缩率下，HPE精度（PCK20）最高提升1.5×；端到端时延最高降低5×；网络传输开销最高降低2.5×。在不可靠网络条件下仍保持较高鲁棒性。

Conclusion: 通过VQGAN码本量化与自适应码率、结合Transformer补偿，TinySense在不牺牲甚至提升HPE精度的同时，大幅降低CSI数据量、时延与网络负担，显示出面向设备无感与隐私友好的Wi‑Fi人体感知的可扩展性。

Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.

</details>


### [38] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出一种轻量级、受脑启发的深度学习框架，用于冠脉造影二分类，在复杂形态、类别极不平衡与算力受限场景下仍取得稳定高性能和高效率。


<details>
  <summary>Details</summary>
Motivation: 临床真实世界CAG图像存在病变形态复杂、类别极度不平衡、标注不确定、算力有限等问题，传统深度学习模型在鲁棒性与泛化性上受限，需一种兼顾精度与部署效率的方案。

Method: 以预训练CNN为骨干，构建轻量级混合神经表征；采用“选择性神经可塑性”训练策略，仅对部分参数高效自适应；设计受脑启发的注意调制损失：Focal Loss结合标签平滑以兼顾困难样本与不确定标注；并引入类不平衡感知采样与余弦退火（带热重启）以模拟生物神经系统的节律调控与注意分配。

Result: 在CAG二分类任务上取得稳定且强劲的表现（准确率、召回率、F1、AUC均具竞争力），同时保持高计算效率。

Conclusion: 脑启发学习机制在轻量级医学影像分析中有效，可在有限算力条件下为临床智能决策支持提供可部署、具生物可解释性的解决方案。

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [39] [Out-of-Distribution Detection Based on Total Variation Estimation](https://arxiv.org/abs/2601.15867)
*Dabiao Ma,Zhiba Su,Jian Yang,Haojun Fei*

Main category: cs.CV

TL;DR: 提出TV-OOD方法，利用总变差网络估计器为每个输入计算总变差贡献并作为分数，以区分分布内/外样本；在多模型多数据集的图像分类任务上，性能与现有SOTA相当或更优。


<details>
  <summary>Details</summary>
Motivation: 现实部署中模型常遇到分布偏移，现有OOD方法虽有效但仍有改进空间；作者希望用更稳健、可解释的统计量（总变差）来衡量样本与训练分布的偏离程度。

Method: 引入总变差网络估计器（TVNE），对输入样本估计其对整体总变差的贡献，定义为“总变差分数”；将该分数作为判别指标进行OOD检测，并在多种模型与数据集上评测。

Result: 在图像分类基准上，TV-OOD在所有评估指标上普遍达到与现有领先方法相当或更优的表现，跨多模型与数据集一致有效。

Conclusion: 基于总变差分数的OOD检测是有效且稳健的，能够提升实际部署中对分布外样本的识别；该思路为OOD提供了新的可推广框架。

Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.

</details>


### [40] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: 提出首个覆盖11个器官、全配对的跨癌种CT/MR对比增强合成数据集与基准（PMPBench），支持DCE三期与CT/CTC成对评估，并给出现代图像翻译基线结果，旨在推进安全有效的对比剂合成研究。


<details>
  <summary>Details</summary>
Motivation: 对比剂能显著提升病灶显著性，但受患者状况与资源限制并非总能使用。现有AI对比增强合成研究受限于数据：公开集多为脑部、配对不完整/配准不佳、相位标注缺失、私有资源占多，导致方法难以系统评测与泛化。

Method: 构建并发布首个全配对、跨11器官的CT与MR数据集：MR含完整DCE三期（DCE1-3），CT含非增强与增强（CTC）成对采集；严格解剖对应与空间对齐，支持1-to-1、N-to-1、N-to-N翻译设定（如从非增强预测DCE相位）。在该资源上建立综合基准，选取代表性图像到图像翻译方法进行评测。

Result: 给出多种当代图像翻译基线在不同任务设定下的性能结果，实现对比增强合成任务的系统比较与可复现评测；数据与代码公开（GitHub: YifanChen02/PMPBench）。

Conclusion: PMPBench填补了跨器官、全配对对比增强合成数据的空白，为安全、有效的对比剂合成研究提供标准化评测平台，直接服务于多器官肿瘤影像工作流，预期将加速相关方法的发展与临床转化。

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [41] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: 论文在前列腺多参数MRI上实证：预训练目标与下游任务越对齐，迁移微调越有效（性能更高、收敛更快）；用简单的特征分布偏离度量（如MMD）可量化这种对齐程度。


<details>
  <summary>Details</summary>
Motivation: 语言领域的大模型在多任务上泛化良好，但视觉基础模型尽管算力投入巨大，迁移到具体视觉任务时提升不均。作者怀疑是“预训练目标”与“下游任务需求”不匹配所致，尤其在医疗影像中更为突出，需系统验证并给出可测量的对齐指标。

Method: 选取两类视觉预训练范式：重建导向（MAE，ProFound）与对比学习导向（ProViCNet）。在前列腺多参数MRI的五个下游任务上进行微调与评估，同时比较微调前后同一特征表示的分布差异，使用MMD等简单散度指标衡量“预训练-下游对齐度”，并将其与性能提升和收敛速度相关联分析。

Result: 当预训练目标与下游任务更契合时：1）微调性能提升更大；2）训练收敛更快。MMD等度量能反映这种契合度：较小的微调前后特征分布变化（更低MMD）与更好的迁移效果呈正相关。不同范式（MAE vs 对比学习）在不同任务上的收益存在差异，体现了目标-任务匹配的重要性。

Conclusion: 视觉基础模型的预训练应考虑下游应用需求；可通过简单的特征散度指标（如MMD）在微调前后评估对齐度，作为选择或设计预训练策略的依据。任务对齐度高→迁移更好、收敛更快。

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [42] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: 提出RadJEPA：无需语言监督、基于联合嵌入预测架构的自监督胸片表示学习；通过预测被遮挡区域的潜在表示，较现有图文或DINO式方法取得更优分类、分割与报告生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型受限于成对图文数据的稀缺与偏差；需要验证在不依赖语言监督的情况下，能否学到强健的放射影像编码器。

Method: 构建RadJEPA，自监督的Joint Embedding Predictive Architecture：仅用未标注胸部X光预训练，通过预测被遮挡图像区域的潜在表示作为学习目标；区别于图文对齐与DINO自蒸馏，侧重显式的潜在空间预测而非全局对齐。

Result: 在疾病分类、语义分割、报告生成多项基准上，RadJEPA优于包括Rad-DINO在内的最新方法。

Conclusion: 无需语言监督的潜在表示预测式自监督框架在胸片任务上具有强竞争力，可作为医学影像通用编码器的有效路径。

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [43] [ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling](https://arxiv.org/abs/2601.15897)
*Zhaoqi Su,Shihai Chen,Xinyan Lin,Liqin Huang,Zhipeng Su,Xiaoqiang Lu*

Main category: cs.CV

TL;DR: 提出ThermoSplat，将RGB与热红外结合到3D Gaussian Splatting，实现在多光谱条件下更鲁棒的场景重建，并在RGBT-Scenes上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3DGS方法难以充分利用跨模态互补信息：要么忽视跨模态相关性，要么使用共享表示却无法自适应处理可见光与热红外在结构关联与物理差异上的复杂性。

Method: 1) 交叉模态FiLM调制：以热红外的结构先验动态调制共享潜特征，引导可见光纹理合成；2) 模态自适应几何解耦：学习模态特定的不透明度偏移，并为热红外分支执行独立的光栅化；3) 混合渲染：结合显式球谐函数与隐式神经解码，兼顾语义一致性与高频细节。

Result: 在RGBT-Scenes数据集上，对可见光与热红外两种光谱均取得SOTA渲染质量。

Conclusion: 通过主动跨模态特征调制与几何解耦，ThermoSplat有效融合RGB与热红外，实现鲁棒的多谱段3D重建，并在标准数据集上验证其优越性。

Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.

</details>


### [44] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: 本文系统性研究多模态基础模型中情感表征的“机械可解释性”，发现情感能力的关键不在注意力层，而集中在前馈网络的门控投影（gate_proj）。只微调该模块即可以显著更少参数接近SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态情感模型在任务上表现强，但其内部如何实现情感理解/生成仍缺乏机制层面的解释。作者希望跨架构与任务识别情感能力在网络中的结构归因与参数定位，从而提升可解释性与微调效率。

Method: 跨多种基础模型与训练策略，比较“情感导向监督”前后内部参数变化；进行受控模块迁移、单模块定向微调与破坏性消融实验，聚焦对注意力模块与前馈层各子投影（含gate_proj）的作用对比与定位。

Result: 情感适配的主要变化不在注意力模块，而稳定地集中于前馈层的gate_proj。仅调整gate_proj即可在理解与生成任务上取得接近完全微调的性能；通过模块转移与消融验证其“充要性与必要性”。在与AffectGPT对比时，调参量约为其24.5%，平均达成其96.6%的性能，显示出显著的参数效率。

Conclusion: 多模态基础模型的情感能力在结构上由前馈门控机制介导，gate_proj是情感建模的核心部位。针对该模块的轻量化微调可在保持性能的同时显著降低参数与计算成本。

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [45] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: 论文基于VR/HCI场景评估零样本面部表情识别在CPU实时环中的可行性，发现检测稳健但分类受“时延墙”限制；YOLOv11n在检测上时延最优（~54ms），而CLIP/SigLIP等通用Transformer在准确率与时延上均不达标，指向需轻量化、领域特定模型以满足实时治疗需求。


<details>
  <summary>Details</summary>
Motivation: 在ASD辅助社交训练的VR疗法中，需要实时情绪识别来维持交互的“同步感”（MTP<140ms）。现成深度模型多追求精度忽视时延，尤其在仅CPU的普通硬件上难以满足实时要求，因此需要基准评估并找出现有方法的瓶颈与合适折中。

Method: 使用UIBVFED数据集，在“虚拟角色（风格化头像）”上进行零样本FER基准测试。检测阶段对比YOLO v8/v11/v12的Medium与Nano版本；分类阶段评估通用视觉Transformer（CLIP、SigLIP、ViT-FER）。全部在CPU-only推理条件下测量检测准确率、分类准确率与端到端时延，并分析与140ms MTP约束的关系。

Result: - 检测：在风格化头像上面部检测鲁棒，准确率100%；YOLOv11n在检测-时延折中最佳，约54ms。- 分类：出现“时延墙”，通用Transformer（CLIP、SigLIP）零样本分类准确率<23%，且单次推理>150ms，不满足实时回路；因此端到端无法在CPU上达到低时延且可用的准确率。

Conclusion: 现有通用大模型在CPU端的零样本FER不适用于VR实时治疗环；需要轻量化、领域特定的表情分类架构（可能含蒸馏/量化/多任务优化）以跨越时延墙，并为可及的VR疗法提供实用的实时AI。

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [46] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出无需领域微调的多视角3D手部姿态估计管线，并发布包含6.8万帧与带三角化3D真值的手部标注数据集；在手术场景中显著优于基线（2D误差降31%，3D误差降76%）。


<details>
  <summary>Details</summary>
Motivation: 手术场景中的强光、遮挡、手套导致外观单一，以及缺乏标注数据，使3D手部姿态估计困难，但其在技能评估、机器人辅助手术与流程分析中至关重要。

Method: 构建基于多视角的训练免调管线：使用稳健的人体检测与全身姿态估计定位与跟踪手部，基于现成预训练模型在手部裁剪上进行2D关键点预测；随后进行带约束的3D优化与三角化。并发布在仿真手术室拍摄、包含>68k帧与3k手部2D手动标注及对应3D真值的基准数据集。

Result: 在量化实验中，相比基线，2D平均关节误差降低31%，3D平均每关节位置误差降低76%，在不同场景复杂度下保持稳定优势。

Conclusion: 提供强力基线与完整数据资源：无需领域特定微调的3D手部姿态估计管线与大规模手术基准数据集，为手术计算机视觉研究奠定基础。

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [47] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: 提出一种在损失层面进行“类别+置信度”联合重加权的长尾学习方法，用Ω(pt, fc)同时利用样本预测置信度与类别相对频次，作为对现有logit校正方法的互补；在CIFAR-100-LT、ImageNet-LT、iNaturalist2018上显著提升。


<details>
  <summary>Details</summary>
Motivation: 长尾分布中头部类样本占主导、尾部类样本稀少，导致深度模型对尾部类性能下降。现有工作多在决策/对数it层进行先验偏置校正，较少关注由样本置信度差异引起的优化过程不平衡问题。

Method: 提出在损失函数级别的类与置信度感知重加权方案：通过Ω(pt, fc)函数根据样本预测置信度pt与类别相对频次fc调节每个样本对训练的贡献；与基于logit的校正方法正交可叠加。

Result: 在CIFAR-100-LT、ImageNet-LT、iNaturalist2018等多数据集及不同不均衡因子下，实验显示该方案带来显著性能提升，实验观察与理论分析一致。

Conclusion: 基于损失层面的类与置信度联合重加权能有效缓解长尾学习中的优化不平衡，与logit层调整方法互补，具有普适性与实证效果。

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [48] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: 提出NeuroMamba：结合线性复杂度Mamba的全局建模与局部边界强化的多视角神经元分割框架，既保持长程依赖又保留体素级细节，在4个EM数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有CNN缺乏长程上下文易致边界歧义；Transformer需切块导致体素细节丢失、边界不精确。需要一种既能高效全局建模又能保留细粒度边界信息的方法，适配不同分辨率（各向异性/各向同性）EM数据。

Method: 构建多视角框架NeuroMamba：1) 设计通道门控的边界判别特征提取器（BDFE）强化局部形态与边界线索；2) 在Visual Mamba中引入分辨率感知的扫描机制，形成空间连续特征提取器（SCFE），实现无需切块的全局依赖建模并适配不同分辨率；3) 通过跨调制机制融合局部与全局特征，实现互补增强；整体利用Mamba的线性复杂度进行高效全局建模。

Result: 在四个公开EM数据集上取得SOTA性能，展示对各向异性与各向同性分辨率数据的强泛化与适配能力。

Conclusion: NeuroMamba通过无切块的全局建模与边界敏感的局部特征学习协同提升神经元分割精度与边界质量，兼具效率与鲁棒性，适用于多分辨率EM数据；代码将开源。

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [49] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: EvolSplat4D是一种用于城市静态与动态场景的新视角合成的前馈框架，融合体素与像素层面的高斯表示，通过三分支设计兼顾质量、速度与全景覆盖，在多数据集上优于逐场景优化与现有前馈方法。


<details>
  <summary>Details</summary>
Motivation: 现有NVS方法在城市场景中难以兼顾训练/重建时间与渲染质量：NeRF/3DGS等需耗时的逐场景优化；而前馈方法多为逐像素高斯，跨视角/时间聚合时产生3D不一致，尤其在包含动态体的复杂环境中问题突出。

Method: 提出EvolSplat4D，统一体积与像素层面的高斯预测，包含三条专用分支：1) 近距离静态区域：从3D特征体直接预测多帧一致的3D高斯几何，并辅以语义增强的基于图像渲染以估计外观；2) 动态主体：基于对象中心的规范（canonical）空间与运动自适配渲染，聚合时序特征，鲁棒于噪声运动先验，实现稳定的4D重建；3) 远距背景：高效的逐像素高斯分支保证全景覆盖。

Result: 在KITTI-360、KITTI、Waymo与PandaSet上，静态与动态场景的重建精度与一致性均优于逐场景优化方法与最先进前馈基线，表明在质量与效率上实现新的权衡优势。

Conclusion: 通过三分支统一高斯表示与时空特征聚合，EvolSplat4D在城市静/动态NVS中实现高质量、快速、稳定的4D重建，为自动驾驶仿真提供更实用的前馈方案。

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [50] [HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models](https://arxiv.org/abs/2601.15968)
*Xin Xie,Jiaxian Guo,Dong Gong*

Main category: cs.CV

TL;DR: 提出HyperAlign：通过超网络在推理时生成低秩适配权重，动态调制扩散模型算子，实现高效的偏好对齐，优于微调与测试时缩放方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽性能强，但常与人类偏好不符，出现审美差、语义不一致等问题。现有对齐方法两难：微调易因奖励过优化而丧失多样性；测试时缩放开销大且易欠优化。

Method: 训练一个超网络，根据输入潜变量、时间步与文本提示，生成用于调制扩散模型生成算子的低秩适配（LoRA样式）权重，实现奖励条件的自适应对齐。设计多种应用频率的变体在性能与效率间折中；以奖励分数为目标并用偏好数据正则化，抑制奖励黑客。

Result: 在Stable Diffusion、FLUX等扩展生成范式上评测，HyperAlign显著提升语义一致性与视觉吸引力，优于主流微调与测试时缩放基线。

Conclusion: HyperAlign以低开销实现高效可控的推理期对齐，兼顾性能、效率与多样性，并通过偏好正则减轻奖励黑客，成为扩散模型对齐的有效新范式。

Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.

</details>


### [51] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: PhysicsMind提出统一基准，评估MLLM与视频生成模型是否遵守三大经典力学原理，通过VQA与视频生成两类任务，发现现有模型常凭外观启发式而违背物理规律。


<details>
  <summary>Details</summary>
Motivation: 现有评测零散：多为模板化VQA或关注视频感知质量，无法系统、可信地衡量模型对基础物理（如质心、杠杆、惯性）的理解与遵守程度。需要一个兼具真实与仿真环境、同时覆盖推理与生成的统一基准。

Method: 构建PhysicsMind基准，围绕质心、杠杆平衡、牛顿第一定律三原则：1）VQA任务，从图像/短视频估计物理量与判断；2）视频生成任务，检验预测轨迹是否满足与真值一致的质心、力矩与惯性约束。数据含真实与仿真场景。对多种MLLM与视频生成模型进行系统评测。

Result: 多款近期MLLM与视频生成模型在PhysicsMind上表现出依赖外观启发式，常违反基本力学约束；相较感知质量，物理一致性薄弱，显示当前能力与鲁棒的物理理解存在显著差距。

Conclusion: 仅靠规模化与常规训练不足以获得稳健物理理解。PhysicsMind提供聚焦的物理一致性测试平台，促使未来多模态模型在遵守物理定律的推理与生成上改进。

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [52] [Keyframe-Based Feed-Forward Visual Odometry](https://arxiv.org/abs/2601.16020)
*Weichen Dai,Wenhan Su,Da Kong,Yuhang Ming,Wanzeng Kong*

Main category: cs.CV

TL;DR: 提出一种结合关键帧策略的端到端前向视觉里程计，通过强化学习自适应选择关键帧，减少冗余、提升精度与效率，在多数据集上优于现有前向VO方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉大模型的VO/SLAM虽能端到端完成位姿与稠密重建，但对原始序列无差别处理，导致低视差帧引入计算冗余与性能下降；传统几何启发式关键帧策略难以直接应用于依赖高维潜表示的大模型框架。需要一种与模型表征相契合、数据驱动的关键帧选择机制。

Method: 提出关键帧式前向VO：将关键帧选择建模为强化学习问题，学习策略网络根据大模型的潜在特征/上下文自适应挑选关键帧；在TartanAir上训练代理，使其对低视差、冗余帧抑制，同时保留提供有效立体上下文的帧；整体仍为单次前向推理框架，无手工规则。

Result: 在多个真实数据集上进行广泛评测，相比SOTA前向VO方法（如VGGT-Long等）取得稳定且显著的性能提升，并减少计算冗余。

Conclusion: 数据驱动的强化学习关键帧策略能有效弥合传统几何启发与视觉基础模型之间的鸿沟，在不破坏前向端到端推理范式的前提下，提升VO的精度与效率，具有跨数据集的泛化优势。

Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

</details>


### [53] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: 提出PAINT：以结构优先的自回归框架把H&E到IHC合成重构为“先结构、后分子”的条件生成，用3S-Map保证与形态对齐，提升结构一致性与下游临床表现。


<details>
  <summary>Details</summary>
Motivation: 现有H&E→IHC虚拟染色多做直接外观翻译，缺少明确结构先验，易出现语义/结构不一致；而H&E形态对蛋白表达提示模糊、同形态可对应多分子状态，需更稳健的跨模态生成范式。

Method: 提出PAINT（Pathology-Aware Integrated Next-Scale Transformation）：将任务表述为结构优先的条件生成，按因果顺序先确定全局结构再细化分子细节；核心引入空间结构起始图3S-Map，将自回归初始化锚定在已观测的形态上，实现确定性、空间对齐的生成。与直接图像翻译不同，采用视觉自回归逐步细化。

Result: 在IHC4BC与MIST数据集上，相比SOTA方法，PAINT在结构保真度指标和临床下游任务表现均更优。

Conclusion: 结构引导的自回归建模与3S-Map能显著缓解跨模态语义不一致问题，支持更可靠的虚拟IHC合成并具备临床应用潜力。

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [54] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: 提出ProGiDiff：利用预训练扩散模型与ControlNet式条件编码器，实现可由自然语言提示控制的医学图像（CT/MR）多类别分割，并支持多提案与少样本跨模态迁移。


<details>
  <summary>Details</summary>
Motivation: 现有医学分割方法多为确定性且难以与自然语言交互，缺乏多提案估计、人机交互与跨模态适配能力；直接训练文本到图像扩散模型又需大规模数据，且通常只做二分类、难以被自然语言条件化。

Method: 提出ProGiDiff：在预训练扩散模型上加入类似ControlNet的条件机制，配套自定义图像条件编码器以引导扩散模型输出分割掩码；通过自然语言提示指定目标器官，天然扩展到多类别；并设计低秩、少样本的自适应策略，实现从CT到MR的迁移。

Result: 在CT器官分割上优于既有方法，能生成多种分割提案，利于专家在环的交互；所学条件机制可通过低秩、少样本适配成功迁移到MR分割。

Conclusion: ProGiDiff以预训练扩散模型+ControlNet式条件实现可提示、多提案、多类与跨模态的医学分割，在数据受限场景下具备实用性并支持专家交互。

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [55] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: 提出一种即插即用的“分散注意力图像token剪枝”(DTP)方法，在线检测并裁剪任务无关的视觉token，重塑VLA模型的注意力分布，在不改架构不加输入的前提下提升操控任务成功率，并在SIMPLER基准上对多种VLA泛化有效。


<details>
  <summary>Details</summary>
Motivation: VLA能端到端从视觉理解到动作输出，但常把注意力分配给任务无关的图像区域（干扰token），干扰动作token生成，导致任务成功率下降。需要一种不改模型、可泛化的方法，纠正注意力并提升表现，同时探查模型潜在上限。

Method: 提出Distracting Token Pruning：在推理中动态估计每步注意力中落在任务无关区域的图像token，按得分阈值/比例剪枝这些token，使后续注意力集中于关键区域；方法为即插即用，不改网络、无需额外输入，适配Transformer式VLA。

Result: 在SIMPLER基准上，对多种新型VLA均带来稳定的相对成功率提升，表现出良好通用性；相关性分析显示任务成功率与任务无关区域注意力占比呈显著负相关。

Conclusion: VLA存在普遍的“注意力跑偏”问题；通过DTP可有效矫正注意力并提升操控成功率，且不破坏原模型结构。该现象与方法为后续改进VLA提供方向与上界参考。

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [56] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: 提出DSFedMed：在联邦环境中通过中央FM与轻量客户端模型的双尺度互蒸馏，实现医图分割的高效与可扩展；Dice均值+2%，通信与推理成本降近90%。


<details>
  <summary>Details</summary>
Motivation: FM在视觉任务上泛化强，但在联邦场景中受限于计算、通信与推理成本高，难以落地；需要既能保留FM泛化能力、又能在资源受限端高效部署的机制。

Method: 构建双尺度联邦框架DSFedMed：中央侧为基础模型（FM），客户端为轻量模型；通过互知识蒸馏在两端之间传递与融合知识。为支撑蒸馏，生成一组高质量医疗影像替代真实公共数据；并提出基于可学习性引导的样本选择策略，提高双尺度蒸馏的效率与有效性。轻端获FM的通用知识，FM吸收各客户端的特定知识进行精炼。

Result: 在5个医图分割数据集上，较现有联邦FM基线，平均Dice提升约2个百分点；通信开销与推理时间减少近90%。

Conclusion: DSFedMed在资源受限的联邦部署中兼顾精度与效率，具备良好可扩展性，显示互蒸馏与合成数据/样本选择策略的有效性。

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [57] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: 提出MoRo：一种基于掩码生成建模的单目视频人体运动重建方法，在严重遮挡下仍能稳健重建，并以端到端、实时（70 FPS）推理实现高精度与高真实性。


<details>
  <summary>Details</summary>
Motivation: 单目视频人体运动重建在AR/VR、机器人等领域关键，但现实场景频繁遮挡导致观测缺失。回归法高效但易崩；优化/扩散法更稳却慢且依赖繁重预处理。需要一种既稳健应对遮挡又高效端到端的方案。

Method: 将重建视作“视频条件的掩码生成”任务：使用生成式掩码建模来天然处理遮挡并实现端到端推理。为解决视频-动作配对数据稀缺，提出跨模态学习：1) 在MoCap上训练“具轨迹感知”的运动先验；2) 在图像-姿态数据上训练图像条件姿态先验以覆盖多样帧级姿态；3) 以视频条件的掩码Transformer融合上述先验，并在视频-动作数据上微调，以结合视觉线索与运动动力学，最终在一致的全局坐标系下恢复动作。

Result: 在EgoBody与RICH数据集上，在遮挡场景显著超越SOTA的精度与运动真实感；非遮挡场景表现与SOTA相当；在单块H200上可实时推理达70 FPS。

Conclusion: MoRo通过掩码生成与跨模态先验有效解决遮挡鲁棒性与效率的权衡，提供无需繁重预处理的端到端实时重建方案，并在真实遮挡环境中显著提升准确性与自然度。

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [58] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: SAMTok 将任意区域分割掩码离散化为两个“特殊掩码 token”，从而把像素级任务转化为标准语言建模的下一个词预测/强化学习问题，免去复杂像素分支与特定损失，实现可扩展的像素级多模态能力，并在多项区域理解与生成任务上达到SOTA/可比水平。


<details>
  <summary>Details</summary>
Motivation: 像素级交互智能系统需要精细的区域理解与生成，但现有像素级 MLLM 依赖复杂的区域编码器、专用分割解码器和不兼容的训练目标，导致难以扩展与统一训练。作者希望以最小架构改动、通用目标函数，赋予现成 MLLM 强像素级能力。

Method: 提出 SAMTok：基于 SAM2 的掩码离散化器。用掩码编码器+残差向量量化（RVQ）把任意掩码编码为两个离散 token，并可高保真重建掩码。将掩码视为“语言 token”，在基座 MLLM（如 QwenVL）上用常规下一词预测训练，并配合简单的强化学习。构建了 2.09 亿多样掩码训练 SAMTok，并制作 500 万条 SAMTok 格式数据用于掩码理解与生成；另外引入“文本答案匹配（TAM）”奖励以高效优化掩码生成。

Result: QwenVL-SAMTok 在区域描述、区域 VQA、定点对话、指代分割、场景图解析、多轮交互分割等任务上达到 SOTA 或可比表现；TAM 奖励的强化学习显著提升 GRES 与 GCG 基准成绩；在不改动基座架构/无需专用损失的前提下实现强像素级能力。

Conclusion: 将掩码离散化为少量离散 token 可把像素级任务统一进语言建模与轻量 RL 框架，提供易扩展、实现简洁且效果强劲的范式，为为 MLLM 赋予像素级交互与生成能力提供可复制路径；代码与模型已开源。

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [59] [Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2601.16098)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Quinn Ledingham,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出CSSMamba框架，通过聚类引导的空间-光谱Mamba与注意力驱动的token选择，实现更短更优token序列与更强特征学习，在多个HSI数据集上优于SOTA且边界保持更好。


<details>
  <summary>Details</summary>
Motivation: Mamba在HSI分类上有潜力，但受限于如何定义高效、自适应的token序列以及同时高效建模空间与光谱信息；长序列造成计算与学习负担，且缺乏与聚类先验的有机融合。

Method: 1) 设计CSpaMamba：将聚类机制嵌入空间Mamba，用聚类引导压缩/重排token，缩短序列并增强空间特征学习；2) 结合SpeMamba构成空间-光谱双模块；3) 注意力驱动的Token选择（ADTS）进一步优化序列构成与顺序；4) 可学习聚类模块（LCM）端到端学习聚类成员关系，使聚类与Mamba无缝耦合。

Result: 在Pavia University、Indian Pines、Liao-Ning 01三数据集上，相较CNN、Transformer与现有Mamba方法取得更高分类精度，并在边界保持方面更优。

Conclusion: 通过聚类引导与注意力选择，CSSMamba有效构建高效自适应的token序列并联合建模空间与光谱信息，提升HSI分类准确率与边界质量，优于当前主流方法。

Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

</details>


### [60] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: EDIR提出一个通过可控图像编辑合成查询的细粒度CIR基准，涵盖5大类15子类，共5,000条，高度揭示现有多模态嵌入模型在不同修改类型上的显著性能缺口，并通过域内训练验证其可用性与挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有CIR基准类别单一、覆盖面不足且存在模态偏置，难以反映真实场景下多样的查询需求，需要一个可控、细粒度且广覆盖的评测来客观衡量模型能力。

Method: 构建一个基于图像编辑的合成流水线，对修改类型与内容进行精确控制，生成跨多类别的查询；据此构建EDIR数据集（5大类、15子类、5,000高质量查询）；组织13个多模态嵌入模型进行系统评测，并开展对比分析与域内训练实验。

Result: 评测显示显著能力鸿沟：包括RzenEmbed和GME在内的SOTA模型在不同子类上一致性欠佳；发现现有基准存在模态偏置与类别覆盖不足等问题；域内训练在部分类别上有效，但也暴露出当前模型结构的内在局限。

Conclusion: EDIR是一个更严苛且细粒度的CIR基准，能够更全面地揭示模型在不同编辑类型下的真实能力与短板；其可控的合成方式既用于评测也利于有针对性的训练，同时指出未来需在架构与数据层面解决的固有限制。

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [61] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 提出DistSeal：在生成模型的潜空间进行统一水印，适用于扩散与自回归模型；通过蒸馏把潜空间水印器注入生成模型或解码器，实现高效、难察觉且健壮的水印，较像素空间基线最高可提速20倍。


<details>
  <summary>Details</summary>
Motivation: 像素空间的后处理水印往往带来计算开销、可见伪影与与生成流程解耦的问题；同时跨不同生成范式（扩散/自回归）的一致水印框架缺乏。需要一种更高效、可泛化、与模型更紧耦合且鲁棒的水印方案。

Method: 在生成模型的潜空间训练后置水印器（latent watermarker），再通过蒸馏将其整合到生成模型（in-model）或潜空间解码器中，实现生成时内嵌水印；比较从潜空间水印器与像素空间水印器进行蒸馏的差异，并评估不可感知性、鲁棒性与速度。

Result: 潜空间水印在不可感知性上与像素空间方法相当，但鲁棒性具有竞争力；内嵌后显著加速，最高达20倍；从潜空间进行蒸馏优于从像素空间蒸馏，在效率与鲁棒性上都更好；方法适用于扩散与自回归两类模型。

Conclusion: DistSeal提供了统一的潜空间水印方案，可无缝整合进生成模型或解码器，实现更快、更稳、更难察觉的水印；潜空间蒸馏优于像素空间蒸馏，指明了未来高效鲁棒水印的方向。

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [62] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: ActionMesh 提出“时序3D扩散”框架，端到端生成可动画的高质量一致拓扑网格，支持视频/文本/网格+文本输入，速度快、无需绑定，达SOTA 几何与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有动画3D生成方法在实际应用中受限：设置苛刻、运行缓慢、质量或可用性不足（如需要绑定、拓扑不一致）。需要一种既快又高质、可直接用于生产的动画网格生成方案。

Method: 1) 将3D扩散模型扩展到时间轴：在扩散阶段直接生成一段同步潜变量，表示随时间变化但相互独立的3D形状序列（“时序3D扩散”）。2) 设计时序3D自编码器：把独立形状序列映射为相对于预定义参考形状的形变场，从而得到一致拓扑、无绑定的动画网格。3) 支持多模态条件（单目视频、文本、或网格+文本），以前向推理快速生成。

Result: 在 Consistent4D、Objaverse 等视频到4D基准上取得SOTA：几何精度和时序一致性优于现有方法；推理速度显著提升；生成结果为一致拓扑、无绑定网格，便于后续纹理、重定向等应用。

Conclusion: ActionMesh 通过“时序3D扩散+时序3D自编码器”高效生成可动画的生产级网格，实现多模态到4D的快速高质生成，兼顾速度、拓扑一致性与无绑定优势，适用于实际内容创作与下游应用。

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [63] [HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval](https://arxiv.org/abs/2601.16155)
*Zequn Xie,Xin Liu,Boyun Zhang,Yuxiao Lin,Sihang Cai,Tao Jin*

Main category: cs.CV

TL;DR: 提出HVD框架，通过“帧级筛选+补丁级压缩”的粗到细对齐，缓解文本稀疏导致的视频特征交互“失明”问题，在五个基准上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: CLIP推动了文本-视频检索，但文本查询稀疏、视频信息冗余导致模型难以聚焦关键信息，出现“盲目”交互；作者受人类视觉的宏-微感知启发，想让模型自动聚焦关键帧与显著实体。

Method: 构建Human Vision-Driven(HVD)模型，包含两模块：1) FFSM帧特征选择模块，模拟宏观感知，筛选关键帧以去除时间冗余；2) PFCM补丁特征压缩模块，模拟微观感知，通过改进注意力将补丁聚合为显著视觉实体，实现实体级对齐；整体形成粗到细的对齐流程。

Result: 在五个基准数据集上进行大量实验，显示HVD能更好地抓取人类式视觉关注点，并在检索性能上达到或超过现有SOTA。

Conclusion: 人类视觉驱动的粗到细对齐（关键帧选择+实体级压缩）有效缓解文本稀疏与视频冗余的错配问题，显著提升文本-视频检索效果并达到SOTA。

Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.

</details>


### [64] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 提出360Anything：无需相机标定的从透视图像/视频到360°全景生成框架，基于预训练扩散Transformer，将输入与目标视为token序列学习映射，并通过环形潜编码消除ERP接缝伪影，图像和视频任务均达SOTA，同时展现零样本视场与朝向估计能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖透视与ERP空间的显式几何对齐，需要精确相机元数据，不适用于缺乏或噪声标定的“野外”数据；同时ERP边界接缝伪影常见，影响全景质量。

Method: 以几何无关的、数据驱动的方式学习透视到ERP映射：在预训练扩散Transformer上，将透视输入与全景目标统一为token序列进行条件生成；发现VAE编码器零填充导致ERP边界伪影，提出Circular Latent Encoding（环形潜编码）以周期边界条件替代零填充，实现无缝生成；同一框架扩展到视频。

Result: 在图像与视频的透视转360°任务上达到SOTA，优于依赖真实相机信息的先前方法；边界接缝显著减少；在零样本相机视场（FoV）与朝向估计基准上取得有竞争力结果。

Conclusion: 360Anything无需相机几何先验即可高质量生成全景，并通过环形潜编码解决ERP接缝问题；其学到的几何表征支持下游相机参数估计，展示广泛实用性。

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [65] [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2601.16208)
*Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie*

Main category: cs.CV

TL;DR: 论文探讨将表示自编码器（RAE）从 ImageNet 语义高维潜空间的扩散建模扩展到大规模自由文本生成图像（T2I），并系统比较于当前主流的 FLUX VAE；结果显示在预训练与微调阶段 RAE 更稳、更快收敛、质量更优，且框架在规模化后可大幅简化。


<details>
  <summary>Details</summary>
Motivation: 现有 T2I 多采用 VAE 潜空间，但在大模型与长训练中易过拟合且收敛慢；RAE 在 ImageNet 上显示潜力，尚不清楚能否在大规模自由域 T2I 上可扩展并优于 VAE，同时需要厘清 RAE 设计中哪些组件在规模化下仍关键。

Method: 1) 以冻结的表征编码器（SigLIP-2）为基础，扩展并训练 RAE 解码器，数据涵盖网页、合成与文字渲染，研究数据配比对特定域（如文本渲染）的影响；2) 对 ImageNet 版 RAE 设计做应力测试，评估降噪日程、宽头、噪声增强解码等组件在大规模下的作用；3) 在 0.5B–9.8B 参数的扩散 Transformer 规模上，受控对比 RAE 与 FLUX VAE，于预训练与高质量数据微调阶段评测收敛速度、过拟合与生成质量。

Result: - 扩展 RAE 解码器后，规模提升带来整体保真度提升，但特定域需目标化数据配比；- 规模化下，维度相关的噪声日程仍关键，而宽扩散头与噪声增强解码收益可忽略；- 跨全部模型规模，RAE 在预训练中始终优于 VAE；在高质量数据微调中，VAE 在约64轮后灾难性过拟合，而 RAE 稳定至256轮并取得更好效果；- RAE 模型收敛更快、生成质量更高。

Conclusion: RAE 为大规模 T2I 提供更简单且更强的基础，相较 VAE 具备更快收敛、更稳训练与更佳生成质量；统一的表征空间还使理解与生成共享潜表征，从而可直接在生成潜变量上做多模态推理，利于构建统一模型。

Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.

</details>


### [66] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: PyraTok 提出一种语言对齐的金字塔式视频离散化 tokenizer，在多尺度上量化视频特征并与文本联合训练，显著提升重建、文生视频与多项零样本视频理解任务的表现，并可扩展至超高分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有离散视频 VAE 的 tokenizer 多在单一尺度、词表小且语言监督浅，导致视觉-语言对齐弱、跨任务零样本迁移差。

Method: 在预训练视频 VAE 基础上，引入语言对齐的金字塔量化模块 LaPQ：在编码器多层深度以共享的大型二进制码本进行多尺度离散化，形成层级化的紧凑视频 token；并联合优化两部分目标：多尺度的文本引导量化损失与作用于整个 token 层级的全局自回归目标，从而实现强语义对齐与层级建模。

Result: 在10个基准上取得SOTA：更好的视频重建质量，稳定提升文生视频质量；在视频分割、时间动作定位与视频理解的零样本设置下创下新SOTA；在分辨率上可鲁棒扩展至4K/8K。

Conclusion: 多尺度、共享码本的金字塔离散化结合语言对齐与全局自回归训练，可得到语义结构化的视频离散表示，兼顾压缩性与表达力，显著增强跨模态对齐与零样本迁移，并具备高分辨率可扩展性。

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [67] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: 论文研究组合式视频理解（CVU），指出现有零样本组合动作识别（ZS-CAR）主要因“由物体驱动的动词捷径”失败，并提出RCORE框架，通过组合感知增强与时间顺序正则，强化有时间依据的动词学习，提升对未见动词-物体组合的泛化。


<details>
  <summary>Details</summary>
Motivation: CVU要求模型识别动词与物体并在未见组合上泛化，但现有ZS-CAR在未见组合表现差。作者发现忽视的失败模式：模型利用物体线索猜动词，导致对共现统计过拟合、忽略视觉与时间证据。需要方法抑制这类捷径并提高动词学习的时间扎根性。

Method: 提出RCORE框架：1）组合感知的数据增强，打乱/多样化动词-物体组合，同时不破坏运动线索，缓解监督稀疏与偏斜；2）时间顺序正则化损失，显式建模时间结构，惩罚依赖物体共现的捷径，迫使模型学习与时间相关的动词表征。

Result: 在Sth-com与新构建的EK100-com两基准上，RCORE显著提高未见组合的准确率，降低对共现偏差的依赖，并稳定获得正向的组合差距（compositional gaps）。

Conclusion: ZS-CAR的关键瓶颈是由物体驱动的动词捷径。通过引入组合感知增强与时间顺序正则，RCORE实现了更稳健的组合式视频理解，在未见动词-物体组合上取得实质改进。

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [68] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出CamPilot：在相机可控视频扩散中，用高效相机感知3D解码器将视频潜变量与相机位姿解码为3D高斯，并以新视图像素一致性作为奖励，结合可见性筛选，显著增强视频-相机对齐与可控性，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有相机可控视频扩散虽改进了对齐，但可控性仍弱。直接套用ReFL有三难：1) 奖励模型无法评估视频-相机对齐；2) 需把潜变量解码到RGB计算奖励，代价高；3) 解码忽略3D几何，难以反映相机-视频失配。

Method: 设计相机感知3D解码器：将视频潜变量与相机位姿共同解码为3D高斯，并把相机位姿既作为输入又作为投影参数。若潜变量与位姿失配，会引发3D几何扭曲与渲染模糊。以此构造奖励：优化渲染新视图与真值视图的像素级一致性。为处理生成随机性，引入可见性项，通过几何变形选择性监督仅确定性的可见区域。基于此进行Reward Feedback Learning以提升相机可控性。

Result: 在RealEstate10K与WorldScore基准上广泛实验，方法显著提升视频-相机对齐与相机可控性，并更高效（避免RGB全解码）的奖励评估，得到更清晰稳定的新视图渲染。

Conclusion: 借助相机感知3D解码与可见性引导的像素一致性奖励，CamPilot有效解决ReFL在相机可控视频中的评估与效率瓶颈，提升可控性与对齐质量。

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>
