<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 236]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MOTION: ML-Assisted On-Device Low-Latency Motion Recognition](https://arxiv.org/abs/2512.00008)
*Veeramani Pugazhenthi,Wei-Hsiang Chu,Junwei Lu,Jadyn N. Miyahira,Soheil Salehi*

Main category: cs.CV

TL;DR: 提出在可穿戴设备上仅用三轴加速度计进行低延迟手势识别的高效方案，借助AutoML提取特征并评估多种轻量模型，最终选用神经网络在准确率、时延与内存间取得最佳权衡，实现WeBe Band上的实时、可靠手势识别，适用于医疗监测。


<details>
  <summary>Details</summary>
Motivation: 医疗与日常HCI场景需要在资源受限设备上进行快速、可靠的动作/手势识别，以减少误报并满足跌倒检测、康复跟踪、患者看护等对实时性与能效的要求。

Method: 使用仅含三轴加速度计的数据，基于AutoML流水线对分段数据进行特征工程与特征选择；在此基础上训练多种轻量级机器学习模型（含神经网络）并在WeBe Band的MCU上评估其准确率、延迟与内存占用，选择最优折中方案。

Result: 在多种模型中，神经网络在准确率、时延和内存使用之间达到最佳平衡；在WeBe Band上实现了可行的、可靠的实时手势识别。

Conclusion: 通过AutoML特征提取与轻量模型选择，可在资源受限可穿戴设备上实现高效、低延迟的手势识别；该方案对实时医疗监测具有应用潜力，兼顾准确性、延时与内存约束。

Abstract: The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.

</details>


### [2] [Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions](https://arxiv.org/abs/2512.00042)
*Egemen Sert,Şeyda Ertekin*

Main category: cs.CV

TL;DR: 他们构建高质量多模态SFT数据并配合优化的推理语法，对开源VL模型微调，在新基准上接近闭源SOTA，证明“数据与表示”对多模态推理至关重要。


<details>
  <summary>Details</summary>
Motivation: 多模态推理评测常依赖算法/强化学习改进，忽视数据与表示层；标准化考试题目提供严格、可验证的推理测试，因此作者想检验高质量SFT数据与表示是否能在不开源RL的情况下达到接近SOTA的效果。

Method: 汇编一个1.614亿token的多模态数据集：教材题-解对、与课程一致的图示、上下文资料；提出并采用优化的推理语法QMSA；用这些数据对Qwen-2.5VL-32B进行监督微调；构建并发布YKSUniform基准（1854道、309主题）以标准化多模态考试题评测。

Result: 微调后的模型在YKSUniform上达78.6%准确率，仅比Gemini 2.0 Flash低1.0%；显示SFT结合高质量数据与合适的推理格式可媲美专有方法。

Conclusion: 数据组成与表示语法对多模态推理性能具有决定性作用；精心策划、与课程对齐的多模态数据可将SFT推至接近最先进水平，为开源视觉语言模型提供数据中心的进步框架。

Abstract: Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.
  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.
  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.

</details>


### [3] [PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2512.00060)
*Abdolazim Rezaei,Mehdi Sookhak*

Main category: cs.CV

TL;DR: 提出PEFT-DML：一种参数高效的深度度量学习框架，将多模态传感器映射到共享潜空间，以在传感器缺失或新组合下保持稳健3D目标检测；通过LoRA与适配器层提升训练效率与在高速运动/天气/域移位下的鲁棒性，并在nuScenes上达到更高精度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D检测通常假设固定的传感器可用性与模态组合，实际自动驾驶中常遇到传感器掉线、天气影响与域移导致性能退化；同时全量微调成本高。需要一种既能适应模态变化又训练高效且具鲁棒性的框架。

Method: 采用参数高效微调（PEFT）思路，将各模态（LiDAR、雷达、相机、IMU、GNSS）通过深度度量学习映射到共享潜在空间；引入LoRA与adapter层以小参数增量适配不同模态与域；设计在传感器缺失/未见模态组合下的一致性与对比式目标函数，提升跨模态对齐与检出稳健性。

Result: 在nuScenes基准上，相较传统多模态检测方法取得更高准确率；在传感器掉线、快速运动、恶劣天气和域移等场景保持更稳健表现；训练效率显著提升（参数开销与微调成本更低）。

Conclusion: PEFT-DML通过参数高效的度量学习与共享潜空间对齐，实现对多模态3D检测的鲁棒与高效训练，能在不确定的传感器可用性与复杂环境下保持高精度，适用于实际自动驾驶部署。

Abstract: This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.

</details>


### [4] [DL-CapsNet: A Deep and Light Capsule Network](https://arxiv.org/abs/2512.00061)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 提出深层胶囊网络DL-CapsNet：多级胶囊层+“胶囊汇总”层，参数更少、训练推理更快，同时在多类别、重叠与仿射变换场景下保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统CNN对姿态/仿射变换与类别重叠鲁棒性不足；现有CapsNet虽在这方面更优，但层数浅、参数与计算开销高、难以扩展到高类别复杂数据集。作者希望在保持CapsNet判别优势的同时降低参数与复杂度、提升可扩展性与速度。

Method: 构建“深层CapsNet”（DL-CapsNet），堆叠多级胶囊层以增强表征；引入“胶囊汇总（Capsule Summarization）”层，在层间对胶囊进行聚合/降维，减少路由所需参数与计算，整体实现更深却更轻的结构；优化训练与推理流程以提升速度。

Result: DL-CapsNet在复杂、高类别数据集上实现更高或相当的准确率，同时显著减少参数量，并带来更快的训练与推理。

Conclusion: 通过多层胶囊与汇总机制，DL-CapsNet在保持/提升精度的同时降低模型复杂度并提升效率，适用于多类别、类别重叠与仿射变换明显的视觉任务。

Abstract: Capsule Network (CapsNet) is among the promising classifiers and a possible successor of the classifiers built based on Convolutional Neural Network (CNN). CapsNet is more accurate than CNNs in detecting images with overlapping categories and those with applied affine transformations. In this work, we propose a deep variant of CapsNet consisting of several capsule layers. In addition, we design the Capsule Summarization layer to reduce the complexity by reducing the number of parameters. DL-CapsNet, while being highly accurate, employs a small number of parameters and delivers faster training and inference. DL-CapsNet can process complex datasets with a high number of categories.

</details>


### [5] [Satellite to Street : Disaster Impact Estimator](https://arxiv.org/abs/2512.00065)
*Sreesritha Sai,Sai Venkata Suma Sreeja,Deepthi,Nikhil*

Main category: cs.CV

TL;DR: 提出“Satellite-to-Street: Disaster Impact Estimator”，一种联合处理灾前/灾后卫星图像的深度学习框架，基于改进的双输入U-Net与增强特征融合，并配合类别感知加权损失，以缓解类别不平衡并提升对高破坏区域的检测。实验表明较传统分割与变更检测基线在定位与分级上更优，能快速、稳定地产出精细化损伤图以辅助应急决策。


<details>
  <summary>Details</summary>
Motivation: 灾后快速、客观、可规模化的损伤评估至关重要；人工判读慢且主观，现有U-Net或变更检测方法对细微结构变化和严重类别不平衡敏感度不足，导致重度损伤检出率低。

Method: 构建深度学习框架：以改进的双输入U-Net为骨干，联合处理灾前与灾后影像；通过增强的多尺度/跨层特征融合捕捉局部结构变化与全局语境；在训练中引入类别感知加权损失（如对“严重/摧毁”类别加权）以对抗未损坏像素的主导地位，获得细粒度像素级损伤分布。

Result: 在公开灾害数据集上，相较传统语义分割与基线变更检测模型，模型在损伤定位与分级精度上均提升，尤其在“重大/被毁”类别的召回与总体检测质量上有改善。

Conclusion: 该方法能更快、更一致地产出精细化损伤图，用于支持而非取代专家判读，从而提升数据驱动的灾害响应与资源优先级分配效率。

Abstract: Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.

</details>


### [6] [ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN](https://arxiv.org/abs/2512.00073)
*Aswinkumar Varathakumaran,Nirmala Paramanandham*

Main category: cs.CV

TL;DR: 提出ProvRain管线：用轻量MobileNet‑U‑Net做去雨/去噪并配合课程式训练，结合PVDN与合成数据，前置于检测器以改善雨夜车辆预警检测；相较仅用PVDN训练的Faster R‑CNN，显著提升准确率、召回率与感知质量指标。


<details>
  <summary>Details</summary>
Motivation: 夜间预警式（未入画前）车辆检测依赖除车灯外的弱特征，雨雪与传感器噪声导致检测易失败；需要一种既能鲁棒去噪又不牺牲检测准确性的端到端方案。

Method: 构建ProvRain两阶段流水线：1) 轻量化MobileNet‑U‑Net作为图像复原/去雨前端；2) 下游车辆检测器（基线为PVDN上训练的Faster R‑CNN）。通过课程式训练提升在多种恶劣天气下的泛化，训练数据为PVDN与合成雨夜数据混合。对比仅检测器方案，评估检测与图像质量指标（PSNR/SSIM/LPIPS）。

Result: 雨夜检测中，相比仅用Faster R‑CNN：准确率+8.94%，召回率+10.25%。复原模型相较其它Transformer方法：PSNR提升10–15%，SSIM提升5–6%，LPIPS感知误差最多下降67%。

Conclusion: 在雨夜场景下，先去噪/去雨再检测的ProvRain显著提升预警车辆检测性能；轻量MobileNet‑U‑Net结合课程学习能在保持效率的同时增强鲁棒性，优于仅检测或部分Transformer复原方法。

Abstract: Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\% increase in accuracy and a 10.25\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\% improvement in PSNR, a 5-6\% increase in SSIM, and upto a 67\% reduction in perceptual error (LPIPS) compared to other transformer approaches.

</details>


### [7] [Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation](https://arxiv.org/abs/2512.00075)
*Jun Jia,Hongyi Miao,Yingjie Zhou,Wangqiu Zhou,Jianbo Zhang,Linhan Cao,Dandan Zhu,Hua Yang,Xiongkuo Min,Wei Sun,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出Adapter Shield：在零样本图像到图像扩散生成场景中，通过“加密可逆的嵌入+认证解密”阻止未授权的身份/风格滥用，同时允许授权用户正常生成。


<details>
  <summary>Details</summary>
Motivation: 零样本图像到图像生成可用单张人像或艺术图无权复制身份/风格，带来知识产权与隐私风险；现有防护要么不通用、要么影响授权使用，缺乏同时支持强防护与访问控制的一体化方案。

Method: 分析零样本方法利用图像编码器得到嵌入，并通过交叉注意力注入UNet。基于此构建：1）可逆加密系统：将原始嵌入按密钥映射为加密嵌入；授权用户用解密模块+正确密钥还原真实嵌入；2）多目标对抗扰动：对输入图像施加小扰动，使其在编码器空间中被推向指定加密模式，从而在未授权管线中仅得到失真/加密输出。

Result: 在广泛评测中，相比现有最先进防护方法，本方法更有效阻止未授权的零样本生成，并保持授权用户的高保真可用性与灵活的安全访问控制。

Conclusion: Adapter Shield以通用、可认证的嵌入级加密与对抗扰动相结合，为个人图像在零样本扩散生成中的滥用提供有效防护，同时不牺牲授权使用体验。

Abstract: With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.

</details>


### [8] [Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection](https://arxiv.org/abs/2512.00078)
*Mario de Jesus da Graca,Jörg Dahlkemper,Peer Stelldinger*

Main category: cs.CV

TL;DR: 用扩散模型生成合成明场显微图像，并与真实数据混训，能以低成本提升细胞目标检测（YOLOv8/YOLOv9/RT-DETR）性能；人类专家难以分辨真伪。


<details>
  <summary>Details</summary>
Motivation: 明场显微细胞检测依赖大量标注数据，但显微数据难获取且标注成本高，限制了深度学习检测器的性能与泛化能力。作者探索用无条件生成模型合成图像以缓解数据稀缺与标注瓶颈。

Method: 训练基于U-Net的扩散模型（无条件）生成高逼真显微图像；构建不同“合成/真实”比例的数据集；使用YOLOv8、YOLOv9、RT-DETR进行训练与评估；开展人类专家盲测以评估合成图像的真实感。

Result: 在多种检测器上，加入合成数据可提升检测准确率，且成本低；专家对真假图像的区分准确率为50%，显示合成图像与真实图像难以区分。

Conclusion: 扩散模型生成的显微合成数据能有效增强训练集，减少人工标注依赖，并可能提升细胞检测模型的稳健性，是显微图像分析中可行且有前景的数据增广策略。

Abstract: Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.

</details>


### [9] [Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels](https://arxiv.org/abs/2512.00080)
*André Dehne,Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 论文提出用深度视觉双目里程计（DVSO）替代/补充现有基于激光与二维码的MARWIN机器人的隧道导航，以在XFEL这类长且单调的加速器隧道中获得更灵活的定位与运动估计。DVSO利用双目视差与光流，通过自监督联合估计深度与自运动，并可与绝对参考融合以实现全局一致。


<details>
  <summary>Details</summary>
Motivation: 现有方案依赖激光边缘、轮/激光里程计与二维码基准，在预定义路段较稳健，但对未知几何与障碍缺乏适应性，难以在长、单调且传统定位困难的隧道中实现更自主的导航。需要一种低成本、可扩展、在弱标注条件下仍可学习的视觉定位替代方案。

Method: 探索并概念性评估DVSO：以双目视觉为核心，结合光流与自监督学习，联合估计尺度一致的深度与自运动；利用3D几何约束减小尺度漂移；后端可与地标/其他传感器进行融合以保证全局一致性。以欧洲XFEL隧道为案例进行场景化分析。

Result: 未给出实装实验数据，提供概念性评估与可预期收益/挑战：收益——双目降低尺度漂移、低成本传感器、数据采集可扩展；挑战——低纹理与光照变化、计算负载、辐射环境鲁棒性。

Conclusion: DVSO在加速器隧道具有潜在优势，可提升MARWIN在受限与安全关键环境中的自主性；需开展针对低纹理、光照与辐射鲁棒性、算力与多传感器融合的研究 agenda，以实现可落地的全局一致导航。

Abstract: The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.

</details>


### [10] [Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages](https://arxiv.org/abs/2512.00082)
*Divendar Murtadak,Yoon Kim,Trilokya Akula*

Main category: cs.CV

TL;DR: 研究比较“诊断式提示”与基于格式塔原则的常规提示，在评估亚马逊搜索结果页（SRP）视觉复杂度任务上对多模态大语言模型（MLLM）可靠性的影响。诊断式提示显著提升与人类判断的一致性（F1从0.031到0.297），但总体一致性仍低（κ=0.071）。模型更看重视觉设计杂乱（如徽章拥挤），而人类更关注内容相似性；模型在产品相似性与色彩强度感知上仍易失败。结论：诊断式提示是走向人类对齐的初步可行方向，但需更大标注集与更优提示以达可用可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在UI/网页等复杂视觉场景的主观属性评估（如“视觉复杂度”）上可靠性不足。仅用高层原则（如格式塔）提示难以引导模型产生可解释、与人类一致的判断。作者希望通过更细粒度、面向诊断维度的提示，引导模型显式评估关键因素，从而提升与人类一致性并揭示人机推理差异。

Method: - 数据：200个亚马逊SRP页面，配有人类专家复杂度标注。
- 比较：诊断式提示（分解为若干可诊断子维度）vs. 基于格式塔原则的常规提示。
- 评估：预测与人类标注的一致性（F1、Cohen’s κ），并用决策树分析模型特征重要性；对失败案例做误差分析。

Result: - F1由0.031→0.297（相对+858%）；整体一致性仍低（κ=0.071）。
- 决策树：模型偏重视觉设计元素，如“徽章拥挤/杂乱”（重要性38.6%），与人类关注的“内容相似性”存在偏差。
- 失败点：产品相似性识别、颜色强度/饱和度感知等视觉感知仍不稳健。

Conclusion: 诊断式提示可明显提升MLLM在SRP视觉复杂度评估中的人类对齐度，但绝对性能仍不足以支撑可靠部署。未来需：更大规模高质量真值数据、改进与扩展诊断维度的提示、强化模型在相似性与颜色感知方面的视觉能力，以减少系统性的人机分歧。

Abstract: This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\% relative improvement), though absolute performance remains modest (Cohen's $κ$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.

</details>


### [11] [A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation](https://arxiv.org/abs/2512.00084)
*Venkata Siddharth Dhara,Pawan Kumar*

Main category: cs.CV

TL;DR: 提出FastTextDiff：在扩散分割框架中引入长文本医疗注释，利用ModernBERT替代Clinical BioBERT，通过跨模态注意力提升少标注条件下的分割精度与训练效率。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割往往依赖昂贵的像素级标注。扩散模型虽强，但在低标注场景下语义监督不足。医疗文献/病历包含丰富语义，如能高效对齐到图像，将缓解标注稀缺并提升分割。

Method: 构建FastTextDiff：在扩散式分割管线中引入文本编码器，用ModernBERT对MIMIC-III/IV临床长文本进行编码，并与视觉特征做跨模态注意力引导扩散去噪与特征学习；以ModernBERT替代Clinical BioBERT，借助FlashAttention 2、交替注意力与2T语料预训练，实现更快更强的文本表示；整体作为弱/少标注的条件扩散分割模型。

Result: 与传统扩散分割模型和使用Clinical BioBERT的版本相比，采用ModernBERT的FastTextDiff在分割准确率与训练效率均有提升；验证了ModernBERT在该任务中作为快速、可扩展替代方案的有效性。

Conclusion: 多模态（文本-图像）对齐可显著缓解医学分割对密集标注的依赖。ModernBERT为扩散分割带来更优语义表达与效率，FastTextDiff展示了在临床文本辅助下的标签高效分割潜力。

Abstract: In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.

</details>


### [12] [Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs](https://arxiv.org/abs/2512.00086)
*Davide Nadalini,Manuele Rusci,Elia Cereda,Luca Benini,Francesco Conti,Daniele Palossi*

Main category: cs.CV

TL;DR: 在超低功耗物联网平台上，为应对单目深度估计在现场域偏移导致的精度崩塌，作者提出一种利用辅深度传感器生成伪标签并在MCU上微调的小型多模态设备端学习方案，借助内存驱动的稀疏更新，将微调内存降至1.2MB、能耗≈300mW，并在实测中用3k自标数据把RMSE从4.9m降至0.6m、耗时17.8分钟。


<details>
  <summary>Details</summary>
Motivation: IoT节点受算力/存储/能耗限制，MDE模型参数极小，遇到域偏移（环境变化）时精度大幅下滑；云端适配受带宽、隐私、时延与能耗限制。需要一种在节点上即可高效、低内存、低能耗完成域自适应/微调的方案。

Method: 在GAP9 MCU+超低功耗相机+8×8像素深度传感器硬件上：常态仅用相机驱动107k参数的μPyD-Net推理；进入新环境时暂时激活深度传感器，与相机同步采集并生成伪标签；在MCU本地执行反向传播微调。提出内存驱动的稀疏更新策略（只更新关键权重/梯度、减少中间激活保留）将训练内存降至1.2MB（较完整更新缩小2.2倍），同时维持精度。

Result: 在KITTI与NYUv2上稀疏更新带来仅约2%与1.5%的精度损失；实地部署中，以≈300mW功耗运行，使用3k自标样本，在17.8分钟内完成设备端微调，将RMSE从4.9m降至0.6m。

Conclusion: 多模态伪标签驱动的设备端微调可在超低功耗IoT节点上高效缓解MDE的域偏移，提出的内存稀疏更新在大幅降低训练内存与能耗的同时保持精度，首次展示了在真实部署中几分钟级完成有效ODL微调的可行性。

Abstract: Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.

</details>


### [13] [Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data](https://arxiv.org/abs/2512.00087)
*Ivo Bueno,Ruikun Hou,Babette Bühler,Tim Fütterer,James Drimalla,Jonathan Kyle Foster,Peter Youngs,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 该研究利用多模态AI自动分析课堂视频与文本，构建视频“教学活动”与转录“课堂话语”识别管线；在164小时视频与68份课程转录上微调模型并处理类不平衡与多标签问题，宏F1：视频0.577、文本0.460，优于基于提示的LLM，证明可扩展的自动课堂反馈可行。


<details>
  <summary>Details</summary>
Motivation: 人工观课依赖人工标注，成本高、不可扩展。需要自动化、多模态的方法来识别课堂活动与话语类型，为教师提供可操作的反馈。

Method: 构建并密集标注数据（164小时视频、68课次转录）；视频端评估零样本多模态LLM、微调视觉-语言模型、自监督视频Transformer，用24类活动标签；文本端以上下文化输入微调Transformer分类器，并与提示式LLM比较，涵盖19类话语标签；采用按标签阈值、上下文窗口、类不平衡损失等处理多标签与不平衡。

Result: 微调模型在两端均优于提示式LLM：视频宏F1=0.577，文本宏F1=0.460；显示针对性训练与不平衡处理显著提升性能。

Conclusion: 自动化课堂多模态分析在现实数据上可行，为可扩展的教师反馈系统奠定基础；微调优于仅依赖提示的通用LLM，方法学设计（阈值、上下文、不平衡损失）关键。

Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.

</details>


### [14] [SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features](https://arxiv.org/abs/2512.00088)
*Mohammad Zare*

Main category: cs.CV

TL;DR: SemImage将文本映射为二维“语义图像”，用HSV三通道分别编码主题、情感与强度，并在句子间插入动态边界行，使CNN可直接处理文档，兼顾精度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有文本分类模型虽强（如BERT、HAN），但可解释性不足、难以显式呈现主题/情感变化；同时希望利用成熟的2D CNN能力。作者尝试把文本转成图像，让语义特征在视觉上可见并可被CNN捕捉。

Method: 提出SemImage：每个词作为像素；行对应句子；句间插入依据语义差异动态计算的边界行。像素值是HSV向量：Hue（以H_cos/H_sin双通道处理环形特性）编码主题；Saturation编码情感；Value编码强度/确定性。通过多任务学习的ColorMapper把词向量映射到HSV，对Hue和Saturation施加辅助监督（主题与情感标签），同时优化主任务（分类）。用标准2D CNN（如ResNet）进行文档分类。

Result: 在多标签（含主题与情感）与单标签数据集上，SemImage达到与强基线（BERT、HAN）相当或更优的准确率；可解释性增强。消融表明HSV多通道和动态边界行均显著贡献性能。

Conclusion: 将文本转为可视化的语义图像，使主题与情感变化形成清晰图样，2D CNN得以有效利用并提升可解释性与性能；该方向为文本-视觉融合表示提供了新的可行方案。

Abstract: We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.

</details>


### [15] [TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts](https://arxiv.org/abs/2512.00089)
*Ioannis Prapas,Nikolaos Papadopoulos,Nikolaos-Ioannis Bountos,Dimitrios Michail,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: 提出TeleViT：结合本地细粒度火驱动、粗尺度全球场与遥相关指数的多尺度Transformer，用于8天至数月提前期的全球野火预测，在SeasFire数据上对比U-Net++、ViT与气候学基线取得更高AUPRC，尤其在最长约4个月提前期仍保持优势。注意力/归因表明以本地信息为主，全球与指数提供背景；在季节稳定的火区（非洲稀树草原）表现最佳，寒带与干旱区较弱。结论：显式编码大尺度地球系统背景可扩展次季节-季节尺度的可预报性。


<details>
  <summary>Details</summary>
Motivation: 长期（周-月）野火预测对燃料管理与资源调度关键，但短期模型依赖局地天气，难以捕捉跨区域遥相关与全球环流等大尺度影响，需要能融合多尺度与地球系统上下文的架构。

Method: 提出TeleViT：遥相关感知的Vision Transformer。通过不对称分词将三类输入（细粒度局地驱动、粗尺度全球场、遥相关指数）转为异质token，统一经Transformer编码器，再由解码器将局地token映射回空间预测块，保持空间结构。

Result: 在SeasFire（2001-2021，8天分辨率）上，TeleViT在所有提前期AUPRC优于U-Net++、ViT与气候学。0步提前AUPRC 0.630（优于ViT 0.617、U-Net 0.620）；16×8天（约4个月）仍达0.601-0.603（ViT 0.582、U-Net 0.578），全程超过气候学0.572。区域上季节一致性强的区域（非洲稀树草原）最好，寒带与干旱区较差。注意力/归因显示预测主要依赖局地token，全球场与指数提供粗尺度语境。

Conclusion: 通过显式融合局地与大尺度（全球场、遥相关指数）的多尺度Transformer，可在次季节到季节尺度延展野火可预报性，优于常见卷积与纯ViT基线；模型侧重局地信号，但大尺度背景提升稳健性与长期提前期表现。

Abstract: Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.
  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.

</details>


### [16] [Deep Filament Extraction for 3D Concrete Printing](https://arxiv.org/abs/2512.00091)
*Karam Mawas,Mehdi Maboudi,Pedro Achanccaray,Markus Gerke*

Main category: cs.CV

TL;DR: 提出一种与传感器无关的自动化质量控制流程，用于挤出式与喷射式（SC3DP）大型3D混凝土打印的线材（filament）几何质量检测，适用于新拌与硬化阶段，支持在线与后期检测。


<details>
  <summary>Details</summary>
Motivation: 3D混凝土打印中，层状“线材”几何直接决定构件质量与性能，但缺乏统一、自动化、可跨设备/材料与工艺的质量控制方法；行业需要兼容多种传感器与不同打印方法的QC流程。

Method: 设计一套自动化QC流程：以数字控制喷嘴逐层成型的线材为对象，采集传感数据（相机、结构光、地面激光扫描等均可）；对数据进行几何重建与评价，量化线材几何指标，并支持在材料新拌或硬化状态下执行，实现在线与离线QC。

Result: 实现了一个与传感器无关的工作流，可在挤出式与喷射式3DCP中稳定评估线材几何质量，实现在线与后处理场景的质量检查。

Conclusion: 所提流程为大型3D混凝土打印提供了通用、自动化的线材几何质量控制方法，增强了跨工艺与跨传感器的适用性，能提升打印质量与施工可靠性。

Abstract: The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.

</details>


### [17] [Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images](https://arxiv.org/abs/2512.00103)
*Ifeanyi Okala*

Main category: cs.CV

TL;DR: 比较VGG16、ViT-B/16与CoAtNet-Tiny在将日常腕式活动记录(转为30×48图像)上区分抑郁、精神分裂症与健康对照的能力。三折受试者划分评估。CoAtNet-Tiny平均最优且最稳定；VGG16稳步但精度偏低；ViT-B/16波动较大。CoAtNet在少数类(抑郁、精神分裂症)上也有更高Precision/Recall/F1。


<details>
  <summary>Details</summary>
Motivation: 探索将可穿戴设备产生的活动强度序列转化为图像后，使用不同视觉模型进行多类精神健康状态识别的可行性与稳健性，尤其关注少数类的分类表现与跨折泛化。

Method: 将Psykose与Depresjon数据集的腕式活动信号转为30×48的图像表示；采用VGG16、ViT-B/16、CoAtNet-Tiny三种架构；进行三折受试者级划分训练/验证；比较准确率、精确率、召回率、F1，并观察训练/验证曲线稳定性。

Result: 三者训练集拟合均好，但泛化差异显著：VGG16验证精度稳步提高但上限较低；ViT-B/16部分折结果很强但跨折波动大；CoAtNet-Tiny获得最高平均准确率、最稳定的曲线，并在抑郁与精神分裂症类上取得最佳Precision/Recall/F1。

Conclusion: 基于图像的混合架构(如CoAtNet-Tiny)对源自活动记录的心理健康分类更稳健、效果更佳；而经典CNN与纯Transformer在该任务上表现不够稳定或略逊，提示混合设计更适合此类基于体动图像的心理健康应用。

Abstract: This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.

</details>


### [18] [TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening](https://arxiv.org/abs/2512.00117)
*Ishwaryah Pandiarajan,Mohamed Mansoor Roomi Sindha,Uma Maheswari Pandyan,Sharafia N*

Main category: cs.CV

TL;DR: 提出TinyViT：仅用可见光图像，实现光伏板表面七类故障的分割分类与严重度评估，兼顾精度、可解释性与成本可扩展性。


<details>
  <summary>Details</summary>
Motivation: 多模态（如EL/IR）监测虽准确，但在大规模、分布式光伏电站中成本高、部署复杂；需要仅依赖廉价可见光成像，也能可靠检测并优先级排序表面故障。

Method: 构建紧凑流水线TinyViT：以Transformer为核心的分割网络获取区域；结合光谱-空间特征工程；通过集成回归输出故障严重度。输入为消费级彩色相机拼接图，支持七种细粒度表面故障分类并给出可执行的维修等级。

Result: 在真实公开数据集上验证了分类与回归两个子模块，性能（准确率与可解释性）与依赖专用传感器的方案相竞争。

Conclusion: 仅靠可见光图像即可实现经济、可扩展的光伏健康监测；为资源受限场景提供可落地方案，推动场景从实验室/专用传感器走向普适现场应用。

Abstract: Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.

</details>


### [19] [Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance](https://arxiv.org/abs/2512.00125)
*Ruo-Syuan Mei,Sixian Jia,Guangze Li,Soo Yeon Lee,Brian Musser,William Keller,Sreten Zakula,Jorge Arinez,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出一种混合式合成数据生成（SDG）框架，结合仿真渲染、域随机化与真实背景合成，实现工业零样本质检训练；仅用合成数据训练的两阶段检测-分类模型在真实数据上取得接近工业可用的高性能，并显著优于少样本真实数据基线。


<details>
  <summary>Details</summary>
Motivation: 制造业质检中的深度学习受限于标注数据昂贵、缺陷样本稀少与类别严重不平衡，导致模型泛化与鲁棒性不足，难以在真实产线大规模落地。需要一种能低成本、可扩展地生成充足、平衡且带完整标注的数据方案。

Method: 提出混合式SDG流水线：1) 基于仿真渲染变化零件几何、光照、表面属性；2) 域随机化增强多样性；3) 将合成零件与真实背景进行合成；在1小时内生成12,960张标注图。模型采用两阶段结构：YOLOv8n进行目标检测，MobileNetV3-small做质量分类，完全用合成数据训练，并在真实零件上评估。

Result: 在300个真实工业零件上，检测mAP@0.5=0.995；分类准确率96%，平衡准确率90.1%。对比少样本真实数据基线，所提方法在严重类别不平衡下平衡准确率达90–91%，而基线仅约50%。

Conclusion: 混合式SDG实现零标注训练，能高效、可扩展地缓解数据稀缺与类不平衡问题，使基于计算机视觉的工业质检在真实场景中具备鲁棒性与可落地性，并显著优于少样本真实数据方案。

Abstract: Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.

</details>


### [20] [Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation](https://arxiv.org/abs/2512.00129)
*Jayan Adhikari,Prativa Joshi,Susish Baral*

Main category: cs.CV

TL;DR: 提出将ResNet50为核心的OOD过滤与YOLO系列检测器联合，用于乳腺X线片乳腺癌检测，在过滤非域图像的同时保持高检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在乳腺癌检测中对分布外输入（如CT、MRI、普通X光或设备差异）极不鲁棒，易误检、误诊。为在异构临床环境中可靠部署，需要在进入检测前严格剔除非乳腺X线片图像，提升系统稳健性与可信度。

Method: 1) 以ResNet50建立特征空间并通过余弦相似度构建域内图库，对输入进行OOD过滤，确保仅乳腺X线片进入检测流程；2) 经过12种CNN骨干搜索后选定ResNet50为最佳；3) 在通过过滤后的图像上使用YOLOv8/v11/v12进行目标检测；4) 结合Grad-CAM提供可解释性可视化。

Result: OOD模块总体准确率99.77%，在OOD测试集上达到100%识别率；在检测方面达到mAP@0.5=0.947；联合框架在阻断分布外图像误报的同时维持或提升乳腺X线片上的检测准确度。

Conclusion: 将强健的OOD过滤与高性能检测器耦合，可显著提升乳腺癌AI系统在多源异构环境中的可靠性与可部署性，为临床应用提供基础框架。

Abstract: Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\% general accuracy with immaculate 100\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.

</details>


### [21] [Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition](https://arxiv.org/abs/2512.00130)
*Fadi Dornaika,Danyang Sun*

Main category: cs.CV

TL;DR: 提出LGCOAMix：一种基于超像素的上下文与目标部件感知的CutMix增强方法，避免矩形剪贴与双前向，提升分类与弱监督定位表现，并适用于CNN与Transformer。


<details>
  <summary>Details</summary>
Motivation: 现有CutMix类增强：1) 以图像级全局语义为主，忽视类别判别的局部上下文，形成性能瓶颈；2) 采用矩形/方形裁剪导致目标部件信息丢失；3) 为缓解图像与混合标签不一致，常需双前向或外部预训练网络，效率低。

Method: 提出LGCOAMix：以超像素为基本单元进行网格式混合与标签融合；引入“超像素注意”以选择判别性区域，并进行跨图像超像素对比，兼顾上下文与部件一致性；无需双前向或外部中心定位网络，实现高效、上下文感知与部件感知的混合。

Result: 在多个基准数据集上，LGCOAMix在图像分类上优于SOTA CutMix类方法；在CUB200-2011上弱监督目标定位也取得更好结果；同时对CNN与Transformer均有效。

Conclusion: 基于超像素注意的标签混合与局部对比学习能有效保留判别性局部与目标部件信息，提升泛化与定位能力，并以更高效率替代传统矩形CutMix流程。

Abstract: Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.

</details>


### [22] [Efficient Edge-Compatible CNN for Speckle-Based Material Recognition in Laser Cutting Systems](https://arxiv.org/abs/2512.00179)
*Mohamed Abdallah Salem,Nourhan Zein Diab*

Main category: cs.CV

TL;DR: 提出一种轻量级CNN用于激光散斑材料识别，在包含59类的SensiCut数据集上以仅34.1万参数达到95.05%准确率，推理295 FPS，可在树莓派/Jetson部署，并在材料族层面召回率>98%。


<details>
  <summary>Details</summary>
Motivation: 激光切割需要准确识别材料以避免劣质切割、设备损伤或有害烟雾。现有散斑感知分类要么依赖昂贵的大型骨干网络、要么覆盖材料种类有限，难以在边缘设备上实时部署。

Method: 针对散斑图样设计专用、参数高效的轻量级CNN：小模型（约34.1万可训练参数、约1.3MB），在完整SensiCut 59类数据集上训练与评估；并将材料重组为9类与5类族，评估族级性能与实际应用价值。

Result: 在59类上达成95.05%测试准确率，宏/加权F1均为0.951；推理速度约295张/秒；参数量比ResNet-50少70倍以上；在9类与5类材料族上召回率分别超过98%并接近100%。

Conclusion: 面向散斑的紧凑领域特化CNN能在材料分类上优于大型通用骨干，并可在树莓派/Jetson等边缘设备实时部署，提升材料感知与激光切割系统的可行性。

Abstract: Accurate material recognition is critical for safe and effective laser cutting, as misidentification can lead to poor cut quality, machine damage, or the release of hazardous fumes. Laser speckle sensing has recently emerged as a low-cost and non-destructive modality for material classification; however, prior work has either relied on computationally expensive backbone networks or addressed only limited subsets of materials. In this study, A lightweight convolutional neural network (CNN) tailored for speckle patterns is proposed, designed to minimize parameters while maintaining high discriminative power. Using the complete SensiCut dataset of 59 material classes spanning woods, acrylics, composites, textiles, metals, and paper-based products, the proposed model achieves 95.05% test accuracy, with macro and weighted F1-scores of 0.951. The network contains only 341k trainable parameters (~1.3 MB) -- over 70X fewer than ResNet-50 -- and achieves an inference speed of 295 images per second, enabling deployment on Raspberry Pi and Jetson-class devices. Furthermore, when materials are regrouped into nine and five practical families, recall exceeds 98% and approaches 100%, directly supporting power and speed preset selection in laser cutters. These results demonstrate that compact, domain-specific CNNs can outperform large backbones for speckle-based material classification, advancing the feasibility of material-aware, edge-deployable laser cutting systems.

</details>


### [23] [AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI](https://arxiv.org/abs/2512.00194)
*Zag ElSayed,Grace Westerkamp,Gavin Gammoh,Yanchen Liu,Peyton Siekierski,Craig Erickson,Ernest Pedapati*

Main category: cs.CV

TL;DR: ICVision 以多模态大模型直接“看图+讲理”来判别EEG ICA成分，超越基于手工特征的传统方法，在一致性、可解释性与临床信号保留上表现更佳，作为开源Autoclean核心模块推动可扩展、可复现、可解释的EEG流程。


<details>
  <summary>Details</summary>
Motivation: 现有ICA成分分类（如ICLabel）依赖手工特征和固定分类器，难以像专家那样综合可视化面板进行直观判断，且在可解释性与临床相关信号保留方面不足。需要一种能“看图—推理—解释”的系统以接近专家水平并提升可复现性与可扩展性。

Method: 提出ICVision：用多模态大语言模型（GPT‑4V）直接读取ICA仪表盘的拓扑、时序、功率谱、ERP等可视化，输出六类标签（脑/眼/心/肌/通道噪声/其他噪声）、置信度及自然语言解释；作为EEG Autoclean模块，与专家式视觉认知一致。

Result: 在124个EEG数据集的3,168个组件上，ICVision与专家一致性k=0.677，优于MNE ICLabel；在模糊情形下更好地保留临床相关脑信号；>97%的输出被专家评为可解释且可操作。

Conclusion: ICVision展示了AI代理在神经生理中“看—判—说”的能力，提升分类性能与可解释性，推动全球可扩展、可解释、可复现的EEG工作流，标志着面向脑科学及更广领域的专家级视觉决策AI的出现。

Abstract: We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.

</details>


### [24] [Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting](https://arxiv.org/abs/2512.00198)
*Shantanu Ghosh,Vedant Parthesh Joshi,Rayan Syed,Aya Kassem,Abhishek Varshney,Payel Basak,Weicheng Dai,Judy Wawira Gichoya,Hari M. Trivedi,Imon Banerjee,Shyam Visweswaran,Clare B. Poynton,Kayhan Batmanghelich*

Main category: cs.CV

TL;DR: 提出Mammo-FM：首个专用于乳腺X线摄影的领域型基础模型，基于140,677名患者（821,326张片）预训练，覆盖多机构多分布；在诊断、定位、报告生成与风险预测等任务上，在内外部分布数据集上均优于通用大模型且参数更少，具备图文对齐与可解释性。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌致死率高，临床需要在同一框架下完成从诊断、病灶定位、结构化报告到风险预测的多任务，但通用基础模型在分辨率、数据分布、可解释性与临床评测一致性上存在不足，因此需要一个面向乳腺影像的领域专属基础模型并进行严格、领域对齐的评估。

Method: 构建并预训练Mammo-FM：在四家美国机构的最大规模、多样化乳腺X线数据集上进行图像-文本对齐的多任务预训练；模型以原生分辨率处理乳腺片，参数量约为通用FM的三分之一；在统一框架下支持癌症诊断、病理定位、结构化报告生成与风险预后；强调可解释性与审计性。

Result: 在诊断、预后和报告生成任务上，Mammo-FM在多项公共与私有基准（含分布内与分布外）上稳定优于SOTA通用基础模型，尽管参数更少且处理高分辨率；展示了良好的视觉与文本可解释性。

Conclusion: 领域特定的基础模型在临床影像中更高效、更有效；围绕临床全任务谱系设计并进行严格、领域一致的评估能带来实质收益，提示未来应重视面向特定临床场景的基础模型与可解释性对齐。

Abstract: Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.

</details>


### [25] [ReactionMamba: Generating Short &Long Human Reaction Sequences](https://arxiv.org/abs/2512.00208)
*Hajra Anwar Beg,Baptiste Chopin,Hao Tang,Mohamed Daoudi*

Main category: cs.CV

TL;DR: 提出ReactionMamba：将运动VAE与Mamba状态空间模型结合，实现长时长3D人体反应动作生成，兼顾真实感、多样性与速度，并在多数据集上优于或匹敌现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体反应动作生成方法在长序列一致性、复杂动作（如舞蹈、武术）建模与推理效率上存在不足，需要一种既能保持时间一致性又能高效推理的生成框架。

Method: 采用两阶段/双模块设计：1）运动VAE对动作进行紧凑表征；2）基于Mamba的状态空间模型从潜空间解码，建模长程时间依赖，生成时序一致的反应动作。系统可生成短简单与长复杂序列。

Result: 在NTU120-AS、Lindy Hop、InterX三个数据集上，相比InterFormer、ReMoS、Ready-to-React达到有竞争力的真实感与多样性，并在长序列生成上更优，同时显著提升推理速度。

Conclusion: VAE编码+Mamba解码的ReactionMamba在长序列3D反应动作生成中实现了质量-效率的平衡，对复杂动作与长时依赖建模有效，优于或不逊于现有方法并更快。

Abstract: We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.

</details>


### [26] [DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation](https://arxiv.org/abs/2512.00226)
*Zirui Wang,Tao Zhang*

Main category: cs.CV

TL;DR: DenseScan提出一个融合几何与语义的3D场景理解数据集与自动标注流水线，利用多视角2D图像与多模态大模型生成稠密物体级描述与情景化问答注释，显著提升3D环境中的对象理解与问答性能，并开放数据与工具链。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景数据集多聚焦几何与实例层信息，缺少可支撑细粒度视觉-语言任务的丰富语义注释，限制了导航、交互问答等下游能力的发展。

Method: 构建自动化标注管线：基于多视角2D图像与MLLM进行稠密物体级描述生成；进一步面向场景情景提出高层次问题与答案，融合对象属性、空间关系与上下文；将语义注释与几何信息耦合，形成多层次描述数据集DenseScan。

Result: 在3D环境评测中，相比传统标注流程，该方法在对象级理解与问答性能上取得显著提升；数据集与管线可复用并已开放。

Conclusion: DenseScan通过自动化、多层语义标注丰富了3D场景理解生态，拓展了导航与交互问答等任务边界，有望推动机器人与AR等现实应用的发展。

Abstract: 3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.

</details>


### [27] [Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views](https://arxiv.org/abs/2512.00255)
*Kunwar Maheep Singh,Jianchun Chen,Vladislav Golyanik,Stephan J. Garbin,Thabo Beeler,Rishabh Dabral,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: RHC提出一种基于Transformer的单次前向推理的人体自由视角重光照方法，从稀疏视角RGB视频重建并重光照动态全身人物，避免OLAT基分解与多次渲染，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人体重光照多依赖OLAT捕获与线性组合，成本高、推理慢且对动态全身与稀疏视角不友好；同时需要能兼顾运动跟踪、复杂光照与高动态内容的数据与模型。

Method: 1) 新的多视角光舞台采集：在随机环境光照帧与均匀光照跟踪帧间交替，既覆盖多样照明又确保精确运动跟踪。2) 物理启发特征：从粗人体网格与输入视图推导几何、反照率、阴影与虚拟相机视图等特征。3) RelightNet（Transformer）：将上述特征与目标光照条件进行跨注意力融合，单次前向回归重光照外观。输出形式为与粗网格对齐的3D高斯splat贴素（texel-aligned 3D Gaussian splats），实现高效可渲染表示。

Result: 在自由视角与新光照条件下，对动态全身人物实现更高的视觉保真度与光照复现准确性，优于多种SOTA基线；推理无需OLAT基展开，单次网络前向即可得结果。

Conclusion: RHC将物理启发特征与Transformer跨注意力相结合，学习在一次前向中近似求解渲染方程，实现基于稀疏视角视频的可重光照全身人像自由视角渲染，具备更高质量与效率。

Abstract: We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/

</details>


### [28] [UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations](https://arxiv.org/abs/2512.00261)
*Yuzhen Hu,Saurabh Prasad*

Main category: cs.CV

TL;DR: 提出UniDiff：以少量可调参数将单个ImageNet预训练扩散模型无监督适配到多种遥感模态（HSI、SAR），缓解标注稀缺并实现多模态融合。


<details>
  <summary>Details</summary>
Motivation: 遥感多模态任务（尤其HSI、SAR）长期受限于标注稀缺；即便是最新的监督方法（如MSFMamba）也依赖大量标注。虽然ImageNet预训练模型具备强表征，但直接迁移到异构模态在缺少标注时效果差且易遗忘预训练知识。需要一种无需大量标注、可跨模态适配且避免灾难性遗忘的方案。

Method: 提出UniDiff框架：以单个ImageNet预训练扩散模型为基础，进行参数高效适配（约5%参数可训练）；通过FiLM式的时间步-模态联合条件化实现对不同传感模态的适配；引入pseudo-RGB锚定策略，维持与预训练表示的一致性以防遗忘；使用仅目标域数据进行无监督适配，并将适配后的模型作为通用特征提取器用于多模态融合。

Result: 在两个公认的多模态基准上，UniDiff在无监督设置下有效缓解标注不足带来的性能瓶颈，并实现对HSI、SAR等模态的有效特征抽取与融合，性能优于依赖大规模标注的传统方法趋势。

Conclusion: 通过将单一扩散模型以少量参数适配至多模态，UniDiff在缺乏标注的遥感场景中实现稳健的跨模态表征与融合，显示无监督扩散模型适配是突破遥感标注瓶颈的可行路径。

Abstract: Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.

</details>


### [29] [HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction](https://arxiv.org/abs/2512.00264)
*Zhengda Ma,Abhirup Banerjee*

Main category: cs.CV

TL;DR: 提出HeartFormer：首个基于点云的几何深度学习框架，用于从cine MRI重建3D四腔心脏，并配套大规模多类别数据集HeartCompv1，跨域实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统cine MRI多为2D切片，难以全面捕捉心脏三维形态与功能，限制对健康与病理的理解。现有点云补全多为单类别，缺乏适用于心脏多亚结构（四腔）的统一方法与公开数据集。

Method: 提出HeartFormer点云补全网络，扩展到多类别/多结构：1) SA-DSTNet（语义感知的双结构Transformer）同时学习全局几何与子结构几何，生成粗点云；2) SA-GFRTNet（语义感知几何特征精炼Transformer）在语义-几何先验引导下级联细化，提升细节与一致性。并构建HeartCompv1数据集（1.7万高分辨3D多类别心脏网格与点云）。

Result: 在HeartCompv1与UK Biobank进行跨域评测，HeartFormer在稳健性、精度与泛化性上持续超越当前SOTA方法。

Conclusion: 基于点云的语义-几何协同Transformer可实现高保真、几何一致的四腔心脏三维重建；HeartCompv1为该方向提供了首个大规模公共基准，代码与数据将开源。

Abstract: We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.

</details>


### [30] [USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing](https://arxiv.org/abs/2512.00269)
*Jun Wang,Peirong Liu*

Main category: cs.CV

TL;DR: 提出USB，一个端到端统一框架，可在同一模型中进行病理与健康脑MRI的双向生成与编辑，并通过成对扩散与一致性引导保持解剖与病灶对应；在6个公开数据集上产生多样且真实的结果，构建了首个统一基准并促进可扩展数据集与稳健神经影像分析。


<details>
  <summary>Details</summary>
Motivation: 临床上难以获取同一受试者的病理-健康成对数据（需治疗前后影像与长期随访），导致现有方法多为领域特定、重视觉质量而缺少跨域统一建模，难以同时处理病理与健康图像生成/编辑并保持解剖一致与病灶对应。

Method: 提出USB（Unified Synthetic Brain）：1）成对扩散（paired diffusion）机制，直接建模病灶与脑解剖的联合分布，实现病理与健康图像的双向生成；2）一致性引导（consistency guidance）算法，在双向编辑时约束并保留解剖结构一致性与病灶对应关系；3）端到端框架，统一生成、编辑与配对建模，并在多数据集上训练与评估。

Result: 在包含健康对照、卒中、阿尔茨海默症等6个公开MRI数据集上，USB生成的图像多样、真实，能在病理↔健康之间进行结构一致的编辑，优于或补充现有仅关注单域或视觉质量的方法；同时建立了首个统一的脑影像生成与编辑基准。

Conclusion: USB把病灶与解剖的联合建模纳入一个端到端统一框架，实现病理-健康的双向生成与编辑，并保持结构与病灶对应一致；该方法有望用于可扩展数据集合成与更稳健的神经影像分析，推动诊断、预测与治疗规划研究。

Abstract: Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.

</details>


### [31] [HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention](https://arxiv.org/abs/2512.00275)
*Yi Liu,Yi Wan,Xinyi Liu,Qiong Wu,Panwang Xia,Xuejun Huang,Yongjun Zhang*

Main category: cs.CV

TL;DR: 提出HIMOSA轻量级遥感图像超分框架，通过内容感知稀疏注意力与分层窗口扩展，兼顾高重建质量与实时高效，实验达SOTA且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 遥感场景（如灾害监测）需要实时与轻量化，但现有超分方法在性能与效率间权衡明显，难以同时满足推理速度与重建质量。

Method: 1) 利用遥感图像的冗余性；2) 设计内容感知稀疏注意力机制，根据内容动态选择注意连接以降低复杂度；3) 引入分层窗口扩展以捕获多尺度重复模式，并通过调节注意力稀疏度进一步降耗；4) 形成轻量化SR框架HIMOSA，实现快速推理。

Result: 在多个人遥感数据集上进行大量实验，较现有方法在保持或降低计算量的同时取得SOTA重建性能与更高的推理效率。

Conclusion: HIMOSA在不牺牲重建质量的前提下显著提升计算效率，适用于对实时性与资源受限敏感的遥感应用场景。

Abstract: In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.

</details>


### [32] [Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth](https://arxiv.org/abs/2512.00281)
*Sylvain Bodard,Pierre Baudot,Benjamin Renoust,Charles Voyton,Gwendoline De Bie,Ezequiel Geremia,Van-Khoa Le,Danny Francis,Pierre-Henri Siot,Yousra Haddou,Vincent Bobin,Jean-Christophe Brisset,Carey C. Thomson,Valerie Bourdes,Benoit Huet*

Main category: cs.CV

TL;DR: 提出一套在低剂量CT上对结节级别同时“检测+恶性诊断”的AI系统，基于浅层深度学习与手工特征模型的集成，在大规模数据上训练，AUC达0.98/0.945，低假阳性下保持极高敏感度，全面超越放射科医师与现有方法，尤其在早期与生长缓慢结节上提前诊断。


<details>
  <summary>Details</summary>
Motivation: 现有筛查依赖结节大小和生长（如体积倍增时间），对早期与缓慢生长肺癌诊断滞后，且AI模型受数据规模与可解释性限制，难以临床落地。

Method: 构建结节级别的端到端流程：在低剂量CT上先定位结节，再直接判别恶性；采用“浅层深度学习+基于手工与影像特征”的专科化模型集成；在25,709次扫描、69,449个标注结节上训练与验证，并与放射科医师、Lung-RADS及主流AI（Sybil、Brock、Google、Kaggle）对比评估。

Result: 内部AUC=0.98，独立队列AUC=0.945；在0.5次/扫的假阳性率下实现99.3%敏感度；在各种结节大小、分期（尤其1期）和基于生长的指标上均优于放射科医师，显著改善最不可靠的体积倍增时间指标；对不确定与慢生长结节的诊断可较医师提前至多一年。

Conclusion: 结节级别的“检测+恶性诊断”集成AI在大规模验证中显著超越人类与现有基准，兼顾高敏感度与低假阳性，能提前识别早期与慢生长肺癌，有望降低筛查延误并提高临床可采用性。

Abstract: Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.

</details>


### [33] [Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR](https://arxiv.org/abs/2512.00294)
*Lixing Guo,Tobias Höllerer*

Main category: cs.CV

TL;DR: 提出一个模块化AR智能体，将多模态大语言模型与具坐标感知的视觉工具结合，实现开放词表语言驱动的空间检索与关系推理，并发布评测基准GroundedAR-Bench。


<details>
  <summary>Details</summary>
Motivation: 传统AR依赖固定类别检测或标靶，难以理解开放词表自然语言与复杂多对象关系，缺乏将语言理解与真实三维空间精准对齐的能力。

Method: 设计自适应任务代理协调MLLM与坐标感知视觉模型：构建包含9类关系（空间、结构-语义、因果-功能）的动态AR场景图；语言条件的空间检索与ROI高亮；根据查询难度动态调用选择、测量、比较、执行等工具；返回米级精度3D锚点；模块化架构支持即插即用VL模型，无需再训练。

Result: 系统能够从简单目标识别到多对象关系推理，进行上下文空间检索，指导用户注意力并支持人机在环细化；对复杂查询能调用操作型工具以物理操作来落地语言理解；在多样环境下提供语言驱动定位与关系落地的统一评测基准GroundedAR-Bench。

Conclusion: AR智能体可作为桥梁，为MLLM注入现实世界空间智能，实现交互式场景理解与精确三维定位；模块化、可扩展且无需再训练的设计提升了开放词表AR应用的实用性，并为评测提供了新基准。

Abstract: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.

</details>


### [34] [TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion](https://arxiv.org/abs/2512.00300)
*Rui Qian,Haozhi Cao,Tianchen Deng,Tianxin Hu,Weixiang Guo,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: 提出TGSFormer：一种面向具身3D语义场景补全的可扩展时序高斯泼溅框架，通过持久高斯记忆与置信度感知的时序/体素融合，实现更少原语、更高精度与更好可扩展性，适用于本地与具身SSC基准并保持长期场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的SSC方法需在固定边界内随机初始化大量原语，冗余高且难以扩展到无界场景；深度引导方案虽减少冗余，但仍是局部方法，随着规模增长会带来时延与内存开销。需要一种在连续自我视角下既高效又可扩展、能长期维持场景完整性的SSC方法。

Method: 提出TGSFormer：1) 持久高斯记忆用于时序预测，无需依赖图像连贯性或帧缓存；2) 双时序编码器（Dual Temporal Encoder）通过置信度感知的交叉注意力联合处理当前与历史高斯特征，实现时序融合；3) 置信度感知体素融合（Confidence-aware Voxel Fusion）将重叠原语融合为体素对齐表示，调节密度并保持模型紧凑；整体以时序高斯泼溅为表示与学习框架。

Result: 在本地与具身SSC基准上取得SOTA，较少原语数量下获得更高精度与更强可扩展性，并在长时程场景中保持一致性；实验表明延迟与内存占用优于现有方法。

Conclusion: TGSFormer通过持久记忆与置信度驱动的时序/体素融合，解决了高斯表示在SSC中的冗余与可扩展性难题，在准确率、效率与长期一致性上优于现有方法；代码将于论文接收后开源。

Abstract: Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.

</details>


### [35] [Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation](https://arxiv.org/abs/2512.00308)
*Xiao Cui,Yulei Qin,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 将数据集蒸馏表述为最优传输(OT)距离最小化，以在全局和实例层面对齐分布；通过OT引导的扩散采样、标签-图像对齐的软重标注、以及OT基础的logit匹配三部分，保留局部模态与类内变化；在ImageNet-1K等大规模数据与多架构上高效超越SOTA，在IPC=10下各架构至少+4%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大规模数据集蒸馏多匹配均值/方差等全局统计，忽视样本级特征与类内差异，导致泛化不足。需要一种能在保留分布几何与细粒度结构的同时对齐真实与蒸馏数据的方法。

Method: 将蒸馏目标重构为最优传输距离最小化，构建三组件：1) OT引导的扩散采样：在扩散模型潜空间中对齐真实与蒸馏图像的潜在分布，实现几何保真采样；2) 标签-图像对齐的软重标注：依据蒸馏图像分布复杂度自适应调整标签分布，使标签与图像细粒度结构一致；3) 基于OT的logit匹配：以OT对齐学生模型输出与软标签分布，保留类内模式与局部结构。

Result: 在多种架构与大规模数据集上均优于现有方法，效率较高；在ImageNet-1K的IPC=10设定下，每种架构至少提升4% Top-1准确率。

Conclusion: 以OT为核心的几何保真对齐能同时兼顾全局与实例级结构，提升蒸馏数据的代表性与模型泛化能力；结合扩散采样、软重标注与logit匹配，形成端到端高效方案，稳定超越SOTA。

Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.

</details>


### [36] [ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays](https://arxiv.org/abs/2512.00310)
*Qinyi Cao,Jianan Fan,Weidong Cai*

Main category: cs.CV

TL;DR: 提出ART-ASyn：一种面向胸片的解剖感知、真实纹理驱动的异常合成框架，利用PBTSeg分割肺部并在正常片上注入与肺实变相关的逼真纹理异常，生成成对的图像与精确像素掩码，从而进行显式监督；不仅适用于无监督异常检测，还支持零样本异常分割并在未见数据集上泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于合成的异常检测能提供可控异常与已知掩码，但常与真实病理脱节、忽视解剖结构且多停留在一类分类；需要一种既解剖一致又视觉逼真的异常生成方法，并将其用于更具挑战的零样本分割与跨域泛化。

Method: 构建ART-ASyn：1) 提出PBTSeg逐级二值阈值分割以获得稳健的胸片肺野掩码；2) 在肺野内进行纹理级增强，注入与肺不透明影相关的真实感纹理异常，保证解剖一致性；3) 为每张正常图生成对应的合成异常与精确像素级掩码，供显式分割监督；4) 训练产生的模型用于无监督检测与零样本异常分割，并在未标注目标域评估。

Result: 相较基线，合成的病变更接近真实病理且具解剖一致性；利用成对数据训练的模型在无像素标注场景下实现更好的异常检测，并在未见数据集上实现零样本异常分割的可观性能，显示良好跨域泛化。

Conclusion: 通过解剖感知的真实纹理异常合成与PBTSeg肺分割，ART-ASyn将合成数据与显式监督结合，突破仅一类分类的限制，实现在胸片上的无监督检测与零样本分割并具备跨域泛化潜力。

Abstract: Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.

</details>


### [37] [Odometry Without Correspondence from Inertially Constrained Ruled Surfaces](https://arxiv.org/abs/2512.00327)
*Chenqi Zhu,Levi Burner,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出一种基于“直线在时空中形成的可展（有支线）曲面”来进行视觉里程计与3D重建的新方法，结合IMU约束，避免逐点光流对应，提升效率与稳健性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉里程计依赖特征点匹配与光流，计算代价高且易受纹理/光照影响，导致姿态与尺度估计不稳。虽有基于线特征或多传感器融合（事件相机、IMU）的尝试，但仍绕不开对应关系的脆弱性。若相机观测到的空间直线在相机运动下，于“图像-时间”体素中生成一条可展的有支线曲面；直接分析该曲面几何可获取运动信息，潜在地降低求解难度与噪声敏感性。

Method: 利用事件相机对边缘/线的敏感性，构造图像-时间体中的“直线扫掠曲面”（ruled surface）；通过点到线的微分更新估计该曲面的形状与参数，从而联合恢复相机运动与3D结构。再引入机载IMU的惯性测量对曲面/运动参数施加约束，显著收缩解空间与歧义；整体框架避免逐点对应，转而进行基于曲面几何的估计。

Result: 方法能够从直线扫掠形成的时空曲面中重建场景并估计视觉里程计，在无需密集点对应的情况下实现稳健、计算更高效的估计；IMU约束进一步提升精度与可观性、减少漂移与不适定性。（摘要层面未给出具体数值对比）

Conclusion: 将“时空中由线生成的有支线曲面”作为核心表示，并结合IMU约束，可以在弱化对应需求的同时完成3D重建与里程计估计，具有更高的效率与稳健性，适合事件相机等对边缘敏感的传感配置。

Abstract: Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.

</details>


### [38] [MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection](https://arxiv.org/abs/2512.00336)
*Mengxue Hu,Yunfeng Diao,Changtao Miao,Jianshu Li,Zhe Li,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: MVAD提出首个面向“视频+音频”双模态AI生成内容检测的大规模数据集，覆盖多伪造模式、跨风格与多类别内容，质量高、分布广，为多模态伪造检测研究提供基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多仅关注视觉或局限于人脸深伪音视频，无法反映更广泛的多模态合成内容生态，制约了可靠检测方法的研发与评测。

Method: 构建MVAD数据集：1）依据三种真实世界的视频-音频伪造模式生成样本；2）采用多种SOTA生成模型以保证高感知质量；3）在视觉风格（真实/二次元）、内容类别（人/动物/物体/场景）与四类视频-音频数据类型上实现全面多样性。提供公开访问。

Result: 得到一个规模全面、质量高、覆盖多伪造类型与多内容分布的多模态视频-音频数据集，可直接用于训练与评测多模态伪造检测器。

Conclusion: MVAD填补了通用多模态合成内容检测数据集的空白，为鲁棒、可信的多模态伪造检测研究与系统落地奠定了数据基础。

Abstract: The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.

</details>


### [39] [Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models](https://arxiv.org/abs/2512.00343)
*Zhongqi Wang,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: AMDET 提出一种无需先验的模型级后门检测方法，利用后门文本编码器的“特征同化”现象，通过梯度反演恢复触发相关隐式特征，并结合损失景观区分自然后门与注入后门，实现高效、鲁棒的检测。


<details>
  <summary>Details</summary>
Motivation: 现实中常用的 CLIP 等视觉-语言预训练模型易被第三方微调时注入后门，现有检测依赖训练数据、触发器或下游分类器等先验，实际难以获得，迫切需要一种不依赖先验、对黑盒微调模型有效的检测方法。

Method: 1) 发现并刻画后门文本编码器中的“特征同化”属性：含触发的样本在所有词元表示间相似度显著升高，源于注意力权重过度集中到触发词。2) 设计 AMDET：对词嵌入进行梯度式反演，恢复能够激活后门行为的隐式特征，以此扫描模型。3) 识别并分离 OpenAI 官方 CLIP 中存在的“自然后门特征”（非人为注入但具后门样态），通过分析其损失景观将其与真实注入后门区分并过滤。

Result: 在两种攻击范式、三类 VLP 结构、共3600个后门与良性微调模型上，AMDET 的后门检测 F1 达到89.90%；单次完整检测在 RTX 4090 上约5分钟；对自适应攻击保持较强鲁棒性。

Conclusion: AMDET 无需任何数据或触发先验即可进行模型级后门检测，依托特征同化与梯度反演机制实现高效、鲁棒的检测，并能区分自然与注入后门，为实际部署的 VLP 安全审计提供可行方案。

Abstract: Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET

</details>


### [40] [mmPred: Radar-based Human Motion Prediction in the Dark](https://arxiv.org/abs/2512.00345)
*Junqiao Fan,Haocong Rao,Jiarui Zhang,Jianfei Yang,Lihua Xie*

Main category: cs.CV

TL;DR: 提出mmPred：首个以毫米波雷达为感知模态的人体运动预测扩散框架，结合时域精修与频域主运动引导，并以全局骨架关系Transformer建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RGB-D方法受光照影响且存在隐私问题，限制在消防、医疗等场景应用；毫米波雷达具备鲁棒与隐私优势，但其信号易受镜面反射、多径导致的噪声与时序不连贯（如关节漏检）。需要一个能针对雷达噪声特性的专门HMP方法。

Method: 提出mmPred扩散式生成框架：1) 双域历史运动表征作为条件引导——时域TPR分支学习细粒度姿态并做精修；频域FDM分支提取主导运动趋势，抑制逐帧不一致；2) 设计GST（Global Skeleton-relational Transformer）作为扩散骨干，建模全局关节协同，使受损关节可从其他关节自适应聚合信息。

Result: 在两个毫米波数据集上达到SOTA：对mmBody提升8.6%，对mm-Fi提升22%。实验显示在噪声和漏检情况下仍能稳定预测。

Conclusion: 毫米波雷达可行且优于RGB-D在隐私与鲁棒性方面；mmPred通过时/频双域引导与全局骨架Transformer有效缓解雷达特有噪声与不一致问题，实现最先进的人体运动预测性能。

Abstract: Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.

</details>


### [41] [SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction](https://arxiv.org/abs/2512.00355)
*Junqiao Fan,Pengfei Liu,Haocong Rao*

Main category: cs.CV

TL;DR: 提出SMamDiff：单阶段扩散的人体运动预测模型，结合残差-DCT编码与“火柴人”空间Mamba模块，在Human3.6M与HumanEva上以更低时延和内存取得单阶段概率方法SOTA的时空一致性与准确-多样性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有HMP要么单一确定性预测忽视不确定性，要么概率模型缺乏运动学可行性；扩散法虽提升准确-多样性，但多为多阶段管线，不利于边缘部署。需要一个单阶段、具时空一致性、且高效的概率HMP方法。

Method: 构建基于Spatial Mamba的单阶段扩散模型SMamDiff，含两项关键设计：1) 残差-DCT运动编码：先减去最后观测姿态再做时间域DCT，削弱f=0直流成分主导，强调高频信息，使模型学习“如何动”。2) Stickman绘制式空间Mamba：按关节顺序逐关节处理，使后续关节条件化于先前关节，建立长程跨关节依赖，增强时空一致性。

Result: 在Human3.6M与HumanEva基准上，较单阶段概率HMP方法取得SOTA；相较多阶段扩散基线，达成更低推理时延与内存占用，同时保持更好的时空一致性与样本多样性/准确性权衡。

Conclusion: SMamDiff在不依赖多阶段管线的情况下，实现高效且具运动学可行性的概率人体运动预测，关键在于残差-DCT编码与空间Mamba的有序关节建模，适合边缘部署并可作为统一单阶段框架推广。

Abstract: With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.

</details>


### [42] [MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters](https://arxiv.org/abs/2512.00363)
*Jianhong Han,Yupei Wang,Yuan Zhang,Liang Chen*

Main category: cs.CV

TL;DR: 提出MM-DETR：以Mamba为核心、具线性复杂度的多模态目标检测框架，在保证轻量的同时提升鲁棒与精度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态遥感检测要么用共享骨干导致模态特异性不足，要么双流结构参数翻倍；注意力/可变形卷积融合同样算力高、难兼顾轻量与性能，限制部署。

Method: 1) Mamba式双粒度融合编码器：将全局交互重构为通道级动态门控，利用1D selective scan实现线性复杂度跨模态建模；2) 将融合视作“模态补全”，引入区域感知的2D selective scanning completion分支，沿双向金字塔路径做细粒度融合、低开销恢复模态线索；3) 在共享骨干中插入轻量的频率感知模态适配器（空间-频率co-expert结构），并用像素级router动态分配专家贡献，实现高效空间-频率融合与模态特异建模，减少冗余参数。

Result: 在四个多模态基准数据集上做了广泛实验，验证方法的有效性与泛化性；在同等或更低参数/计算下取得更优检测性能（摘要未给具体数值）。

Conclusion: MM-DETR通过线性复杂度的Mamba融合、模态补全分支与频率感知适配器，实现轻量而强大的多模态检测，在精度、鲁棒性与效率间取得更佳权衡并具备良好泛化。

Abstract: Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.

</details>


### [43] [Towards aligned body representations in vision models](https://arxiv.org/abs/2512.00365)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 研究比较人类与语义分割模型是否形成类似“粗粒度体积”表征，发现小模型更接近人类的粗身体表征，大模型倾向过细编码。


<details>
  <summary>Details</summary>
Motivation: 心理物理学表明人类进行物理推理时依赖对象的粗体积近似，但这种内部表征的结构尚不清楚。作者想检验计算视觉模型（尤其分割模型）是否会自发形成与人类类似的粗身体表征，从而为理解大脑物理推理的表征结构提供线索。

Method: 将一项涉及50名人类被试的心理物理实验改编为语义分割任务；测试七种不同规模的分割网络，比较其在与人类任务等价条件下形成的对象表征的“粗/细”程度。

Result: 较小的分割模型自然形成更接近人类的粗粒度体积表征；较大的模型则偏向过度精细、边界细节更丰富的编码。

Conclusion: 在受限计算资源下，粗表征可自发涌现；机器学习模型的表征差异为探索大脑物理推理的表征结构提供了可扩展路径。

Abstract: Human physical reasoning relies on internal "body" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.

</details>


### [44] [THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering](https://arxiv.org/abs/2512.00368)
*Jian Zhu*

Main category: cs.CV

TL;DR: 提出THCRL：用去噪的层次融合+基于同簇近邻的对比学习，修复多视角聚类中的不可信融合问题，刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 多视角聚类常需跨视角融合，但存在两大痛点：单视角含噪导致融合失真；对比学习只用“同一实例跨视角”的相似性，忽略同簇结构与近邻关系，造成融合方向偏离。

Method: 构建Trusted Hierarchical Contrastive Representation Learning (THCRL)，含两模块：1) DSHF：基于UNet结构并集成多重去噪机制，实现对多视角表征的深度对称层次融合，得到更可信的全局融合表示；2) AKCL：以平均K近邻为正样本锚，拉近同簇样本间的表示，使融合表示与各视角表示对齐，超越仅用“同一实例跨视角”匹配的传统CL。

Result: 在多项深度多视角聚类任务上进行广泛实验，THCRL达到或超过现有最佳方法（SOTA）。

Conclusion: 通过去噪的层次融合与同簇近邻对比对齐，有效缓解不可信融合，提升多视角聚类的表示质量与聚类性能，具有通用性与鲁棒性。

Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.

</details>


### [45] [POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models](https://arxiv.org/abs/2512.00369)
*Wenshuo Chen,Haosen Li,Shaofeng Liang,Lei Wang,Haozhe Jia,Kaishen Yuan,Jieming Wu,Bowen Tian,Yutao Yue*

Main category: cs.CV

TL;DR: 提出POLARIS框架，针对扩散模型反演中的近似噪声误差，通过将指导系数ω设为逐步自适应并给出最小化误差的闭式公式，显著减小误差累积，提升反演与下游任务精度，几乎零额外开销、代码改动极小。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的反演-去噪范式在图像编辑/恢复中表现强，但反演重建会退化。作者发现关键未被充分重视的原因是“近似噪声误差”：在t步用t-1步的噪声预测近似，导致误差在反演过程中累积。现有方法多通过调参或优化隐变量去补偿漂移，治疗结果不治本。

Method: 将反演视作“误差起源”问题而非“误差补偿”问题：把指导尺度ω设为每一步的可调变量，推导出一个有理论依据的逐步最小化反演误差的公式；以投影-正交最小二乘（Projection-Orthogonal Least Squares）为核心，在每一步根据当前状态自适应更新ω，从而抑制由噪声近似带来的累积偏差；实现层面仅需“一行代码”修改，开销可忽略。

Result: 显著降低近似噪声误差，提高反演潜变量质量；在多种编辑与恢复下游任务上稳定提升重建准确性；在几乎不增加计算成本的前提下带来一致收益。

Conclusion: POLARIS通过逐步自适应的ω与正交最小二乘投影，将反演中的噪声近似误差从源头抑制，避免误差累积；无需复杂的潜变量优化即可稳健提升反演与下游表现，具备简洁、高效、可集成的优势。

Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.

</details>


### [46] [Pore-scale Image Patch Dataset and A Comparative Evaluation of Pore-scale Facial Features](https://arxiv.org/abs/2512.00381)
*Dong Li,HuaLiang Lin,JiaYu Li*

Main category: cs.CV

TL;DR: 提出PorePatch毛孔级人脸图像补丁数据集与评测基准，通过数据-模型共演化框架生成高质量数据，训练SOTA描述子在补丁匹配上显著优于传统PSIFT，但在3D重建任务中的优势不明显，显示弱纹理人脸区域仍是难点。


<details>
  <summary>Details</summary>
Motivation: 人脸皮肤区域弱纹理使局部特征匹配困难，影响人脸运动分析与三维重建。深度描述子在多领域领先，但缺乏毛孔尺度补丁数据集限制其在人脸领域的发展，因此需要高质量数据与合理评测来推动研究。

Method: 1) 构建PorePatch高质量毛孔级补丁数据集并制定评测基准；2) 提出DMCE（Data-Model Co-Evolution）框架，从高分辨率人脸图像迭代生成、筛选与精炼高质量数据；3) 在该数据集上训练现有SOTA深度描述子；4) 进行广泛实验，评估匹配任务与3D重建任务表现。

Result: 在补丁匹配任务上，SOTA模型FPR95=1.91%，相较PSIFT的22.41%提升约20.5个百分点；但在3D重建任务中，深度描述子的整体表现与传统描述子相当，优势减弱。

Conclusion: 提供了首个高质量毛孔级人脸补丁数据集与评测框架，证明深度描述子在纯匹配上显著优于传统方法，但在弱纹理场景下的端到端3D重建收益有限，提示仍需在特征鲁棒性、跨尺度与跨域泛化、以及任务级联中的有效性方面进一步研究。

Abstract: The weak-texture nature of facial skin regions presents significant challenges for local descriptor matching in applications such as facial motion analysis and 3D face reconstruction. Although deep learning-based descriptors have demonstrated superior performance to traditional hand-crafted descriptors in many applications, the scarcity of pore-scale image patch datasets has hindered their further development in the facial domain. In this paper, we propose the PorePatch dataset, a high-quality pore-scale image patch dataset, and establish a rational evaluation benchmark. We introduce a Data-Model Co-Evolution (DMCE) framework to generate a progressively refined, high-quality dataset from high-resolution facial images. We then train existing SOTA models on our dataset and conduct extensive experiments. Our results show that the SOTA model achieves a FPR95 value of 1.91% on the matching task, outperforming PSIFT (22.41%) by a margin of 20.5%. However, its advantage is diminished in the 3D reconstruction task, where its overall performance is not significantly better than that of traditional descriptors. This indicates that deep learning descriptors still have limitations in addressing the challenges of facial weak-texture regions, and much work remains to be done in this field.

</details>


### [47] [EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation](https://arxiv.org/abs/2512.00385)
*Louis Geist,Loic Landrieu,Damien Robert*

Main category: cs.CV

TL;DR: 提出EZ-SP：一种全GPU、可学习的超点（superpoint）分割算法，替代传统CPU分割瓶颈，13×更快分区，与轻量分类器结合实现实时、低显存、跨场景3D语义分割，并在精度上匹配点级SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于超点的3D语义分割虽然高效，但受限于需要在CPU上进行几何分割，成为系统整体的速度瓶颈且依赖手工特征。需要一种可学习、可端到端训练、在GPU上高速运行、兼顾几何与语义一致性的分区方法。

Method: 提出紧凑的GPU端可学习分区模块（<60K参数），利用可微替代损失训练，无需手工特征，生成几何/语义一致的超点；与轻量级超点分类器组合，形成端到端管线。该方法可在多百万点场景中进行实时推理，整体显存占用<2MB。

Result: 分区速度较以往方法提升13×；整体推理速度提升72×、参数量减少120×，同时在S3DIS、KITTI-360、DALES三大数据集上达到与点级SOTA相当的精度；训练时间<20分钟。

Conclusion: EZ-SP消除了超点管线的CPU分区瓶颈，实现全GPU、低参数、低显存、实时的3D语义分割，在多数据域上保持SOTA级精度并具备良好可扩展性；代码与模型已开源。

Abstract: Superpoint-based pipelines provide an efficient alternative to point- or voxel-based 3D semantic segmentation, but are often bottlenecked by their CPU-bound partition step. We propose a learnable, fully GPU partitioning algorithm that generates geometrically and semantically coherent superpoints 13$\times$ faster than prior methods. Our module is compact (under 60k parameters), trains in under 20 minutes with a differentiable surrogate loss, and requires no handcrafted features. Combine with a lightweight superpoint classifier, the full pipeline fits in $<$2 MB of VRAM, scales to multi-million-point scenes, and supports real-time inference. With 72$\times$ faster inference and 120$\times$ fewer parameters, EZ-SP matches the accuracy of point-based SOTA models across three domains: indoor scans (S3DIS), autonomous driving (KITTI-360), and aerial LiDAR (DALES). Code and pretrained models are accessible at github.com/drprojects/superpoint_transformer.

</details>


### [48] [WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing](https://arxiv.org/abs/2512.00387)
*Kaihang Pan,Weile Chen,Haiyi Qiu,Qifan Yu,Wendong Bu,Zehan Wang,Yun Zhu,Juncheng Li,Siliang Tang*

Main category: cs.CV

TL;DR: WiseEdit提出一个面向“认知与创造”图像编辑能力的知识密集型基准，通过三阶段任务与三类知识维度、共1220个用例，系统评测现有模型在认知推理与创意合成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑评测范围狭窄，无法全面衡量新一代具备认知与创造能力的编辑模型。需要一个能覆盖更深任务链条与更广知识类型的基准，客观揭示模型在知识驱动编辑中的能力与短板。

Method: 构建WiseEdit基准：1）将图像编辑抽象为类人认知创作三阶段——Awareness（觉察）、Interpretation（解释）、Imagination（想象），并为每阶段设计对应任务；2）设置“复杂任务”，要求三个阶段综合协同；3）覆盖三类知识：陈述性（Declarative）、程序性（Procedural）、元认知（Metacognitive）；4）共收集与标注1220个测试用例，并给出评测流程与代码。

Result: 在WiseEdit上测试多种SOTA图像编辑模型，结果显示其在基于知识的认知推理与创意组合方面存在明显局限，无法稳定完成分阶段及复杂任务。

Conclusion: WiseEdit能够更全面、客观地刻画图像编辑模型在认知与创造导向任务中的真实能力与缺陷；其发布将为后续方法改进与可比评测提供标准化平台。

Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.

</details>


### [49] [Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction](https://arxiv.org/abs/2512.00395)
*Jiazhen Liu,Mingkuan Feng,Long Chen*

Main category: cs.CV

TL;DR: 提出STAMP：在保持对话能力的同时，以一次前向预测整张分割掩码，解决MLLM中对话-分割性能-速度三难题。


<details>
  <summary>Details</summary>
Motivation: 现有在MLLM中做分割的方法两类：1）嵌入预测（像素/patch级监督）会引入与对话预训练冲突的目标，损伤对话能力；2）将分割改为自回归的下一词预测，可保留对话，但在稀疏输出下分割性能差，在稠密输出下推理极慢。因此需要一种既不损害对话、又能高质量分割且推理高效的新范式。

Method: 提出“All-Mask Prediction”范式并实现为STAMP：先自回归生成文本回答；随后将分割视为对整幅图像patch的并行“填空”任务，在单次前向中预测全部掩码token，利用双向空间上下文进行非自回归解码，从而与对话目标解耦。

Result: 在多个分割基准上显著优于SOTA；同时保持强对话能力，并在推理速度上优于自回归分割方案。

Conclusion: 通过将对话与分割解耦为自回归文本与非自回归全掩码预测，STAMP在不牺牲任何一端的情况下，实现对话、分割精度与速度的三赢，为MLLM集成分割提供可行新路线。

Abstract: Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel "fill-in-the-blank" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.

</details>


### [50] [Low-Bitrate Video Compression through Semantic-Conditioned Diffusion](https://arxiv.org/abs/2512.00408)
*Lingdong Wang,Guan-Ming Su,Divya Kothandaraman,Tsung-Wei Huang,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.CV

TL;DR: 提出DiSCo语义视频压缩：仅传输语义与粗外观/运动线索，利用条件扩散生成先验重建高质视频；在极低码率下感知质量显著优于传统与语义基线。


<details>
  <summary>Details</summary>
Motivation: 传统编解码器追求像素保真，在极低码率下与人类感知不匹配，导致严重伪影；需要一种以感知相关信息为核心、用生成模型补全细节的压缩范式。

Method: 将源视频分解为三种紧凑模态：文本描述（语义）、时空降质视频（外观）、可选草图/姿态（运动）。使用条件视频扩散模型从这些模态重建视频。提出时间前向填充、token交织以及面向不同模态的专用编解码器，以提升多模态生成效果与模态紧凑性。

Result: 在低码率下，在多种感知指标上较语义与传统编解码基线提升约2-10倍。

Conclusion: 语义驱动+生成先验的压缩框架能在超低码率下显著提升感知质量，证明与人类感知对齐的表示优于像素级还原；所提多模态设计与训练技巧有效。

Abstract: Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.

</details>


### [51] [SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control](https://arxiv.org/abs/2512.00413)
*Ji Gan,Lingxu Chen,Jiaxu Leng,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出SplatFont3D：基于3D高斯点渲染的结构感知文本到3D艺术字体生成框架，实现多风格文本提示驱动与精细部件级风格控制，较NeRF更快且风格一致性与视觉质量更优。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于二维艺术字体，缺乏个性化三维生成方法；三维字体可用于沉浸式应用并反哺二维视角合成，同时三维字体具有强结构语义与部件级风格控制需求，现有方法难以兼顾。

Method: 1) Glyph2Cloud模块：从2D字形/部件渐进增强形状与风格，生成对应3D点云以初始化3D高斯；2) 采用得分蒸馏采样(SDS)与预训练2D扩散模型交互优化3D高斯；3) 动态组件分配：利用3D高斯几何先验划分部件，缓解优化过程中的漂移耦合，实现部件级控制。

Result: 与现有3D模型相比，在风格-文本一致性、视觉质量与渲染效率上均取得更好表现，并实现更明确有效的部件级风格控制，渲染速度优于基于NeRF的方法。

Conclusion: SplatFont3D有效解决三维艺术字体的结构约束与细粒度控制难题，提供高效、可控的文本到3D字体生成方案，并对三维与二维字体应用均具潜在价值。

Abstract: Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.

</details>


### [52] [PhysGen: Physically Grounded 3D Shape Generation for Industrial Design](https://arxiv.org/abs/2512.00422)
*Yingxuan You,Chen Zhao,Hantao Zhang,Mingda Xu,Pascal Fua*

Main category: cs.CV

TL;DR: 提出物理引导的3D形状生成：在潜空间中交替进行速度更新与物理细化，并用SP‑VAE联合编码形状与物理，实现更逼真、物理一致的工业设计形状。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型虽能生成高保真外观，但缺乏工程物理知识（如汽车气动效率），导致外观真实却物理不可信、难以用于工业设计。需要把物理约束融入生成过程以提升“工程真实感”。

Method: 构建统一的物理引导生成管线：1) 设计带显式物理指导的flow matching模型，进行交替更新——(a) 速度驱动的潜码更新；(b) 基于物理评估/优化的细化，使潜码逐步满足目标形状与物理指标。2) 在速度更新中加入物理感知正则项，提升物理一致性。3) 训练形状-物理联合变分自编码器（SP‑VAE），在统一潜空间内同时表征几何与物理性质，为物理引导更新提供梯度与表征支撑。

Result: 在三项基准上，相比仅看视觉的生成方法，该方法在形状逼真度与物理一致性（如空气动力学指标等）上均取得更优表现，生成结果更符合工程期望。

Conclusion: 通过在潜空间中结合flow matching与物理优化、并用SP‑VAE联合编码形状与物理信息，可显著提升工业设计类3D形状的物理可信度与真实感，证明物理引导对形状生成的有效性。

Abstract: Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.

</details>


### [53] [Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali](https://arxiv.org/abs/2512.00424)
*Nthenya Kyatha,Jay Taneja*

Main category: cs.CV

TL;DR: 利用公交车内既有CCTV，构建YOLOv12检测+BotSORT跟踪+OSNet行人Re-ID+OCR时间戳+车载遥测站点识别的流水线，恢复公交OD。低密度良光条件下计数准确（R≈95%、P≈91%、F1≈93%），OD与人工吻合；拥挤、黑白画面、姿态变化、非常规上下客等现实压力下显著退化（高峰低估约40%、黑白段召回降~17个百分点），提示需面向部署的更鲁棒Re-ID方法。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲公交常态化拥挤，现有自动化系统难以可靠获取客流与OD；车辆已部署的安防CCTV提供可利用数据源，若能转为客流计数与OD推断，可支撑运营优化与规划。

Method: 提出一个端到端基线流程：1) YOLOv12进行乘客目标检测；2) BotSORT多目标跟踪；3) OSNet嵌入做跨摄像头/长时段Re-ID；4) OCR从视频叠加信息提取时间戳；5) 结合车辆遥测（速度/经纬度/开门状态）进行站点识别与上下客事件归因；6) 聚合生成OD矩阵。并在内罗毕与基加利公交CCTV片段上标注评测。

Result: 在低密度、光照良好条件下，计数表现高（召回约95%、精度约91%、F1约93%），生成的OD矩阵与人工统计高度一致；在拥挤、彩转黑白、乘客姿态变化、非标准车门使用等压力条件下，计数和Re-ID显著退化：高峰期上客约40%低估，黑白段召回下降约17个百分点。

Conclusion: 安防CCTV可用于恢复公交OD，基线系统在理想条件下有效，但对SSA场景特有的挑战极为敏感；未来需开发更鲁棒、面向部署的Re-ID与拥挤场景计数方法，并改进对光照/画质/行为异质性的适应。

Abstract: Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\approx$95\%, precision $\approx$91\%, F1 $\approx$93\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\sim$40\% undercount in peak-hour boarding and a $\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.

</details>


### [54] [What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards](https://arxiv.org/abs/2512.00425)
*Minh-Quan Le,Yuanzhi Zhu,Vicky Kalogeiton,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出NewtonRewards：一种基于“可验证奖励”的视频扩散模型后训练框架，用光流近似速度、外观特征近似质量，施加牛顿学约束与质量守恒奖励，在自建NewtonBench-60K上提升物理可信度、平滑性与时序一致性，并具OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当代视频扩散模型虽有较强视觉保真度，但常违背基本物理规律（漂浮、加速度异常、碰撞不一致），说明视觉真实与物理真实存在差距。缺乏可验证、可扩展的物理对齐手段，现有依赖人工或VLM反馈的方法不稳定、成本高、难以验证。

Method: 后训练阶段引入“可验证奖励”。用冻结的工具模型从生成视频中提取可测代理：光流≈速度；高层外观特征≈质量。基于此设计两类奖励：1) 牛顿运动学约束，鼓励恒定加速度的动力学；2) 质量守恒奖励，抑制退化解（如通过外观变形规避动力学约束）。在五类牛顿运动基元（自由落体、水平/抛掷、斜坡下/上滑）上进行优化与评估。

Result: 在NewtonBench-60K基准上，相比既有后训练方法，在视觉与物理指标上均有一致提升，包括更高的物理可信度、更平滑的运动与更强的时序一致性；在高度、速度、摩擦等分布外设置下仍保持良好性能。

Conclusion: 基于物理约束的可验证奖励为视频生成引入可扩展的物理对齐路径；NewtonRewards有效缩小视觉真实与物理真实间的鸿沟，并具备稳健的OOD泛化能力。

Abstract: Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\texttt{NewtonRewards}$ extracts $\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.

</details>


### [55] [Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana](https://arxiv.org/abs/2512.00428)
*Jiachuan Peng,Kyle Lam,Jianing Qiu*

Main category: cs.CV

TL;DR: 用谷歌新模型“Nano Banana”生成的合成胸片训练分类器，零实拍训练的情况下在两套真实数据集上检测肺炎取得高AUROC/AUPR，证明合成数据可行，但目前受限于提示词控制多样性与后处理对齐，临床落地仍需充分验证与监管伦理。


<details>
  <summary>Details</summary>
Motivation: 探索仅用合成医学影像训练模型，能否在真实世界胸片上取得可靠性能，从而缓解真实医疗数据稀缺、隐私与标注成本高的问题。

Method: 使用Nano Banana生成多样化的合成胸片数据及标签，训练肺炎分类器；不进行在真实数据上的微调，直接在两个真实公开数据集（RSNA 2018与Chest X-Ray 5,856张）做外部验证，报告AUROC与AUPR及95%置信区间；讨论生成控制（prompt）与后处理策略。

Result: 在RSNA数据集上AUROC 0.923（95%CI:0.919–0.927）、AUPR 0.900（95%CI:0.894–0.907）；在Chest X-Ray数据集上AUROC 0.824（95%CI:0.810–0.836）、AUPR 0.913（95%CI:0.904–0.922）。显示合成训练在真实测试上有较好泛化。

Conclusion: 仅用合成胸片训练的模型在真实数据上取得强性能，显示该路径可行并具潜力；但受限于提示词难以精准控制多样性、需后处理贴合真实分布等。临床转化前仍需更广泛验证、监管审批与伦理审查。

Abstract: We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.

</details>


### [56] [FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal](https://arxiv.org/abs/2512.00438)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 提出FR-TTS：在NTP生成中用“填充式奖励”估计中间序列的未来完成，提升测试时扩展（TTS）的筛选有效性与最终质量。


<details>
  <summary>Details</summary>
Motivation: 传统TTS在图像生成中依赖并行采样+奖励模型筛选，但迁移到NTP时遇到困难：中间token序列解码成的“不完整图像”与最终图像的奖励相关性低，无法有效指导剪枝与保留路径。

Method: 提出Filling-Based Reward（FR）：对每个中间样本，通过搜索合理的“填充方案”来补全后续token，近似其未来轨迹，并用该补全结果的奖励作为对中间样本的评价信号。基于FR设计FR-TTS：高效搜索填充方案，并引入动态权重的多样性奖励，使筛选既兼顾质量又保持多样性。

Result: FR显著提高中间样本与最终样本奖励的相关系数，且与内在信号（如token置信度）一致；FR-TTS在多项基准与多种奖励模型上均优于现有方法。

Conclusion: 通过对中间序列进行合理补全来估计其终局质量，可使TTS在NTP框架中有效运作；FR-TTS实现更稳健的中间样本评估与选择，提升最终生成质量与多样性。

Abstract: Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.

</details>


### [57] [RecruitView: A Multimodal Dataset for Predicting Personality and Interview Performance for Human Resources Applications](https://arxiv.org/abs/2512.00450)
*Amit Kumar Gupta,Farhan Sheth,Hammad Shaikh,Dheeraj Kumar,Angkul Puniya,Deepak Panwar,Sandeep Chaurasia,Priya Mathur*

Main category: cs.CV

TL;DR: 提出RecruitView数据集与CRMF几何深度学习框架，在多流形（双曲/球面/欧式）上建模行为表征，通过几何专家与自适应路由+切空间融合，实现更高效更准确的人格与软技能评估，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 多模态行为数据的人格与软技能自动评估受限于数据规模与方法无法刻画人类特质的几何结构（层级性、方向性与连续性）。需要更贴合特质几何结构的模型与具有成对比较标注的公开数据集。

Method: 构建RecruitView数据集：300+被试、2011段自然面试视频、跨12维度（大五、总体人格、6项面试表现）27,000对成对比较标注。提出CRMF框架：在双曲、球面、欧式三种流形上分别用几何特定专家网络学习层级结构、方向模式与连续变化；通过自适应路由按输入特征动态加权专家；采用切空间（tangent space）融合实现跨流形回归；以更少参数完成跨模态预测。

Result: 在广泛实验中，相比选择的多模态基线，CRMF在Spearman相关最高提升11.4%，在一致性指数提升6.0%；同时较大型多模态模型减少约40–50%的可训练参数。

Conclusion: 利用多流形几何建模与自适应专家融合，可以更有效地从自然面试多模态数据预测人格与软技能。RecruitView为该方向提供了公开基准，CRMF在准确性与参数效率上均优于现有方法。

Abstract: Automated personality and soft skill assessment from multimodal behavioral data remains challenging due to limited datasets and methods that fail to capture geometric structure inherent in human traits. We introduce RecruitView, a dataset of 2,011 naturalistic video interview clips from 300+ participants with 27,000 pairwise comparative judgments across 12 dimensions: Big Five personality traits, overall personality score, and six interview performance metrics. To leverage this data, we propose Cross-Modal Regression with Manifold Fusion (CRMF), a geometric deep learning framework that explicitly models behavioral representations across hyperbolic, spherical, and Euclidean manifolds. CRMF employs geometry-specific expert networks to capture hierarchical trait structures, directional behavioral patterns, and continuous performance variations simultaneously. An adaptive routing mechanism dynamically weights expert contributions based on input characteristics. Through principled tangent space fusion, CRMF achieves superior performance while training 40-50% fewer trainable parameters than large multimodal models. Extensive experiments demonstrate that CRMF substantially outperforms the selected baselines, achieving up to 11.4% improvement in Spearman correlation and 6.0% in concordance index. Our RecruitView dataset is publicly available at https://huggingface.co/datasets/AI4A-lab/RecruitView

</details>


### [58] [CausalAffect: Causal Discovery for Facial Affective Understanding](https://arxiv.org/abs/2512.00456)
*Guanyu Hu,Tangzheng Lian,Dimitrios Kollias,Oya Celiktutan,Xinyu Yang*

Main category: cs.CV

TL;DR: 提出CausalAffect，一个用于面部情感分析的因果图发现框架，在无需联合标注或手工因果先验的情况下，通过两层极性/方向感知层级和特征级反事实干预，学习AU与表情间的因果依赖，并在6个基准上同时提升AU检测与表情识别。


<details>
  <summary>Details</summary>
Motivation: 现有情感计算多依赖AU，但大多只做相关性建模，缺乏从数据中直接学习“心理学上合理”的AU-AU与AU-表情的因果关系；同时容易受伪相关影响，影响可解释性与泛化。

Method: 1) 因果层级：构建两层因果图，建模AU-AU与AU-表情的依赖，具备极性（促进/抑制）与方向性；融合群体层面的规律与样本自适应结构。2) 反事实干预：在特征层面实施反事实干预，强化真实因果效应、抑制伪相关。3) 无需联合标注数据或手工因果先验，直接从数据发现结构。

Result: 在6个基准上取得SOTA，同时恢复的因果结构与心理学理论一致，并揭示新的抑制性和未刻画的依赖关系。

Conclusion: CausalAffect将因果发现与面部行为可解释建模相结合，在不依赖额外先验与联合标注的前提下，提升AU检测与表情识别性能，并提供与心理学一致且可解释的因果图；代码与模型将开源。

Abstract: Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.

</details>


### [59] [RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards](https://arxiv.org/abs/2512.00473)
*Junyan Ye,Leiqi Zhu,Yuncheng Guo,Dongzhi Jiang,Zilong Huang,Yifan Zhang,Zhiyuan Yan,Haohuan Fu,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 提出RealGen框架，通过“检测器奖励”与GRPO优化，使文本到图像更具照片真实感；并发布自动化评测基准RealBench，显示在真实感、细节、美学上优于通用与专门模型。


<details>
  <summary>Details</summary>
Motivation: 尽管现有T2I模型在文本一致性与常识方面进步显著，但在照片级真实感上仍产生显著AI伪迹（如过度平滑皮肤、油亮高光），与“以假乱真”的初衷相悖，亟需系统性方法提升真实感并提供可靠的无人工评测。

Method: 提出RealGen：将LLM用于提示词优化，扩散模型用于生成；受对抗式思路启发，引入“检测器奖励”，利用语义级与特征级合成图像检测器量化伪迹与真实感；用GRPO强化学习算法利用该奖励端到端优化生成流程。另构建RealBench评测，包含Detector-Scoring与Arena-Scoring，实现无人工的真实感测评。

Result: 实验证明RealGen在真实感、细节与美学上显著优于GPT-Image-1、Qwen-Image等通用模型以及FLUX-Krea等专注真实感的模型；自动评测与用户体验更一致。

Conclusion: 结合检测器驱动的奖励与RL优化能有效提升T2I照片真实感；RealBench提供可靠的人类无参与评估途径，整体方法推动“以假乱真”级别生成。

Abstract: With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.

</details>


### [60] [Structured Context Learning for Generic Event Boundary Detection](https://arxiv.org/abs/2512.00475)
*Xin Gu,Congcong Li,Xinyao Wang,Dexiang Hong,Libo Zhang,Tiejian Luo,Longyin Wen,Heng Fan*

Main category: cs.CV

TL;DR: 提出结构化上下文学习（SCL）用于通用事件边界检测（GEBD）：用序列结构化划分（SPoS）提供线性复杂度的时序上下文，计算组间相似性并用轻量FCN判定边界，配合高斯核软化标注。在Kinetics-GEBD、TAPOS和镜头切换数据集上优于SOTA，兼容多种时序模型且速度/精度权衡更佳。


<details>
  <summary>Details</summary>
Motivation: GEBD需准确、快速地捕捉人类感知的事件边界，但现有方法依赖特定时序模型、计算昂贵或对标注模糊敏感，难以在长视频上兼顾效率与精度。

Method: 1) 提出SPoS，将帧序列结构化分组，提供有组织的时序上下文，整体复杂度对视频长度为线性；2) 计算组间相似性以显式建模帧间差异；3) 用轻量级全卷积网络在相似性图上判定边界；4) 采用高斯核预处理真值边界，缓解标注不确定性；5) 框架端到端，可与GRU/LSTM/Transformer等灵活搭配。

Result: 在Kinetics-GEBD、TAPOS及镜头转场检测数据集上取得优于现有SOTA的性能，同时展现更优的速度-精度权衡。

Conclusion: 结构化上下文学习通过SPoS与组相似性建模，在保持线性复杂度与模型兼容性的同时提升GEBD性能，并通过高斯核处理增强对标注模糊的鲁棒性，适用于多种时序架构与相关任务。

Abstract: Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.

</details>


### [61] [Learning What Helps: Task-Aligned Context Selection for Vision Tasks](https://arxiv.org/abs/2512.00489)
*Jingyu Guo,Emir Konuk,Fredrik Strand,Christos Matsoukas,Kevin Smith*

Main category: cs.CV

TL;DR: 提出TACS：让模型学习挑选“能提升任务表现”的配对示例，而非仅相似的例子；通过梯度监督+强化学习联合优化选择器与任务模型，在18个细粒度/医疗分类与分割数据集上优于相似度检索，尤其在困难或小数据场景。


<details>
  <summary>Details</summary>
Motivation: 类ViT的判别模型在遇到视觉不确定性时缺乏人类式“找对例子参照”的能力；现有基于相似度的检索往往选到看起来相似但对任务无益的样本，无法稳定提升下游性能，尤其在细粒度与医疗场景中。

Method: 提出Task-Aligned Context Selection (TACS)。将“示例选择”纳入学习目标：训练一个选择器网络与任务模型联合优化。优化采用混合策略：1) 基于梯度的监督信号引导选择器朝提升任务损失的方向更新；2) 结合强化学习，以任务奖励作为回报，直接优化非可微的检索/选择过程，从而学会挑选真正提升性能的配对样本。

Result: 在18个数据集（细粒度识别、医疗图像分类与分割）上，TACS相较相似度检索持续提升性能，提升在更具挑战或数据受限条件下更显著。

Conclusion: 让示例选择与任务奖励对齐可显著增强判别模型的上下文利用能力；整合梯度监督与强化学习的混合优化，使检索成为端到端学习的一部分，带来跨任务、跨数据集的稳健收益。

Abstract: Humans often resolve visual uncertainty by comparing an image with relevant examples, but ViTs lack the ability to identify which examples would improve their predictions. We present Task-Aligned Context Selection (TACS), a framework that learns to select paired examples which truly improve task performance rather than those that merely appear similar. TACS jointly trains a selector network with the task model through a hybrid optimization scheme combining gradient-based supervision and reinforcement learning, making retrieval part of the learning objective. By aligning selection with task rewards, TACS enables discriminative models to discover which contextual examples genuinely help. Across 18 datasets covering fine-grained recognition, medical image classification, and medical image segmentation, TACS consistently outperforms similarity-based retrieval, particularly in challenging or data-limited settings.

</details>


### [62] [CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration](https://arxiv.org/abs/2512.00493)
*Boshi Tang,Henry Zheng,Rui Huang,Gao Huang*

Main category: cs.CV

TL;DR: 提出CC-FMO：一种零样本、相机条件的单图到3D场景生成方法，兼顾输入图像的物体布局与实例保真度，通过混合实例生成和相机条件尺度求解，生成高保真、相机对齐且空间一致的组合场景，优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖小数据专模、泛化差；大型3D基础模型虽提升了单实例生成，但场景级生成仍受限于物体姿态不准与全局空间不一致，难以从单张图重建连贯的3D场景。

Method: 提出CC-FMO管线：1) 相机条件（camera-conditioned）场景生成框架；2) 混合实例生成器，结合语义感知的向量集表示（保证语义与布局）与富细节的结构化潜表示（保证几何与纹理质量），得到语义合理且高质量的对象几何；3) 简洁有效的相机条件尺度求解算法，使基础姿态估计模型可用于场景生成，并在全局层面约束尺度与空间一致性，确保相机对齐的场景组合。

Result: 在大量实验中，方法能稳定生成高保真、相机对齐且组合一致的场景；在多项评测上全面优于所有SOTA方法。

Conclusion: CC-FMO实现了零样本单图到3D场景生成的语义-几何双重保真与全局一致性，解决姿态误差与尺度不一致问题，为AR/VR与具身智能提供更可靠的场景生成能力。

Abstract: High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.

</details>


### [63] [Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching](https://arxiv.org/abs/2512.00514)
*Tanaka Nobuaki*

Main category: cs.CV

TL;DR: 提出一种基于智能手机的结构光系统，通过投射网格并用新算法重建地面微小不平，改善低成本移动平台在不平整地形上的稳定行驶。核心贡献为拓扑约束的二维DTW，用于稳健匹配变形网格以支持三角测量与地形重建。


<details>
  <summary>Details</summary>
Motivation: 低成本移动机器人在崎岖地形上，视觉难以感知的小幅起伏会显著影响行驶稳定性。现有视觉/深度方案成本高、对光照或纹理敏感，且移动端算力受限，促使探索低成本、单设备、在资源受限平台上可靠工作的地形感知方法。

Method: 设计一套智能手机结构光方案：投射二维规则网格到地面，用相机观测其在透视变形与部分遮挡下的图样，提出“拓扑约束二维动态时间规整(2D-DTW)”算法。该算法以列为单位进行对齐，在全局网格一致性约束下匹配投影网格与观测网格，从而保持网格拓扑，便于后续三角测量重建局部地形起伏。算法强调简单高效，适配资源受限设备实时或近实时运行。

Result: 2D-DTW在存在透视畸变和局部遮挡情况下，能够稳健地匹配规则网格与其变形观测，保持网格结构，支持精准三角测量重建局部地形不平。同时显示出在一般图像处理中匹配结构化网格图案的潜在通用性。系统级演示验证了方法在地形感知场景中的有效性。

Conclusion: 提出的智能手机结构光系统与拓扑约束2D-DTW为低成本平台提供了鲁棒、轻量的地形起伏感知方案，既能提升移动平台在不平地形上的稳定性，也可作为通用网格匹配工具应用于其他图像处理任务。

Abstract: Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.

</details>


### [64] [Image Generation as a Visual Planner for Robotic Manipulation](https://arxiv.org/abs/2512.00532)
*Ye Pang*

Main category: cs.CV

TL;DR: 将大规模语言-图像预训练的图像生成模型，经少量LoRA微调后，用于机器人操作任务的视频式可视化规划：给定首帧+文本或首帧+2D轨迹覆盖，模型可生成平滑连贯、与条件一致的机器人视频，在Jaco Play、Bridge V2、RT1上有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型依赖大量特定领域数据且泛化差；而图像生成模型在语言-图像预训练下具备强组合性并能合成“时间一致”的网格图，暗示其潜在的时序建模能力。希望以更低数据/监督成本获得可泛化的机器人视觉规划器。

Method: 提出两阶段/两模式的LoRA轻微调框架：1) 文本条件生成：输入语言指令与初始帧，生成后续视频；2) 轨迹条件生成：输入初始帧与2D轨迹叠加，引导生成与轨迹一致的视频。使用预训练图像生成器作为基础模型，仅作小幅参数适配。

Result: 在Jaco Play、Bridge V2、RT1数据集上，两种模式均生成平滑、时序连贯且与条件（文本或轨迹）一致的机器人操作视频，显示出良好的对齐与视觉质量。

Conclusion: 大型图像生成模型内含可迁移的时间先验，经最小监督与轻量微调即可充当“类视频”的机器人规划器，为统一感知-规划-行动提供新路径；代码已开源。

Abstract: Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.
  We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.
  Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.

</details>


### [65] [Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update](https://arxiv.org/abs/2512.00534)
*Zeyuan An,Yanghang Xiao,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 提出Cross-Temporal 3DGS：利用稀疏图像与历史先验，高效跨时间重建与更新3D场景，通过相机跨时标定、干涉式置信初始化与渐进式跨时优化，实现非连续采集下的场景版本化与时态复原，较基线在质量与数据效率上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实应用（城市规划、灾害评估、遗址保护）常缺乏密集扫描；需在不同时间点、以稀疏视角更新或回溯3D场景，并保持跨时间一致性。现有方法难以高效利用先验并处理非连续采集与时变变化。

Method: 三阶段框架：1) 跨时间相机对齐：估计并对齐不同时间戳的相机位姿；2) 基于“干涉”的置信初始化：识别时变与不变区域，用于指导更新；3) 渐进式跨时优化：将历史先验逐步融入当前3D高斯点云，利用稀疏新视图细化或从当前捕获反推过去场景。支持从稀疏图像生成可后续高精重建的时变表示。

Result: 在跨时间重建质量与数据效率上显著优于基线，能用极少视角实现场景更新与过去场景复原，并产生可重建成细致3D表示的时态变化。

Conclusion: Cross-Temporal 3DGS为场景版本化、跨时数字孪生与长期空间档案提供高效方案，解决稀疏视角与非连续采集条件下的跨时间一致性与更新问题。

Abstract: Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.

</details>


### [66] [SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning](https://arxiv.org/abs/2512.00539)
*Yongkang Hu,Yu Cheng,Yushuo Zhang,Yuan Xie,Zhaoxia Yin*

Main category: cs.CV

TL;DR: 提出SAIDO框架以提升AI生成图像检测在开放环境下的泛化与持续学习能力，通过“场景感知的专家模块+重要性引导的动态优化”兼顾塑性与稳定，显著降低误检与遗忘并提升开放世界准确率。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测在真实世界中新型生成方法与多样内容类型下泛化差，且在持续学习中易灾难性遗忘，难以同时保持对新任务的适应（塑性）与已学知识的稳定。

Method: 1) 场景感知专家模块（SAEM）：利用VLLMs识别/引入新场景，为每个场景动态分配独立专家以建模场景特异伪造特征，提升跨场景泛化。2) 重要性引导的动态优化机制（IDOM）：基于神经元重要性进行梯度投影，约束对关键参数的更新，兼顾模型塑性与稳定，缓解多生成方法下的遗忘。3) 整体为持续学习检测框架（SAIDO），可在任务序列上动态扩展与优化。

Result: 在持续学习任务上，相比SOTA，平均检测错误率与遗忘率分别相对降低44.22%与40.57%；在开放世界数据集上，平均检测准确率提升9.47%。

Conclusion: SAIDO以场景感知的专家化建模与重要性引导优化，有效提升了AI生成图像检测在开放环境下的泛化与持续学习性能，在稳定-塑性权衡上优于现有方法。

Abstract: The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.

</details>


### [67] [Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions](https://arxiv.org/abs/2512.00547)
*Sandika Biswas,Qianyi Wu,Biplab Banerjee,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 论文提出一种面向多人体、多物体动态场景的混合方法：用3D生成模型提供高保真初始网格，语义感知形变对人/物分别进行刚体与LBS变形，再以3D高斯溅射对各元素独立优化精对齐，从而在遮挡下保持结构一致并获得时空一致的几何重建，于HOI-M3数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实人造环境动态复杂，多人多物体相互作用频繁，单目条件下遮挡严重导致仅依赖GS渲染损失难以保持结构一致，现有GS方法鲜有覆盖多人人物体场景；需要一种能在强遮挡与多样运动中仍保证几何与时序一致的重建方案。

Method: 混合管线包含三步：1) 利用3D生成模型为场景中各元素生成高保真初始网格；2) 语义感知形变：对刚体物体施加刚体变换、对人类采用LBS（线性蒙皮）变形，并将形变后的网格映射到动态场景时序中；3) 基于3D高斯溅射对各元素分别进行优化，细化配准与几何细节，以增强对齐和一致性。

Result: 在多人人物体交互数据集HOI-M3上评估，能够在多视角与时间上保持几何一致，尤其在严重遮挡情况下保持物体结构，并在表面重建质量上优于最新方法。

Conclusion: 将生成先验（高保真网格）、语义驱动形变与GS精调相结合，可有效应对单目多人人物体动态场景中的遮挡与运动多样性问题，提升重建质量与时空一致性。

Abstract: Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.

</details>


### [68] [NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives](https://arxiv.org/abs/2512.00557)
*Haomiao Chen,Keith W Jamison,Mert R. Sabuncu,Amy Kuceyeski*

Main category: cs.CV

TL;DR: 提出NeuroVolve：用预训练视觉-语言模型的嵌入空间与“可编程”神经目标函数优化，实现受脑区信号指导的图像生成与编辑，验证单区选择性并扩展到多区协同/拮抗约束与个体化偏好。


<details>
  <summary>Details</summary>
Motivation: 以往生成模型多在孤立脑区复制已知类别选择性（如FFA-人脸），难揭示自然视场中多脑区如何协同编码与交互。需要一种能将脑信号直接转化为可控图像合成的框架，解析不同脑区及其组合的表征与关系。

Method: 构建NeuroVolve：在预训练视觉-语言模型（VLM）的嵌入空间中，通过优化一个由脑区活动组成的“神经目标函数”来合成图像。该目标可以激活/抑制单个或多个ROI；跟踪优化过程中的嵌入轨迹，实现统一的“偏好刺激生成+脑引导图像编辑”。验证包括单ROI低级与语义特征选择性、以及多ROI的协同（共激活）与对抗（去相关）约束。支持受试者特异化。

Result: 成功重现已知单脑区选择性，并在多区约束下生成语义连贯的场景；通过优化轨迹揭示语义路径；能生成符合低级和高级特征要求的刺激；实现多脑区协同/拮抗关系的可视化与操控；捕捉个体差异，完成个性化脑驱动合成。

Conclusion: NeuroVolve将脑活动与VLM嵌入优化结合，统一了偏好刺激生成与编辑，提供可解释、可编程的多脑区约束以探查视觉神经表征及其交互，并具备个体化应用潜力。

Abstract: What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.

</details>


### [69] [Describe Anything Anywhere At Any Moment](https://arxiv.org/abs/2512.00565)
*Nicolas Gorlo,Lukas Schmid,Luca Carlone*

Main category: cs.CV

TL;DR: 提出DAAAM：一种用于大规模、实时4D场景理解的时空记忆框架，结合优化型前端与层次化4D场景图，实现高效、几何对齐的开放词汇语义描述，并在NaVQA与SG3D上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放词汇丰富语义与实时3D/4D几何对齐之间存在权衡：细粒度语义说明通常导致无法实时；而追求实时又牺牲语义表达与跨时空一致性。需要一种既能快速推理又能生成几何落地、时空一致的语义记忆表示，以支持问答、任务推理与工具调用。

Method: 提出DAAAM框架：1) 优化型前端，将局部化描述模型（如DAM）通过批处理与优化策略加速一个数量级，在线推理细粒度语义描述；2) 构建层次化4D场景图（SG），将物体、关系、事件在空间与时间上全局对齐，提供一致的记忆表示；3) 以该4D SG对接工具调用智能体进行推理与推断，支持复杂时空问答与顺序任务定位。

Result: 在NaVQA上显著提升：问题准确率+53.6%，位置误差-21.9%，时间误差-21.6%；在SG3D上任务定位准确率+27.8%，并扩展并评测了大尺度长时OC-NaVQA基准。保持实时性能同时提供几何对齐的详细描述。

Conclusion: DAAAM通过优化推理与4D层次场景图实现了实时、几何落地、开放词汇的时空记忆表征，支持与代理工具衔接，达成SOTA并验证了在问答与任务 grounding 上的泛化能力；代码与数据开源。

Abstract: Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.
  We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.

</details>


### [70] [Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models](https://arxiv.org/abs/2512.00572)
*Mohammed Mohiuddin,Syed Mohammod Minhaz Hossain,Sumaiya Khanam,Prionkar Barua,Aparup Barua,MD Tamim Hossain*

Main category: cs.CV

TL;DR: 研究提出Yoga-16数据集，并系统比较三种CNN与三种输入（原图、MediaPipe骨架、YOLOv8骨架），发现骨架输入更优，VGG16+MediaPipe骨架达96.09%准确，并用Grad-CAM做可解释与交叉验证分析。


<details>
  <summary>Details</summary>
Motivation: 瑜伽广受欢迎但姿势错误易致伤，需要自动化姿态识别以减少对专家依赖。现有工作多用原始图像或单一关键点提取器，缺乏面向瑜伽的系统性基准与高质量数据集。

Method: 构建Yoga-16数据集；在三种深度架构（VGG16、ResNet50、Xception）与三种输入模态（原图、MediaPipe Pose骨架图、YOLOv8 Pose骨架图）上系统评测；以分类准确率为主度量，并用Grad-CAM进行可解释性分析与交叉验证验证稳健性。

Result: 骨架表征整体优于原图输入；最佳组合为VGG16+MediaPipe骨架，准确率96.09%；YOLOv8骨架与其他模型组合次之；提供了可视化揭示模型关注关键关节区域。

Conclusion: 针对瑜伽姿态识别，基于关键点的骨架输入优于原图；简单的VGG16在配合优质骨架（MediaPipe）时可达SOTA级别精度；Grad-CAM支持其决策合理性。数据集与基准为后续研究提供参考与可复现实验平台。

Abstract: Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.

</details>


### [71] [SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension](https://arxiv.org/abs/2512.00582)
*Yue Jiang,Haiwei Xue,Minghao Han,Mingcheng Li,Xiaolu Hou,Dingkang Yang,Lihua Zhang,Xu Zheng*

Main category: cs.CV

TL;DR: 提出SatireDecoder：一个无需训练的多智能体视觉解耦与不确定性引导链式推理框架，用于更好地理解纯视觉讽刺，提升准确性并降低幻觉。


<details>
  <summary>Details</summary>
Motivation: 视觉讽刺包含幽默与隐含批评，具有重要社会价值，但现有视觉-语言模型难以理解，容易忽视局部实体与全局语境的结合，产生偏差与幻觉。

Method: 构建训练免调的SatireDecoder：1) 多智能体系统对图像进行级联式视觉解耦，获得细粒度的局部与全局语义表示；2) 采用基于不确定性分析的链式推理，将复杂的讽刺理解拆解为不确定性最小化的连续子任务；3) 在推理中联合实体关系与语境信息以解析讽刺含义与指涉对象。

Result: 在视觉讽刺理解任务上显著优于现有基线，解释性更强并减少幻觉，实验验证方法有效性。

Conclusion: 多智能体视觉解耦结合不确定性引导的链式推理能有效提升高层次语义任务（如视觉讽刺）中的理解与稳健性，为视觉-语言推理提供新方向。

Abstract: Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.

</details>


### [72] [Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models](https://arxiv.org/abs/2512.00597)
*Thuraya Alzubaidi,Farhad R. Nezami,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出MedCT-VLM，用LoRA对CT-CLIP进行参数高效适配，在胸部CT多标签病灶零样本分类上显著提升（AUROC 61.3→68.9，Acc 67.2→73.6，Macro-F1 32.1→36.9）。


<details>
  <summary>Details</summary>
Motivation: 通用视觉-语言大模型在自然图像上具备强零样本能力，但在体数据（如CT）中的应用受限；全量微调成本高且医学标注稀缺，亟需参数高效的适配方法，将大规模预训练迁移到下游临床任务，尤其在零样本场景。

Method: 以CT-CLIP（在25,692例胸部CT体积上对比学习训练）为基础，采用LoRA在视觉与文本编码器的注意力层插入低秩分解矩阵，仅训练约1.67M（占总参数0.38%），目标为多标签胸部病理零样本分类：推理时以未见过的文本提示与CT嵌入对齐。

Result: 在18类胸部病理零样本分类上，相较未适配的基线，LoRA适配后AUROC提升7.6个百分点（61.3%→68.9%）、准确率提升6.4个百分点（67.2%→73.6%）、Macro-F1提升4.8个百分点（32.1%→36.9%）。

Conclusion: 参数高效的LoRA适配能够有效将大规模CT视觉-语言预训练迁移至下游医学影像任务，特别在标注稀缺的零样本场景中带来稳定收益。

Abstract: Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\% to 68.9\% (+7.6 pp), accuracy from 67.2\% to 73.6\% (+6.4 pp), and macro-F1 from 32.1\% to 36.9\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.

</details>


### [73] [Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning](https://arxiv.org/abs/2512.00625)
*Tzu-I Liao,Mahmoud Fakhry,Jibin Yesudas Varghese*

Main category: cs.CV

TL;DR: 研究评估YOLOv9、U‑Net、Swin Transformer、DeepLabV3、Mask R‑CNN用于树木横截面髓心自动检测，Swin准确率最高(0.94)，NMS显著提升Mask R‑CNN的IoU(0.45→0.80)，并通过外部橡木小数据集测试泛化性；结果表明深度学习可行且模型选择取决于数据特性与应用需求。


<details>
  <summary>Details</summary>
Motivation: 髓心定位对林业与木材质量分析至关重要，但目前多依赖人工、易出错且效率低；需要自动化、稳健的方法以提升准确性与可扩展性。

Method: 构建582张标注数据集并进行动态增强；比较五类主流模型（检测与分割）：YOLOv9、U‑Net、Swin Transformer、DeepLabV3、Mask R‑CNN；通过调参与数据增强解决张量不匹配与边界不一致问题；对OSU树轮实验室的11张橡木图像进行外部泛化测试；另外用64张额外树截面数据对表现最差模型再训练以探索泛化改进；对Mask R‑CNN引入NMS。

Result: Swin Transformer在精细分割上表现最佳，准确率0.94；YOLOv9框检出好但边界精度不足；U‑Net对结构化纹理有效；DeepLabV3具多尺度优势但边界略欠；Mask R‑CNN原始IoU较低(0.45)，加入NMS后显著提升至0.80；总体流程在小样本下通过增强与调参与NMS缓解关键问题。

Conclusion: 深度学习能有效自动化髓心检测；具体模型需依据数据规模、纹理复杂度与目标（框定位 vs 精细分割）选择：Swin适合精细边界，YOLO用于快速定位，U‑Net/DeepLabV3视结构与尺度需求取舍；NMS等后处理对实例分割至关重要；增强与合理调参与外部数据验证有助提升泛化。

Abstract: Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.

</details>


### [74] [XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance](https://arxiv.org/abs/2512.00626)
*Kim Gerard A. Villanueva,Priyanka Kumar*

Main category: cs.CV

TL;DR: 提出一个结合DCGAN数据增强、微调ResNet-50分类与LIME/SHAP可解释性的皮肤病多分类CAD系统，在HAM10000等不均衡数据上达到92.5%准确率与98.82%宏AUC，并优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 多类别皮肤病诊断依赖主观经验、数据集类别不均衡严重且深度模型缺乏可解释性，限制了临床落地。需要一种既高性能又可验证、可解释的自动化诊断框架。

Method: 1) 以DCGAN对每个类别进行针对性数据增强以缓解类别不均衡；2) 在增强后的数据上微调ResNet-50进行7类皮肤病分类；3) 融合LIME与SHAP，对模型判别区域与特征进行可解释性验证，关注不规则形态等临床关键征象。

Result: 在测试集上取得总体准确率92.50%、宏AUC 98.82%，性能超过多种既有架构；各类中Melanoma NOS的F1为0.8602，仍有改进空间。

Conclusion: 所提框架在保持高准确性的同时提供可验证的临床可解释性，适合安全诊断部署。后续应进一步提升对关键高风险类别（如Melanoma NOS）的判别能力。

Abstract: Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the "black box" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).

</details>


### [75] [Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation](https://arxiv.org/abs/2512.00639)
*Mahmoud El Hussieni*

Main category: cs.CV

TL;DR: 用YOLOv5进行甲状腺结节超声实例分割；含多普勒图像的数据显著提升性能；最佳为YOLOv5-L，Dice 91%、mAP 0.87；实时性与临床自动化潜力显现。


<details>
  <summary>Details</summary>
Motivation: 甲状腺癌发病率上升，需要准确、自动化的辅助诊断工具。结节精准分割是后续检测与决策支持的关键前置步骤，但临床常排除的多普勒图像是否有助于分割尚不明确。

Method: 在甲状腺超声数据上对YOLOv5的多个规模变体（N/S/M/L/XL）进行实例分割评估；构建两版数据集：包含多普勒与不包含多普勒；比较各模型在两版数据上的Dice和mAP表现，以量化多普勒信息的影响。

Result: 包含多普勒的版本整体优于不包含版本。YOLOv5-L在含多普勒数据上获得最佳：Dice 91%、mAP 0.87；不含多普勒时，YOLOv5-S仅有约79% Dice；加入多普勒后所有变体均有提升。

Conclusion: YOLOv5实例分割对甲状腺结节具有有效、接近实时的性能；多普勒图像虽临床上常被忽略，但能显著提升分割效果，支持其纳入自动化诊断流程，具备潜在临床应用价值。

Abstract: The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.

</details>


### [76] [Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition](https://arxiv.org/abs/2512.00641)
*Razieh Ghaedi,AmirReza BabaAhmadi,Reyer Zwiggelaar,Xinqi Fan,Nashid Alam*

Main category: cs.CV

TL;DR: 提出GAT-ADA：以ResNet-50为骨干，批级图注意力建模+对抗与统计双重对齐，用于无监督跨域表情识别；在多目标域上显著提升，均值74.39%，在RAF→FER2013上98.0%，较基线+36点。


<details>
  <summary>Details</summary>
Motivation: 跨域表情识别在训练与部署数据分布差异大，传统特征或单一对齐方法难以稳健泛化；需要同时挖掘批内样本间关系与实现源-目标分布对齐，以提升无监督域自适应效果。

Method: 混合框架GAT-ADA：1) 特征提取：ResNet-50；2) 批级图注意力：将每个mini-batch构成稀疏环形图，GAT在批内进行跨样本注意力聚合，利用互补线索缓解域偏移；3) 域对齐：对抗学习（GRL+域判别器）实现边训练边混淆域；配合统计对齐（CORAL对齐协方差，MMD对齐分布均值差异）；4) 无监督协议：源域有标签训练，多个目标域无标签适配。

Result: 在标准UDA协议下，源为RAF-DB，目标为CK+、JAFFE、SFEW 2.0、FER2013、ExpW：平均跨域准确率74.39%。在RAF→FER2013任务上达98.0%准确率，相比相同骨干与预处理的最佳基线约+36个百分点。

Conclusion: 批级图注意力与对抗+统计的双重对齐互补，可有效缓解跨域表情识别中的域偏移，显著优于强基线，表明利用批内关系进行适配是有效方向。

Abstract: Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.

</details>


### [77] [MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba](https://arxiv.org/abs/2512.00647)
*Shanhui Liu,Rui Xu,Yunke Wang*

Main category: cs.CV

TL;DR: 提出CF-ViM：先粗后细、按图像复杂度自适应分配分辨率与计算的Vision Mamba推理框架，兼顾效率与准确率，在ImageNet上优于基线与现有token压缩方法。


<details>
  <summary>Details</summary>
Motivation: 现有Vision Mamba效率仍受输入token数量限制。主流降token方法（剪枝/合并）会丢失信息，且一刀切地对所有图像做细粒度处理不合理；简单图像可粗粒度即可，复杂图像才需要细粒度。

Method: 设计Coarse-to-Fine Vision Mamba（CF-ViM）：1）粗阶段：将图像划分为大patch进行推理，显著缩短序列长度；2）置信度门控：若预测置信度低，则选择性地对低置信/关键区域以更细patch重采样并复算；3）动态分辨率分配：只对复杂区域追加计算，从而最小化额外成本并保留关键信息。

Result: 在ImageNet上，相较基线Vision Mamba与最先进token剪枝/合并方法，CF-ViM在准确率与效率上均有提升（更少计算下更高或相当的精度）。

Conclusion: 按图像复杂度自适应的粗到细推理可在不牺牲关键视觉信息的前提下大幅提升ViM推理效率，优于静态token压缩策略。

Abstract: Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.

</details>


### [78] [Realistic Handwritten Multi-Digit Writer (MDW) Number Recognition Challenges](https://arxiv.org/abs/2512.00676)
*Kiri L. Wagstaff*

Main category: cs.CV

TL;DR: 提出基于同一书写者的多位数字识别（MDW）基准，发现单字分类好但多位数字识别差，需新方法和任务特定指标。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的数字通常以多位形式出现（如邮编、支票金额、时间），且由同一人书写；现有研究多聚焦于孤立数字，乐观估计了真实场景的性能。作者希望构建更贴近现实的评测以推动对真实问题的解决。

Method: 利用NIST手写数字数据中书写者身份信息，构建多位数字、同一书写者的基准数据集（MDW）。设计伴随的、超越常规错误率的任务特定绩效指标，衡量真实应用中的影响；比较单字分类器在MDW上的表现与在孤立数字上的表现。

Result: 实验证实：许多在孤立数字上表现优异的分类器，在多位数字（同一书写者）识别任务上显著退化。任务特定指标揭示了与实际应用相关的性能差距。

Conclusion: 仅提升孤立数字识别不足以解决真实数字识别问题；需要利用任务/领域知识的新方法与评测框架。MDW基准与指标为开发超越单字分类的系统提供了机会与推动。

Abstract: Isolated digit classification has served as a motivating problem for decades of machine learning research. In real settings, numbers often occur as multiple digits, all written by the same person. Examples include ZIP Codes, handwritten check amounts, and appointment times. In this work, we leverage knowledge about the writers of NIST digit images to create more realistic benchmark multi-digit writer (MDW) data sets. As expected, we find that classifiers may perform well on isolated digits yet do poorly on multi-digit number recognition. If we want to solve real number recognition problems, additional advances are needed. The MDW benchmarks come with task-specific performance metrics that go beyond typical error calculations to more closely align with real-world impact. They also create opportunities to develop methods that can leverage task-specific knowledge to improve performance well beyond that of individual digit classification methods.

</details>


### [79] [Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer](https://arxiv.org/abs/2512.00677)
*Dong In Lee,Hyungjun Doh,Seunggeun Chi,Runlin Duan,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 提出Dynamic-eDiTor：一个无需训练的文本驱动4D场景编辑框架，结合MM-DiT与4D Gaussian Splatting，通过局部时空子网格注意与上下文token传播，提升多视角与时间一致性并直接优化预训练4DGS，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有4D重建已有进展，但文本驱动4D编辑难以同时保持多视角与时序一致性。基于2D扩散逐帧编辑易产生运动畸变、几何漂移与编辑不完整，迫切需要能跨时间与视角协同的编辑机制。

Method: 在4DGS表示上进行直接优化，利用多模态Diffusion Transformer（MM-DiT）作为编辑引导。提出两大关键组件：1) 时空子网格注意（STGA），在局部视角与时间邻域内进行注意力融合，确保局部一致性；2) 上下文Token传播（CTP），通过token继承与光流引导的token替换实现全局一致传播。整个流程免训练，以文本条件驱动编辑并保持几何与运动稳定。

Result: 在DyNeRF多视角视频数据集上进行大量实验，相比现有2D驱动或其他4D编辑方法，在编辑保真度、多视角一致性与时序一致性上均取得更好表现，能够无缝地产生多视角视频并稳定编辑动态对象。

Conclusion: Dynamic-eDiTor验证了结合MM-DiT与4DGS、并通过STGA与CTP实现局部与全局一致性的可行性。无需额外训练即可进行高保真、跨视角和时间一致的文本驱动4D编辑，为4D内容生成与编辑提供高效实用的方案。

Abstract: Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/

</details>


### [80] [Silhouette-based Gait Foundation Model](https://arxiv.org/abs/2512.00691)
*Dingqiang Ye,Chao Fan,Kartik Narayan,Bingzhe Wu,Chengwen Luo,Jianqiang Li,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出FoundationGait：一个可扩展的自监督步态基础模型，在12个数据集上预训练，0.13B参数，跨任务跨模态强泛化，零样本在Gait3D/OU-MVLP上达48.0%/64.5% rank-1。


<details>
  <summary>Details</summary>
Motivation: 现有步态研究受限于“小而专”的模型，难以随规模提升而进步（未遵循Scaling Laws），且不同任务（识别、健康筛查等）各自为政、泛化差。作者欲构建一个可扩展、统一的步态基础模型，回答为何步态模型过去难以扩展，以及单一模型能否服务多样步态任务与模态。

Method: 提出FoundationGait：在12个公开步态数据集（>200万行走序列）上进行自监督预训练的大模型（最大约1.3亿参数）。采用统一的预训练框架与多模态/多条件输入设计，预训练后可零样本推理或少量微调适配多任务（识别、健康筛查、属性估计等）。

Result: 在广泛数据集、场景与任务上表现稳健。零样本rank-1：Gait3D（野外，1000受试者）48.0%，OU-MVLP（实验室，5000+受试者）64.5%，创下鲁棒步态识别新里程碑；在其他任务如脊柱侧弯筛查、抑郁预测、属性估计等亦表现良好，且对输入模态具有适应性。

Conclusion: 通过大规模自监督预训练与统一框架，可打破步态建模在可扩展性与泛化上的瓶颈，实现一个能跨数据集、条件、任务与模态的步态基础模型，为步态识别与健康分析提供通用基座；代码与模型将开源以促进社区发展。

Abstract: Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.

</details>


### [81] [Affordance-First Decomposition for Continual Learning in Video-Language Understanding](https://arxiv.org/abs/2512.00694)
*Mengzhu Xu,Hanzhi Liu,Ningkang Peng,Qianyu Chen,Canran Xiao*

Main category: cs.CV

TL;DR: 提出AFD框架：将视频映射为稳定的“可供性”令牌作为共享底座，并用轻量可路由的调度器进行针对性适配，在仅复习问题的约束下实现持续学习并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 持续视频-语言理解面临非平稳数据/领域/查询，现有方法要么混淆稳定与可塑部分、依赖静态路由/容量、或需重放视频，受内存与隐私限制。需要一种明确划分稳定与可塑、且在有限资源下迭代适配的方案。

Method: 提出Affordance-First Decomposition（AFD）：1）将视频编码为变化缓慢、时间对齐的“可供性”token作为共享底座（substrate）；2）使用轻量级、按查询路由、冲突感知的调度器，将适配集中在易变部分，并按需增长容量；3）通过弱对齐与教师一致性稳定底座；4）训练中仅进行“仅问题回放”（不重放视频）。

Result: 在多协议上达SOTA：域增量VideoQA平均51.6%准确率、遗忘-1.8%；ViLCo上R@1@0.5分别为29.6%(MQ)、20.7%(NLQ)，VQ任务stAP@0.25为18.4%；时间增量iVQA准确率39.5%、遗忘-1.6%。

Conclusion: AFD实现了清晰、可解释的稳定-可塑分离：用交互中心的稳定底座承载通用表征，将适配聚焦在查询路由的轻量模块，并在内存/隐私限制下取得强泛化与低遗忘。

Abstract: Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.

</details>


### [82] [CAR-Net: A Cascade Refinement Network for Rotational Motion Deblurring under Angle Information Uncertainty](https://arxiv.org/abs/2512.00700)
*Ka Chung Lai,Ahmet Cetinkaya*

Main category: cs.CV

TL;DR: 提出CAR-net级联细化网络，用于旋转运动模糊的半盲去模糊：频域初解+多阶段残差细化，支持可选角度检测，合成与真实实验有效。


<details>
  <summary>Details</summary>
Motivation: 旋转运动模糊常见且退化强，角度信息常噪声化，现有去模糊方法在半盲场景下易产生伪影、细节恢复差，缺乏对参数不确定性的稳健处理与端到端方案。

Method: 提出CAR-net：先用频域反演得到初步去模糊估计；随后采用多级级联细化模块，对当前结果预测并应用残差校正，逐步抑制伪影、恢复细节；引入可选的角度检测分支，可与细化模块端到端联合训练以应对角度不确定。提供实现与数据链接。

Result: 在合成与真实图像上展示效果与效率，逐级细化可显著减少伪影并提升细节，证明在半盲旋转模糊场景下的有效性。

Conclusion: 级联细化结合频域初解与角度检测的端到端框架能稳健应对旋转模糊及角度不确定性，实证有效，代码与模型已开源。

Abstract: We propose a new neural network architecture called CAR-net (CAscade Refinement Network) to deblur images that are subject to rotational motion blur. Our architecture is specifically designed for the semi-blind scenarios where only noisy information of the rotational motion blur angle is available. The core of our approach is progressive refinement process that starts with an initial deblurred estimate obtained from frequency-domain inversion; A series of refinement stages take the current deblurred image to predict and apply residual correction to the current estimate, progressively suppressing artifacts and restoring fine details. To handle parameter uncertainty, our architecture accommodates an optional angle detection module which can be trained end-to-end with refinement modules. We provide a detailed description of our architecture and illustrate its efficiency through experiments using both synthetic and real-life images. Our code and model as well as the links to the datasets are available at https://github.com/tony123105/CAR-Net

</details>


### [83] [Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation](https://arxiv.org/abs/2512.00706)
*Chengzhi Yu,Yifan Xu,Yifan Chen,Wenyi Zhang*

Main category: cs.CV

TL;DR: 提出一种基于“按策略(on-policy)”数据与鲁棒DPO的迭代对齐框架，用二分类幻觉检测器做偏好标注，显著降低LVLM幻觉并让开源模型逼近/超越GPT-4V。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在多模态任务上易产生幻觉；现有偏好标注多依赖离策略(off-policy)数据与人工/启发式标注，可能把带幻觉的样本引入对齐训练，反而放大幻觉模式。作者从数据生成机制出发，证明按策略数据更优，但需要高效、可靠的偏好标注机制。

Method: 1) 分析并实证对比on-policy与off-policy数据，确立on-policy优越性；2) 训练一个幻觉二分类器，对模型生成(on-policy)的候选响应进行二元偏好标注，保证“选择样本”干净；3) 提出鲁棒迭代DPO：在多轮生成-标注-对齐中引入动态样本重加权，抑制噪声、聚焦难例，持续提升对齐质量。

Result: 在三个基准上优于8个SOTA：对LLaVA-1.5-7B在MMHalBench上幻觉率降低50.8%，在Object HalBench上平均降低79.5%；LLaVA-1.5-13B经该方法可超越GPT-4V表现。

Conclusion: 通过干净的按策略偏好数据与鲁棒迭代DPO（动态重加权），可系统性降低LVLM幻觉并充分挖掘开源模型潜力，提供了一条低成本高效的对齐路径。

Abstract: Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.

</details>


### [84] [Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks](https://arxiv.org/abs/2512.00714)
*Emmanuella Avwerosuoghene Oghenekaro*

Main category: cs.CV

TL;DR: 该摘要讨论利用深度学习与多模态医学影像及放射基因组学融合，实现无创、早期、个体化肿瘤检测与表型/基因型预测的前沿进展。


<details>
  <summary>Details</summary>
Motivation: 早期癌症发现困难且延误诊断显著降低生存率，传统影像判读对微小异常敏感性有限，需要更强的自动化方法提升检测与分型能力并减少侵入性操作。

Method: 采用深度学习计算机视觉模型（CNN、Transformer、混合注意力）从MRI、CT、PET、乳腺X线、病理、超声等多模态影像中自动提取复杂空间、形态与时序特征，并与基因组、转录组、表观遗传等组学数据进行放射基因组学融合，以预测肿瘤分子与免疫特征。

Result: 深度学习模型可超越传统放射科评估，发现人眼难以识别的细微异常与肿瘤微环境变化；多模态影像与组学融合能无创预测肿瘤基因型、免疫反应、分子亚型及耐药性。

Conclusion: 深度学习驱动的多模态影像分析与放射基因组学融合为个体化肿瘤学开辟新范式，有望提升早筛、精准分型与疗效预测，减少侵入性活检。

Abstract: Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.

</details>


### [85] [RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images](https://arxiv.org/abs/2512.00718)
*Deliang Wang,Peng Liu*

Main category: cs.CV

TL;DR: 提出RS-ISRefiner：一种面向遥感图像的点击式交互分割框架，通过适配器微调、混合注意力与改进概率图调制，提升跨数据集的精度、效率与交互成本表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有交互式图像分割方法主要针对自然场景，面对遥感图像的尺度多样、边界不规则与背景复杂时泛化差；同时遥感标注稀缺、计算开销大，亟需高效可泛化的IIS方案。

Method: 1) 适配器式微调：在视觉基础模型上插入轻量适配器，保留通用表征并高效学习遥感特性（空间与边界）。2) 混合注意力：结合卷积的局部建模与Transformer的全局推理，增强对尺度多样与复杂场景的鲁棒性。3) 改进的概率图调制：更有效地融合历史用户点击/交互信息，稳定迭代细化、提升边界保真度。

Result: 在iSAID、ISPRS Potsdam、SandBar、NWPU、LoveDA Urban、WHUBuilding六个遥感数据集上，RS-ISRefiner在分割精度、推理效率与交互成本上均超过最新方法，表现稳定且泛化强。

Conclusion: RS-ISRefiner能在遥感场景下实现高质量、低成本的交互式实例分割；适配器+混合注意力+概率图调制的组合有效缓解数据稀缺与尺度/边界复杂问题，具有良好的实用性与可扩展性。

Abstract: Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.

</details>


### [86] [TrajDiff: End-to-end Autonomous Driving without Perception Annotation](https://arxiv.org/abs/2512.00723)
*Xingtai Gui,Jianbo Zhao,Wencheng Han,Jikai Wang,Jiahao Gong,Feiyang Tan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: TrajDiff 提出一种无需感知标注的端到端自动驾驶生成式框架：以轨迹为中心，用BEV热图作为条件，利用扩散模型直接生成多样且可行的未来轨迹，在NAVSIM上达到了SOTA的无标注表现（PDMS 87.5，扩展数据到88.5）。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶常依赖辅助感知任务与昂贵的人工标注。为了降低成本并避免感知管线的依赖，需要一种在无感知标注下也能学到有效规划表征并生成高质量轨迹的方法，同时还能受益于数据规模扩展。

Method: 1) 用未来轨迹构造高斯BEV热图目标，作为无标注的“自监督”信号；2) 设计轨迹导向的BEV编码器（TrajBEV encoder），从原始传感器输入直接提取TrajBEV特征；3) 提出Traj-oriented BEV Diffusion Transformer（TB-DiT），结合自车状态与TrajBEV特征，通过扩散生成模型直接生成多样、可信的未来轨迹，摆脱手工运动先验；4) 系统在无感知标注设定下进行训练，并研究数据扩展的收益。

Result: 在NAVSIM基准上达到87.5 PDMS，刷新无标注方法SOTA；进一步扩展数据后达88.5 PDMS，性能接近依赖感知的先进方法。

Conclusion: 基于轨迹导向BEV条件的扩散生成框架可在无感知标注的端到端驾驶中有效学习与规划，具有多样性与可行性，同时能从数据规模扩展中获益，性能接近感知驱动的强方法。

Abstract: End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.

</details>


### [87] [Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards](https://arxiv.org/abs/2512.00743)
*Qiang Lyu,Zicong Chen,Chongxiao Wang,Haolin Shi,Shibo Gao,Ran Piao,Youwei Zeng,Jianlou Si,Fei Ding,Jing Li,Chun Pong Lau,Weiqiang Wang*

Main category: cs.CV

TL;DR: Multi-GRPO通过“时间分组+奖励分组”两种正交分组机制改进GRPO在T2I对齐中的信任分配与多目标混合问题，利用树式轨迹精细评估早期去噪步骤贡献，并为各奖励独立估计优势后再聚合，在PickScore-25k与OCR-Color-10上取得更稳更优的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO在T2I模型对齐中存在两大痛点：1) 共享信任分配，将终端稀疏奖励经组归一后平均施加到所有时间步，导致早期去噪步骤（探索空间大、影响深）被误估；2) 多奖励混合时需预设权重且不同奖励尺度/方差不匹配，易引发梯度不稳定与冲突更新。需要一种方法既能精确早期步骤的优势估计，又能稳健处理多目标奖励。

Method: 提出Multi-GRPO：
- 时间分组（temporal grouping）：受MCTS启发，在早期若干去噪步进行分叉，形成树式轨迹；以共享前缀摊销采样成本，并用后代叶节点的回报为早期步估计更准确的优势。
- 奖励分组（reward-based grouping）：对每个目标奖励（如文本准确性、图像质量、文字颜色）独立计算优势，避免尺度/方差不匹配造成的梯度冲突，再进行聚合更新。并构建OCR-Color-10数据集评测多目标对齐。

Result: 在单目标PickScore-25k与多目标OCR-Color-10上，Multi-GRPO表现出更好的训练稳定性和对齐性能，能有效平衡冲突目标。

Conclusion: 通过正交的时间与奖励分组，Multi-GRPO显著改进GRPO的信用分配与多目标优化，提升T2I模型对齐的稳定性与效果，并提供新数据集促进多目标评测。

Abstract: Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \emph{reward-based grouping} to compute advantages for each reward function \textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \textit{PickScore-25k} and multi-objective \textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.

</details>


### [88] [Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression](https://arxiv.org/abs/2512.00744)
*Zhengxin Chen,Xiaohai He,Tingrong Zhang,Shuhua Xiong,Chao Ren*

Main category: cs.CV

TL;DR: 提出MGTPCN压缩框架：用先验引导卷积（PGConv）强化局部、高频与骨架特征，用多尺度门控Transformer（MGT）捕获非局部多尺度信息，并通过重参数化与门控在保证复杂度的同时提升率失真性能，整体超越SOTA与VVC。


<details>
  <summary>Details</summary>
Motivation: 学习式图像压缩优于传统编解码器（如VVC）主要得益于非线性变换编码，但常规卷积难以充分捕捉高频/骨架局部信息，Swin-T在非局部多尺度建模与非线性表达上亦存在不足，且复杂度需控制。

Method: 1) 提出PGConv：在卷积分支中加入非对称卷积（AConv）以强化结构/骨架元素，加入差分卷积（DConv）以提取高频细节；并通过重参数化将训练时多分支合并为推理时单分支以降复杂度。2) 提出MGT：在Swin-T基础上引入空洞窗口多头自注意力（不同膨胀率）与不同核大小的深度可分离卷积实现多尺度特征提取，并叠加门控机制增强非线性与选择性。3) 构建联合框架MGTPCN，将PGConv用于局部表征、MGT用于非局部表征，集成到端到端学习式压缩网络中。

Result: 在标准数据集上的实验显示MGTPCN在率失真权衡与复杂度方面优于现有SOTA，部分设置下超越VVC；具体指标未给出，但强调性能-复杂度更佳。

Conclusion: 通过在局部与非局部两端同时增强表征（PGConv+MGT）并采用重参数化与门控控制复杂度，可显著提升学习式图像压缩的效率与效果，证明联合多尺度与先验引导设计的有效性。

Abstract: Recently, learned image compression methods have made remarkable achievements, some of which have outperformed the traditional image codec VVC. The advantages of learned image compression methods over traditional image codecs can be largely attributed to their powerful nonlinear transform coding. Convolutional layers and shifted window transformer (Swin-T) blocks are the basic units of neural networks, and their representation capabilities play an important role in nonlinear transform coding. In this paper, to improve the ability of the vanilla convolution to extract local features, we propose a novel prior-guided convolution (PGConv), where asymmetric convolutions (AConvs) and difference convolutions (DConvs) are introduced to strengthen skeleton elements and extract high-frequency information, respectively. A re-parameterization strategy is also used to reduce the computational complexity of PGConv. Moreover, to improve the ability of the Swin-T block to extract non-local features, we propose a novel multi-scale gated transformer (MGT), where dilated window-based multi-head self-attention blocks with different dilation rates and depth-wise convolution layers with different kernel sizes are used to extract multi-scale features, and a gate mechanism is introduced to enhance non-linearity. Finally, we propose a novel joint Multi-scale Gated Transformer and Prior-guided Convolutional Network (MGTPCN) for learned image compression. Experimental results show that our MGTPCN surpasses state-of-the-art algorithms with a better trade-off between performance and complexity.

</details>


### [89] [Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization](https://arxiv.org/abs/2512.00748)
*Ke Liu,Shangde Gao,Yichao Fu,Shangqi Gao,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出ProSeg，用两个潜变量分别建模专家偏好与图像边界不确定，通过变分推断采样生成既多样又个性化的分割，并在NPC与LIDC-IDRI上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多评阅者医学分割存在数据不确定性：影像边界模糊、专家间标注差异。现有方法要么只追求多样性但缺少专家特异性，要么个性化却近似复制单个标注，难以兼顾“多样化+个性化”。

Method: 提出ProSeg：构建概率生成模型，引入两个潜变量，分别刻画（1）专家标注偏好（personalization），（2）图像边界模糊/不确定性（diversification）。采用变分推断学习潜变量的条件分布，推理时从两者分布中采样以生成分割。

Result: 在鼻咽癌（NPC）与肺结节（LIDC-IDRI）数据集上实现新的SOTA；能产生既多样又与特定专家一致的分割结果。

Conclusion: 通过显式分解专家偏好与边界不确定性，ProSeg在多评阅者分割中兼顾多样性与个性化，优于现有方法，并具备可采样生成能力与实用潜力。

Abstract: Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.

</details>


### [90] [Charts Are Not Images: On the Challenges of Scientific Chart Editing](https://arxiv.org/abs/2512.00752)
*Shawn Li,Ryan Rossi,Sungchul Kim,Sunav Choudhary,Franck Dernoncourt,Puneet Mathur,Zhengzhong Tu,Yue Zhao*

Main category: cs.CV

TL;DR: 提出FigEdit大型科学图表编辑基准（3万+样本），覆盖10类图表与多种复杂编辑任务，显示现有生成模型在结构化编辑上表现不佳，并指出传统像素级指标难以衡量语义正确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散/自回归生成模型在自然图像编辑上强，但默认图表也是像素阵列的假设错误。图表承载结构化数据与图形语法，编辑本质是结构化变换而非像素操作；缺乏针对这一特点的系统基准与评价体系。

Method: 构建FigEdit数据集与基准：来源于真实数据，覆盖10种图表类型；设计5类递进任务（单步编辑、多步编辑、对话式编辑、视觉引导编辑、风格迁移）；定义复杂编辑指令词表；对多种SOTA模型进行统一评测；对评价指标（如SSIM/PSNR）与语义正确性的一致性进行分析。

Result: 各类现有生成/编辑模型在FigEdit上整体表现不佳，常无法执行所需的结构化变换，产生语义不一致或无效编辑；像素级相似度指标与编辑语义正确性相关性弱，难以反映有效图表编辑质量。

Conclusion: 图表编辑需要结构感知的方法而非像素级操作；FigEdit为开发与评测结构化感知模型提供了标准基准与公平比较平台，呼吁未来工作在视觉外形与语义/数据层面联合建模与评价。

Abstract: Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.

</details>


### [91] [Seeing the Wind from a Falling Leaf](https://arxiv.org/abs/2512.00762)
*Zhiyuan Gao,Jiageng Mao,Hong-Xing Yu,Haozhe Lou,Emily Yue-Ting Jia,Jernej Barbic,Jiajun Wu,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出一个端到端可微的逆图形框架，从视频中恢复“看不见”的力场（如风场），通过回传从物体运动估计驱动力，并在合成与真实数据上验证，支持基于物理的视频生成与编辑。


<details>
  <summary>Details</summary>
Motivation: 视觉界长期能观测运动，却难以还原导致运动与形变的隐含物理交互（力）。希望仅凭视频像素推断作用力，如从落叶轨迹估计风场，从而让视觉更贴近物理因果。

Method: 构建可微物理+几何的逆图形管线：联合建模物体几何、物理属性与交互；将力场作为可学习表示，借助可微渲染/物理仿真把力→运动→像素的过程端到端连接，通过反向传播以视频重建误差驱动学习力场与属性。

Result: 在合成与真实场景上能从视频中重建合理的力场，呈现与观察到的物体运动一致的风/力分布；展示了物理一致的视频生成与编辑示例。

Conclusion: 端到端可微逆图形能从视频反推隐含力，弥合视觉与物理之间的鸿沟，并为物理驱动的视频生成与编辑提供可行路径。

Abstract: A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \href{https://chaoren2357.github.io/seeingthewind/}{project page}.

</details>


### [92] [The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches](https://arxiv.org/abs/2512.00765)
*Haojie Jia,Te Hu,Haowen Li,Long Jin,Chongshi Xin,Yuchi Yao,Jiarui Xiao*

Main category: cs.CV

TL;DR: 论文提出TESP-Attack：一种面向交通标志分类的“边缘贴合、频域与颜色纹理约束”的隐蔽型对抗补丁方法，在有限查询下跨多模型攻击成功率>90%，具备良好迁移性与实物稳健性。


<details>
  <summary>Details</summary>
Motivation: 物理对抗样本会导致自动驾驶误判，并在V2X网络中放大成级联故障。现有物理攻击多把扰动放在标志中央，显著且易被人眼察觉，缺乏隐蔽性与真实世界可用性。因此需要一种既有效又在人眼注意下不显眼的攻击方法。

Method: 1) 观察人类视觉更多聚焦标志中心，采用实例分割生成“边缘对齐”的掩膜，限制补丁位于标志边缘并匹配其形状；2) 用U-Net生成器产生补丁；3) 通过颜色与纹理一致性约束，以及频域分析（抑制突兀高频/匹配背景频谱）来优化补丁，使其与背景无缝融合；4) 在有限查询设定下对多种交通标志分类器实施黑箱攻击，并评估跨模型迁移与物理稳健性（不同角度/距离）。

Result: 在多种架构的交通标志分类模型上，在有限查询预算下攻击成功率超过90%，同时表现出强跨模型迁移能力。物理世界测试在不同视角与距离下保持稳定高效。

Conclusion: 边缘贴合掩膜+U-Net生成+颜色/纹理/频域联合约束可显著提升物理对抗补丁的隐蔽性，同时不牺牲攻击效果，具有跨模型与真实场景稳健性，对智能驾驶与V2X安全提出更高要求。

Abstract: Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.

</details>


### [93] [EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes](https://arxiv.org/abs/2512.00771)
*Xiaoshan Wu,Yifei Yu,Xiaoyang Lyu,Yihua Huang,Bo Wang,Baoheng Zhang,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: EAG3R将事件相机信息与点图(pointmap)重建结合，在动态与低光场景下显著提升三维几何估计的稳健性与精度。


<details>
  <summary>Details</summary>
Motivation: RGB-only点图重建（如DUSt3R/MonST3R）在动态物体、剧烈光照变化、低照度等真实场景中易失效；事件相机具备高时域分辨率与高动态范围，能弥补传统相机在这些条件下的局限。

Method: 在MonST3R骨干上扩展：(1) 视网膜Retinex风格的图像增强模块+轻量事件适配器，并通过SNR感知的特征融合机制，按局部置信度自适应融合RGB与事件特征；(2) 设计事件驱动的光度一致性损失，在全局优化阶段利用事件流的时空信息约束几何与外参的一致性；整体输出为点图回归，实现无位姿先验的高效重建。

Result: 在动态低光等挑战场景中，无需夜间数据再训练，EAG3R在单目深度、相机位姿跟踪与动态重建等任务上显著优于现有仅RGB方法。

Conclusion: 将事件相机与点图重建深度融合，并在训练/优化中利用事件一致性约束，可在真实复杂条件下获得更稳健的3D几何估计，推动无位姿重建在低光与动态环境中的实用化。

Abstract: Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.

</details>


### [94] [DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering](https://arxiv.org/abs/2512.00773)
*Toshiki Katsube,Taiga Fukuhara,Kenichiro Ando,Yusuke Mukuta,Kohei Uehara,Tatsuya Harada*

Main category: cs.CV

TL;DR: 提出DEJIMA管线与数据集，构建3.88M日文图文对的图文描述与VQA资源，质量和文化贴合度优于翻译/人工基线，并显著提升日文多模态基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有日文视觉-语言资源稀缺、规模小、质量参差，尤其缺乏兼具日本文化语境与可商用许可的大规模数据，限制了面向日本语境的多模态模型能力。

Method: 提出可扩展、可复现的数据构建管线：1) 大规模网页抓取与严格过滤/去重；2) 基于目标检测的证据抽取以保证图文对的视觉扎根；3) 在扎根约束下用大语言模型进行细化与重写；据此分别构建DEJIMA-Cap与DEJIMA-VQA，各含3.88M图文对。

Result: 人工评估显示，相比翻译或人工标注数据，DEJIMA在“日本性”和语言自然度显著更高，同时事实正确性与人工标注相当；图像特征分布分析显示覆盖日本典型多样视觉领域；在多项日文多模态基准上训练的模型稳定提升。

Conclusion: 文化扎根的大规模、可商用日文多模态数据至关重要；DEJIMA提供高质量、广覆盖的数据与开源元数据/管线，推动研究与产业应用。

Abstract: This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling.

</details>


### [95] [PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery](https://arxiv.org/abs/2512.00794)
*Bo Guo,Sijia Wen,Yifan Zhao,Jia Li,Zhiming Zheng*

Main category: cs.CV

TL;DR: PolarGS 将偏振信息（DoLP/AoLP）引入3D Gaussian Splatting重建，通过偏振引导的光度校正与偏振增强的高斯增密，解决反射与无纹理区域的光度歧义，显著提升几何精度，且与现有3DGS框架无关。


<details>
  <summary>Details</summary>
Motivation: 纯RGB驱动的3DGS在镜面反射、弱纹理等光度模糊场景中，光度一致性被破坏，几何估计不稳。偏振成像可从反射中揭示表面法向相关信息，天然补足光度线索，因而引入偏振以缓解歧义、提高重建质量。

Method: 提出PolarGS，两大模块：1）偏振引导的光度校正：利用线偏振度(DoLP)检测反射区域，并通过颜色细化图(Color Refinement Maps)对这些高斯的颜色进行校正，恢复光度一致性；2）偏振增强的高斯增密：在基于PatchMatch的深度补全中融合偏振角/度(AoLP/DoLP)，完成深度推断；将新高斯回投与融合以填补无纹理区域，完善几何。方法可无缝集成到不同3DGS框架。

Result: 在多个基准上较SOTA取得更优的几何精度与完整度，尤其在强反射与低纹理区域表现显著提升。

Conclusion: 偏振作为光学先验有效缓解3DGS的光度歧义问题；PolarGS通过偏振引导的光度校正与增密机制，稳健提升表观复杂场景中的重建质量，并具备框架无关性。

Abstract: Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.

</details>


### [96] [CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target](https://arxiv.org/abs/2512.00796)
*Jiajian He,Enjie Hu,Shiqi Chen,Tianchen Qiu,Huajun Feng,Zhihai Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 提出CircleFlow：通过流引导的边缘定位与隐式神经表示联合优化，实现高保真、物理一致的PSF估计，在仿真与实拍上达SOTA。


<details>
  <summary>Details</summary>
Motivation: PSF（点扩散函数）决定成像模糊，但从强度图像做反卷积既病态又多解；现有方法难以在空间可变与各向异性条件下精确、稳健地标定真实相机PSF，尤其受噪声、去马赛克与边缘定位误差影响。

Method: 设计“圈阵列”结构化拍摄以编码局部各向异、空间变化的PSF，并利用靶标二值先验解耦图像与核；用光流引导的亚像素对齐重建潜在清晰二值图；用能量约束的隐式神经表示建模PSF；在考虑去马赛克的可微框架中联合优化清晰图与PSF，依托精准边缘定位保证物理一致与稳健性。

Result: 在合成与真实数据上进行广泛实验，CircleFlow取得比现有方法更高的PSF估计精度与可靠性（SOTA），适用于实际标定。

Conclusion: 通过结构化拍摄+光流亚像素边缘定位+隐式神经PSF的联合可微优化，可稳健、物理一致地恢复空间可变、各向异的PSF，适合实际PSF校准场景。

Abstract: The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.

</details>


### [97] [Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding](https://arxiv.org/abs/2512.00805)
*Pengfei Hu,Meng Cao,Yingyao Wang,Yi Wang,Jiahua Dong,Jun Song,Yu Cheng,Bo Zheng,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出SpecTemp：一种用于长视频理解的推测式时间推理框架，采用轻量“草稿”MLLM选帧 + 强力“目标”MLLM验证与推理的双模型协作，并以强化学习训练；在保持准确度的同时显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有“thinking-with-frames”范式通过全局时序推理与局部帧检查提升视频MLLM的推理力，但随着上下文不断扩张，出现多模态冗余和效率瓶颈，难以在长时程视频中兼顾速度与准确度。

Method: 引入SpecTemp的“双模型+RL”设计：1) 轻量草稿MLLM在密集采样的时间段中快速探索并提出候选显著帧（粗到细的证据建议）；2) 强力目标MLLM专注时序推理并对草稿建议进行验证与迭代细化，直到注意力收敛；3) 构建SpecTemp-80K数据集，提供粗粒度证据跨度与细粒度帧级证据的同步双层标注，用于强化学习训练与监督。

Result: 在多项视频理解基准上，SpecTemp在保持与现有thinking-with-frames方法相当的准确率的同时，实现了显著的推理速度提升。

Conclusion: 通过将时间感知与推理解耦并采用协作式推测验证机制，SpecTemp在长视频理解中达成效率与准确性的平衡，验证了双模型与强化学习在多模态长时程推理中的有效性。

Abstract: Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.

</details>


### [98] [IRPO: Boosting Image Restoration via Post-training GRPO](https://arxiv.org/abs/2512.00814)
*Haoxuan Xu. Yi Liu,Boyuan Jiang,Jinlong Peng,Donghao Luo,Xiaobin Hu,Shuicheng Yan,Haoang Li*

Main category: cs.CV

TL;DR: 提出IRPO：面向低层视觉图像恢复的GRPO后训练范式，结合“挑选欠拟合样本”的数据构造与“三重奖励”(结构、感知、恢复)的奖励建模，在6个域内与5个OOD基准上取得SOTA，较AdaIR在域内+0.83 dB、OOD+3.43 dB。


<details>
  <summary>Details</summary>
Motivation: 传统IR依赖像素级监督，易过平滑、泛化差；而大模型后训练在高层任务成功，但低层视觉探索不足。作者动机是把后训练思想引入低层IR，通过更合适的数据选择与奖励设计提升主客观质量与泛化。

Method: 提出IRPO：一个基于GRPO的低层视觉后训练框架。1) 数据构造：从预训练阶段挑选表现欠佳的样本进行后训练，提高效率与收益。2) 奖励建模：构建三类互补奖励——General Reward衡量结构保真（客观结构一致性），Expert Reward利用Qwen-VL进行人类感知对齐（感知偏好），Restoration Reward针对具体低层任务的质量（任务特定指标）。结合GRPO进行策略优化。

Result: 在6个域内与5个OOD基准、多个退化类型上达到SOTA，相比AdaIR域内提升0.83 dB，OOD提升3.43 dB，显示更强的泛化与感知质量。

Conclusion: 针对低层视觉，后训练可通过“困难样本优先”的数据策略与多维奖励有效提升IR性能和泛化。IRPO验证了GRPO在低层任务上的可行性与优势，并在多基准上领先SOTA。

Abstract: Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.

</details>


### [99] [PanFlow: Decoupled Motion Control for Panoramic Video Generation](https://arxiv.org/abs/2512.00832)
*Cheng Zhang,Hanwen Liang,Donny Y. Chen,Qianyi Wu,Konstantinos N. Plataniotis,Camilo Cruz Gambardella,Jianfei Cai*

Main category: cs.CV

TL;DR: PanFlow是一种针对全景视频生成的模型，聚焦于显式运动控制与大幅度复杂运动的合成，显著提升运动逼真度、视觉质量与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有全景视频生成方法缺乏对相机运动（尤其是快速旋转）与场景物体运动的明确解耦，导致在大范围、复杂运动场景中出现失真、抖动与边界不连贯。此外，缺乏带姿态与流标注的大规模数据限制了模型的可控性与泛化能力。

Method: 1) 球面建模：利用全景视频的球面特性，将相机高动态旋转从输入光流条件中显式解耦，实现对大运动的更精细控制；2) 球面噪声扭曲（spherical noise warping）：在球面坐标上进行噪声场的扭曲与对齐，强化视频在经度边界处的环路一致性；3) 数据集构建：整理大规模、运动丰富的全景视频数据，提供逐帧姿态与光流标注；4) 应用演示：在运动迁移与视频编辑任务上验证可控性与通用性。

Result: 在多个基准与应用上，PanFlow在运动保真、视觉质量和时间一致性上显著优于现有方法；能够稳健处理大幅度、复杂运动并保持全景边界的循环一致性。

Conclusion: 通过球面视角下的相机旋转-光流解耦与球面噪声扭曲，PanFlow实现对全景视频中大运动的精细可控生成，并在公开代码、数据与模型支持下，展示出良好的实用性与可拓展性。

Abstract: Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.

</details>


### [100] [AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent](https://arxiv.org/abs/2512.00846)
*Neeraj Anand,Rishabh Jain,Sohan Patnaik,Balaji Krishnamurthy,Mausoom Sarkar*

Main category: cs.CV

TL;DR: 提出AFRAgent：一种基于InstructBLIP的轻量多模态代理，通过自适应特征重归一化增强视觉嵌入，在GUI自动化任务上达SOTA，体积不到现有最强模型的四分之一。


<details>
  <summary>Details</summary>
Motivation: 移动UI自动化需求上涨；VLM可直接理解屏幕并用人类动作执行，但现有方法空间信息不足、控件识别与动作决策不准，且模型庞大训练与推理成本高。

Method: 构建基于InstructBLIP的多模态架构AFRAgent；在LLM流水线中对图像嵌入施加自适应特征重归一化（token级仿射变换），以丰富低分辨率嵌入并融合高分辨率细节，从而提升控件定位与动作判断。

Result: 在Meta-GUI和AITW基准上取得新SOTA，且模型大小小于最近竞争者的四分之一，推理更高效。

Conclusion: AFRAgent在保持小模型规模的同时，显著提升GUI自动化的识别与执行能力，为智能手机自动化设定新的基线。

Abstract: There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.

</details>


### [101] [Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting](https://arxiv.org/abs/2512.00850)
*Haishan Wang,Mohammad Hassan Vali,Arno Solin*

Main category: cs.CV

TL;DR: Smol-GS 提出一种让3D Gaussian Splatting 以极小表示进行高效压缩且保持渲染质量的方法。核心是用递归体素层级编码空间坐标，并以“每个高斯斑点的特征”存储颜色/不透明度/变换/材质等语义线索，实现数量级压缩同时维持SOTA质量，并为导航、规划等下游任务提供离散表示基础。


<details>
  <summary>Details</summary>
Motivation: 3DGS 渲染质量高但参数量大、存储与传输成本高、难以作为通用离散表征用于下游任务；需要在不牺牲视觉保真度的前提下显著压缩与结构化表示。

Method: 构建递归体素层级（hierarchical voxel）来紧凑编码每个高斯斑点的空间位置；为每个斑点学习特征向量，涵盖颜色、不透明度、几何变换、材质等抽象属性；在训练中联合优化层级坐标编码与斑点特征，使其在保持渲染质量的同时实现大幅压缩。

Result: 在标准基准上达到当前最优的压缩率（数量级压缩），同时维持高保真渲染质量；与现有方法相比，兼顾存储效率与灵活性。

Conclusion: Smol-GS 提供一种将空间与语义紧密耦合的离散、紧凑3D表示，既能进行高质量渲染，也可作为导航、规划与3D理解等下游任务的基础表征。

Abstract: We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.

</details>


### [102] [TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models](https://arxiv.org/abs/2512.00872)
*Tim Veenboer,George Yiasemis,Eric Marcus,Vivien Van Veldhuizen,Cees G. M. Snoek,Jonas Teuwen,Kevin B. W. Groot Lipman*

Main category: cs.CV

TL;DR: 提出TAP-CT：把ViT与DINOv2适配为体数据的自监督3D CT基础模型，几乎无需微调即可作为强通用编码器；在10.5万例CT上预训练，冻结特征对多下游任务泛化强，将开源模型与代码。


<details>
  <summary>Details</summary>
Motivation: 医学影像的许多基础模型需要大量微调或依赖昂贵的解码器，且现有编码器预训练目标偏向特定任务，缺少一个任务无关、微调开销小的强大通用3D CT基础模型。

Method: 将2D ViT与DINOv2简洁地扩展到3D体数据：对patch embedding、位置编码和体素级数据增强做针对性修改，使网络具备深度感知；在大规模院内3D CT数据（10.5万体积）上进行可扩展的自监督预训练，得到可冻结的表征。

Result: 大规模3D自监督预训练产生稳定、鲁棒的冻结特征，在多种下游任务上具有强泛化能力，且几乎无需超出特征提取的微调；训练和架构保持简单。

Conclusion: TAP-CT提供了一个任务无关、低资源需求且强泛化的3D CT基础模型与基线；将公开预训练权重、实验配置和下游基准代码以促进透明与复现。

Abstract: Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.

</details>


### [103] [Neural Discrete Representation Learning for Sparse-View CBCT Reconstruction: From Algorithm Design to Prospective Multicenter Clinical Evaluation](https://arxiv.org/abs/2512.00873)
*Haoshen Wang,Lei Chen,Wei-Hua Zhang,Linxia Wu,Yong Luo,Zengmao Wang,Yuan Xiong,Chengcheng Zhu,Wenjuan Tang,Xueyi Zhang,Wei Zhou,Xuhua Duan,Lefei Zhang,Gao-Jun Teng,Bo Du,Huangxuan Zhao*

Main category: cs.CV

TL;DR: 提出DeepPriorCBCT三阶段深度学习重建框架，在仅用约1/6常规剂量的稀疏采样下实现诊断级CBCT图像；在多中心大样本回顾数据与前瞻性交叉试验中显示与标准重建在图像质量与诊断性能上等效，同时显著降低辐射暴露。


<details>
  <summary>Details</summary>
Motivation: CBCT引导穿刺在胸部肿瘤诊疗中常用，但常规剂量的辐射提高继发肿瘤风险。现有低剂量策略缺乏大规模多中心验证与前瞻性临床评估，临床落地性不足。

Method: 构建DeepPriorCBCT三阶段深度学习框架，在稀疏投影（约1/6剂量）条件下进行诊断级重建。使用来自12家中心的4102名患者、8675次CBCT扫描进行开发与验证，并开展注册的前瞻性交叉试验（NCT07035977，138例）评估临床可用性。由多名影像科医师与介入医师进行图像质量、诊断一致性与手术指导偏好评价。

Result: 多中心回顾性评估中，11位医师认为模型重建与原始全采样图像不可区分，诊断性能与图像质量与标准算法相当。前瞻性试验中，5位放射科医师在图像质量与病灶评估上与临床标准无显著差异（均P>0.05）；25位介入医师在术中指导对两者无偏好（Kappa<0.2）。辐射剂量约降至常规的1/6。

Conclusion: DeepPriorCBCT在显著降低辐射暴露的同时，仍可在稀疏采样下实现与临床标准相当的诊断级CBCT重建，具备多中心验证与前瞻性临床可行性，支撑其在胸部介入中的临床推广。

Abstract: Cone beam computed tomography (CBCT)-guided puncture has become an established approach for diagnosing and treating early- to mid-stage thoracic tumours, yet the associated radiation exposure substantially elevates the risk of secondary malignancies. Although multiple low-dose CBCT strategies have been introduced, none have undergone validation using large-scale multicenter retrospective datasets, and prospective clinical evaluation remains lacking. Here, we propose DeepPriorCBCT - a three-stage deep learning framework that achieves diagnostic-grade reconstruction using only one-sixth of the conventional radiation dose. 4102 patients with 8675 CBCT scans from 12 centers were included to develop and validate DeepPriorCBCT. Additionally, a prospective cross-over trial (Registry number: NCT07035977) which recruited 138 patients scheduled for percutaneous thoracic puncture was conducted to assess the model's clinical applicability. Assessment by 11 physicians confirmed that reconstructed images were indistinguishable from original scans. Moreover, diagnostic performance and overall image quality were comparable to those generated by standard reconstruction algorithms. In the prospective trial, five radiologists reported no significant differences in image quality or lesion assessment between DeepPriorCBCT and the clinical standard (all P>0.05). Likewise, 25 interventionalists expressed no preference between model-based and full-sampling images for surgical guidance (Kappa<0.2). Radiation exposure with DeepPriorCBCT was reduced to approximately one-sixth of that with the conventional approach, and collectively, the findings confirm that it enables high-quality CBCT reconstruction under sparse sampling conditions while markedly decreasing intraoperative radiation risk.

</details>


### [104] [Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling](https://arxiv.org/abs/2512.00877)
*Zhening Liu,Rui Song,Yushi Huang,Yingdong Hu,Xinjie Zhang,Jiawei Shao,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: 提出一种面向3D Gaussian Splatting(3DGS)的通用前馈压缩框架，通过大规模上下文与注意力变换编码，显著提升长程相关建模，达到约20×压缩率并刷新通用编解码SOTA。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽在表示质量与渲染效率上表现突出，但原始高冗余与巨大存储使其难以部署与传播。现有前馈压缩器受限于：1) 变换编码网络感受野有限，难以捕获长距离空间相关；2) 熵模型上下文容量不足，无法充分利用跨高斯的统计相关性。因此需要一种既能高效前向推理、又能强建模长程依赖的通用压缩方法。

Method: - 构建“大规模上下文结构”：基于Morton(Z-order)序列化，选取上千个空间邻近高斯作为上下文。
- 细粒度“空间-通道”自回归熵模型：在空间与特征通道两个维度上自回归，充分利用大上下文以改进概率估计。
- 注意力式变换编码网络：通过跨广域邻域的注意力聚合，提取信息量高的潜先验，提高可压缩性与重建质量。
- 全流程为前向推理（无每场景再训练），面向通用场景的编解码。

Result: 在不需要逐场景训练的前馈推理下，实现约20×的3DGS压缩率；在通用型（generalizable）编解码对比中达到SOTA性能。

Conclusion: 通过大上下文+细粒度自回归熵建模+注意力变换编码，显著强化长程依赖捕获能力，解决3DGS通用前馈压缩的关键瓶颈，实现高压缩比与强泛化，可作为3DGS实际部署的有力方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.

</details>


### [105] [Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning](https://arxiv.org/abs/2512.00880)
*Haijian Shao,Wei Liu,Xing Deng*

Main category: cs.CV

TL;DR: 提出量子灵感的几何框架，用奇异值谱在Bloch超球上表征神经算子，建立光谱到功能的等价理论，并据此构建度量驱动的冗余图与一次性结构化剪枝；初步仿真优于基线，大规模多模态与国产异构硬件实证将留待期刊版。


<details>
  <summary>Details</summary>
Motivation: 多模态智能在资源受限、异构国产硬件上遇到三大瓶颈：模态特征异质、动态场景实时性需求、以及硬件特定算子冗余难以识别与替换，缺乏跨模态/跨架构的可替代性度量与系统化剪枝方法。

Method: 提出将神经算子以其归一化奇异值谱嵌入到Bloch超球，定义以Fubini–Study/Wasserstein-2为核心的光谱几何距离；证明“距离小则功能近”的严密等价定理；在此度量上构建量子度量驱动的功能冗余图（QM-FRG），并实施一次性结构化剪枝；用受控仿真与幅值/随机基线对比。

Result: 理论上给出紧致的光谱-功能等价；仿真显示所提度量在识别冗余与保持功能方面优于幅值与随机基线；大规模多模态Transformer与华为昇腾、寒武纪MLU、昆仑芯等硬件上的系统性实验尚未完成并将于扩展期刊版呈现。

Conclusion: 量子几何化的算子表征与度量为跨模态、跨架构算子可替代性提供了首个严格基础，并支持基于功能冗余的一次性结构化剪枝；初步结果积极，但需要在真实大规模与异构硬件上进一步验证。

Abstract: The rapid growth of multimodal intelligence on resource-constrained and heterogeneous domestic hardware exposes critical bottlenecks: multimodal feature heterogeneity, real-time requirements in dynamic scenarios, and hardware-specific operator redundancy. This work introduces a quantum-inspired geometric framework for neural operators that represents each operator by its normalized singular value spectrum on the Bloch hypersphere. We prove a tight spectral-to-functional equivalence theorem showing that vanishing Fubini--Study/Wasserstein-2 distance implies provable functional closeness, establishing the first rigorous foundation for cross-modal and cross-architecture operator substitutability. Based on this metric, we propose Quantum Metric-Driven Functional Redundancy Graphs (QM-FRG) and one-shot structured pruning. Controlled simulation validates the superiority of the proposed metric over magnitude and random baselines. An extensive experimental validation on large-scale multimodal transformers and domestic heterogeneous hardware (Huawei Ascend, Cambricon MLU, Kunlunxin) hardware is deferred to an extended journal version currently in preparation.

</details>


### [106] [Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints](https://arxiv.org/abs/2512.00882)
*Xisheng Feng*

Main category: cs.CV

TL;DR: 提出“看-背-答”三阶段、参数高效的VLM增强框架，通过自生成知识提示与冻结骨干，缓解精细农业等领域的推理驱动幻觉与模态鸿沟，在AgroBench上显著超越现有模型（如Qwen-VL、GPT-4o）。


<details>
  <summary>Details</summary>
Motivation: 在精细农业等专业场景中，VLM常被语言先验牵引而忽视视觉证据，出现“推理驱动式幻觉”。根因是视觉表征难以有效触发模型参数中已内化的细粒度专家知识（模态鸿沟）。亟需一种能将视觉线索转化为可控知识检索的机制，减少幻觉并激活领域知识。

Method: 提出“Look, Recite, Then Answer”框架并冻结主干参数：1）Look阶段产出客观视觉描述与候选集合；2）Recite阶段用轻量1.7B路由器把视觉线索转为针对性查询，触发候选相关的参数化知识；3）Answer阶段并行对齐视觉描述与背诵出的知识证据，选择一致性最高的标签；全流程参数高效、模块化、可控。

Result: 在AgroBench上达SOTA：除总体领先外，除草识别任务较Qwen-VL提升23.6%，并在无外部检索的前提下超过GPT-4o。

Conclusion: 通过把被动感知改造成主动、可控的知识检索，框架缩小模态鸿沟并抑制推理驱动幻觉，证明参数高效的分阶段设计在专业领域VLM中有效且可扩展。

Abstract: Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to "Reasoning-Driven Hallucination" where linguistic priors override visual perception. A key bottleneck is the "Modality Gap": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose "Look, Recite, Then Answer," a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.6% over Qwen-VL and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval

</details>


### [107] [HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics](https://arxiv.org/abs/2512.00885)
*Masatoshi Tateno,Gido Kato,Hirokatsu Kataoka,Yoichi Sato,Takuma Yagi*

Main category: cs.CV

TL;DR: 提出HanDyVQA：一个聚焦手-物体交互（HOI）细粒度时空理解的视频问答基准，涵盖操控与效果两方面，含11.1K多选QA与10.3K分割掩码；现有视频基础模型在该数据集上距离人类表现仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有HOI数据集/基准多偏重“操控动作”或“结果效果”的粗粒度标签，缺乏可刻画操控—效果时空耦合机制的细粒度问题设置与评估，尤其在部件级状态变化、空间关系与运动几何等方面无法深入检验模型。

Method: 构建HanDyVQA视频VQA基准，设计六类互补问题（Action、Process、Objects、Location、State Change、Object Parts），收集11.1K多选问答对，覆盖操控风格、手/物体运动、部件级状态变化等；并为Objects与Object Parts问题配套10.3K对象/部件分割掩码，用于对象/部件级推理与视频目标分割评测；同时评测多种最新视频基础模型。

Result: 在HanDyVQA上，当前最强模型（Gemini-2.5-Pro）平均准确率约73%，显著低于人类97%；误差集中于空间关系理解、运动理解与部件级几何推理。引入显式HOI相关线索到视觉特征可带来性能提升。

Conclusion: HanDyVQA为HOI细粒度时空理解提供了全面评测基准，揭示了现有视频基础模型在空间、运动与部件级几何理解上的明显不足；整合HOI显式线索是提升模型对HOI动态理解能力的有效方向。

Abstract: Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.

</details>


### [108] [Multilingual Training-Free Remote Sensing Image Captioning](https://arxiv.org/abs/2512.00887)
*Carlos Rebelo,Gil Rocha,João Daniel Silva,Bruno Martins*

Main category: cs.CV

TL;DR: 提出一种无需训练的多语种遥感图像描述方法：用经领域适配的SigLIP2检索相似样例与字幕，通过提示喂给LLM或VLM，并用PageRank图重排序提升一致性；在4个数据集10种语言上接近或优于英文监督基线，PageRank可带来最高35%的指标提升，直接生成目标语优于翻译流程。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像描述依赖大规模标注且多聚焦英语，限制全球适用性与可扩展性。需要一种低/零训练、能多语言泛化的方法，减少数据与标注成本，提升包容性与跨语言能力。

Method: 提出基于检索增强提示的训练免方法：1) 用领域适配的SigLIP2编码器从数据存储检索相关图像字幕与少样本示例；2) 两种推理范式：图像盲（多语LLM仅基于文本提示生成）与图像感知（VLM联合处理图像与提示）；3) 为提升检索内容连贯性，构建图像-字幕图并用PageRank进行重排序；4) 多语言直接生成目标语描述，比较与翻译策略；在四套基准、十种语言上评测。

Result: 在无需训练条件下，对比完全监督的英文系统仍具竞争力，并能泛化到多语言。PageRank重排序显著提升性能，最高提升约35%。VLM生成更具视觉落地但词汇多样；LLM在BLEU与CIDEr上更强。直接以目标语言生成稳定优于“先英后译”等策略。

Conclusion: 检索增强+图重排序能够在零训练条件下实现高质量、多语言的遥感图像描述，缩减标注依赖并提升跨语言可用性；VLM与LLM各具优势，建议直接生成目标语言。该方法为构建包容、可扩展的多模态地球观测系统迈出实用一步。

Abstract: Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.

</details>


### [109] [Accelerating Streaming Video Large Language Models via Hierarchical Token Compression](https://arxiv.org/abs/2512.00891)
*Yiyu Wang,Xuyang Liu,Xiyan Gui,Xinying Lin,Boxue Yang,Chenfei Liao,Tailai Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出STC框架，通过缓存与剪枝在ViT编码与LLM预填阶段压缩视频token，实现实时流式VideoLLM加速，同时几乎不损精度。


<details>
  <summary>Details</summary>
Motivation: 流式视频理解中，连续相邻帧高度相似，导致ViT对冗余帧反复编码，计算昂贵；进入LLM的视觉token序列也被“膨胀”，带来预填充阶段的时延与显存压力，需要一种在不明显牺牲精度下减冗、加速的通用方法。

Method: 提出可插拔的层级式Streaming Token Compression（STC），包含两类token级加速器：1) STC-Cacher：检测时间上相似的帧并缓存其特征，复用以减少ViT重复计算；2) STC-Pruner：在送入LLM前根据空间与时间相关性筛选保留最具信息量的视觉token，压缩序列长度，从而降低LLM预填成本。STC可无缝集成到现有流式VideoLLM流程中，同时优化ViT编码与LLM预填两阶段。

Result: 在4个基线流式VideoLLM与5个基准上优于其他压缩方法；在ReKV框架上保持最高达99%原始精度，同时将ViT编码时延降低24.5%，LLM预填时延降低45.3%。

Conclusion: STC在不显著牺牲性能的前提下，通过缓存与剪枝有效压缩流式视频token，显著降低端到端延迟与内存开销，可作为现有Streaming VideoLLM的通用加速插件。

Abstract: Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \textbf{S}treaming \textbf{T}oken \textbf{C}ompression (\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \textbf{99\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \textbf{24.5\%} and \textbf{45.3\%}.

</details>


### [110] [SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead](https://arxiv.org/abs/2512.00903)
*Chaojun Ni,Cheng Chen,Xiaofeng Wang,Zheng Zhu,Wenzhao Zheng,Boyuan Wang,Tianrun Chen,Guosheng Zhao,Haoyun Li,Zhehao Dong,Qiang Zhang,Yun Ye,Yang Wang,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: SwiftVLA用紧凑模型实现4D时空理解：用预训练4D视觉几何Transformer+时间缓存提取4D特征，引入Fusion Tokens与未来预测目标统一2D/4D表示，并以mask-and-reconstruct训练让推理时可去掉4D分支，性能接近甚至匹敌更大VLA，同时速度更快、内存更省。


<details>
  <summary>Details</summary>
Motivation: 现有基于大VLM的VLA性能强但参数庞大，不适合边缘部署；改用轻量VLM会牺牲时空/3D理解；引入3D/4D输入通常仍依赖大VLM融合，且时序建模不足。需要一种既轻量又具备4D理解与时序推理的架构。

Method: 1) 预训练4D视觉几何Transformer带时间缓存，从连续2D图像中提取4D(时空/几何)特征；2) 设计Fusion Tokens为可学习标记，通过未来预测目标训练，使VLM有效融合2D图像与4D特征，得到统一动作生成表示；3) mask-and-reconstruct训练：对送入VLM的4D输入做掩码并要求VLA重建，从而学得有效4D表征，并允许在推理时移除4D分支而损失极小。

Result: 在真实与模拟环境实验中，SwiftVLA优于轻量基线，并可匹敌参数量最高大7倍的VLA；在边缘设备上取得相当性能的同时，推理速度提升约18倍，内存占用降低约12倍。

Conclusion: SwiftVLA在保持设计紧凑高效的同时赋予模型4D理解与时序推理能力，通过融合与重建训练策略实现无需依赖大VLM的高效VLA，在资源受限场景具有实际部署价值。

Abstract: Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.

</details>


### [111] [Hierarchical Semantic Alignment for Image Clustering](https://arxiv.org/abs/2512.00904)
*Xingyu Zhu,Beier Zhu,Yunfan Li,Junfeng Fang,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: 提出一种无需训练的层次语义对齐方法（CAE）用于图像聚类，联合对齐名词概念与图像描述，通过最优传输提升语义判别性，在8个数据集上显著优于现有训练免方法，ImageNet-1K上ACC+4.2%、ARI+2.9%。


<details>
  <summary>Details</summary>
Motivation: 现有利用名词作为外部语义知识提升聚类的工作忽略了名词的语义歧义，导致语义表征偏差、聚类质量下降；需要一种方式兼顾高层类别概念与细粒度属性，缓解歧义并增强判别性。

Method: 构建层次语义空间并与图像特征对齐：1) 从WordNet筛选相关名词（高层概念）与从字幕数据集筛选描述（细粒度属性）；2) 通过最优传输分别将图像特征与所选名词、字幕对齐，获得更判别的语义嵌入；3) 融合增强后的语义特征与原图像特征进行聚类。全流程为training-free。

Result: 在8个数据集上验证，相比SOTA训练免方法显著提升；在ImageNet-1K上准确率提升4.2%，ARI提升2.9%。

Conclusion: 结合名词级与字幕级两种互补文本语义，并用最优传输进行层次对齐，可在无需训练的条件下显著提高图像聚类表现。方法通用、有效，适用于大规模数据集。

Abstract: Image clustering is a classic problem in computer vision, which categorizes images into different groups. Recent studies utilize nouns as external semantic knowledge to improve clus- tering performance. However, these methods often overlook the inherent ambiguity of nouns, which can distort semantic representations and degrade clustering quality. To address this issue, we propose a hierarChical semAntic alignmEnt method for image clustering, dubbed CAE, which improves cluster- ing performance in a training-free manner. In our approach, we incorporate two complementary types of textual seman- tics: caption-level descriptions, which convey fine-grained attributes of image content, and noun-level concepts, which represent high-level object categories. We first select relevant nouns from WordNet and descriptions from caption datasets to construct a semantic space aligned with image features. Then, we align image features with selected nouns and captions via optimal transport to obtain a more discriminative semantic space. Finally, we combine the enhanced semantic and image features to perform clustering. Extensive experiments across 8 datasets demonstrate the effectiveness of our method, notably surpassing the state-of-the-art training-free approach with a 4.2% improvement in accuracy and a 2.9% improvement in adjusted rand index (ARI) on the ImageNet-1K dataset.

</details>


### [112] [TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model](https://arxiv.org/abs/2512.00909)
*Alireza Javanmardi,Pragati Jaiswal,Tewodros Amberbir Habtegebrial,Christen Millerdurai,Shaoxiang Wang,Alain Pagani,Didier Stricker*

Main category: cs.CV

TL;DR: 提出TalkingPose：一种面向长时长、上半身人像动画的扩散模型框架，能从单张目标RGB图与驱动姿态序列生成高质量且时间一致的动画，并通过无额外训练或计算开销的反馈机制实现无限时长生成；并发布大规模数据集作为新基准。


<details>
  <summary>Details</summary>
Motivation: 现有角色动画扩散模型虽能从单图生成逼真动作，但受限于短视频训练与计算/显存瓶颈，难以保持长时序的连贯性与稳定性，尤其在表情与手部的细节一致性方面。

Method: 构建TalkingPose：以稳定扩散为骨干，将驱动帧中的面部与手部运动精确抽取并迁移到目标演员；引入基于图像扩散模型的反馈驱动机制，循环利用已生成帧以强化跨帧一致性，无需附加训练阶段或增加推理开销；支持从单张目标图与姿态序列生成长视频。并提供一个覆盖广泛的上半身人像动画大规模数据集作为评测基准。

Result: 在长时长生成中实现更高的时间一致性与细节保真，能够稳定地迁移面部与手部的细微动作，并支持理论上无限时长的动画生成；在新提出的大规模数据集及相关基准上表现优于现有短序列方法。

Conclusion: 通过反馈驱动的扩散框架，TalkingPose有效突破长序列生成的连贯性与资源限制难题，在无需额外训练的情况下实现上半身人像动画的持续稳定生成，并为领域提供新的评测数据集与基准。

Abstract: Recent advancements in diffusion models have significantly improved the realism and generalizability of character-driven animation, enabling the synthesis of high-quality motion from just a single RGB image and a set of driving poses. Nevertheless, generating temporally coherent long-form content remains challenging. Existing approaches are constrained by computational and memory limitations, as they are typically trained on short video segments, thus performing effectively only over limited frame lengths and hindering their potential for extended coherent generation. To address these constraints, we propose TalkingPose, a novel diffusion-based framework specifically designed for producing long-form, temporally consistent human upper-body animations. TalkingPose leverages driving frames to precisely capture expressive facial and hand movements, transferring these seamlessly to a target actor through a stable diffusion backbone. To ensure continuous motion and enhance temporal coherence, we introduce a feedback-driven mechanism built upon image-based diffusion models. Notably, this mechanism does not incur additional computational costs or require secondary training stages, enabling the generation of animations with unlimited duration. Additionally, we introduce a comprehensive, large-scale dataset to serve as a new benchmark for human upper-body animation.

</details>


### [113] [Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision](https://arxiv.org/abs/2512.00911)
*Yuhao Shan,Qianyi Yuan,Jingguo Liu,Shigang Li,Jianfeng Li,Tong Chen*

Main category: cs.CV

TL;DR: 提出一种双流角度感知生成网络，同时估计全景相机的倾斜角并重建正立全景图，结合CNN与ViT在不同投影域的特长，通过自适应融合与高频增强等设计，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在稀疏特征环境中依赖全景视觉，但机器人姿态不稳定导致全景图非正立，影响后续定位、建图与感知。基于IMU的校正会随时间漂移且受外界扰动，纯视觉的稳健替代方案需求强烈。

Method: 构建双流角度感知生成网络：1) CNN分支在等距柱状（equirectangular）投影上提取局部几何结构；2) ViT分支在立方体投影（cubemap）上建模全局上下文；3) 通过双投影自适应融合模块对齐并融合跨域空间特征；4) 设计高频增强块、环形填充与通道注意力以保持360°连续性与增强几何敏感性；5) 多任务联合学习同时进行倾角估计与正立全景重建。

Result: 在SUN360与M3D数据集上，倾角估计与正立全景生成均优于现有方法；消融实验显示各模块均有贡献且双任务具有协同增益。

Conclusion: 双投影双流与自适应融合可有效结合局部几何与全局上下文，提升全景图正立化与倾角估计的准确性，视觉方法可作为IMU方案的稳健替代。

Abstract: Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360° continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.

</details>


### [114] [ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices](https://arxiv.org/abs/2512.00912)
*Abdelghafour Halimi,Ali Alibrahim,Didier Barradas-Bautista,Ronell Sicat,Abdulkader M. Afifi*

Main category: cs.CV

TL;DR: 提出一套从3D微CT到2D切片的有孔虫自动分类深度学习流水线，基于严格的标本级划分与迁移学习，最终通过ConvNeXt-L与EffNetV2-S集成在12个物种上达95.64%测试准确率，并提供可交互部署仪表盘。


<details>
  <summary>Details</summary>
Motivation: 传统有孔虫形态鉴定依赖专家、效率低且主观性强；3D微CT数据丰富但缺乏标准化、可复现实验与可部署工具。需要在严格防泄漏前提下建立高质量数据集与基准，推动地学中AI的实用化。

Method: 从97个样本（27物种）构建数据集，选取样本量充足的12物种；以标本为单位进行数据划分，得到约10.96万张2D切片（训练/验证/测试分别为44,103/14,046/51,468）；对7种SOTA 2D CNN进行迁移学习评估；将ConvNeXt-L与EfficientNetV2-S做集成；并开发交互式仪表盘，支持实时切片分类与基于SSIM、NCC、Dice的3D切片匹配。

Result: 集成模型在测试集上Top-1准确率95.64%、Top-3准确率99.6%、AUC=0.998，优于单模型基线；建立了可复现实验流程与新基准。

Conclusion: 严格的数据划分与集成迁移学习能在2D切片层面高精度识别有孔虫物种；所提供的数据与工具为微体古生物学的AI应用奠定基准并可直接部署到实际工作流程。

Abstract: This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.

</details>


### [115] [LAHNet: Local Attentive Hashing Network for Point Cloud Registration](https://arxiv.org/abs/2512.00927)
*Wentao Qu,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: 提出LAHNet：在点云配准中结合局部注意力与哈希分窗，实现更大且合理的感受野与跨窗口交互，提升特征判别性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型点云描述子多偏重局部邻域，感受野不足限制了特征判别性与长程依赖建模，需要在保持局部归纳偏置的同时有效扩大、合理控制感受野。

Method: 1) 以局部注意力为核心的LAHNet，引入卷积式“局部性”归纳偏置；2) 通过Locality-Sensitive Hashing进行线性近邻搜索，将点云均匀划分为不重叠窗口（Group Transformer）以捕获合理的长程上下文；3) 采用高效跨窗口策略扩展感受野；4) 基于该分窗，提出Interaction Transformer：将每个窗口抽象为全局信号，计算点云对之间的重叠矩阵，增强重叠区域的特征交互与匹配。

Result: 在真实室内与室外基准上取得显著配准性能提升，学到更鲁棒且判别性强的特征；实验表明所提窗口化与交互机制有效。

Conclusion: 将局部注意力与LSH分窗/跨窗交互相结合，可在保持局部归纳偏置的同时扩大合理感受野，显著提升点云配准中的特征质量与注册效果。

Abstract: Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.

</details>


### [116] [SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding](https://arxiv.org/abs/2512.00936)
*Keita Otani,Tatsuya Harada*

Main category: cs.CV

TL;DR: 提出SceneProp：将场景图定位表述为MRF的MAP推断，用可微分的置信传播在整图上做全局推理，定位多对象与关系；在四个基准上显著超越现有方法，且随查询图复杂度提升而更准。


<details>
  <summary>Details</summary>
Motivation: 传统短语定位擅长单物体，但缺乏结构归纳偏置，难以解析多对象多关系的复杂描述；现有场景图定位方法本应受益于更多关系信息，却在图变大时性能下降，未能有效利用结构约束。

Method: 将场景图定位重构为MRF中的MAP推断：节点为对象候选区域，边编码关系兼容性；通过端到端、可微的Belief Propagation对整张查询图进行全局推理，寻找同时满足所有约束的最优区域赋值。实现为可训练框架，联合学习节点与边势能。

Result: 在四个数据集/基准上显著优于现有方法；与以往方法相反，随着查询图规模与复杂度增加，准确率持续提升，首次证明更多关系上下文带来更好定位。

Conclusion: 将场景图 grounding 视为MRF的全局推断，并用可微置信传播优化，能充分利用关系结构信息，解决复杂组合查询；方法稳健扩展到更大更复杂的图并取得SOTA表现。

Abstract: Grounding complex, compositional visual queries with multiple objects and relationships is a fundamental challenge for vision-language models. While standard phrase grounding methods excel at localizing single objects, they lack the structural inductive bias to parse intricate relational descriptions, often failing as queries become more descriptive. To address this structural deficit, we focus on scene-graph grounding, a powerful but less-explored formulation where the query is an explicit graph of objects and their relationships. However, existing methods for this task also struggle, paradoxically showing decreased performance as the query graph grows -- failing to leverage the very information that should make grounding easier. We introduce SceneProp, a novel method that resolves this issue by reformulating scene-graph grounding as a Maximum a Posteriori (MAP) inference problem in a Markov Random Field (MRF). By performing global inference over the entire query graph, SceneProp finds the optimal assignment of image regions to nodes that jointly satisfies all constraints. This is achieved within an end-to-end framework via a differentiable implementation of the Belief Propagation algorithm. Experiments on four benchmarks show that our dedicated focus on the scene-graph grounding formulation allows SceneProp to significantly outperform prior work. Critically, its accuracy consistently improves with the size and complexity of the query graph, demonstrating for the first time that more relational context can, and should, lead to better grounding. Codes are available at https://github.com/keitaotani/SceneProp.

</details>


### [117] [Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation](https://arxiv.org/abs/2512.00944)
*An Yang,Chenyu Liu,Jun Du,Jianqing Gao,Jia Pan,Jinshui Hu,Baocai Yin,Bing Yin,Cong Liu*

Main category: cs.CV

TL;DR: 提出一种针对3D Gaussian Splatting分割的高效方案：用二进制编码压缩类别表示、分解泛optic分割为多阶段子任务并在训练中微调不透明度，达到SOTA分割、显著降内存并加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D-GS的分割依赖高维类别特征，带来巨大内存开销；同时细粒度分割因标签空间拥挤和缺乏稳定的多粒度控制而困难；此外，照片测度渲染与语义分割不兼容导致前景-背景混淆。

Method: 1) 提出逐层粗到细的二进制编码：每个高斯的类别以二进制位表示并映射为单个整数，极大压缩类别特征内存；2) 渐进式训练：将全景分割分解为若干独立子任务，降低类间冲突，增强细粒度能力；3) 在分割训练中微调高斯不透明度，缓解渲染与语义目标的不一致、减少前景背景混淆。

Result: 在多个基准上达到SOTA分割性能，同时显著降低内存消耗并提升推理速度。

Conclusion: 通过二进制类别编码、渐进式子任务训练与不透明度微调，方法在保证或提升精度的同时大幅降低资源占用，证明3D-GS可作为高效的分割基础表示。

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.

</details>


### [118] [Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval](https://arxiv.org/abs/2512.00953)
*Haojian Huang,Kaijing Ma,Jin Chen,Haodong Chen,Zhou Wu,Xianghao Zang,Han Fang,Chao Ban,Hao Sun,Mulin Chen,Zhongjiang He*

Main category: cs.CV

TL;DR: 提出DEMR，用 evidential learning 改进视频片段检索中的不确定性估计与跨模态对齐，在标准与去偏数据集上显著提升精度、鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有时刻检索方法依赖预训练编码器，难以处理细粒度与确定性推理，导致文本-视频对齐在复杂/模糊片段上失效；直接用DER作基线又存在模态不平衡与不确定性正则结构不匹配，出现“难样本低不确定、易样本高不确定”的错配。

Method: 提出DEMR框架：1) 反思翻转融合（RFF）块进行跨模态对齐，缓解模态不平衡；2) 文本查询重建任务提升对文本细节的敏感度，降低不确定性偏置；3) 几何正则器（Geom-regularizer）重塑不确定性学习，使其自适应地对难片段给出更高不确定，改善检索决策。

Result: 在标准数据集及去偏数据集ActivityNet-CD、Charades-CD上，DEMR在有效性（检索精度）、鲁棒性（面对分布偏移/困难样本）与可解释性（不确定性与难度更一致）方面均显著优于现有方法。

Conclusion: 通过去偏的不确定性学习与跨模态对齐，DEMR实现对复杂视频时刻的自适应对齐与更可靠的不确定性估计，为时刻检索的时序-语义鲁棒性提供了有效方案，具备推广潜力；代码已开源。

Abstract: In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at https://github.com/KaijingOfficial/DEMR.

</details>


### [119] [Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction](https://arxiv.org/abs/2512.00960)
*Boran Wen,Ye Lu,Keyan Wan,Sirui Wang,Jiahong Zhou,Junxuan Liang,Xinpeng Liu,Bang Xiao,Dingbang Huang,Ruiyang Liu,Yong-Lu Li*

Main category: cs.CV

TL;DR: 提出4DHOISolver框架与Open4DHOI数据集，从网络单目视频中高效重建高时空一致、物理可行的人-物体4D交互；少量人工接触点标注作为关键约束，并用于驱动RL模仿学习。现有3D基础模型仍难以自动预测精确接触对应，凸显人类在环的必要性。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要从大规模多样的人-物交互中学习，但互联网上的单目视频虽多样、廉价，却难以稳定提取时空一致且物理合理的4D交互数据。现有自动方法在精确接触与交互恢复上欠缺，缺少可用的大规模4D HOI数据集与高质量重建流程。

Method: 提出4DHOISolver：以少量人类标注的接触点为约束，通过高效优化在单目视频上联合恢复人体与对象的4D姿态/形状/位姿，强调时空一致性与物理可行性。基于此流程构建Open4DHOI数据集（144类对象、103种动作）。并用恢复的轨迹驱动RL智能体进行模仿学习评估。

Result: 1) 得到可扩展、高质量的4D HOI重建与大规模数据集Open4DHOI；2) RL智能体能模仿重建的动作，验证重建的实用性；3) 基准评估显示现有3D基础模型难以自动预测精确人-物接触对应。

Conclusion: 人类在环的稀疏接触点标注是当前从野外单目视频获得可靠4D HOI的关键；所提框架与数据集为通用机器人学习提供有效数据来源，同时提出开放挑战：如何让3D基础模型自动、精确地推断人-物接触对应。

Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/

</details>


### [120] [MM-ACT: Learn from Multimodal Parallel Generation to Act](https://arxiv.org/abs/2512.00975)
*Haotian Liang,Xinyi Chen,Bin Wang,Mingkang Chen,Yitian Liu,Yuhao Zhang,Zanxin Chen,Tianshuo Yang,Yilun Chen,Jiangmiao Pang,Dong Liu,Xiaokang Yang,Yao Mu,Wenqi Shao,Ping Luo*

Main category: cs.CV

TL;DR: MM-ACT 是一个统一的视觉-语言-动作（VLA）模型，在共享的token空间内同时生成文本、图像和动作；通过并行解码与跨模态共享上下文训练，显著提升机器人任务规划与交互预测能力，并在仿真与真实机器人上取得高成功率。


<details>
  <summary>Details</summary>
Motivation: 通用型机器人策略需要既能理解语义以做任务规划，又能预测并执行与环境交互的动作。现有方法往往割裂或效率不足，缺乏在文本、图像、动作三模态上的统一生成与训练框架。

Method: 提出MM-ACT：1）统一的VLA架构，将文本、图像、动作映射到共享token空间，可在三模态上进行生成；2）解码策略：文本与图像采用re-mask并行解码以提升生成效率与质量，动作采用一步并行解码以高效输出动作序列；3）上下文共享的多模态学习（Context-Shared Multimodal Learning），在相同上下文下统一监督三模态生成，利用跨模态学习增强动作生成能力；4）在LIBERO仿真、Franka真实机器人与RoboTwin2.0双臂平台上进行评测，覆盖域内与域外泛化。

Result: 在LIBERO上成功率96.3%；在真实Franka三个任务上72.0%；在RoboTwin2.0的8个双臂任务上52.38%；跨模态学习带来额外9.25%的性能增益。

Conclusion: 统一的共享上下文VLA建模与并行解码策略能在三模态上高效生成，显著提升机器人语义理解与动作执行的协同能力，并在仿真与真实场景中取得强性能与泛化。

Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.

</details>


### [121] [PhotoFramer: Multi-modal Image Composition Instruction](https://arxiv.org/abs/2512.00993)
*Zhiyuan You,Ke Wang,He Zhang,Xin Cai,Jinjin Gu,Tianfan Xue,Chao Dong,Zhoutong Zhang*

Main category: cs.CV

TL;DR: 提出PhotoFramer：一个多模态构图指导框架，可对糟糕构图图像给出文字改进建议并生成改进示例图。通过“平移、缩放、视角变换”三级任务构建大规模训练数据，并微调能同时处理/生成文本与图像的模型，实现可执行的文字指引与图像示例，实验证明文本+示例优于仅示例。


<details>
  <summary>Details</summary>
Motivation: 大众拍照常见构图问题，现有工具多给示例或裁剪，缺少可操作的、可解释的指导与跨视角的构图建议；需一种能像人类拍照思考那样分解构图动作并提供文字+图像联动指导的系统。

Method: 提出分层任务：平移(shift)、缩放(zoom-in)、视角变换(view-change)。前两者由现有裁剪数据采样；视角变换通过两阶段：1) 从多视角数据集中采样不同视点对，训练退化模型将好构图图像退化为差构图；2) 用退化模型把专家照片合成差图，得到成对训练样本。用该数据微调能联合处理与生成文本和图像的多模态模型，从差图生成文字指导并输出改进示例图。

Result: 实验表明：文字指令能有效引导构图；文字与示例结合较仅示例基线有稳定提升。系统能对差构图图像输出可执行的文字建议并生成更佳构图示例。

Conclusion: PhotoFramer将专家摄影先验转化为日常可用的构图助手。分层任务与退化合成数据使模型学会从差到好的构图变换；文字+图像的联动输出在引导构图上更优，具备实用价值，并已开源代码、权重与数据集。

Abstract: Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.

</details>


### [122] [S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud](https://arxiv.org/abs/2512.00995)
*Han Su,Tianyu Huang,Zichen Wan,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: S2AM3D通过融合2D分割先验与3D一致性监督，提出一致性的点特征编码与可控尺度的解码框架，并配套10万级高质量部件级点云数据集，在多设置下取得领先、稳健且可控的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D部件级点云分割要么受限于3D数据稀缺、泛化差，要么直接引入2D预训练特征导致跨视角不一致，难以在复杂结构与尺度差异大的零部件上稳定工作。

Method: 1) 点一致性部件编码器：将多视角2D特征通过原生3D对比学习进行聚合，获得全局一致的点级特征；2) 尺度感知提示解码器：引入连续尺度信号作为prompt，实时调节分割粒度；3) 构建>10万样本的大规模高质量部件级点云数据集，为训练提供充足监督。

Result: 在多种评测设定上取得领先性能；在复杂结构与尺度变化显著的部件上展现出卓越鲁棒性与可控性。

Conclusion: 融合2D先验与3D一致性监督是可行且有效的路线；通过一致性特征学习与尺度可控解码，可以显著提升部件级点云分割的精度、稳健性与可控性，并受益于大规模高质量数据集的支持。

Abstract: Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.

</details>


### [123] [Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints](https://arxiv.org/abs/2512.00999)
*Mohsin Rasheed,Abdullah Al-Mamun*

Main category: cs.CV

TL;DR: 提出一种语义感知的医学图像重建框架，并结合轻量区块链溯源层，提升结构一致性、重建精度与可追溯性，增强诊断可信度与合规性。


<details>
  <summary>Details</summary>
Motivation: 现有重建方法过度追求像素级视觉逼真，可能牺牲解剖结构的真实性；同时临床数据存在噪声、损坏与篡改风险，AI 解释的可靠性与审计追踪不足，影响临床安全与监管合规。

Method: 在重建网络中引入高层语义潜在嵌入，与混合式 U-Net 架构融合以保留临床相关结构；叠加轻量化、基于无标度图设计的区块链溯源层，记录每次重建事件，实现可验证但低开销的来源追踪；在多数据集、多类型损坏场景下进行评测。

Result: 相较现有方法，在结构一致性（保持解剖形态）、重建准确度以及重建事件的可信溯源完整性方面均有提升。

Conclusion: 语义引导的重建与安全可追溯机制相结合，可提升医学影像 AI 的可靠性与合规性，增强临床诊断信心。

Abstract: Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.

</details>


### [124] [LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency](https://arxiv.org/abs/2512.01008)
*Zhongbin Guo,Jiahe Liu,Wenyu Gao,Yushan Li,Chengzhi Li,Ping Jian*

Main category: cs.CV

TL;DR: LISA-3D通过几何感知LoRA让语言指令分割跨视角一致，并把生成的掩码作为RGBA提示交给冻结的SAM-3D，零额外3D文本监督即可产出高质量3D（高斯点或网格），在ScanRefer/Nr3D显著提升准确率（最多+15.6），仅微调11.6M参数。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动3D重建需要既能理解开放词汇指令又能在多视角保持一致的掩码生成器，但单视图分割跨视角不一致、数据标注昂贵、端到端3D监督稀缺。

Method: 两阶段框架：1）将指令跟随分割模型LISA加入几何感知的LoRA并利用多视角RGB-D与已知相机位姿构建可微重投影一致性损失，无需额外3D-文本监督；2）把跨视角一致的语言掩码与RGB拼成RGBA，作为提示喂给冻结的SAM-3D重建器，直接输出高斯splat或贴图网格，无需重训。

Result: 在ScanRefer与Nr3D基准上，相比单视图基线，语言到3D定位/重建精度最高提升+15.6点，同时仅适配11.6M参数；模块化、数据高效，并能零样本泛化到未见类别。

Conclusion: LISA-3D提供了实用的语言引导3D内容创建方案：以低参数开销和无额外3D文本标注，实现跨视角一致的语言分割并无缝对接冻结的3D重建器，具备强泛化与易部署性。

Abstract: Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.

</details>


### [125] [Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model](https://arxiv.org/abs/2512.01030)
*Jing He,Haodong Li,Mingzhi Sheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: Lotus-2提出一种两阶段、确定性的致密几何预测框架，用少量数据充分利用扩散模型先验，在单目深度和法线估计上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单幅图像恢复逐像素几何本质上不适定；判别式回归依赖大规模标注且物理推理有限；扩散模型虽含强先验但随机生成范式不适合需要稳定、准确的几何推断。

Method: 两阶段确定性流程：1) 核心预测器：单步确定性推理，采用干净数据目标与轻量局部连续性模块（LCM），生成全局一致、无栅格伪影的结构；2) 细节锐化器：在核心预测器定义的流形内进行受约束的多步rectified-flow确定性细化，通过无噪声的flow matching增强细节。

Result: 仅用59K样本（<1%常见大规模数据）在单目深度估计上达新SOTA，并在表面法线预测上取得高度竞争的结果。

Conclusion: 扩散模型可作为确定性的世界先验，通过合适的适配协议，实现超越传统判别式与生成式范式的高质量几何推理。

Abstract: Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.

</details>


### [126] [TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models](https://arxiv.org/abs/2512.01048)
*Maya Varma,Jean-Benoit Delbrouck,Sophie Ostmeier,Akshay Chaudhari,Curtis Langlotz*

Main category: cs.CV

TL;DR: 提出TRoVe：用于自动发现时序视觉-语言模型（VLM）中的“静态特征偏置”并量化其对错误的影响与模型依赖度；在101个模型上验证并优于基线28.6%，还用于7个现成VLM与2个任务，揭示新偏置并在测试时提升性能。


<details>
  <summary>Details</summary>
Motivation: 时序理解需要捕捉随时间变化的动态信息，但VLM常走捷径，依赖背景/物体等静态特征，导致系统性错误。现实部署前需要发现并刻画这些可致错的静态偏置。现有方法缺乏自动、可量化地识别与评估偏置的工具与统一评测框架。

Method: 提出TRoVe：给定已训练VLM与带标注的验证集，(1)从数据中自动提取候选静态特征；(2)为每个特征打分：a)该特征对分类错误的影响；b)模型在预测时对该特征的依赖程度。为评估方法，构建包含101个时序VLM及其静态偏置真值标注的评测框架。随后将TRoVe应用于7个现成VLM和2个时序任务以发现未知偏置并做测试时改进。

Result: 在含101个模型的评测中，TRoVe能准确识别致错静态偏置，相比最接近的基线提升28.6%。在实际VLM与任务上，TRoVe挖掘出先前未知的静态偏置，并据此在测试时提升模型性能。

Conclusion: 静态特征偏置在时序VLM中普遍且会诱发错误。TRoVe提供了自动化、可量化的偏置发现与评估手段，并在大规模评测与真实模型上验证有效，揭示偏置后还能指导测试时改进，从而提升时序理解可靠性。

Abstract: Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.

</details>


### [127] [Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction](https://arxiv.org/abs/2512.01059)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CV

TL;DR: 论文研究在ViT-B/16上通过对MLP模块进行参数共享或减宽，减少约32.7%参数仍能小幅提升ImageNet-1K精度并显著改善训练稳定性；说明该配置处于过参数化，合理参数分配与约束可作为有效归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 缩放规律常暗示更大模型更好，但实证表明精度与训练行为不总是随规模单调提升。作者怀疑ViT-B/16在标准配方下MLP容量过大，想检验是否能在不增算力的前提下降低参数、提升稳定性与效率。

Method: 在ImageNet-1K上训练ViT-B/16，提出两种对MLP的降参策略：1) GroupedMLP：在相邻Transformer块之间共享MLP权重，不增计算量；2) ShallowMLP：将MLP隐层宽度减半，从而提高推理吞吐。两者各自移除32.7%参数，并对比基线的精度、吞吐与训练稳定性（峰值到最终精度退化）。

Result: GroupedMLP在相同算力下达81.47% top-1；ShallowMLP以更高吞吐达81.25% top-1；均优于86.6M参数基线的81.05%。训练稳定性显著提升，峰值到最终精度退化由0.47%降至0.03%–0.06%。

Conclusion: ViT-B/16在标准训练下处于过参数化，MLP容量可减且可能略提性能。参数共享与减宽等结构性约束提供有效归纳偏置，模型设计应重视参数分配而非一味增大规模。

Abstract: Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\% of the baseline parameters. Our \emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\% top-1 accuracy while maintaining the baseline computational cost. Our \emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\% top-1 accuracy with a 38\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\% to the range 0.03\% to 0.06\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.

</details>


### [128] [Generalized Medical Phrase Grounding](https://arxiv.org/abs/2512.01085)
*Wenjun Zhang,Shekhar S. Chandra,Aaron Nicolson*

Main category: cs.CV

TL;DR: 提出通用医学短语定位（GMPG）：每句可对应0/1/多图像区域，解决真实报告中的多区域、不可定位与非诊断文本；提出两阶段训练的MedGrounder，少量标注下在PadChest-GR与MS-CXR上优于REC式与生成式基线，并能与现有报告生成器无缝组合输出带定位的报告。


<details>
  <summary>Details</summary>
Motivation: 现实放射学报告常含多处病灶、否定、正常结构或叙述性文字，传统REC只返回单一框且默认短语必可定位，无法覆盖这些情况，影响可解释性与实用性。

Method: 任务重定义为GMPG：每句映射到0/1/多区域并带置信分数。提出MedGrounder，采用两阶段训练：1) 预训练在句子—解剖学框对齐数据上学习对齐；2) 在句子—人工框标注数据上微调。模型可与现有报告生成器后向组合，无需重训生成器。

Result: 在PadChest-GR与MS-CXR上，MedGrounder展现强零样本迁移能力；在多区域与不可定位短语场景下超越REC范式与带定位报告生成基线，同时需要更少人工框标注。

Conclusion: GMPG更贴近真实临床文本特性，MedGrounder以两阶段训练实现少标注、可零样本迁移的稳健定位，并可与现有报告生成器直接组合生成可解释（带定位）报告。

Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.

</details>


### [129] [Accelerating Inference of Masked Image Generators via Reinforcement Learning](https://arxiv.org/abs/2512.01094)
*Pranav Subbaraman,Shufan Li,Siyan Zhao,Aditya Grover*

Main category: cs.CV

TL;DR: 提出Speed-RL，用强化学习微调掩码生成模型，在更少采样步中生成高质量图像，实现约3倍加速且质量相当。


<details>
  <summary>Details</summary>
Motivation: MGM在图像生成质量高，但需要大量采样步导致推理缓慢；传统加速依赖蒸馏匹配教师分布，可能受限于教师多步过程，且难以同时优化速度与质量。

Method: 将加速视为强化学习问题：以图像质量奖励与速度奖励的加权和为目标，对预训练MGM进行微调，从而直接优化在少步数下的生成表现，而非逼近教师分布。

Result: 在大量实验中，Speed-RL将基础模型生成速度提升约3倍，同时保持与原模型可比的图像质量。

Conclusion: 通过将加速建模为RL优化并引入质量与速度联合奖励，可在显著减少采样步数的同时维持高保真度生成，为MGM推理加速提供有效范式。

Abstract: Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.

</details>


### [130] [CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions](https://arxiv.org/abs/2512.01095)
*Simon Kohaut,Daniel Ochs,Shun Zhang,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.CV

TL;DR: CycliST 是一个专为评估视频-语言模型在“周期性状态转变”文本推理能力而构建的新基准；结果显示现有VLM难以稳定识别并利用周期模式、缺乏时间理解与数量感知。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大量过程呈周期性（如线性/轨道运动、颜色/尺度随时间变化），而现有VLM是否具备对这类周期动态的时空认知与文本推理能力尚不明确，缺乏针对性的评测基准。

Method: 构建合成但结构丰富的视频数据集，包含具周期模式的物体运动与视觉属性变化；设计分层难度（循环物体数量、场景杂乱度、光照等可控因素）；用该基准对多款开源与商用最先进VLM进行系统评测，覆盖检测周期、时序理解、数量统计等任务。

Result: 所有现代表现不稳定：难以可靠检测/利用周期模式，对时间依赖关系理解不足，无法稳健抽取定量信息（如运动物体计数）；模型规模与架构与成绩相关性弱，无单一模型在所有任务上领先。

Conclusion: 当前VLM在周期性时空推理上存在显著技术缺口。CycliST提供了针对性挑战与全面评测框架，可推动未来模型在理解周期模式与时序认知方面超越现状。

Abstract: We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.

</details>


### [131] [Learning Eigenstructures of Unstructured Data Manifolds](https://arxiv.org/abs/2512.01103)
*Roy Velich,Arkadi Piven,David Bensaïd,Daniel Cremers,Thomas Dagès,Ron Kimmel*

Main category: cs.CV

TL;DR: 提出一种直接从非结构化数据学习谱基的框架，绕过算子选取、离散化与特征分解；通过最优近似视角训练网络分解隐式近似算子，同时恢复度量采样密度与特征值，在点云与高维图像流形上得到类似拉普拉斯的有意义谱基。


<details>
  <summary>Details</summary>
Motivation: 传统几何处理依赖手工选择并离散化如拉普拉斯之类的算子，再做特征分解；对非结构化、高维数据代价高、易受离散细节与网格/维度假设限制。需要一种无需显式构造算子、可扩展到任意维度与未网格化数据的谱方法。

Method: 基于最优逼近理论，定义一个隐式近似算子与一组探针函数分布；训练神经网络学习一组基，使得在该基下对隐式算子的重构误差最小化，相当于学习算子的“特征分解”。合适的探针分布下，该算子近似拉普拉斯。训练过程同时估计谱基、隐式度量的采样密度以及对应“特征值”。完全无监督，对数据流形不作网格或维度假设。

Result: 在3D表面点云与高维图像流形上，学到的谱基具有几何意义，往往与拉普拉斯特征基相似，而无需显式构造或离散拉普拉斯。方法统一恢复谱基、密度与特征值，显示良好的可扩展性与适用性。

Conclusion: 以学习替代传统“算子选择—构造—特征分解”管线，为非结构化与高维数据的几何处理提供数据驱动、可扩展的新范式，开启无需显式算子的谱分析与处理可能性。

Abstract: We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.

</details>


### [132] [Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis](https://arxiv.org/abs/2512.01116)
*Yilan Zhang,Li Nanbo,Changchun Yang,Jürgen Schmidhuber,Xin Gao*

Main category: cs.CV

TL;DR: 提出SlotSPE：用“槽”(slot)将病理图像与基因数据压缩为少量、可区分的结构化事件表征，以高效建模多模态内/跨模态交互；在10个癌症数据集上多数胜出并更可解释。


<details>
  <summary>Details</summary>
Motivation: 多模态生存预测受限于输入维度高、结构复杂以及关键预后事件稀疏、无标注且个体化，现有方法难以有效捕捉并利用这些决定结局的高层结构信号（如空间组织学模式、通路共激活）。

Method: 基于因子化编码思想，使用slot attention将每位患者的病理图像与基因表达分别压缩为紧凑、互相区分的“槽”集合，作为潜在预后事件的编码；在这些槽上建模复杂的模态内与跨模态交互，并支持纳入生物学先验以提升预后相关性。

Result: 在10个癌症基准上，方法在8个队列优于现有方法，整体提高约2.9%；在缺失基因组数据时仍稳健，并在结构化事件分解上显著提升可解释性。

Conclusion: 以槽为核心的结构化事件表示能高效、有效地整合病理与基因多模态信息，既提升生存预测性能与鲁棒性，也带来更好的生物学可解释性。

Abstract: The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.

</details>


### [133] [OmniFD: A Unified Model for Versatile Face Forgery Detection](https://arxiv.org/abs/2512.01128)
*Haotian Liu,Haoyu Chen,Chenhui Pan,You Hu,Guoying Zhao,Xiaobai Li*

Main category: cs.CV

TL;DR: OmniFD 提出一个统一模型，同时完成四项人脸伪造检测任务：图像分类、视频分类、空间定位、时间定位；以共享 Swin Transformer 编码器、跨任务交互模块、轻量解码头构成；多任务学习带来更高精度与效率，参数减63%、训练时长减50%，并在多基准上优于单任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为各任务独立建模，架构重复、计算冗余，且忽略任务间关联（如分类与定位的互补性），导致泛化与效率受限。需要一个统一、可扩展的框架，能在图像与视频、分类与定位之间共享表征并进行知识迁移。

Method: 提出 OmniFD：1) 共享的 Swin Transformer 编码器，从图像/视频提取统一的4D时空特征；2) 跨任务交互模块，使用可学习查询，通过注意力机制在任务间进行信息交互与依赖建模；3) 轻量级多头解码器，将细化后的表示分别用于图像/视频分类、空间与时间定位。整体以多任务学习联合训练。

Result: 在多项基准上优于任务专用模型；多任务带来互惠增益，例如引入图像数据使视频分类准确率提升4.63%；统一设计显著提升效率：参数减少约63%，训练时间减少约50%，并展现良好的可扩展性。

Conclusion: 统一的人脸伪造检测框架可在单一模型中高效解决图像/视频分类与时空定位，利用任务间知识迁移提升性能与泛化，并降低算力与训练成本，具备实际应用价值与可扩展性。

Abstract: Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.

</details>


### [134] [Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network](https://arxiv.org/abs/2512.01145)
*Riyadh Mohammed Almushrafy*

Main category: cs.CV

TL;DR: 提出仅用弱时间标签（起始/峰值/结束）来进行微表情连续强度估计的统一框架：用三角先验从稀疏标注生成稠密伪强度轨迹，并以ResNet18+双向GRU回归帧级强度；在SAMM与CASME II上与伪轨迹高度一致（Spearman≈0.90/0.91，Kendall≈0.80/0.82），优于逐帧基线；消融表明时序建模与结构化伪标签对捕捉上升-峰值-下降动态至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有微表情研究多做离散类别识别，较少建模强度随时间的连续演化；缺少帧级强度标注使得全监督回归不可行。因此需要一种仅依赖常见的稀疏时间标注（onset/apex/offset）即可学习连续强度的方案，降低标注成本并提升跨数据集的一致性。

Method: 1) 弱标注到伪标签：以简单三角形先验，将onset→apex→offset三点转为帧级稠密伪强度轨迹；2) 模型：轻量级时序回归器=ResNet18图像编码器+BiGRU时序模块，直接预测帧级强度；3) 统一预处理与时间对齐流水线，跨数据集一致应用；4) 训练时可选apex排序损失（但在CASME II上关闭更优）。

Result: 在SAMM上Spearman 0.9014、Kendall 0.7999，优于逐帧基线；在CASME II上最高Spearman 0.9116、Kendall 0.8168（不含apex排序项时）。与伪强度轨迹具有强时间一致性；消融显示时序建模和结构化伪标签显著提升对上升-峰值-回落动态的捕捉。

Conclusion: 首次提出仅用稀疏时间标注实现微表情连续强度估计的统一框架。三角先验+轻量时序回归在多数据集上有效，降低标注成本并捕捉细微时序动态；未来可探索更灵活的先验、真实强度标注或跨域/自监督以进一步提高鲁棒性。

Abstract: Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical.
  We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline.
  Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements.
  To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.

</details>


### [135] [SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models](https://arxiv.org/abs/2512.01148)
*Hamza Tahboub,Weiyan Shi,Gang Hua,Huaizu Jiang*

Main category: cs.CV

TL;DR: 论文指出现有VLM在多种社会感知任务上出现负迁移，原因是预训练导致视觉编码器的社会表示能力退化（“社会降解”）。作者通过线性探针与梯度冲突分析验证问题，并提出SocialFusion：冻结视觉编码器，仅用最小连接到语言模型，取得跨五个社交任务的正迁移与与SOTA相当的表现，提示需要更具社会意识的预训练范式。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM具备强大的通用能力，但在同时学习多种社会感知任务（如情绪、关系、互动意图等）时表现不佳，甚至相互干扰。作者动机是找出负迁移的根源，并构建能统一多任务且不损伤社会信息表征的框架。

Method: 1) 诊断：提出“社会降解”概念，并用两类分析验证：a) 线性探针评估社会信息可解码性；b) 梯度冲突分析评估任务兼容性。发现预训练过程显著削弱可解码性。2) 方案：SocialFusion——冻结视觉编码器，与LLM之间仅学习一个最小连接（轻量映射/适配器），在联合训练下实现跨社交任务的知识融合与正迁移。

Result: 在五个社会任务上实现全面正迁移，任务间互相增益；在多项基准上达到与任务特定SOTA相当的性能，优于现有VLM多任务设置。

Conclusion: 当前VLM的通用预训练可能损害社会能力表征，需采用更具社会意识的训练策略。SocialFusion作为轻量、统一的连接范式能缓解社会降解，促进多任务正迁移与高性能。

Abstract: Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term "social degradation," whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.

</details>


### [136] [DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling](https://arxiv.org/abs/2512.01153)
*Han-Jin Lee,Han-Ju Lee,Jin-Seong Kim,Seok-Hwan Choi*

Main category: cs.CV

TL;DR: 提出DPAC：在扩散模型对抗引导中将对抗梯度投影到生成分布等密度（score正交）切空间，以在保持攻击成功率的同时减少分布漂移，理论上等价于最小化路径KL（即控制能量），从而改进FID/2-W距离与离散求解误差阶；实验在ImageNet-100验证更低FID与更小估计path-KL。


<details>
  <summary>Details</summary>
Motivation: 对抗引导扩散常能达成目标类别，但由于受控轨迹与无控制轨迹偏离累积，样本质量下降。缺乏对这种退化的统一度量与与感知质量指标（如FID、W2）的理论联系与可控方法。

Method: 将受控与无控制扩散的差异形式化为路径空间KL（path-KL），利用Girsanov定理证明其等于控制能量，建立SOC视角；从变分最优性导出一阶条件：在同等分类收益的方向中，沿等密度（或等log密度）切向（与score正交）的分量最小化path-KL，而法向分量增加分布漂移；据此提出DPAC，将对抗梯度投影到由生成score几何定义的切空间；在离散求解器中分析误差，证明切向投影抵消O(Δt)主导误差，质量差达O(Δt^2)，并具二阶鲁棒性。

Result: 理论：最小化path-KL可同时收紧W2与FID上界，建立控制能量与感知保真度的联系；切向投影给出误差阶提升与鲁棒性。实验：在ImageNet-100上，DPAC在相同攻击成功率下获得更低FID与更小估计path-KL。

Conclusion: 通过SOC与变分分析揭示对抗控制能量与生成质量的内在耦合，提出的DPAC以几何切向投影在不牺牲攻击效果的前提下显著缓解质量退化，并在理论与实验上均优于未约束的对抗引导。

Abstract: Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fréchet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O(Δt) leading error term in the Wasserstein distance, achieving an O(Δt^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.

</details>


### [137] [Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation](https://arxiv.org/abs/2512.01165)
*Mohamed Abdallah Salem,Ahmed Harb Rabia*

Main category: cs.CV

TL;DR: 提出一种在边缘设备上基于YOLO的实时标注框架，相比传统离线人工标注显著缩短数据准备时间，同时保持较高标注质量；预训练与单类别配置在收敛、性能和鲁棒性上更优。


<details>
  <summary>Details</summary>
Motivation: 农业等需要快速决策的场景中，部署目标检测模型受制于数据集高效且准确的标注难题；传统标注耗时耗力，滞后于数据采集。

Method: 将YOLO(含v5、v8、v12)部署到边缘端，在图像采集时即时生成标签，实现“拍摄即标注”的实时标注流程；对单类/多类、预训练/从零训练多种配置进行对比，结合统计检验与学习动态分析。

Result: 预训练与单类别配置在收敛速度、性能指标与鲁棒性上显著占优；系统在保持高标注质量的同时，明显减少数据准备时间。

Conclusion: 实时边缘端YOLO标注是可行且有效的，可大幅提升数据集构建效率；在实践中优先采用预训练模型与单类别设定更易获得稳定、高性能的标注效果。

Abstract: Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.

</details>


### [138] [VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering](https://arxiv.org/abs/2512.01178)
*Zihua Liu,Hiroki Sakuma,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: VSRD++用弱监督替代3D标注：用体渲染+SDF多视角自动标注生成3D框，再用其训练单目3D检测器；在动态场景中引入速度和置信度建模，性能在KITTI-360上显著优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测对3D标注依赖强，标注成本高且通常依赖LiDAR。希望用更廉价的2D弱监督与体渲染从多视角自动生成高质量3D伪标注，以覆盖静态与动态场景。

Method: 两阶段：1) 多视角3D自动标注：以神经场体渲染为核心，用实例感知的剪影体渲染优化每个目标的SDF；将实例SDF分解为长方体SDF+残差距离场RDF，以便同时拟合规则盒与形状偏差；为解决动态物体体渲染中的几何不一致，引入带速度的3D框参数化，并为伪标注分配置信度；提供3D属性初始化模块以稳健初始化动态框。2) 单目检测训练：用优化后的3D框作为伪标注训练常规单目3D检测器。

Result: 在KITTI-360上，针对静态与动态场景均显著优于现有弱监督单目3D检测方法（定量细节未给出），展示了更高的检测精度与鲁棒性。

Conclusion: 弱监督的体渲染+SDF分解框架能在无3D真实标注下产生高质量伪标注，并有效训练单目3D检测器；对动态目标的速度建模与伪标注置信度进一步提升了稳定性与性能。

Abstract: Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus

</details>


### [139] [TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image](https://arxiv.org/abs/2512.01204)
*Ziqian Wang,Yonghao He,Licheng Yang,Wei Zou,Hongxuan Ma,Liu Liu,Wei Sui,Yuxin Guo,Hu Su*

Main category: cs.CV

TL;DR: 提出TabletopGen：无需训练、自动从参考图像生成可物理交互的3D桌面场景，利用分割与实例重建，并通过分阶段姿态与尺度对齐，实现高保真、布局准确、物理可行的场景生成，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本/图像的3D场景生成主要面向大规模室内/室外环境，难以应对桌面场景的高密度摆放与复杂空间关系，且缺乏物理交互与仿真就绪的要求。因此需要一种能从2D参考稳健恢复实例级几何与布局、并保证物理可行性的系统。

Method: 输入参考图（可由文生图生成）。1) 对参考图做实例分割与补全，得到每个物体的实例图。2) 将每个实例重建为3D模型并进行规范坐标对齐。3) 估计各实例的姿态与尺度并装配到桌面；核心是两阶段对齐：a) 可微旋转优化器（Differentiable Rotation Optimizer）精确恢复旋转；b) 俯视空间对齐（Top-view Spatial Alignment）进行鲁棒的平移与尺度估计。4) 进行碰撞检测，生成仿真可用、可交互的无碰撞场景。

Result: 在大量实验与用户研究中，TabletopGen在视觉保真度、布局准确性、物理可行性上显著优于现有方法，能生成风格与空间多样的逼真桌面场景。

Conclusion: TabletopGen提供了一个训练-free、端到端自动化流程，从2D参考稳健恢复实例级3D几何与布局，通过新颖的姿态与尺度对齐策略保证物理可行与仿真就绪，达到SOTA性能，并将开源代码。

Abstract: Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.

</details>


### [140] [Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations](https://arxiv.org/abs/2512.01213)
*Yangbangyan Jiang,Qianqian Xu,Huiyang Shao,Zhiyong Yang,Shilong Bao,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 论文针对部分AUC（PAUC）在受限FPR/TPR区间下的优化难题，提出两种简单的实例级极小极大重构：一种具有渐近零近似误差，另一种无偏但变量更多。通过阈值学习替代样本选择并配合平滑技术与高效求解器，实现线性迭代复杂度与O(ε^{-1/3})收敛；并给出紧致泛化界，明确展示α/β对泛化的影响为~O~(α^{-1}n_+^{-1}+β^{-1}n_-^{-1})；实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 现实场景往往存在类别不平衡与决策约束，只关心ROC曲线中特定FPR/TPR区间的性能，因而PAUC是关键指标。然而在该区间内选择样本的计算是NP难，现有近似优化方法要么近似误差不可控，要么规模化能力差。因此需要一种既可控又高效的PAUC优化框架。

Method: 将PAUC优化转化为两个“实例级”极小极大问题：1) 渐近无缝隙（approximation gap随样本量消失）的重构；2) 无偏但引入更多变量的重构。核心做法：先将原问题等价为实例级形式以降低时间复杂度；用阈值学习替代复杂的样本选择；再对目标进行不同的平滑处理。配合高效求解器，获得线性于样本数的每迭代复杂度，并证明典型单向/双向PAUC的收敛率为O(ε^{-1/3))。

Result: 提出的两类重构均能稳定高效地优化PAUC，具有线性每步复杂度与理论收敛保证；给出紧致的泛化界，清晰体现TPR/FPR约束α/β的影响，界为~O(α^{-1}n_+^{-1}+β^{-1}n_-^{-1})；在多项基准数据集上实验优于或不逊于现有方法。

Conclusion: 通过实例级极小极大重构与阈值学习+平滑技术，闭合了PAUC优化中的近似缺口，实现可扩展、收敛有保证、且具紧泛化界的优化方案；方法在实证上表现强劲，并揭示了约束参数α/β对泛化的量化影响。

Abstract: As a variant of the Area Under the ROC Curve (AUC), the partial AUC (PAUC) focuses on a specific range of false positive rate (FPR) and/or true positive rate (TPR) in the ROC curve. It is a pivotal evaluation metric in real-world scenarios with both class imbalance and decision constraints. However, selecting instances within these constrained intervals during its calculation is NP-hard, and thus typically requires approximation techniques for practical resolution. Despite the progress made in PAUC optimization over the last few years, most existing methods still suffer from uncontrollable approximation errors or a limited scalability when optimizing the approximate PAUC objectives. In this paper, we close the approximation gap of PAUC optimization by presenting two simple instance-wise minimax reformulations: one with an asymptotically vanishing gap, the other with the unbiasedness at the cost of more variables. Our key idea is to first establish an equivalent instance-wise problem to lower the time complexity, simplify the complicated sample selection procedure by threshold learning, and then apply different smoothing techniques. Equipped with an efficient solver, the resulting algorithms enjoy a linear per-iteration computational complexity w.r.t. the sample size and a convergence rate of $O(ε^{-1/3})$ for typical one-way and two-way PAUCs. Moreover, we provide a tight generalization bound of our minimax reformulations. The result explicitly demonstrates the impact of the TPR/FPR constraints $α$/$β$ on the generalization and exhibits a sharp order of $\tilde{O}(α^{-1}\n_+^{-1} + β^{-1}\n_-^{-1})$. Finally, extensive experiments on several benchmark datasets validate the strength of our proposed methods.

</details>


### [141] [M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis](https://arxiv.org/abs/2512.01214)
*Hang Wu,Ke Sun,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出M4-BLIP框架，用局部人脸先验与BLIP-2提取的局部/全局特征对齐融合，并结合LLM提升可解释性，在多模态媒体篡改检测上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态操纵检测方法忽视“局部信息”，而篡改常集中在特定区域（如人脸）。需要能利用局部先验并兼顾全局上下文，同时提升结果可解释性。

Method: 以BLIP-2为特征主干，利用其局部特征提取能力；引入人脸局部先验（如人脸区域/关键点/掩膜）；设计对齐与融合模块，将局部与全局特征精细对齐并融合；与LLM对接以生成解释与推理。

Result: 在多组定量指标与可视化实验上优于多种SOTA基线，显示更高的检测准确性与稳健性，并展示更好的可解释输出。

Conclusion: 结合局部先验与多模态大模型特征，并通过对齐融合实现精细检测，可有效提升多模态媒体操纵检测性能与可解释性。

Abstract: In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.

</details>


### [142] [S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance](https://arxiv.org/abs/2512.01223)
*Beining Xu,Siting Zhu,Zhao Jin,Junxian Li,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出S^2-MLLM，通过“隐式空间推理”提升MLLM在3D视觉指代中的空间理解，无需显式点云重建，效率更高、泛化更好，并在ScanRefer、Nr3D、Sr3D上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM擅长2D，但难以仅凭有限视角理解3D空间；依赖点云重建并渲染提供结构线索的方法效率低、推理受视角限制、空间推理能力有限。需要一种高效且能深度理解3D结构的方案。

Method: 提出S^2-MLLM：1) 空间引导策略：利用前馈式3D重建的结构感知，在训练中获得3D结构理解，从而在推理时无需显式点云重建即可隐式推理；2) 结构增强模块(SE)：采用视内与视间注意力捕获同视图依赖与跨视图对应；结合多层级位置编码，将视觉表征与空间位置与视点信息对齐，增强结构理解。

Result: 在ScanRefer、Nr3D、Sr3D数据集上实现显著性能提升，同时展现更好的泛化性与效率，优于现有基线/方法。

Conclusion: 隐式空间推理结合结构增强与多层级位置编码，可在无需低效点云重建的情况下显著提升MLLM的3DVG能力，实现性能、泛化与效率的统一。

Abstract: 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

</details>


### [143] [PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards](https://arxiv.org/abs/2512.01236)
*Shulei Wang,Longhui Wei,Xin He,Jianbo Ouyang,Hui Lu,Zhou Zhao,Qi Tian*

Main category: cs.CV

TL;DR: 提出一个多主体个性化图像生成方案：用单主体模型生成高质量多主体数据，再结合成对一致性与通用奖励做强化学习，显著提升多主体一致性与文本可控性，并提供新基准评测。


<details>
  <summary>Details</summary>
Motivation: 单主体个性化生成效果好，但扩展到多主体时易丢失主体一致性、文本遵循差。主要原因是缺乏高质量多主体数据和合适的后训练策略。

Method: 1) 数据：构建可扩展多主体数据生成流水线，利用强大的单主体生成模型合成多图、多主体多样化训练数据。2) 预训练/适配：让单主体个性化模型学习多图、多主体场景合成能力。3) 强化学习微调：设计成对主体一致性奖励（Pairwise Subject-Consistency Rewards）与通用奖励（提升文本可控性等），在精炼的RL阶段优化模型。4) 评测：提出包含7个子集、3个维度的新基准系统性衡量多主体个性化能力。

Result: 在广泛实验中，方法在多主体一致性与文本可控性上优于现有方法，展示了多主体个性化图像生成的显著提升。

Conclusion: 高质量合成数据+针对性的RL奖励可以有效弥补多主体个性化场景中的数据与优化缺口，方法兼具可扩展性与效果，并提供了标准化评测基准。

Abstract: Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: https://github.com/wang-shulei/PSR

</details>


### [144] [Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation](https://arxiv.org/abs/2512.01242)
*Zirui Zhao,Boye Niu,David Hsu,Wee Sun Lee*

Main category: cs.CV

TL;DR: 提出一个结合几何约束与神经语义的“AlphaGo式”搜索生成框架，在抽象几何组合（如七巧板）任务上比扩散与自回归基线更高的可行性与语义一致性。


<details>
  <summary>Details</summary>
Motivation: 抽象视觉组合的身份主要由少量几何元件的空间关系决定（对纹理不敏感）。在几何约束和模糊目标（文本）下组合固定组件很难：位置组合爆炸、数据有限、可行性离散（不重叠、允许朝向），导致解空间稀疏，不适合纯像素统计生成器。

Method: 提出“约束引导”的框架：将显式几何可行性推理与神经语义打分结合。用AlphaGo式蒙特卡洛树搜索（MCTS）在组合空间中规划动作并强制几何可行；策略网络作为启发函数并由搜索产生的计划反向微调。使用视觉-语言模型评估与文本/语义的对齐作为奖励；引入对抗式奖励精炼：用生成样本与真值对比训练打分器，使其难以区分二者，逐步提升奖励质量与生成质量。

Result: 在Tangram Assembly（七巧板装配）任务上，较扩散与自回归基线获得更高的方案有效率（无重叠、合法姿态）与更好的语义一致性，且在约束更严格时优势更明显。

Conclusion: 将MCTS的显式几何约束搜索与神经语义评分/对抗式奖励结合，能有效解决抽象几何组合中的稀疏可行解与语义对齐难题，优于纯像素空间生成方法，具有推广到其他结构化组合任务的潜力。

Abstract: We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.

</details>


### [145] [TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition](https://arxiv.org/abs/2512.01248)
*Junyuan Zhang,Bin Wang,Qintong Zhang,Fan Wu,Zichen Wen,Jialin Lu,Junjie Shan,Ziqi Zhao,Shuya Yang,Ziling Wang,Ziyang Miao,Huaping Zhong,Yuhang Zang,Xiaoyi Dong,Ka-Ho Chow,Conghui He*

Main category: cs.CV

TL;DR: 提出TRivia，一种无需标注数据即可从野外表格图像自监督微调VLM的表格识别方法，并推出3B开源模型在多基准上超越现有系统。


<details>
  <summary>Details</summary>
Motivation: 表格识别高度依赖大规模标注数据，开源模型受限于数据与隐私难以获得高质量标注，性能落后于专有模型；需要一种能利用未标注表格图像进一步提升VLM在TR任务性能的方法。

Method: 基于Group Relative Policy Optimization的自监督闭环：对每张表格图像由注意力引导模块生成多样问题；模型输出结构化结果后通过问答式奖励机制评估其能否正确回答问题；以此自动选择最有助学习的无标注样本并优化模型，无需人工标注。

Result: 构建TRivia训练管线并发布TRivia-3B（约3B参数）开源模型，在三个常用基准上超越包括Gemini 2.5 Pro与MinerU2.5等现有系统。

Conclusion: TRivia证明了无需标注即可让VLM学会识别、结构化与推理表格的可行性，缩小开源与专有模型在TR上的差距，并提供可复现的SOTA开源方案。

Abstract: Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia

</details>


### [146] [ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process](https://arxiv.org/abs/2512.01268)
*Jongwon Sohn,Juhyeon Moon,Hyunjoon Jung,Jaewook Nam*

Main category: cs.CV

TL;DR: 提出一种基于计算机视觉的“免接触”黏度计，通过观察搅拌液体自由表面对背景图案的折射变形来估计黏度，达到低误差回归与较高分类准确率，并加入不确定性量化与多图案增强以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统黏度计需要侵入式测量与严格实验条件，难以适配真实工艺场景与自动化需求；需要一种非接触、可在多样光照与开放环境下工作的黏度测量方法，并能提供可靠性评估。

Method: 布置固定背景图案，摄像机捕获搅拌驱动下连续变形的液面；利用光折射导致的图案光学畸变特征，经计算机视觉/学习模型回归黏度或进行黏度等级分类；采用多图案策略丰富视觉线索；引入不确定性量化为预测提供置信度。

Result: 在多样光照条件下，回归任务的平均绝对误差为0.113（log m2 s^-1），黏度分类准确率最高达81%；当黏度类别彼此接近时性能下降，但多图案策略提升了鲁棒性。

Conclusion: 该远距、非侵入式视觉黏度计在复杂环境下可用，带有不确定性评估，适合自动化与过程监控，提供了传统黏度测量的实用替代方案。

Abstract: Viscosity measurement is essential for process monitoring and autonomous laboratory operation, yet conventional viscometers remain invasive and require controlled laboratory environments that differ substantially from real process conditions. We present a computer-vision-based viscometer that infers viscosity by exploiting how a fixed background pattern becomes optically distorted as light refracts through the mixing-driven, continuously deforming free surface. Under diverse lighting conditions, the system achieves a mean absolute error of 0.113 in log m2 s^-1 units for regression and reaches up to 81% accuracy in viscosity-class prediction. Although performance declines for classes with closely clustered viscosity values, a multi-pattern strategy improves robustness by providing enriched visual cues. To ensure sensor reliability, we incorporate uncertainty quantification, enabling viscosity predictions with confidence estimates. This stand-off viscometer offers a practical, automation-ready alternative to existing viscometry methods.

</details>


### [147] [nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis](https://arxiv.org/abs/2512.01273)
*Xin Li,Wenhui Zhu,Xuanzhao Dong,Hao Wang,Yujian Xiong,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: 提出nnMobileNet++：在轻量CNN基础上引入动态蛇形卷积与阶段性Transformer，并结合视网膜预训练，在多数据集上以低算力达SOTA/竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 纯CNN难以捕获视网膜图像中的长程依赖与不规则边界（如细长血管、弥散病灶），限制临床关键特征的表征；在保持轻量高效的前提下，需要更强的全局上下文与边界敏感能力。

Method: 在nnMobileNet上构建混合架构：1) 动态蛇形卷积用于边界感知特征提取；2) 自第二次下采样后引入阶段性Transformer块以建模全局上下文与长程依赖；3) 采用视网膜图像预训练以增强泛化。并在多公共视网膜分类数据集进行实验与消融。

Result: 在多项视网膜分类基准上达到SOTA或高度竞争的准确率，同时保持低计算开销；消融证实三项组件分别带来稳健收益。

Conclusion: nnMobileNet++在轻量成本下有效结合卷积与Transformer，强化边界与全局信息建模，适合视网膜影像自动分析的实际部署与临床应用前景。

Abstract: Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases. Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation. As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient. However, purely convolutional architectures inherently struggle to capture long-range dependencies and model the irregular lesions and elongated vascular patterns that characterize on retinal images, despite the critical importance of vascular features for reliable clinical diagnosis. To further advance this line of work and extend the original vision of nnMobileNet, we propose nnMobileNet++, a hybrid architecture that progressively bridges convolutional and transformer representations. The framework integrates three key components: (i) dynamic snake convolution for boundary-aware feature extraction, (ii) stage-specific transformer blocks introduced after the second down-sampling stage for global context modeling, and (iii) retinal image pretraining to improve generalization. Experiments on multiple public retinal datasets for classification, together with ablation studies, demonstrate that nnMobileNet++ achieves state-of-the-art or highly competitive accuracy while maintaining low computational cost, underscoring its potential as a lightweight yet effective framework for retinal image analysis.

</details>


### [148] [Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI](https://arxiv.org/abs/2512.01291)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 提出将“有针对性的对比去学”与“可解释去学框架”结合，用于声呐图像目标检测/分类，减少海底背景依赖，提升泛化、鲁棒与可解释性，并在真/合成数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 当前高精度AI模型对海底地形/纹理等背景特征过度依赖，导致跨域或不同海床条件下泛化差；需要一种方法显式削弱背景偏置，并能可解释地验证“忘掉了什么”。

Method: 1) Targeted Contrastive Unlearning (TCU)：将三元组损失扩展为面向“背景偏置”的对比学习，鼓励模型在相同目标/不同海床之间拉近、在相同海床/不同目标之间拉远，从而降低海床特征的贡献。2) UESF：在训练后提供“去学解释”，可视化模型有意遗忘的区域；并改造LIME以产生更局部、更忠实的归因，用于评估去学效果。

Result: 在真实与合成声呐数据集上，TCU+UESF带来显著的去学效果提升，模型对海床变化更稳健，并给出更可靠的可视化解释；整体表现为更强的泛化与鲁棒性。

Conclusion: 面向声呐图像的去学与可解释联合框架能有效缓解背景偏置问题，提升跨域泛化与鲁棒性，同时以更忠实的解释证明模型确实减少了对海床特征的依赖。

Abstract: Acoustic sonar image analysis plays a critical role in object detection and classification, with applications in both civilian and defense domains. Despite the availability of real and synthetic datasets, existing AI models that achieve high accuracy often over-rely on seafloor features, leading to poor generalization. To mitigate this issue, we propose a novel framework that integrates two key modules: (i) a Targeted Contrastive Unlearning (TCU) module, which extends the traditional triplet loss to reduce seafloor-induced background bias and improve generalization, and (ii) the Unlearn to Explain Sonar Framework (UESF), which provides visual insights into what the model has deliberately forgotten while adapting the LIME explainer to generate more faithful and localized attributions for unlearning evaluation. Extensive experiments across both real and synthetic sonar datasets validate our approach, demonstrating significant improvements in unlearning effectiveness, model robustness, and interpretability.

</details>


### [149] [Diffusion Model in Latent Space for Medical Image Segmentation Task](https://arxiv.org/abs/2512.01292)
*Huynh Trinh Ngoc,Toan Nguyen Hai,Ba Luong Son,Long Tran Quoc*

Main category: cs.CV

TL;DR: 提出MedSegLatDiff：结合VAE与潜空间扩散的医学图像分割框架，在保持多样性与不确定性估计的同时显著提升效率与小结构保真度，并在多个数据集上达SOTA或接近SOTA的Dice/IoU。


<details>
  <summary>Details</summary>
Motivation: 传统分割输出单一掩码，无法表达医学图像固有的不确定性；现有生成式多假设方法虽可模拟多名临床医生的多样解读，但计算开销大、训练慢，且对微小病灶（如小结节）保真不足。

Method: 采用VAE将图像与掩码压缩到低维潜空间，在该空间内进行潜扩散以生成多样分割；用加权交叉熵替代VAE掩码重构分支中的MSE以强化小目标；通过潜空间噪声注入与采样产生多种可行掩码与置信图；整体框架在紧凑表示上训练与推理以提升效率。

Result: 在ISIC-2018、CVC-Clinic、LIDC-IDRI上取得SOTA或高度竞争的Dice/IoU，同时可生成多样化分割假设与置信度图；较确定性基线在可解释性与可靠性上更优。

Conclusion: MedSegLatDiff在保证分割精度的同时显著降低计算成本，并提供多样解与不确定性量化，更贴合临床需求，具备落地潜力。

Abstract: Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.

</details>


### [150] [EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly](https://arxiv.org/abs/2512.01296)
*Xiaokun Pan,Zhenzhe Li,Zhichao Ye,Hongjia Zhai,Guofeng Zhang*

Main category: cs.CV

TL;DR: EGG-Fusion提出基于可微高斯surfel的实时SLAM/重建系统，通过稳健稀疏到稠密跟踪与信息滤波融合，显著提升几何精度与鲁棒性，在保持24 FPS的同时将表面误差降至0.6cm，优于现有GS系方法20%+。


<details>
  <summary>Details</summary>
Motivation: 可微渲染驱动的SLAM（如NeRF、3DGS）虽能获得高保真外观，但在实时性与对传感器噪声的敏感性上受限，导致几何退化与实用性不足。需要一种既能实时又鲁棒、并明确建模噪声以提升几何精度的方法。

Method: 提出EGG-Fusion：1) 稳健的稀疏到稠密相机跟踪；2) 几何感知的Gaussian surfel映射，用可微高斯surfel建模多视一致表面；3) 基于信息滤波的融合，显式考虑传感器噪声；4) 高效参数优化以满足实时（24 FPS）。

Result: 在Replica与ScanNet++基准上实现0.6cm表面重建误差，较SOTA基于高斯渲染的方法提升20%+；同时实现实时处理24 FPS。

Conclusion: EGG-Fusion在可微渲染实时重建中兼顾精度与速度，通过几何感知的高斯surfel与信息滤波，有效抑制噪声、提升几何保真度，成为当前最准确的实时可微渲染重建系统之一。

Abstract: Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/

</details>


### [151] [TBT-Former: Learning Temporal Boundary Distributions for Action Localization](https://arxiv.org/abs/2512.01298)
*Thisara Rathnayaka,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 提出 TBT-Former，用更强 Transformer 骨干、跨尺度FPN融合和边界分布回归头，提升模糊边界定位与多尺度上下文融合，在 THUMOS14、EPIC-Kitchens100 上 SOTA，ActivityNet1.3 具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有单阶段无锚TAL模型（如 ActionFormer）虽强，但在两点上仍有瓶颈：1）对时间边界模糊的动作实例定位不准；2）多尺度时序上下文融合不充分。需要一种能同时强化时序建模容量、跨尺度融合与边界不确定性表达的方法。

Method: 在 ActionFormer 基线之上提出 TBT-Former，包含三点改进：1）更大容量的缩放式 Transformer 骨干（更多注意力头与更大MLP维度）以增强时序特征提取；2）带自顶向下与横向连接的跨尺度FPN，用于融合高层语义与低层时序细节；3）受GFL启发的边界分布回归头，将边界回归转为概率分布学习，显式刻画边界不确定性。

Result: 在 THUMOS14 与 EPIC-Kitchens 100 上超过前人基准，刷新性能；在 ActivityNet-1.3 上保持有竞争力。

Conclusion: 通过强化骨干容量、跨尺度融合与引入边界分布学习，TBT-Former有效缓解模糊边界定位与多尺度信息融合难题，推动Transformer范式下的TAL性能上限。

Abstract: Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or "fuzzy" temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects/tree/main/projects/210536K-Multi-Modal-Learning_Video-Understanding

</details>


### [152] [DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy](https://arxiv.org/abs/2512.01302)
*Jaewoo Song,Jooyoung Choi,Kanghyun Baek,Sangyub Lee,Daemin Park,Sungroh Yoon*

Main category: cs.CV

TL;DR: DCText是一种无需训练的视觉文本生成方法，通过“分而治之”将长/多段文本拆分并分配到指定区域，配合两阶段注意力掩码与局部噪声初始化，在保证图像一致性的同时显著提升文本渲染准确率与速度。


<details>
  <summary>Details</summary>
Motivation: 尽管最新文本到图像模型在短文本渲染上表现良好，但对长文本或多段文本仍易因全局注意力被稀释而失败，需要一种无需额外训练、能稳健处理多文本并保持图像质量与效率的方法。

Method: 提出DCText：1) 将提示中的目标文本解析并拆分为片段，给每段分配图像中的指定区域；2) 在扩散去噪过程中顺序应用两种注意力掩码：Text-Focus（聚焦每段文本所在区域以提升字符渲染）与Context-Expansion（逐步扩展上下文以保持整体一致性）；3) 引入Localized Noise Initialization，在对应区域初始化局部噪声以改进文本准确性与区域对齐，且不增加计算成本；基于多模态扩散Transformer的可靠短文本能力实现全流程训练免。

Result: 在单句与多句基准上取得最佳文本准确率，同时不牺牲图像质量，并实现最低生成延迟。

Conclusion: DCText通过区域化文本分解、掩码化注意力调度与局部噪声初始化，有效缓解长/多文本渲染中的注意力稀释问题，达到更高文本准确性与更快生成速度且保持画面一致性。

Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.

</details>


### [153] [Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians](https://arxiv.org/abs/2512.01306)
*Hongru Yan,Xiang Zhang,Zeyuan Chen,Fangyin Wei,Zhuowen Tu*

Main category: cs.CV

TL;DR: 提出“Gaussian Swaying”：用3D高斯作连续曲面表示，同时用于气动受力计算与轻量渲染，实现真实感摆动/飘动的统一框架，性能与效果达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实世界中风引起的摆动（树枝、旗帜、船只）对视觉与图形真实感至关重要；现有基于网格的气动仿真需昂贵网格化，基于粒子的办法以离散点为主，难以连续、细粒度地表达表面并统一渲染与动力学。

Method: 用3D高斯作为“高斯贴片”连续建模表面：在同一表示上进行气动力计算（基于高斯参数化的表面法向与面积近似、风场交互）与渲染（由法线进行轻量着色）；避免网格化，支持细粒度交互与高效计算。

Result: 在合成与真实数据集、多个评价指标上，方法在效果与效率上达到或超过现有方法，展现良好可扩展性与SOTA性能。

Conclusion: 3D高斯表面的统一表示可同时驱动气动仿真与渲染，提供高效、可扩展且真实的场景气动模拟方案。

Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.

</details>


### [154] [Lost in Distortion: Uncovering the Domain Gap Between Computer Vision and Brain Imaging - A Study on Pretraining for Age Prediction](https://arxiv.org/abs/2512.01310)
*Yanteng Zhang,Songheng Li,Zeyu Shen,Qizhen Lan,Lipei Zhang,Yang Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 研究探讨“低质量/含噪神经影像”在预训练中的作用：不同质量层级的预训练对外部数据脑龄预测性能影响显著，提示需进行面向领域的数据质量策划，以构建可信的神经影像基础模型。


<details>
  <summary>Details</summary>
Motivation: 与计算机视觉中较为同质、结构清晰的自然图像不同，大规模神经影像数据质量高度异质，存在失真、缺失等问题。尚不清楚低质量或含噪样本在预训练中是有益的数据增强，还是会干扰表征学习；这一问题直接影响到面向临床应用的领域基础模型的可信度与可泛化性。

Method: 将预训练数据按质量分层（从高质量到严重失真/不完整），分别在各层级数据上进行表征预训练；随后在外部独立队列上针对脑龄预测任务进行微调与评估，比较不同预训练质量层级对下游性能的影响，并讨论与计算机视觉数据实践和临床神经影像质量标准之间的差异。

Result: 不同数据质量层级预训练导致显著性能差异；部分质量较低的数据并未带来收益，甚至可能削弱下游泛化；高质量或经过审慎策划的数据在脑龄预测上取得更优且更稳定的结果。

Conclusion: 在构建神经影像领域基础模型时，数据质量至关重要；应采用面向领域的质量控制与策划策略，而非简单套用通用CV预训练做法，以获得可信、可泛化的模型。

Abstract: Large-scale brain imaging datasets provide unprecedented opportunities for developing domain foundation models through pretraining. However, unlike natural image datasets in computer vision, these neuroimaging data often exhibit high heterogeneity in quality, ranging from well-structured scans to severely distorted or incomplete brain volumes. This raises a fundamental question: can noise or low-quality scans contribute meaningfully to pretraining, or do they instead hinder model learning? In this study, we systematically explore the role of data quality level in pretraining and its impact on downstream tasks. Specifically, we perform pretraining on datasets with different quality levels and perform fine-tuning for brain age prediction on external cohorts. Our results show significant performance differences across quality levels, revealing both opportunities and limitations. We further discuss the gap between computer vision practices and clinical neuroimaging standards, emphasizing the necessity of domain-aware curation to ensure trusted and generalizable domain-specific foundation models.

</details>


### [155] [IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval](https://arxiv.org/abs/2512.01312)
*Ning Han,Yawen Zeng,Shaohua Long,Chengqing Li,Sijie Yang,Dun Tan,Jianfeng Dong,Jingjing Chen*

Main category: cs.CV

TL;DR: 提出交互式视频语料检索(IVCR)任务与IVCR-200K数据集，并基于多模态大模型构建可对话、多轮、人机交互的视频/片段检索框架，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索/片段检索多为单轮、单向检索，缺乏与用户的有效交互，难以满足大多数用户的个性化与动态需求。

Method: 1) 定义IVCR任务：支持多轮、对话式、真实交互；既能检索整段视频也能定位片段。2) 构建IVCR-200K：高质量、双语、多轮对话、抽象语义标注的数据集。3) 基于多模态大语言模型的综合框架：支持多种交互模式、可解释反馈与检索，整合视频与文本模态以完成检索与时刻定位。

Result: 在广泛实验中，所提数据集与框架在交互式检索和时刻检索场景中表现优异，证明了其有效性与实用价值。

Conclusion: IVCR定义了更贴近真实使用的交互式视频检索范式；IVCR-200K与MLLM框架为研究提供了标准数据与强大基线，推动个性化、可解释的视频检索发展。

Abstract: In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful "interaction" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.

</details>


### [156] [TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance](https://arxiv.org/abs/2512.01314)
*Pei Yang,Yepeng Liu,Kelly Peng,Yuan Gao,Yiren Song*

Main category: cs.CV

TL;DR: 提出TokenPure：一种基于Diffusion Transformer的条件生成框架，通过令牌化的视觉与结构条件来一致性移除水印，并在重建保真度与感知质量上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数字经济中，海量可复制内容（含AIGC）需要可靠的所有权证明，水印是核心手段；但现实中水印经常遭受裁剪、压缩、滤波、叠加等操作。现有强鲁棒水印同时也促生“去水印”需求的攻防研究；而现有去水印方法往往在“彻底破坏水印信号”和“保持内容一致性”之间难以兼顾。作者旨在提出一种既能稳定去水印又能保持视觉一致性的通用方案。

Method: 将去水印任务重构为条件生成：完全绕过携带水印的初始噪声，基于Diffusion Transformer进行重建。核心是把含水印图像分解为两类互补令牌：视觉tokens（纹理外观）与结构tokens（几何与形状）。二者共同作为扩散过程的条件，引导模型合成无水印图像，实现细粒度外观一致与结构完整。

Result: 在多组综合实验中，相比现有基线，TokenPure在水印消除强度、重建保真度与感知质量、以及与原内容的一致性方面均取得SOTA，显著领先。

Conclusion: 基于令牌条件的Diffusion Transformer可在不依赖原始噪声的前提下，实现强去水印与高一致性重建，验证了视觉与结构双令牌条件的有效性，为水印攻防与内容修复提供了新路径。

Abstract: In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.

</details>


### [157] [FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection](https://arxiv.org/abs/2512.01315)
*Ashish Vashist,Qiranul Saadiyean,Suresh Sundaram,Chandra Sekhar Seelamantula*

Main category: cs.CV

TL;DR: 提出FOD-S2R数据集：在模拟飞机油箱内采集3114张真实HD图像与3137张Unreal Engine合成图像，涵盖多视场、距离、光照、颜色与尺寸。基于多种SOTA检测器实验证明：加入合成数据能提升对真实场景的检测准确率与泛化，缩小Sim2Real差距，用于自动化油箱FOD检测。


<details>
  <summary>Details</summary>
Motivation: 油箱内部FOD会导致污染、故障与高维护成本，但缺乏针对封闭油箱环境的专用数据集，限制了自动检测系统的发展。作者希望填补这一数据空白，并系统评估合成数据对真实检测性能的增益。

Method: 构建FOD-S2R数据集：在受控油箱复刻环境采集真实图像，并用Unreal Engine生成合成图像；设计多样化拍摄条件（FOV、距离、光照、颜色、尺寸）；用多种SOTA目标检测模型进行基准测试，比较仅真实数据与真实+合成数据的训练效果。

Result: 在多种检测模型上，加入合成数据训练显著提升对真实油箱场景的检测准确率与泛化能力，相比仅用真实数据训练取得更好表现。

Conclusion: FOD-S2R为封闭油箱环境提供首个系统性Sim2Real评估数据集；合成数据有效提升FOD检测性能并缩小Sim2Real差距，为航空维护中的自动化FOD检测提供坚实基础。

Abstract: Foreign Object Debris (FOD) within aircraft fuel tanks presents critical safety hazards including fuel contamination, system malfunctions, and increased maintenance costs. Despite the severity of these risks, there is a notable lack of dedicated datasets for the complex, enclosed environments found inside fuel tanks. To bridge this gap, we present a novel dataset, FOD-S2R, composed of real and synthetic images of the FOD within a simulated aircraft fuel tank. Unlike existing datasets that focus on external or open-air environments, our dataset is the first to systematically evaluate the effectiveness of synthetic data in enhancing the real-world FOD detection performance in confined, closed structures. The real-world subset consists of 3,114 high-resolution HD images captured in a controlled fuel tank replica, while the synthetic subset includes 3,137 images generated using Unreal Engine. The dataset is composed of various Field of views (FOV), object distances, lighting conditions, color, and object size. Prior research has demonstrated that synthetic data can reduce reliance on extensive real-world annotations and improve the generalizability of vision models. Thus, we benchmark several state-of-the-art object detection models and demonstrate that introducing synthetic data improves the detection accuracy and generalization to real-world conditions. These experiments demonstrate the effectiveness of synthetic data in enhancing the model performance and narrowing the Sim2Real gap, providing a valuable foundation for developing automated FOD detection systems for aviation maintenance.

</details>


### [158] [Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications](https://arxiv.org/abs/2512.01319)
*Feiyang Xiao,Yichi Zhang,Xigui Li,Yuanye Zhou,Chen Jiang,Xin Guo,Limei Han,Yuxin Li,Fengping Zhu,Yuan Cheng*

Main category: cs.CV

TL;DR: 提出IAVS数据集与评测体系，面向颅内动脉瘤及其母血管的分割，并以CFD适用性为核心评价；发布两阶段基准与简单有效的基线方法，实现从分割到CFD模型的标准化转换。


<details>
  <summary>Details</summary>
Motivation: 现有方法多以图像分割指标评估，忽视结果在计算流体力学（CFD）中的实际可用性与拓扑完整性；缺乏含有血流动力学结果、能支撑临床相关评测的大规模多中心数据集。

Method: 构建IAVS多中心3D MRA数据集（641例、587注释），包含动脉瘤与IA-Vessel标注以及对应的血流动力学分析结果；设计两阶段评测基准（I：动脉瘤全局定位；II：IA-Vessel精细分割）；提出简洁的两阶段分割框架作为开箱即用的强基线；建立标准化CFD适用性评估系统，实现分割掩膜到CFD模型的自动一致转换并开展适用性评价。

Result: 形成首个兼顾图像与CFD适用性的IA-Vessel数据与评测体系，提供可复现实验基线；实现对分割结果在CFD中的可用性进行系统化、自动化评估。

Conclusion: IAVS为社区提供临床与工程双重导向的基准：既支持传统分割指标评测，又能衡量CFD可用性；配套数据、代码与模型将公开，推动面向真实临床应用的分割与血流动力学研究。

Abstract: The precise segmentation of intracranial aneurysms and their parent vessels (IA-Vessel) is a critical step for hemodynamic analyses, which mainly depends on computational fluid dynamics (CFD). However, current segmentation methods predominantly focus on image-based evaluation metrics, often neglecting their practical effectiveness in subsequent CFD applications. To address this deficiency, we present the Intracranial Aneurysm Vessel Segmentation (IAVS) dataset, the first comprehensive, multi-center collection comprising 641 3D MRA images with 587 annotations of aneurysms and IA-Vessels. In addition to image-mask pairs, IAVS dataset includes detailed hemodynamic analysis outcomes, addressing the limitations of existing datasets that neglect topological integrity and CFD applicability. To facilitate the development and evaluation of clinically relevant techniques, we construct two evaluation benchmarks including global localization of aneurysms (Stage I) and fine-grained segmentation of IA-Vessel (Stage II) and develop a simple and effective two-stage framework, which can be used as a out-of-the-box method and strong baseline. For comprehensive evaluation of applicability of segmentation results, we establish a standardized CFD applicability evaluation system that enables the automated and consistent conversion of segmentation masks into CFD models, offering an applicability-focused assessment of segmentation outcomes. The dataset, code, and model will be public available at https://github.com/AbsoluteResonance/IAVS.

</details>


### [159] [Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI](https://arxiv.org/abs/2512.01333)
*A S M Ahsanul Sarkar Akib,Raduana Khawla,Abdul Hasib*

Main category: cs.CV

TL;DR: 提出可解释的中风风险预测框架：基于集成学习（RF+ExtraTrees+XGBoost）并结合LIME解释，达到99.09%准确率，关键特征为年龄、高血压、血糖。


<details>
  <summary>Details</summary>
Motivation: 中风致死致残率高，临床需要早期、准确且可解释的风险评估工具，以支持及时干预和个体化预防。现有模型要么准确度不足，要么可解释性弱，且数据不平衡常影响性能。

Method: 多数据集上进行特征工程与预处理，采用ROS处理类别不平衡；对10种机器学习模型做5折交叉验证评估；优化并集成Random Forest、ExtraTrees、XGBoost；使用LIME进行特征级解释，识别关键临床变量。

Result: 在Stroke Prediction Dataset上，优化的集成模型达到了99.09%准确率；LIME解释显示年龄、高血压、血糖为最重要的风险变量。

Conclusion: 集成学习结合XAI能够实现高准确且可解释的中风风险评估，可用于早期预测，支持数据驱动的预防与个体化临床决策，具有转化潜力。

Abstract: Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.

</details>


### [160] [AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation](https://arxiv.org/abs/2512.01334)
*Yexin Liu,Wen-Jie Shu,Zile Huang,Haoze Zheng,Yueze Wang,Manyuan Zhang,Ser-Nam Lim,Harry Yang*

Main category: cs.CV

TL;DR: 提出AlignVid，一个训练-free的文本引导图到视频方法，通过注意力缩放与调度缓解“语义忽视”，在不显著损害美学质量下更好服从细粒度提示；并发布OmitI2V数据集评测添加/删除/修改场景。


<details>
  <summary>Details</summary>
Motivation: 现有TI2V能保持主体一致与时序连贯，但对需要大幅变化（加物、删物、属性修改）的提示往往不遵从，出现语义忽视。先验试验发现对输入图像施加高斯模糊能提升语义服从，注意力图显示前景/背景分离更清晰且跨注意力分布熵更低，启发了从能量角度进行注意力调控。

Method: 提出AlignVid，训练-free，由两部分组成：1) 注意力缩放调制（ASM）：在变换器的交叉注意力中对Q或K进行轻量缩放以直接重加权注意力；2) 引导调度（GS）：在不同变换器层与去噪步数上选择性启用ASM，降低画质劣化。还构建OmitI2V基准，含367个人工标注样本，覆盖添加、删除、修改三类。

Result: 在广泛实验中，AlignVid显著提升文本提示的细粒度语义一致性，同时将视觉/美学退化控制在较低水平；在OmitI2V上验证其在加/删/改三类任务中的优势。

Conclusion: 简单的注意力缩放与调度即可在不训练的前提下缓解TI2V的语义忽视问题，提升提示服从且保持画质；OmitI2V为该问题提供了评测基准。

Abstract: Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.

</details>


### [161] [EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans](https://arxiv.org/abs/2512.01340)
*Yingjie Zhou,Xilei Zhu,Siyu Ren,Ziyi Zhao,Ziwen Wang,Farong Wen,Yu Zhou,Jiezhang Cao,Xiongkuo Min,Fengjiao Chen,Xiaoyu Li,Xuezhi Cao,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 该论文构建首个多说话人驱动的说话人头像质量评估数据集THQA-MT，并提出评测框架EvalTalker，可感知全局质量、人物特征与身份一致性，并融合Qwen-Sync进行多模态同步感知，在与主观分数的相关性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单一说话人驱动（Talker）难以满足多主体同时驱动的沉浸式交互需求；现有Multi-Talker生成质量参差且评估缺乏标准与高质量数据集，影响体验与研究进展。

Method: 1) 构建THQA-MT：从15个代表性Multi-Talker方法，对400张真实人像生成5,492个多说话人视频；组织主观实验，分析不同方法的感知差异并归纳12类常见失真。2) 提出EvalTalker评估框架：同时建模全局视频质量、人物外观与身份一致性；引入Qwen-Sync以感知语音-视觉同步；输出与主观感受一致的质量评分。

Result: 在多项实验中，EvalTalker与主观评分的相关性显著高于现有评估方法，验证其在Multi-Talker质量评估任务上的有效性与鲁棒性。

Conclusion: THQA-MT为多说话人头像质量评估提供了标准数据基础；EvalTalker作为新评估器能全面感知质量、身份与时序同步，为高质量Multi-Talker生成与评测研究奠定基准。

Abstract: Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.

</details>


### [162] [InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision](https://arxiv.org/abs/2512.01342)
*Chenting Wang,Yuhan Zhu,Yicheng Xu,Jiange Yang,Ziang Yan,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: 该论文提出InternVideo-Next：在不依赖文本监督的前提下，通过改进MVM的架构与训练方式，学习兼具语义一致性与细节保真的视频表示，最终在多项基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频-文本预训练依赖嘈杂、覆盖有限的合成字幕，难以涵盖隐式世界知识（运动、3D与物理）；而纯MVM虽可直接利用时空结构，却在通用任务上落后。作者归因于常见架构问题：像素重建的收敛困难与低层次目标与语义冲突；潜变量预测易出现捷径学习。

Method: 将传统编码器-解码器拆解为编码器-预测器-解码器（EPD），把预测器视作潜在的世界模型，并采用两阶段预训练：阶段1以条件扩散解码器替代线性解码器，并注入可靠的图像级语义先验，促使预测器输出的潜在表示既可高保真重建又具高层语义；阶段2在冻结阶段1目标的潜在空间内进行预测学习，以避免捷径并进一步灌输世界知识。

Result: 在仅使用公开、无标注视频进行预训练的条件下，模型在多项视频理解基准上达到最新SOTA（具体任务/指标虽未在摘要列出，但总体优于文本监督方法与传统MVM）。

Conclusion: 通过EPD框架与两阶段训练（扩散解码+冻结目标的潜在预测），可在不依赖噪声文本的情况下学得语义一致且细节完备的表示，缩小乃至超越视频-文本监督与MVM之间的性能差距，为通用视频表示学习提供可扩展路径。

Abstract: Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.

</details>


### [163] [Handwritten Text Recognition for Low Resource Languages](https://arxiv.org/abs/2512.01348)
*Sayantan Dey,Alireza Alaei,Partha Pratim Roy*

Main category: cs.CV

TL;DR: 提出BharatOCR：一种无需显式分割的段落级印地语/乌尔都手写文字识别系统，采用ViT特征提取+Transformer解码器，并结合预训练RoBERTa语言模型后处理/融合，基于自建与公开数据集，在多套乌尔都与印地语数据上取得SOTA或基准水平的字符识别率（乌尔都最高96.24%，印地语80.64%）。


<details>
  <summary>Details</summary>
Motivation: 段落级手写识别在低资源语言（印地语、乌尔都等）上仍难：缺乏完善的语言资源、复杂脚本形态、需要鲁棒OCR而不依赖昂贵的行/字分割与语言先验。现有方法在段落级无分割和语言建模融合方面不足。

Method: 提出无分割段落级HTR框架：1) 视觉端采用DeiT/ViT提取图像视觉特征；2) Transformer解码器从视觉嵌入逐步生成文本序列；3) 通过预训练RoBERTa（MLM优化）作为语言模型对输出进行精炼，提升准确性、流畅性与一致性；4) 采用隐式行分割，按行迭代处理整段图像；5) 在自建“Parimal Urdu/Parimal Hindi”及公开数据集上训练/评估。

Result: 在NUST-UHWR、PUCIT-OUHL、Parimal-Urdu上字符识别率分别为96.24%、92.05%、94.80%，均达基准/领先；在印地语数据集上字符识别率80.64%。总体超越多种乌尔都最先进方法。

Conclusion: 结合ViT视觉编码、Transformer解码与RoBERTa语言模型的无分割段落级HTR对低资源脚本有效，可在乌尔都与印地语取得强性能；隐式行分割策略可行。未来可拓展到更多低资源脚本、优化印地语性能与端到端语言-视觉联合训练。

Abstract: Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.

</details>


### [164] [OpenBox: Annotate Any Bounding Boxes in 3D](https://arxiv.org/abs/2512.01352)
*In-Jae Lee,Mungyeom Kim,Kwonyoung Ryu,Pierre Musacchio,Jaesik Park*

Main category: cs.CV

TL;DR: OpenBox提出一个两阶段、无需自训练的自动标注流程，结合2D大模型与3D点云跨模态对齐，并按刚性与运动状态自适应生成3D框，实现更高质量、更高效率的无监督开放词汇3D检测标注。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要低成本标注与识别未见类别的能力。现有无监督/开放词汇3D检测多：统一画3D框、忽视物体物理状态（刚性/非刚性、静止/运动）、依赖多轮自训练来迭代精化，导致质量受限与算力开销大。

Method: 两阶段管线：1) 利用2D视觉基础模型在图像上提取实例级线索，并通过跨模态实例对齐将其与对应的3D点云关联，实现2D→3D实例级映射；2) 按实例的刚性与运动状态进行分类，并依据类别/状态的尺寸统计生成自适应3D边界框，无需自训练。

Result: 在Waymo、Lyft Level 5与nuScenes上，相比基线在准确性与效率上均有提升；产出的3D框标注质量更高且计算成本更低。

Conclusion: OpenBox通过2D基础模型与3D点云的跨模态对齐、再结合物理状态感知与尺寸先验，提供了一种无需自训练的高效高质自动3D标注方案，适用于开放词汇与无监督场景。

Abstract: Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.

</details>


### [165] [BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud](https://arxiv.org/abs/2512.01366)
*Yunzhe Li,Jiajun Yan,Yuzhou Wei,Kechen Liu,Yize Zhao,Chong Zhang,Hongzi Zhu,Li Lu,Shan Chang,Minyi Guo*

Main category: cs.CV

TL;DR: BlinkBud是一种利用单只耳机与配对手机，在线检测从身后接近的危险目标（车辆等）的系统。其核心是在极少采样的耳机相机图像上，结合3D跟踪、卡尔曼滤波与强化学习采样策略，实现高精度、低功耗的目标跟踪与危险判定，同时通过头部姿态补偿提高稳健性。原型在真实环境中实现超低功耗（耳机29.8 mW、手机702.6 mW）与低误报/漏报（FPR 4.90%，FNR 1.47%）。


<details>
  <summary>Details</summary>
Motivation: 行人与骑行者难以及时感知从身后高速接近的车辆，存在严重安全隐患。传统视觉/感知方案要么功耗高、要么需要多传感器或前向视角，难以部署在轻量级可穿戴设备上。因此需要一种在资源受限（单只耳机、有限图像采样）的条件下，仍能稳定准确感知后向来车风险的方案。

Method: 提出BlinkBud系统：1) 使用耳机微型相机少量取样图像并在手机侧做视觉检测；2) 设计新型3D目标跟踪算法，将卡尔曼滤波用于轨迹估计；3) 以强化学习学习最优图像采样策略，在保证精度的同时降低采样与计算频率；4) 借助IMU估计用户头部俯仰/偏航角，对目标深度估计进行校正，并将相机坐标对齐到人体坐标，以抵消头动影响；5) 在原型系统上进行实测评估。

Result: 原型在真实环境中展示：耳机端平均功耗29.8 mW、手机端702.6 mW；危险检测准确，平均FPR 4.90%、FNR 1.47%。

Conclusion: BlinkBud在单耳机+手机的低资源设置下实现了稳定高精度的后向危险检测，显著降低功耗并抑制头部运动带来的误差，具有实际部署与道路安全应用潜力。

Abstract: Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.

</details>


### [166] [SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation](https://arxiv.org/abs/2512.01373)
*Sheng Liu,Tianyu Luan,Phani Nuney,Xuelu Feng,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出一种无需真实参考的3D网格“真实感对齐”评估指标：将网格编码到语言token空间，结合专门的真实感解码器与LLM对齐人类感知，并构建无GT的人评数据集RealismGrading；在跨对象k折验证中与人评高度相关，优于现有方法且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有3D评估多依赖有GT的几何/拓扑误差，而内容创作中的“逼真度”常无可用GT，且主观真实感难以被几何误差捕捉，需要一种能直接对齐人类感知、可在无GT场景中适用的客观指标。

Method: 1) 网格语言化编码：将3D形状（几何/拓扑/纹理等）转换为语言token序列；2) 使用LLM作为语义桥，提取与“真实感”相关的高层表征；3) 设计真实感解码器，将LLM输出对齐为标量分数/等级；4) 构建RealismGrading数据集：16种生成算法、十余类对象的人评真实感分；5) 通过跨对象k折训练/验证以测试泛化。

Result: 所提指标与人类主观评分具有更高的相关性，较现有基准显著提升；在跨对象设置中保持稳定表现，显示良好的跨分布泛化能力。

Conclusion: LLM可作为3D形状与人类真实感评估之间的对齐器；结合网格语言化编码与真实感解码器，可在无GT情形下可靠评估网格真实感；该指标在多算法、多对象上验证有效并具备泛化潜力。

Abstract: 3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.

</details>


### [167] [Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network](https://arxiv.org/abs/2512.01380)
*Tianyu Luan,Xuelu Feng,Zixin Zhu,Phani Nuney,Sheng Liu,Xuan Gong,David Doermann,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出一种无需渲染、直接在带纹理网格上评估保真度的新指标TGE，并构建含真实失真与人工标注的数据集；实验显示TGE优于基于渲染和仅几何的方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D保真度度量（如Chamfer Distance）与人类主观评估不一致；基于渲染的学习式指标受视角覆盖不全与视角敏感影响，且多在合成失真上训练导致域偏移，无法准确反映真实世界失真。

Method: 直接在带纹理的3D网格上度量，将几何与颜色信息联合建模，输入为待评估纹理网格与参考彩色形状（网格），无需渲染中间图像。为训练与评测，构建含真实世界失真的人类标注数据集，并以此监督/校准指标。

Result: 在包含真实失真的数据集上，TGE在与人类意见一致性方面优于基于渲染的学习指标和仅几何度量（如Chamfer）。

Conclusion: 绕开渲染、直接结合几何与纹理可更贴近人类对3D保真的判断；真实失真与人标注的数据对训练评测有效。TGE为评估纹理化3D模型提供更稳健、对视角无关的度量。

Abstract: Textured high-fidelity 3D models are crucial for games, AR/VR, and film, but human-aligned evaluation methods still fall behind despite recent advances in 3D reconstruction and generation. Existing metrics, such as Chamfer Distance, often fail to align with how humans evaluate the fidelity of 3D shapes. Recent learning-based metrics attempt to improve this by relying on rendered images and 2D image quality metrics. However, these approaches face limitations due to incomplete structural coverage and sensitivity to viewpoint choices. Moreover, most methods are trained on synthetic distortions, which differ significantly from real-world distortions, resulting in a domain gap. To address these challenges, we propose a new fidelity evaluation method that is based directly on 3D meshes with texture, without relying on rendering. Our method, named Textured Geometry Evaluation TGE, jointly uses the geometry and color information to calculate the fidelity of the input textured mesh with comparison to a reference colored shape. To train and evaluate our metric, we design a human-annotated dataset with real-world distortions. Experiments show that TGE outperforms rendering-based and geometry-only methods on real-world distortion dataset.

</details>


### [168] [Reversible Inversion for Training-Free Exemplar-guided Image Editing](https://arxiv.org/abs/2512.01382)
*Yuke Li,Lianli Gao,Ji Zhang,Pengpeng Zeng,Lichuan Xiang,Hongkai Wen,Heng Tao Shen,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出一种无需训练的可逆反演(ReInversion)用于示例引导图像编辑，通过两阶段去噪（先源图后参考图）和掩码引导的选择性去噪(MSD)，在保证背景结构一致的同时实现高质量、高效率编辑，达到SOTA且计算开销最低。


<details>
  <summary>Details</summary>
Motivation: 现有EIE方法往往依赖大规模预训练来学习源图与参考图的对应关系，计算成本高。训练free的反演可替代，但标准反演用于EIE时质量差、效率低，亟需一种既有效又高效的训练free方案。

Method: 提出ReInversion：将编辑过程建模为两阶段扩散去噪/反演流程，第一阶段以源图条件进行反演，使其进入可编辑潜空间；第二阶段以参考图条件继续去噪实现风格/属性迁移。同时提出Mask-Guided Selective Denoising（MSD），用掩码限制编辑至目标区域，保护背景结构一致性与细节。整体为训练free框架，避免大规模预训练。

Result: 在定性与定量比较中均优于现有方法，达到EIE任务SOTA；同时具有最低计算开销与更高效率。

Conclusion: ReInversion通过可逆两阶段反演与掩码选择性去噪，实现训练free的高质量、区域可控的示例引导编辑，在效果与效率上均优于以往方法。

Abstract: Exemplar-guided Image Editing (EIE) aims to modify a source image according to a visual reference. Existing approaches often require large-scale pre-training to learn relationships between the source and reference images, incurring high computational costs. As a training-free alternative, inversion techniques can be used to map the source image into a latent space for manipulation. However, our empirical study reveals that standard inversion is sub-optimal for EIE, leading to poor quality and inefficiency. To tackle this challenge, we introduce \textbf{Reversible Inversion ({ReInversion})} for effective and efficient EIE. Specifically, ReInversion operates as a two-stage denoising process, which is first conditioned on the source image and subsequently on the reference. Besides, we introduce a Mask-Guided Selective Denoising (MSD) strategy to constrain edits to target regions, preserving the structural consistency of the background. Both qualitative and quantitative comparisons demonstrate that our ReInversion method achieves state-of-the-art EIE performance with the lowest computational overhead.

</details>


### [169] [PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications](https://arxiv.org/abs/2512.01383)
*Yunze Liu,Zifan Wang,Peiran Wu,Jiayang Ao*

Main category: cs.CV

TL;DR: 提出PointNet4D：轻量级4D点云视频骨干，结合Mamba与Transformer做时序融合，并配套4DMAP帧级掩码自回归预训练；在7个数据集9项任务上稳定提升，并在两套机器人应用上取得显著增益。


<details>
  <summary>Details</summary>
Motivation: 现有4D点云骨干依赖时空卷积与Transformer，计算开销大、难实时，并且在线/离线场景下对变长序列与资源约束适配性差；需要一种既高效又能利用时序信息的统一4D骨干与更有效的时序预训练。

Method: 1) 设计PointNet4D骨干：以混合Mamba-Transformer时序融合模块为核心，利用Mamba的高效状态空间建模实现线性复杂度的在线处理，并结合Transformer的双向建模增强离线性能；可处理变长序列。2) 提出4DMAP预训练：帧级掩码自回归，跨帧建模运动线索，提升时序理解。3) 在多任务多数据集上评估，并将骨干集成到机器人策略（4D Diffusion Policy、4D Imitation Learning）。

Result: 在7个数据集的9项任务上取得一致性能提升；在RoboTwin与HandoverSim两大机器人基准中，集成PointNet4D后显著优于对比方法（摘要未给出具体数值）。

Conclusion: PointNet4D在保证实时性与资源友好前提下，兼顾在线与离线时序建模能力；结合4DMAP预训练，可广泛提升4D点云任务与下游机器人策略的表现，表明轻量级混合时序建模是有效路线。

Abstract: Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks.

</details>


### [170] [FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution](https://arxiv.org/abs/2512.01390)
*Seungho Choi,Jeahun Sung,Jihyong Oh*

Main category: cs.CV

TL;DR: FRAMER是一种可插拔训练方案，用于缓解扩散式真实图像超分中“低频偏置、逐层先低后高”的问题，通过频域蒸馏与对比学习提升高频细节还原，同时不改动骨干与推理流程。


<details>
  <summary>Details</summary>
Motivation: 现实退化未知/混合的Real-ISR中，扩散模型感知质量优于GAN，但存在低频偏置，导致高频纹理重建不足；扩散U-Net/DiT呈现深度上“先低频后高频”的层级特性，现有训练难以针对不同频段与层级进行精细监督。

Method: 提出FRAMER训练：在每个去噪步，使用最终层特征作为教师，蒸馏到所有中间层学生。教师与学生特征经FFT掩膜分解为LF/HF，分别对齐监督。LF采用Intra Contrastive Loss稳定全局结构；HF采用Inter Contrastive Loss利用随机层与batch内负样本强化实例化细节。并引入两种自适应调制器：FAW按层与频段重加权LF/HF信号，FAM按当前相似度门控蒸馏强度。整体为训练期方法，不改U-Net/DiT骨干或推理流程。

Result: 在U-Net与DiT（如Stable Diffusion 2/3）上，FRAMER一致提升重建与感知指标：PSNR/SSIM及LPIPS、NIQE、MANIQA、MUSIQ均有改进；消融验证了“最终层作教师”和“随机层负样本”的有效性。

Conclusion: 通过频域分解与对比蒸馏、结合自适应调制，FRAMER缓解扩散模型的低频偏置并增强高频细节，在不改变推理的前提下，为Real-ISR带来全面、稳健的画质提升。

Abstract: Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise "low-first, high-later" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.

</details>


### [171] [Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries](https://arxiv.org/abs/2512.01419)
*Tushar Pranav,Eshan Pandey,Austria Lyka Diane Bala,Aman Chadha,Indriyati Atmosukarto,Donny Soh Cheng Lock*

Main category: cs.CV

TL;DR: 提出RICE-VL基准与SEA-LAVE评测，用于衡量VLM在东南亚文化理解与定位上的能力；结果显示现有VLM在低资源国家与抽象文化领域存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型存在以西方为中心的偏见，难以适配东南亚等多元文化场景，缺乏系统评测其文化理解与定位能力的标准化数据与指标。

Method: 构建RICE-VL数据集：覆盖11个东盟国家，>28k人类标注VQA样本（判断、填空、开放问答）和1k视觉指代引导框对，14个子领域类别，由具备文化背景的专家标注；提出SEA-LAVE指标，扩展LAVE，从文本准确性、文化对齐度、国家识别三方面综合评分；对6个开源与闭源VLM进行评测，并加入视觉指引任务测试空间与语境定位能力。

Result: 多模型在低资源国家与抽象文化维度上表现明显不足；在视觉指引任务中对复杂场景中文化要素的空间与语境定位能力欠佳，暴露出现有VLM文化理解与泛化能力不足。

Conclusion: RICE-VL与SEA-LAVE为评估VLM文化能力提供了系统工具；当前VLM存在显著文化理解缺口，需更包容的数据与训练策略以服务全球多样化人群。

Abstract: Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA). To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries. RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories. We propose SEA-LAVE, an extension of the LAVE metric, assessing textual accuracy, cultural alignment, and country identification. Evaluations of six open- and closed-source VLMs reveal significant performance gaps in low-resource countries and abstract cultural domains. The Visual Grounding task tests models' ability to localize culturally significant elements in complex scenes, probing spatial and contextual accuracy. RICE-VL exposes limitations in VLMs' cultural comprehension and highlights the need for inclusive model development to better serve diverse global populations.

</details>


### [172] [MDiff4STR: Mask Diffusion Model for Scene Text Recognition](https://arxiv.org/abs/2512.01422)
*Yongkun Du,Miaomiao Zhao,Songlin Fan,Zhineng Chen,Caiyan Jia,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文将掩码扩散模型（MDM）首次用于场景文本识别（STR），提出MDiff4STR，通过对训练-推理噪声不匹配与推理过度自信两大问题的改进，实现以仅3步去噪达到或超越主流自回归模型的精度与速度。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型在STR上精度高但推理序列化、延迟大；MDM并行高效但在STR上精度落后。作者动机是：把MDM引入STR并针对其关键短板（训练/推理噪声差异与过度自信）做机制性改造，使其同时兼顾效率与准确率。

Method: 提出MDiff4STR：1）针对训练-推理“noising gap”，设计六种噪声策略，使训练更贴近推理过程；2）为抑制推理中过度自信的错误，引入“token-replacement”非掩码噪声，在解码过程中用替换而非纯掩码扰动，促使模型对可能错误的高置信预测进行复核与修正；3）整体沿用MDM的掩码扩散范式，仅需三步去噪即可完成识别。

Result: 在标准与挑战性STR基准（含不规则、艺术化、遮挡、中文、多/无预训练）上，MDiff4STR均优于主流STR模型，在准确率上超过最先进自回归方法，同时保持高速（仅3次去噪）。

Conclusion: 通过缩小训练/推理噪声差距与引入替换型噪声以缓解过度自信，MDiff4STR把MDM在STR上的准确率提升到SOTA水平，并保留并行、高效的推理优势；验证了MDM在视觉-语言识别任务上的可行性与潜力。

Abstract: Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.

</details>


### [173] [\textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models](https://arxiv.org/abs/2512.01424)
*Xusen Hei,Jiali Chen,Jinyu Yang,Mengchen Zhao,Yi Cai*

Main category: cs.CV

TL;DR: ViRectify是一个用于评估并提升多模态大模型在视频推理中“识错与改错”能力的基准与方法框架，含3万+样本、细粒度逐步纠错与证据定位要求，并提出基于轨迹与奖励的证据驱动纠错训练；在16个模型上验证其挑战性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基准侧重最终答案，对MLLM在复杂视频任务中的错误识别、传播分析与纠正能力缺乏系统性评测与训练信号，导致模型弱点难以暴露与改进。

Method: 1) 构建ViRectify数据集：AI辅助标注+人工校验，覆盖动态感知、科学推理、具身决策三大域，包含逐步错误识别、关键帧/时间戳证据定位与解释。2) 评测协议：要求模型按步骤定位错误、给出证据扎根的因果解释与纠正。3) 训练/推理框架：提出“轨迹证据驱动纠错”（错误轨迹分步标注+基于视觉证据的奖励建模），鼓励关注错误传播链与关键时间点。4) 在16个先进MLLM上进行系统评测与对比。

Result: 基线评测显示任务高度挑战：如GPT-5纠错准确率仅31.94%。所提框架使Qwen2.5-VL-7B在ViRectify上稳定优于72B变体，说明证据驱动的纠错训练显著提升小模型效率与性能；发现不同模型在纠错上存在系统性不对称与偏差。

Conclusion: ViRectify为评估与提升MLLM视频推理纠错能力提供了全面基准与方法，强制模型进行基于证据的逐步反思与修正，可作为反思学习的数据资源，并为未来改进视频推理模型提供方向。

Abstract: As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.

</details>


### [174] [ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers](https://arxiv.org/abs/2512.01426)
*Yiyang Ma,Feng Zhou,Xuedan Yin,Pu Cao,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: ResDiT是针对预训练Diffusion Transformer在超分辨率生成中布局坍塌与纹理退化的问题，提出的无需再训练、可高效扩展分辨率的方法：通过位置编码缩放修正高分辨率下的位置信息，并用基于基准分辨率的局部注意与拼接融合提升细节与消除栅格伪影。


<details>
  <summary>Details</summary>
Motivation: DiT在超过训练分辨率进行生成时，常出现空间布局错乱（坍塌）与细节纹理变差。以往方法依赖先在训练分辨率去噪再引导高分生成，流程复杂且效率受限。作者希望在不重新训练的前提下，直接从模型内部机制出发，找到导致问题的根因并提出简单高效的分辨率扩展策略。

Method: 1) 诊断：定位到位置编码（PE）在高分辨率外推时编码了错误的位置信息，是布局坍塌的核心因素。2) 提出PE缩放：在分辨率变化时按比例变换/重标定PE，使高分生成时的相对位置信号与训练时一致。3) 局部细节增强：利用基准分辨率的局部注意来提升高分细节。4) 补丁级融合：设计融合模块聚合全局（高分生成）与局部（基准分辨率）线索。5) 高斯加权拼接：对补丁边界进行高斯权重融合，去除网格伪影。全流程无需额外训练。

Result: 在多项评测中，ResDiT可稳定生成高保真、高分辨率图像，显著缓解布局坍塌与纹理退化，并能无缝用于下游如空间可控生成任务，展示出更优的质量与鲁棒性。

Conclusion: 导致DiT高分生成失败的关键在于位置编码外推失真。通过训练外的PE缩放与局部增强/融合与平滑拼接，ResDiT在不改动或再训练模型的情况下，实现高效、稳定的高分辨率生成，并兼容多种下游任务。

Abstract: Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.

</details>


### [175] [Language-Guided Open-World Anomaly Segmentation](https://arxiv.org/abs/2512.01427)
*Klara Reichard,Nikolas Brasch,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: 提出Clipomaly：基于CLIP的零样本开放世界与异常分割方法，可在自动驾驶场景中同时分割未知目标并给出可解释命名，且推理时可动态扩展词汇，无需异常特定训练数据，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界/异常分割能检测未知但无法赋予语义标签，且难以为未知类学习表征；开放词汇分割虽能泛化但需要固定词表，不适用于开放集合的异常检测。需要一种既能发现未知又能命名且无需再训练的方法。

Method: 利用CLIP的图文共享嵌入空间进行零样本分割：将图像像素/区域与文本嵌入对齐，在推理时动态扩展文本词汇，从而对未知对象进行分割并赋予人类可读的名称；无需异常特定训练数据，适配超出如Cityscapes等常见类定义的场景。

Result: 在既有的异常分割基准上取得SOTA性能，同时能对检测到的异常提供语义命名。

Conclusion: Clipomaly在开放世界与异常分割上实现了准确、可解释且灵活的零样本能力，适合实际自动驾驶部署，弥补了现有方法无法命名未知和词表受限的不足。

Abstract: Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.

</details>


### [176] [FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation](https://arxiv.org/abs/2512.01444)
*Jian Shu,Nanjie Yao,Gangjian Zhang,Junlong Ren,Yu Feng,Hao Wang*

Main category: cs.CV

TL;DR: 提出统一学习框架，端到端解决模板构建与目标姿态变形：用U-Net解耦纹理与姿态快速生成模板，并以数据驱动的精修增强结构一致性；在多姿态上兼顾效率与质量，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法需繁琐骨骼绑定，模板阶段易产生伪影；目标姿态阶段依赖LBS导致结构失真，影响真实感与泛化与效率，缺乏统一且高效的解决方案。

Method: 两阶段统一学习框架：1) 模板构建：采用U-Net式网络，在前馈过程中显式解耦纹理与姿态信息，快速生成无骨骼绑定的人体模板；2) 目标姿态变形：基于数据驱动的几何/形变精修模块，对初始变形结果进行结构一致性增强，缓解LBS类伪影与体积损失。

Result: 在多种姿态和数据集上进行广泛实验，模型在效率（推理速度/流程简化）与质量（几何完整性、结构一致性）间取得最优平衡，定量与定性均超越SOTA。

Conclusion: 解耦式模板生成+数据驱动精修的统一框架可稳定提升3D人体动画的真实性与鲁棒性，避免繁琐绑定与LBS失真，并在多姿态场景下实现高效高质的动画生成。

Abstract: 3D human avatar animation aims at transforming a human avatar from an arbitrary initial pose to a specified target pose using deformation algorithms. Existing approaches typically divide this task into two stages: canonical template construction and target pose deformation. However, current template construction methods demand extensive skeletal rigging and often produce artifacts for specific poses. Moreover, target pose deformation suffers from structural distortions caused by Linear Blend Skinning (LBS), which significantly undermines animation realism. To address these problems, we propose a unified learning-based framework to address both challenges in two phases. For the former phase, to overcome the inefficiencies and artifacts during template construction, we leverage a U-Net architecture that decouples texture and pose information in a feed-forward process, enabling fast generation of a human template. For the latter phase, we propose a data-driven refinement technique that enhances structural integrity. Extensive experiments show that our model delivers consistent performance across diverse poses with an optimal balance between efficiency and quality,surpassing state-of-the-art (SOTA) methods.

</details>


### [177] [CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball](https://arxiv.org/abs/2512.01478)
*Omer Sela,Michael Chertok,Lior Wolf*

Main category: cs.CV

TL;DR: 提出CourtMotion框架：用骨架GNN+带专用注意力的Transformer，将细粒度身体动作与战术语义对齐，用事件投影头预测传球/投篮/抢断等；在NBA数据上显著优于仅位置方法，轨迹误差降35%，多项下游任务大幅提升。


<details>
  <summary>Details</summary>
Motivation: 仅用球员位置难以理解篮球事件的语义与意图，忽视身体朝向、防守姿态、投篮准备等关键信号；需要能同时建模细粒度动作模式与多球员交互，从而提前预判比赛事件与战术展开。

Method: 两阶段：1) 将骨架级跟踪数据输入图神经网络，捕捉关节与肢体的时空细节；2) 用带专门交互注意力的Transformer建模球员间关系与战术上下文；提出“事件投影头”，把运动表征显式映射到传球、投篮、抢断等事件空间，并通过多任务训练将动作与战术目的对齐。

Result: 在NBA跟踪数据上，相比仅位置的SOTA模型，轨迹预测误差降低35%；在掩护检测、出手者识别、助攻预测、出手位置分类、出手类型识别等关键分析任务上均取得一致且显著的性能提升。

Conclusion: 融合骨架级动作与交互建模能更好地预测与解释篮球事件。CourtMotion作为预训练基础模型，为多种篮球分析下游任务提供了强大可迁移的表征，优于位置仅基线。

Abstract: This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.

</details>


### [178] [ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling](https://arxiv.org/abs/2512.01481)
*Qisen Wang,Yifan Zhao,Peisen Shen,Jialu Li,Jia Li*

Main category: cs.CV

TL;DR: ChronosObserver提出一种无需训练的方法，通过“世界状态超空间”统一表示4D场景的时空约束，并以“超空间引导采样”在扩散过程中同步多视角轨迹，从而生成高保真、三维一致、时间同步的多视角视频。


<details>
  <summary>Details</summary>
Motivation: 现有相机控制的视频生成模型难以直接扩展到既三维一致又时间同步的多视角视频生成。已有方法依赖数据增强或测试时优化，存在泛化与可扩展性不足的问题，亟需一种无需额外训练、能稳健处理4D时空一致性的方案。

Method: 提出ChronosObserver，包括两部分：1）世界状态超空间（World State Hyperspace，WSH），用于显式表示4D场景的时空约束与一致性；2）超空间引导采样（Hyperspace Guided Sampling，HGS），在扩散模型采样阶段利用WSH对多视角的采样轨迹进行同步与约束，从而维持跨视角与时间的一致性。整个流程为训练/微调无关的推理期方法。

Result: 实验显示，在无需训练或微调的情况下，方法能生成高保真、3D一致且时间严格同步的多视角视频，优于依赖数据增强或测试时优化的方案。

Conclusion: 通过在扩散采样中引入世界状态超空间的统一约束并进行轨迹同步，ChronosObserver在零训练设置下实现了对4D世界的高质量多视角视频生成，表明该思路具备良好的通用性与可扩展性。

Abstract: Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.

</details>


### [179] [A variational method for curve extraction with curvature-dependent energies](https://arxiv.org/abs/2512.01494)
*Majid Arthaud,Antonin Chambolle,Vincent Duval*

Main category: cs.CV

TL;DR: 提出一种基于能量离散化与Smirnov向量场分解的变分方法，从候选端点集中自动提取图像中的曲线与一维结构，并扩展到曲率依赖能量，通过在位置-方向空间的提升与次黎曼/芬斯勒度量求解。


<details>
  <summary>Details</summary>
Motivation: 传统图像中曲线/细线结构（道路、血管、边缘）自动提取常依赖强监督或启发式连通规则，难以稳健处理多端点匹配、噪声与拓扑变化。需要一种几何一致、可全局优化或近似全局的变分框架，既能自动选择端点对并形成曲线网络，又能融入曲率先验提升光滑性与感知一致性。

Method: 1) 定义以端点集为候选的曲线能量，并进行离散化；2) 利用Smirnov向量场分解定理，将曲线族表示为无散度流的分解，从而把曲线选取问题转化为凸或近凸的流/测度优化；3) 在此之上构建双层最小化：上层选择端点与结构，下层给定选择求解能量最小的曲线；4) 扩展到曲率依赖能量：将曲线提升到位置-方向SE(2)空间，引入次黎曼或芬斯勒度量以惩罚方向变化/曲率，采用相应的几何最短路径或测地线求解。

Result: 方法可在基本无监督条件下，从图像自动抽取连接给定端点集的曲线与一维结构；曲率项的引入提升了平滑性与感知连贯性，能更好地处理噪声与细弱边缘，得到更具几何一致性的网络。

Conclusion: 基于Smirnov分解的流表示与双层优化提供了一个稳健的曲线/1D结构提取框架，并可在SE(2)提升下自然融合曲率先验（次黎曼/芬斯勒度量）。该思路兼具几何严谨性与计算可行性，适合在道路、血管等细线结构提取中应用。

Abstract: We introduce a variational approach for extracting curves between a list of possible endpoints, based on the discretization of an energy and Smirnov's decomposition theorem for vector fields. It is used to design a bi-level minimization approach to automatically extract curves and 1D structures from an image, which is mostly unsupervised. We extend then the method to curvature-dependent energies, using a now classical lifting of the curves in the space of positions and orientations equipped with an appropriate sub-Riemanian or Finslerian metric.

</details>


### [180] [ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark](https://arxiv.org/abs/2512.01495)
*Joanne Lin,Ruirui Lin,Yini Li,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 提出ELVIS框架，将先进的视频实例分割(VIS)适配到低光场景：合成低光视频管线+无校准退化画像合成网络(VDP-Net)+增强解码头以分离退化与内容，在合成低光YouTube-VIS 2019上最高提升+3.7 AP。


<details>
  <summary>Details</summary>
Motivation: 低光视频存在噪声、模糊、低对比度等时空退化，缺乏大规模标注与能刻画时间退化的合成数据管线；现有VIS方法对低光退化鲁棒性差，即便在低光数据上微调也效果不佳，需要一种能有效迁移/适配到低光域的方法。

Method: 提出ELVIS框架：1) 无监督的合成低光视频管线，同时建模空间与时间退化；2) 校准无关的退化剖面合成网络VDP-Net，生成与视频匹配的退化配置/画像；3) 在VIS模型上加入增强解码头，将退化因素与内容特征解耦并进行增强，从而提升分割与跟踪性能。

Result: 在合成低光版的YouTube-VIS 2019数据集上，相比基线方法最高提升+3.7 AP，展示了对低光域适配的有效性。

Conclusion: 通过合成低光数据、退化画像建模与特征解耦增强三管齐下，ELVIS能有效将SOTA VIS模型适配至低光视频场景，显著提升性能；代码将开源。

Abstract: Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce \textbf{ELVIS} (\textbf{E}nhance \textbf{L}ow-light for \textbf{V}ideo \textbf{I}nstance \textbf{S}egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to \textbf{+3.7AP} on the synthetic low-light YouTube-VIS 2019 dataset. Code will be released upon acceptance.

</details>


### [181] [Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation](https://arxiv.org/abs/2512.01510)
*Franz Thaler,Martin Urschler,Mateusz Kozinski,Matthias AF Gsell,Gernot Plank,Darko Stern*

Main category: cs.CV

TL;DR: 提出SRCSM方法，实现医学图像分割的单源域泛化：训练仅用单一域（如CT），直接泛化到另一域（如MR）无适配与无目标域数据/标注。训练期通过“语义感知随机卷积”对不同语义区域进行差异化增强；测试期对目标域强度映射以更接近源域。跨模态、跨中心与心动周期/设备差异场景下显著优于现有DG方法，多场景接近甚至达到域内基线。


<details>
  <summary>Details</summary>
Motivation: 医学分割在跨模态（CT↔MR）、跨中心与设备/相位差异时性能急剧下降，而实际应用往往无法获取目标域图像或标注，也不允许上线后再做自适应。需要一种仅用单源域训练即可稳健泛化的方法。

Method: SRCSM包含两部分：1) 训练期的语义感知随机卷积（Semantic-aware Random Convolution, SRC）：依据像素/区域的标签对不同语义区域施加不同的随机卷积与强度扰动，扩大源域分布并打破语义-外观的耦合，从而学习与外观无关的判别特征。2) 测试期的强度映射（Style Mapping）：对目标域图像进行强度分布映射/标准化，使其外观更接近源域，作为推理时的轻量外观对齐；不需要目标域训练数据或参数更新。整体在标准分割网络上作为训练与推理策略插拔使用。

Result: 在腹部、全心脏、前列腺等数据集上的跨模态与跨中心泛化实验中，SRCSM在绝大多数对比中优于既有DG方法；在全心脏CT或MR训练、在不同硬件采集的cine MR舒张/收缩相位测试这一更具挑战的设置中，同样缩小了域间差距。部分场景中其分割性能可与域内训练基线持平。

Conclusion: 通过训练时语义感知多样化与测试时轻量外观对齐，SRCSM在单源域医学分割DG上达到新的SOTA，能在无目标域数据与无模型适配的条件下实现强泛化，并在若干设置中接近或达到域内基线表现。

Abstract: We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.

</details>


### [182] [QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions](https://arxiv.org/abs/2512.01519)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.CV

TL;DR: QuantumCanvas 将二体量子系统视为物质的基元，构建多模态（图像+数值）大规模基准，展示了以可解释的视觉表征学习可转移量子相互作用的能力，并在多项电子与能量属性上取得有竞争力误差，同时作为预训练提升下游任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有分子/材料机器学习多依赖整分子/晶体层面的相关性，缺乏物理可迁移性，未直接学习主导成键、荷重分布、杂化和耦合的二体相互作用；需要一种既物理扎实又可解释的表示与基准来推动可转移量子学习。

Method: 提出QuantumCanvas：以元素-元素二体为基本单元，覆盖2850种配对；为每对提供18种电子、热力学与几何属性，并构建10通道图像表征（来自l/m分辨轨道密度、角场变换、共占据图、电荷密度投影），在不显式坐标的前提下编码空间、角度与静电对称性。基于此进行多架构基准，并探索多模态融合与预训练-微调范式。

Result: 在18个目标上评测8种架构：能隙MAE 0.201 eV（GATv2），HOMO/LUMO分别0.265/0.274 eV（EGNN）；能量相关上DimeNet达总能2.27 eV MAE、排斥能0.132 eV MAE；多模态融合模型在Mermin自由能上2.15 eV MAE。基于该数据的预训练可提升在QM9、MD17、CrysMTM等更大数据集上的收敛稳定性与泛化。

Conclusion: 通过将轨道物理与视觉表征学习统一，QuantumCanvas提供了可解释且可迁移的量子相互作用学习基础；其多模态基准与预训练能力为分子与材料建模带来更强的物理归纳偏置与跨任务迁移潜力。

Abstract: Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs. Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems. We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter. The dataset spans 2,850 element-element pairs, each annotated with 18 electronic, thermodynamic, and geometric properties and paired with ten-channel image representations derived from l- and m-resolved orbital densities, angular field transforms, co-occupancy maps, and charge-density projections. These physically grounded images encode spatial, angular, and electrostatic symmetries without explicit coordinates, providing an interpretable visual modality for quantum learning. Benchmarking eight architectures across 18 targets, we report mean absolute errors of 0.201 eV on energy gap using GATv2, 0.265 eV on HOMO and 0.274 eV on LUMO using EGNN. For energy-related quantities, DimeNet attains 2.27 eV total-energy MAE and 0.132 eV repulsive-energy MAE, while a multimodal fusion model achieves a 2.15 eV Mermin free-energy MAE. Pretraining on QuantumCanvas further improves convergence stability and generalization when fine-tuned on larger datasets such as QM9, MD17, and CrysMTM. By unifying orbital physics with vision-based representation learning, QuantumCanvas provides a principled and interpretable basis for learning transferable quantum interactions through coupled visual and numerical modalities. Dataset and model implementations are available at https://github.com/KurbanIntelligenceLab/QuantumCanvas.

</details>


### [183] [Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling](https://arxiv.org/abs/2512.01533)
*Hailong Yang,Te Zhang,Kup-sze Choi,Zhaohong Deng*

Main category: cs.CV

TL;DR: 提出DFS：基于模糊规则引导的潜空间多路径扩散模型，解决异质特征难以统一建模与多路径协同低效、高计算的问题，在LSUN与MS COCO上实现更稳更快收敛、更优画质与文图对齐，并降低计算。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在处理特征差异巨大的图像集合时易出现冲突和复杂特征捕获不足；现有通过多路径学习不同区域的方案协同低效、计算开销高，需要一种既能分治异质特征、又能高效协调并降低成本的方法。

Method: 提出Diffusion Fuzzy System（DFS）：在潜空间中设置多条扩散路径，每条路径专注于一个类别的图像特征；用基于模糊规则链的推理动态调度与协调各路径；并通过基于隶属度的潜空间压缩机制减少计算开销。

Result: 在LSUN Bedroom、LSUN Church、MS COCO上，DFS相较单路径与多路径基线实现更稳定训练与更快收敛；生成图像质量、文本-图像对齐度、以及与目标参考比对的准确性均更高。

Conclusion: 通过将模糊规则与多路径扩散在潜空间结合，DFS有效建模异质图像特征并提高路径协同效率，同时降低计算成本，整体优于现有单/多路径扩散模型。

Abstract: Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.

</details>


### [184] [Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis](https://arxiv.org/abs/2512.01534)
*Alexander Frotscher,Christian F. Baumgartner,Thomas Wolfers*

Main category: cs.CV

TL;DR: 该研究构建了一个大规模多中心基准，用于评估脑MRI中的深度无监督异常检测，发现各算法分割性能差异大、对扫描仪与人群因素敏感，重建类（尤其扩散启发）方法分割更强，特征类方法更稳健；数据量增加收益有限，强调需要方法学改进与公平、稳健的临床转化策略。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测研究零散、数据与评测不统一，难以比较与临床转化；缺乏系统性评估其在不同设备、病灶类型/大小及人群变量下的鲁棒性与偏差。

Method: 构建多中心基准：6台扫描仪、不同年龄段健康受试者T1/T2影像用于训练（约6k张），独立验证集用于调参与阈值估计，大规模测试集覆盖健康与多种临床队列；比较重建类与特征类多种AD算法，采用Dice等指标，系统分析扫描仪、病灶特征与人口学变量对性能与偏差的影响。

Result: 总体Dice分割从0.03到0.65，差异显著；重建类尤其扩散式方法在病灶分割上最好；特征类在分布偏移下更稳健；存在系统性偏差（扫描仪效应、小/低对比病灶漏检、年龄/性别相关的误报率变化）；增加健康训练数据仅带来有限提升。

Conclusion: 当前无监督异常检测受限于算法而非数据规模。未来应聚焦：原生影像预训练、合理的偏差度量、考虑公平性的建模、以及稳健的域自适应。该基准为透明可比的后续研究与临床转化提供基础。

Abstract: Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.

</details>


### [185] [FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention](https://arxiv.org/abs/2512.01540)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: 提出FlashVGGT：用紧凑描述子跨注意力替代全局自注意，显著加速多视角3D重建并保持精度，支持长序列与在线推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的前馈3D重建方法（如VGGT）依赖对所有图像tokens的全局自注意，计算/显存随token数二次增长，难以处理上千张图像的长序列，推理缓慢且不易在线扩展。

Method: 以“描述子为核心”的注意力：每帧将空间信息压缩成少量描述子tokens；用全量图像tokens对这些描述子执行跨注意力，替代密集全局自注意，降低复杂度。设计chunk-recursive在线机制：长序列分块处理并缓存前块描述子，复用以实现可扩展推理。整体保持前馈重建框架。

Result: 在多视角3D重建上与VGGT精度相当；对1000张图像推理时间降至VGGT的9.3%；可扩展到超过3000张图像的序列。

Conclusion: 描述子压缩+跨注意力有效缓解自注意二次复杂度瓶颈，使前馈3D重建在长序列上实现高效、可扩展、接近SOTA精度的推理。

Abstract: 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.

</details>


### [186] [MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration](https://arxiv.org/abs/2512.01563)
*Thao Thi Phuong Dao,Tan-Cong Nguyen,Nguyen Chi Thanh,Truong Hoang Viet,Trong-Le Do,Mai-Khiem Tran,Minh-Khoi Pham,Trung-Nghia Le,Minh-Triet Tran,Thanh Dinh Le*

Main category: cs.CV

TL;DR: MasHeNe 提供了首个同时覆盖头颈部肿瘤与囊肿的对比增强CT像素级标注数据集（3,779张切片），并构建基线与评测；提出WEMF模型，通过三窗增强与多频注意力集成于U形Mamba骨干，实现当前最优分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据多聚焦恶性肿瘤，忽视其他占位性病变，缺乏统一基准导致方法难以公平比较，且头颈部解剖复杂、病灶多样，分割任务仍具挑战。

Method: 1) 数据集MasHeNe：包含肿瘤与囊肿，提供像素级标注与标准评测协议；2) 基线与指标：基于常见分割模型建立基准并报告Dice、IoU、NSD、HD95等；3) 模型WEMF：在U形Mamba骨干上，输入端进行三窗（tri-window）窗宽增强以丰富外观；在跳跃连接处引入多频注意力模块进行频域信息融合。

Result: 在MasHeNe上，WEMF优于其他方法：Dice 70.45%、IoU 66.89%、NSD 72.33%、HD95 5.12 mm，表现稳定且强。

Conclusion: MasHeNe为头颈部占位（不限恶性）分割提供了首个基准与数据资源；WEMF在该基准上取得SOTA。错误模式显示任务仍困难，需后续研究。

Abstract: Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git.

</details>


### [187] [RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions](https://arxiv.org/abs/2512.01582)
*Junran Peng,Yiheng Huang,Silei Shen,Zeji Wei,Jingwei Yang,Baojie Wang,Yonghao He,Chuanchen Luo,Man Zhang,Xucheng Yin,Wei Sui*

Main category: cs.CV

TL;DR: 提出RoleMotion大型人体动作数据集，围绕“场景-角色-功能”精细设计，含手身体全身动作与细粒度文本标注，并构建更强评测器，系统评测多种文生动方法，验证数据集在文本驱动全身生成上的质量与功能性。


<details>
  <summary>Details</summary>
Motivation: 现有文生动/社会场景数据集多由分散子集拼接，动作功能性弱、场景与角色不成体系；动作质量参差不齐，文本标注粗糙，难支持细粒度、全身（含手部）生成与可靠评测。

Method: 1）按“经典场景—功能角色—行为”三层设计，采集清洗10,296条高质量身体+手部动作；2）覆盖25个场景、110个角色、500+行为，并配27831条细粒度文本；3）构建比现有更强、经可靠性验证的评测器；4）在RoleMotion上系统评测多种text-to-motion方法，并研究身体与手部联动生成。

Result: RoleMotion在文本驱动全身（含手）动作生成任务上带来更高的质量与功能一致性；所提出评测器表现优于现有方法并具可靠性；对身体与手部协同生成的影响进行了实证分析。

Conclusion: 以场景与角色为核心的RoleMotion提供高质量、功能性强、细粒度标注的全身动作数据及可靠评测基准，推动文生动模型在多场景、角色化、手-身协同生成方面的发展。

Abstract: In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.

</details>


### [188] [Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation](https://arxiv.org/abs/2512.01589)
*Thao Thi Phuong Dao,Tan-Cong Nguyen,Trong-Le Do,Truong Hoang Viet,Nguyen Chi Thanh,Huynh Nguyen Thuan,Do Vo Cong Nguyen,Minh-Khoi Pham,Mai-Khiem Tran,Viet-Tham Huynh,Trong-Thuan Nguyen,Trung-Nghia Le,Vo Thanh Toan,Tam V. Nguyen,Minh-Triet Tran,Thanh Dinh Le*

Main category: cs.CV

TL;DR: 提出AbscessHeNe：一个包含4,926张对比增强头颈CT、带像素级标注与临床元数据的脓肿数据集；基线分割模型表现较低（Dice 0.39），显示任务困难并期待后续研究；数据将公开发布。


<details>
  <summary>Details</summary>
Motivation: 头颈部脓肿若延误诊治可致败血症或死亡；影像上准确识别与勾画病灶边界对诊断、治疗与手术规划至关重要，但目前缺乏高质量、系统标注的数据集与可靠分割基线。

Method: 构建并发布一个经临床确诊病例的头颈部脓肿CT数据集AbscessHeNe，含4,926张增强CT切片、像素级病灶与深颈间隙受累标注及临床元数据；评测多类先进分割架构（CNN、Transformer、Mamba）。

Result: 最佳模型分割指标：Dice 0.39、IoU 0.27、NSD 0.67，显示当前方法对该任务效果有限、难度较高。

Conclusion: AbscessHeNe为头颈部脓肿分割与深颈间隙评估提供基准与资源，可推动更鲁棒模型与下游检索、知识驱动工作流的研究；数据集将公开获取。

Abstract: Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.

</details>


### [189] [Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager](https://arxiv.org/abs/2512.01611)
*Fengfeng Li,Zhou Feng,Hongliang Wu,Hao Zhang,Han Tian,Peng Liu,Lixin Yuan*

Main category: cs.CV

TL;DR: 提出基于ShapeDTW的井壁成像深度匹配方法，通过局部形状特征（HOG1D+原始信号）构建形态敏感距离矩阵，实现上/下垫片图像在复杂纹理与尺度失配下的精确对齐，并具备可扩展特征框架。


<details>
  <summary>Details</summary>
Motivation: OBM微电阻率成像仪的上下垫片采用交错设计，尽管做了速度校正，仍存在垫片图像间的深度错位，影响地质解释与成像拼接质量。需要一种能够在复杂纹理、局部尺度变化下仍能保持结构相似性的稳健对齐方法。

Method: 采用Shape Dynamic Time Warping（ShapeDTW）进行序列对齐：提取局部形状特征以构造形态敏感距离矩阵，具体以一维方向梯度直方图（HOG1D）与原始信号组合为形状描述子，提升对纹理与局部尺度变化的鲁棒性，并执行深度匹配。框架允许替换/扩展特征描述子以适配特定地质特征。

Result: 现场数据测试显示：该方法能在复杂纹理、深度偏移或局部缩放条件下实现上下垫片成像的高精度对齐，优于仅基于幅值或传统DTW的对齐表现。

Conclusion: 基于ShapeDTW的深度匹配方法在OBM微电阻率成像的垫片间对齐中效果显著，既提高对齐精度又保持结构相似性；其特征可扩展框架为针对特定地质目标的定制化提供了可能。

Abstract: In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction. This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm. The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment. We implement this by employing a combined feature set of the one-dimensional Histogram of Oriented Gradients (HOG1D) and the original signal as the shape descriptor. Field test examples demonstrate that our method achieves precise alignment for images with complex textures, depth shifts, or local scaling. Furthermore, it provides a flexible framework for feature extension, allowing the integration of other descriptors tailored to specific geological features.

</details>


### [190] [SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge](https://arxiv.org/abs/2512.01629)
*Yumeng He,Ying Jiang,Jiayin Lu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: SPARK 从单张RGB图像重建可物理仿真的关节式三维物体：用VLM粗估URDF与部件参考图，引导扩散Transformer生成部件与整体几何，并通过可微前向运动学与可微渲染在“打开态”监督下优化关节参数，最终产出可用于仿真的高质量资产。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能与机器人需要大量可交互的关节式三维资产，但手工建模部件层次与运动结构成本高、专业性强。缺乏从稀少观测（如单图）获得“仿真就绪”的形状与URDF。

Method: 1) 用VLM从输入图像抽取粗URDF（关节类型、轴、原点等）并生成部件级参考图；2) 结合部件图与结构图，将其输入生成式扩散Transformer，同步合成一致的部件形状与整体形状；3) 引入可微前向运动学与可微渲染，在VLM生成的开合状态监督下，对URDF关节类型、轴与原点进行连续—离散联合优化与细化。

Result: 在多类别数据上，SPARK 生成高质量、结构一致、可物理仿真的关节式资产，能直接用于下游机器人抓取/操作与交互建模，优于现有基线。

Conclusion: 通过把VLM先验、扩散Transformer生成与可微物理/渲染优化相结合，SPARK 实现从单张图像到仿真就绪的部件级关节物体重建，显著降低资产制作成本并提升下游应用可用性。

Abstract: Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.

</details>


### [191] [Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2512.01636)
*Xin Wang,Haipeng Zhang,Mang Li,Zhaohui Xia,Yueguo Chen,Yu Zhang,Chunyu Wei*

Main category: cs.CV

TL;DR: 提出Fusion-Diff，一种用于零样本组合图像检索（ZS-CIR）的生成式编辑框架，通过在联合视觉-语言空间进行融合特征编辑与轻量Control-Adapter微调，在200K合成数据上即可取得SOTA，显著缩小模态鸿沟并提升CIRR、FashionIQ、CIRCO表现。


<details>
  <summary>Details</summary>
Motivation: ZS-CIR需要不依赖昂贵三元组标注实现精细化检索，但现有文本中心或扩散式方法难以有效对齐视觉与语言模态，导致性能受限。因此需要一种既高效又能缩小模态差距的对齐与编辑机制。

Method: 1) 在联合视觉-语言空间进行多模态融合特征编辑（将参考图像与文本修改融合为可编辑的表示），以缩小模态差距；2) 设计轻量的Control-Adapter，利用仅约20万规模的合成数据进行微调，提高数据效率与泛化；3) 可视化融合后的多模态表示以提升可解释性。

Result: 在标准CIR基准（CIRR、FashionIQ、CIRCO）上，相比以往零样本方法取得显著提升，达成SOTA；在数据量仅200K的条件下仍能获得高性能。

Conclusion: 融合式生成编辑与轻量适配的组合能有效弥合视觉-语言鸿沟，在低标注/低数据条件下实现强大的ZS-CIR性能，并具备更好的可解释性。

Abstract: Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.

</details>


### [192] [ViT$^3$: Unlocking Test-Time Training in Vision](https://arxiv.org/abs/2512.01643)
*Dongchen Han,Yining Li,Tianyu Li,Zixuan Cao,Ziming Wang,Jun Song,Yu Cheng,Bo Zheng,Gao Huang*

Main category: cs.CV

TL;DR: 论文系统性研究视觉序列建模中的测试时训练（TTT），提出设计原则并据此构建ViT^3，在多任务上以线性复杂度达到或优于现有线性模型，逼近强大Transformer。


<details>
  <summary>Details</summary>
Motivation: TTT将注意力重构为测试时的在线学习，带来灵活设计与线性复杂度，但视觉场景下如何选择内模块与内训练策略缺乏系统认识与实践准则，阻碍高效而强大的视觉TTT模型设计。

Method: 开展大规模实证研究，比较不同TTT内模型与训练策略，提炼六条实用洞见并据此构建纯TTT架构ViT^3：以键值对在测试时构建紧凑内模型，实现并行计算与线性复杂度；在图像分类、生成、检测、分割等任务上统一评测。

Result: ViT^3在多种视觉任务上稳定匹配或超过先进线性复杂度模型（如Mamba、线性注意力变体），并显著缩小与高度优化的Vision Transformer的性能差距。

Conclusion: 合理的TTT设计可在保持线性复杂度与可并行性的同时取得强竞争力；所提六项原则与ViT^3基线为后续视觉TTT研究与应用提供方向与参考。

Abstract: Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.

</details>


### [193] [DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation](https://arxiv.org/abs/2512.01657)
*Hongyu Xu,Panpan Meng,Meng Wang,Dayu Hu,Liming Liang,Xiaoqi Sheng*

Main category: cs.CV

TL;DR: 提出DB-KAUNet，一种结合CNN、Transformer与KAN思想的自适应双分支UNet，用于视网膜血管分割；通过跨分支通道交互与几何自适应的空间特征增强，强化血管形态、抑制噪声；在DRIVE、STARE、CHASE_DB1上取得领先与稳健表现。


<details>
  <summary>Details</summary>
Motivation: 传统CNN难以建模长程依赖与复杂非线性关系，导致在细长、稀疏且形态复杂的视网膜血管分割任务中表现受限；需要一种既能捕捉全局依赖又能精细表达几何结构、同时控制计算成本的方法。

Method: 提出DB-KAUNet：1) 异构双分支编码器HDBE并行CNN与Transformer路径，交替引入KANConv与KAT块以增强非线性表达与全局建模；2) 跨分支通道交互CCI促进两路特征的信息互补；3) 基于注意力的空间特征增强SFE融合并强化空间细节；4) 进一步提出具几何自适应融合的SFE-GAF，引入自适应采样关注真实血管形态，提升显著血管、抑制背景并降低计算；5) 基于UNet解码进行重建。

Result: 在DRIVE、STARE、CHASE_DB1三个数据集上进行大量实验，DB-KAUNet在分割精度等指标上达到或超过现有方法，并显示出对噪声与形态变化的鲁棒性与较低的计算开销。

Conclusion: 将CNN局部细节与Transformer全局依赖结合，并以KAN相关块与自适应几何融合增强非线性与形态建模，可有效改进视网膜血管分割；所提DB-KAUNet在多个基准上领先且稳健，适合临床应用场景。

Abstract: Accurate segmentation of retinal vessels is crucial for the clinical diagnosis of numerous ophthalmic and systemic diseases. However, traditional Convolutional Neural Network (CNN) methods exhibit inherent limitations, struggling to capture long-range dependencies and complex nonlinear relationships. To address the above limitations, an Adaptive Dual Branch Kolmogorov-Arnold UNet (DB-KAUNet) is proposed for retinal vessel segmentation. In DB-KAUNet, we design a Heterogeneous Dual-Branch Encoder (HDBE) that features parallel CNN and Transformer pathways. The HDBE strategically interleaves standard CNN and Transformer blocks with novel KANConv and KAT blocks, enabling the model to form a comprehensive feature representation. To optimize feature processing, we integrate several critical components into the HDBE. First, a Cross-Branch Channel Interaction (CCI) module is embedded to facilitate efficient interaction of channel features between the parallel pathways. Second, an attention-based Spatial Feature Enhancement (SFE) module is employed to enhance spatial features and fuse the outputs from both branches. Building upon the SFE module, an advanced Spatial Feature Enhancement with Geometrically Adaptive Fusion (SFE-GAF) module is subsequently developed. In the SFE-GAF module, adaptive sampling is utilized to focus on true vessel morphology precisely. The adaptive process strengthens salient vascular features while significantly reducing background noise and computational overhead. Extensive experiments on the DRIVE, STARE, and CHASE_DB1 datasets validate that DB-KAUNet achieves leading segmentation performance and demonstrates exceptional robustness.

</details>


### [194] [Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery](https://arxiv.org/abs/2512.01665)
*Zhicheng Zhao,Yin Huang,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出ScaleBridge-Det：首个面向遥感微小目标的大模型式检测框架，通过尺度自适应专家路由与密度引导查询分配，在多尺度（密集微小与大型目标共存）上取得平衡并达SOTA与更强跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中目标尺度跨度极大、密度分布不均，微小目标与大目标共存使检测难以兼顾；现有大模型/基础模型虽在通用视觉任务表现强，但直接用于微小目标检测效果欠佳，存在“偏向主导尺度”的问题。

Method: 1) 提出ScaleBridge-Det大检测框架。2) 路由增强混合注意力（REM）：引入尺度专家（Mixture-of-Experts），通过自适应路由动态选择与融合不同尺度的专家特征，缓解MoE偏向主导尺度；获得兼顾微小与大型目标的判别性多尺度表示。3) 密度引导动态查询（DGQ）：预测目标密度以自适应调整查询的位置与数量，实现对不同尺度与密度目标的资源分配优化。

Result: 在AI-TOD-V2与DTOD取得SOTA；在VisDrone跨域测试表现出更优的鲁棒性。

Conclusion: ScaleBridge-Det通过尺度自适应专家路由与密度引导查询机制，有效弥合极端尺度差异，在不牺牲大型/通用目标性能的情况下显著提升密集微小目标检测，并具备良好的跨域泛化。

Abstract: Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.

</details>


### [195] [GRASP: Guided Residual Adapters with Sample-wise Partitioning](https://arxiv.org/abs/2512.01675)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: GRASP 通过样本簇划分+残差适配器微调，缓解扩散模型在长尾数据中的梯度冲突，显著提升尾类质量与多样性，并带来下游分类增益，通用且轻量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在长尾场景（如医疗稀有病灶）易发生模式坍塌，尾类生成质量差且多样性不足；主流采样/条件方法只在推理阶段引导，不改变已学到的分布，无法解决训练过程中头尾类别梯度冲突这一根因。

Method: 提出 GRASP：1) 使用外部先验对样本进行静态聚类划分，最小化簇内梯度冲突；2) 在 Transformer 的 FFN 中为每个簇注入簇特定的残差适配器进行微调；3) 绕过已有门控机制以确保稳定与高效；整体作为对预训练扩散模型的轻量插件，与现有管线易集成。

Result: 在 MIMIC-CXR-LT 上，GRASP 的 FID 与多样性指标显著优于基线（普通微调、MoE 变体），对尾类提升尤为明显；在 NIH-CXR-LT 的下游分类任务中，尾标签性能显著提升；在 ImageNet-LT 上同样取得良好泛化表现。

Conclusion: 通过基于样本簇的残差适配器微调，直接缓解头尾类别的梯度冲突，可在不显著增加开销的情况下，提升长尾扩散生成质量与多样性，并带来下游任务收益，方法通用、可扩展、易集成。

Abstract: Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.

</details>


### [196] [Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation](https://arxiv.org/abs/2512.01677)
*Haodong Yan,Hang Yu,Zhide Zhong,Weilin Yuan,Xin Gong,Zehang Luo,Chengxi Heyu,Junfeng Li,Wenxuan Song,Shunbo Zhou,Haoang Li*

Main category: cs.CV

TL;DR: 提出一种无需3D标注的“结构与接触感知”表征与联合生成范式，用于生成物理逼真、时序连贯的人手-物体交互视频，优于SOTA并具备开放域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有HOI视频生成在2D/3D表征上存在两难：2D可扩展但交互物理不精确，3D保真但数据与计算代价高且难以泛化，导致接触/遮挡等物理约束难以建模。

Method: 1) 设计结构与接触感知表征（SCAR），在无3D标注情况下捕获三类关键信息：手-物体接触、手-物体遮挡、整体结构上下文；作为可扩展的监督信号。2) 提出共享-专精的联合生成范式：一套共享骨干生成两条分支，分别生成交互导向表征与视频，相互促进以习得细粒度交互物理与时序一致性。

Result: 在两个真实数据集上，生成的HOI视频在物理逼真度与时序一致性上超过SOTA；并在开放域场景中展现强泛化与鲁棒性。

Conclusion: 在不依赖3D标注的前提下，SCAR表征与联合生成策略有效解决了2D/3D两难，显著提升HOI视频的物理与时序质量，并具备良好的可扩展性与开放域泛化。

Abstract: Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.

</details>


### [197] [Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies](https://arxiv.org/abs/2512.01681)
*Farzaneh Seyedshahi,Francesca Damiola,Sylvie Lantuejoul,Ke Yuan,John Le Quesne*

Main category: cs.CV

TL;DR: 提出一种将基于切除标本训练的自监督编码器迁移到小活检中的方法，用以自动提取形态学特征，实现间皮瘤亚型分类与生存预测，从而辅助临床诊疗。


<details>
  <summary>Details</summary>
Motivation: 现实临床中间皮瘤多以小活检为主，而多数计算病理模型依赖大幅面切除标本训练，限制了在真实场景中的可用性。因此需要一种能在活检尺度上仍发挥作用、并能支持预后评估与亚型判别的模型。

Method: 先在切除组织的全幅病理图像上进行自监督学习训练图像编码器；随后将该编码器直接应用于活检样本以提取形态学表示，并基于这些表示构建下游任务（生存预测与亚型分类）模型。

Result: 在活检材料上，编码器提取的形态学模式具有可迁移性，能够有效区分肿瘤亚型并对患者生存进行预测，表现出临床相关的预测能力。

Conclusion: 自监督学习得到的切除标本编码器可泛化至活检样本，支持间皮瘤的诊断分型与预后评估，显示AI在实际临床流程中的应用潜力。

Abstract: Accurate subtype classification and outcome prediction in mesothelioma are essential for guiding therapy and patient care. Most computational pathology models are trained on large tissue images from resection specimens, limiting their use in real-world settings where small biopsies are common. We show that a self-supervised encoder trained on resection tissue can be applied to biopsy material, capturing meaningful morphological patterns. Using these patterns, the model can predict patient survival and classify tumor subtypes. This approach demonstrates the potential of AI-driven tools to support diagnosis and treatment planning in mesothelioma.

</details>


### [198] [DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models](https://arxiv.org/abs/2512.01686)
*Patrick Kwon,Chen Chen*

Main category: cs.CV

TL;DR: DreamingComics是一个“带布局意识”的故事可视化框架，基于视频DiT引入空间-时间先验，实现角色与风格一致；通过RegionalRoPE和掩码条件损失实现按布局区域的精确控制；再用LLM生成漫画式布局，从自然语言脚本到可控版面。实验显示角色一致性+29.2%、风格相似度+36.2%，且空间准确度高。


<details>
  <summary>Details</summary>
Motivation: 现有故事可视化方法主要依赖文本定位主体，难以保证画面角色与风格在多个画格中的一致性，且缺乏对版面/布局的精确空间控制。需要一种既能保持身份/风格一致，又能按指定布局生成漫画分镜的方案。

Method: 1) 以预训练视频扩散Transformer为基础，引入时空先验提高身份与风格一致性；2) 提出RegionalRoPE：根据目标布局对位置编码进行区域重索引，从而实现按区域的精确条件控制；3) 探索掩码条件损失，使每个主体的视觉特征被约束在对应布局区域内；4) 训练一个LLM式布局生成器，从自然语言脚本推断漫画风格布局，提供灵活可控的布局条件。

Result: 相较以往方法，角色一致性提升29.2%，风格相似度提升36.2%，并保持高空间准确度；定性展示表明各画格间身份与风格更稳定，位置控制更精确。

Conclusion: 通过将视频DiT的时空先验、区域感知位置编码与掩码条件约束结合，并引入LLM生成的漫画布局条件，DreamingComics在故事可视化中显著提高了角色与风格一致性及空间可控性，为从脚本到漫画分镜的可控生成提供了有效框架。

Abstract: Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/

</details>


### [199] [SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation](https://arxiv.org/abs/2512.01701)
*Xiuli Bi,Die Xiao,Junchao Fan,Bin Xiao*

Main category: cs.CV

TL;DR: 提出SSR方法用于CLIP驱动的弱监督语义分割，通过语义与空间双重校正缓解非目标前景与背景过激活，PASCAL VOC/MS COCO上达79.5/50.6 mIoU，优于单/多阶段方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的WSSS常出现：1) 跨模态语义对齐不足导致不同类别特征重叠，引发非目标前景过激活；2) 空间先验利用不足，导致背景区域被错误激活。需要一种同时在语义和空间层面校正的框架。

Method: 提出SSR，包括两大组件：1) 语义层的跨模态原型对齐（CMPA）：构建图文原型并进行对比学习，对齐跨模态特征空间，降低类间重叠、增强语义相关性，从而抑制非目标前景激活；2) 空间层的超像素引导校正（SGC）：利用超像素产生的空间先验，在亲和传播过程中精确滤除非目标区域干扰，显著纠正背景过激活。整体在CLIP框架上进行弱监督训练。

Result: 在PASCAL VOC与MS COCO上取得mIoU 79.5%和50.6%，超越所有单阶段方法，并优于更复杂的多阶段方法。

Conclusion: 语义-空间双重校正有效缓解CLIP在WSSS中的过激活问题；跨模态原型对齐与超像素引导亲和传播相辅相成，显著提升分割精度，具备简洁高效且可推广的优势。

Abstract: In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.

</details>


### [200] [StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos](https://arxiv.org/abs/2512.01707)
*Daeun Lee,Subhojyoti Mukherjee,Branislav Kveton,Ryan A. Rossi,Viet Dac Lai,Seunghyun Yoon,Trung Bui,Franck Dernoncourt,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出StreamGaze基准，评估多模态大模型在流式视频中利用人眼注视（gaze）进行时间与前瞻性推理的能力；构建凝视引导的过去/现在/前瞻任务与QA生成管线，显示SOTA与人类差距显著，并分析失败模式与改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频基准主要考察时间推理，但缺乏对“如何解释并利用人类凝视信号”的评估；实际应用（如AR眼镜）需要模型在仅见到过去与当前帧时追踪用户注意力并预测意图，因此需要一个凝视引导的评测框架。

Method: 提出StreamGaze，包括三类任务（gaze-guided past、present、proactive），要求模型在实时流中利用注视轨迹完成时空对齐与意图推断。为构建数据，设计凝视-视频QA生成管线：从原始凝视轨迹中抽取凝视停留（fixation），进行区域特定视觉提示（region-specific prompting），并构造注视路径（scanpath），以生成紧贴人类感知动态的时空落地QA对。

Result: 在人类与多种SOTA MLLM对比中，所有任务上均存在显著性能差距，暴露出当前模型在凝视驱动的时间推理、意图建模与前瞻预测方面的根本局限。并报告不同凝视提示策略、推理行为与任务特定失败模式的细粒度分析。

Conclusion: StreamGaze填补了流式视频理解中对凝视利用能力的评测空白，揭示当前MLLM的不足并指向未来模型需强化的关键能力；数据与代码将公开，期望推动凝视引导的流式视频理解研究。

Abstract: Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.

</details>


### [201] [FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing](https://arxiv.org/abs/2512.01755)
*Yucheng Liao,Jiajun Liang,Kaiqian Cui,Baoquan Zhao,Haoran Xie,Wei Liu,Qing Li,Xudong Mao*

Main category: cs.CV

TL;DR: FreqEdit是一个无需训练的多轮图像编辑框架，通过高频特征注入、自适应空间调制与路径补偿，显著缓解多次编辑导致的画质劣化，稳定支持10+轮编辑，在身份保持与指令遵循上优于7个SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑在单步效果好，但多轮编辑会逐步丢失细节与清晰度，核心问题是高频信息在迭代中被持续抹除，导致身份偏移与纹理退化。需要一种既能保持细节又能精确控制编辑区域的稳定方案。

Method: 提出FreqEdit，一个无需训练的框架，包含三部分：1) 高频特征注入：从参考的“速度场”（如扩散过程中的更新方向/残差）中注入高频分量以保留细粒度纹理与身份特征；2) 自适应注入策略：对注入强度进行空间调制，实现对编辑相关区域强注入、无关区域弱注入；3) 路径补偿：周期性地校准编辑轨迹，避免累计约束导致的偏移与过度编辑。整体可在现有扩散式编辑流程上即插即用。

Result: 在超过10轮连续编辑场景中保持稳定画质，显著优于7个SOTA基线；在身份保持与指令遵循两类指标上取得更好分数，展示更清晰的高频细节与更准确的区域控制。

Conclusion: 多轮编辑劣化的关键在于高频信息的渐进流失。FreqEdit通过高频注入、空间自适应与路径补偿三者协同，有效维持细节并增强可控性，实现训练-free、可扩展的稳定多轮图像编辑。

Abstract: Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.

</details>


### [202] [HiconAgent: History Context-aware Policy Optimization for GUI Agents](https://arxiv.org/abs/2512.01763)
*Xurui Zhou,Gongwei Chen,Yuquan Xie,Zaijing Li,Kaiwen Zhou,Shuai Wang,Shuo Yang,Zhuotao Tian,Rui Shao*

Main category: cs.CV

TL;DR: 提出HiconAgent与HCPO方法，通过动态上下文采样与锚点引导的历史压缩，高效利用GUI任务中的历史信息，显著提升精度并减少计算。


<details>
  <summary>Details</summary>
Motivation: GUI智能体在多步导航中需要历史上下文，但简单使用完整历史会导致计算开销大且信息噪声多，影响决策效率与效果。因此需要一种既能利用关键信息又能控制成本的历史使用机制。

Method: 提出History Context-aware Policy Optimization (HCPO)，包含两部分：1) 动态上下文采样（DCS）：在采样过程中为智能体提供可变长度的历史，使其自动适配最相关的上下文；2) 锚点引导的历史压缩（AHC）：在策略更新时采用双分支，一支压缩掉历史观测但保留历史动作作为“信息流锚点”，另一支保留完整历史；通过历史增强的一致性对齐损失将两分支耦合，鼓励在效率与效果间取得平衡。

Result: 在GUI-Odyssey上，HiconAgent-3B比GUI-R1-7B提升+8.46%定位准确率和+11.32%步骤成功率；在AndroidControl与AITW上达到相当性能，同时实现最高2.47倍推理加速与60% FLOPs降低。

Conclusion: HCPO使智能体能有选择地利用历史信息，在保证或提升性能的同时显著降低计算成本；HiconAgent在多项GUI基准上验证了有效性与效率优势。

Abstract: Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.

</details>


### [203] [VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis](https://arxiv.org/abs/2512.01769)
*Hafsa Billah*

Main category: cs.CV

TL;DR: 提出一个通用视频情境分析（VSA）框架：先用先进的内容抽取获取对象与事件，再以扩展关系模型（R++）与图模型双表示，结合连续查询语言与图算法，跨领域以参数化模板检测多种情境，并在AL/CM/SL三域多视频上验证准确性、效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频分析能做对象检测/跟踪等内容抽取，但“情境/活动”识别仍依赖人工或为特定场景定制的算法，通用性差、维护成本高，难以跨域复用。需要一个统一、可扩展、通用的VSA方法。

Method: 1) 先进行一次性视频内容抽取；2) 用两种表示：扩展关系模型（R++）将结果作为数据流，配套连续查询语言进行持续查询；图模型用于表达/检测关系复杂、关系型难以捕捉的情境；3) 结合现有与新开发的图算法进行情境检测；4) 通过跨域的“原始情境变体”参数化模板实现域无关；5) 在AL、CM、SL三个领域的不同长度视频上实验评估。

Result: 框架能够在多个领域的多种情境上运行，展示了较好的准确性、效率与鲁棒性；图模型与R++互补，连续查询语言支持在线检测；参数化模板使情境检测可跨域迁移。（具体数值未在摘要中给出）

Conclusion: 该通用VSA框架通过“内容一次抽取、双模型表示、连续查询+图算法、参数化模板”实现跨域情境检测，相比手工或定制算法更通用、可扩展且高效，并在多域数据上验证了其有效性。

Abstract: Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.
  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.

</details>


### [204] [Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels](https://arxiv.org/abs/2512.01771)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Malik Galijasevic,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 提出一种将可学习边缘卷积核与学习型刚性/非刚性配准相结合的方法，在多数据集和多设置下均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像配准在多模态对比差异、空间畸变和模态特异性变化下表现欠佳；需要更稳健的结构特征以提升配准鲁棒性与准确性。

Method: 以预定义的边缘检测核作为初始化（并加入随机扰动），在端到端训练中使这些“可学习边缘核”自适应更新，用于提取对配准最关键的结构边缘特征；同时设计了四个刚性与四个非刚性变体以进行消融/组件贡献分析；在三种设置（有/无去颅骨的刚性配准、非刚性配准）以及两个公开数据集上评估。

Result: 在内部医学院数据集的三种设置与两套公开数据上，所提方法在各项指标上稳定超越当前SOTA方法，表现出更好的多模态对齐与结构识别能力。

Conclusion: 可学习的自适应边缘检测与学习型配准深度结合，能显著提升刚性与非刚性多模态医学图像配准效果，具有推广潜力并为解剖结构分析提供更可靠的基础。

Abstract: Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.

</details>


### [205] [Evaluating SAM2 for Video Semantic Segmentation](https://arxiv.org/abs/2512.01774)
*Syed Hesham Syed Ariff,Yun Liu,Guolei Sun,Jing Yang,Henghui Ding,Xue Geng,Xudong Jiang*

Main category: cs.CV

TL;DR: 论文探索将SAM2扩展到视频语义分割（VSS），提出两条管线：基于SAM2掩码与并行语义网络的融合，以及基于SAM2掩码提取特征再分类的组合。实验显示SAM2凭借精确边界提升VSS表现。


<details>
  <summary>Details</summary>
Motivation: VSS需要同时满足空间精度、时间一致性、以及多目标复杂边界与尺度变化的跟踪，现有方法难以兼顾。SAM2在视频对象分割上表现强，具备时序记忆与精细边界，但直接用于密集语义分割存在差距，促使作者探索如何将其能力迁移到VSS。

Method: 提出两种扩展策略：1）以SAM2在单帧抽取独特对象掩码，同时用语义分割网络产生初始预测并相互细化，结合SAM2的边界优势与语义网络的类别判定。2）利用SAM2预测掩码在每个对象上抽取特征向量，输入一个轻量分类器获得类别，再将分类结果与掩码融合生成最终分割。两种方案都依赖SAM2的时序记忆在视频中传播掩码。

Result: 实验表明，引入SAM2能提升VSS整体性能，主要体现在对象边界的精度提高；两条管线均有效，但以边界质量提升为主要增益来源。

Conclusion: SAM2可作为VSS的有力基础模块，通过掩码引导与特征提取—分类的组合提升密集视频语义分割，尤其在边界处理上受益显著；仍需进一步解决时空一致性与多尺度多目标的稳定跟踪等挑战。

Abstract: The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.

</details>


### [206] [Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks](https://arxiv.org/abs/2512.01788)
*Christian Mollière,Iker Cumplido,Marco Zeulner,Lukas Liesenhoff,Matthias Schubert,Julia Gottfriedsen*

Main category: cs.CV

TL;DR: 评估在卫星遥感场景中，用任务定制的学习式压缩替代传统JPEG2000，以减少数据量且保持下游分割性能。学习式方法在多通道光学影像上同时提升PSNR与分割精度；在小规模单通道热红外数据上传统编解码仍有竞争力；端到端联合优化未优于分别优化。


<details>
  <summary>Details</summary>
Motivation: 卫星EO数据规模迅速增长导致传输与存储压力巨大。通用图像压缩不一定保留下游任务（如火情、云、建筑分割）所需的判别信息，亟需评估任务特定的学习式压缩能否在降低比特率的同时保持甚至提升任务性能。

Method: 对比传统JPEG 2000与学习式压缩（使用Discretized Mixed Gaussian Likelihood作为重建似然）在三项分割任务（火、云、建筑检测）上的表现。数据涵盖大规模多通道光学与小规模单通道热红外。评估指标包括重建PSNR与下游分割准确度，并测试压缩与分割模型的端到端联合训练与独立训练两种方案。

Result: 在大规模、多通道光学影像上，学习式压缩在PSNR与分割精度上均优于JPEG 2000；在小规模、单通道热红外数据上，由于数据量有限与模型架构限制，传统JPEG 2000仍具竞争力。端到端联合优化未带来超过独立优化的收益。

Conclusion: 任务定制的学习式压缩对多通道光学EO分割有明显优势，但其收益依赖于数据规模与通道丰富度；在资源受限或单通道场景下传统编解码仍是强基线。联合端到端优化不是必要条件，分离式训练即可达到最好效果。

Abstract: The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.

</details>


### [207] [SAM3-UNet: Simplified Adaptation of Segment Anything Model 3](https://arxiv.org/abs/2512.01789)
*Xinyu Xiong,Zihuang Wu,Lei Lu,Yufa Xia*

Main category: cs.CV

TL;DR: SAM3-UNet 将 SAM3 精简并适配下游分割任务：用 SAM3 编码器 + 轻量适配器 + U-Net 解码器，实现低显存、参数高效微调，并在镜像检测与显著性目标检测等任务上超越 SAM2-UNet 与SOTA。


<details>
  <summary>Details</summary>
Motivation: 原始 SAM3 功能强大但下游特定任务微调成本高、显存占用大。需要一种在保持强大表示能力的同时，降低训练资源和调整复杂度的方法，以便在镜像检测、显著性检测等密集预测任务中高效应用。

Method: 提出 SAM3-UNet 框架：1) 采用 SAM3 图像编码器作为特征主干；2) 设计简单的参数高效适配器（PEFT）以实现少量参数的微调；3) 采用轻量级 U-Net 风格解码器进行像素级预测。整体结构简化、端到端训练，兼顾性能与资源效率。

Result: 在镜像检测与显著性目标检测等多任务初步实验中，SAM3-UNet 优于 SAM2-UNet 与其他最新方法；训练时批量大小为 12 下显存占用不足 6GB。

Conclusion: 通过将 SAM3 与轻量适配器和轻量解码器结合，SAM3-UNet 以较低训练成本实现对下游密集预测任务的高性能适配，显示出作为通用低成本分割骨干的潜力；代码已开源，便于复现与扩展。

Abstract: In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.

</details>


### [208] [Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos](https://arxiv.org/abs/2512.01803)
*Xavier Thomas,Youngsun Lim,Ananya Srinivasan,Audrey Zheng,Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: 提出一种评估生成视频中复杂人类动作“视觉+时序正确性”的新指标：在学习到的真实人类动作潜空间中度量生成视频与真实分布的距离，并构建新基准验证，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有纯视觉编码器与多模态大模型偏外观、缺乏时序理解，难以识别动作细节与解剖学不合理之处；缺少能可靠评估复杂人类动作质量（时序连贯、运动可行性）的指标与基准。

Method: 学习一个结合外观无关的人体骨架几何特征与外观相关特征的联合潜空间，捕捉真实动作的细微差异、约束与时序平滑性。对生成视频，提取其对应表示并计算与真实动作分布的距离作为动作质量分数。同时构建多维度、强调时序挑战的人类动作评测基准用于严谨验证。

Result: 在新提出的基准上较SOTA方法提升超过68%；在外部既有基准上表现具竞争力；与人类主观感知的相关性更强；分析揭示当前视频生成模型在动作真实性与时序一致性上的关键缺陷。

Conclusion: 将骨架几何与外观特征融合的潜空间可作为评估人类动作可行性的稳健表示，距离度量能有效评估生成视频的动作质量；新基准与指标为视频生成研究提供更可靠的评测标准与发展方向。

Abstract: Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.

</details>


### [209] [Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights](https://arxiv.org/abs/2512.01816)
*Juanxi Tian,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.CV

TL;DR: 提出Envision基准与Envision-Score，用链式文本到多图生成评估模型的因果-时空一致性；发现统一多模态模型在因果叙事上优于专用T2I，但整体仍弱于闭源，且时空一致性仍是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型主要用单张T2I来校准语义一致，但训练/评估偏向静态图像，导致过拟合于模式匹配与语义拼接，难以建模随时间演化的动态过程与因果链，限制了对世界知识的内化。

Method: 构建Envision基准：基于世界知识与时空因果结构，覆盖六大领域，包含1000个四阶段链式提示，要求文本到多图（序列帧）生成；提出综合指标Envision-Score，从多维一致性、物理性和美学三方面评估因果-时序约束下的生成质量；对15个模型（10个专用T2I，5个统一多模态）进行系统评测。

Result: 专用T2I在美学呈现上更强，但缺乏内在世界知识；统一多模态模型在因果叙事连贯性上更好，整体仍落后于闭源模型；所有开源模型在跨帧时空一致性上存在明显短板。

Conclusion: 单张、因果隔离的训练与评估范式抑制了多帧推理与生成，强化静态匹配而非动态世界建模；应转向基于因果-时序约束的多帧评测与训练，以促进世界知识内化与更可靠的多图生成。

Abstract: Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.

</details>


### [210] [Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling](https://arxiv.org/abs/2512.01821)
*Meng Cao,Haokun Lin,Haoyuan Li,Haoran Tang,Rongtao Xu,Dong An,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出MILО：在MLLM中引入视觉生成器与相对位姿编码(RePE)，并配以几何感知生成数据集GeoGen，从而用感知反馈隐式锚定符号推理，显著提升空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型多靠文本监督学习空间概念，缺乏对视觉几何的真实感知（“视觉文盲”），导致空间推理弱。需要将符号推理与感知经验连接起来，提升三维空间理解。

Method: 1) MILO范式：在MLLM外接几何感知的视觉生成器，以生成/重建式反馈作为“想象”信号，隐式约束并校正模型的空间推理；2) RePE：提出相对相机位姿编码，捕获相机间的相对变换，优于绝对坐标表示；3) 数据：构建GeoGen，含约2241个视频与67827组“观测-动作-结果”三元组，用于训练与评估。

Result: 在多种基线与基准上显著提升空间推理表现；RePE优于绝对坐标系；结合生成器的几何反馈能更好地对齐感知与符号推理，获得更全面的三维理解。

Conclusion: 通过将视觉生成器与相对位姿编码融入MLLM训练，MILO有效缓解“视觉文盲”，实现人类式的空间想象与更强的3D空间推理，并由GeoGen数据支撑其有效性。

Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.

</details>


### [211] [CauSight: Learning to Supersense for Visual Causal Discovery](https://arxiv.org/abs/2512.01827)
*Yize Zhang,Meiqi Chen,Sirui Chen,Bo Peng,Yanxi Zhang,Tianyu Li,Chaochao Lu*

Main category: cs.CV

TL;DR: 提出视觉因果发现任务，构建带实体级因果图的大规模数据集VCG-32K，并提出因果感知的VL模型CauSight；通过ToCT推理轨迹与因果奖励强化学习训练，显著超越GPT-4.1。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型多停留在“看见了什么”，缺乏对“为什么发生”的因果理解与推理能力。为使AI具备人类式因果思维，需要定义并评测视觉场景中实体间的因果关系，并提供相应的数据与方法。

Method: 1) 数据：构建VCG-32K，给每张图像标注实体级因果图；2) 模型：提出CauSight，一个可进行因果感知推理的视觉-语言模型；3) 训练：a) 从VCG-32K精选训练样本；b) 设计Tree-of-Causal-Thought（ToCT）以合成多步因果推理轨迹；c) 采用带因果奖励的强化学习优化推理策略。

Result: 在视觉因果发现任务上，CauSight显著优于GPT-4.1，取得超过三倍的性能提升（绝对提升约21%）。

Conclusion: 以VCG-32K数据、ToCT推理与因果奖励RL为核心的CauSight能有效进行视觉因果发现，推动从感知到因果理解的转变；相关代码、模型与数据已开源。

Abstract: Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.

</details>


### [212] [OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic](https://arxiv.org/abs/2512.01830)
*Songyan Zhang,Wenhui Huang,Zhan Chen,Chua Jiahao Collister,Qihang Huang,Chen Lv*

Main category: cs.CV

TL;DR: 提出OpenREAD：一个支持从高层推理到低层轨迹规划端到端强化微调（RFT）的VLM-AD框架，通过LLM评价器为开放式推理构建奖励，实现对上游理解与下游规划的联合提升，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段策略SFT→RFT虽有效，但SFT学习范式限制推理泛化；RFT多用于下游封闭任务，开放式场景理解难以量化奖励，限制端到端提升。

Method: 1) 构建大规模驾驶知识数据的CoT标注；2) 以Qwen3 LLM作为RFT的critic，对开放式问题的推理质量进行打分并建模奖励；3) 将RFT从高层推理贯穿到低层轨迹规划，实现端到端联合强化微调的VLM-AD框架OpenREAD。

Result: 端到端联合RFT在上游（推理/理解）与下游（规划/决策）均显著提升；在多项推理与规划基准上取得SOTA表现。

Conclusion: 通过LLM驱动的开放式推理奖励与端到端RFT，OpenREAD突破SFT泛化瓶颈与RFT仅限下游的限制，实现从理解到规划的整体性能提升。

Abstract: Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.

</details>


### [213] [PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models](https://arxiv.org/abs/2512.01843)
*Zeqing Wang,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: 提出PID数据集与PhyDetEx方法，微调VLM以检测并解释视频中的物理不可信事件，并据此评测T2V模型的物理合规律；结果显示闭源进步明显但整体仍存在较大物理合理性缺陷，开源模型尤甚。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频模型虽在质量和指令跟随上进步显著，但其是否理解并遵循物理规律存疑；通用VLM作为评估器难以识别物理不可能的内容，因此需要专门的数据与方法来训练评估器并系统评测T2V的物理合规律。

Method: 构建PID数据集：含500条手工标注测试视频与2,588对训练视频，每个不可信视频由重写真实视频字幕诱导T2V生成物理违背内容；在此数据上对VLM进行轻量微调，使其能检测物理不可信并生成违反物理原理的文本解释；将微调后的VLM（PhyDetEx）作为检测与解释器，对多种SOTA T2V模型进行基准评测。

Result: 微调后的VLM能够有效识别并解释物理违背事件；利用该工具对多款T2V模型评测显示：尽管近期模型在物理合理性方面有所提升，但整体仍难以稳定遵循物理规律，尤其是开源模型表现较弱。

Conclusion: 物理合理性仍是T2V的关键挑战。提供PID数据集与PhyDetEx作为标准化评测与解释工具，有助于推动模型在理解与遵守物理规律上的进步；相关资源已开源可复现与扩展。

Abstract: Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \textbf{PID} (\textbf{P}hysical \textbf{I}mplausibility \textbf{D}etection) dataset, which consists of a \textit{test split} of 500 manually annotated videos and a \textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.

</details>


### [214] [Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching](https://arxiv.org/abs/2512.01850)
*Yue Pan,Tao Sun,Liyuan Zhu,Lucas Nunes,Iro Armeni,Jens Behley,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出将点云配准视为条件生成：学习一个连续点级速度场，把带噪点移动到统一配准场景，并从中恢复各视角位姿；无需显式匹配与全局优化，达成SOTA，尤其低重叠场景，且跨尺度/传感器泛化，支持重定位、多机器人SLAM、跨会话地图融合。


<details>
  <summary>Details</summary>
Motivation: 传统配准依赖两两点云的对应匹配与变换估计，再做全局优化，易受低重叠、噪声、遮挡影响，流程复杂且误差累积；期望一种能直接生成全局一致配准结果、鲁棒且可泛化的方法。

Method: 将配准建模为条件生成：学习一个连续的点级速度场，输入为未配准、含噪点云，输出为将点运输到统一注册场景的流；由生成的注册点云推断每个视角/扫描的位姿。采用轻量级局部特征提取器，并在测试时施加刚性约束（rigidity enforcement）以保证物理一致性。

Result: 在成对与多视角配准基准上取得SOTA，尤其在低重叠条件下表现突出；方法能跨尺度与不同传感器模态泛化。

Conclusion: 把配准转化为条件生成与连续速度场学习，可在无需显式对应与繁琐全局优化下，直接生成注册点云并稳健恢复位姿，具备强鲁棒性与泛化能力，且可用于重定位、多机器人SLAM与多会话地图融合等下游任务。

Abstract: Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.

</details>


### [215] [COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis](https://arxiv.org/abs/2512.01853)
*Tsz-To Wong,Ching-Chun Huang,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出一个可重构的多智能体系统用于体育视频理解，通过迭代调用与灵活组合不同“认知工具”代理，统一处理从短期推理（如回合问答）到长期生成（如比赛摘要）的任务，在羽毛球两项代表性任务上展示了跨层次时序建模、可扩展与可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 现有端到端模型难以同时兼顾体育视频中的时序层级（微观动作到宏观战术），迁移新任务代价高、泛化差、且可解释性弱，需要一种既灵活又可扩展、能跨任务复用并具备可解释性的框架。

Method: 提出可重构的多智能体系统（MAS）：每个代理作为专门的“认知工具”，负责特定分析维度；通过代理的迭代调用与可组合管线来适配不同时间尺度与任务（短期的分析推理与长期的生成总结），在羽毛球场景中将细粒度事件检测与全局语义组织贯通。

Result: 在两项羽毛球代表性任务上验证：系统能构建自适应流水线，兼顾微观事件识别与宏观语义汇总，体现出良好的跨任务适配性与可解释性（具体指标未在摘要中给出）。

Conclusion: 多智能体、可重构的框架为体育视频智能提供了灵活、可扩展且可解释的范式，可在不同时间尺度和任务间通用，改善现有端到端方法的泛化与开发成本问题。

Abstract: Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct "cognitive tool" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence.The project homepage is available at https://aiden1020.github.io/COACH-project-page

</details>


### [216] [TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals](https://arxiv.org/abs/2512.01885)
*Florian Bürger,Martim Dias Gomes,Nica Gutu,Adrián E. Granada,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: TransientTrack 是一个用于多通道、具有瞬态荧光信号（随时间波动）的细胞时序追踪的轻量级深度学习框架，能同时追踪并识别有丝分裂与凋亡，结合Transformer、多阶段匹配与卡尔曼滤波补轨，跨条件表现稳健，并用于评估化疗药物的单细胞效应。


<details>
  <summary>Details</summary>
Motivation: 现有细胞追踪方法多针对单一且稳定信号的视频，难以处理随昼夜节律等过程引起的瞬态荧光波动，且通常不显式检测关键生物事件（如细胞死亡），导致谱系不完整、药效与耐药分析受限。

Method: 提出TransientTrack：1) 在多通道显微视频上以检测嵌入直接匹配进行追踪（无需手工特征量化）；2) 融合Transformer以建模时空与多通道信息；3) 采用利用全部检测框的多阶段匹配策略；4) 通过卡尔曼滤波插值补全漏检轨迹；5) 显式事件识别模块检测有丝分裂与凋亡，从而构建完整细胞谱系。

Result: 框架在多种实验条件下表现稳健，能有效追踪细胞并准确捕获分裂与死亡事件，支持构建完整轨迹与谱系；作者展示了其在单细胞水平评估化疗药物效力的应用。

Conclusion: TransientTrack为处理多通道、瞬态信号的细胞追踪提供了统一且轻量的解决方案，可用于量化癌细胞动力学，细化治疗响应与耐药机制的表征；代码开源，具备推广与复现价值。

Abstract: Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.

</details>


### [217] [KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM](https://arxiv.org/abs/2512.01889)
*Zaid Nasser,Mikhail Iumanov,Tianhao Li,Maxim Popov,Jaafar Mahmoud,Malik Mohrat,Ilya Obrubov,Ekaterina Derevyanka,Ivan Sosin,Sergey Kolyubin*

Main category: cs.CV

TL;DR: KM-ViPE 是一个面向动态场景、未标定单目相机的实时开放词汇 SLAM，直接基于RGB视频做定位与语义建图，融合DINO特征与几何约束，并与语言嵌入对齐，实现同时定位与开放词汇语义映射，性能与SOTA竞争且无需深度/里程计/离线流程。


<details>
  <summary>Details</summary>
Motivation: 现有方法多需深度传感器、离线标定或对动态场景鲁棒性差，难以满足自中心视角、互联网规模视频训练与在线机器人/AR应用对实时、开放词汇语义与鲁棒性的需求。

Method: 提出 KM-ViPE：1) 以未标定单目RGB输入实时运行；2) 将DINO视觉特征与几何约束紧耦合；3) 设计基于高层特征的自适应鲁棒核，抑制动态/可移动静态物体对估计的干扰；4) 将几何与深度视觉特征融合并与语言嵌入对齐，实现在线定位与开放词汇语义建图。

Result: 在基准上取得与SOTA相当的结果，同时比现有方案更在线、更少依赖（无需深度或里程计），并在动态场景中表现更稳健。

Conclusion: KM-ViPE 将在线性、未标定单目输入、动态鲁棒性与开放词汇语义建图统一到一个系统，适合自治机器人与AR/VR，并推动具身智能的空间理解能力。

Abstract: We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.

</details>


### [218] [StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data](https://arxiv.org/abs/2512.01895)
*Avirup Dey,Vinay Namboodiri*

Main category: cs.CV

TL;DR: 提出StyleYourSmile：一种无需多风格成对数据的一次性跨域人脸表情重定向方法，通过数据增广与双编码器实现身份与风格解耦，并用扩散模型进行表情迁移，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域人脸重定向方法在训练于真实人脸时，往往难以泛化到卡通/绘画等风格域；有的需要测试时优化或在精心整理的多风格配对数据上微调以获得域不变身份表征，代价高、适用性差。因此需要一种无需昂贵数据与测试时优化、仍能实现强身份保持与高保真跨域表情迁移的方法。

Method: 提出StyleYourSmile：1) 设计高效的数据增广策略，合成多样的风格变化以逼近跨域分布；2) 构建双编码器框架，一支提取域不变身份线索，另一支捕获域特定风格属性，实现身份与风格的解耦控制；3) 基于上述控制信号对扩散模型进行条件生成，实现跨域的表情重定向。该方法为一次性设置（one-shot），无需多风格成对数据、无需测试时优化或额外微调。

Result: 在广泛的视觉域上进行实验，StyleYourSmile在身份保持与表情重定向保真度上优于现有方法，显示出更强的跨域泛化能力。

Conclusion: 通过数据增广+双编码器解耦与条件扩散生成，实现无需成对多风格数据的一次性跨域人脸表情重定向，在身份保持和迁移质量上取得领先效果，验证了方法的有效性与通用性。

Abstract: Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.

</details>


### [219] [SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception](https://arxiv.org/abs/2512.01908)
*Gurmeher Khurana,Lan Wei,Dandan Zhang*

Main category: cs.CV

TL;DR: 提出SARL，一种空间感知的自监督学习框架，在BYOL基础上加入三种特征图级别损失（SAL、PPDA、RAM），保持跨视角的显著性、部件组成与几何关系一致。用于融合视觉-触觉图像，在6个下游任务上优于9个SSL基线，尤其在边缘姿态回归上将MAE降至0.3955（较次优方法提升约30%），接近监督上限。结论：对视触融合数据，结构化的空间等变性信号最有效，提升接触丰富操控的感知能力。


<details>
  <summary>Details</summary>
Motivation: 接触丰富的机器人操控需要对局部几何、纹理、硬度等属性的精准表征。视觉提供全局但缺乏触觉细节，触觉补充但缺全局语义。现代视触传感器将两者融合为对齐的单幅图像，但主流SSL通常将特征压缩为全局向量，丢失空间结构，不利于需要空间对齐与几何敏感性的操控任务。因此需要能在表征学习阶段保留并对齐空间结构的SSL方法。

Method: 在BYOL框架上引入三种作用于中间特征图的空间一致性损失：1) Saliency Alignment（SAL）：对齐跨视角的注意/显著性分布，保持关注区域一致；2) Patch-Prototype Distribution Alignment（PPDA）：通过补丁到原型的分布对齐，保持部件/组成稳定；3) Region Affinity Matching（RAM）：保持区域间的亲和/几何关系一致。上述损失与原有全局目标互补，促进特征对视角变化的空间等变性与结构保持。输入为单幅融合视触图像，天然对齐。

Result: 在6个下游任务中，SARL相对于9个SSL基线均有一致提升。在几何敏感的边缘姿态回归任务上，SARL实现0.3955的MAE，相对次优SSL（0.5682）提升约30%，并接近监督学习上界。

Conclusion: 对融合视觉-触觉数据，维持特征图层面的空间结构与等变性是关键。SARL通过SAL、PPDA、RAM在SSL中显式约束空间一致性，相比仅用全局向量的SSL显著增强了对几何与部件关系的建模能力，从而提升机器人在接触丰富场景下的感知与操控表现。

Abstract: Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.

</details>


### [220] [Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2512.01922)
*Zahra Mahdavi,Zahra Khodakaramimaghsoud,Hooman Khaloo,Sina Bakhshandeh Taleshani,Erfan Hashemi,Javad Mirzapour Kaleybar,Omid Nejati Manzari*

Main category: cs.CV

TL;DR: 提出Med-VCD：一种稀疏视觉对比解码方法，在不引入二次解码开销的前提下降低医疗LVLM幻觉，平均提升事实准确率13%、幻觉准确率6%。


<details>
  <summary>Details</summary>
Motivation: 医疗场景中LVLM用于VQA与报告生成，但易产生看似可信的幻觉，现有自然图像领域的防幻觉解码依赖二次解码/回滚，推理变慢，且方法常域专一、易引入跨模态或与真值不对齐的问题。需要一种既高效又通用、能强化视觉证据的医疗解码策略。

Method: 提出Med-VCD（Medical Visual-Contrastive Decoding）：在解码过程中引入稀疏化的视觉对比机制。核心是“token稀疏化”策略，在线选择受视觉证据支撑的token，剪除冗余、保留关键视觉上下文，以在不进行二次解码的情况下强化视觉一致性并维持效率。

Result: 在8个医疗数据集（眼科、影像学、病理学），覆盖VQA、报告生成和专门的幻觉基准上评测，较基线医疗LVLM：事实准确率平均提升约13%，幻觉准确率提升约6%。

Conclusion: Med-VCD能在不增加明显推理时间的情况下缓解医疗LVLM的幻觉，兼顾效率与可靠性，并具有跨任务与跨子领域的泛化潜力。

Abstract: Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\% and improves hallucination accuracy by 6\% relative to baseline medical LVLMs.

</details>


### [221] [Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory](https://arxiv.org/abs/2512.01934)
*Chenyi Wang,Yanmao Man,Raymond Muller,Ming Li,Z. Berkay Celik,Ryan Gerdes,Jonathan Petit*

Main category: cs.CV

TL;DR: 提出AdvTraj：一种在线、物理可执行的ID操纵攻击，通过对抗性轨迹在不攻击检测模块的情况下将攻击者ID转移给目标，显著混淆多目标跟踪的关联。


<details>
  <summary>Details</summary>
Motivation: 现有MOT攻击多集中于单目标劫持或通过数字域攻击检测器来影响ID，往往模型特定、鲁棒性差且仅作用于离线样本；而MOT中对象关联阶段的安全性尚未被充分研究。错误ID关联会导致轨迹预测等下游任务严重后果，亟需揭示并检验此环节的脆弱性。

Method: 面向tracking-by-detection框架，设计AdvTraj：攻击者在物理世界执行特定“对抗性轨迹/机动”，诱导跟踪器将其ID转移到目标对象，从而混淆ID分配；不干预或攻击检测器。通过CARLA仿真评估，进行白盒针对SORT与黑盒/转移到多种SOTA MOT的实验；分析生成轨迹的共同模式，并提炼两种可由行人/驾驶者在日常中可实施的通用对抗机动。

Result: 在CARLA中对白盒SORT攻击场景达成100% ID混淆成功率；对多种SOTA MOT呈现高迁移性，最高可达93%成功率。并总结出具有通用性的轨迹模式与两类可执行的对抗机动。

Conclusion: SOTA MOT系统在对象关联阶段存在未充分认识的脆弱性，物理世界中的对抗性轨迹即可导致ID错配。研究为提升MOT鲁棒性提供了新视角与改进方向（如强化关联策略、防御对抗机动等）。

Abstract: Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.

</details>


### [222] [Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2512.01949)
*Zhongyu Yang,Dannong Xu,Wei Pang,Yingfang Yuan*

Main category: cs.CV

TL;DR: 提出Script，一种无需重训、可泛化到多种MLLM的即插即用视觉token剪枝方法，结合图结构剪枝与查询条件语义剪枝，在14个图像/视频理解基准上提升效率与准确性；在LLaVA-NeXT-7B上最高实现6.8倍prefill加速、10倍FLOPs下降，保持96.88%性能。


<details>
  <summary>Details</summary>
Motivation: MLLM在高分辨率图像/视频中视觉token激增，导致显存占用大、推理延迟高。现有剪枝要么忽视与用户查询的相关性，要么受限于注意力机制，泛化与效果不足。

Method: 提出Script，包含两部分：1) 图结构剪枝模块，基于视觉token之间的图关系移除视觉冗余；2) 查询条件的语义剪枝模块，根据用户查询保留相关视觉信息。方法为即插即用、无需重训，适配多种MLLM。

Result: 在14个跨图像与视频理解的基准上，较现有剪枝方法持续获得更高效率与预测精度。在LLaVA-NeXT-7B上达到最高6.8×的prefill阶段加速与10× FLOPs降低，同时保持原始性能的96.88%。

Conclusion: 结合结构冗余与查询相关性的双重剪枝能在不重训的前提下显著提升MLLM推理效率并尽量保持精度，具备良好的通用性与实用价值。

Abstract: The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.

</details>


### [223] [GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment](https://arxiv.org/abs/2512.01952)
*Haoyang He,Jay Patrikar,Dong-Ki Kim,Max Smith,Daniel McGann,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei,Sebastian Scherer*

Main category: cs.CV

TL;DR: 提出RLWG和GrndCtrl，通过可验证几何/时序奖励对预训练视频世界模型进行自监督后训练，使其具备稳定导航与空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型虽具高保真视觉生成能力，但缺乏几何约束与长时稳定性，难以用于需要空间一致性的导航与控制。需要一种能把生成先验与物理可验证结构对齐的方法。

Method: 提出RLWG框架：利用自监督、可验证的奖励信号（位姿循环一致性、深度重投影、时间连贯性）对预训练世界模型进行后训练对齐。具体实现为GrndCtrl，基于GRPO（Group Relative Policy Optimization），将多种奖励整合以优化世界模型，使其在滚动预测中保持几何与轨迹稳定。

Result: 与仅监督微调相比，经过GrndCtrl的世界模型在户外场景中展现更好的空间一致性、稳定轨迹和更可靠的长时滚动预测，导航稳定性显著提升。

Conclusion: 通过RLWG与GrndCtrl，用可验证的几何与感知奖励对齐预训练世界模型，成功弥合生成式预训练与具身、可验证的几何行为之间的鸿沟，优于监督微调，适用于具身导航。

Abstract: Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.

</details>


### [224] [SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation](https://arxiv.org/abs/2512.01960)
*Zisu Li,Hengye Lyu,Jiaxin Shi,Yufeng Zeng,Mingming Fan,Hanwang Zhang,Chen Liang*

Main category: cs.CV

TL;DR: SpriteHand 是一种自回归视频生成框架，可在实时条件下从静态物体图像与手部驱动视频合成多样的手-物交互效果，在单张GPU上达18 FPS、640×368分辨率、约150 ms延迟，长时稳定输出，视觉质量与物理可信度优于生成式与物理引擎基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于仿真的方法需刚体模型与脚本化手势，难以覆盖可变形、关节式或毛发表面等复杂对象与真实场景中的动态交互；物理引擎在这些非刚体/复杂接触上建模困难、成本高，缺乏视觉真实感与时序一致性。

Method: 提出 SpriteHand：以静态物体图像和包含“想象交互”的手部视频为条件，采用因果推断式架构进行自回归生成；并通过混合式后训练（hybrid post-training）提升视觉真实感与时间一致性。系统实现实时流式生成，面向多种物体类别与运动模式。

Result: 1.3B 参数模型在单张 RTX 5090 上以约18 FPS、640×368、150 ms 延迟实现实时生成，支持超过一分钟连续输出；在实验中，相比生成式与基于引擎的基线，在视觉质量、物理合理性与交互保真度上更优。

Conclusion: 以数据驱动的自回归视频生成取代显式物理建模，可在复杂手-物交互中取得更高的真实感与稳定性，并具备实时性与通用性，成为传统物理仿真难以处理场景的有效替代或补充。

Abstract: Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.

</details>


### [225] [SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning](https://arxiv.org/abs/2512.01975)
*Xu Zhang,Jin Yuan,Hanwang Zhang,Guojin Zhong,Yongsheng Zang,Jiacheng Lin,Zhiyong Li*

Main category: cs.CV

TL;DR: 提出“SegCaptioning”任务：给出简单提示（如框），同时生成多样化且语义对齐的描述-分割掩码对；方法为场景图引导的扩散模型（SGDiff），含提示到场景图适配器、双模态Transformer扩散生成，以及多实体对比学习对齐；在两数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有可控图像语义任务依赖昂贵或稀少信息的提示（文本/框），且通常只能输出单一结果，难以满足用户对多样且细粒度语义的需求。需要一种能用最小提示获得多样化、可选择且语义一致的跨模态结果（描述与分割）的方案。

Method: 提出SGDiff：1）Prompt-Centric Scene Graph Adaptor将用户简单提示映射为场景图，捕获意图与上下文关系；2）Scene Graph Guided Bimodal Transformer融入扩散过程，联合生成相关的caption与mask对，显式建模两者间相关性；3）Multi-Entities Contrastive Learning从实体级别度量跨模态相似度，强化文本实体与视觉区域对齐。

Result: 在两个数据集上进行大量实验，SGDiff在新任务SegCaptioning上取得最佳性能，同时在描述与分割指标上显著领先，且在最小提示条件下生成质量更高、对齐更好的caption-mask对。

Conclusion: 通过场景图引导与实体级对齐，SGDiff能从极简提示生成多样且语义一致的描述-掩码对，解决了以往方法输出单一、对齐不足的问题，为可控图像理解提供更灵活高效的范式。

Abstract: Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.

</details>


### [226] [Artemis: Structured Visual Reasoning for Perception Policy Learning](https://arxiv.org/abs/2512.01988)
*Wei Tang,Yanpeng Sun,Shan Zhang,Xiaofan Li,Piotr Koniusz,Wei Li,Na Zhao,Zechao Li*

Main category: cs.CV

TL;DR: 论文提出Artemis：用结构化（标签,框）的中间推理替代纯语言链式推理，以提升视觉感知策略学习；在定位/检测、计数与几何感知等任务上表现强，且在通用多模态基准上竞争力。


<details>
  <summary>Details</summary>
Motivation: 纯语言中间推理在视觉感知任务中常降低性能。原因并非“推理”本身，而是推理空间不匹配：视觉需要空间与物体中心的结构化推理，而语言链处在无结构语义空间，易引入歧义和不可验证的中间状态。

Method: 提出Artemis框架：用结构化的proposal式中间步骤，每一步为(标签, 边界框)对，代表可验证的视觉状态；显式跟踪中间状态、对proposal质量进行直接监督，避免语言歧义。以Qwen2.5-VL-3B为底座，实现空间对齐的感知-策略学习。

Result: 在指代定位与目标检测任务上取得强性能；对计数与几何感知任务具有显著泛化能力；在多种设置下稳定提升；在通用多模态大模型基准上也具竞争力。

Conclusion: 将推理与空间表示对齐（以标签+框的结构化提案）能系统性提升视觉感知策略学习的效果与可扩展性，并为通用感知策略提供可验证、可监督的中间过程。

Abstract: Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.

</details>


### [227] [PAI-Bench: A Comprehensive Benchmark For Physical AI](https://arxiv.org/abs/2512.01989)
*Fengzhe Zhou,Jiannan Huang,Jialuo Li,Deva Ramanan,Humphrey Shi*

Main category: cs.CV

TL;DR: 论文提出PAI-Bench，一个用于评估“物理智能”的统一基准，覆盖视频生成、条件视频生成与视频理解三类任务，通过2808个真实案例与物理一致性/领域推理导向指标系统测评当前模型，发现生成模型视觉逼真但物理连贯性差，多模态大模型在预测与因果解释上不足，指出现阶段系统仍难满足物理感知与预测需求。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLM和视频生成模型是否具备对真实世界物理的感知与预测能力缺乏系统认知；缺少统一、全面、可量化地反映物理可行性与领域推理能力的评测基准，难以定位模型在“物理AI”上的真实进展与短板。

Method: 构建PAI-Bench：覆盖视频生成、条件视频生成、视频理解三大类任务，共2808个真实世界案例；为每类任务设计与任务对齐、强调物理合理性与领域推理的评测指标；对当下代表性的视频生成模型与多模态大模型进行系统化评估与对比分析。

Result: 评测显示：视频生成模型尽管视觉质量高，但难以保持物理上连贯的动力学；多模态大模型在未来态预测与因果解释方面表现有限。总体性能揭示当前系统对物理感知与预测的支持仍不成熟。

Conclusion: PAI-Bench为评估物理AI提供了现实、统一的基础框架，清晰暴露了现有模型在物理连贯性、预测与因果推理方面的关键缺口，为后续模型改进与研究指明方向。

Abstract: Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.

</details>


### [228] [Learning Visual Affordance from Audio](https://arxiv.org/abs/2512.02005)
*Lidong Lu,Guo Chen,Zhu Wei,Yicheng Liu,Tong Lu*

Main category: cs.CV

TL;DR: 提出音视协同的可供性定位任务（AV-AG）：仅凭动作声音在图像上分割可交互区域；并发布首个数据集与模型AVAGFormer，性能超越相关基线。


<details>
  <summary>Details</summary>
Motivation: 现有可供性分割多依赖文本指令或示范视频，易受歧义、遮挡影响。动作声音提供独立于视觉、语义丰富且实时的线索，可更直观地指示物体交互区域，填补这一空白。

Method: 构建AV-AG数据集：包含动作声音、对象图像与像素级可供性标注，含未见子集评估零样本。提出AVAGFormer：语义条件的跨模态混合器融合音/视特征；双头解码器进行掩膜预测；端到端训练。

Result: 在AV-AG任务上取得SOTA，显著优于来自相关任务的多种基线。消融与分析展示端到端建模收益、各组件贡献，并与音视分离(AVS)任务差异明显。

Conclusion: 声音是有效的可供性线索。AVAGFormer与新数据集验证了从声音定位交互区域的可行性与优势，并具备一定零样本泛化能力。

Abstract: We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.

</details>


### [229] [MV-TAP: Tracking Any Point in Multi-View Videos](https://arxiv.org/abs/2512.02006)
*Jahyeok Koo,Inès Hyeonsu Kim,Mungyeom Kim,Junghyun Park,Seohyun Park,Jaeyeong Kim,Jung Yi,Seokju Cho,Seungryong Kim*

Main category: cs.CV

TL;DR: MV-TAP 是一种针对多视角视频中动态场景的点跟踪方法，利用相机几何与跨视角注意力整合时空信息，显著提升轨迹完整性与可靠性，并在合成训练数据与真实评测集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单视角或简单多视角点跟踪在动态场景中容易丢失目标、轨迹不完整；需要一种能跨视角融合信息、利用几何约束的通用跟踪器，以支持真实复杂场景与应用。

Method: 提出 MV-TAP：结合相机几何约束与跨视角注意力机制，跨时间与视角聚合特征与可见性信息，进行点级匹配与轨迹估计；构建大规模合成训练集，并提供针对多视角跟踪的真实世界评测数据。

Result: 在多个具有挑战性的多视角基准上，MV-TAP 相比现有点跟踪方法取得更高的准确率与轨迹完整性，表现更稳健。

Conclusion: 利用几何先验与跨视角注意力的 MV-TAP 为多视角点跟踪提供了有效基线，能在动态复杂场景中产生更完整可靠的轨迹，并推动该方向研究。

Abstract: Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.

</details>


### [230] [AirSim360: A Panoramic Simulation Platform within Drone View](https://arxiv.org/abs/2512.02009)
*Xian Ge,Yuling Pan,Yuhang Zhang,Xiang Li,Weijun Zhang,Dizhe Zhang,Zhaoliang Wan,Xin Lin,Xiangkai Zhang,Juntao Liang,Jason Li,Wenjie Jiang,Bo Du,Ming-Hsuan Yang,Lu Qi*

Main category: cs.CV

TL;DR: AirSim360 提供空中视角的全景(360°)仿真平台，整合高质量渲染对齐的数据与标注、人类行为建模与自动轨迹生成，收集6万+样本并在多任务上验证，有望填补全景理解的大规模数据缺口。


<details>
  <summary>Details</summary>
Motivation: 全景场景理解需要大规模、多样化且精确标注的数据，但现实采集成本高、难以覆盖空中视角与动态人–机交互，现有模拟器也缺少系统化的全景(时空4D)建模。

Method: 构建名为 AirSim360 的无人机全景仿真平台，包含：1) 渲染对齐的数据与标注范式，提供像素级几何/语义/实例级别理解；2) 行人与互动建模的人类行为系统；3) 自动化轨迹生成以支持导航任务。并在平台上批量生成>60K 全景样本。

Result: 利用该平台在多种任务上开展大量实验，验证了仿真数据与系统在全景理解、导航等方面的有效性；相较现有模拟器，首次在全景设定下系统化地对现实世界进行4D建模。

Conclusion: AirSim360 缓解了全景理解领域的数据匮乏瓶颈，提供从数据、交互到任务的端到端仿真生态，公开代码、插件与数据集以促进研究发展。

Abstract: The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.

</details>


### [231] [Improved Mean Flows: On the Challenges of Fastforward Generative Models](https://arxiv.org/abs/2512.02012)
*Zhengyang Geng,Yiyang Lu,Zongze Wu,Eli Shechtman,J. Zico Kolter,Kaiming He*

Main category: cs.CV

TL;DR: 提出改进版 MeanFlow（iMF），通过重新表述训练目标与灵活的显式条件引导，实现单步（1-NFE）在 ImageNet 256×256 上达到 1.72 FID，显著优于现有一步法并逼近多步法且无需蒸馏。


<details>
  <summary>Details</summary>
Motivation: 原始 MeanFlow 作为一步生成框架虽快，但存在两大痛点：1）训练目标依赖网络自身，导致不稳定；2）分类器自由引导（CFG）尺度在训练时固定，测试时缺乏调节灵活性，限制性能与通用性。

Method: - 将目标改写为对“瞬时速度”v 的损失，同时以网络预测的“平均速度”u 进行重参数化，从而把训练转化为更标准的回归问题，提升稳定性。
- 将引导机制改为显式条件变量，并通过 in-context conditioning 统一处理多样条件，使得测试时可灵活调节，引导同时缩小模型规模并提升效果。

Result: 在 ImageNet 256×256 上，从零训练的 iMF 以单步（1-NFE）达到 1.72 FID，显著超过既有一步生成方法，且在无蒸馏前提下与多步方法的性能差距缩小。

Conclusion: 通过目标函数重构与条件引导的显式化+iCC 设计，iMF 提升了一步生成的稳定性与灵活性，证明“快进式”生成可作为独立范式取得与多步方法接近的性能。

Abstract: MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.

</details>


### [232] [TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models](https://arxiv.org/abs/2512.02014)
*Zhiheng Liu,Weiming Ren,Haozhe Liu,Zijian Zhou,Shoufa Chen,Haonan Qiu,Xiaoke Huang,Zhaochong An,Fanny Yang,Aditya Patel,Viktar Atliha,Tony Ng,Xiao Han,Chuyan Zhu,Chenyang Zhang,Ding Liu,Juan-Manuel Perez-Rua,Sen He,Jürgen Schmidhuber,Wenhu Chen,Ping Luo,Wei Liu,Tao Xiang,Jonas Schult,Yuren Cong*

Main category: cs.CV

TL;DR: TUNA提出统一多模态模型，通过级联VAE编码器与表示编码器构建连续统一视觉表示，实现图像/视频理解与生成端到端统一处理，并在多项基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有UMM多采用解耦表示（理解与生成各自编码器），导致表示格式不匹配、任务互相干扰、性能受限。需要一种在同一表示空间内统一图像与视频、理解与生成的原生框架，提升跨任务泛化与效率。

Method: 构建TUNA：以VAE将像素映射到连续潜空间，再接入强大的预训练表示编码器得到统一视觉表征；在该表征上用单一模型同时处理理解与生成任务。进行联合训练，使理解数据与生成数据共享表征并互促。比较不同表示编码器强度对整体性能的影响。

Result: 在多模态理解（图像/视频问答、检索等）与生成（图像/视频生成与编辑）基准上均取得SOTA。统一表示避免了解耦方案的格式不匹配，联合训练无干扰反而互利；更强的预训练表示编码器带来全线提升。

Conclusion: 统一连续视觉表示是实现原生UMM的关键；通过VAE+表示编码器可在同一空间统一处理理解与生成，并具备可扩展性。选择更强的表示编码器与联合训练策略可稳定提升多模态任务表现。

Abstract: Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.

</details>


### [233] [Generative Video Motion Editing with 3D Point Tracks](https://arxiv.org/abs/2512.02015)
*Yao-Chih Lee,Zhoutong Zhang,Jiahui Huang,Jui-Hsien Wang,Joon-Young Lee,Jia-Bin Huang,Eli Shechtman,Zhengqi Li*

Main category: cs.CV

TL;DR: 提出一种以3D点轨迹为条件的V2V方法，实现联合编辑相机与物体运动，解决深度与遮挡问题，支持多样运动编辑。


<details>
  <summary>Details</summary>
Motivation: 现有I2V缺乏全场景上下文导致一致性差；V2V虽能改视角或平移物体，但对细粒度物体运动控制有限，尤其在复杂运动与遮挡情况下难以精确编辑。

Method: 以源视频和成对的3D点轨迹（源/目标）作为条件，构建稀疏对应，将源视频上下文迁移到新运动中；相比2D轨迹，3D轨迹提供显式深度线索以解析前后关系与遮挡；采用两阶段训练（合成数据+真实数据）的生成模型，实现稳定的时空一致性。

Result: 模型能在多种场景下进行精确运动编辑，保持时空一致性与上下文保真，较现有方法更好地处理遮挡与深度排序；支持视角与物体运动的联合控制及细粒度编辑。

Conclusion: 3D轨迹条件化的V2V框架有效提升视频运动编辑的可控性与一致性，解锁联合相机/物体操作、运动迁移与非刚性变形等新型编辑能力。

Abstract: Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.

</details>


### [234] [Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now](https://arxiv.org/abs/2512.02016)
*Varun Varma Thozhiyoor,Shivam Tripathi,Venkatesh Babu Radhakrishnan,Anand Bhattad*

Main category: cs.CV

TL;DR: 论文评估视频生成器作为“世界模型”的物理一致性，聚焦重力。作者发现主流模型生成的下落加速度显著偏低且方差大；提出单位无关的两物体相对计时测试以剥离尺度/帧率等混淆，揭示对伽利略等效原理的违背；再用小样本低秩适配微调，显著提升有效重力并具零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 若视频生成器要充当世界模型，就需内化物理定律。现有物理评测常被尺度、帧率、焦距等单位歧义混淆，难以判断模型是否真正“懂”重力。作者欲消除这些混淆，准确定性/定量模型的重力表征，并探索低成本修正路径。

Method: 1) 实证测量主流视频生成器的下落轨迹，估计有效重力g_eff；2) 系统考察时间重标定等是否能解释偏差；3) 设计单位无关的两物体协议，检验t1^2/t2^2=h1/h2的相对定律，避免g、焦距、尺度等混淆；4) 用轻量低秩适配（LoRA样式）在仅100段单球下落视频上微调基座模型，评估对单球、双球与斜面场景的泛化。

Result: - 原始模型普遍生成“慢重力”，g_eff≈1.81 m/s^2且方差大；时间重标定也无法消除偏差。- 相对测试显示违反伽利略等效原理。- 微调后g_eff提升至≈6.43 m/s^2，达到地球重力约65%，并在双球与斜面任务上零样本带来物理一致性改善。

Conclusion: 当前视频生成器未能可靠内化重力规律，即便去除单位与帧率歧义仍存在系统性偏差；但通过极少数据与低秩适配可显著修正并具一定泛化，提示以小样本、定律导向的专业化微调是提升物理可信度的可行路径。

Abstract: Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\mathrm{eff}}$ from $1.81\,\mathrm{m/s^2}$ to $6.43\,\mathrm{m/s^2}$ (reaching $65\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.

</details>


### [235] [Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion](https://arxiv.org/abs/2512.02017)
*Shaowei Liu,David Yifan Yao,Saurabh Gupta,Shenlong Wang*

Main category: cs.CV

TL;DR: 提出VisualSync：基于多视角动态与极线几何的无标定、多摄像头视频自动同步方法，利用跟踪与3D重建提取跨视角对应，联合优化时间偏移，实验证明在多数据集上实现亚百毫秒级（<50 ms中位数）对齐，优于基线。


<details>
  <summary>Details</summary>
Motivation: 消费级相机普及使多机位拍摄常见，但视频间时间不同步且缺乏统一触发，现有方案依赖受控场景、特定目标、人工干预或昂贵硬件，难以在真实环境下通用，需要一种自动、鲁棒、低成本的时序对齐方法。

Method: 关键洞见：同一运动3D点在正确同步后跨视角应满足极线约束。流程：1) 使用现成的3D重建、特征匹配与稠密跟踪，提取多视角tracklets、相对位姿与跨视角对应；2) 以各摄像头时间偏移为变量，构建并联合最小化跨视角的极线误差目标函数；3) 通过优化求解各相机时间偏移，实现毫秒级对齐。无需受控设置或特殊硬件。

Result: 在四个多样且具有挑战的数据集上评测，相比基线方法显著提升，同步中位误差低于50毫秒，展示了鲁棒性与精度。

Conclusion: VisualSync在无标定、非受控场景下实现多摄像头视频的高精度自动同步，依托极线几何与联合优化，优于现有方法，具备实际应用潜力。

Abstract: Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.

</details>


### [236] [Data-Centric Visual Development for Self-Driving Labs](https://arxiv.org/abs/2512.02018)
*Anbang Liu,Guanzhong Hu,Jiayi Wang,Ping Guo,Han Liu*

Main category: cs.CV

TL;DR: 提出一套融合真实与虚拟数据的混合管线，为自驾实验室中的移液气泡检测提供高精度训练数据，在减少人工成本的同时保持近乎99.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: SDL（自驾实验室）在生物学中可降低人工与重复性问题，但其对精度要求高且需大量标注数据，尤其是难以获取的负样本。以移液为关键环节，缺乏高质量视觉反馈数据限制了鲁棒模型训练。

Method: 构建双轨混合数据生成管线：1）真实数据轨：人机协同的人在回路采集——自动获取并由人选择性核验，尽量少的人力保证高准确；2）虚拟数据轨：使用参考条件和提示词引导的图像生成来扩增数据，并对合成图像进行筛选与验证。两轨融合形成类别均衡的数据集用于训练气泡检测模型。

Result: 在独立的真实测试集上，仅用自动采集的真实图像训练的模型达99.6%准确率；将真实与生成数据混合训练仍达99.4%准确率，同时显著降低数据采集与审核负担。

Conclusion: 混合真实-虚拟的数据策略可规模化、低成本地为SDL提供视觉反馈数据，在稀有事件检测与更广泛视觉任务中缓解数据稀缺问题，同时维持高精度与鲁棒性。

Abstract: Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.

</details>
