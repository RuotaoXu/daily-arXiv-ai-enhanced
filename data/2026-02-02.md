<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 首个系统性基准评估5个开放词汇目标检测模型在航拍数据上的零样本表现，发现严重域迁移失败：最佳仅F1 27.6%，误报率69%；主要瓶颈是语义混淆而非定位。缩小词表显著提升性能，提示需面向航拍的领域自适应方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测在自然图像上效果好，但其向航拍遥感场景的可迁移性未知；缺少统一基准和明确拆解语义与定位因素的评估，以指导研究。

Method: 构建LAE-80C数据集（3,592图像、80类），在严格零样本设定下评测5个SOTA OVD模型；设计三种推理模式（Global/Oracle/Single-Category）以分离语义混淆与定位；系统比较不同数据集与提示工程策略（领域前缀、同义词扩展）的影响。

Result: 在LAE-80C上，最佳OWLv2仅F1 27.6%，假阳性率69%；将词汇从80类缩减到≈3.2类带来约15倍提升，显示语义混淆为主因；提示工程无显著收益；跨数据集性能差异大（DIOR F1≈0.53，FAIR1M F1≈0.12），暴露对成像条件的脆弱性。

Conclusion: 当前自然图像训练的OVD难以零样本迁移到航拍场景，语义混淆是核心瓶颈；需要领域自适应和更稳健的语义建模，而非仅靠提示工程；所给基准为后续研究提供了基线和诊断工具。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [2] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: 提出一个针对科学图表的VQA基准：图表与底层数据不存在一一对应，需访问底层数据才能精准回答；作者合成直方图并提供完整底层数据与标注。


<details>
  <summary>Details</summary>
Motivation: 现有VQA多针对实拍图像或简单图示；少数图表VQA默认图形元素与数据一一对应，忽视图表是对数据的变换（分析、简化、修改），导致无法评估模型在“需底层数据支持”的推理能力。

Method: 1) 调研与梳理现有VQA与图表VQA数据集并总结其局限；2) 基于已知分布参数合成带有真实底层数据的直方图；3) 设计需要访问底层数据才可精确回答的问题；4) 让人类与大型推理模型共同作答；5) 公开数据集：图像、底层数据、生成分布参数、以及图形与文本的边界框标注。

Result: 构建了一个开放数据集与评测基准，展示了在缺乏一一对应且需底层数据的设定下，精确作答对模型构成挑战；人类与大型模型的基线结果表明此任务不等同于传统图表读取。

Conclusion: 需要一个专门面向科学图表、非一一对应场景的VQA基准；提供的数据与标注为后续研究（包括更强的推理与数据访问机制）奠定基础。

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

</details>


### [3] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: 论文指出：当前VLM在3D空间理解与多视角推理上存在显著短板。借助新基准VRRPI-Bench与诊断集VRRPI-Diag（从无标注自摄视频中构建并配以语言化相对位姿标注），作者发现VLM在相机相对位姿估计任务上大量依赖浅层2D启发式，尤其难以处理深度变化与光轴方向的滚转；多图像融合推理也不稳定。SOTA模型（如GPT-5得分0.64）显著落后于几何基线（0.97）及人类（0.92）。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在2D语义与感知任务表现突出，但其对3D结构与多视角空间关系的理解能力未被充分检验且疑似不足。相对相机位姿估计（RCPE）是基础而具诊断性的任务，可揭示VLM是否真正具备3D归纳与时空整合能力；现有评测多偏二维或缺乏系统化隔离自由度的基准。作者希望以更贴近真实自摄场景的基准，系统量化VLM在3D与多帧空间推理的缺陷。

Method: 1) 提出VRRPI-Bench：从无标注的第一视角视频中构建，围绕共享目标产生同时存在平移与旋转的片段；以语言化的相对相机运动标注为监督/评测信号。2) 提出VRRPI-Diag：将运动的各自由度（如深度变化、绕光轴滚转等）解耦，便于逐项诊断。3) 系统评测多种主流VLM在单对图像与多帧整合（multi-image reasoning）上的表现，并与经典几何方法与人类表现对比。

Result: - 大多数VLM无法超越浅层2D启发式，遇到深度变化与绕光轴滚转表现尤差。- 多图像融合推理表现不稳定，最优仅约59.7%。- 即便是SOTA模型（示例为GPT-5，0.64）也显著落后几何基线（0.97）与人类（0.92）。

Conclusion: VLM在3D落地与多视角空间推理方面存在系统性短板；RCPE虽简单却能有效揭示这些缺陷。需要引入显式的3D归纳偏置、几何一致性约束或多视角/深度监督，改进对深度与滚转等关键自由度的理解，并提升跨帧空间线索的稳健整合能力。

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [4] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: 论文从几何视角重审视觉Transformer中的位置编码（PE），提出token级诊断来量化多视角几何一致性对一致PE的依赖；在14个基础ViT上实验证明PE是塑造表征空间结构的因果机制。


<details>
  <summary>Details</summary>
Motivation: 现有工作常把位置编码当作索引或辅助信息，缺乏对其在空间几何结构与多视角一致性中的作用的系统理解。作者希望揭示PE是否以及如何作为几何先验影响ViT的空间推理。

Method: 1) 将PE视为几何先验，提出token级诊断指标，评估在多视角/多视图设置下表征的几何一致性如何随PE的一致性变化；2) 设计控制实验，对比一致/扰动/移除/替换PE等条件；3) 在14个基础ViT模型上广泛评测。

Result: 实验证明：PE不仅是token位置标识，更直接决定了ViT表征的空间结构；当PE保持跨视图一致时，多视角几何一致性与空间推理显著提升；破坏或不一致的PE会明显削弱几何与空间能力。

Conclusion: PE在ViT中充当关键几何先验，是支配表征空间结构的因果因素；理解与设计更好的PE对提升多视角几何一致性与空间推理至关重要。

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

</details>


### [5] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 论文探问：在匹配表示容量并抑制码本坍塌的前提下，单层VQ-VAE是否能达到与层级VQ-VAE相同的重建精度？结论是可以。作者通过改进训练与量化细节，让单层模型在ImageNet高分辨率上重建效果与两层模型持平，挑战了“层级量化天然更优于重建”的常见看法。


<details>
  <summary>Details</summary>
Motivation: 层级VQ-VAE常被认为更擅长重建，因为其分离全局/局部特征。但高层仅从低层获取信息，理论上不应携带额外重建信息。随着训练目标与量化技术进步，作者怀疑以往单层劣势源于码本利用不足与容量不匹配，而非层级结构本身。

Method: 在高分辨率ImageNet上，对比两层VQ-VAE与容量匹配的单层VQ-VAE。系统性地：1) 控制表示预算（码本大小、embedding维度、token速率）匹配；2) 诊断并抑制码本坍塌；3) 采用轻量干预：数据初始化码本、周期性重置不活跃码字、系统调参（如码本规模、承诺损权重、温度/ema等），并考察高维embedding对量化稳定性的影响。以重建误差为主指标，隔离感知质量等下游因素。

Result: 复现并量化了两个现象：单层VQ-VAE受限于码本利用率；embedding维度过高导致量化不稳和码本坍塌。采用上述轻量手段后，显著降低坍塌并提升码本利用率。在表示预算匹配且坍塌得到缓解时，单层VQ-VAE的重建精度与两层层级模型相当。

Conclusion: 层级结构并非重建精度提升的充分必要条件。只要匹配表示容量并控制码本坍塌，单层VQ-VAE可达到与层级VQ-VAE相同的重建水平。层级可能仍有感知或下游优势，但其在纯重建上的“固有优越性”应被重新审视。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [6] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: 提出VMonarch，将视频扩散Transformer的注意力由全量转换为基于Monarch矩阵的结构化稀疏注意力，实现次二次复杂度并显著加速且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散Transformer的注意力计算在时空维度呈二次复杂度，限制了长视频上下文与吞吐；观察到其注意力模式天然稀疏且具结构性，可能被Monarch类结构矩阵良好拟合，从而用结构化稀疏替代密集注意力。

Method: 1) 将Monarch分解适配到时空维度，显式建模帧内与帧间相关性；2) 在交替最小化求解Monarch因子时引入重计算策略以缓解不稳定导致的伪影；3) 设计与FlashAttention融合的在线熵算法，实现长序列下的快速Monarch矩阵更新与动态稀疏模式跟踪。

Result: 在VBench上经少量调参即可达到与全注意力相当或更优的生成质量；在长视频场景中，注意力FLOPs降低17.5倍，注意力计算提速超过5倍；在90%稀疏度下性能优于SOTA稀疏注意力方法。

Conclusion: 结构化Monarch矩阵可有效表达视频DiT的时空稀疏注意力模式；VMonarch实现次二次复杂度与显著加速，同时保持甚至提升生成质量，突破了视频DiT注意力扩展瓶颈。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [7] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: C2R提出从粗糙3D仿真生成真实风格的城市人群视频的生成式渲染框架，通过粗渲染控制布局/相机/轨迹，结合文本引导的神经渲染生成逼真的外观与动态，并用混合CG-真实两阶段训练在无配对数据下实现可控与时序一致的视频合成。


<details>
  <summary>Details</summary>
Motivation: 传统渲染对高质量资产、精确材质与光照、算力依赖大，且在大规模、动态人群场景的可扩展性与真实感方面仍受限；需要一种既可控又能生成真实感强、时序一致的人群城市视频的新范式。

Method: 提出Coarse-to-Real框架：1) 以粗糙3D渲染作为显式控制信号（场景布局、相机运动、人物轨迹）；2) 文本提示引导的神经渲染器负责真实外观、光照与细节动态生成；3) 采用两阶段混合CG-真实训练，先从大规模真实视频学习强生成先验，再通过跨域共享的隐式时空特征引入可控性，解决无配对数据问题；4) 支持从粗到细的控制与多源CG/游戏输入。

Result: 模型能够从最小3D输入生成时间一致、可控、逼真的城市人群视频；在多种CG与游戏输入上具备良好泛化与稳定性。

Conclusion: C2R证明了将粗3D控制与生成式神经渲染结合的有效性，在缺乏配对数据下实现可控、高真实感的人群视频合成，为可扩展城市场景生成提供了实用方案，并计划开源模型与项目。

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

</details>


### [8] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap提出一种无需显式几何投影、可适配任意相机数量与布局的HD地图构建方法，通过几何感知的基础模型与跨帧注意力，在特征空间隐式编码3D理解，并用时空增强与相机感知解码器，实现对缺失视角与传感器变化的鲁棒性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有HD地图方法依赖多相机标定与显式/隐式的2D到BEV几何变换，且通常绑定于固定的N相机阵列，导致在相机故障、视角缺失或车队配置不一致时性能脆弱、部署困难。

Method: 提出FlexMap：1) 使用几何感知的基础模型结合跨帧注意力，在特征空间隐式编码3D场景而不需显式投影；2) 时空增强模块，将跨视角的空间推理与时间动态解耦；3) 相机感知解码器引入潜在“相机token”，实现无投影矩阵的视角自适应注意力；整体对不同相机配置无需改结构或重训。

Result: 在多种相机配置下，FlexMap较现有方法取得更高精度；对缺失视角与传感器变化表现出更强鲁棒性，验证其实用部署潜力。

Conclusion: 通过取消显式几何投影并引入相机自适应与时空分离机制，FlexMap在灵活性、鲁棒性与性能上优于现有HD地图构建方法，适合真实世界多变的车载传感器部署。

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

</details>


### [9] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 提出一种针对视觉-语言模型的越狱框架：结合CoT提示越狱与ReAct驱动的自适应图像加噪，显著提升攻击成功率且保持文本与图像自然性。


<details>
  <summary>Details</summary>
Motivation: VLM在多任务中关键，但对提示极度敏感，安全对齐易被提示变体绕过；需要系统化方法评估并利用这一脆弱性，以检验和改进安全机制。

Method: 两部分：1) 利用后训练的Chain-of-Thought提示构造隐蔽越狱提示，绕过安全过滤；2) 设计ReAct范式驱动的自适应加噪，对输入图像进行迭代扰动，根据模型反馈聚焦可能触发安全防御的区域，逐步精炼对抗噪声。

Result: 双策略在实验中显著提高攻击成功率（ASR），同时文本与视觉输出的“自然性”保持良好，说明具有更强的隐蔽与规避能力。

Conclusion: 将CoT越狱提示与ReAct自适应图像扰动结合，是提升对VLM安全机制规避能力的有效途径；提示与视觉层面的联合攻击更具隐蔽性且更稳定。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [10] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: 该研究评估一种基于变分推断的概率型多视角无标记动作捕捉（MMMC）在步态参数与关节运动学上的“置信区间校准性”和可靠性，发现其置信区间总体校准良好（ECE多<0.1），误差低（步长/跨步长中位误差约16/12 mm；偏差校正后下肢关节角误差1.5–3.8°），且模型预测的不确定性与实际误差强相关，可在无地面真值设备时识别不可靠输出。


<details>
  <summary>Details</summary>
Motivation: 临床与研究需要视频化、易部署的人体运动评估。但MMMC在临床落地不仅要准确，还要能为个体给出可信的误差范围（不确定性量化与校准），以建立信任并支持无地面真值情况下的质量控制。

Method: 在既有工作（用变分推断估计关节角后验分布）基础上，提出/评估一个概率化MMMC框架；在两家机构、共68名受试者数据上，与仪器化步道与标准标记式动作捕捉作对照；用期望校准误差（ECE）评估置信区间校准；报告步/跨步长度及偏差校正后下肢关节运动学误差；分析模型预测不确定性与实际误差的相关性。

Result: 置信区间整体校准良好（ECE通常<0.1）；步长与跨步长中位误差分别约16 mm与12 mm；偏差校正后下肢关节角中位误差1.5–3.8°；模型的不确定性大小与观测误差强相关。

Conclusion: 该概率MMMC能量化并校准其认知不确定性（epistemic），在无并行地面真值的情况下也能标识不可靠输出，具备临床与研究应用的可行性与可信度提升。

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

</details>


### [11] [Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework](https://arxiv.org/abs/2601.22451)
*Shiyu Liu,Xinyi Wen,Zhibin Lan,Ante Wang,Jinsong Su*

Main category: cs.CV

TL;DR: 论文提出一种无需训练的自验证框架，通过语言先验无关的验证步骤，显著降低LVLM在图像描述中的对象幻觉（在LLaVA‑v1.5‑7B上CHAIRI提升65.6%）。


<details>
  <summary>Details</summary>
Motivation: LVLM图像描述常出现“对象幻觉”，主要因为模型在长序列生成中越来越依赖语言先验，导致不存在对象的token概率被抬高。现有方法多做logits校准，但未深入分析“过度依赖”的形成机制，也难以从根源缓解。

Method: 1) 实证分析：发现生成长度增加会加剧对语言先验的依赖，从而增加幻觉对象token的概率。2) 语言先验无关验证（Language-Prior-Free Verification）：让模型在与语言先验解耦的设置下核验候选对象是否存在于图像。3) 自验证框架（Self-Validation），训练免：先对采样得到的候选caption逐对象进行存在性验证，再通过caption选择或聚合来抑制含幻觉的描述。

Result: 在图像描述任务上显著减少对象幻觉，对LLaVA‑v1.5‑7B的CHAIRI指标相对提升65.6%，并优于此前SOTA。

Conclusion: 对象幻觉可通过解耦语言先验并利用LVLM自身的验证能力显著缓解；无需额外训练即可取得SOTA改进，提示从模型内部潜能出发是抑制幻觉的有效路径。

Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and attempts to mitigate it through logits calibration. However, they still lack a thorough analysis of the over-reliance. To gain a deeper understanding of over-reliance, we conduct a series of preliminary experiments, indicating that as the generation length increases, LVLMs' over-reliance on language priors leads to inflated probability of hallucinated object tokens, consequently exacerbating object hallucination. To circumvent this issue, we propose Language-Prior-Free Verification to enable LVLMs to faithfully verify the confidence of object existence. Based on this, we propose a novel training-free Self-Validation Framework to counter the over-reliance trap. It first validates objects' existence in sampled candidate captions and further mitigates object hallucination via caption selection or aggregation. Experiment results demonstrate that our framework mitigates object hallucination significantly in image captioning task (e.g., 65.6% improvement on CHAIRI metric with LLaVA-v1.5-7B), surpassing the previous SOTA methods. This result highlights a novel path towards mitigating hallucination by unlocking the inherent potential within LVLMs themselves.

</details>


### [12] [ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction](https://arxiv.org/abs/2601.22455)
*Yudi Zhang,Yeming Geng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出ScribbleSense：将多模态大模型与图像生成结合，用于3D模型纹理的涂鸦式交互编辑，自动理解涂鸦意图并用全局生成图像提供局部纹理锚点，达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有交互式3D纹理编辑多依赖草图勾勒轮廓，难以处理粗粒度涂鸦；涂鸦语义抽象导致编辑意图与目标位置不明确，影响交互效率与质量。

Method: 利用MLLM视觉理解能力预测涂鸦背后的编辑意图；基于全局生成图像获取局部纹理细节并进行语义锚定，缓解目标定位与语义歧义；整体形成“意图识别—全局生成—局部提取/对齐”的管线。

Result: 实验显示在涂鸦驱动的纹理编辑任务上达到先进性能，交互效果更好，能更准确地把握用户意图并定位目标语义区域。

Conclusion: 将MLLM的语义理解与生成模型的细节合成相结合，可显著提升涂鸦式纹理编辑的可用性与精度；为更自然的3D资产创作提供有效方案。

Abstract: Interactive 3D model texture editing presents enhanced opportunities for creating 3D assets, with freehand drawing style offering the most intuitive experience. However, existing methods primarily support sketch-based interactions for outlining, while the utilization of coarse-grained scribble-based interaction remains limited. Furthermore, current methodologies often encounter challenges due to the abstract nature of scribble instructions, which can result in ambiguous editing intentions and unclear target semantic locations. To address these issues, we propose ScribbleSense, an editing method that combines multimodal large language models (MLLMs) and image generation models to effectively resolve these challenges. We leverage the visual capabilities of MLLMs to predict the editing intent behind the scribbles. Once the semantic intent of the scribble is discerned, we employ globally generated images to extract local texture details, thereby anchoring local semantics and alleviating ambiguities concerning the target semantic locations. Experimental results indicate that our method effectively leverages the strengths of MLLMs, achieving state-of-the-art interactive editing performance for scribble-based texture editing.

</details>


### [13] [Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector](https://arxiv.org/abs/2601.22468)
*Wenqiang Zu,Shenghao Xie,Bo Lei,Lei Ma*

Main category: cs.CV

TL;DR: 提出一种在扩散模型采样过程中注入无监督表征的指导方法，通过投影器在中间去噪阶段对语义进行对齐锚定，显著降低ImageNet类条件生成的FID并提升语义一致性，且与CFG互补。


<details>
  <summary>Details</summary>
Motivation: 现有推理时指导（如CFG、代表性指导）虽能改善语义对齐，但未充分利用无监督视觉表征；同时，扩散Transformer在早期去噪阶段存在由随机性引发的语义漂移，导致即便相同条件也会出现语义不一致。

Method: 训练（或使用）一个表示对齐投影器，将预测的语义表示在采样的若干中间步注入扩散模型，对中间特征进行约束而无需改动主模型架构；该指导可与现有指导（如CFG）叠加。

Result: 在SiT与REPA系模型上，ImageNet类条件生成显著改善：例如REPA-XL/2的FID由5.9降至3.3；相较代表性指导在SiT上亦更优，并与CFG结合可进一步提升语义一致性与画质。

Conclusion: 利用无监督表征对扩散采样进行表示对齐指导，可有效抑制早期语义漂移，增强语义保持与图像一致性，是一种实用且可与现有指导互补的策略。

Abstract: Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.

</details>


### [14] [Head-Aware Visual Cropping: Enhancing Fine-Grained VQA with Attention-Guided Subimage](https://arxiv.org/abs/2601.22483)
*Junfei Xie,Peng Pan,Xulong Zhang*

Main category: cs.CV

TL;DR: 提出HAVC：一种无需训练的“注意力头感知裁剪”方法，通过筛选并精炼多模态模型中的有效注意力头，生成可靠的视觉裁剪引导图，对原图进行二次裁剪，从而提升细粒度VQA中的定位与推理能力，优于现有裁剪策略。


<details>
  <summary>Details</summary>
Motivation: MLLM在VQA上总体表现强，但细粒度推理仍受限：输入分辨率低导致细节丢失；多头注意力聚合含噪，真实视觉对齐（grounding）不稳定。亟需一种无需再训练、可泛化的机制，自动提取与任务相关的高质量视觉证据并聚焦于关键区域。

Method: HAVC分两阶段：1) 头部筛选：利用OCR诊断任务过滤注意力头，仅保留具备真实视觉对齐能力的头。2) 推理时精炼：对保留头施加两类度量——空间熵（促进注意力空间集中）与梯度敏感性（衡量对预测的贡献）——进行再加权与融合，生成稳定的视觉裁剪引导图；据此裁剪最相关子图，并与原图-问题对一起输入MLLM。

Result: 在多个细粒度VQA基准上，HAVC相较SOTA裁剪策略取得更高准确率与更精确的定位与grounding表现，说明其在不改变模型参数的前提下能稳定提升细粒度推理能力。

Conclusion: 通过对多模态注意力头进行“可靠性甄别+推理时精炼”，HAVC在无需训练的条件下显著增强MLLM的视觉对齐与细粒度推理，提供了简单有效、可插拔的提升途径，具有良好的泛化潜力。

Abstract: Multimodal Large Language Models (MLLMs) show strong performance in Visual Question Answering (VQA) but remain limited in fine-grained reasoning due to low-resolution inputs and noisy attention aggregation. We propose \textbf{Head Aware Visual Cropping (HAVC)}, a training-free method that improves visual grounding by leveraging a selectively refined subset of attention heads. HAVC first filters heads through an OCR-based diagnostic task, ensuring that only those with genuine grounding ability are retained. At inference, these heads are further refined using spatial entropy for stronger spatial concentration and gradient sensitivity for predictive contribution. The fused signals produce a reliable Visual Cropping Guidance Map, which highlights the most task-relevant region and guides the cropping of a subimage subsequently provided to the MLLM together with the image-question pair. Extensive experiments on multiple fine-grained VQA benchmarks demonstrate that HAVC consistently outperforms state-of-the-art cropping strategies, achieving more precise localization, stronger visual grounding, providing a simple yet effective strategy for enhancing precision in MLLMs.

</details>


### [15] [PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization](https://arxiv.org/abs/2601.22492)
*Duncan McCain,Hossein Kashiani,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 提出PromptMAD：一种结合跨模态提示与语义对齐的无监督视觉异常检测与定位方法，在MVTec-AD上达到像素级SOTA（mAUC 98.35%、AP 66.54%）。


<details>
  <summary>Details</summary>
Motivation: 多类别视觉异常检测面临类别多样、异常样本稀缺、伪装缺陷难以识别以及像素级类别不均衡的问题。现有重建/对比方法在细粒度纹理与微小异常上易漏检，缺乏高层语义引导与鲁棒的像素级优化。

Method: 1) 跨模态提示：利用CLIP编码的文本提示，描述各类别正常与异常属性，将语义嵌入引入视觉重建与对齐；2) 损失设计：采用Focal Loss缓解像素级正负不均衡，突出难检异常；3) 架构：包含一个监督分割器，融合多尺度卷积特征与Transformer空间注意力，并结合扩散式迭代细化，生成高分辨率异常热力图。

Result: 在MVTec-AD上像素级性能达到SOTA：mAUC 98.35%，AP 66.54%，且在多类别场景保持高效与鲁棒。

Conclusion: 语义提示+视觉语言对齐能增强无监督异常检测对微小与纹理异常的敏感性；结合Focal Loss与多尺度/注意力/扩散细化的分割器，可在多类别数据上实现精确、高分辨率的异常定位并达到SOTA。

Abstract: Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.

</details>


### [16] [MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control](https://arxiv.org/abs/2601.22501)
*Renjie Lu,Xulong Zhang,Xiaoyang Qu,Jianzong Wang,Shangfei Wang*

Main category: cs.CV

TL;DR: 提出MirrorTalk：基于条件扩散模型与语义解耦风格编码器（SDSE）的个性化说话人脸生成方法，实现高保真口型同步与个性风格保留，并通过层级调制在不同面部区域动态融合音频与风格特征，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，说话风格（个体化表情、节奏、习惯动作）与语义内容在面部运动中耦合，导致难以将某个说话者的独特风格可靠迁移到任意语音上，同时还要保持精确的口型同步。

Method: 1) 设计Semantically-Disentangled Style Encoder（SDSE），从短参考视频中提取不含语义信息的“纯风格”表示；2) 采用条件扩散生成框架，以音频与风格为条件；3) 在扩散过程中引入分层（区域级）调制策略，按面部区域（如口部与全脸表情）动态平衡音频与风格特征贡献，确保口型由音频主导、表情和个性由风格主导。

Result: 在多项评测中，相比SOTA显著提升：口型同步更准确，个性化风格保留度更高，全脸动态更自然。

Conclusion: 通过语义-风格解耦与分层调制的扩散生成，MirrorTalk兼顾了口型同步与个性化表达，为个性化说话人脸合成提供了有效范式。

Abstract: Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.

</details>


### [17] [DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation](https://arxiv.org/abs/2601.22507)
*Xin Jiang,Jingwen Chen,Yehao Li,Yingwei Pan,Kezhou Chen,Zechao Li,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: DreamVAR是一种基于视觉自回归（VAR）模型的主体驱动图像生成方法，通过预填充多尺度主体特征并结合强化学习，提升语义对齐与主体一致性，相比扩散模型在外观保真上更优。


<details>
  <summary>Details</summary>
Motivation: 现有主体驱动生成多依赖扩散模型，虽然质量高但推理效率与统一性受限；而VAR模型在统一架构与高效推理方面有潜力，但在主体驱动任务上尚未充分探索，且多尺度条件在自回归推断中存在训练-测试不一致与依赖复杂的问题。

Method: 1) 使用视觉tokenizer提取参考主体的多尺度特征；2) 在生成前，将完整的主体特征序列“预填充”，而不是与目标图像token跨尺度交叉编排，从而简化自回归依赖并缓解多尺度条件下的train-test偏差；3) 引入强化学习联合优化语义对齐与主体一致性。

Result: 在广泛实验中，DreamVAR在主体外观保真和一致性上优于主流扩散方法，同时保持VAR推理高效优势。

Conclusion: 通过预填充多尺度主体特征与RL优化，VAR框架可在主体驱动图像生成中达到或超越扩散方法的外观保真，同时降低自回归依赖复杂度与训练-测试不一致。

Abstract: Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.

</details>


### [18] [CoVA: Text-Guided Composed Video Retrieval for Audio-Visual Content](https://arxiv.org/abs/2601.22508)
*Gyuwon Han,Young Kyun Jang,Chanho Eom*

Main category: cs.CV

TL;DR: 提出CoVA任务与AV-Comp数据集，把音频纳入组合视频检索；并提出AVT选择性对齐文本与最相关模态实现三模态融合，在新基准上优于传统单模态融合。


<details>
  <summary>Details</summary>
Motivation: 现有组合视频检索仅考虑视觉变化，忽视音频差异，导致在真实多模态场景下能力不足与评测不充分。需要一个同时评估视觉与听觉变化的任务、数据与方法。

Method: 1) 构建AV-Comp基准：提供含视觉与音频跨模态变化的视频对以及描述差异的文本查询。2) 提出AVT Compositional Fusion：提取视频、音频、文本特征，通过选择性对齐机制将文本与最相关模态对齐，并进行三模态融合以完成检索。

Result: 在CoVA任务上，AVT优于传统的单模态或简单融合基线，成为强有力基线。示例与资源在项目页提供。

Conclusion: 将音频纳入组合检索显著提升对真实场景的建模与评测；AV-Comp与AVT为该方向提供了基准与有效方法，为多模态检索研究奠定基础。

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a target video from a large gallery using a reference video and a textual query specifying visual modifications. However, existing benchmarks consider only visual changes, ignoring videos that differ in audio despite visual similarity. To address this limitation, we introduce Composed retrieval for Video with its Audio CoVA, a new retrieval task that accounts for both visual and auditory variations. To support this, we construct AV-Comp, a benchmark consisting of video pairs with cross-modal changes and corresponding textual queries that describe the differences. We also propose AVT Compositional Fusion (AVT), which integrates video, audio, and text features by selectively aligning the query to the most relevant modality. AVT outperforms traditional unimodal fusion and serves as a strong baseline for CoVA. Examples from the proposed dataset, including both visual and auditory information, are available at https://perceptualai-lab.github.io/CoVA/.

</details>


### [19] [DNA: Uncovering Universal Latent Forgery Knowledge](https://arxiv.org/abs/2601.22515)
*Jingtong Dou,Chuancheng Shi,Yemin Wang,Shiming Guo,Anqi Yi,Wenhua Wu,Li Zhang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 作者提出DNA框架，通过在预训练模型中唤醒内在“伪造敏感”神经单元来做高保真生成内容检测，无需昂贵的端到端微调；并发布新基准HIFI-Gen。实验显示在少样本、跨架构与未知生成器上具备强鲁棒性、性能优越。


<details>
  <summary>Details</summary>
Motivation: 生成式AI已达到以假乱真的程度，传统依赖表面伪影的检测法失效；现有主流方法依赖对黑箱大模型大规模微调，成本高且泛化差。作者假设伪造检测能力已潜藏于预训练模型中，可被挖掘而非重训。

Method: 提出DNA（Discriminative Neural Anchors）：1）特征解耦与注意力分布迁移分析，定位模型从全局语义转向局部异常的关键中间层；2）引入三元融合打分与曲率截断策略，剔除语义冗余，精确筛取对伪迹敏感的伪造判别单元（FDU）；3）构建高保真合成基准HIFI-Gen以匹配最新生成模型。

Result: 仅依赖选出的“锚点”单元，DNA在少样本设置下取得优于现有方法的检测性能；对不同骨干架构与未见过的生成模型保持强鲁棒与泛化。

Conclusion: 无需昂贵微调，通过“唤醒”预训练模型潜在神经元即可高效、稳健地进行伪造检测；HIFI-Gen为评测前沿生成模型提供了更贴近现实的基准。

Abstract: As generative AI achieves hyper-realism, superficial artifact detection has become obsolete. While prevailing methods rely on resource-intensive fine-tuning of black-box backbones, we propose that forgery detection capability is already encoded within pre-trained models rather than requiring end-to-end retraining. To elicit this intrinsic capability, we propose the discriminative neural anchors (DNA) framework, which employs a coarse-to-fine excavation mechanism. First, by analyzing feature decoupling and attention distribution shifts, we pinpoint critical intermediate layers where the focus of the model logically transitions from global semantics to local anomalies. Subsequently, we introduce a triadic fusion scoring metric paired with a curvature-truncation strategy to strip away semantic redundancy, precisely isolating the forgery-discriminative units (FDUs) inherently imprinted with sensitivity to forgery traces. Moreover, we introduce HIFI-Gen, a high-fidelity synthetic benchmark built upon the very latest models, to address the lag in existing datasets. Experiments demonstrate that by solely relying on these anchors, DNA achieves superior detection performance even under few-shot conditions. Furthermore, it exhibits remarkable robustness across diverse architectures and against unseen generative models, validating that waking up latent neurons is more effective than extensive fine-tuning.

</details>


### [20] [Can 3D point cloud data improve automated body condition score prediction in dairy cattle?](https://arxiv.org/abs/2601.22522)
*Zhou Tang,Jin Wang,Angelo De Castro,Yuxi Zhang,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Xu Wang,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 比较深度图像与三维点云在奶牛体况评分（BCS）预测中的效果。基于1020头奶牛、四种数据设置（原始、全身分割、后躯分割、手工特征），深度图像在原始与全身分割下更准；后躯分割两者相当；手工特征均变差。点云对噪声与模型更敏感，总体未显示稳定优势。


<details>
  <summary>Details</summary>
Motivation: 传统目测BCS主观且费力，影响代谢、繁殖和健康管理。尽管计算机视觉已用于BCS，点云被认为可提供更丰富几何信息，但缺少与深度图像的直接对比评估。作者旨在厘清在实际农场条件下点云是否优于深度图像。

Method: 在商业化牧场采集1020头奶牛数据，比较四种数据形态：未分割原始数据、分割全身、分割后躯、以及基于手工特征的数据。分别基于深度图像和三维点云训练预测模型，并进行以“个体奶牛”为单位的交叉验证以避免数据泄漏，系统评估精度与鲁棒性。

Result: 深度图像在未分割与全身分割设置下精度更高；后躯分割时两者性能相当；手工特征设置使两类方法精度均降低。点云方法对噪声与模型结构更敏感。

Conclusion: 在评估条件下，三维点云并未对BCS预测提供一致或稳定的优势；深度图像更稳健、更易取得较高精度。后续应用应谨慎权衡点云引入的复杂度与收益，并优先考虑后躯区域与端到端学习而非手工特征。

Abstract: Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions.

</details>


### [21] [SHED Light on Segmentation for Dense Prediction](https://arxiv.org/abs/2601.22529)
*Seung Hyun Lee,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: SHED是一种用于密集预测的编码器-解码器框架，通过显式引入分割层级先验来提升深度与语义一致性和跨域泛化，能在仅使用最终输出监督的情况下自发形成层级段结构，带来更锐利的边界、更连贯的区域及更好的3D重建与语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有密集预测方法大多将每个像素独立建模，忽视场景中的结构性，导致边界模糊、区域不一致和跨域泛化差。需要一种能在不增加额外分割标注的前提下，将场景的几何与分割先验融入像素级预测的模型。

Method: 提出SHED：一种包含双向层级推理的编码器-解码器。编码阶段将特征聚合为多层级的segment tokens（层级池化），解码阶段再进行层级反池化，将全局到局部的结构信息传播回像素。训练只在最终输出处监督，无需显式分割标签，模型通过结构诱导自组织出段层级。

Result: 在合成到真实跨域设置下，SHED提升深度边界锐利度与段内一致性，其层级感知解码器更好捕获全局3D布局，从而改进语义分割表现；同时提升3D重建质量，并显现可解释的部件级结构，优于传统像素独立方法。

Conclusion: 将层级分割先验与密集预测耦合，通过无额外分割监督实现结构一致、泛化更强的3D感知。SHED在深度估计、语义分割与3D重建等任务上取得整体提升，并提供更具可解释性的场景部件结构。

Abstract: Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.

</details>


### [22] [Hybrid Cross-Device Localization via Neural Metric Learning and Feature Fusion](https://arxiv.org/abs/2601.22551)
*Meixia Lin,Mingkai Liu,Shuxue Peng,Dikai Fan,Shengyu Gu,Xianliang Huang,Haoyang Ye,Xiao Liu*

Main category: cs.CV

TL;DR: 提出一种用于 CroCoDL 2025 挑战的混合跨设备定位管线，结合共享检索编码器、几何 PnP 分支与神经前馈分支，并加入神经引导候选裁剪和深度约束精化，在 HYDRO 与 SUCCU 基准上显著提升召回与精度，最终在 R@0.5m,5° 上得分 92.62。


<details>
  <summary>Details</summary>
Motivation: 跨设备（不同传感器/相机）场景下的视觉定位存在外观域差、尺度与几何不一致等问题，单一几何或单一神经方法往往在召回、鲁棒性或精度上存在短板。需要一种能够兼顾检索、粗到细定位、以及尺度/姿态精化的混合方案，以在挑战基准（HYDRO、SUCCU）上提升整体性能。

Method: 1) 共享检索编码器：统一图像表示以进行候选地图帧检索；2) 两条互补定位分支：a) 经典几何分支，做特征融合、匹配与 PnP 位姿估计；b) 神经前馈分支（MapAnything），在几何先验条件下直接回归度量位姿；3) 神经引导候选裁剪：依据平移一致性过滤不可靠地图帧；4) 深度条件定位：在 Spot 场景利用深度信息校正度量尺度与平移，提高精度；整体形成混合、级联的定位流水线。

Result: 在 HYDRO 与 SUCCU 两个基准上召回率与定位精度显著提升，在 CroCoDL 2025 挑战中 R@0.5m,5° 指标取得 92.62 分。

Conclusion: 混合式管线结合检索、几何与神经回归，并辅以候选裁剪与深度条件精化，能在跨设备定位中实现高召回与高精度；该策略在公开基准与比赛中验证有效。

Abstract: We present a hybrid cross-device localization pipeline developed for the CroCoDL 2025 Challenge. Our approach integrates a shared retrieval encoder and two complementary localization branches: a classical geometric branch using feature fusion and PnP, and a neural feed-forward branch (MapAnything) for metric localization conditioned on geometric inputs. A neural-guided candidate pruning strategy further filters unreliable map frames based on translation consistency, while depth-conditioned localization refines metric scale and translation precision on Spot scenes. These components jointly lead to significant improvements in recall and accuracy across both HYDRO and SUCCU benchmarks. Our method achieved a final score of 92.62 (R@0.5m, 5°) during the challenge.

</details>


### [23] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 提出一个针对多种视觉语言任务的“即插即用”选择性预测方案PaPSP，并进一步引入带检索记忆与对比归一化的MA-PaPSP，在选择性字幕、图文匹配与细粒度分类上显著提升拒绝机制的稳定性与校准性。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测大多面向封闭集任务，难以覆盖从封闭到开放、从有限到无界词表的视觉语言任务（如图像描述），且训练代价高、对基础模型侵入性强。作者希望用训练自由、低复杂度、可泛化到任意基础模型的方法，为开放设定下的视觉语言模型提供可靠的拒绝策略。

Method: 提出PaPSP：利用外部视觉-语言嵌入（如CLIP）对模型输出进行置信评估，实现“即插即用”的选择性预测。发现两大问题：图文表示不稳定导致嵌入方差高，及相似度分数校准差。为此提出MA-PaPSP：1) 引入图文对检索记忆库，对输入样本检索最近邻并对其嵌入做均值以降低方差；2) 采用对比式归一化改进相似度分数的校准，从而更稳健地设定拒绝阈值。

Result: 在多个数据集上，MA-PaPSP在选择性图像描述、图文匹配和细粒度分类任务中均优于原始PaPSP和其他选择性预测基线，表现为更好的风险-覆盖权衡与更稳健的置信度校准。

Conclusion: 基于外部嵌入的训练自由选择性预测在开放视觉语言任务中可行，而通过检索记忆与对比归一化可显著缓解嵌入不稳定与分数失校准问题，提升选择性预测性能。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [24] [DELNet: Continuous All-in-One Weather Removal via Dynamic Expert Library](https://arxiv.org/abs/2601.22573)
*Shihong Liu,Kun Zuo,Hanguang Xiao*

Main category: cs.CV

TL;DR: 提出DELNet，一个用于天气图像恢复的持续学习框架：通过“判别阀”评估任务相似度、动态专家库复用与增量扩展，实现对已知/未知退化的无重训适配，并在多数据集上显著超越现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有一体化天气图像复原方法依赖预收集数据，对未见退化需重训，成本高、部署不便；需要能在持续到来的新退化任务上增量学习且不破坏已学能力。

Method: 构建DELNet：1) 判别阀（judging valve）度量任务相似度，用于判别新/已知任务；2) 动态专家库存储针对不同退化训练的专家。面对新任务时，阀门选择top-k相关专家进行知识迁移，同时新增专家学习任务特异特征；面对已知任务时，直接复用对应专家。通过这种选择与扩展机制实现连续优化，无需重训已有模型。

Result: 在OTS、Rain100H、Snow100K上，相比SOTA持续学习方法，PSNR分别提升约16%、11%、12%。表现出更好的效果、鲁棒性与效率。

Conclusion: DELNet能在不重训的情况下持续适配新退化并复用已学专家，显著提升恢复质量并降低部署成本，适合实际场景的长周期上线与扩展。

Abstract: All-in-one weather image restoration methods are valuable in practice but depend on pre-collected data and require retraining for unseen degradations, leading to high cost. We propose DELNet, a continual learning framework for weather image restoration. DELNet integrates a judging valve that measures task similarity to distinguish new from known tasks, and a dynamic expert library that stores experts trained on different degradations. For new tasks, the valve selects top-k experts for knowledge transfer while adding new experts to capture task-specific features; for known tasks, the corresponding experts are directly reused. This design enables continuous optimization without retraining existing models. Experiments on OTS, Rain100H, and Snow100K demonstrate that DELNet surpasses state-of-the-art continual learning methods, achieving PSNR gains of 16\%, 11\%, and 12\%, respectively. These results highlight the effectiveness, robustness, and efficiency of DELNet, which reduces retraining cost and enables practical deployment in real-world scenarios.

</details>


### [25] [Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding](https://arxiv.org/abs/2601.22574)
*Yuansheng Gao,Jinman Zhao,Tong Zhang,Xingguo Xu,Han Bao,Zonghui Wang,Wenzhi Chen*

Main category: cs.CV

TL;DR: 提出一种用于视频大语言模型的“时空-语义对比解码”方法，通过与破坏一致性的负特征对比，抑制幻觉，同时保持通用视频理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM尽管在理解与问答上表现强，但仍存在“幻觉”（与视频内容或事实不一致的输出）。已有解码方法多为启发式，难以精确刻画幻觉的根因及其细粒度的时序与语义关联，鲁棒性与泛化性在复杂场景下受限。

Method: 在推理阶段构造“负特征”：有意识地打破视频特征的时空一致性与语义关联；随后进行对比式解码，将生成过程与原始视频特征对齐、与负特征拉开，从而抑制与破坏一致性的偏差对齐。

Result: 大量实验表明，该方法显著降低视频幻觉发生率，同时不损伤（甚至保持）模型原有的视频理解与推理能力。

Conclusion: 时空与语义层面的对比解码可作为缓解视频LLM幻觉的有效策略，兼顾鲁棒性与通用性，在复杂场景中具备更好泛化。

Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.

</details>


### [26] [PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios](https://arxiv.org/abs/2601.22575)
*Xudong Lu,Huankang Guan,Yang Bo,Jinpeng Chen,Xintong Guo,Shuhan Li,Fang Liu,Peiwen Sun,Xueying Li,Wei Zhang,Xue Yang,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: PhoStream提出首个面向移动场景的连续流多模态评测基准，涵盖屏上/屏外、多模态与时间推理，发现现有MLLM在“何时开口”上存在显著短板：前向预测任务分数骤降，主要因过早作答。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型多在离线、短视频或选择题环境中评测，难以反映手机端连续音视频流的真实交互需求（需要持续跟踪并在适时回应）。缺少统一覆盖屏上/屏外情境、开放式问答与时间维度的基准。

Method: 构建PhoStream基准：- 场景与能力：4类场景、10项能力，578段视频，共5,572条开放式QA，统一评估视频、音频与时间推理（即时、回溯、前向）。- 数据管线：自动生成式数据管线产出候选QA，辅以严格的人类校验。- 评测管线：模拟真实在线推理（流式输入与响应时机），并采用LLM-as-a-Judge对开放式回答打分（0-100）。

Result: 在LLM裁判评分下，模型在“即时与回溯”任务表现良好（如Gemini 3 Pro>80），但在“前向”任务大幅下降（约16.40）。分析表明关键原因是模型在关键信息尚未出现前提前作答（时机判断失误）。

Conclusion: 当前MLLM不仅在“说什么”上需要提升，更在“何时说”上存在核心瓶颈。PhoStream为移动端连续流场景提供系统化评测与分析工具，有助于推动模型的时间敏感决策与多模态流式交互能力发展。

Abstract: Multimodal Large Language Models excel at offline audio-visual understanding, but their ability to serve as mobile assistants in continuous real-world streams remains underexplored. In daily phone use, mobile assistants must track streaming audio-visual inputs and respond at the right time, yet existing benchmarks are often restricted to multiple-choice questions or use shorter videos. In this paper, we introduce PhoStream, the first mobile-centric streaming benchmark that unifies on-screen and off-screen scenarios to evaluate video, audio, and temporal reasoning. PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios and 10 capabilities. We build it with an Automated Generative Pipeline backed by rigorous human verification, and evaluate models using a realistic Online Inference Pipeline and LLM-as-a-Judge evaluation for open-ended responses. Experiments reveal a temporal asymmetry in LLM-judged scores (0-100): models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before the required visual and audio cues appear. This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/PhoStream.

</details>


### [27] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: 提出MIFOMO用于跨域小样本高光谱图像分类，通过冻结基础模型骨干并引入快速适配与域混合策略，在严苛设定下显著优于现有方法，最高提升约14%。


<details>
  <summary>Details</summary>
Motivation: 现有CDFSL-HSI方法常依赖不现实的外部噪声增广，且需要大量参数更新，易过拟合；尚未利用具有强泛化能力的遥感基础模型来快速适配下游任务。

Method: 以大规模遥感预训练的基础模型为骨干，提出三项关键技术：1) 共聚投影（CP）：在冻结骨干下，通过轻量投影层实现快速适配；2) 混合域适配（MDM）：通过mixup策略缓解源-目标极端域差异；3) 标签平滑：缓解伪标签噪声带来的训练不稳定。

Result: 在多组严谨实验中，MIFOMO在跨域小样本HSI分类上优于现有方法，最高领先约14%，验证了方法的有效性和鲁棒性。

Conclusion: 利用RS基础模型的可迁移表示，结合CP、MDM和标签平滑，可在数据稀缺与强域移位下实现更稳健的HSI CDFSL；代码已开源，便于复现与拓展。

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [28] [FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data](https://arxiv.org/abs/2601.22596)
*Abdelrrahman Moubane*

Main category: cs.CV

TL;DR: FOTBCD 是一个覆盖法国本土28个省的大规模建筑变化检测数据集（0.2 m/像素），提供二值变化掩膜（约2.8万对前后影像）与实例级子集，并专为跨地域域偏移评测而设计；基准表明更高的地理多样性带来更好的跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有建筑变化检测基准多局限于单城或小区域，导致模型在跨地域场景下泛化不足；需要一个规模更大、地域多样、可用于域外评测且高质量标注的数据集。

Method: 从法国国家地理信息局的正射影像与地形建筑数据构建数据集：覆盖28个省（25训练、3保留评测），0.2 m分辨率；发布FOTBCD-Binary（约2.8万对前后影像，像素级二值变化掩膜与补丁级空间元数据），并人工核验验证/测试样本；同时发布FOTBCD-Instances示例性实例级标注子集，展示完整标注方案。使用固定参考基线，将FOTBCD-Binary与LEVIR-CD+、WHU-CD进行对比基准。

Result: 在固定基线下，对比LEVIR-CD+与WHU-CD，FOTBCD 的地理多样性带来更强的跨域泛化性能；验证与测试来自地理隔离省份，标注质量经人工核验。

Conclusion: FOTBCD 为建筑变化检测提供了一个地理覆盖广、标注高质量、适合大规模与跨域评测的新基准；结果表明提升数据集地理多样性是改进跨域泛化的有效途径。

Abstract: We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.

</details>


### [29] [TTSA3R: Training-Free Temporal-Spatial Adaptive Persistent State for Streaming 3D Reconstruction](https://arxiv.org/abs/2601.22615)
*Zhijie Zheng,Xinhao Xiang,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出TTSA3R：一个无需训练的自适应状态更新框架，通过时间与空间一致性信号缓解流式循环3D重建中的遗忘问题；在长序列上将误差增幅控制在15%，显著优于基线的>200%。


<details>
  <summary>Details</summary>
Motivation: 流式循环模型通过持久状态实现高效3D重建，但在长序列中因需要在历史与新观测间权衡而产生灾难性遗忘。现有注意力导向的方法多只在单一维度自适应，忽视时间演化与空间观测质量的一致性，导致更新策略不稳、长期重建劣化。

Method: 提出训练自由的TTSA3R框架：1) 时间自适应更新模块（TAU），分析状态的时间演化模式以调节更新幅度；2) 空间上下文更新模块（SCU），通过观测-状态对齐与场景动态性定位需要更新的空间区域；3) 融合两类互补信号，形成逐步或局部的状态更新策略。

Result: 在多种3D任务与长序列设置下，TTSA3R显著提升稳定性：相较基线在超长序列中>200%误差恶化，TTSA3R仅有约15%误差增幅。定量与可视化结果均验证其有效性。

Conclusion: 结合时间演化与空间质量的自适应信号可有效缓解流式循环3D重建的长期遗忘，TTSA3R无需训练、兼容多任务与模型，并在长序列上显著提升鲁棒性与精度。

Abstract: Streaming recurrent models enable efficient 3D reconstruction by maintaining persistent state representations. However, they suffer from catastrophic memory forgetting over long sequences due to balancing historical information with new observations. Recent methods alleviate this by deriving adaptive signals from attention perspective, but they operate on single dimensions without considering temporal and spatial consistency. To this end, we propose a training-free framework termed TTSA3R that leverages both temporal state evolution and spatial observation quality for adaptive state updates in 3D reconstruction. In particular, we devise a Temporal Adaptive Update Module that regulates update magnitude by analyzing temporal state evolution patterns. Then, a Spatial Contextual Update Module is introduced to localize spatial regions that require updates through observation-state alignment and scene dynamics. These complementary signals are finally fused to determine the state updating strategies. Extensive experiments demonstrate the effectiveness of TTSA3R in diverse 3D tasks. Moreover, our method exhibits only 15% error increase compared to over 200% degradation in baseline models on extended sequences, significantly improving long-term reconstruction stability. Our codes will be available soon.

</details>


### [30] [UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating](https://arxiv.org/abs/2601.22616)
*Xing Yi,Jinyang Huang,Feng-Qi Cui,Anyang Tong,Ruimin Wang,Liu Liu,Dan Guo*

Main category: cs.CV

TL;DR: 提出UniGeo统一3D室内点云检测框架，通过几何感知学习与动态通道门控，显式建模稀疏场景几何关系并自适应强化关键通道特征，在6个室内数据集上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨数据集统一训练方法难以在稀疏点云中有效建模几何关系，并忽略重要区域的特征分布，限制了3D检测性能，尤其在机器人与AR等实际应用中需要更稳健和泛化的检测。

Method: 1) 几何感知学习模块：从空间关系到特征权重的可学习映射，实现显式几何特征增强；2) 动态通道门控：基于可学习的通道级权重，对稀疏3D U-Net输出进行自适应优化，突出关键几何信息；整体形成统一的室内3D检测训练框架UniGeo。

Result: 在六个不同的室内场景数据集上做了广泛实验，方法优于现有方法（摘要未给出具体数值），显示更强的跨数据集与稀疏场景适应性。

Conclusion: 结合几何关系建模与通道级动态加权能显著提升点云3D室内检测的表达与泛化能力，UniGeo在多数据集上验证了其有效性。

Abstract: The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.

</details>


### [31] [LINA: Linear Autoregressive Image Generative Models with Continuous Tokens](https://arxiv.org/abs/2601.22630)
*Jiahao Wang,Ting Pan,Haoge Deng,Dongchen Han,Taiqiang Wu,Xinlong Wang,Ping Luo*

Main category: cs.CV

TL;DR: 论文提出并系统研究了在连续token自回归视觉生成中使用线性注意力的高效设计，并据此构建了全线性注意力的T2I模型LINA，在显著降低FLOPs的同时取得与SOTA相当的生成质量。


<details>
  <summary>Details</summary>
Motivation: 连续token的自回归视觉生成（尤其T2I）前景看好，但计算开销大；现有线性注意力设计众多，缺乏针对生成任务的可扩展性比较与最佳实践指南。

Method: 1) 对比两类线性注意力归一化：除法式（division-based）与减法式（subtraction-based），并按参数规模进行扩展性实证分析；2) 评估并引入深度可分离卷积以增强局部性；3) 将因果线性注意力中的门控机制扩展到双向情形，提出对K/V状态的可学习数据无关门（KV gate），实现token级记忆权重分配；4) 基于上述发现构建全线性注意力的T2I模型LINA。

Result: - 除法式归一化在生成型线性Transformer上随参数规模扩展更佳（尽管减法式在分类有效）。- 融合卷积的局部建模对自回归生成至关重要，与扩散模型结论一致。- KV gate在双向线性注意力中提供灵活“遗忘/记忆”管理。- LINA可从指令生成1024×1024高保真图像：ImageNet类条件FID=2.18（~1.4B参数），GenEval T2I=0.74（~1.5B参数）。单个线性注意力模块较softmax注意力FLOPs下降约61%。代码开源。

Conclusion: 为连续token自回归T2I提供了经过验证的线性注意力设计准则：优先采用除法归一化、加入局部卷积、在双向设置中使用KV门以管理记忆；据此实现的LINA在显著省算力的同时保持强竞争力，指向更大规模、低成本的线性生成Transformer路线。

Abstract: Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation.
  Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models.
  We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models.
  Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: https://github.com/techmonsterwang/LINA.

</details>


### [32] [What can Computer Vision learn from Ranganathan?](https://arxiv.org/abs/2601.22634)
*Mayukh Bagchi,Fausto Giunchiglia*

Main category: cs.CV

TL;DR: 论文指出计算机视觉中的语义鸿沟源于视觉与词汇语义错配，提出借鉴Ranganathan的分类原则来指导数据集与标注方法（vTelos），并提供初步实验显示标注质量与模型准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前CV数据集和基准因视觉-词汇语义错配而导致标注含糊、类别定义不一致与评测失真，缺乏系统化的分类与标注框架；作者希望找到一个有理论根基的方法学来缩小语义鸿沟并提升数据集质量。

Method: 将S.R. Ranganathan的图书馆学分类原则（如范型分析、互斥且穷尽、清晰的类目定义与组合规则）适配到CV标注流程，形成vTelos标注方法学；通过原则化的本体/词汇设计、属性与关系的分解、以及系统化的标注指引来构建/修正数据集；并进行对比实验评估标注一致性与模型性能。

Result: 基于vTelos的标注在一致性与清晰度上优于传统做法，且在下游CV任务上带来准确率提升；实验为初步证据，规模与细节有限但趋势正向。

Conclusion: Ranganathan分类原则可作为解决CV语义鸿沟的制度化起点；vTelos方法在实践中改进了标注与模型表现，支持其可行性与价值，建议进一步在更大规模和多任务上验证与完善。

Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.

</details>


### [33] [Unsupervised Synthetic Image Attribution: Alignment and Disentanglement](https://arxiv.org/abs/2601.22663)
*Zongfang Liu,Guangyi Chen,Boyang Sun,Tongliang Liu,Kun Zhang*

Main category: cs.CV

TL;DR: 提出一种无监督的合成图像归因方法“对齐与解缠”（Alignment & Disentanglement），利用对比自监督对齐与Infomax解缠，无需成对标注即可在AbC基准上超越有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法依赖“合成图像—原始训练源”的成对标注，获取成本高且不现实；随着合成图像质量提升，识别其背后训练概念对版权保护与模型透明度至关重要，因此需要摆脱昂贵标注的无监督方案。

Method: 两阶段：1）用对比自监督学习（如MoCo/DINO风格）做基础概念对齐，学习跨域一致的表示；2）引入Infomax损失促进表示解缠，提高归因可分性。理论上以跨协方差假设刻画自监督模型的天然跨域对齐能力，并将“对齐+解缠”解释为对典型相关分析（CCA）目标的分解来近似概念匹配。

Result: 在真实世界基准AbC上，无监督方法出人意料地优于有监督对照，在合成图像归因任务上取得SOTA或显著提升（摘要未给出具体数值）。

Conclusion: 无需成对标注即可通过对齐与解缠实现有效归因，验证了自监督模型天然的跨域对齐能力。该思路为合成图像归因提供了新的无监督范式，并对版权与透明度场景具有实用价值。

Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.

</details>


### [34] [ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding](https://arxiv.org/abs/2601.22666)
*Junyi Hu,Tian Bai,Fengyi Wu,Wenyan Li,Zhenming Peng,Yi Zhang*

Main category: cs.CV

TL;DR: ExpAlign是一种基于多实例学习的轻量级开放词汇视觉-语言对齐框架，通过期望对齐头进行注意力加权的软MIL池化与能量式多尺度一致性正则，显著提升开放词汇检测与零样本实例分割，特别是在长尾类上（LVIS minival AP_r 36.2）。


<details>
  <summary>Details</summary>
Motivation: 开放词汇定位需要在弱监督下实现精细的视觉-语言对齐。现有方法要么依赖全局句向量，缺乏细粒度表达；要么进行显式的token级对齐，需额外标注或使用沉重的交叉注意力，影响效率与可扩展性。

Method: 提出ExpAlign，基于多实例学习（MIL）的理论框架：1）期望对齐头（Expectation Alignment Head）对token-区域相似度进行注意力驱动的软MIL池化，实现隐式token与实例选择，无需额外标注；2）能量式多尺度一致性正则，包括Top-K多正样本对比学习目标与几何感知一致性目标（由带拉格朗日约束的自由能最小化推导），以稳定对齐训练；3）整体保持轻量与推理高效。

Result: 在开放词汇检测与零样本实例分割任务上取得稳定提升，尤其对长尾类别显著有效；在LVIS minival上达到AP_r 36.2，在同等模型规模下优于其他SOTA。

Conclusion: 通过将注意力化的软MIL与能量式一致性正则相结合，ExpAlign在无需额外监督与保持高效的前提下，实现更稳健、细粒度的视觉-语言对齐，带来开放词汇定位类任务的SOTA或准SOTA表现，特别改善长尾类。

Abstract: Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.

</details>


### [35] [VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)
*Hanxun Yu,Wentong Li,Xuan Qu,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: 提出VisionTrim：一个训练-free的MLLM加速框架，通过选择关键视觉token与文本引导的token合并，显著减少视觉token且保持/提升性能，适用于高分辨率与视频场景。


<details>
  <summary>Details</summary>
Motivation: MLLM在高分辨率图像与视频任务中有大量视觉token，计算昂贵。现有token裁剪/合并方法多在单一环节操作，忽视与文本对齐，常导致性能下降，因此需要兼顾效率与语义对齐的统一加速方案。

Method: 构建训练-free、可插拔的两模块框架：1) DVTS（Dominant Vision Token Selection）：结合全局-局部视角选取最关键的视觉token，保留语义核心信息；2) TGVC（Text-Guided Vision Complement）：利用文本线索进行上下文感知的token合并，在减少冗余的同时补足与文本相关的视觉信息。两个模块联合实现统一的token裁剪与合并策略，适配图像与视频MLLM。

Result: 在多种图像与视频多模态基准上进行大量实验，VisionTrim在显著降低视觉token的同时优于现有方法，兼顾速度与精度，展现出更优的实际部署价值。

Conclusion: VisionTrim作为训练-free、统一且可插拔的加速框架，通过DVTS与TGVC在不牺牲对齐的前提下有效减少视觉token，提升MLLM在实际应用中的可用性与效率；代码已开源。

Abstract: Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.

</details>


### [36] [Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition](https://arxiv.org/abs/2601.22675)
*Shuhan Ye,Yuanbin Qian,Yi Yu,Chong Wang,Yuqi Xie,Jiazhen Xu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: 论文指出：SNN 的固有时间动态像低通滤波，抑制运动信息而偏好静态内容，致使其在视频任务上落后；提出仅含两参数的 PBO 模块，优化时间通带以突出运动频段，在多种视频任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管 SNN 在能耗与时间建模上具优势，但在视频等动态任务上明显落后于 ANN。作者怀疑根源不在于训练或结构规模，而在于神经动力学对频带的偏置：标准脉冲动力学等效于时间低通，导致与视频任务中“运动频带为主”的需求不匹配。需要一种方法在不改变 SNN 架构的前提下，校正这种通带错配。

Method: 提出 Pass-Bands Optimizer（PBO）：一个可插拔、几乎零开销的时间频带整形模块。核心思想是通过仅两个可学习参数调整时间响应，抑制静态成分、强调运动承载频带，相当于对输入/膜电位进行高通强化；并加入轻量的一致性约束，保持语义与边界不被破坏。PBO无需改动网络结构、训练代价极小，可直接集成到现有 SNN 管线。

Result: 在 UCF101 上带来超过 10 个百分点的准确率提升；在更复杂的多模态动作识别与弱监督视频异常检测任务上也获得显著且稳定的增益。

Conclusion: SNN 在视频任务上的短板源于时间通带与任务需求的不匹配。通过 PBO 对通带进行任务对齐、突出运动信息，可在不改变结构的情况下显著提升 SNN 的视频理解能力，为 SNN 处理动态视觉提供了新视角。

Abstract: Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to artificial neural networks (ANNs). In this work, we diagnose a fundamental pass-band mismatch: Standard spiking dynamics behave as a temporal low pass that emphasizes static content while attenuating motion bearing bands, where task relevant information concentrates in dynamic tasks. This phenomenon explains why SNNs can approach ANNs on static tasks yet fall behind on tasks that demand richer temporal understanding.To remedy this, we propose the Pass-Bands Optimizer (PBO), a plug-and-play module that optimizes the temporal pass-band toward task-relevant motion bands. PBO introduces only two learnable parameters, and a lightweight consistency constraint that preserves semantics and boundaries, incurring negligible computational overhead and requires no architectural changes. PBO deliberately suppresses static components that contribute little to discrimination, effectively high passing the stream so that spiking activity concentrates on motion bearing content. On UCF101, PBO yields over ten percentage points improvement. On more complex multi-modal action recognition and weakly supervised video anomaly detection, PBO delivers consistent and significant gains, offering a new perspective for SNN based video processing and understanding.

</details>


### [37] [Visual Personalization Turing Test](https://arxiv.org/abs/2601.22680)
*Rameen Abdal,James Burgess,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出视觉个性化图灵测试（VPTT），以“与目标人物可能创作/分享内容在感知上不可区分”为标准评估个性化生成；给出基准集、检索增强生成器与文本打分指标，并校准至人类/VLM判断，显示高相关性；所提VPRAG在对齐-原创性上最佳，且可扩展与隐私友好。


<details>
  <summary>Details</summary>
Motivation: 现有个性化视觉生成多以“复制身份/风格”评估，忽视现实场景中的“与某人可能产出的内容是否不可分辨”的目标；缺乏统一基准与可扩展、隐私安全的评测指标和生成框架。

Method: 1) 定义VPTT：若生成结果被人类或校准的VLM判为与某人可能产出不可区分，则通过；2) 构建VPTT框架：包含1万画像基准（VPTT-Bench）、视觉检索增强生成器（VPRAG），以及仅文本的VPTT Score指标；3) 以人类与VLM评审对VPTT Score进行校准与相关性验证。

Result: 人类、VLM与VPTT Score三者评估高度相关；VPRAG在保持个性化对齐与内容原创性之间取得最佳平衡，优于对比方法。

Conclusion: VPTT提供了以感知不可分辨为核心的个性化评估范式；VPTT Score可作为可靠的感知代理；VPRAG为可扩展、隐私安全的个性化生成奠定基础。

Abstract: We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.

</details>


### [38] [OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection](https://arxiv.org/abs/2601.22685)
*Binyi Su,Chenghao Huang,Haiyong Chen*

Main category: cs.CV

TL;DR: 提出OOVDet：在零样本场景下既识别已知(IV)类又可靠拒识未知(OOV)类，通过合成区域级OOV提示与挖掘高不确定性伪OOV图像，并以低密度先验构建决策边界，显著提升OOV检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本OOV检测方法容易对IV类过拟合，导致未知类被高置信度误判为已知类；缺少对OOV数据分布的先验与有效的拒识机制。

Method: 1) 在隐藏空间中对各类的条件高斯分布进行建模，从其低似然区域采样，合成区域级OOV提示，基于“未知语义更可能出现在潜空间低密度区域”的假设；2) 提出基于Dirichlet的梯度归因机制，把归因梯度解释为Dirichlet证据以估计预测不确定性，从高不确定性样本中挖掘伪OOV图像；3) 结合合成的OOV提示与伪OOV图像，用高斯核密度估计施加低密度先验约束，构建OOV决策边界并正则化OOV类的优化。

Result: 在零样本场景的实验中，方法在OOV检测指标上显著优于现有方法（摘要未给具体数值），代码已开源。

Conclusion: 利用低密度先验+合成OOV提示+Dirichlet不确定性挖掘，可在零样本条件下更好地区分已知与未知类别，有效缓解对IV类的过拟合，提升OOV拒识能力。

Abstract: Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model's lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.

</details>


### [39] [PEAR: Pixel-aligned Expressive humAn mesh Recovery](https://arxiv.org/abs/2601.22693)
*Jiahao Wu,Yunfei Liu,Lijian Lin,Ye Zhu,Lei Zhu,Jingyi Li,Yu Li*

Main category: cs.CV

TL;DR: PEAR提出一种快速稳健的像素对齐表达式人体网格重建框架，从单张野外图像恢复细致3D人体（含脸手）并在>100 FPS实时输出SMPLX与scaled-FLAME参数，显著提升姿态与细节精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于SMPLX的方法推理慢、只能给出粗糙姿态、在人脸与手等细粒度区域易错位或出现不自然伪影，难以满足下游任务对实时性与精细度的需求。

Method: 采用统一简洁的ViT主干在低开销下回归粗几何；为弥补简化架构导致的细节损失，引入像素级监督以优化几何、强化脸手等细节；并提出模块化数据标注策略以丰富训练监督、提升鲁棒性。框架无需预处理，可同时输出SMPLX与scaled-FLAME（EHM-s）参数。

Result: 在多项基准上，相比既有SMPLX方法在姿态估计精度上取得显著提升；推理速度超过100 FPS，兼具精度与实时性；重建在脸与手等细节部位更准确、伪影更少。

Conclusion: PEAR通过统一ViT+像素级监督+模块化标注，实现无需预处理的实时、细粒度的表达性人体重建，缓解了现有方法的速度、细节定位与表情捕获不足的问题，适用于下游应用。

Abstract: Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as the face and hands. These issues make current approaches difficult to apply to downstream tasks. To address these challenges, we propose PEAR-a fast and robust framework for pixel-aligned expressive human mesh recovery. PEAR explicitly tackles three major limitations of existing methods: slow inference, inaccurate localization of fine-grained human pose details, and insufficient facial expression capture. Specifically, to enable real-time SMPLX parameter inference, we depart from prior designs that rely on high resolution inputs or multi-branch architectures. Instead, we adopt a clean and unified ViT-based model capable of recovering coarse 3D human geometry. To compensate for the loss of fine-grained details caused by this simplified architecture, we introduce pixel-level supervision to optimize the geometry, significantly improving the reconstruction accuracy of fine-grained human details. To make this approach practical, we further propose a modular data annotation strategy that enriches the training data and enhances the robustness of the model. Overall, PEAR is a preprocessing-free framework that can simultaneously infer EHM-s (SMPLX and scaled-FLAME) parameters at over 100 FPS. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves substantial improvements in pose estimation accuracy compared to previous SMPLX-based approaches. Project page: https://wujh2001.github.io/PEAR

</details>


### [40] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: 论文提出Bi-MCQ，将视觉-语言对齐从“全局相似度最大化”改为“条件语义比较”，通过双向多选任务与方向特异交叉注意力，显著提升医学VLM对否定陈述的理解与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在医学领域对否定（如“无肺炎”）理解薄弱，因对比学习将否定当作微小表述差异，且多标签、提示式InfoNCE易偏向“易正样本”，导致对疾病缺失的学习不足。

Method: 将对齐重构为条件语义比较：设计双向多选（Bi-MCQ）框架，联合训练图到文与文到图两种MCQ任务，构造肯定/否定/混合提示作为选项；并引入方向特异的Cross-Attention融合模块，分别适配双向推理的非对称线索，减少对齐干扰。

Result: 在ChestXray14、Open-I、CheXpert、PadChest上，相比SOTA零样本CARZero，否定理解AUC最高提升0.47；在PNC评价上最高提升0.08；相比InfoNCE微调，肯-否AUC差距平均缩小0.12。

Conclusion: 通过目标函数重构与双向多选建模，Bi-MCQ有效增强医学VLM对否定陈述的语义理解与稳健性，缩小肯定与否定性能差距，优于基于对比学习的微调范式。

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [41] [DAVIS: OOD Detection via Dominant Activations and Variance for Increased Separation](https://arxiv.org/abs/2601.22703)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: 提出DAVIS，一种在GAP后加入通道方差与最大激活统计的后处理方法，缓解GAP信息丢失，显著提升OOD检测（多架构、多数据集FPR95显著降低）。


<details>
  <summary>Details</summary>
Motivation: 现有后处理OOD检测多依赖GAP得到的倒数第二层特征，但GAP会丢弃激活图的重要统计信息，导致判别力不足；作者认为被忽视的通道内方差与最大激活对区分OOD很关键。

Method: 在既有网络与后处理框架不变的前提下，对卷积激活图在GAP之前提取每个通道的方差与最大值，并与GAP均值拼接成扩展特征向量（可直接替换原特征用于常见分数/分类器）；方法简单、架构无关、可即插即用。

Result: 在ResNet、DenseNet、EfficientNet、MobileNet-v2等上，DAVIS刷新OOD检测SOTA：以FPR95为例，CIFAR-10/ResNet-18提升48.26%，CIFAR-100/ResNet-34提升38.13%，ImageNet-1k/MobileNet-v2提升26.83%。

Conclusion: 超越仅用均值（GAP）的做法，补充通道方差与最大激活等统计能显著增强OOD检测；分析解释了其改进机理，为后处理OOD检测提供了有原则的特征强化方向。

Abstract: Detecting out-of-distribution (OOD) inputs is a critical safeguard for deploying machine learning models in the real world. However, most post-hoc detection methods operate on penultimate feature representations derived from global average pooling (GAP) -- a lossy operation that discards valuable distributional statistics from activation maps prior to global average pooling. We contend that these overlooked statistics, particularly channel-wise variance and dominant (maximum) activations, are highly discriminative for OOD detection. We introduce DAVIS, a simple and broadly applicable post-hoc technique that enriches feature vectors by incorporating these crucial statistics, directly addressing the information loss from GAP. Extensive evaluations show DAVIS sets a new benchmark across diverse architectures, including ResNet, DenseNet, and EfficientNet. It achieves significant reductions in the false positive rate (FPR95), with improvements of 48.26\% on CIFAR-10 using ResNet-18, 38.13\% on CIFAR-100 using ResNet-34, and 26.83\% on ImageNet-1k benchmarks using MobileNet-v2. Our analysis reveals the underlying mechanism for this improvement, providing a principled basis for moving beyond the mean in OOD detection.

</details>


### [42] [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/abs/2601.22709)
*Yanlong Chen,Amirhossein Habibian,Luca Benini,Yawei Li*

Main category: cs.CV

TL;DR: 提出GRACE：在信息瓶颈框架下统一蒸馏与量化感知训练的VLM压缩方法，借助置信门控蒸馏、关系CKA对齐与拉格朗日自适应控制器，实现INT4下显著提速和内存节省，同时精度接近甚至超过FP16基线与教师。


<details>
  <summary>Details</summary>
Motivation: VLM推理昂贵，后量化常显著掉点；而QAT在VLM上研究不足。需要一种在受限比特预算下有原则地保留对任务有用信息的训练框架，并兼顾稳定性与泛化。

Method: 以信息瓶颈为统一视角：量化限制容量，蒸馏决定保留的信息。具体包括：1) 置信门控的解耦蒸馏：以教师置信度筛除不可靠监督，并对不同模态/目标分量采用解耦损失；2) 关系型中心化核对齐（R-CKA）：对齐视觉token间的结构关系以保留视觉语义几何；3) 拉格朗日松弛的自适应控制器：动态平衡保真度（对教师/任务的拟合）与容量约束（量化误差/正则），自动调节权重；4) 在VLM（LLaVA、Qwen家族）上进行INT4 QAT，配合真实INT4算子实现端到端评估。

Result: INT4模型在多基准上稳定优于FP16基线并逼近教师：如LLaVA-1.5-7B在SQA 70.1 vs 66.8，Qwen2-VL-2B在MMBench 76.9 vs 72.6；使用真实INT4内核达3倍吞吐、54%显存降低，显著优于既有量化方法。

Conclusion: GRACE在信息瓶颈下将蒸馏与QAT系统化结合，兼顾效率与精度，能在INT4上实现大幅加速与内存节省且精度接近教师，为资源受限场景提供实用的VLM部署方案。

Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.

</details>


### [43] [OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation](https://arxiv.org/abs/2601.22725)
*Jin Li,Tao Chen,Shuai Jiang,Weijie Wang,Jingwen Luo,Chenhui Wu*

Main category: cs.CV

TL;DR: 提出OpenVTON-Bench：一个约10万对、最高1536×1536分辨率的大规模VTON评测基准，并配套一个多模态、五维度的评测协议，较传统指标显著更符合人类判断。


<details>
  <summary>Details</summary>
Motivation: 现有VTON评估难以可靠衡量细粒度纹理与语义一致性；公开数据集在规模、多样性与商业级覆盖上不足，导致研究与产业评估脱节。

Method: 1) 数据集：用DINOv3层级聚类进行语义均衡抽样，结合Gemini密集描述，覆盖20个细粒度服饰类别，形成约10万对高分辨率图像。2) 评测协议：提出五个可解释维度（背景一致性、身份保真、纹理保真、形状合理性、整体真实感）；结合VLM进行语义推理；引入基于SAM3分割与形态学腐蚀的多尺度表示指标，将边界对齐误差与内部纹理伪影分离度量。

Result: 与人类主观一致性显著提高：肯德尔τ=0.833，相比SSIM的0.611更优；验证了方法在可靠性上的优势。

Conclusion: OpenVTON-Bench及其多模态评测协议为VTON提供了更大规模、更细粒度且与人类感知更一致的评测基准，有望成为后续研究与工业落地的标准工具。

Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $τ$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.

</details>


### [44] [GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction](https://arxiv.org/abs/2601.22729)
*A. Enes Doruk,Hasan F. Ates*

Main category: cs.CV

TL;DR: GaussianOcc3D 以连续3D高斯为统一表示，融合相机与LiDAR，解决体素重、BEV有损与多模态错位，借助四个模块实现高效稳健的3D语义占据预测，并在多个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要稠密、细粒度的3D环境理解。单模态方法在相机语义和激光几何之间权衡受限；多模态框架易受模态异质性、空间不对齐，以及体素表示计算开销大、BEV信息丢失等“表示危机”困扰。亟需一种既内存高效又能自然对齐两种模态的统一3D表示。

Method: 提出基于连续3D高斯的统一表征，并设计四个关键模块：1) LDFA：以深度可变形采样将稀疏LiDAR信号提升并投射到高斯基元上；2) EBFS：基于熵的特征平滑以抑制域噪声；3) ACLF：不确定性感知的相机-激光融合，自适应重加权不同传感器可靠性；4) Gauss-Mamba Head：结合选择性状态空间模型（SSM），以线性复杂度建模全局上下文。

Result: 在Occ3D、SurroundOcc、SemanticKITTI上分别获得49.4%、28.9%、25.2的mIoU，且在雨天、夜间等恶劣条件下表现更稳健，达成SOTA。

Conclusion: 连续3D高斯作为多模态统一支撑能缓解体素/BEV表示矛盾，并结合不确定性感知融合与SSM头部实现高效、鲁棒的3D语义占据预测。

Abstract: 3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.

</details>


### [45] [ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model](https://arxiv.org/abs/2601.22730)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Taichun Zhou,Xinwang Liu*

Main category: cs.CV

TL;DR: 提出ImgCoT：将链式思维(CoT)压缩的重构目标由文本改为图像，从而减少语言偏置、突出推理结构；并提出“松弛版”用少量关键文本步补全细节，兼顾结构与细节，显著用更少token保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有用自编码器把长CoT压成潜在token，多以文本重构为目标，导致潜在表示不得不保留词汇与句法等表层语言特征，形成强语言归纳偏置，难以抽象出跨步推理结构，影响压缩与泛化效率。

Method: 1) ImgCoT：将文本CoT渲染为“视觉CoT”（图像化的推理步骤及其布局），自编码器以视觉重构为目标，学习带空间归纳偏置的潜在token，聚焦全局推理结构；2) Loose ImgCoT：在视觉潜在token基础上，用基于低token对数似然选取的少量关键文本推理步进行增强，形成图像+关键文本的混合表示，既保留结构又补充细节，同时减少总token。

Result: 在多数据集与多种LLM上，两版ImgCoT均优于基线：以更少token达到相当或更好的推理正确率，显示对全局结构的捕获更强；松弛版在保持高正确率的同时进一步压缩token。

Conclusion: 以视觉重构替代文本重构能有效缓解语言偏置，使潜在token更关注推理结构；结合少量关键文本可兼顾细节与精度，提供一种高效、紧凑的CoT表示与推理方案。

Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.

</details>


### [46] [Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models](https://arxiv.org/abs/2601.22737)
*Enyi Shi,Pengyang Shao,Yanxin Zhang,Chenhang Cui,Jiayi Lyu,Xu Xie,Xiaobo Xia,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出Lingua-SafetyBench，一个覆盖10种语言、包含100,440个有害图文对的多语言多模态安全基准，用于系统评估VLLMs在图像主导与文本主导风险下的鲁棒性差异，发现高资源语言在图像主导攻击下ASR更高，非高资源语言在文本主导攻击下更脆弱；模型规模/版本升级总体降ASR但加剧语言不均衡，提示需进行语言与模态感知的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有安全评测要么多语言但仅文本、要么多模态但单语种，缺乏语义扎根的图文对与真实跨模态交互覆盖，无法全面揭示VLLMs在多语言多模态输入下的安全风险与不均衡。

Method: 构建Lingua-SafetyBench：10种语言、100,440个有害图文对，并按风险来源拆分为图像主导与文本主导子集；评测11个开源VLLMs；对Qwen系列做受控实验，分析规模化与版本升级对攻击成功率（ASR）的影响与语言差异。

Result: 出现稳定不对称：图像主导风险在高资源语言（HRLs）上ASR更高；文本主导风险在非高资源语言（Non-HRLs）更严重。扩大规模/升级版本整体降低ASR，但收益偏向HRLs，扩大HRLs与Non-HRLs差距，尤其在文本主导风险下。

Conclusion: 单纯扩模无法解决多语言多模态安全的不均衡；需要面向语言与模态的细粒度安全对齐策略与数据。将公开基准、代码与模型以促复现与后续研究。

Abstract: Robust safety of vision-language large models (VLLMs) under joint multilingual and multimodal inputs remains underexplored. Existing benchmarks are typically multilingual but text-only, or multimodal but monolingual. Recent multilingual multimodal red-teaming efforts render harmful prompts into images, yet rely heavily on typography-style visuals and lack semantically grounded image-text pairs, limiting coverage of realistic cross-modal interactions. We introduce Lingua-SafetyBench, a benchmark of 100,440 harmful image-text pairs across 10 languages, explicitly partitioned into image-dominant and text-dominant subsets to disentangle risk sources. Evaluating 11 open-source VLLMs reveals a consistent asymmetry: image-dominant risks yield higher ASR in high-resource languages, while text-dominant risks are more severe in non-high-resource languages. A controlled study on the Qwen series shows that scaling and version upgrades reduce Attack Success Rate (ASR) overall but disproportionately benefit HRLs, widening the gap between HRLs and Non-HRLs under text-dominant risks. This underscores the necessity of language- and modality-aware safety alignment beyond mere scaling.To facilitate reproducibility and future research, we will publicly release our benchmark, model checkpoints, and source code.The code and dataset will be available at https://github.com/zsxr15/Lingua-SafetyBench.Warning: this paper contains examples with unsafe content.

</details>


### [47] [StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing](https://arxiv.org/abs/2601.22738)
*Han Wang,Deyi Ji,Lanyun Zhu,Jiebo Luo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 提出StreamSense：一种流式社交信号检测框架，用轻量级流式编码器处理常规时间点，对困难/模糊片段选择性升级到VLM，并在上下文不足时延迟决策；通过跨模态对比学习和IoU加权损失训练；在多种直播社交任务上，比仅用VLM更准、更低时延与计算。


<details>
  <summary>Details</summary>
Motivation: 直播场景需要对视频、文本、音频的社会信号进行低延迟、连续监测与响应，证据往往部分且异步；仅依赖大型VLM推理成本高、延迟大，且训练标签在时间片段边界处易相互干扰。需要一种既高效又可靠、能在不确定时合理处理的流式检测方法。

Method: 设计StreamSense架构：1) 轻量级流式编码器持续处理多模态输入（视频/音频/文本），并在大多数时间戳直接给出预测；2) 选择性路由机制：当样本困难或含糊时升级到VLM专家；3) 决策延迟策略：在上下文不足时暂缓输出，等待更多证据；4) 训练目标包括：i) 跨模态对比损失，将视觉/音频线索与文本信号对齐；ii) IoU加权损失，对与目标段重叠度低的帧/段降低权重，从而减少跨段标签干扰。

Result: 在多种直播社交检测任务（如情感分类、仇恨内容审核）上评测：相较仅用VLM的流式方案，StreamSense以较少频次调用VLM即可取得更高准确率，同时显著降低平均时延与计算开销。

Conclusion: 选择性升级与决策延迟是理解流式社交任务的有效原语；通过轻量级编码器+VLM专家的分层推理，能在保证精度的同时降低延迟与成本。代码已在GitHub开源。

Abstract: Live streaming platforms require real-time monitoring and reaction to social signals, utilizing partial and asynchronous evidence from video, text, and audio. We propose StreamSense, a streaming detector that couples a lightweight streaming encoder with selective routing to a Vision-Language Model (VLM) expert. StreamSense handles most timestamps with the lightweight streaming encoder, escalates hard/ambiguous cases to the VLM, and defers decisions when context is insufficient. The encoder is trained using (i) a cross-modal contrastive term to align visual/audio cues with textual signals, and (ii) an IoU-weighted loss that down-weights poorly overlapping target segments, mitigating label interference across segment boundaries. We evaluate StreamSense on multiple social streaming detection tasks (e.g., sentiment classification and hate content moderation), and the results show that StreamSense achieves higher accuracy than VLM-only streaming while only occasionally invoking the VLM, thereby reducing average latency and compute. Our results indicate that selective escalation and deferral are effective primitives for understanding streaming social tasks. Code is publicly available on GitHub.

</details>


### [48] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: 提出FaceDefense：通过新的扩散损失与方向性人脸属性编辑，并采用两阶段交替优化，实现对扩散式换脸的更强主动防御，在不显著破坏人脸结构的前提下提升防护效果。


<details>
  <summary>Details</summary>
Motivation: 扩散式换脸效果强但加剧滥用风险（肖像权侵犯、名誉受损）。现有主动防御存在核心权衡：扰动大则面部结构被破坏、可感知性差；扰动小则防护弱，效果不稳。需要一种同时兼顾不可感知性与防护强度的方法。

Method: 提出FaceDefense框架：1）设计新的“扩散损失”，直接在扩散生成过程中度量与优化对换脸模型的干扰，增强对扩散式换脸的抑制；2）引入“方向性人脸属性编辑”，针对扰动引起的面部结构/属性偏移进行可控回正，恢复自然外观、提高不可感知性；3）两阶段交替优化，先强化防御性，再以属性编辑校正失真，迭代至收敛，得到最终对抗样本。

Result: 大量实验表明，相比现有方法，FaceDefense在视觉不可感知性与防御有效性两方面均显著领先，取得更优权衡；在同等可感知水平下能更有效地阻断扩散式换脸，在同等防御强度下视觉质量更高。

Conclusion: FaceDefense通过扩散损失+方向性属性编辑+两阶段优化，打破“扰动强度-外观失真”的权衡，成为对扩散式换脸更实用且鲁棒的主动防御方案。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [49] [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/abs/2601.22754)
*Guillermo Gil de Avalle,Laura Maruster,Christos Emmanouilidis*

Main category: cs.CV

TL;DR: 论文评估两种视觉语言模型在从工业故障排查指南中抽取结构化知识的效果，对比标准指令提示与融入版式线索的增强提示，揭示模型在版式敏感度与语义稳健性间的取舍。


<details>
  <summary>Details</summary>
Motivation: 工业现场的故障排查指南以类似流程图的版式呈现，空间布局与技术语言共同表达含义。为了将其集成到操作员支持系统，需要先将信息自动抽取为机器可理解的结构。但人工抽取成本高且易出错。VLM 有望结合视觉与文本来自动化这一流程，但其在此类资料上的表现尚缺系统评估。

Method: 选择两种代表性的视觉语言模型，设计并对比两类提示策略：1) 常规的指令式提示；2) 融合流程图布局模式线索的增强提示。以从指南中抽取结构化知识（如节点、条件、操作步骤及其关系）为任务，评测两模型在布局理解与语义抽取上的表现。

Result: 两模型在不同提示下表现出模型特定的权衡：一种对布局线索更敏感、能更好还原流程结构但语义提取较脆弱；另一种语义鲁棒性更强、术语与指令理解更准确，但对复杂布局的捕捉较弱。增强提示能提升对布局的把握，但可能引入语义误判或过拟合特定模式。

Conclusion: 在将故障指南知识注入支持系统时，需要根据应用权衡：若强调流程结构完整性，应选版式敏感更强并配合布局增强提示；若强调术语与指令的正确性，应偏向语义鲁棒模型与标准提示。结果为实际部署提供了提示策略选择与模型选型的依据。

Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.

</details>


### [50] [Is Training Necessary for Anomaly Detection?](https://arxiv.org/abs/2601.22763)
*Xingwu Zhang,Guanxuan Li,Paul Henderson,Gerardo Aragon-Camarasa,Zijun Long*

Main category: cs.CV

TL;DR: 提出无需训练的基于检索的多类无监督异常检测（RAD），用记忆库多层检索替代重建残差，跨多基准达SOTA，并证明检索分数上界重建残差。


<details>
  <summary>Details</summary>
Motivation: 现有MUAD多依赖编码器-解码器重建正常样本特征，通过重建残差判异常，但存在“保真-稳定”权衡：提高重建保真度削弱对异常的敏感性，反之亦然，需要新范式突破。

Method: 摒弃重建范式，构建训练无关的记忆库存储正常特征；在测试时对图像/补丁进行多级检索，与记忆库匹配得到相似度/距离作为异常分数。方法不需任务特定训练，适配标准与小样本设置；并给出理论结果：检索得分为重建残差得分的上界。

Result: 在MVTec-AD、VisA、Real-IAD、3D-ADAM四个基准上达SOTA；在MVTec-AD中，仅用1张正常图像即可达到96.7% Pixel AUROC，接近使用全部数据的98.5%。

Conclusion: MUAD并非必须任务特定训练；基于记忆的检索即可实现SOTA异常检测，且理论上优于基于重建残差的打分，上述结果打破长期假设。

Abstract: Current state-of-the-art multi-class unsupervised anomaly detection (MUAD) methods rely on training encoder-decoder models to reconstruct anomaly-free features. We first show these approaches have an inherent fidelity-stability dilemma in how they detect anomalies via reconstruction residuals. We then abandon the reconstruction paradigm entirely and propose Retrieval-based Anomaly Detection (RAD). RAD is a training-free approach that stores anomaly-free features in a memory and detects anomalies through multi-level retrieval, matching test patches against the memory. Experiments demonstrate that RAD achieves state-of-the-art performance across four established benchmarks (MVTec-AD, VisA, Real-IAD, 3D-ADAM) under both standard and few-shot settings. On MVTec-AD, RAD reaches 96.7\% Pixel AUROC with just a single anomaly-free image compared to 98.5\% of RAD's full-data performance. We further prove that retrieval-based scores theoretically upper-bound reconstruction-residual scores. Collectively, these findings overturn the assumption that MUAD requires task-specific training, showing that state-of-the-art anomaly detection is feasible with memory-based retrieval. Our code is available at https://github.com/longkukuhi/RAD.

</details>


### [51] [Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2601.22778)
*Nan Zhong,Yiran Xu,Mian Zou*

Main category: cs.CV

TL;DR: 提出DCCT，通过模拟相机CFA与去马赛克引发的颜色相关性差异，训练自监督模型来区分相机拍照与AI生成图像，跨20+未见生成器显著提升泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于生成伪迹的检测器在新模型/后处理下泛化失效。相机成像链（CFA采样与去马赛克）在真实照片中引入稳定的颜色相关结构，而AI生成图像往往不遵循这些物理约束。利用这一本征差异可获得更稳健的鉴伪特征。

Method: 提出“去马赛克引导的颜色相关训练”(DCCT)。模拟CFA，将RGB图分解：选取单通道为条件输入，另两通道为预测目标；用自监督U-Net学习条件分布，采用逻辑斯蒂混合模型参数化输出以建模像素级条件分布。理论上证明该训练显式捕获并放大真实照片与AI图在颜色相关特征上的分布差异。基于学得的特征构建二分类器进行真伪判别。

Result: 在涉及20+未见生成器的数据上达到SOTA的泛化与鲁棒性，显著优于基于生成伪迹的方法；对多种扰动与后处理仍保持高性能。

Conclusion: 利用相机物理成像导致的颜色相关结构作为鉴伪信号，可绕过特定生成器伪迹依赖，实现强泛化AI图像检测。DCCT提供了可证明的分布差异刻画与有效的自监督训练范式。

Abstract: As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.

</details>


### [52] [Diachronic Stereo Matching for Multi-Date Satellite Imagery](https://arxiv.org/abs/2601.22808)
*Elías Masquil,Luca Savant Aira,Roger Marí,Thibaud Ehret,Pablo Musé,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 提出首个用于遥感卫星影像的“异时立体匹配”（Diachronic Stereo Matching, DSM）方法，实现跨季节/光照变化的两时相影像三维重建；通过在包含异时样本的数据集上微调具备单目深度先验的SOTA立体网络（MonSter），在WorldView‑3多时相数据上，较传统与未适配深度模型在同步与异步场景均显著提升，平均高程误差由3.99 m降至1.23 m。


<details>
  <summary>Details</summary>
Motivation: 传统立体重建假设两视影像近同步获取；一旦跨月甚至跨季节，季节、光照、阴影差异导致匹配失败。多时相NeRF/Gaussian方法虽准确但需大量观测且不适用于仅两张相隔甚久的影像。需能在仅有一对时间相隔显著的卫星影像条件下实现稳健三维重建。

Method: 以预训练的MonSter立体网络为基础（在SceneFlow、KITTI等合成与真实数据上训练），引入单目深度先验；在专门整理的DFC2019派生的遥感立体数据上进行微调，数据涵盖同步与异步、多季节与多光照情形；训练后在WorldView‑3多时相对上评估，与经典管线和未微调深度模型比较。

Result: 在WorldView‑3多日期数据上，方法在同步与异步场景都优于传统管线与零样本深度模型；在Omaha异时（冬-秋）样例中，平均高程误差从3.99 m（零样本）降至1.23 m（本文方法），几何更接近LiDAR真值。

Conclusion: 在包含异时样本的数据上微调、并结合单目深度先验，是实现跨时间差异卫星影像稳健立体匹配的关键；该策略将此前不兼容的异时影像对转化为可用的三维重建输入，具备实际价值并可推广到多种季节与光照条件。

Abstract: Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions. On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations. On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs. However, when the two images are captured months apart, strong seasonal, illumination, and shadow changes violate standard stereoscopic assumptions, causing existing pipelines to fail. This work presents the first Diachronic Stereo Matching method for satellite imagery, enabling reliable 3D reconstruction from temporally distant pairs. Two advances make this possible: (1) fine-tuning a state-of-the-art deep stereo network that leverages monocular depth priors, and (2) exposing it to a dataset specifically curated to include a diverse set of diachronic image pairs. In particular, we start from a pretrained MonSter model, trained initially on a mix of synthetic and real datasets such as SceneFlow and KITTI, and fine-tune it on a set of stereo pairs derived from the DFC2019 remote sensing challenge. This dataset contains both synchronic and diachronic pairs under diverse seasonal and illumination conditions. Experiments on multi-date WorldView-3 imagery demonstrate that our approach consistently surpasses classical pipelines and unadapted deep stereo models on both synchronic and diachronic settings. Fine-tuning on temporally diverse images, together with monocular priors, proves essential for enabling 3D reconstruction from previously incompatible acquisition dates. Left image (winter) Right image (autumn) DSM geometry Ours (1.23 m) Zero-shot (3.99 m) LiDAR GT Figure 1. Output geometry for a winter-autumn image pair from Omaha (OMA 331 test scene). Our method recovers accurate geometry despite the diachronic nature of the pair, exhibiting strong appearance changes, which cause existing zero-shot methods to fail. Missing values due to perspective shown in black.  Mean altitude error in parentheses; lower is better.

</details>


### [53] [FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images](https://arxiv.org/abs/2601.22809)
*Haiyang Wu,Weiliang Mu,Jipeng Zhang,Zhong Dandan,Zhuofei Du,Haifeng Li,Tao Chao*

Main category: cs.CV

TL;DR: 提出FarmMind：一种面向农田遥感影像的“推理-查询驱动”的动态分割框架，先识别歧义成因再按需查询外部辅助图像（高分辨率/大范围/邻时相等），突破仅用单幅补丁的静态分割局限，在精度与泛化上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FRSI分割多依赖单一输入补丁，信息受限，遇到模糊、遮挡、尺度变化、类间相似等复杂情景时推理能力不足；而人类专家会主动调取辅助影像进行交叉验证。作者希望让模型具备类似“先分析歧义、再有针对性取证”的能力。

Method: 设计FarmMind框架的“推理-查询”机制：1）对当前分割不确定性进行因果式分析，定位歧义来源（如分辨率不足、上下文缺失、时间差异导致的外观变化等）；2）基于分析策略化决定查询何种辅助影像（更高分辨率、更大空间范围、邻近时相等）；3）动态融合主图与被查询到的辅助图，完成更可靠的语义分割；与直接盲目查询不同，强调先推理、后有目的地查询。

Result: 在多个数据集上进行广泛实验，FarmMind在分割精度和跨场景泛化能力上显著优于现有方法。代码与数据公开以便复现。

Conclusion: 通过将人类专家的“先推理后取证”策略引入遥感分割，FarmMind打破静态分割范式限制，实现按需动态查询与融合外部信息，在复杂、歧义场景下显著提升性能与泛化。

Abstract: Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.

</details>


### [54] [A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions](https://arxiv.org/abs/2601.22830)
*Ji Zhou,Yilin Ding,Yongqi Zhao,Jiachen Xu,Arno Eichberger*

Main category: cs.CV

TL;DR: 论文系统评估10个代表性大视觉语言模型（LVLM）在PeSOTIF数据集上的2D目标检测表现，与YOLO基线比较。结果：LVLM在复杂自然场景与退化下召回率显著更高（>25%），但YOLO在合成扰动下的几何精度更好。结论：语义推理与几何回归互补，LVLM适合作为SOTIF安全验证的高层感知冗余。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在不利条件下的环境感知不足导致SOTIF风险；传统检测器在长尾与退化场景下易失效。LVLM具备语义推理潜力，但其在安全关键2D检测任务上的定量有效性缺乏系统研究。

Method: 构建或采用PeSOTIF基准（覆盖长尾交通情景与多种环境退化），选取10个代表性LVLM，统一评测其2D目标检测能力，并与YOLO基线进行量化比较（召回、精度、对自然/合成退化鲁棒性等）。

Result: 顶级LVLM（如Gemini 3、豆包）在复杂自然场景中相对YOLO召回率提升超过25%，对视觉退化更鲁棒；但YOLO在合成扰动下几何定位精度仍占优。

Conclusion: LVLM与传统检测器具有互补性：LVLM提供强语义鲁棒性与高召回，传统方法提供更稳定的几何精度。建议在SOTIF框架中将LVLM用作高层安全验证与冗余感知模块。

Abstract: Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.

</details>


### [55] [NativeTok: Native Visual Tokenization for Improved Image Generation](https://arxiv.org/abs/2601.22837)
*Bin Wu,Mengqi Huang,Weinan Jia,Zhendong Mao*

Main category: cs.CV

TL;DR: 论文提出NativeTok，一种在token化阶段引入因果依赖的原生视觉token化框架，通过约束token序列关系，缓解两阶段VQ生成中token化与生成不匹配的问题，并在高效重建与训练效率上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有VQ两阶段生成中，改进第一阶段tokenizer并不能保证第二阶段生成更好，因为token之间缺乏依赖约束，生成模型需从“无序”分布中学习，导致偏差与弱一致性。作者希望在token化阶段就注入因果关系，使后续生成直接受益。

Method: 提出原生视觉token化（NativeTok），在token化时强制因果依赖：
- Meta Image Transformer（MIT）进行潜在图像建模，提供全局/局部潜在特征。
- Mixture of Causal Expert Transformer（MoCET）：多个轻量专家块，每个专家仅生成一个token，条件为先前tokens与潜在特征，形成自回归式因果链。
- 分层原生训练（Hierarchical Native Training）：逐层/逐专家解冻，仅更新新增专家块，提高训练效率并稳定因果结构学习。

Result: 在多项实验中，NativeTok实现高效重建（重建质量提升、码率效率更优）并改善生成的一致性与偏差问题；相较基线展现更好的关系建模与下游生成性能。

Conclusion: 在token化阶段注入因果依赖能显著缓解VQ两阶段生成中的分布不匹配。NativeTok通过MIT+MoCET与分层训练，实现了兼顾重建质量、关系约束与训练效率的视觉token化方案，实验验证其有效性。

Abstract: VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.

</details>


### [56] [Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model](https://arxiv.org/abs/2601.22838)
*Zhijing Yang,Weiwei Zhang,Mingliang Yang,Siyuan Peng,Yukai Shi,Junpeng Tan,Tianshui Chen,Liruo Zhong*

Main category: cs.CV

TL;DR: 提出Cu-VTON任务与NCT框架，基于扩散模型并引入语义增强与语义控制模块，实现可定制人物（外观、姿态、属性）下的高保真服装试穿。实验显示优于公开基线。


<details>
  <summary>Details</summary>
Motivation: 传统VTON通常固定人物与有限编辑能力，难以满足用户对化身外观、姿态、属性等自由定制与高保真服饰语义/纹理保真的双重需求。需要一种既能灵活编辑人物又能准确保留服装语义与细节的方法，提升虚拟试穿的灵活性与沉浸感。

Method: 提出NCT框架：1) 语义增强模块，利用服装语义描述与视觉-语言编码器学习跨模态对齐特征，并作为条件注入扩散模型，增强服装语义与纹理保留；2) 语义控制模块，将服装图像、定制姿态（含表情）图像与语义描述作为控制信号，协同编辑人物姿态/外观/属性，同时维持服装细节；整体基于可控扩散模型实现生成与编辑。

Result: 在公开基准上进行大量实验，NCT在保持服装语义与细节、以及对人物姿态和属性的灵活编辑方面表现优于现有方法（具体数值未给出）。

Conclusion: NCT有效解决了Cu-VTON场景下“可定制人物+服装高保真”双目标，语义增强与控制模块协同提升服装保真与编辑灵活性，达到当前最优或领先性能。

Abstract: This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.

</details>


### [57] [How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models](https://arxiv.org/abs/2601.22841)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 作者研究遥感领域的大模型是否像计算机视觉那样需要大规模扩展，发现并非如此：在相同FLOPs压缩到1%时，遥感FMs还能保持>71%的相对精度，而CV的ImageNet MAE仅<10%。这表明RS模型在较小规模就进入过参数化，存在大量冗余。作者提出“事后瘦身”和“可学习瘦身”作为部署与诊断工具，并用方差解释率与特征相关性分析揭示冗余机制。


<details>
  <summary>Details</summary>
Motivation: 现有RS基础模型多沿用CV的扩展范式，但RS数据分布、任务需求与CV不同，直接套用“越大越好”的假设可能低效或错误。需要系统检验RS模型在多大规模进入过参数化，以及是否存在可利用的表示冗余，以指导高效训练与部署。

Method: 提出“事后瘦身”（post-hoc slimming）：对已预训练的编码器按宽度均匀缩减以测量表示冗余；在六个SOTA RS FMs上、四个下游分类任务中评估不同FLOPs预算下的性能保持率。对比一个在ImageNet上的MAE作为CV基线。进一步引入“可学习瘦身”（slimmable training）来同时训练多宽度子网，适用于MoCo与MAE框架。用解释方差比（EVR）与特征相关性分析揭示信息冗余分布。

Result: 在1% FLOPs时，CV的ImageNet MAE精度<10%，而RS FMs能保持>71%的相对精度，显示约7倍差异，强力支持RS模型更早进入过参数化。可学习瘦身训练在MoCo和MAE模型上带来额外性能改进。EVR和特征相关性结果表明任务相关信息在通道/特征间高度冗余且分布广泛。

Conclusion: RS基础模型较小尺度即出现过参数化，简单增大参数常导致冗余而非新表征。事后瘦身既是资源受限环境的实用部署策略，也是检验扩展假设的诊断工具。该工作挑战了RS沿用CV扩展范式的共识，并建议通过可学习瘦身等方法实现更高效的RS模型训练与推理；代码将开源。

Abstract: Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.

</details>


### [58] [Inference-Time Dynamic Modality Selection for Incomplete Multimodal Classification](https://arxiv.org/abs/2601.22853)
*Siyi Du,Xinzhe Luo,Declan P. O'Regan,Chen Qin*

Main category: cs.CV

TL;DR: 提出DyMo：在推理时动态选择并整合可靠的重建模态，以最大化与任务相关的信息，避免“丢弃或填补”的两难；通过将信息量与任务损失建立联系，设计可计算的奖励作为选择准则，并配合可组合的多模态网络与训练策略，在多数据集和缺失场景上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实多模态数据常不完整。现有方法要么丢弃缺失模态丢信息，要么恢复缺失模态引入噪声，导致性能不稳。需要一种能在推理时自适应地权衡“哪些恢复模态有用、哪些该舍弃”的机制，以最大化与任务相关的信息利用。

Method: 1) 提出DyMo推理框架：对每个测试样本动态评估各候选（真实与恢复）模态的“任务相关信息”。2) 理论上把不可直接估计的信息量与任务损失建立可计算联系，在推理时用任务损失构造原则化奖励函数，作为模态选择的代理目标。3) 设计可兼容任意模态组合的灵活多模态网络结构，并提出相应训练策略以获得稳健表示。

Result: 在多种自然图像与医学影像数据集、各类缺失比例/模式下，DyMo在准确率等指标上显著超过不完整/动态多模态学习的SOTA方法。

Conclusion: 通过基于任务损失的奖励来在推理时选择并整合可靠的（包括恢复的）模态，DyMo打破“丢弃或填补”的二选一框架，充分挖掘任务相关信息，带来稳健而优越的性能。

Abstract: Multimodal deep learning (MDL) has achieved remarkable success across various domains, yet its practical deployment is often hindered by incomplete multimodal data. Existing incomplete MDL methods either discard missing modalities, risking the loss of valuable task-relevant information, or recover them, potentially introducing irrelevant noise, leading to the discarding-imputation dilemma. To address this dilemma, in this paper, we propose DyMo, a new inference-time dynamic modality selection framework that adaptively identifies and integrates reliable recovered modalities, fully exploring task-relevant information beyond the conventional discard-or-impute paradigm. Central to DyMo is a novel selection algorithm that maximizes multimodal task-relevant information for each test sample. Since direct estimation of such information at test time is intractable due to the unknown data distribution, we theoretically establish a connection between information and the task loss, which we compute at inference time as a tractable proxy. Building on this, a novel principled reward function is proposed to guide modality selection. In addition, we design a flexible multimodal network architecture compatible with arbitrary modality combinations, alongside a tailored training strategy for robust representation learning. Extensive experiments on diverse natural and medical image datasets show that DyMo significantly outperforms state-of-the-art incomplete/dynamic MDL methods across various missing-data scenarios. Our code is available at https://github.com//siyi-wind/DyMo.

</details>


### [59] [Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction](https://arxiv.org/abs/2601.22861)
*Refael Sheffer,Chen Pinchover,Haim Zisman,Dror Ozeri,Roee Litman*

Main category: cs.CV

TL;DR: 提出仅用常规RGB图像，在茂密林冠下重建无遮挡、逼真的地表视图的方法，基于NeRF并配合低光损失与按射线积分控制以去除冠层遮挡；在搜救与林业清查（人数/树木计数）等任务上表现接近或优于依赖专用传感器的方案，具成本低、分辨率高优势。


<details>
  <summary>Details</summary>
Motivation: 森林下层（understory）和地形在茂密树冠遮挡下难以感知，但对搜救、步道绘制、林业清查等应用至关重要。现有方法依赖重且昂贵的机载LiDAR或专为人体探测的AOS（热成像合成孔径），成本高、适用性受限。希望用廉价、普适的RGB相机获得类似能力。

Method: 以NeRF为核心进行3D重建，并设计拍摄策略以保证林下区域的充分曝光；为应对弱光，引入低光损失函数（low light loss）；通过控制每条射线的体渲染积分过程，提出两种互补的冠层去遮挡策略，从而得到“去冠层”的逼真地面视图。

Result: 在两个下游任务上验证：1）搜救（SAR）场景中，仅用RGB图像即可实现对人的检测，效果与热AOS相当且有前景；2）林业清查如树木计数展示潜力。整体表明所法能以更低成本和高分辨率替代专用传感器执行SAR、步道绘制与林业清查。

Conclusion: 利用常规RGB与NeRF，通过低光优化与按射线积分控制实现冠层去遮挡和地面视图重建，在多个任务上取得有竞争力表现，具备成本优势与广泛应用潜力，可作为LiDAR/AOS的高分辨率、经济替代方案。

Abstract: Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.
  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.
  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.

</details>


### [60] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 该论文聚焦“情境化异常检测”，提出用主体-情境兼容性来定义视觉异常，并提供新数据集与方法，实证优于现有方法且对传统结构异常检测形成互补。


<details>
  <summary>Details</summary>
Motivation: 现实中同一对象/行为在不同情境下可正常或异常（如跑道上跑步正常，高速公路上跑步异常），传统假设“异常是样本固有属性、与上下文无关”失效，需要能刻画主体与情境关系的异常检测范式与基准。

Method: 1) 构建CAAD-3K基准：控制主体身份、系统性变化情境以隔离“情境型异常”。2) 提出条件兼容性学习框架：利用视觉-语言表示建模“主体—情境”关系，在有限监督下学习兼容性评分；通过条件化机制将上下文信息注入判别过程，以判断给定主体在特定情境下是否异常。

Result: 在CAAD-3K上显著优于现有方法；在MVTec-AD与VisA上达到SOTA，显示情境依赖建模对传统结构异常检测具有增益与互补效果。

Conclusion: 将异常由“固有外观异常”扩展到“主体-情境不兼容”更贴近真实应用。所提数据集与兼容性学习框架有效、可泛化，建议将上下文建模纳入异常检测标准流程；代码与数据集将开源。

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [61] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出DINO-SAE：结合球面潜空间与余弦对齐的生成自编码器，显著提高重建细节与语义一致性，并用黎曼流匹配训练DiT在球面潜在上高效生成，ImageNet-1K上达SOTA rFID/PSNR与快速收敛的gFID。


<details>
  <summary>Details</summary>
Motivation: 现有基于VFM（如DINO）的生成自编码器重建易丢失高频细节；原因之一是强制特征幅度匹配抑制了细粒度信息，且未充分利用SSL表示位于超球面的几何结构。需要兼顾语义一致性与像素级细节，并在合适的流形上进行生成建模。

Method: 1) 设计分层卷积Patch Embedding，增强局部结构与纹理保真；2) 提出余弦相似度对齐目标，仅约束特征方向（语义），放宽幅度以保留细节；3) 观察SSL表示位于超球面，采用黎曼流匹配在球面潜在流形上训练Diffusion Transformer（DiT），直接建模球面分布并提升收敛效率。

Result: 在ImageNet-1K上：重建质量达0.37 rFID、26.2 dB PSNR，语义与预训练VFM保持强一致；Riemannian Flow Matching 的DiT在80个epoch内达到3.47 gFID，显示较快收敛与强生成性能。

Conclusion: 通过方向对齐+幅度自由与球面流形上的生成建模，DINO-SAE在不牺牲语义的前提下显著提升重建细节与质量，并以Riemannian Flow Matching促成DiT高效训练，达到SOTA水平。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [62] [Multi-Cue Anomaly Detection and Localization under Data Contamination](https://arxiv.org/abs/2601.22913)
*Anindya Sundar Das,Monowar Bhuyan*

Main category: cs.CV

TL;DR: 提出一个在工业视觉场景中鲁棒的异常检测框架，融合少量已标注异常与自适应偏差学习，构建由偏差、熵不确定性与分割异常三部分组成的复合异常分数，实现准确检测、可解释的梯度化定位，并在含污染数据下仍具鲁棒性，MVTec与VisA上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实工业数据常被异常污染，且通常无法获得纯净正常训练集；同时缺乏已标注异常的利用，导致现有方法难以学习真实异常的判别特征，检测与定位性能下降。

Method: 在“少量异常”范式下，将有限标注异常纳入自适应偏差学习：1) 设计复合异常分数，包括：偏差分数（统计异常）、基于熵的不确定性分数（预测不一致）、分割分数（空间异常）；2) 利用梯度可导的统一打分机制，支持可解释的定位；3) 通过自适应样本加权减轻被污染样本的影响。

Result: 在MVTec与VisA基准上，相比多种SOTA基线取得更优的检测与定位效果；在不同污染水平下保持鲁棒，并具备良好的可解释性。

Conclusion: 有限异常监督与自适应偏差学习相结合、并以复合打分统一检测与定位，可在受污染数据条件下实现鲁棒而可解释的工业视觉异常检测，优于现有方法。

Abstract: Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.

</details>


### [63] [Deep in the Jungle: Towards Automating Chimpanzee Population Estimation](https://arxiv.org/abs/2601.22917)
*Tom Raynes,Otto Brookes,Timm Haucke,Lukas Bösch,Anne-Sophie Crunchant,Hjalmar Kühl,Sara Beery,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 用单目深度估计替代手工测距，在黑猩猩相机陷阱数据上可实现近似的密度/丰度估计（与人工方法相差约22%），但存在系统性偏差与检测失败的限制。


<details>
  <summary>Details</summary>
Motivation: 传统灵长类（大型类人猿）相机陷阱密度/丰度估计依赖动物到相机距离，人工逐帧判读耗时且难扩展，需要探索自动化、可扩展的计算机视觉方案以降低成本并加快生态监测与保育决策。

Method: 在220段野外黑猩猩相机视频上，将两种单目深度估计模型（DPT与Depth Anything）与多种距离抽样策略组合，输出个体检测距离；经（对DPT）标定后，将距离用于下游密度与丰度推断，并与人工“真值”距离比较误差与偏差。还分析跨距离范围的检测失败对估计的影响。

Result: 标定后的DPT在距离估计精度以及由此推导的密度/丰度上均优于Depth Anything；两者在复杂森林环境中普遍高估距离，导致密度与丰度被低估；距离段上的动物检测缺失是限制精度的主要因素；整体上，MDE驱动流程与传统方法的估计差距约为22%。

Conclusion: MDE集成到相机陷阱工作流是可行且实用的人工替代方案，但需校准与偏差校正，并提升跨距离的检测召回；未来应改进模型在林下光照/遮挡条件的鲁棒性、联合目标检测与深度估计、以及基于现场标定与误差传播建模以减少系统误差。

Abstract: The estimation of abundance and density in unmarked populations of great apes relies on statistical frameworks that require animal-to-camera distance measurements. In practice, acquiring these distances depends on labour-intensive manual interpretation of animal observations across large camera trap video corpora. This study introduces and evaluates an only sparsely explored alternative: the integration of computer vision-based monocular depth estimation (MDE) pipelines directly into ecological camera trap workflows for great ape conservation. Using a real-world dataset of 220 camera trap videos documenting a wild chimpanzee population, we combine two MDE models, Dense Prediction Transformers and Depth Anything, with multiple distance sampling strategies. These components are used to generate detection distance estimates, from which population density and abundance are inferred. Comparative analysis against manually derived ground-truth distances shows that calibrated DPT consistently outperforms Depth Anything. This advantage is observed in both distance estimation accuracy and downstream density and abundance inference. Nevertheless, both models exhibit systematic biases. We show that, given complex forest environments, they tend to overestimate detection distances and consequently underestimate density and abundance relative to conventional manual approaches. We further find that failures in animal detection across distance ranges are a primary factor limiting estimation accuracy. Overall, this work provides a case study that shows MDE-driven camera trap distance sampling is a viable and practical alternative to manual distance estimation. The proposed approach yields population estimates within 22% of those obtained using traditional methods.

</details>


### [64] [Q-Hawkeye: Reliable Visual Policy Optimization for Image Quality Assessment](https://arxiv.org/abs/2601.22920)
*Wulin Xie,Rui Dai,Ruidong Ding,Kaikui Liu,Xiangxiang Chu,Xinwen Hou,Jie Wen*

Main category: cs.CV

TL;DR: 提出Q-Hawkeye：在RL框架下通过不确定性感知动态优化与感知感知优化，提升IQA预测稳定性与视觉依据可靠性，显著超越现有方法并具更强跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的IQA方法存在两大可靠性问题：1) 训练样本间预测稳定性差异大，但GRPO等方法采用统一优势加权，导致不稳定样本的噪声被放大，影响策略更新；2) 过度依赖文本推理，忽视对图像内容本身的感知与对齐，易产生与视觉证据脱节的评分与描述。

Method: 提出Q-Hawkeye框架，包含两大核心：1) 不确定性感知动态优化（Uncertainty-Aware Dynamic Optimization）：通过多次rollout对同一样本的预测分数方差估计预测不确定性，用其对样本的更新强度进行重加权，抑制不稳定样本对梯度的干扰，稳定策略学习。2) 感知感知优化（Perception-Aware Optimization）：构造劣化图-原图成对输入，引入隐式感知损失（Implicit Perception Loss），约束模型必须依据真实视觉证据做质量判断，从而提升视觉感知可靠性。

Result: 在多数据集上进行广泛实验，Q-Hawkeye在性能上优于SOTA方法，并在跨数据集的泛化方面表现更佳，验证了不确定性重加权与感知约束的有效性。

Conclusion: 通过对学习信号的重新设计（不确定性重加权与视觉感知约束），Q-Hawkeye有效提升了RL式IQA的稳定性与可解释的视觉依据，取得SOTA并具更强泛化，代码与模型将开源。

Abstract: Image Quality Assessment (IQA) predicts perceptual quality scores consistent with human judgments. Recent RL-based IQA methods built on MLLMs focus on generating visual quality descriptions and scores, ignoring two key reliability limitations: (i) although the model's prediction stability varies significantly across training samples, existing GRPO-based methods apply uniform advantage weighting, thereby amplifying noisy signals from unstable samples in gradient updates; (ii) most works emphasize text-grounded reasoning over images while overlooking the model's visual perception ability of image content. In this paper, we propose Q-Hawkeye, an RL-based reliable visual policy optimization framework that redesigns the learning signal through unified Uncertainty-Aware Dynamic Optimization and Perception-Aware Optimization. Q-Hawkeye estimates predictive uncertainty using the variance of predicted scores across multiple rollouts and leverages this uncertainty to reweight each sample's update strength, stabilizing policy optimization. To strengthen perceptual reliability, we construct paired inputs of degraded images and their original images and introduce an Implicit Perception Loss that constrains the model to ground its quality judgments in genuine visual evidence. Extensive experiments demonstrate that Q-Hawkeye outperforms state-of-the-art methods and generalizes better across multiple datasets. The code and models will be made available.

</details>


### [65] [Semantic Leakage from Image Embeddings](https://arxiv.org/abs/2601.22929)
*Yiyi Chen,Qiongkai Xu,Desmond Eliott,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CV

TL;DR: 论文提出SLImE框架，证明即便图像被压缩为嵌入、且无法重建原图，仍可从保留的语义邻域结构中泄露出丰富语义，跨多种开放/闭源嵌入模型稳定恢复标签、符号表示与自然语言描述，揭示图像嵌入的隐私脆弱性。


<details>
  <summary>Details</summary>
Motivation: 业界普遍认为图像嵌入隐私风险较低，因为难以从嵌入重建原图。作者质疑这一假设：即便不能像素级重建，只要嵌入对齐后保留局部语义邻域，就可能通过一系列有损映射传播语义信息，导致“语义泄露”。

Method: 形式化“语义泄露”为从压缩图像嵌入中恢复语义结构的能力；提出SLImE：不训练特定解码器，利用本地训练的语义检索器（对齐/邻域保持）+现成模型，分步从对齐嵌入检索到标签、符号结构，再到语法连贯的描述；强调保留邻域结构即可沿有损映射链路传播语义。

Result: 在GEMINI、COHERE、NOMIC、CLIP等多种开闭源嵌入上，SLImE在多种推理任务中稳定恢复语义：从对齐的嵌入到检索标签、符号表示与完整文本描述均取得一致效果；实证验证每个环节，显示仅凭嵌入及邻域保持即可泄露大量语义。

Conclusion: 图像嵌入在对齐后保留的语义邻域使语义信息可被推断，构成基础性隐私漏洞。传统“无法重建=安全”的假设不成立，需重新审视嵌入对齐、发布与使用中的隐私保护策略。

Abstract: Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1

</details>


### [66] [Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.22959)
*Anmin Wang,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: 提出Triage：一个训练免调、即插即用的视频-语言推理框架，通过分层视觉预算压缩冗余，缩短token序列、提速并降内存，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 视频包含大量时空冗余，VLM在逐帧/逐patch编码时产生极长token序列，带来推理慢、显存高、成本大。需要一种不改模型、可泛化的方法，在不牺牲性能的情况下高效筛选关键信息。

Method: 将视频推理视作资源分配问题，进行分层视觉预算：1) 帧级预算：根据视觉动态与相关性评估帧的重要性，选取关键帧并形成先验；2) token级预算：在先验指导下先分配“核心token”（高相关），再用批量化的MMR选“上下文token”（多样性强），以保证相关性与多样性的平衡；全流程训练免调、可插入现有VLM。

Result: 在多种视频推理基准上，较基线与其他方法实现更快的推理速度和更低的显存占用，同时性能持平或更优。

Conclusion: 分层预算将视频信息选择转化为可控的资源分配，训练免调且通用，能有效缓解VLM在视频场景的计算瓶颈，实现效率与性能兼得。

Abstract: Vision-Language Models (VLMs) face significant computational challenges in video processing due to massive data redundancy, which creates prohibitively long token sequences. To address this, we introduce Triage, a training-free, plug-and-play framework that reframes video reasoning as a resource allocation problem via hierarchical visual budgeting. Its first stage, Frame-Level Budgeting, identifies keyframes by evaluating their visual dynamics and relevance, generating a strategic prior based on their importance scores. Guided by this prior, the second stage, Token-Level Budgeting, allocates tokens in two phases: it first secures high-relevance Core Tokens, followed by diverse Context Tokens selected with an efficient batched Maximal Marginal Relevance (MMR) algorithm. Extensive experiments demonstrate that Triage improves inference speed and reduces memory footprint, while maintaining or surpassing the performance of baselines and other methods on various video reasoning benchmarks.

</details>


### [67] [Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion](https://arxiv.org/abs/2601.22961)
*Dennis Sprute,Hanna Senke,Holger Flatt*

Main category: cs.CV

TL;DR: 利用GenAI（Stable Diffusion、CycleGAN）扩充缺陷样本稀少的热成像工业数据集，用于联合收割机部件的分割；Stable Diffusion带来最佳提升，Mean IoU 提高4.6%，达84.6%。


<details>
  <summary>Details</summary>
Motivation: 工业视觉质检中监督学习受限于“多正常、少缺陷”的极度类别不平衡，传统做法（重加权损失、常规数据增强）要么需要繁琐调参，要么仅改变低层次图像属性，难以弥补数据代表性不足。作者因此探索用生成式AI来合成更具多样性的样本，缓解数据稀缺与不平衡。

Method: 比较两类生成模型用于数据集扩充：1) Stable Diffusion（文生/图生图扩增，合成多样热成像部件样本）；2) CycleGAN（风格迁移/域转换）。在联合收割机部件热成像数据上进行语义分割训练与评估，指标为Mean IoU。

Result: 使用Stable Diffusion扩充训练集后，分割性能提升4.6%，最终Mean IoU达到84.6%；CycleGAN亦有尝试，但效果不及Stable Diffusion。

Conclusion: 在热成像工业分割任务中，基于扩展性和生成质量，Stable Diffusion优于CycleGAN，可有效缓解数据不平衡并改进监督模型性能。建议在工业质检中将GenAI纳入数据工程管线。

Abstract: Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.

</details>


### [68] [About an Automating Annotation Method for Robot Markers](https://arxiv.org/abs/2601.22982)
*Wataru Uemura,Takeru Nagashima*

Main category: cs.CV

TL;DR: 他们用ArUco标记的自带位姿与ID作为“自动标签器”，自动生成用于训练YOLO的标注数据，从而在模糊、虚焦、光照变化等难况下，比传统OpenCV阈值/几何检测更稳健，且减少人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 工厂自动化需要AMR可靠识别与定位；传统OpenCV对黑白标记的检测在噪声、运动模糊、失焦与光照变化下易失效；深度学习更鲁棒但依赖大量人工标注，数据制备成为瓶颈。ArUco标记天生包含ID与位姿，可用于自动生成训练标签，缓解标注难题。

Method: 利用ArUco模块对图像中标记进行检测，读取其ID与位姿/边界框信息，自动生成YOLO格式标注；据此构建数据集并训练YOLO模型；在多种退化条件（模糊、失焦、光照变化等）下对比深度学习模型与传统OpenCV识别表现。

Result: YOLO模型在自动标注数据上训练后，对受模糊或失焦影响的图像识别率优于传统图像处理方法；自动标注降低人工成本并提高标注一致性。

Conclusion: 基于ArUco的自动标注可以高效构建训练集，显著提升在恶劣成像条件下的标记识别性能。未来将研究置信度阈值与识别表现的关系，以进一步优化推理策略。

Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.

</details>


### [69] [Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI](https://arxiv.org/abs/2601.22990)
*Yinsong Wang,Thomas Fletcher,Xinzhe Luo,Aine Travers Dineen,Rhodri Cusack,Chen Qin*

Main category: cs.CV

TL;DR: 提出GaussianSVR：用3D高斯表示与自监督切片前向成像模拟，进行胎儿MRI的切片到体积重建，无需真值，多分辨率联合优化参数与配准，实验优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统SVR耗时且需多正交栈；学习法推理快但训练依赖不可得的体积真值。需要一种既高保真、训练不需真值、还能高效稳健应对胎儿运动的重建方法。

Method: 以3D Gaussian表示目标体（点/椭球带密度与各向异性协方差、颜色/强度等属性），构建模拟MR切片获取的前向模型（包含采样、PSF/模糊与噪声），用自监督重投影损失驱动学习；引入多分辨率训练，自粗到细联合优化高斯参数与每个切片/栈的空间变换，提高收敛与精度。

Result: 在胎儿MRI数据上，相比基线（传统SVR与学习法），在重建质量指标上取得更好表现，同时训练/推理效率提升（摘要未给具体数值）。

Conclusion: GaussianSVR无需体积真值即可实现高保真、效率更高的胎儿MR体积重建；多分辨率与自监督前向模拟是关键。代码将在录用后开源。

Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.

</details>


### [70] [Leveraging Multi-Rater Annotations to Calibrate Object Detectors in Microscopy Imaging](https://arxiv.org/abs/2601.23007)
*Francesco Campi,Lucrezia Tondo,Ekin Karabati,Johannes Betge,Marie Piraud*

Main category: cs.CV

TL;DR: 提出一种利用多标注者的分体训练与集成的方法，改善显微成像目标检测器的置信度校准，同时保持检测精度。


<details>
  <summary>Details</summary>
Motivation: 显微成像中的深度学习检测器虽准确，但置信度常未校准，影响生物医学场景的可靠性；多标注者存在分歧，如何利用这种分歧提升校准尚待解决。

Method: 针对每位专家的标注分别训练独立检测模型，然后在推理时聚合（集成）其预测以模拟共识；与将不同标注混合或采样的训练方式对比，强调以评分者为粒度显式建模分歧。

Result: 在一个由两位专家标注的结直肠类器官数据集上，评分者特定的集成方法显著提升了校准性能，同时保持与基线相当的检测准确度。

Conclusion: 显式建模并利用评分者分歧（通过评分者特定模型的集成）可产出更可信的生物医学目标检测器，兼顾准确性与校准性。

Abstract: Deep learning-based object detectors have achieved impressive performance in microscopy imaging, yet their confidence estimates often lack calibration, limiting their reliability for biomedical applications. In this work, we introduce a new approach to improve model calibration by leveraging multi-rater annotations. We propose to train separate models on the annotations from single experts and aggregate their predictions to emulate consensus. This improves upon label sampling strategies, where models are trained on mixed annotations, and offers a more principled way to capture inter-rater variability. Experiments on a colorectal organoid dataset annotated by two experts demonstrate that our rater-specific ensemble strategy improves calibration performance while maintaining comparable detection accuracy. These findings suggest that explicitly modelling rater disagreement can lead to more trustworthy object detectors in biomedical imaging.

</details>


### [71] [One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs](https://arxiv.org/abs/2601.23041)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: 提出OSGA：一次优化得到输入无关的“引导向量”，在VLM推理时注入即可显著降低幻觉并提升安全性，开销极低。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态任务上强，但仍存在幻觉与安全失误；现有steering要么依赖输入、要么输入无关，但都在效率与效果间折衷，缺乏可扩展、低开销且稳定有效的方案。

Method: 观察到当任务语义意图对齐时，引导向量可跨输入泛化。基于此，提出OSGA：1) 用基于方差的样本选择策略挑出一个信息量高的样本；2) 在该样本上通过对比式目标学习单个steering vector，并加入“生成式锚点”正则以稳定优化；3) 在推理时将该向量注入到特定层，无需改动模型参数。

Result: 在多项基准上，一个经OSGA优化的单一向量即可稳定提升VLM的幻觉缓解与安全表现，且推理开销可忽略。

Conclusion: 一次性、输入无关的steering在VLM中可作为实用且可扩展的可靠性增强手段；OSGA验证了通过语义对齐的跨输入泛化能带来低成本又有效的安全与稳健性改进。

Abstract: Vision Language Models (VLMs) achieve strong performance on multimodal tasks but still suffer from hallucination and safety-related failures that persist even at scale. Steering offers a lightweight technique to improve model performance. However, steering, whether input-dependent or input-independent, achieves a meaningful trade-off between efficiency and effectiveness. In this work, we observe that steering vectors can generalize across inputs when tasks share aligned semantic intent. Based on this insight, we propose \textbf{OSGA} (\textbf{O}ne-shot \textbf{S}teering with \textbf{G}enerative \textbf{A}nchor), an input-independent framework that improves model performance with a single optimization instance. OSGA first selects an informative sample via a variance-based data selection strategy and learns a single steering vector with a contrastive objective with generative anchor regularization. The resulting vector can be universally applied at a certain layer during inference time without modifying model parameters. Experiments across multiple benchmarks show that a single OSGA-optimized steering vector consistently improves hallucination mitigation and safety enhancement with negligible overhead, highlighting one-shot steering as a practical and scalable solution for reliable VLMs.

</details>


### [72] [HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation](https://arxiv.org/abs/2601.23064)
*Hari Krishna Gadi,Daniel Matos,Hongyi Luo,Lu Liu,Yongliang Wang,Yanfeng Zhang,Liqiu Meng*

Main category: cs.CV

TL;DR: 提出一种基于层级地理实体与双曲嵌入的视觉定位方法，以更少实体向量实现SOTA精度与可解释预测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉地理定位面临全球尺度、视觉歧义与地理层级结构挑战。主流方案要么依赖大规模检索（需存储海量图像向量）、网格分类（割裂地理连续性）、或生成式空间扩散（细粒度不足）。

Method: 用“实体中心”的层级建模替换图像到图像的检索：将国家-区域-子区域-城市等地理实体组织为紧凑层级，并在双曲空间中嵌入。提出Geo-Weighted Hyperbolic对比学习，把哈弗辛距离直接纳入对比目标，使图像表征与地理实体对齐；推理时直接输出实体层级预测。

Result: 在OSV5M基准上用约24万实体嵌入取代500万+图像嵌入，达到新的SOTA：平均测地误差降低19.5%，细粒度子区域准确率提升43%。

Conclusion: 几何感知的层级双曲嵌入为全球图像地理定位提供了可扩展、可解释且精度更高的替代方案。

Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.

</details>


### [73] [Rethinking Transferable Adversarial Attacks on Point Clouds from a Compact Subspace Perspective](https://arxiv.org/abs/2601.23102)
*Keke Tang,Xianheng Liu,Weilong Peng,Xiaofei Wang,Daizong Liu,Peican Zhu,Can Lu,Zhihong Tian*

Main category: cs.CV

TL;DR: 提出CoSA：在共享低维语义子空间中进行可迁移点云对抗攻击，抑制模型依赖噪声，显著提升跨模型迁移性并保持不可察觉性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有点云对抗样本多依赖特定模型梯度或启发式规则，难以泛化到未知架构。作者希望在不依赖代理模型细节的情况下，提高攻击在不同网络间的迁移能力。

Method: 将点云表示为类别原型的紧凑线性组合，并在低秩子空间内优化对抗扰动。该子空间由共享的语义结构与类特定原型刻画，使扰动沿语义一致、与架构无关的方向变化，减少模型特异性噪声。

Result: 在多个数据集与多种网络上，CoSA优于现有最先进的可迁移攻击；在视觉不可察觉性和面对常见防御的鲁棒性方面维持竞争力。

Conclusion: 通过在共享低维语义子空间内进行扰动优化，CoSA提升了点云对抗攻击的跨模型迁移性且不依赖代理模型伪影，展示了更强的通用性与实用潜力。

Abstract: Transferable adversarial attacks on point clouds remain challenging, as existing methods often rely on model-specific gradients or heuristics that limit generalization to unseen architectures. In this paper, we rethink adversarial transferability from a compact subspace perspective and propose CoSA, a transferable attack framework that operates within a shared low-dimensional semantic space. Specifically, each point cloud is represented as a compact combination of class-specific prototypes that capture shared semantic structure, while adversarial perturbations are optimized within a low-rank subspace to induce coherent and architecture-agnostic variations. This design suppresses model-dependent noise and constrains perturbations to semantically meaningful directions, thereby improving cross-model transferability without relying on surrogate-specific artifacts. Extensive experiments on multiple datasets and network architectures demonstrate that CoSA consistently outperforms state-of-the-art transferable attacks, while maintaining competitive imperceptibility and robustness under common defense strategies. Codes will be made public upon paper acceptance.

</details>


### [74] [FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows](https://arxiv.org/abs/2601.23107)
*Ilir Tahiraj,Peter Wittal,Markus Lienkamp*

Main category: cs.CV

TL;DR: FlowCalib利用静态物体的场景流中由旋转失准引入的系统性偏差，检测LiDAR与车体坐标系的角度失准，无需额外传感器。在nuScenes上验证，能稳健判别是否失准及具体轴向。


<details>
  <summary>Details</summary>
Motivation: 现有方法多做传感器间外参校正，忽视单个传感器相对车辆坐标系的失准源头；LiDAR角度微小误差会导致自动驾驶安全风险，且缺乏无需额外硬件、能在运行中检测失准的手段。

Method: 提出FlowCalib：从连续点云估计场景流，利用神经场景流先验获取全局流特征；构造手工几何描述子，和学习到的全局特征在双分支检测网络中融合。执行两类二分类：1) 全局是否失准；2) 按轴分别判断每个旋转轴是否失准。核心假设：静态物体的场景流在旋转失准下产生可辨识的系统性偏差。

Result: 在nuScenes数据集上进行实验，能鲁棒检测LiDAR-车体的失准，建立了传感器到车辆失准检测的基准表现。

Conclusion: 场景流中的系统偏差可作为诊断LiDAR角度失准的有效线索；结合神经场景流先验与几何特征的双分支架构，实现无需额外传感器的在线式失准检测，并具备全局与轴向细粒度判断能力。

Abstract: Accurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor-to-sensor errors without considering the miscalibration of individual sensors that cause these errors in the first place. We introduce FlowCalib, the first framework that detects LiDAR-to-vehicle miscalibration using motion cues from the scene flow of static objects. Our approach leverages the systematic bias induced by rotational misalignment in the flow field generated from sequential 3D point clouds, eliminating the need for additional sensors. The architecture integrates a neural scene flow prior for flow estimation and incorporates a dual-branch detection network that fuses learned global flow features with handcrafted geometric descriptors. These combined representations allow the system to perform two complementary binary classification tasks: a global binary decision indicating whether misalignment is present and separate, axis-specific binary decisions indicating whether each rotational axis is misaligned. Experiments on the nuScenes dataset demonstrate FlowCalib's ability to robustly detect miscalibration, establishing a benchmark for sensor-to-vehicle miscalibration detection.

</details>


### [75] [Segment Any Events with Language](https://arxiv.org/abs/2601.23159)
*Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: SEAL提出首个针对事件相机的开放词汇实例分割框架，支持基于视觉提示的实例级与部件级分割与分类，构建四个从粗到细的基准并显著超越基线；附录给出无需提示的时空通用变体。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇场景理解多聚焦于图像/点云/LiDAR，对事件相机的数据研究稀缺且多停留在语义级别，缺乏能进行实例级与部件级细粒度分割与开放词汇分类的统一方法与系统评测。

Method: 提出SEAL（Semantic-aware Segment Any Events）：在给定视觉提示下，统一执行事件数据的分割与开放词汇掩码分类；支持多粒度（实例级、部件级）理解；采用参数高效的体系结构；并构建四个覆盖不同标签与语义粒度的基准用于评测；附录提供无需用户提示的通用时空OV-EIS变体。

Result: 在四个新基准上，SEAL在性能与推理速度上均大幅优于构建的基线，同时保持较少参数；能够在实例与部件粒度上实现开放词汇分割与分类。

Conclusion: SEAL为事件相机领域带来首个语义感知的Segment-Any式开放词汇实例分割方案，验证了其实用性与效率，并通过基准推进社区评测；其无提示变体展示了进一步的时空泛化潜力。

Abstract: Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL

</details>


### [76] [Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm](https://arxiv.org/abs/2601.23167)
*Xiangrui Liu,Haoxiang Li,Yezhou Yang*

Main category: cs.CV

TL;DR: Hi-Light提出一个无需训练的高保真高分辨率视频重光照框架，并提出新的光照稳定性评价指标，显著降低闪烁、保持细节与时间一致性，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频重光照在创意与商业上价值巨大，但面临三大痛点：缺乏专用客观评价指标；重光照引入严重的时域闪烁；编辑过程损失高频细节和造成模糊。作者希望在不额外训练的前提下实现稳健、高质量的视频重光照并建立可量化的稳定性衡量。

Method: 提出Hi-Light框架，包含三项关键技术：1）基于“亮度先验锚定”的引导式重光照扩散过程，稳定中间重光照结果；2）混合运动自适应光照平滑滤波器（利用光流）在不引入运动模糊的前提下实现时间一致性；3）基于LAB颜色空间的细节融合模块，将原视频的高频细节与重光照结果融合。同时提出用于量化光照一致性的Light Stability Score指标。

Result: 在大量实验中，Hi-Light在定性与定量上均显著优于SOTA：生成的重光照视频更加稳定、细节更丰富，光照一致性评分更高，闪烁明显减弱。

Conclusion: Hi-Light无需训练即可实现高保真、时域稳定、细节保真的视频重光照，并提供首个专门衡量光照一致性的定量指标，为后续研究与应用提供统一评测与强大基线。

Abstract: Video relighting offers immense creative potential and commercial value but is hindered by challenges, including the absence of an adequate evaluation metric, severe light flickering, and the degradation of fine-grained details during editing. To overcome these challenges, we introduce Hi-Light, a novel, training-free framework for high-fidelity, high-resolution, robust video relighting. Our approach introduces three technical innovations: lightness prior anchored guided relighting diffusion that stabilises intermediate relit video, a Hybrid Motion-Adaptive Lighting Smoothing Filter that leverages optical flow to ensure temporal stability without introducing motion blur, and a LAB-based Detail Fusion module that preserves high-frequency detail information from the original video. Furthermore, to address the critical gap in evaluation, we propose the Light Stability Score, the first quantitative metric designed to specifically measure lighting consistency. Extensive experiments demonstrate that Hi-Light significantly outperforms state-of-the-art methods in both qualitative and quantitative comparisons, producing stable, highly detailed relit videos.

</details>


### [77] [Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training](https://arxiv.org/abs/2601.23220)
*Anglin Liu,Ruichao Chen,Yi Lu,Hongxia Xu,Jintai Chen*

Main category: cs.CV

TL;DR: 提出Med-Scout，用基于无标注医学图像的几何一致性代理任务驱动的强化学习，缓解MLLM在医学场景中的“几何失明”，在新基准Med-Scout-Bench上大幅超越现有模型，并泛化提升医学VQA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在医学诊断中虽语言能力强，但缺乏对几何约束的可靠感知，导致“看图不懂形”的事实性谬误。昂贵的专家标注限制了直接监督的可扩展性，需要一种能从无标注数据中提取可验证几何信号的方法。

Method: 构建无需人工标注的三类代理任务以产生可验证的奖励信号，并用RL优化MLLM：1) 分层尺度定位（Hierarchical Scale Localization）以学习绝对/相对尺度与位置；2) 拓扑拼图复原（Topological Jigsaw Reconstruction）以学习部件拓扑与空间关系；3) 异常一致性检测（Anomaly Consistency Detection）以保证异常标注与影像区域、形状的一致性；同时提出Med-Scout-Bench评测几何感知能力。

Result: 在Med-Scout-Bench上较领先的专有与开源MLLM提升40%以上；在放射学与综合医学VQA任务上也获得领先表现，说明几何感知增强可迁移至更广泛医学理解。

Conclusion: 通过无标注几何代理任务+RL，Med-Scout有效缓解MLLM几何失明，并以显著幅度提升几何与下游医学任务表现；表明几何一致性是医学多模态推理的关键并具备可扩展训练路径。

Abstract: Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that "cures" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.

</details>


### [78] [Region-Normalized DPO for Medical Image Segmentation under Noisy Judges](https://arxiv.org/abs/2601.23222)
*Hamza Kalisch,Constantin Seibold,Jens Kleesiek,Ken Herrmann,Frederic Jonske*

Main category: cs.CV

TL;DR: 论文探讨在没有额外像素级标注的前提下，用现成但噪声较大的质量控制(QC)信号来偏好优化分割模型，并提出一种区域归一化的DPO方法以提升稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 医学分割的像素级标注昂贵且难以扩展，而实务系统已产生低成本的“伪监督”信号（模型一致性、不确定度、掩膜质量分数等）。直接用这些噪声偏好信号做微调容易被误导，需要一种能在弱/偏置评审者下仍稳健学习的方式。

Method: 以小标注集训练的基线分割器生成候选掩膜；使用来自自动QC的“评审者”对候选两两打分形成偏好对；研究标准DPO在不同偏好挖掘策略下的行为，并提出RN-DPO：按两个掩膜的“分歧区域”大小对更新进行归一化，降低大面积错误比较对于优化的杠杆效应，提升鲁棒性与稳定性。

Result: 在两个医学数据集与多种训练设定下，RN-DPO相对标准DPO与强基线提升峰值与持续性能，且在评审者较弱时显著提高训练稳定性，避免有害偏好被放大；当评审者可靠时，选择其最高分候选可带来更高峰值，但在弱评审者下会放大错误。

Conclusion: RN-DPO在无需新增像素标注的前提下，使基于偏好的分割微调更稳健有效，优于标准DPO与强基线；关键在于用分歧区域归一化限制有害比较的影响，并谨慎设计偏好对挖掘策略以匹配评审者质量。

Abstract: While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations.

</details>


### [79] [Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning](https://arxiv.org/abs/2601.23224)
*Xiangyu Zeng,Zhiqiu Zhang,Yuhan Zhu,Xinhao Li,Zikang Wang,Changlian Ma,Qingyu Zhang,Zizheng Huang,Kun Ouyang,Tianxiang Jiang,Ziang Yan,Yi Wang,Hongjie Zhang,Yali Wang,Limin Wang*

Main category: cs.CV

TL;DR: Video-o3 是一个面向长视频理解的多模态大模型框架，通过迭代“寻证—细看—适时停止”来从冗余中找稀疏关键信息，显著提升多跳证据检索与推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有长视频多模态模型多用均匀采样与单轮推理，容易被大量冗余干扰，难以持续发现稀疏但关键的视觉证据，且多轮工具调用会造成注意力分散与上下文膨胀。

Method: 提出 Video-o3 框架，核心包括：1) 迭代式证据搜索与关键片段细粒度检查，并在证据充足时自适应终止；2) 任务解耦注意力掩码（Task-Decoupled Attention Masking），在交错的推理与工具调用中隔离每步注意力以保持专注，同时共享全局上下文；3) 可验证轨迹引导奖励（Verifiable Trajectory-Guided Reward），在多轮交互中以可验证中间轨迹控制上下文长度增长，平衡探索覆盖与推理效率；4) 构建大规模数据合成管线，形成 Seeker-173K（173K 高质量工具交互轨迹），用于监督与强化学习训练。

Result: 在长视频基准上显著领先：MLVU 72.1% 准确率、Video-Holmes 46.5%，优于现有 SOTA；展示出更强的多跳证据寻址与推理能力。

Conclusion: Video-o3 通过原生工具调用与注意力/奖励设计，有效缓解冗余干扰与上下文膨胀问题，实现可迭代的证据发现与高效推理，在长视频理解任务上达到新的性能水平。

Abstract: Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.

</details>


### [80] [ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search](https://arxiv.org/abs/2601.23232)
*Tao Yu,Haopeng Jin,Hao Wang,Shenghua Chai,Yujia Yang,Junhao Gong,Jiaming Guo,Minghui Zhang,Xinlong Chen,Zhenghao Zhang,Yuxuan Zhou,Yanpei Gong,YuanCheng Liu,Yiming Ding,Kangwei Zeng,Pengfei Yang,Zhongtian Luo,Yufei Xiong,Shanbin Zhang,Shaoxiong Cheng,Huang Ruilin,Li Shuo,Yuxi Niu,Xinyuan Zhang,Yueya Xu,Jie Mao,Ruixuan Ji,Yaru Zhao,Mingchen Zhang,Jiabing Yang,Jiaqi Liu,YiFan Zhang,Hongzhu Yi,Xinming Wang,Cheng Zhong,Xiao Ma,Zhang Zhang,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出ShotFinder：一个开放域视频镜头检索基准与三阶段方法（想象扩展→搜索检索→描述引导定位），覆盖五类可控单因子约束（时序、颜色、视觉风格、音频、分辨率），1210个样本，实验证明与人类差距显著且各约束不均衡，时序较易、颜色与风格更难。


<details>
  <summary>Details</summary>
Motivation: 现有LLM检索研究多聚焦文本或静态多模态，欠缺对具有时间结构与复杂语义的开放域视频镜头检索的系统性基准与分析；缺少可控、可诊断的维度来评测模型对编辑需求的满足能力。

Method: 1) 构建ShotFinder基准：将编辑需求形式化为以关键帧为中心的镜头描述，并设计五类可控单因子约束（时序、颜色、视觉风格、音频、分辨率）；从YouTube 20个主题采集1210个高质量样本，借助大模型生成并人工核验。2) 提出三阶段检索定位流水线：a) 通过“视频想象”进行查询扩展；b) 借助搜索引擎进行候选视频检索；c) 使用描述引导的时间定位找到目标镜头。3) 在多种闭源与开源模型上评测。

Result: 与人类相比存在显著性能差距；在五类约束上表现不均衡：时间定位相对可行，颜色与视觉风格成为主要瓶颈；凸显当前多模态大模型在开放域视频镜头检索上的不足。

Conclusion: ShotFinder为开放域视频镜头检索提供了首个系统化、可控的评测基准与参考方法；实验显示该任务仍具挑战性，特别是颜色与风格维度，未来应加强对动态多模态特征与跨模态对齐的建模与评测。

Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.

</details>


### [81] [Structured Over Scale: Learning Spatial Reasoning from Educational Video](https://arxiv.org/abs/2601.23251)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 提出DoraVQA：从《爱探险的朵拉》8季中自动抽取、时间戳对齐的5,344个视频问答对，用以训练/评估VLM在幼儿式推理（计数、空间、组合）上的能力。用教育视频的“情境-提问-停顿-答案”结构作为清晰监督，采用GRPO微调Qwen2/Qwen3，虽仅38小时数据，却在DoraVQA提升8-14分，在CVBench达SOTA 86.16%，并迁移到Video-MME与NExT-QA，表明结构化内容同样关键。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在常规视频理解上表现强，但在幼儿即可完成的基本推理任务上失效。作者认为教育视频的教学结构提供了更适合学习推理能力的训练信号，能够弥补仅靠大规模无结构数据的不足。

Method: 构建DoraVQA数据集：从《爱探险的朵拉》8季中自动抽取问题-答案及其精确时间戳，利用统一的“情境-提问-停顿-答案”片段作为训练样本；以该清晰的正确性信号和推理轨迹，使用Group Relative Policy Optimization（GRPO）对Qwen2/Qwen3进行对齐/微调，学习计数、空间、组合等推理。

Result: 仅用约38小时儿童教育视频训练，模型在DoraVQA上提升8-14分；在CVBench达到86.16%的SOTA；并在Video-MME与NExT-QA上有显著迁移增益，显示从狭窄教育领域向广泛多模态理解的泛化。

Conclusion: 教育内容的结构（清晰的提问-停顿-答案与因果链）能显著提升VLM的基础推理能力，重要性可与数据规模相当。将结构化教学信号融入训练，可使模型更好地习得可迁移的计数与空间推理等能力。

Abstract: Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.

</details>


### [82] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出TaTa：一种无需训练与反向传播的测试时自适应方法，利用Brownian距离协方差进行多模态依赖对齐，并结合属性增强提示、动态聚类与伪标签精炼，在多数据集上以更低计算成本达成SOTA的域外与跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在域移位下性能显著下降，现实部署受限；现有TTA方法计算开销大、依赖反向传播且多聚焦单模态，容易引入不稳定的参数更新，需要一种高效、稳健、可多模态协同的测试时自适应策略。

Method: 提出TaTa：以Brownian Distance Covariance衡量并最大化图像与文本特征间（含非线性）依赖，通过无需梯度的动态适配过程实现分布对齐；引入属性增强式提示，将描述性视觉线索注入文本提示；结合动态聚类生成伪标签，并进行伪标签精炼以提高稳健性，整体在测试时对模型进行前向推理级的自适应，而不更新权重。

Result: 在多种数据集与域泛化/跨数据集设定下，TaTa以显著更低的计算成本取得SOTA性能，较现有TTA方法在准确率与稳定性上均有提升。

Conclusion: 基于Brownian距离协方差的无训练测试时自适应可有效缓解VLM的域移问题；结合属性提示、动态聚类与伪标签精炼，能在不修改模型权重的前提下实现高效稳健的跨域泛化。

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


### [83] [User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments](https://arxiv.org/abs/2601.23281)
*Junfeng Lin,Yanming Xiu,Maria Gorlatova*

Main category: cs.CV

TL;DR: 本文评估开集目标检测（OSOD）在真实XR图像下对多样用户提示的鲁棒性。两种模型（GroundingDINO、YOLO-E）对标准/信息不足提示较稳，但在语用歧义提示下显著退化；过度详细提示主要拖累GroundingDINO。引入提示增强后，在歧义条件下mIoU提升>55%，平均置信度提升>41%。基于实证提出面向XR的提示策略与增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有OSOD工作多在标准化基准上验证，缺乏对现实交互式XR场景中“用户提示往往模糊、欠详或过详”的系统考察。需要量化不同提示类型对OSOD的影响，并探索能提升鲁棒性的提示增强策略，以指导实用部署。

Method: 使用真实世界XR图像，对两种OSOD模型（GroundingDINO、YOLO-E）进行评测。构建四类提示：标准、信息不足（underdetailed）、过度详细（overdetailed）、语用歧义（pragmatically ambiguous）。利用视觉语言模型模拟用户提示，并考察两种提示增强策略对上述提示的影响。以mIoU与平均置信度等指标衡量性能与鲁棒性。

Result: 两模型在标准与信息不足提示下性能稳定；在语用歧义提示下均明显退化。过度详细提示对GroundingDINO影响更大。采用提示增强后，在歧义场景中mIoU提升超过55%，平均置信度提升超过41%。

Conclusion: OSOD在XR交互下对提示质量敏感，特别易受语用歧义影响。针对XR应用，应采用经过设计的提示与增强方法以提升鲁棒性；同时提示过度细节需谨慎，GroundingDINO尤为敏感。研究为OSOD在XR中的实际部署提供了提示工程与增强策略的经验依据。

Abstract: Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.

</details>


### [84] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: 提出VideoGPA：用几何偏好对齐提高视频扩散模型的3D一致性，无需人工标注，显著改善时空稳定与物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽然画面好看，但缺乏3D结构一致性，出现形变与空间漂移。根因在于标准去噪目标缺少几何一致性的显式约束，需要一种能为生成模型提供几何一致性偏好信号的方法，且应数据高效、无需人工标注。

Method: 提出VideoGPA：利用几何基础模型自动产生稠密的几何偏好信号（偏好对），并通过DPO（Direct Preference Optimization）对视频扩散模型进行偏好对齐，直接调整生成分布以偏向3D一致的结果。框架为自监督、数据高效，仅需少量偏好对即可训练。

Result: 在大量实验中，用极少偏好对即可显著提升VDM的时间稳定性、物理可 plausibility 与运动连贯性，并在各项指标与视觉比较上均超过SOTA基线。

Conclusion: 通过几何偏好对齐，VDM可被有效引导产生具备内在3D一致性的影片，无需人工标注，具有良好的数据效率和泛化潜力。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>
