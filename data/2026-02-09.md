<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors](https://arxiv.org/abs/2602.06122)
*Ding-Jiun Huang,Yuanhao Wang,Shao-Ji Yuan,Albert Mosella-Montoro,Francisco Vicente Carrasco,Cheng Zhang,Fernando De la Torre*

Main category: cs.CV

TL;DR: SuperHead 提出一种面向低清素材的三维可动画说话人头像超分方案，通过动态感知的3D反演把低清输入映射到高保真的3DGS头像，并与参数化头模绑定以驱动表情与视角变化，显著提升动态细节与一致性。


<details>
  <summary>Details</summary>
Motivation: 沉浸式应用需要高保真、可动画的3D说话人头像，但现实中常见的低清图像/视频会导致三维重建质量差；现有图像/视频/3D超分方法难以处理动态三维输入并保持身份与时序一致性。

Method: 利用预训练三维生成模型的先验，提出“动态感知的3D反演”：优化生成模型潜变量，生成高分辨率的3D Gaussian Splatting(3DGS)头像；再与参数化头模（如FLAME）绑定以实现动画。反演同时以少量上采样的2D人脸渲染及其深度图（覆盖多表情与多视角）进行联合监督，确保几何、纹理与动态一致性。

Result: 在动态表情与视角变化下生成细节丰富、身份保持的高质量3D头像，视觉质量显著优于基线方法。

Conclusion: 通过引入动态感知的3D反演并结合3DGS与参数化骨架绑定，SuperHead 能在低清输入条件下实现高保真、时序与三维一致的可动画头像重建，适用于沉浸式应用等场景。

Abstract: Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.

</details>


### [2] [EgoAVU: Egocentric Audio-Visual Understanding](https://arxiv.org/abs/2602.06139)
*Ashish Seth,Xinhao Mei,Changsheng Zhao,Varun Nagaraja,Ernie Chang,Gregory P. Meyer,Gael Le Lan,Yunyang Xiong,Vikas Chandra,Yangyang Shi,Dinesh Manocha,Zhipeng Cai*

Main category: cs.CV

TL;DR: 提出EgoAVU数据引擎，自动生成自我中心（第一视角）音视叙述与QA，构建训练集与评测集，揭示现有MLLM偏视觉的问题，并通过微调显著提升跨模态理解与迁移表现。


<details>
  <summary>Details</summary>
Motivation: MLLM虽能接收图像与音频，但缺乏带有连贯“音+视”对齐监督的数据，导致模型难以真正联合理解两种模态，尤其在第一视角场景中更复杂且标注昂贵。

Method: 构建EgoAVU数据引擎：1) 以人类叙述为基，注入多模态上下文并进行跨模态相关性建模，生成音视叙述；2) 通过基于token的视频过滤与模块化、图式策展提升多样性与质量；3) 自动生成音视问题与答案。由此产出3M规模的EgoAVU-Instruct训练集与人工核验的EgoAVU-Bench评测集。

Result: EgoAVU-Bench显示现有MLLM强烈偏向视觉，忽视或无法对齐音频。用EgoAVU-Instruct微调后，在EgoAVU-Bench上最高提升113%，并在EgoTempo与EgoIllusion上实现最高28%的相对增益。

Conclusion: 系统化的数据引擎与大规模音视指令数据可显著改善MLLM对第一视角多模态（音+视）的联合理解，提升对齐能力并带来可迁移的下游收益；代码将开源。

Abstract: Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.

</details>


### [3] [MGP-KAD: Multimodal Geometric Priors and Kolmogorov-Arnold Decoder for Single-View 3D Reconstruction in Complex Scenes](https://arxiv.org/abs/2602.06158)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: 提出MGP-KAD：融合RGB与几何先验，并用KAN混合解码器，单视图3D重建在Pix3D上达SOTA，提升几何完整性、光滑度与细节。


<details>
  <summary>Details</summary>
Motivation: 单视图3D重建在真实复杂场景中受噪声、对象多样性与数据稀缺制约，现有方法难以兼顾几何一致性与细节保真，线性解码器处理多模态输入能力有限。

Method: 1) 多模态特征融合：将RGB特征与从真实物体数据采样聚类得到的类级几何先验融合；该先验在训练中动态更新/调整以强化几何理解。2) KAN混合解码器：采用基于Kolmogorov–Arnold Networks的解码结构替代传统线性解码器，更好拟合复杂多模态关系，提升重建质量。3) 在Pix3D数据集上进行实验与对比评测。

Result: 在Pix3D上达到SOTA，相比基线显著提升几何完整性、表面光滑性与细节保持能力。

Conclusion: MGP-KAD通过引入可学习的类级几何先验与KAN混合解码器，为复杂场景下的单视图3D重建提供了鲁棒且有效的方案，并在基准上验证其优越性。

Abstract: Single-view 3D reconstruction in complex real-world scenes is challenging due to noise, object diversity, and limited dataset availability. To address these challenges, we propose MGP-KAD, a novel multimodal feature fusion framework that integrates RGB and geometric prior to enhance reconstruction accuracy. The geometric prior is generated by sampling and clustering ground-truth object data, producing class-level features that dynamically adjust during training to improve geometric understanding. Additionally, we introduce a hybrid decoder based on Kolmogorov-Arnold Networks (KAN) to overcome the limitations of traditional linear decoders in processing complex multimodal inputs. Extensive experiments on the Pix3D dataset demonstrate that MGP-KAD achieves state-of-the-art (SOTA) performance, significantly improving geometric integrity, smoothness, and detail preservation. Our work provides a robust and effective solution for advancing single-view 3D reconstruction in complex scenes.

</details>


### [4] [Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving](https://arxiv.org/abs/2602.06159)
*Xuyang Chen,Conglang Zhang,Chuanheng Fu,Zihao Yang,Kaixuan Zhou,Yizhi Zhang,Jianan He,Yanfeng Zhang,Mingwei Sun,Zengmao Wang,Zhen Dong,Xiaoxiao Long,Liqiu Meng*

Main category: cs.CV

TL;DR: 提出DwD框架：用VFM/DINO特征作为统一控制信号，通过子空间投影与随机信道尾部丢弃在一致性与真实感间权衡，并引入空间对齐与因果时序聚合提升高分辨率控制与时序稳定。


<details>
  <summary>Details</summary>
Motivation: 现有Sim2Real视频生成依赖中间表征，低层信号可精确控制但带来“质感烘焙”导致不真实，高层先验更真实但缺乏结构细节，形成一致性-真实感两难，需要一种既能保留结构控制又能维持写实性的统一桥梁。

Method: 1) 以VFM（DINOv3）特征为统一桥梁；2) 主子空间投影：去除导致纹理烘焙的高频成分；3) 随机信道尾部丢弃：缓解刚性降维造成的结构损失；4) 可学习空间对齐模块：将高分辨率DINO特征对齐到扩散模型骨干以增强控制精度；5) 因果时序聚合器：用因果卷积在融合逐帧DINO特征时保留历史运动上下文，减少运动模糊并提升时序稳定。

Result: 在自动驾驶视频的Sim2Real生成中，DwD在真实感与几何/控制一致性上同时提升；高分辨率特征带来更精确的控制，引入因果聚合后显著减轻运动模糊并提高时间稳定性；总体优于依赖低/高层单一模态的现有方法。

Conclusion: VFM/DINO特征可作为跨域统一桥梁，通过频谱与维度策略、空间对齐和因果时序建模，可同时实现控制一致性与写实性，提供更稳定的自动驾驶视频Sim2Real生成方案。

Abstract: Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by "baking in" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for "texture baking," while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: https://albertchen98.github.io/DwD-project/

</details>


### [5] [MetaSSP: Enhancing Semi-supervised Implicit 3D Reconstruction through Meta-adaptive EMA and SDF-aware Pseudo-label Evaluation](https://arxiv.org/abs/2602.06163)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: 提出MetaSSP：一种面向单视图3D重建的半监督SDF方法，利用未标注图像，通过基于梯度的重要度估计来稳健更新EMA参数，并以SDF方差与增强一致性联合加权伪标签。在仅10%标注热身下，统一框架联合优化有标与无标数据，在Pix3D上显著提升（CD降约20.61%，IoU升约24.09%），达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 隐式SDF单视图重建虽能得到高质表面，但对大规模标注依赖强、难以扩展；现有半监督方法在如何有效利用未标注数据、控制伪标签噪声与稳定训练方面仍不足，需新的机制提升数据效率与鲁棒性。

Method: 1) 半监督统一管线：先用10%标注进行warm-up，再与未标注数据联合训练。2) 参数重要度估计：基于梯度大小评估各参数重要性，用于正则化自适应EMA（使关键参数更新更稳健）。3) SDF感知伪标签加权：将数据增强一致性信号与SDF方差结合，为未标注样本的伪标签分配权重，降低不确定伪标签影响。

Result: 在Pix3D基准上，相比现有半监督基线，Chamfer Distance下降约20.61%，IoU提升约24.09%，达到新的SOTA性能。

Conclusion: 通过对参数更新稳定性的控制与对伪标签不确定性的显式建模，MetaSSP有效利用了大量未标注图像，在仅少量标注条件下显著提高单视图SDF重建质量与鲁棒性，具备良好的可扩展性。

Abstract: Implicit SDF-based methods for single-view 3D reconstruction achieve high-quality surfaces but require large labeled datasets, limiting their scalability. We propose MetaSSP, a novel semi-supervised framework that exploits abundant unlabeled images. Our approach introduces gradient-based parameter importance estimation to regularize adaptive EMA updates and an SDF-aware pseudo-label weighting mechanism combining augmentation consistency with SDF variance. Beginning with a 10% supervised warm-up, the unified pipeline jointly refines labeled and unlabeled data. On the Pix3D benchmark, our method reduces Chamfer Distance by approximately 20.61% and increases IoU by around 24.09% compared to existing semi-supervised baselines, setting a new state of the art.

</details>


### [6] [M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.06166)
*Bangji Yang,Ruihan Guo,Jiajun Fan,Chaoran Cheng,Ge Liu*

Main category: cs.CV

TL;DR: 提出M3：一个免训练、用于文本到图像的多模态-多智能体-多轮迭代框架，通过规划-校验-细化-编辑-验证的闭环，在推理时逐约束修正，显著提升复杂组合约束生成效果，开源模型+M3在OneIG-EN上达SOTA（0.532），超越多款商用系统，并在GenEval上大幅提升空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成虽在保真度上强，但对包含多实体、多属性、空间关系等复杂组合约束的提示词表现不稳，常违背部分条件。训练大型模型以增强组合泛化代价高、不可得或受限。需要一种无需再训练、可泛化到任意预训练T2I模型的推理时方法，系统性地分解并逐步满足多重约束。

Method: 设计M3多智能体迭代框架：1) Planner将提示分解为可验证的检查清单（约束项）；2) Checker对当前图像逐项检测是否满足；3) Refiner/Editor针对未满足的单一约束进行有针对性的修复与编辑；4) Verifier比较前后版本，确保单调改进；多轮循环直至收敛或上限步数。框架可调用现成基础模型（识别、编辑、生成）而无需再训练，可插拔到任意T2I基座（如Qwen-Image）。

Result: 在OneIG-EN基准上，Qwen-Image+M3总分0.532，超过Imagen4（0.515）与Seedream 3.0（0.530），达SOTA；在GenEval的组合与空间推理子项显著提升，报告称空间推理性能近乎翻倍；对多约束提示的鲁棒性和一致性明显增强。

Conclusion: 智能体式、可验证的多轮推理能在不增训的前提下，显著改善开源T2I模型的组合生成能力，甚至超越闭源系统。M3作为即插即用模块，为复杂条件生成提供新范式，并具备良好通用性与成本效益。

Abstract: Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.

</details>


### [7] [Unsupervised Anomaly Detection of Diseases in the Female Pelvis for Real-Time MR Imaging](https://arxiv.org/abs/2602.06179)
*Anika Knupfer,Johanna P. Müller,Jordina A. Verdera,Martin Fenske,Claudius S. Mathy,Smiti Tripathy,Sebastian Arndt,Matthias May,Michael Uder,Matthias W. Beckmann,Stefanie Burghaus,Jana Hutter*

Main category: cs.CV

TL;DR: 提出一个与疾病和参数无关、可实时运行的女性骨盆MRI无监督异常检测基线：用只含健康T2矢状位数据训练残差VAE，推理时以重建误差热图标出异常；在子宫肌瘤公开数据上AUC 0.736、灵敏度0.828、特异度0.692，重建约92.6 FPS；并开展多病种临床评估与观察者一致性分析。


<details>
  <summary>Details</summary>
Motivation: 妇女育龄期骨盆疾病负担重，但骨盆解剖差异大导致MRI解读困难、诊断延迟。现有AI多为特定疾病、难以实时、泛化差，不利于临床整合。需要一个与疾病和序列参数无关、可实时的无监督异常检测框架与基线，用于广谱病变发现与后续临床集成。

Method: 构建残差变分自编码器（residual VAE），只用来自多协议的健康T2矢状位骨盆MRI进行训练，学习正常解剖分布；利用扩散模型生成的合成健康数据做数据增强以提升鲁棒性。推理阶段通过重建误差热图定位与正常结构偏离的区域，实现无需标注异常的病变检测。

Result: 在Uterine Myoma MRI Dataset上实现平均AUC 0.736，灵敏度0.828、特异度0.692；重建速度约92.6帧/秒，满足实时兼容。额外的临床观察者研究扩展到子宫内膜癌、子宫内膜异位症和子宫腺肌病，揭示解剖异质性与观察者差异对性能解读的影响。

Conclusion: 该框架为女性骨盆MRI无监督异常检测提供了首个（或关键）实时兼容的基线，能在无标注异常的情况下指示潜在病灶并具备较高速度；尽管受解剖变异与观察者一致性影响，结果支持其向实时MRI集成与更广泛多病种应用推进，代码与前瞻数据集对学术合作开放。

Abstract: Pelvic diseases in women of reproductive age represent a major global health burden, with diagnosis frequently delayed due to high anatomical variability, complicating MRI interpretation. Existing AI approaches are largely disease-specific and lack real-time compatibility, limiting generalizability and clinical integration. To address these challenges, we establish a benchmark framework for disease- and parameter-agnostic, real-time-compatible unsupervised anomaly detection in pelvic MRI. The method uses a residual variational autoencoder trained exclusively on healthy sagittal T2-weighted scans acquired across diverse imaging protocols to model normal pelvic anatomy. During inference, reconstruction error heatmaps indicate deviations from learned healthy structure, enabling detection of pathological regions without labeled abnormal data. The model is trained on 294 healthy scans and augmented with diffusion-generated synthetic data to improve robustness. Quantitative evaluation on the publicly available Uterine Myoma MRI Dataset yields an average area-under-the-curve (AUC) value of 0.736, with 0.828 sensitivity and 0.692 specificity. Additional inter-observer clinical evaluation extends analysis to endometrial cancer, endometriosis, and adenomyosis, revealing the influence of anatomical heterogeneity and inter-observer variability on performance interpretation. With a reconstruction time of approximately 92.6 frames per second, the proposed framework establishes a baseline for unsupervised anomaly detection in the female pelvis and supports future integration into real-time MRI. Code is available upon request (https://github.com/AniKnu/UADPelvis), prospective data sets are available for academic collaboration.

</details>


### [8] [PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining](https://arxiv.org/abs/2602.06184)
*Cheng Liang,Chaoyi Wu,Weike Zhao,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 提出PhenoKG与PhenoLIP，通过将表型本体结构知识蒸馏进医疗VLM，实现更精确、可解释的表型识别与跨模态检索，显著优于现有SOTA；并发布专家验证的评测集PhenoBench。


<details>
  <summary>Details</summary>
Motivation: 现有医疗VLM多依赖粗粒度的图文对比学习，难以捕捉医学表型本体所蕴含的系统化视觉与语义先验，导致表型识别与可解释性不足。

Method: 1) 构建PhenoKG：首个大规模表型中心的多模态知识图，含52万+高质量图文对、3000+表型；2) 两阶段预训练PhenoLIP：先从文本本体学习结构化表型嵌入（知识增强表型空间），再以教师引导的知识蒸馏将该结构知识注入多模态预训练；3) 发布PhenoBench：专家验证的表型识别基准（7800+图文、1000+表型）。

Result: 与SOTA相比：表型分类较BiomedCLIP提升8.85%，跨模态检索较BIOMEDICA提升15.03%；在PhenoBench与多项实验中全面优于基线，展现更强的结构化与可解释表型理解能力。

Conclusion: 将表型本体的结构化先验融入医疗VLM可显著提升表型识别与检索性能并增强可解释性；PhenoKG与PhenoLIP提供可复用的数据与方法范式，PhenoBench为标准化评测奠基。

Abstract: Recent progress in large-scale CLIP-like vision-language models(VLMs) has greatly advanced medical image analysis. However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies. To address this gap, we construct PhenoKG, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes. Building upon PhenoKG, we propose PhenoLIP, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process. We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective. To support evaluation, we further introduce PhenoBench, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image--caption pairs covering more than 1,000 phenotypes. Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85\% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.

</details>


### [9] [DeDPO: Debiased Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2602.06195)
*Khiem Pham,Quang Nguyen,Tung Nguyen,Jingsen Zhu,Michele Santacatterina,Dimitris Metaxas,Ramin Zabih*

Main category: cs.CV

TL;DR: 提出DeDPO，将因果推断中的去偏估计融入DPO，使扩散模型在有限人工偏好+大量合成AI反馈下也能稳健对齐，性能可达甚至超越全人工标注上限，显著降低成本并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有DPO对齐依赖大量高质量人工偏好，成本高且难以扩展；合成反馈便宜但含系统性偏差与噪声，直接使用会劣化模型。需要一种方法能有效利用不完美的合成标注并校正其偏差。

Method: 提出半监督框架：少量人工偏好数据 + 大量未标注对，通过合成AI（如自训练或VLM）生成偏好标注。核心是Debiased DPO（DeDPO）：将因果推断中的去偏估计策略融入DPO目标，显式识别并校正合成标注的系统性偏差与噪声，从而在离线、无奖励建模的设定下稳健学习。

Result: 在不同合成标注方案下均表现稳健，性能可与完全依赖人类标注训练的理论上限匹配，且有时超越；表明即便反馈不完美，DeDPO仍能有效对齐扩散模型。

Conclusion: DeDPO为人机对齐提供了可扩展、低成本的解决方案：通过因果去偏将不完美的合成监督转化为有效信号，减少对大量人工偏好的依赖。

Abstract: Direct Preference Optimization (DPO) has emerged as a predominant alignment method for diffusion models, facilitating off-policy training without explicit reward modeling. However, its reliance on large-scale, high-quality human preference labels presents a severe cost and scalability bottleneck. To overcome this, We propose a semi-supervised framework augmenting limited human data with a large corpus of unlabeled pairs annotated via cost-effective synthetic AI feedback. Our paper introduces Debiased DPO (DeDPO), which uniquely integrates a debiased estimation technique from causal inference into the DPO objective. By explicitly identifying and correcting the systematic bias and noise inherent in synthetic annotators, DeDPO ensures robust learning from imperfect feedback sources, including self-training and Vision-Language Models (VLMs). Experiments demonstrate that DeDPO is robust to the variations in synthetic labeling methods, achieving performance that matches and occasionally exceeds the theoretical upper bound of models trained on fully human-labeled data. This establishes DeDPO as a scalable solution for human-AI alignment using inexpensive synthetic supervision.

</details>


### [10] [AnyThermal: Towards Learning Universal Representations for Thermal Perception](https://arxiv.org/abs/2602.06203)
*Parv Maheshwari,Jay Karhade,Yogesh Chawla,Isaiah Adu,Florian Heisen,Andrew Porco,Andrew Jong,Yifei Liu,Santosh Pitla,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: AnyThermal 是一个面向热成像的通用骨干网络，通过将视觉基础模型（如 DINOv2）的表征蒸馏到热成像编码器中，学习跨环境、跨任务的稳健热特征；并配套开源的同步 RGB-热成像采集平台 TartanRGBT 与多环境数据集，零任务特化训练即可在多项下游任务与数据集上取得最高性能，最高提升达 36%。


<details>
  <summary>Details</summary>
Motivation: 现有热成像骨干多依赖小规模、任务特定的数据与训练范式，导致泛化性差、仅限特定场景与任务使用；缺乏覆盖多环境的 RGB-热成像同步数据也限制了通用模型的学习。

Method: 1) 提出 AnyThermal：以热成像编码器为主体，将 DINOv2 等视觉基础模型的特征通过蒸馏迁移到热域；2) 跨多环境（室内、航空、越野、城市场景）的热数据进行训练以增强泛化；3) 推出 TartanRGBT 开源同步采集平台，并据此采集平衡多样的 TartanRGBT 数据集；4) 在无需任务特定微调的前提下，直接用于跨模态地点识别、热分割、单目热深度估计等任务。

Result: 在多种公开数据集与多类任务上取得新的 SOTA，相比现有方法最高提升可达 36%，验证了跨环境与跨任务的鲁棒性与可迁移性。

Conclusion: 通过将视觉基础模型表征蒸馏到热域并配合多环境、多模态数据平台与数据集，AnyThermal 成为无需任务特化训练的通用热成像骨干，能在多任务与多环境下稳定取得领先表现。

Abstract: We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific training from small-scale data result in utility limited to a specific environment and task. Unlike prior methods, AnyThermal can be used for a wide range of environments (indoor, aerial, off-road, urban) and tasks, all without task-specific training. Our key insight is to distill the feature representations from visual foundation models such as DINOv2 into a thermal encoder using thermal data from these multiple environments. To bridge the diversity gap of the existing RGB-Thermal datasets, we introduce the TartanRGBT platform, the first open-source data collection platform with synced RGB-Thermal image acquisition. We use this payload to collect the TartanRGBT dataset - a diverse and balanced dataset collected in 4 environments. We demonstrate the efficacy of AnyThermal and TartanRGBT, achieving state-of-the-art results with improvements of up to 36% across diverse environments and downstream tasks on existing datasets.

</details>


### [11] [DroneKey++: A Size Prior-free Method and New Benchmark for Drone 3D Pose Estimation from Sequential Images](https://arxiv.org/abs/2602.06211)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 提出DroneKey++：无需先验的无人机关键点检测+分类+3D位姿估计框架，并发布大规模合成数据集6DroneSyn；在多机型与户外背景下实现高精度与实时性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机3D位姿方法依赖尺寸/网格等先验，且数据集规模小、机型单一、采集受限，难以验证跨机型泛化与实用性。

Method: 1) 架构：关键点编码器同时做关键点检测与类别识别；位姿解码器基于光线几何推理结合类别嵌入估计3D姿态。2) 数据：构建6DroneSyn，>50K图像，7种机型、88个户外背景，通过360°全景合成生成。

Result: 在基准上，旋转MAE 17.34°/MedAE 17.1°；平移MAE 0.135 m/MedAE 0.242 m；推理速度CPU 19.25 FPS、GPU 414.07 FPS，显示良好跨机型泛化与实时性。

Conclusion: DroneKey++在无先验条件下实现端到端关键点+分类+3D位姿估计，结合大规模6DroneSyn数据集，兼顾精度、泛化与实时应用价值，数据集已公开。

Abstract: Accurate 3D pose estimation of drones is essential for security and surveillance systems. However, existing methods often rely on prior drone information such as physical sizes or 3D meshes. At the same time, current datasets are small-scale, limited to single models, and collected under constrained environments, which makes reliable validation of generalization difficult. We present DroneKey++, a prior-free framework that jointly performs keypoint detection, drone classification, and 3D pose estimation. The framework employs a keypoint encoder for simultaneous keypoint detection and classification, and a pose decoder that estimates 3D pose using ray-based geometric reasoning and class embeddings. To address dataset limitations, we construct 6DroneSyn, a large-scale synthetic benchmark with over 50K images covering 7 drone models and 88 outdoor backgrounds, generated using 360-degree panoramic synthesis. Experiments show that DroneKey++ achieves MAE 17.34 deg and MedAE 17.1 deg for rotation, MAE 0.135 m and MedAE 0.242 m for translation, with inference speeds of 19.25 FPS (CPU) and 414.07 FPS (GPU), demonstrating both strong generalization across drone models and suitability for real-time applications. The dataset is publicly available.

</details>


### [12] [Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models](https://arxiv.org/abs/2602.06214)
*Jorge Daniel Rodríguez-Vidal,Gabriel Villalonga,Diego Porres,Antonio M. López Peña*

Main category: cs.CV

TL;DR: 提出一种可微分车辆模型，将动作序列展开为自车坐标下的路点轨迹，使动作式E2E自动驾驶可在路点基准上训练与评测；在多基准上优于基线，NAVSIM navhard 达到SOTA。


<details>
  <summary>Details</summary>
Motivation: E2E自动驾驶存在“路点式”和“动作式”两大范式。当前基准与训练流程多聚焦于路点式，导致动作式方法难以训练与公平比较，进而阻碍其发展。作者希望打通两者之间的评测与监督鸿沟。

Method: 设计一个新颖的、可微分的车辆动力学/运动学模型，将网络预测的加速/转向/制动等动作序列前向滚动，得到对应的自车坐标系路点轨迹；在路点空间进行监督与评测，从而在不改动既有路点基准协议的前提下训练动作式策略。

Result: 在多个具有挑战性的基准上，框架相较基线取得一致性提升；特别是在NAVSIM的navhard设置上达到当前最优（SOTA）表现。

Conclusion: 该可微车辆模型有效弥合了路点与动作范式的训练与评测差距，使动作式E2E驾驶模型能够在主流路点基准中被统一训练与比较，并带来显著性能提升。

Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are waypoint-based, which makes action-based policies harder to train and compare, slowing their progress. To bridge this waypoint-action gap, we propose a novel, differentiable vehicle-model framework that rolls out predicted action sequences to their corresponding ego-frame waypoint trajectories while supervising in waypoint space. Our approach enables action-based architectures to be trained and evaluated, for the first time, within waypoint-based benchmarks without modifying the underlying evaluation protocol. We extensively evaluate our framework across multiple challenging benchmarks and observe consistent improvements over the baselines. In particular, on NAVSIM \texttt{navhard} our approach achieves state-of-the-art performance. Our code will be made publicly available upon acceptance.

</details>


### [13] [Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings](https://arxiv.org/abs/2602.06218)
*Grégoire Dhimoïla,Thomas Fel,Victor Boutin,Agustin Picard*

Main category: cs.CV

TL;DR: 提出一种对齐稀疏自编码器以检验并利用“等能量”假设，从而把VLM潜在空间分解为双模与单模原子，解释并消除模态间鸿沟，同时改进检索与可控编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在对齐图像与文本上表现出色，其共享嵌入空间的几何结构缺乏可解释性。作者认为真正跨模共享的概念应在不同模态上呈现相同的平均“能量”，据此提出利用跨模冗余来刻画与约束潜在几何。

Method: 提出Aligned Sparse Autoencoder（SAE）：在保持重构质量的同时，引入“等能量”正则以鼓励图像与文本在共享概念上的平均能量一致。先在可控合成数据上进行检验，再将方法应用于主流VLM嵌入，对学得的稀疏原子进行几何与功能分解分析。

Result: (i) 稀疏的双模原子承载全部跨模对齐信号；(ii) 单模原子表现为模态特有偏置，完全解释模态间的gap；(iii) 去除单模原子即可在不损失性能的情况下消除gap；(iv) 仅在双模子空间进行向量运算能实现分布内编辑并提升检索效果。

Conclusion: 合适的归纳偏置（等能量约束+稀疏分解）既能保持模型保真度，又能使VLM潜在几何结构可解释且可操作，带来实用的对齐与编辑改进。

Abstract: Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.

</details>


### [14] [ForeHOI: Feed-forward 3D Object Reconstruction from Daily Hand-Object Interaction Videos](https://arxiv.org/abs/2602.06226)
*Yuantao Chen,Jiahao Chang,Chongjie Ye,Chaoran Zhang,Zhaojie Fang,Chenghong Li,Xiaoguang Han*

Main category: cs.CV

TL;DR: ForeHOI提出一个前馈式模型，从单目手-物交互视频直接重建3D物体形状，结合2D遮挡区域修复与3D形状补全，显著缓解遮挡问题，速度较优化法提升约100倍并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目在野视频广泛记录手-物交互，但由于手物严重遮挡与相机/手/物体耦合运动，现有方法难以可靠、快速地重建物体3D形状；优化式方法慢且依赖预处理。

Method: 提出ForeHOI：一个端到端前馈框架，联合学习2D掩膜修复（inpainting）与3D形状补全，设计2D-3D信息交互以缓解遮挡并提升重建质量；无需预处理，推理<1分钟。并构建首个大规模高保真手-物交互合成数据集，含全面标注用于训练。

Result: 在多项实验中，ForeHOI在物体重建精度上达到SOTA，显著优于以往优化式方法，并带来约100倍的速度提升。

Conclusion: 联合2D与3D补全的前馈框架能有效克服单目手持物体视频中的严重遮挡，实现快速高质量的3D物体重建；配套的大规模合成数据进一步支撑其性能。

Abstract: The ubiquity of monocular videos capturing daily hand-object interactions presents a valuable resource for embodied intelligence. While 3D hand reconstruction from in-the-wild videos has seen significant progress, reconstructing the involved objects remains challenging due to severe occlusions and the complex, coupled motion of the camera, hands, and object. In this paper, we introduce ForeHOI, a novel feed-forward model that directly reconstructs 3D object geometry from monocular hand-object interaction videos within one minute of inference time, eliminating the need for any pre-processing steps. Our key insight is that, the joint prediction of 2D mask inpainting and 3D shape completion in a feed-forward framework can effectively address the problem of severe occlusion in monocular hand-held object videos, thereby achieving results that outperform the performance of optimization-based methods. The information exchanges between the 2D and 3D shape completion boosts the overall reconstruction quality, enabling the framework to effectively handle severe hand-object occlusion. Furthermore, to support the training of our model, we contribute the first large-scale, high-fidelity synthetic dataset of hand-object interactions with comprehensive annotations. Extensive experiments demonstrate that ForeHOI achieves state-of-the-art performance in object reconstruction, significantly outperforming previous methods with around a 100x speedup. Code and data are available at: https://github.com/Tao-11-chen/ForeHOI.

</details>


### [15] [ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning](https://arxiv.org/abs/2602.06251)
*Aman Anand,Amir Eskandari,Elyas Rahsno,Farhana Zulkernine*

Main category: cs.CV

TL;DR: 提出ASMa自监督骨架动作表示学习：通过非对称时空掩蔽与特征对齐、再经知识蒸馏，获得更全面鲁棒的表示，并在多数据集上显著优于现有SSL且接近监督性能，同时大幅压缩模型、加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作自监督多依赖“高运动帧、高手度关节”相关的增强/掩蔽，造成表示偏置，难泛化到多样运动模式与噪声环境；需要一种覆盖全谱时空动态且便于边端部署的方法。

Method: 1) 非对称时空掩蔽ASMa：两条互补视角——(a) 掩蔽高手度关节+低运动区域；(b) 掩蔽低程度关节+高运动帧，迫使模型学习全面时空依赖。2) 可学习特征对齐模块：对两种掩蔽视图的表示进行对齐与融合，减少视图间差异。3) 知识蒸馏：将对齐后的强表征压缩到轻量学生模型，便于资源受限设备部署。

Result: 在NTU RGB+D 60/120与PKU-MMD上，较现有SSL微调平均提升2.7–4.4%，在噪声迁移学习上最高+5.9%；相较全监督达到有竞争力的性能。蒸馏后模型参数减少91.4%，边缘设备推理加速约3倍，精度基本保持。

Conclusion: 通过互补的非对称时空掩蔽与特征对齐，ASMa学得更平衡、鲁棒的骨架动作表征；结合蒸馏实现高效部署，兼顾精度与效率，适合资源受限场景。

Abstract: Self-supervised learning (SSL) has shown remarkable success in skeleton-based action recognition by leveraging data augmentations to learn meaningful representations. However, existing SSL methods rely on data augmentations that predominantly focus on masking high-motion frames and high-degree joints such as joints with degree 3 or 4. This results in biased and incomplete feature representations that struggle to generalize across varied motion patterns. To address this, we propose Asymmetric Spatio-temporal Masking (ASMa) for Skeleton Action Representation Learning, a novel combination of masking to learn a full spectrum of spatio-temporal dynamics inherent in human actions. ASMa employs two complementary masking strategies: one that selectively masks high-degree joints and low-motion, and another that masks low-degree joints and high-motion frames. These masking strategies ensure a more balanced and comprehensive skeleton representation learning. Furthermore, we introduce a learnable feature alignment module to effectively align the representations learned from both masked views. To facilitate deployment in resource-constrained settings and on low-resource devices, we compress the learned and aligned representation into a lightweight model using knowledge distillation. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our approach outperforms existing SSL methods with an average improvement of 2.7-4.4% in fine-tuning and up to 5.9% in transfer learning to noisy datasets and achieves competitive performance compared to fully supervised baselines. Our distilled model achieves 91.4% parameter reduction and 3x faster inference on edge devices while maintaining competitive accuracy, enabling practical deployment in resource-constrained scenarios.

</details>


### [16] [An Interpretable Vision Transformer as a Fingerprint-Based Diagnostic Aid for Kabuki and Wiedemann-Steiner Syndromes](https://arxiv.org/abs/2602.06282)
*Marilyn Lionts,Arnhildur Tomasdottir,Viktor I. Agustsson,Yuankai Huo,Hans T. Bjornsson,Lotta M. Ellingsen*

Main category: cs.CV

TL;DR: 提出用指纹图像+视觉Transformer模型区分Kabuki综合征与Wiedemann‑Steiner综合征及对照，AUC最高0.85，展示注意力可解释性，表明存在综合征特异的皮纹特征，可作为无创、可及的辅助诊断工具。


<details>
  <summary>Details</summary>
Motivation: KS与WSS虽有不同病因，但临床表型重叠且许多患者因基因检测可及性差而未被诊断。皮纹异常是多种遗传综合征的经典体征，却在分子检测时代被低估。若能用指纹自动识别综合征，将提供一种低成本、无创、可解释的早期筛查手段，提升诊断可及性。

Method: 构建基于Vision Transformer的深度学习模型，输入为指纹图像；设置三项二分类任务：对照vs.KS、对照vs.WSS、KS vs.WSS；评估AUC与F1；使用注意力可视化定位对预测最重要的指纹区域以提升可解释性。

Result: 模型在三任务上的AUC分别为0.80、0.73、0.85；对应F1分别为0.71、0.72、0.83。注意力热图显示特定指纹区域对分类最为关键，提示存在综合征特异的皮纹模式。

Conclusion: 指纹中含有区分KS与WSS及对照的可学习特征。基于指纹的AI具备作为无创、可解释且可及的辅助诊断工具的可行性，有望用于早期识别这类易漏诊的遗传综合征。

Abstract: Kabuki syndrome (KS) and Wiedemann-Steiner syndrome (WSS) are rare but distinct developmental disorders that share overlapping clinical features, including neurodevelopmental delay, growth restriction, and persistent fetal fingertip pads. While genetic testing remains the diagnostic gold standard, many individuals with KS or WSS remain undiagnosed due to barriers in access to both genetic testing and expertise. Dermatoglyphic anomalies, despite being established hallmarks of several genetic syndromes, remain an underutilized diagnostic signal in the era of molecular testing. This study presents a vision transformer-based deep learning model that leverages fingerprint images to distinguish individuals with KS and WSS from unaffected controls and from one another. We evaluate model performance across three binary classification tasks. Across the three classification tasks, the model achieved AUC scores of 0.80 (control vs. KS), 0.73 (control vs. WSS), and 0.85 (KS vs. WSS), with corresponding F1 scores of 0.71, 0.72, and 0.83, respectively. Beyond classification, we apply attention-based visualizations to identify fingerprint regions most salient to model predictions, enhancing interpretability. Together, these findings suggest the presence of syndrome-specific fingerprint features, demonstrating the feasibility of a fingerprint-based artificial intelligence (AI) tool as a noninvasive, interpretable, and accessible future diagnostic aid for the early diagnosis of underdiagnosed genetic syndromes.

</details>


### [17] [MMEarth-Bench: Global Model Adaptation via Multimodal Test-Time Training](https://arxiv.org/abs/2602.06285)
*Lucia Gordon,Serge Belongie,Christian Igel,Nico Lang*

Main category: cs.CV

TL;DR: 提出MMEarth-Bench：涵盖12种模态、5个多模态环境任务、全球分布且含域内/域外测试；基准显示多模态自监督预训练在小数据更稳健但地理泛化差。提出测试时训练+多模态重建(TTT-MMR)，利用测试时可得的全部模态作辅助任务，提升随机与地理分布测试性能；地理分批在正则化与专化间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间基准模态少、覆盖差，难以全面评估在全球尺度上的多模态预训练模型，尤其是其在小样本与地理迁移场景中的表现与鲁棒性。需要新的基准与方法来检验与提升多模态预训练的地理泛化与适配能力。

Method: 1) 构建MMEarth-Bench：5个多模态环境任务、12种数据模态、全球分布样本，提供域内(IID)与地理OOD测试划分；2) 系统评测多样的（多模态）预训练模型；3) 提出TTT-MMR：一种与模型结构无关的测试时训练策略，在推理阶段以“多模态重建”为辅助目标，利用测试时可用但可能非模型输入的全部模态进行自监督适配；并引入“地理分批”以平衡正则化与专化。

Result: 基准结果显示：多模态/自监督预训练在小数据情境下提升鲁棒性，但模型的地理泛化仍然较差。采用TTT-MMR后，随机划分与地理OOD划分上性能均有提升；地理分批带来更好的稳健-专化折中。

Conclusion: MMEarth-Bench为评估地理空间多模态预训练与地理泛化提供了新基准；现有模型在地理迁移上仍显不足。TTT-MMR作为通用测试时自适应方案，能有效利用多模态信息改进跨地域泛化与任务适配，建议未来工作结合更强的多模态预训练与地理感知适配策略。

Abstract: Recent research in geospatial machine learning has demonstrated that models pretrained with self-supervised learning on Earth observation data can perform well on downstream tasks with limited training data. However, most of the existing geospatial benchmark datasets have few data modalities and poor global representation, limiting the ability to evaluate multimodal pretrained models at global scales. To fill this gap, we introduce MMEarth-Bench, a collection of five new multimodal environmental tasks with 12 modalities, globally distributed data, and both in- and out-of-distribution test splits. We benchmark a diverse set of pretrained models and find that while (multimodal) pretraining tends to improve model robustness in limited data settings, geographic generalization abilities remain poor. In order to facilitate model adaptation to new downstream tasks and geographic domains, we propose a model-agnostic method for test-time training with multimodal reconstruction (TTT-MMR) that uses all the modalities available at test time as auxiliary tasks, regardless of whether a pretrained model accepts them as input. Our method improves model performance on both the random and geographic test splits, and geographic batching leads to a good trade-off between regularization and specialization during TTT. Our dataset, code, and visualization tool are linked from the project page at lgordon99.github.io/mmearth-bench.

</details>


### [18] [Unsupervised MRI-US Multimodal Image Registration with Multilevel Correlation Pyramidal Optimization](https://arxiv.org/abs/2602.06288)
*Jiazheng Wang,Zeyu Liu,Min Liu,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: 提出一种无监督多模态医学图像配准方法MCPO，通过多层级相关金字塔与耦合凸优化实现稳健的术前-术中配准，并在Learn2Reg 2025的ReMIND2Reg任务验证与测试阶段均获第一，Resect数据集平均TRE达1.798 mm。


<details>
  <summary>Details</summary>
Motivation: 术中导航需将术前与术中多模态影像精准对齐，但多模态间外观差异与术中组织形变（移位、切除）导致传统或单尺度方法难以获得稳定、细致的形变场。Learn2Reg 2025针对这一难题设有挑战任务，亟需一种既能跨模态鲁棒匹配、又能兼顾全局与局部细节的无监督方法。

Method: 1) 采用MINC（modality independent neighborhood descriptor）提取各模态的局部结构特征，将多模态影像映射到统一特征空间。2) 构建多层级相关金字塔，对不同尺度特征做稠密相关性分析，获取多尺度位移线索。3) 设计权重平衡的耦合凸优化，在金字塔层间进行全局到局部的形变场优化与细节补偿，生成平滑且细粒度的位移场。方法为无监督训练/优化，无需配准标注。

Result: 在Learn2Reg 2025的ReMIND2Reg任务中，方法在验证与测试阶段均排名第一；在Resect数据集上实现平均目标配准误差（TRE）1.798 mm，显示出在术前-术中配准中的高精度与泛化能力。代码开源：https://github.com/wjiazheng/MCPO。

Conclusion: MCPO通过跨模态特征表示与多层级相关-优化框架，有效应对多模态差异与术中形变问题，兼顾全局稳健与局部精细，达到领先性能，并具备较好的数据与任务可迁移性，适用于临床术中导航配准场景。

Abstract: Surgical navigation based on multimodal image registration has played a significant role in providing intraoperative guidance to surgeons by showing the relative position of the target area to critical anatomical structures during surgery. However, due to the differences between multimodal images and intraoperative image deformation caused by tissue displacement and removal during the surgery, effective registration of preoperative and intraoperative multimodal images faces significant challenges. To address the multimodal image registration challenges in Learn2Reg 2025, an unsupervised multimodal medical image registration method based on multilevel correlation pyramidal optimization (MCPO) is designed to solve these problems. First, the features of each modality are extracted based on the modality independent neighborhood descriptor, and the multimodal images is mapped to the feature space. Second, a multilevel pyramidal fusion optimization mechanism is designed to achieve global optimization and local detail complementation of the displacement field through dense correlation analysis and weight-balanced coupled convex optimization for input features at different scales. Our method focuses on the ReMIND2Reg task in Learn2Reg 2025. Based on the results, our method achieved the first place in the validation phase and test phase of ReMIND2Reg. The MCPO is also validated on the Resect dataset, achieving an average TRE of 1.798 mm. This demonstrates the broad applicability of our method in preoperative-to-intraoperative image registration. The code is avaliable at https://github.com/wjiazheng/MCPO.

</details>


### [19] [Accelerating Vision Transformers on Brain Processing Unit](https://arxiv.org/abs/2602.06300)
*Jinchi Tang,Yan Guo*

Main category: cs.CV

TL;DR: 将ViT（如DeiT）的线性层与层归一化重构为等价的卷积与算子，使其在只支持4D卷积加速（INT8）的BPU上高效运行，无需重训，精度小幅下降、速度显著提升（至多3.8倍）。


<details>
  <summary>Details</summary>
Motivation: CNN优化的BPU对4D卷积有高效INT8加速，但ViT的线性层/LayerNorm作用于3D张量，结构与BPU加速单元不匹配，导致ViT难以在BPU上高效部署。需要一种将ViT映射到卷积友好形式、同时保留原权重与精度的方案。

Method: 提出对ViT（DeiT）进行结构性重写：用精心设计的卷积算子替换线性层，并以可在BPU上高效执行的形式替换/等效实现LayerNorm；通过权重重排/映射使原模型权重可直接继承，实现无重训或微调的部署；以INT8量化适配BPU的数据通路。

Result: 在ImageNet上，量化后的DeiT-Base从81.8%降至80.4%（-1.4%），同时获得最高3.8倍的推理加速；在花卉分类数据集上，微调后的DeiT-Base仅有约0.5%的精度下降，仍保持优秀表现。

Conclusion: 通过将ViT的关键算子卷积化并适配BPU的4D卷积加速，可在无需重训的前提下高效部署Transformer模型，实现小幅精度损失与显著速度提升；该工作据称为首个在BPU上充分发挥加速优势的ViT部署方案。

Abstract: With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT) models, such as the Data-efficient Image Transformer (DeiT), have demonstrated superior performance and play increasingly crucial roles in computer vision tasks. However, due to the architectural mismatch between CNN-optimized hardware and Vision Transformer computation characteristics--namely, that linear layers in Transformers operate on three-dimensional data while BPU acceleration is designed for four-dimensional convolution operations-it is difficult or even impossible to leverage BPU's advantages when deploying Vision Transformers. To address this challenge, we propose a novel approach that restructures the Vision Transformer by replacing linear layers and layer normalization operations with carefully designed convolutional operators. This enables DeiT to fully utilize the acceleration capabilities of BPUs, while allowing the original weight parameters to be inherited by the restructured models without retraining or fine-tuning. To the best of our knowledge, this is the first successful deployment of Vision Transformers that fully leverages BPU classification datasets demonstrate the effectiveness of our approach. Specifically, the quantized DeiT-Base model achieves 80.4% accuracy on ImageNet, compared to the original 81.8%, while obtaining up to a 3.8* inference speedup. Our finetuned DeiT model on the flower classification dataset also achieves excellent performance, with only a 0.5% accuracy drop for the DeiT-Base model, further demonstrating the effectiveness of our method.

</details>


### [20] [Adaptive and Balanced Re-initialization for Long-timescale Continual Test-time Domain Adaptation](https://arxiv.org/abs/2602.06328)
*Yanshuo Wang,Jinguang Tong,Jun Lan,Weiqiang Wang,Huijia Zhu,Haoxing Chen,Xuesong Li,Jie Hong*

Main category: cs.CV

TL;DR: 论文提出一种在持续测试时域自适应（CTTA）场景下提升长期性能的“自适应与平衡重置”（ABR）策略，通过依据标签翻转变化动态决定权重重置间隔，缓解长期适应退化并在多项基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法多关注短期自适应优化，但在非平稳环境持续变化的长期过程中，模型往往出现漂移、累积误差或灾难性遗忘，导致性能下降。作者观察到长期性能与预测标签翻转的轨迹模式相关，因而动机是利用这一信号设计机制以维持长期稳健性。

Method: 提出ABR策略：监控随时间的标签翻转（预测类别变化）作为环境/模型不稳定性的代理指标；当翻转动态表明不稳定上升时，触发或缩短重初始化间隔，对模型权重进行重置；当环境相对稳定时拉长间隔。核心是基于翻转变化自适应地确定重置频率，实现“自适应与平衡”的权重重置，以避免过度拟合瞬时分布并减轻累积漂移。

Result: 在多个CTTA基准上进行广泛实验，ABR在长期评测指标上取得更优性能，相比现有方法更好地维持随时间的准确率与稳定性。

Conclusion: 利用标签翻转轨迹作为信号的自适应重置策略能有效抑制长期漂移，改善CTTA的长期表现；简单、通用，易于集成到现有CTTA流程中，并在基准上验证其优越性。

Abstract: Continual test-time domain adaptation (CTTA) aims to adjust models so that they can perform well over time across non-stationary environments. While previous methods have made considerable efforts to optimize the adaptation process, a crucial question remains: Can the model adapt to continually changing environments over a long time? In this work, we explore facilitating better CTTA in the long run using a re-initialization (or reset) based method. First, we observe that the long-term performance is associated with the trajectory pattern in label flip. Based on this observed correlation, we propose a simple yet effective policy, Adaptive-and-Balanced Re-initialization (ABR), towards preserving the model's long-term performance. In particular, ABR performs weight re-initialization using adaptive intervals. The adaptive interval is determined based on the change in label flip. The proposed method is validated on extensive CTTA benchmarks, achieving superior performance.

</details>


### [21] [Halt the Hallucination: Decoupling Signal and Semantic OOD Detection Based on Cascaded Early Rejection](https://arxiv.org/abs/2602.06330)
*Ningkang Peng,Chuanjie Cheng,Jingyang Mao,Xiaoqian Peng,Feng Xing,Bo Zhang,Chao Tan,Zhichao Zheng,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出层级式早期拒绝（CER）用于高效稳健OOD检测：入口用拉普拉斯算子做结构能量筛（SES）拦截物理噪声；中层用语义感知超球能量（SHE）分离幅值与方向捕获细粒度语义偏差。计算开销降32%，CIFAR-100上FPR95从33.58%降至22.84%、AUROC到93.97%，传感器故障情境显著优于SOTA，可作为通用插件集成多种模型。


<details>
  <summary>Details</summary>
Motivation: 现有OOD方法对低层物理噪声仍进行全量推理，既浪费算力又易产生“语义幻觉”，将物理异常误判为高置信语义特征，需要一种在早期就能拦截非语义异常、并在中后期精准识别语义分布偏差的机制。

Method: 提出级联式早拒框架CER：1) Structural Energy Sieve（SES）：在网络入口以拉普拉斯算子计算结构能量，构造非参数阈门快速拒绝物理/传感器级异常；2) Semantically-aware Hyperspherical Energy（SHE）：在中间层将特征“幅值-方向”解耦，在超球面评估方向一致性与能量，检测细粒度语义偏移；两者以粗到细级联过滤，作为插件无缝嵌入主干网络。

Result: 在CIFAR-100 OOD检测中，平均FPR95由33.58%降至22.84%，AUROC提升至93.97%；总体计算开销降低约32%；在模拟传感器失效等真实场景中显著优于SOTA。

Conclusion: CER通过入口结构能量筛与中层超球能量检测实现粗到细的早期拒绝，兼顾效率与鲁棒性，减少误触发的语义幻觉，并可作为通用插件为多种SOTA模型带来性能与计算的双重收益。

Abstract: Efficient and robust Out-of-Distribution (OOD) detection is paramount for safety-critical applications.However, existing methods still execute full-scale inference on low-level statistical noise. This computational mismatch not only incurs resource waste but also induces semantic hallucination, where deep networks forcefully interpret physical anomalies as high-confidence semantic features.To address this, we propose the Cascaded Early Rejection (CER) framework, which realizes hierarchical filtering for anomaly detection via a coarse-to-fine logic.CER comprises two core modules: 1)Structural Energy Sieve (SES), which establishes a non-parametric barrier at the network entry using the Laplacian operator to efficiently intercept physical signal anomalies; and 2) the Semantically-aware Hyperspherical Energy (SHE) detector, which decouples feature magnitude from direction in intermediate layers to identify fine-grained semantic deviations. Experimental results demonstrate that CER not only reduces computational overhead by 32% but also achieves a significant performance leap on the CIFAR-100 benchmark:the average FPR95 drastically decreases from 33.58% to 22.84%, and AUROC improves to 93.97%. Crucially, in real-world scenarios simulating sensor failures, CER exhibits performance far exceeding state-of-the-art methods. As a universal plugin, CER can be seamlessly integrated into various SOTA models to provide performance gains.

</details>


### [22] [Taming SAM3 in the Wild: A Concept Bank for Open-Vocabulary Segmentation](https://arxiv.org/abs/2602.06333)
*Gensheng Pei,Xiruo Jiang,Yazhou Yao,Xiangbo Shu,Fumin Shen,Byeungwoo Jeon*

Main category: cs.CV

TL;DR: 提出ConceptBank，为SAM3在开放词汇分割中的参数无校准框架，利用目标域统计动态构建概念库，通过原型锚定、代表样本挖掘和概念融合处理数据漂移与概念漂移，显著提升鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: SAM3依赖预定义概念提示进行开放词汇分割，但在目标域出现数据分布或标签条件分布变化时，提示与视觉证据的对齐会失效，导致性能下降；需要一种无需重新训练、可按目标域即时自适应的校准方法。

Method: 在目标域构建数据集特定的概念库：1) 通过类内视觉原型将像素证据锚定到类别级别；2) 挖掘代表性支持样本以抑制数据漂移下的异常值；3) 融合候选概念以纠正概念漂移。整体为参数无、即插即用的校准流程，作用于SAM3的提示与像素对齐。

Result: 在自然场景与遥感等存在分布漂移的场景中，将ConceptBank用于SAM3可有效适应分布变化，建立了在OVS任务上兼具鲁棒性与效率的新基线；代码与模型已开源。

Conclusion: 通过目标域统计驱动的概念库构建与三步校准策略，ConceptBank在无需训练的前提下恢复提示-视觉对齐，缓解数据/概念漂移，提升开放词汇分割的稳健性与效率。

Abstract: The recent introduction of \texttt{SAM3} has revolutionized Open-Vocabulary Segmentation (OVS) through \textit{promptable concept segmentation}, which grounds pixel predictions in flexible concept prompts. However, this reliance on pre-defined concepts makes the model vulnerable: when visual distributions shift (\textit{data drift}) or conditional label distributions evolve (\textit{concept drift}) in the target domain, the alignment between visual evidence and prompts breaks down. In this work, we present \textsc{ConceptBank}, a parameter-free calibration framework to restore this alignment on the fly. Instead of adhering to static prompts, we construct a dataset-specific concept bank from the target statistics. Our approach (\textit{i}) anchors target-domain evidence via class-wise visual prototypes, (\textit{ii}) mines representative supports to suppress outliers under data drift, and (\textit{iii}) fuses candidate concepts to rectify concept drift. We demonstrate that \textsc{ConceptBank} effectively adapts \texttt{SAM3} to distribution drifts, including challenging natural-scene and remote-sensing scenarios, establishing a new baseline for robustness and efficiency in OVS. Code and model are available at https://github.com/pgsmall/ConceptBank.

</details>


### [23] [SPDA-SAM: A Self-prompted Depth-Aware Segment Anything Model for Instance Segmentation](https://arxiv.org/abs/2602.06335)
*Yihan Shang,Wei Wang,Chao Huang,Xinghui Dong*

Main category: cs.CV

TL;DR: 提出SPDA-SAM：在SAM中引入自提示与深度感知的RGB-D融合，减少人工提示依赖并增强空间结构感知，显著提升多数据集实例分割表现。


<details>
  <summary>Details</summary>
Motivation: SAM虽具强泛化，但强依赖高质量人工提示；常用RGB图像缺乏深度信息，导致空间结构理解与边界刻画不足。需要一种能自动生成有效提示并补充深度结构信息的方法。

Method: 在SAM上构建两大组件：1) 语义-空间自提示模块（SSSPM），分别从SAM的图像编码器和掩码解码器中提取语义与空间提示，实现无需人工的自提示；2) 粗到细RGB-D融合模块（C2FFM），先用单目RGB估计深度，再进行两级融合：以深度的结构信息进行粗粒度引导，结合局部深度变化编码以细粒度融合特征，实现更精确的特征表示。

Result: 在十二个数据集上优于现有SOTA方法。性能提升归因于自提示提供的有效引导与RGB-D粗细融合对空间信息缺失的补偿。

Conclusion: 自提示与深度感知的结合能显著减轻SAM对人工提示的依赖并提升实例分割精度；SPDA-SAM为在SAM中引入自提示与RGB-D融合提供了有效范式。

Abstract: Recently, Segment Anything Model (SAM) has demonstrated strong generalizability in various instance segmentation tasks. However, its performance is severely dependent on the quality of manual prompts. In addition, the RGB images that instance segmentation methods normally use inherently lack depth information. As a result, the ability of these methods to perceive spatial structures and delineate object boundaries is hindered. To address these challenges, we propose a Self-prompted Depth-Aware SAM (SPDA-SAM) for instance segmentation. Specifically, we design a Semantic-Spatial Self-prompt Module (SSSPM) which extracts the semantic and spatial prompts from the image encoder and the mask decoder of SAM, respectively. Furthermore, we introduce a Coarse-to-Fine RGB-D Fusion Module (C2FFM), in which the features extracted from a monocular RGB image and the depth map estimated from it are fused. In particular, the structural information in the depth map is used to provide coarse-grained guidance to feature fusion, while local variations in depth are encoded in order to fuse fine-grained feature representations. To our knowledge, SAM has not been explored in such self-prompted and depth-aware manners. Experimental results demonstrate that our SPDA-SAM outperforms its state-of-the-art counterparts across twelve different data sets. These promising results should be due to the guidance of the self-prompts and the compensation for the spatial information loss by the coarse-to-fine RGB-D fusion operation.

</details>


### [24] [Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering](https://arxiv.org/abs/2602.06343)
*Weiquan Wang,Feifei Shao,Lin Li,Zhen Wang,Jun Xiao,Long Chen*

Main category: cs.CV

TL;DR: U-4DGS通过不确定性建模与双栅格化渲染，缓解单目视频下人体被遮挡导致的重建崩溃与闪烁，实现更稳健、更高保真度的动态人体渲染。


<details>
  <summary>Details</summary>
Motivation: 单目视频中的动态人体渲染在遮挡下常出现严重退化：生成先验会引入时序闪烁，几何硬性假设难以覆盖多样外观。需要一种能识别不可靠观测并自适应抑制伪影、同时防止无可靠线索区域几何漂移的方法。

Method: 将问题表述为异方差观测噪声下的MAP估计。提出U-4DGS：1) 概率形变网络（Probabilistic Deformation Network）预测像素对齐的不确定性；2) 双栅格化（Double Rasterization）生成不确定性与颜色/密度的像素对齐图；3) 不确定性图作为自适应梯度调制器，削弱不可靠观测的影响；4) 置信感知正则（Confidence-Aware Regularizations）利用学习到的置信度在时空上选择性传播有效约束，防止几何漂移。

Result: 在ZJU-MoCap与OcMotion上取得SOTA渲染保真度与鲁棒性，并显著减少遮挡导致的伪影与时序不稳定。

Conclusion: 通过显式建模异方差不确定性并将其融入渲染与优化流程，U-4DGS在遮挡与缺失观测条件下仍能稳定重建动态人体，优于依赖生成式填充或刚性几何先验的方法。

Abstract: High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.

</details>


### [25] [FlowConsist: Make Your Flow Consistent with Real Trajectory](https://arxiv.org/abs/2602.06346)
*Tianyi Zhang,Chengcheng Liu,Jinwei Chen,Chun-Le Guo,Chongyi Li,Ming-Ming Cheng,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出FlowConsist以提升fast flow模型的一步/少步生成稳定性：用模型自回归的边际速度替代条件速度，并在每个时间步做轨迹校正以对齐生成与真实边际分布，在ImageNet 256×256上一步采样达FID 1.52。


<details>
  <summary>Details</summary>
Motivation: 现有fast flow通过学习ODE路径积分实现少步生成，但存在两大问题：1) 用随机配对的噪声-数据构造的条件速度会引入系统性轨迹漂移，模型难以沿一致的ODE路径前进；2) 模型近似误差跨时间步累积，长时间区间偏差严重，影响生成质量与稳定性。需要一种能强制轨迹一致并抑制误差累积的训练框架。

Method: 提出FlowConsist：1) 轨迹一致性训练——以模型自身预测的边际速度（marginal velocity）替代依赖随机配对的条件速度，使优化目标与真实ODE轨迹对齐；2) 轨迹纠正（rectification）——在整个时间轴上，对每个时间步将生成样本的边际分布与真实数据分布对齐，从而抑制误差的跨步累积与放大。整体实现为少步/单步采样的fast flow训练范式。

Result: 在ImageNet 256×256上实现新的SOTA：单步采样FID 1.52，显示出在极少步数下的显著质量提升与稳定性；同时缓解轨迹漂移与长期误差累积问题。

Conclusion: 通过以边际速度替代条件速度并进行逐步分布对齐，FlowConsist在保持采样速度的同时显著提高轨迹一致性与生成质量，证明了纠正漂移与误差累积对fast flow的一步/少步生成至关重要。

Abstract: Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\times$256, achieving an FID of 1.52 with only 1 sampling step.

</details>


### [26] [Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image](https://arxiv.org/abs/2602.06355)
*Sanjana Reddy,Ishaan Malhi,Sally Ma,Praneet Dutta*

Main category: cs.CV

TL;DR: Di3PO提出一种区域稳定的偏好配对方法，用于T2I扩散模型的偏好微调，通过固定上下文、只在需改进区域构造正负样本，提高训练效率与效果；在文字渲染任务上优于SFT与DPO。


<details>
  <summary>Details</summary>
Motivation: 现有偏好微调需要大量生成并筛选正负图像对，代价高且常出现无效差异或无关像素方差过大，影响训练效率与稳定性。需要一种能聚焦于需改进区域、减少无关变化并降低成本的方法。

Method: 提出Di3PO：在构造正负样本时显式隔离/定位待改进的图像区域，使该区域在正负对之间呈系统性差异，而将周围上下文保持稳定，从而形成更有信息量的偏好信号并减少无关噪声。将其用于T2I中文本渲染场景进行训练与评估。

Result: 在文本渲染这一具有挑战性的任务中，Di3PO较SFT与DPO基线取得更好表现，显示出更高的偏好学习效率与效果；训练对的质量更高、无关像素方差更低。

Conclusion: 区域隔离式的偏好对构造能显著提升T2I扩散模型的偏好微调效率与性能。Di3PO通过稳定上下文与聚焦问题区域，在文本渲染任务上优于主流方法，表明该策略可作为偏好学习的有效范式。

Abstract: Existing methods for preference tuning of text-to-image (T2I) diffusion models often rely on computationally expensive generation steps to create positive and negative pairs of images. These approaches frequently yield training pairs that either lack meaningful differences, are expensive to sample and filter, or exhibit significant variance in irrelevant pixel regions, thereby degrading training efficiency. To address these limitations, we introduce "Di3PO", a novel method for constructing positive and negative pairs that isolates specific regions targeted for improvement during preference tuning, while keeping the surrounding context in the image stable. We demonstrate the efficacy of our approach by applying it to the challenging task of text rendering in diffusion models, showcasing improvements over baseline methods of SFT and DPO.

</details>


### [27] [Robust Pedestrian Detection with Uncertain Modality](https://arxiv.org/abs/2602.06363)
*Qian Bie,Xiao Wang,Bin Yang,Zhixi Yu,Jun Chen,Xin Xu*

Main category: cs.CV

TL;DR: 提出TRNT三模态数据集与AUNet框架，能在任意可用模态组合下进行鲁棒的行人检测，通过不确定性感知的模态验证与自适应交互实现可靠融合。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态行人检测主要利用RGB与TIR互补，但TIR缺少纹理细节、RGB在弱光下失效；NIR在低照下可保留纹理，能弥补RGB与TIR不足。此外，真实场景中设备可能无法同时获取全部模态，导致输入模态组合不确定，现有方法在任意模态缺失时性能显著下降。

Method: 1) 构建TRNT数据集：8,281组像素对齐的RGB-NIR-TIR三元组，覆盖日夜与多场景。2) 提出AUNet：包含统一模态验证与细化(UMVR)与模态感知交互(MAI)。UMVR内含不确定性感知路由器用于判别每种模态是否可用，并以语义细化保证该模态信息可信；MAI依据UMVR的判定自适应开启/关闭交互分支，在可用模态间进行有效互补融合。

Result: 在任意模态组合输入下，AUNet较现有CMPD方法显著提升检测性能，减少因模态缺失造成的漏检；TRNT为相关算法研究提供了标准化基准。

Conclusion: 利用NIR补足低光纹理、结合不确定性感知的模态可用性判定与自适应交互融合，可在不确定模态输入条件下实现更鲁棒的24小时行人检测；TRNT与AUNet为该方向提供数据与方法双重支撑。

Abstract: Existing cross-modal pedestrian detection (CMPD) employs complementary information from RGB and thermal-infrared (TIR) modalities to detect pedestrians in 24h-surveillance systems.RGB captures rich pedestrian details under daylight, while TIR excels at night. However, TIR focuses primarily on the person's silhouette, neglecting critical texture details essential for detection. While the near-infrared (NIR) captures texture under low-light conditions, which effectively alleviates performance issues of RGB and detail loss in TIR, thereby reducing missed detections. To this end, we construct a new Triplet RGB-NIR-TIR (TRNT) dataset, comprising 8,281 pixel-aligned image triplets, establishing a comprehensive foundation for algorithmic research. However, due to the variable nature of real-world scenarios, imaging devices may not always capture all three modalities simultaneously. This results in input data with unpredictable combinations of modal types, which challenge existing CMPD methods that fail to extract robust pedestrian information under arbitrary input combinations, leading to significant performance degradation. To address these challenges, we propose the Adaptive Uncertainty-aware Network (AUNet) for accurately discriminating modal availability and fully utilizing the available information under uncertain inputs. Specifically, we introduce Unified Modality Validation Refinement (UMVR), which includes an uncertainty-aware router to validate modal availability and a semantic refinement to ensure the reliability of information within the modality. Furthermore, we design a Modality-Aware Interaction (MAI) module to adaptively activate or deactivate its internal interaction mechanisms per UMVR output, enabling effective complementary information fusion from available modalities.

</details>


### [28] [Revisiting Salient Object Detection from an Observer-Centric Perspective](https://arxiv.org/abs/2602.06369)
*Fuxi Zhang,Yifan Wang,Hengrun Zhao,Zhuohan Sun,Changxing Xia,Lijun Wang,Huchuan Lu,Yangrui Shao,Chen Yang,Long Teng*

Main category: cs.CV

TL;DR: 论文提出“观察者中心”的显著性目标检测(OC-SOD)，用文本提示建数据集并给出一个“感知-反思-调整”的代理式基线，在新数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统显著性检测把问题当作客观单解分割任务，每张图只有一个GT，忽略不同观察者偏好/意图带来的主观性，导致问题欠定、与人类感知脱节。

Method: 1) 问题定义：将显著性预测拓展为结合视觉线索与观察者因素（偏好、意图等）的个性化任务；2) 数据：利用多模态大模型构建标注流水线，形成OC-SODBench，包含约3.3万图像、15.2万文本提示与对象对；3) 模型：设计OC-SODAgent，按“Perceive-Reflect-Adjust”人类式流程进行OC-SOD，结合多模态输入与迭代调整。

Result: 在OC-SODBench上做了大量实验，OC-SODAgent与相关方法比较，显示该观察者中心建模与数据集能更好地匹配人类主观多样性并提升个性化显著性预测性能。

Conclusion: 从观察者角度重塑显著性检测，提供可个性化、情境感知的预测框架与首个对应数据集及基线，缩小人类感知与计算模型之间的鸿沟；代码与数据集已开源。

Abstract: Salient object detection is inherently a subjective problem, as observers with different priors may perceive different objects as salient. However, existing methods predominantly formulate it as an objective prediction task with a single groundtruth segmentation map for each image, which renders the problem under-determined and fundamentally ill-posed. To address this issue, we propose Observer-Centric Salient Object Detection (OC-SOD), where salient regions are predicted by considering not only the visual cues but also the observer-specific factors such as their preferences or intents. As a result, this formulation captures the intrinsic ambiguity and diversity of human perception, enabling personalized and context-aware saliency prediction. By leveraging multi-modal large language models, we develop an efficient data annotation pipeline and construct the first OC-SOD dataset named OC-SODBench, comprising 33k training, validation and test images with 152k textual prompts and object pairs. Built upon this new dataset, we further design OC-SODAgent, an agentic baseline which performs OC-SOD via a human-like "Perceive-Reflect-Adjust" process. Extensive experiments on our proposed OC-SODBench have justified the effectiveness of our contribution. Through this observer-centric perspective, we aim to bridge the gap between human perception and computational modeling, offering a more realistic and flexible understanding of what makes an object truly "salient." Code and dataset are publicly available at: https://github.com/Dustzx/OC_SOD

</details>


### [29] [POINTS-GUI-G: GUI-Grounding Journey](https://arxiv.org/abs/2602.06391)
*Zhongyin Zhao,Yuan Liu,Yikun Liu,Haicheng Wang,Le Tian,Xiao Zhou,Yangxiu You,Zilin Yu,Yang Yu,Jie Zhou*

Main category: cs.CV

TL;DR: 论文提出POINTS-GUI-G-8B，从弱基础模型出发，通过数据工程、训练策略改进与可验证奖励的强化学习，大幅提升GUI定位能力，在多项基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理需要精准GUI定位作为基础，但多依赖已具空间感知能力的强模型微调，限制了从弱模型起步掌握全流程与可扩展性。作者希望证明：从低基线出发，依靠系统性数据与训练方案，也能达成SOTA，并探索RL在感知密集型任务中的价值。

Method: 1) 数据工程：统一多源开源数据格式，进行增强、过滤与难度分级，构建高质量训练集；2) 训练策略：持续微调视觉编码器、保持训练与推理分辨率一致，提升感知精度；3) 强化学习：基于GUI定位任务的可验证、高准确度奖励信号进行RL优化，提升坐标/目标匹配精度。

Result: 模型POINTS-GUI-G-8B在多个基准上达SOTA：ScreenSpot-Pro 59.9，OSWorld-G 66.0，ScreenSpot-v2 95.7，UI-Vision 49.9。

Conclusion: 精心的数据工程、视觉编码器持续优化与分辨率一致性、以及配合可验证奖励的RL，可在从弱基线起步的情况下显著提升GUI grounding精度；RL不只强化推理，在感知密集任务中同样有效，GUI grounding为RL提供天然的高质量奖励。

Abstract: The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.

</details>


### [30] [TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction](https://arxiv.org/abs/2602.06400)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 提出TFusionOcc：一个基于对象的多传感器融合框架，利用t分布与T混合模型、可形变超二次体等更灵活几何基元，在nuScenes上达SOTA，并在nuScenes-C腐蚀场景下展现鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义占据预测多依赖体素或3D高斯作为中间表示，难以高效捕获驾驶场景中的细粒度几何细节；需要一种既高效又能精细建模物体形状的表示与融合框架。

Method: 提出TFusionOcc：1) 对象中心的多阶段多传感器（相机+激光雷达）融合；2) 采用Student’s t分布与T-Mixture Model对不确定性与长尾噪声进行建模与鲁棒估计；3) 使用更灵活的几何基元——带逆形变的可变形超二次体作为中间表示，增强对多样物体形状的拟合；4) 将上述表征回投至3D语义占据以得到体素级预测。

Result: 在nuScenes基准上取得SOTA性能；在nuScenes-C多种相机/激光雷达腐蚀（噪声、遮挡、失真等）设定下仍保持较强鲁棒性。

Conclusion: 面向自动驾驶的3D语义占据预测可以通过对象中心、鲁棒统计建模与灵活几何基元显著提升精度与鲁棒性，优于传统体素或3D高斯中间表示；代码将开源以便复现与拓展。

Abstract: 3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely heavily on 3D voxel volumes or a set of 3D Gaussians, hindering the model's ability to efficiently and effectively capture fine-grained geometric details in the 3D driving environment. This paper introduces TFusionOcc, a novel object-centric multi-sensor fusion framework for predicting 3D semantic occupancy. By leveraging multi-stage multi-sensor fusion, Student's t-distribution, and the T-Mixture model (TMM), together with more geometrically flexible primitives, such as the deformable superquadric (superquadric with inverse warp), the proposed method achieved state-of-the-art (SOTA) performance on the nuScenes benchmark. In addition, extensive experiments were conducted on the nuScenes-C dataset to demonstrate the robustness of the proposed method in different camera and lidar corruption scenarios. The code will be available at: https://github.com/DanielMing123/TFusionOcc

</details>


### [31] [MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing](https://arxiv.org/abs/2602.06402)
*Wenjie Wang,Wei Wu,Ying Liu,Yuan Zhao,Xiaole Lv,Liang Diao,Zengjian Fan,Wenfeng Xie,Ziling Lin,De Shi,Lin Huang,Kaihe Xu,Hong Li*

Main category: cs.CV

TL;DR: 提出MeDocVL，一个面向医疗文档查询解析的后训练视觉语言模型，通过标签精炼与噪声感知的混合后训练（SFT+RL）在含噪标注下仍能实现精确字段级抽取，在医疗发票基准上达SOTA，优于传统OCR与强VLM。


<details>
  <summary>Details</summary>
Motivation: 医疗文档OCR需要字段级精准匹配，但存在复杂版面、专业术语与标注噪声，通用OCR与VLM在此场景稳定性差。需要一种能在噪声监督下仍可靠抽取的专用方法。

Method: 构建MeDocVL：1) 训练驱动的标签精炼（Training-driven Label Refinement），从噪声标注中自动构造高质量监督；2) 噪声感知的混合后训练，将强化学习与有监督微调结合，使模型在查询驱动抽取任务下更稳健、精确。整体为VLM的后训练框架，面向字段级问答/解析。

Result: 在医疗发票基准上，MeDocVL稳定优于传统OCR与强基线VLM，在噪声监督条件下实现SOTA字段抽取准确度与鲁棒性。

Conclusion: 面向医疗文档解析的专用后训练VLM，通过标签精炼与噪声感知混合训练，可在噪声标注下实现精确字段级抽取，优于现有OCR与通用VLM，适合查询驱动的医疗票据解析。

Abstract: Medical document OCR is challenging due to complex layouts, domain-specific terminology, and noisy annotations, while requiring strict field-level exact matching. Existing OCR systems and general-purpose vision-language models often fail to reliably parse such documents. We propose MeDocVL, a post-trained vision-language model for query-driven medical document parsing. Our framework combines Training-driven Label Refinement to construct high-quality supervision from noisy annotations, with a Noise-aware Hybrid Post-training strategy that integrates reinforcement learning and supervised fine-tuning to achieve robust and precise extraction. Experiments on medical invoice benchmarks show that MeDocVL consistently outperforms conventional OCR systems and strong VLM baselines, achieving state-of-the-art performance under noisy supervision.

</details>


### [32] [A neuromorphic model of the insect visual system for natural image processing](https://arxiv.org/abs/2602.06405)
*Adam D. Hines,Karin Nordström,Andrew B. Barron*

Main category: cs.CV

TL;DR: 提出一种受昆虫视觉启发的自监督视觉模型，将稠密输入转为稀疏、判别性表征；以对比学习训练，并在花朵识别、自然图像与模拟定位任务中表现稳健，且提供ANN与SNN两种实现，优于简单下采样基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型多追求任务性能而忽视生物学可解释的处理通路；昆虫视觉系统以稀疏高效编码支持复杂行为，启发构建兼具生物合理性与可泛化能力的表征学习模型。

Method: 构建仿昆虫视觉管线，将稠密视觉流经生物启发的通路变为稀疏表征；采用完全自监督的对比学习目标训练，不依赖标注与特定下游分类器；提供人工神经网络（ANN）与脉冲神经网络（SNN）两种实现；在不同任务上以学得的通用表征进行评估。

Result: 模型产出稳定稀疏码，能区分外观相近输入；在花朵识别与自然图像基准上表现可靠；在模拟定位中显著优于简单图像下采样对比的基线，显示神经形态视觉通路的功能优势。

Conclusion: 该模型将生物学启发与自监督表征学习结合，提供可跨任务复用的稀疏计算框架与ANN/SNN双实现，推进昆虫计算建模并展示在多任务上的通用性与效率优势。

Abstract: Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.

</details>


### [33] [Point Virtual Transformer](https://arxiv.org/abs/2602.06406)
*Veerain Sood,Bnalin,Gaurav Pandey*

Main category: cs.CV

TL;DR: 提出PointViT：在稀疏远距点云中，选择性引入由RGB深度补全生成的虚拟点，与真实LiDAR点进行高效融合，并用Transformer在BEV上进行查询与上下文聚合，显著提升3D/BEV/2D检测精度。


<details>
  <summary>Details</summary>
Motivation: 远距离目标因点云稀疏、几何线索不足而难以检测；直接将所有由RGB深度补全生成的虚拟点并入会导致计算代价高且真实/虚拟信息难以有效融合。需要一种既高效又鲁棒的选择性融合与检测框架。

Method: 提出Point Virtual Transformer（PointViT）。核心做法：1) 从RGB深度补全得到虚拟点，并进行“选择性采样”；2) 设计多种融合策略：从早期点级融合到基于BEV的门控融合，并分析精度/效率权衡；3) 将融合后的点云体素化，用稀疏卷积编码为BEV特征；4) 在BEV上初始化少量高置信目标查询，通过Transformer式上下文聚合模块迭代细化，完成3D检测。

Result: 在KITTI基准（Car类）上取得：3D AP 91.16%、BEV AP 95.94%、2D AP 99.36%，显示相较传统仅LiDAR或无选择的虚拟点融合方法具备更高准确性与效率。

Conclusion: 选择性引入虚拟点并与LiDAR点在合适阶段融合，再结合BEV表示与Transformer查询细化，可有效缓解远距点云稀疏问题，在保持效率的同时显著提升检测性能。

Abstract: LiDAR-based 3D object detectors often struggle to detect far-field objects due to the sparsity of point clouds at long ranges, which limits the availability of reliable geometric cues. To address this, prior approaches augment LiDAR data with depth-completed virtual points derived from RGB images; however, directly incorporating all virtual points leads to increased computational cost and introduces challenges in effectively fusing real and virtual information. We present Point Virtual Transformer (PointViT), a transformer-based 3D object detection framework that jointly reasons over raw LiDAR points and selectively sampled virtual points. The framework examines multiple fusion strategies, ranging from early point-level fusion to BEV-based gated fusion, and analyses their trade-offs in terms of accuracy and efficiency. The fused point cloud is voxelized and encoded using sparse convolutions to form a BEV representation, from which a compact set of high-confidence object queries is initialised and refined through a transformer-based context aggregation module. Experiments on the KITTI benchmark report 91.16% 3D AP, 95.94% BEV AP, and 99.36% AP on the KITTI 2D detection benchmark for the Car class.

</details>


### [34] [Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors](https://arxiv.org/abs/2602.06419)
*Soham Pahari,Sandeep C. Kumain*

Main category: cs.CV

TL;DR: 提出SemGeo-AttentionNet：结合语义与几何的双流3D注意力模型，通过跨模态注意力与扩散语义先验，显著提升3D显著性预测与时序注视路径生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D显著性方法多依赖手工几何特征或缺乏语义意识的学习模型，无法解释人类为何注视语义上重要但几何上不显著的区域。需要一个同时建模自底向上几何显著性与自顶向下语义驱动的统一框架。

Method: 提出SemGeo-AttentionNet双流架构：1) 语义流：基于几何条件的多视角渲染与扩散模型获取语义先验；2) 几何流：点云Transformer提取几何特征；3) 非对称跨模态融合：以几何特征为query去检索语义内容，让底层几何显著性引导顶层语义检索；4) 扩展到时间维度：用强化学习生成扫描路径，首次在3D网格拓扑上建模并引入抑制返回（IOR）机制。

Result: 在SAL3D、NUS3D与3DVA数据集上取得显著性能提升（相较SOTA有大幅度改进，具体数值未给出），验证了方法在预测3D表面人类注意力与生成扫描路径上的有效性。

Conclusion: 以认知动机驱动的语义-几何解耦与跨模态融合是建模3D人类视觉注意的有效途径；引入扩散语义先验与RL扫描路径在尊重网格拓扑与IOR的同时提升了预测质量。

Abstract: Human visual attention on three-dimensional objects emerges from the interplay between bottom-up geometric processing and top-down semantic recognition. Existing 3D saliency methods rely on hand-crafted geometric features or learning-based approaches that lack semantic awareness, failing to explain why humans fixate on semantically meaningful but geometrically unremarkable regions. We introduce SemGeo-AttentionNet, a dual-stream architecture that explicitly formalizes this dichotomy through asymmetric cross-modal fusion, leveraging diffusion-based semantic priors from geometry-conditioned multi-view rendering and point cloud transformers for geometric processing. Cross-attention ensures geometric features query semantic content, enabling bottom-up distinctiveness to guide top-down retrieval. We extend our framework to temporal scanpath generation through reinforcement learning, introducing the first formulation respecting 3D mesh topology with inhibition-of-return dynamics. Evaluation on SAL3D, NUS3D and 3DVA datasets demonstrates substantial improvements, validating how cognitively motivated architectures effectively model human visual attention on three-dimensional surfaces.

</details>


### [35] [Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO](https://arxiv.org/abs/2602.06422)
*Yunze Tong,Mushui Liu,Canyu Zhao,Wanggui He,Shiyi Zhang,Hongwei Zhang,Peng Zhang,Jinlong Liu,Ju Huang,Jiamang Wang,Hao Jiang,Pipei Huang*

Main category: cs.CV

TL;DR: 提出TP-GRPO：在流匹配文本生成图像中，以步级增量奖励与“转折点”长程奖励缓解奖励稀疏与长程依赖问题，较传统GRPO更高效、稳定提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO将最终结果奖励均匀回溯到所有去噪步，无法区分每步的局部贡献；组内排序只在相同时刻比较，不建模轨迹内早期动作对后续状态的延迟影响，导致奖励利用低效、学习信号稀疏。

Method: 1) 将结果式奖励改为步级“增量奖励”，为每个去噪动作提供密集、与时步相关的纯净信号；2) 基于增量奖励符号翻转自动检测“转折点”，并为这些关键步分配聚合的长期奖励，显式建模其对后续奖励走向的延迟影响；算法无需额外超参、仅依赖符号变化，计算高效。

Result: 在多组文本到图像实验中，TP-GRPO较基线GRPO更充分利用奖励信号，带来一致性的生成质量提升（文中给出广泛实验验证与示例），并保持训练稳定与效率。

Conclusion: 通过步级增量奖励与转折点长程奖励，TP-GRPO有效缓解奖励稀疏并刻画轨迹内长程依赖，较传统GRPO更好地优化流匹配去噪策略，稳定提升文本生成图像表现。

Abstract: Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.

</details>


### [36] [POPL-KF: A Pose-Only Geometric Representation-Based Kalman Filter for Point-Line-Based Visual-Inertial Odometry](https://arxiv.org/abs/2602.06425)
*Aiping Wang,Zhaolong Yang,Shuwen Chen,Hai Zhang*

Main category: cs.CV

TL;DR: 提出POPL-KF：一种同时利用点与线的“仅姿态”几何表示的VIO卡尔曼滤波框架，消除特征坐标、实现即时更新，配合统一基帧选择与线特征质量筛选，在公开数据与实测中优于多种SOTA方法并保持实时性。


<details>
  <summary>Details</summary>
Motivation: 传统以点特征为主的VIO在纹理贫乏、动态光照等挑战场景性能下降；MSCKF 类方法受特征3D坐标线性化与量测延迟影响，定位精度受限，需要一种能降低线性化误差并提升鲁棒性的表示与估计框架。

Method: 1) 提出线特征的仅姿态几何表示；2) 将仅姿态表示推广到点与线，形成 POPL-KF 卡尔曼滤波器，在量测方程中显式剔除点/线的3D坐标，实现视觉量测的即时更新以减少线性化误差；3) 设计统一的点/线基帧选择策略，保证对相机位姿的最优约束；4) 提出基于图像网格分割与双向光流一致性的线特征滤波，提升线特征质量。

Result: 在公开数据集与真实场景实验中，POPL-KF 在精度上优于SOTA滤波法（OpenVINS、PO-KF）与优化法（PL-VINS、EPLF-VINS），同时维持实时运行。

Conclusion: 仅姿态几何表示结合统一基帧选择与线特征质量控制，可显著降低线性化误差并提升VIO在挑战场景的精度与鲁棒性；POPL-KF 在不牺牲实时性的前提下超越现有滤波与优化方法。

Abstract: Mainstream Visual-inertial odometry 
(VIO) systems rely on point features for motion estimation and localization. However, their performance degrades in challenging scenarios. Moreover, the localization accuracy of multi-state constraint Kalman filter (MSCKF)-based VIO systems suffers from linearization errors associated with feature 3D coordinates and delayed measurement updates. To improve the performance of VIO in challenging scenes, we first propose a pose-only geometric representation for line features. Building on this, we develop POPL-KF, a Kalman filter-based VIO system that employs a pose-only geometric representation for both point and line features. POPL-KF mitigates linearization errors by explicitly eliminating both point and line feature coordinates from the measurement equations, while enabling immediate update of visual measurements. We also design a unified base-frames selection algorithm for both point and line features to ensure optimal constraints on camera poses within the pose-only measurement model. To further improve line feature quality, a line feature filter based on image grid segmentation and bidirectional optical flow consistency is proposed. Our system is evaluated on public datasets and real-world experiments, demonstrating that POPL-KF outperforms the state-of-the-art (SOTA) filter-based methods (OpenVINS, PO-KF) and optimization-based methods (PL-VINS, EPLF-VINS), while maintaining real-time performance.

</details>


### [37] [Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters](https://arxiv.org/abs/2602.06427)
*Yuxiang Zhao,Yirong Yang,Yanqing Zhu,Yanfen Shen,Chiyu Wang,Zhining Gu,Pei Shi,Wei Guo,Mu Xu*

Main category: cs.CV

TL;DR: 提出一种无外部先验、从室外到室内的指令驱动具身导航任务与方法，并构建首个相应开源数据集；所提视觉中心框架利用图像提示进行决策，在成功率与路径效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法往往局限于纯室内或纯室外，且依赖精确坐标等强先验；室外方法虽能到达目标附近，但无法细粒度进入具体建筑入口，难以满足实际“户外到室内”无缝过渡的应用需求（如末端配送）。

Method: 提出“无先验的户外到室内、指令驱动具身导航”任务设定，只依赖第一人称视觉与指令；构建以视觉为核心的导航框架，以图像式提示（image-based prompts）驱动策略决策；同时发布首个对应数据集，并在数据生成中引入与轨迹条件相关的视频合成流水线，以丰富训练/评测样本。

Result: 在广泛实验中，相比最先进基线，所提方法在成功率和路径效率等关键指标上稳定领先。

Conclusion: 消除对外部高精度先验的依赖，通过视觉提示驱动的具身导航与新数据集，实现了更可靠的户外到室内过渡能力，提升了实际可部署性。

Abstract: Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.

</details>


### [38] [ChatUMM: Robust Context Tracking for Conversational Interleaved Generation](https://arxiv.org/abs/2602.06442)
*Wenxun Dai,Zhiyuan Zhao,Yule Zhong,Yiji Cheng,Jianwei Zhang,Linqing Wang,Shiyi Zhang,Yunlong Lin,Runze He,Fellix Song,Wayne Zhuang,Yong Liu,Haoji Zhang,Yansong Tang,Qinglin Lu,Chunyu Wang*

Main category: cs.CV

TL;DR: ChatUMM 将统一多模态模型从“单轮解题器”推进为支持多轮、交错文本-图像生成的对话式助手。通过多轮交错训练与系统化对话数据合成，显著提升上下文追踪与长程依赖处理能力，并在开放源统一模型中达到SOTA视觉理解与编辑表现，同时保持有竞争力的文生图质量。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型多局限于单轮交互，难以在连续对话中稳健地追踪上下文与跨轮依赖，无法胜任真实的多轮、多模态助手场景。

Method: 提出 ChatUMM：1）交错多轮训练策略，将序列化的文本-图像流建模为连续对话；2）三阶段对话式数据合成管线：a) 构造具备状态保持的基础多轮对话；b) 通过加入“干扰”回合与基于历史的查询重写，强制模型学习长程依赖解析；c) 合成自然交错的多模态响应（文本与图像交替）。

Result: 在视觉理解与指令引导的编辑基准上取得开源统一模型中的SOTA；在文本到图像任务上保持具有竞争力的保真度；在复杂多轮场景中展现更强鲁棒性与流畅、具上下文意识的对话能力。

Conclusion: 通过将训练与数据从单轮范式系统性扩展为多轮交错对话，ChatUMM 有效弥补 UMM 在连续交互中的短板，兼顾理解、编辑与生成，成为更实用的对话式多模态统一模型。

Abstract: Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor'' turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.

</details>


### [39] [What Is Wrong with Synthetic Data for Scene Text Recognition? A Strong Synthetic Engine with Diverse Simulations and Self-Evolution](https://arxiv.org/abs/2602.06450)
*Xingsong Ye,Yongkun Du,JiaXin Zhang,Chen Li,Jing LYU,Zhineng Chen*

Main category: cs.CV

TL;DR: 提出UnionST合成数据引擎与UnionST-S数据集，并配合自进化标注学习SEL，显著缩小合成与真实场景文本识别的域差距，部分场景甚至超越真实数据训练效果，同时用仅9%的真实标注即可取得有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本识别合成数据虽便宜可控，但与真实世界存在明显域差距，导致模型在复杂场景下效果不佳。主因在于主流渲染式合成集在语料、字体和版式多样性不足，难以覆盖“野外”复杂性与长尾分布。

Method: 1) 系统分析主流合成集的不足（语料、字体、版式多样性）。2) 构建UnionST数据引擎，面向“联合难例”覆盖：更丰富语料库、字体集合与复杂版式/布局，增强复杂场景仿真。3) 生成大规模合成数据集UnionST-S，专注困难场景模拟。4) 提出自进化学习（SEL）框架，以小量真实标注为起点，对未标注真实数据进行高质量伪标与迭代自提升，提升真实域适配。

Result: 使用UnionST-S训练的STR模型在多个基准上显著优于以往合成数据训练的模型，并在部分场景上超过仅用真实数据训练的性能。结合SEL，仅使用约9%的真实标注即可达到与全量标注相近或具有竞争力的效果。

Conclusion: 通过UnionST合成引擎与UnionST-S数据集提升合成数据的多样性与场景贴合度，并用SEL高效利用少量真实标注，显著缩小甚至弥合合成-真实域差距，为低成本高性能的场景文本识别提供可行方案。

Abstract: Large-scale and categorical-balanced text data is essential for training effective Scene Text Recognition (STR) models, which is hard to achieve when collecting real data. Synthetic data offers a cost-effective and perfectly labeled alternative. However, its performance often lags behind, revealing a significant domain gap between real and current synthetic data. In this work, we systematically analyze mainstream rendering-based synthetic datasets and identify their key limitations: insufficient diversity in corpus, font, and layout, which restricts their realism in complex scenarios. To address these issues, we introduce UnionST, a strong data engine synthesizes text covering a union of challenging samples and better aligns with the complexity observed in the wild. We then construct UnionST-S, a large-scale synthetic dataset with improved simulations in challenging scenarios. Furthermore, we develop a self-evolution learning (SEL) framework for effective real data annotation. Experiments show that models trained on UnionST-S achieve significant improvements over existing synthetic datasets. They even surpass real-data performance in certain scenarios. Moreover, when using SEL, the trained models achieve competitive performance by only seeing 9% of real data labels.

</details>


### [40] [Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection](https://arxiv.org/abs/2602.06452)
*Hongyan Fei,Zexi Jia,Chuanwei Huang,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: 提出SRI-Net，利用镜面反射与纹理/直射光的一致性来检测高质量（含扩散模型）伪造人脸，跨数据集效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型等生成方法能合成超高质量、人眼难辨的伪造人脸，传统依赖空间/频域线索的方法在“全合成”人脸上效果衰减。作者观察到由复杂物理规律控制的面部属性（如光照中的镜面反射）难以被精确复现，因而可能留下可检线索。

Method: 1) 以Retinex为基础的快速精确人脸纹理估计，用于分离镜面反射分量；2) 基于Phong模型分析镜面反射与人脸纹理、直射光的数学关系，挖掘不一致性；3) 设计SRI-Net，采用两阶段交叉注意力，建模镜面反射—纹理—直射光之间的相关性，并与原始图像特征融合，实现鲁棒伪造检测。

Result: 在传统deepfake与生成式deepfake数据集（尤其包含扩散模型合成的人脸）上取得优于现有方法的检测性能，表现出更强的跨数据集与高质量伪造鲁棒性。

Conclusion: 物理可解释的镜面反射一致性是检测高质量全合成伪造的有效线索；结合Retinex分解与交叉注意力的SRI-Net在多类数据集上验证了该思路的有效性与泛化性。

Abstract: Detecting deepfakes has become increasingly challenging as forgery faces synthesized by AI-generated methods, particularly diffusion models, achieve unprecedented quality and resolution. Existing forgery detection approaches relying on spatial and frequency features demonstrate limited efficacy against high-quality, entirely synthesized forgeries. In this paper, we propose a novel detection method grounded in the observation that facial attributes governed by complex physical laws and multiple parameters are inherently difficult to replicate. Specifically, we focus on illumination, particularly the specular reflection component in the Phong illumination model, which poses the greatest replication challenge due to its parametric complexity and nonlinear formulation. We introduce a fast and accurate face texture estimation method based on Retinex theory to enable precise specular reflection separation. Furthermore, drawing from the mathematical formulation of specular reflection, we posit that forgery evidence manifests not only in the specular reflection itself but also in its relationship with corresponding face texture and direct light. To address this issue, we design the Specular-Reflection-Inconsistency-Network (SRI-Net), incorporating a two-stage cross-attention mechanism to capture these correlations and integrate specular reflection related features with image features for robust forgery detection. Experimental results demonstrate that our method achieves superior performance on both traditional deepfake datasets and generative deepfake datasets, particularly those containing diffusion-generated forgery faces.

</details>


### [41] [LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection](https://arxiv.org/abs/2602.06474)
*Xu Zhang,Zhe Chen,Jing Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出LAB-Det：在不更新权重、每类仅一张标注样本的前提下，使冻结的基础检测器在专业小样本域（如水下、工业缺陷）实现适应，通过语言作为域不变桥接实现训练-free的一次样本检测域泛化，并在UODD与NEU-DET上最高提升mAP 5.4。


<details>
  <summary>Details</summary>
Motivation: 现有基础检测器（GLIP、Grounding DINO）在通用域表现强，但在专业、数据稀缺场景易退化。传统跨域few-shot依赖微调，成本高且易过拟合。作者追问：能否在不训练、每类仅一例的条件下让检测器适应新域？

Method: 提出Training-free One-shot Domain Generalization for Detection设定；方法LAB-Det：保持检测器冻结，将每个类别的单个视觉样本转写为描述性文本，用语言提示对检测器进行条件化与引导，替代梯度更新的适配；语言被视为域不变桥梁，从而提升在目标域的鲁棒性与可解释性。

Result: 在UODD（水下）与NEU-DET（工业缺陷）两个数据稀缺、边界模糊的基准上评测，LAB-Det在无需更新任何参数的情况下，相比最先进微调基线最高提升mAP 5.4。

Conclusion: 语言条件化可在无训练的一次样本设定下有效适配专业检测域，成为替代微调的高效、可解释方案，推动检测的跨域泛化。

Abstract: Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.

</details>


### [42] [Efficient-LVSM: Faster, Cheaper, and Better Large View Synthesis Model via Decoupled Co-Refinement Attention](https://arxiv.org/abs/2602.06478)
*Xiaosong Jia,Yihang Sun,Junqi You,Songbur Wong,Zichen Zou,Junchi Yan,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出Efficient-LVSM：将输入视图与目标视图的注意力解耦，替代LVSM的全量自注意。通过输入视图内自注意、目标视图自注意后再跨注意的双流协同，降低复杂度并提升效率与性能。在RealEstate10K上2视图输入达29.86 dB，较LVSM+0.2 dB，训练收敛2倍快、推理4.4倍快，并具备零样本泛化与KV-cache增量推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的NVS模型（如LVSM）对所有输入与目标视图做全量自注意，带来：(1) 随输入视图数二次增长的计算与显存开销；(2) 将异质token（输入与目标）硬共享参数，限制表达能力与效率。因此需要一种既降低复杂度又更贴合任务结构的注意力机制。

Method: 提出双流（dual-stream）与解耦共精炼（co-refinement）：对输入视图仅做视图内自注意（intra-view self-attn）；对目标视图先自注意再与输入视图做跨注意（self-then-cross），避免不必要的输入-输入、目标-输入全连接；并利用解耦设计实现KV-cache以支持增量推理。

Result: 在RealEstate10K两输入视图设置下，PSNR 29.86 dB，较LVSM提升0.2 dB；训练收敛速度提升约2倍，推理速度提升约4.4倍；在多基准上达SOTA，并对未见过的视图数量表现出强零样本泛化能力。

Conclusion: 解耦的双流注意力替代全量自注意可在NVS中同时提升精度与效率；结构更契合任务，支持KV-cache带来的增量推理，并在多数据集上取得SOTA与良好泛化。

Abstract: Feedforward models for novel view synthesis (NVS) have recently advanced by transformer-based methods like LVSM, using attention among all input and target views. In this work, we argue that its full self-attention design is suboptimal, suffering from quadratic complexity with respect to the number of input views and rigid parameter sharing among heterogeneous tokens. We propose Efficient-LVSM, a dual-stream architecture that avoids these issues with a decoupled co-refinement mechanism. It applies intra-view self-attention for input views and self-then-cross attention for target views, eliminating unnecessary computation. Efficient-LVSM achieves 29.86 dB PSNR on RealEstate10K with 2 input views, surpassing LVSM by 0.2 dB, with 2x faster training convergence and 4.4x faster inference speed. Efficient-LVSM achieves state-of-the-art performance on multiple benchmarks, exhibits strong zero-shot generalization to unseen view counts, and enables incremental inference with KV-cache, thanks to its decoupled designs.

</details>


### [43] [Instance-Free Domain Adaptive Object Detection](https://arxiv.org/abs/2602.06484)
*Hengfu Yu,Jinhong Deng,Lixin Duan,Wen Li*

Main category: cs.CV

TL;DR: 提出“无实例域自适应目标检测（Instance-Free DAOD）”问题：仅有目标域背景数据、无前景实例。方法RSCN基于背景原型对齐并保持前景-背景关系一致性，跨三类基准显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中获取含目标实例的目标域数据昂贵或稀缺（如野生动物监测、病灶检测），而纯背景数据充足。传统DAOD假设目标域含足够实例，不适用于此情景，需解决仅依赖目标域背景进行对齐的适配难题。

Method: 提出RSCN：1) 提取并聚合目标域背景特征，构建背景原型；2) 以这些原型为锚进行跨域特征对齐；3) 在源域与目标域内分别约束前景与背景特征间的关系与结构一致性（关系/结构一致性损失），使模型即便无目标域前景也能稳健迁移。

Result: 在三个新策划的基准（仿真自动驾驶、野生动物、肺结节）上，无实例场景下RSCN显著优于现有DAOD方法，表现更稳健。

Conclusion: 通过背景原型对齐与前景-背景关系结构一致性约束，可在缺乏目标域实例时实现有效DAOD；RSCN为该新问题提供强基线与新基准。

Abstract: While Domain Adaptive Object Detection (DAOD) has made significant strides, most methods rely on unlabeled target data that is assumed to contain sufficient foreground instances. However, in many practical scenarios (e.g., wildlife monitoring, lesion detection), collecting target domain data with objects of interest is prohibitively costly, whereas background-only data is abundant. This common practical constraint introduces a significant technical challenge: the difficulty of achieving domain alignment when target instances are unavailable, forcing adaptation to rely solely on the target background information. We formulate this challenge as the novel problem of Instance-Free Domain Adaptive Object Detection. To tackle this, we propose the Relational and Structural Consistency Network (RSCN) which pioneers an alignment strategy based on background feature prototypes while simultaneously encouraging consistency in the relationship between the source foreground features and the background features within each domain, enabling robust adaptation even without target instances. To facilitate research, we further curate three specialized benchmarks, including simulative auto-driving detection, wildlife detection, and lung nodule detection. Extensive experiments show that RSCN significantly outperforms existing DAOD methods across all three benchmarks in the instance-free scenario. The code and benchmarks will be released soon.

</details>


### [44] [Rebenchmarking Unsupervised Monocular 3D Occupancy Prediction](https://arxiv.org/abs/2602.06488)
*Zizhan Guo,Yi Feng,Mengtan Zhang,Haoran Zhang,Wei Ye,Rui Fan*

Main category: cs.CV

TL;DR: 论文提出无监督单目3D占用预测的新基准与方法：重新界定体渲染变量以获得物理一致的占用概率表示，校正评测协议以与体素级3D真值对齐，并引入考虑遮挡的“极化”机制融合多视角线索，显著优于现有无监督方法并媲美监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法常用NeRF训练、评测时将其输出当作占用概率，导致训练-评测不一致；且主流仅用2D真值监督，无法约束被遮挡区域的几何而产生歧义。需要一个物理一致的占用表示、与3D真值对齐的评测协议，以及能在遮挡区施加约束的机制。

Method: 1) 从体渲染公式出发，解析体密度、透明度、传输率等变量，确定最物理一致的占用概率定义；2) 以此表示为桥梁，将无监督方法的输出与体素级3D占用真值对齐，重构评测协议，使无监督方法可与监督方法同标尺比较；3) 提出遮挡感知的“极化”机制，融合多视角视觉线索，提高遮挡区对占用/自由空间的判别力。

Result: 在多个基准上进行广泛实验，新协议下所提方法在无监督设置显著超越现有方法，并在性能上接近甚至匹配监督方法。

Conclusion: 通过物理一致的占用概率定义与与3D真值对齐的评测协议，结合遮挡感知多视角约束，无监督单目3D占用预测的性能与公平性均得到提升，缩小了与监督方法的差距；代码与评测协议将开源。

Abstract: Inferring the 3D structure from a single image, particularly in occluded regions, remains a fundamental yet unsolved challenge in vision-centric autonomous driving. Existing unsupervised approaches typically train a neural radiance field and treat the network outputs as occupancy probabilities during evaluation, overlooking the inconsistency between training and evaluation protocols. Moreover, the prevalent use of 2D ground truth fails to reveal the inherent ambiguity in occluded regions caused by insufficient geometric constraints. To address these issues, this paper presents a reformulated benchmark for unsupervised monocular 3D occupancy prediction. We first interpret the variables involved in the volume rendering process and identify the most physically consistent representation of the occupancy probability. Building on these analyses, we improve existing evaluation protocols by aligning the newly identified representation with voxel-wise 3D occupancy ground truth, thereby enabling unsupervised methods to be evaluated in a manner consistent with that of supervised approaches. Additionally, to impose explicit constraints in occluded regions, we introduce an occlusion-aware polarization mechanism that incorporates multi-view visual cues to enhance discrimination between occupied and free spaces in these regions. Extensive experiments demonstrate that our approach not only significantly outperforms existing unsupervised approaches but also matches the performance of supervised ones. Our source code and evaluation protocol will be made available upon publication.

</details>


### [45] [DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation](https://arxiv.org/abs/2602.06494)
*Lulu Chen,Yijiang Hu,Yuanqing Liu,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: 提出DreamHome-Pano：一种可控的全景室内生成框架，通过语义桥Prompt-LLM与“无冲突控制”架构，在满足结构布局约束的同时实现风格一致的高保真渲染，并通过SFT+RL训练在美学与结构一致性上均优。


<details>
  <summary>Details</summary>
Motivation: 多条件生成（布局约束+风格偏好）常出现“条件冲突”，风格控制会破坏几何与结构精度。现有方法缺乏对结构的显式保护与跨模态语义对齐能力，需要一种既能精准遵循建筑结构又能表达风格的可控生成方案。

Method: 1) Prompt-LLM：将布局约束与风格参考转译为专业描述性提示，提升跨模态对齐与可控性；2) 冲突无关控制架构：引入结构感知几何先验与多条件解耦策略，抑制风格信息对空间布局的侵蚀；3) 构建全景室内基准与多阶段训练流水线，采用渐进式SFT与RL以强化结构一致性与审美质量。

Result: 在全景室内可控生成任务上，实现更好的美学质量与结构一致性平衡；实验表明较现有多条件生成方法具有显著优势，呈现专业级可视化效果。

Conclusion: DreamHome-Pano在多条件室内全景生成中有效化解风格与结构的冲突，通过Prompt-LLM与冲突无关控制架构实现高保真、结构守恒的可控生成，并配合SFT+RL训练与新基准，提供面向实务的稳健解决方案。

Abstract: In modern interior design, the generation of personalized spaces frequently necessitates a delicate balance between rigid architectural structural constraints and specific stylistic preferences. However, existing multi-condition generative frameworks often struggle to harmonize these inputs, leading to "condition conflicts" where stylistic attributes inadvertently compromise the geometric precision of the layout. To address this challenge, we present DreamHome-Pano, a controllable panoramic generation framework designed for high-fidelity interior synthesis. Our approach introduces a Prompt-LLM that serves as a semantic bridge, effectively translating layout constraints and style references into professional descriptive prompts to achieve precise cross-modal alignment. To safeguard architectural integrity during the generative process, we develop a Conflict-Free Control architecture that incorporates structural-aware geometric priors and a multi-condition decoupling strategy, effectively suppressing stylistic interference from eroding the spatial layout. Furthermore, we establish a comprehensive panoramic interior benchmark alongside a multi-stage training pipeline, encompassing progressive Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Experimental results demonstrate that DreamHome-Pano achieves a superior balance between aesthetic quality and structural consistency, offering a robust and professional-grade solution for panoramic interior visualization.

</details>


### [46] [Forest canopy height estimation from satellite RGB imagery using large-scale airborne LiDAR-derived training data and monocular depth estimation](https://arxiv.org/abs/2602.06503)
*Yongkang Lai,Xihan Mu,Tim R. McVicar,Dasheng Fan,Donghui Xie,Shanxin Guo,Wenli Huang,Tianjie Zhao,Guangjian Yan*

Main category: cs.CV

TL;DR: 利用公开机载LiDAR派生的树冠高程模型（CHM）和PlanetScope/RGB影像训练单目深度估计网络（Depth Anything V2），提出Depth2CHM，可直接从卫星RGB生成连续高分辨率森林树冠高度；在中美独立验证中达到米级偏差和数米RMSE，优于现有全球米级CHM产品。


<details>
  <summary>Details</summary>
Motivation: 现有星载LiDAR（ICESat‑2、GEDI）虽具全球覆盖与结构信息，但采样稀疏且不确定性大；近地面（机载/UAV）LiDAR虽精细但覆盖有限。需要一种既具高分辨率、空间连续性又可规模化的树冠高度制图方法，以更好支撑区域/全球碳水循环研究。

Method: 收集多国约16,000 km²机载LiDAR点云及其派生CHM，与3 m PlanetScope和机载RGB配准；以最先进单目深度估计模型Depth Anything V2为基础进行训练，得到Depth2CHM，实现从RGB直接预测连续CHM；在中国（~1 km²）与美国（~116 km²）独立站点进行精度验证，并与现有全球米级CHM产品对比。

Result: 独立验证偏差分别为0.59 m（中）与0.41 m（美），RMSE分别为2.54 m与5.75 m；相较现有全球米级CHM产品，平均绝对误差降低约1.5 m，RMSE降低约2 m，显示显著性能提升。

Conclusion: 以大规模机载LiDAR‑CHM监督训练的单目深度网络能够从卫星RGB生成高分辨率、空间连续的森林树冠高度，具备可扩展性和现实可用性，为高分辨率全球树冠高度制图提供了有前景的路径。

Abstract: Large-scale, high-resolution forest canopy height mapping plays a crucial role in understanding regional and global carbon and water cycles. Spaceborne LiDAR missions, including the Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) and the Global Ecosystem Dynamics Investigation (GEDI), provide global observations of forest structure but are spatially sparse and subject to inherent uncertainties. In contrast, near-surface LiDAR platforms, such as airborne and unmanned aerial vehicle (UAV) LiDAR systems, offer much finer measurements of forest canopy structure, and a growing number of countries have made these datasets openly available. In this study, a state-of-the-art monocular depth estimation model, Depth Anything V2, was trained using approximately 16,000 km2 of canopy height models (CHMs) derived from publicly available airborne LiDAR point clouds and related products across multiple countries, together with 3 m resolution PlanetScope and airborne RGB imagery. The trained model, referred to as Depth2CHM, enables the estimation of spatially continuous CHMs directly from PlanetScope RGB imagery. Independent validation was conducted at sites in China (approximately 1 km2) and the United States (approximately 116 km2). The results showed that Depth2CHM could accurately estimate canopy height, with biases of 0.59 m and 0.41 m and root mean square errors (RMSEs) of 2.54 m and 5.75 m for these two sites, respectively. Compared with an existing global meter-resolution CHM product, the mean absolute error is reduced by approximately 1.5 m and the RMSE by approximately 2 m. These results demonstrated that monocular depth estimation networks trained with large-scale airborne LiDAR-derived canopy height data provide a promising and scalable pathway for high-resolution, spatially continuous forest canopy height estimation from satellite RGB imagery.

</details>


### [47] [FloorplanVLM: A Vision-Language Model for Floorplan Vectorization](https://arxiv.org/abs/2602.06507)
*Yuanqing Liu,Ziming Yang,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: 提出FloorplanVLM：将栅格户型图直接转为结构化JSON序列，实现全局拓扑一致的工程级矢量化；借助大规模数据集与分阶段训练（SFT+GRPO），在新基准FPBench-2K上取得高外墙IoU（92.52%）与对非曼哈顿结构的强泛化。


<details>
  <summary>Details</summary>
Motivation: 传统像素/启发式与查询式Transformer方案在复杂拓扑（斜墙、圆弧）与严苛几何约束下易碎片化或不一致，难以获得工程级（拓扑完整、参数精确）的矢量图。需要一种能全局表达拓扑并严格满足几何约束的统一方法与相配套的数据与评测。

Method: 将问题重述为“图像条件的序列建模”：直接从图像输出结构化JSON，显式编码全局拓扑与几何元素（墙、门窗、弧线等）。为支撑数据需求，构建Floorplan-2M与高保真子集Floorplan-HQ-300K。训练上采用渐进策略：先SFT进行结构锚定与质量退火，再用GRPO进行几何对齐强化；同时建立标准化评测集FPBench-2K。

Result: 在FPBench-2K上取得92.52%的外墙IoU，展示高结构有效性与对非曼哈顿（斜向、曲线）布局的稳健泛化；输出为全局一致的JSON序列，减少房间碎片化与几何不一致。

Conclusion: “像素到序列”的统一框架结合大规模数据与强化式对齐，可实现对复杂户型的精确、整体矢量化。该方向为工程级CAD/建筑解析提供可扩展范式与标准评测基础。

Abstract: Converting raster floorplans into engineering-grade vector graphics is challenging due to complex topology and strict geometric constraints. To address this, we present FloorplanVLM, a unified framework that reformulates floorplan vectorization as an image-conditioned sequence modeling task. Unlike pixel-based methods that rely on fragile heuristics or query-based transformers that generate fragmented rooms, our model directly outputs structured JSON sequences representing the global topology. This 'pixels-to-sequence' paradigm enables the precise and holistic constraint satisfaction of complex geometries, such as slanted walls and curved arcs. To support this data-hungry approach, we introduce a scalable data engine: we construct a large-scale dataset (Floorplan-2M) and a high-fidelity subset (Floorplan-HQ-300K) to balance geometric diversity and pixel-level precision. We then employ a progressive training strategy, using Supervised Fine-Tuning (SFT) for structural grounding and quality annealing, followed by Group Relative Policy Optimization (GRPO) for strict geometric alignment. To standardize evaluation on complex layouts, we establish and open-source FPBench-2K. Evaluated on this rigorous benchmark, FloorplanVLM demonstrates exceptional structural validity, achieving $\textbf{92.52%}$ external-wall IoU and robust generalization across non-Manhattan architectures.

</details>


### [48] [DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving](https://arxiv.org/abs/2602.06521)
*Feiyang jia,Lin Liu,Ziying Song,Caiyan Jia,Hangjun Ye,Xiaoshuai Hao,Long Chen*

Main category: cs.CV

TL;DR: DriveWorld-VLA 将世界模型与视觉-语言-行动规划在同一潜在空间深度融合，使规划器直接利用潜在状态进行可控、动作条件的想象，从而减少标注需求并显著提升端到端自动驾驶表现。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶尝试结合VLA与世界模型，但两者在架构层面割裂，潜在状态共享不足，导致未来场景演化的“想象”难以有效影响动作决策，且需要大量密集标注监督。

Method: 提出DriveWorld-VLA：1) 在潜在空间统一世界建模与VLA规划，进行表示级别耦合；2) 将世界模型产生的潜在状态作为VLA核心决策状态，评估候选动作对未来场景的影响；3) 全部在特征层面进行动作条件的可控想象，避免像素级rollout的高成本与噪声；4) 降低对密集标注的依赖。

Result: 在开环与闭环评测中达到SOTA：NAVSIMv1 PDMS 91.3、NAVSIMv2 EPDMS 86.8、nuScenes 3秒平均碰撞率0.16。

Conclusion: 潜在空间级的紧密融合使VLA规划器直接受益于整体场景演化建模，带来更高效、更稳健的决策与更低标注成本；证明了统一世界模型与规划的端到端框架在自动驾驶中的有效性。

Abstract: End-to-end (E2E) autonomous driving has recently attracted increasing interest in unifying Vision-Language-Action (VLA) with World Models to enhance decision-making and forward-looking imagination. However, existing methods fail to effectively unify future scene evolution and action planning within a single architecture due to inadequate sharing of latent states, limiting the impact of visual imagination on action decisions. To address this limitation, we propose DriveWorld-VLA, a novel framework that unifies world modeling and planning within a latent space by tightly integrating VLA and world models at the representation level, which enables the VLA planner to benefit directly from holistic scene-evolution modeling and reducing reliance on dense annotated supervision. Additionally, DriveWorld-VLA incorporates the latent states of the world model as core decision-making states for the VLA planner, facilitating the planner to assess how candidate actions impact future scene evolution. By conducting world modeling entirely in the latent space, DriveWorld-VLA supports controllable, action-conditioned imagination at the feature level, avoiding expensive pixel-level rollouts. Extensive open-loop and closed-loop evaluations demonstrate the effectiveness of DriveWorld-VLA, which achieves state-of-the-art performance with 91.3 PDMS on NAVSIMv1, 86.8 EPDMS on NAVSIMv2, and 0.16 3-second average collision rate on nuScenes. Code and models will be released in https://github.com/liulin815/DriveWorld-VLA.git.

</details>


### [49] [MicroBi-ConvLSTM: An Ultra-Lightweight Efficient Model for Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.06523)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: 提出MicroBi-ConvLSTM，一种超轻量级HAR模型，约11.4K参数，采用两阶段卷积+4倍时间池化+单层双向LSTM，在8个基准上保持与更大模型相近的性能；量化到INT8后平均F1仅降0.21%，部署占用约23KB，适合SRAM受限的可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级HAR模型（如TinierHAR 34K、TinyHAR 55K）虽准确但在微控制器上因SRAM与OS开销常超预算；需要进一步缩小参数与内存占用，同时维持接近的识别精度与O(N)时间复杂度，以便在极端资源受限的可穿戴端部署。

Method: 设计MicroBi-ConvLSTM：1) 两阶段卷积特征提取并进行4×时间下采样以减少时序长度； 2) 仅使用一层双向LSTM做时序建模； 3) 结构整体线性时间复杂度； 4) 采用后训练INT8量化以压缩模型与推理内存。进行跨8个HAR数据集评测与消融，分析各组件贡献与任务依赖性。

Result: 平均参数量约11.4K，比TinierHAR降2.9×、比DeepConvLSTM降11.9×；在UCI-HAR达93.41%宏F1，SKODA 94.46%，Daphnet 88.98%。双向性对“情节性事件检测”更有益，对周期性步态收益有限。INT8后训练量化平均F1仅降0.21%，部署占用约23.0KB，满足内存受限边缘设备需求。

Conclusion: MicroBi-ConvLSTM在超轻量级范畴实现显著参数/内存压缩且保持竞争性准确度，适用于SRAM紧张的可穿戴端。双向LSTM的效益与任务类型相关；量化几乎不伤精度，支持小内存微控制器的实际落地。

Abstract: Human Activity Recognition (HAR) on resource constrained wearables requires models that balance accuracy against strict memory and computational budgets. State of the art lightweight architectures such as TinierHAR (34K parameters) and TinyHAR (55K parameters) achieve strong accuracy, but exceed memory budgets of microcontrollers with limited SRAM once operating system overhead is considered. We present MicroBi-ConvLSTM, an ultra-lightweight convolutional-recurrent architecture achieving 11.4K parameters on average through two stage convolutional feature extraction with 4x temporal pooling and a single bidirectional LSTM layer. This represents 2.9x parameter reduction versus TinierHAR and 11.9x versus DeepConvLSTM while preserving linear O(N) complexity. Evaluation across eight diverse HAR benchmarks shows that MicroBi-ConvLSTM maintains competitive performance within the ultra-lightweight regime: 93.41% macro F1 on UCI-HAR, 94.46% on SKODA assembly gestures, and 88.98% on Daphnet gait freeze detection. Systematic ablation reveals task dependent component contributions where bidirectionality benefits episodic event detection, but provides marginal gains on periodic locomotion. INT8 post training quantization incurs only 0.21% average F1-score degradation, yielding a 23.0 KB average deployment footprint suitable for memory constrained edge devices.

</details>


### [50] [AdaptOVCD: Training-Free Open-Vocabulary Remote Sensing Change Detection via Adaptive Information Fusion](https://arxiv.org/abs/2602.06529)
*Mingyu Dou,Shi Qiu,Ming Hu,Yifan Chen,Huping Ye,Xiaohan Liao,Zhe Sun*

Main category: cs.CV

TL;DR: 提出AdaptOVCD：一种无需训练的开放词汇遥感变化检测框架，通过数据/特征/决策三级垂直融合与横向自适应设计协同，整合SAM-HQ、DINOv3、DGTRS-CLIP等异构预训练模型，在零样本任意类别变化检测上显著优于现有免训练方法，并在跨数据集达到全监督上限的84.89%。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法依赖预定义类别与大量像素级标注，难以适配开放世界场景且泛化性弱，需要一种无需再训练、可检测任意类别变化且具备强泛化的方案。

Method: 提出AdaptOVCD，采用双维（垂直+水平）多层信息融合：1）数据级：自适应辐射校准ARA，将辐射统计与原始纹理融合，并与SAM-HQ协同获得辐射一致的分割；2）特征级：自适应变化阈值ACT，融合全局差异分布与边缘结构先验，借助DINOv3实现稳健变化检测；3）决策级：自适应置信过滤ACF，结合语义置信与空间约束，与DGTRS-CLIP协同输出高置信语义。整体通过异构大模型深度协同，缓解误差传播。

Result: 在9个场景上，零样本检测任意类别变化优于现有免训练方法；跨数据集实验达到全监督上限的84.89%，展现更强的泛化能力。

Conclusion: 训练free的开放词汇变化检测可通过多级融合与自适应机制有效实现；异构预训练模型的深度协同能显著提升零样本与跨域泛化表现。

Abstract: Remote sensing change detection plays a pivotal role in domains such as environmental monitoring, urban planning, and disaster assessment. However, existing methods typically rely on predefined categories and large-scale pixel-level annotations, which limit their generalization and applicability in open-world scenarios. To address these limitations, this paper proposes AdaptOVCD, a training-free Open-Vocabulary Change Detection (OVCD) architecture based on dual-dimensional multi-level information fusion. The framework integrates multi-level information fusion across data, feature, and decision levels vertically while incorporating targeted adaptive designs horizontally, achieving deep synergy among heterogeneous pre-trained models to effectively mitigate error propagation. Specifically, (1) at the data level, Adaptive Radiometric Alignment (ARA) fuses radiometric statistics with original texture features and synergizes with SAM-HQ to achieve radiometrically consistent segmentation; (2) at the feature level, Adaptive Change Thresholding (ACT) combines global difference distributions with edge structure priors and leverages DINOv3 to achieve robust change detection; (3) at the decision level, Adaptive Confidence Filtering (ACF) integrates semantic confidence with spatial constraints and collaborates with DGTRS-CLIP to achieve high-confidence semantic identification. Comprehensive evaluations across nine scenarios demonstrate that AdaptOVCD detects arbitrary category changes in a zero-shot manner, significantly outperforming existing training-free methods. Meanwhile, it achieves 84.89\% of the fully-supervised performance upper bound in cross-dataset evaluations and exhibits superior generalization capabilities. The code is available at https://github.com/Dmygithub/AdaptOVCD.

</details>


### [51] [Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance](https://arxiv.org/abs/2602.06530)
*Haipeng Li,Rongxuan Peng,Anwei Luo,Shunquan Tan,Changsheng Chen,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 提出ForgeryEraser：在无需目标检测器访问的情况下，对AIGC真伪检测实施通用反取证攻击，通过多模态引导将伪造图像在VLM特征空间拉向“真实”文本锚点并远离“伪造”锚点，从而显著降低多种先进检测器在全局合成与局部编辑任务上的性能，并误导可解释模型给出“真实般”的解释。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC真伪检测评测忽视反取证攻击，导致在真实场景中的鲁棒性与安全性存疑；许多检测器共享VLM（如CLIP）特征空间，可能存在系统性对抗脆弱性，值得系统挖掘与利用以评估与提升防御。

Method: 不依赖目标检测器参数/梯度，利用公开VLM作为共享骨干；构造多模态引导损失：使伪造图像嵌入向“真实”文本锚点靠拢、远离“伪造”文本锚点，以擦除伪造痕迹；以此生成对抗性修改的伪造图像，实现通用、黑盒的反取证攻击；在全局合成与局部编辑数据集上系统评测，并检验对可解释取证模型的影响。

Result: ForgeryEraser在多种最先进AIGC检测器上造成显著性能下降，覆盖全局合成与局部编辑基准；还能诱导可解释取证模型对伪造图像给出与真实图像一致的解释。

Conclusion: 基于VLM共享特征空间存在可迁移的对抗脆弱性；多模态引导的黑盒反取证可有效抹除伪造痕迹，动摇现有AIGC检测的稳健性；应当发展超越共享骨干依赖、具备更强防对抗与多模态鲁棒性的检测与防御方案。

Abstract: The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forgery anchors. Extensive experiments demonstrate that ForgeryEraser causes substantial performance degradation to advanced AIGC detectors on both global synthesis and local editing benchmarks. Moreover, ForgeryEraser induces explainable forensic models to generate explanations consistent with authentic images for forged images. Our code will be made publicly available.

</details>


### [52] [NECromancer: Breathing Life into Skeletons via BVH Animation](https://arxiv.org/abs/2602.06548)
*Mingxi Xu,Qi Wang,Zhengyu Wen,Phong Dao Thien,Zhengyu Li,Ning Zhang,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 提出NECromancer（NEC），一种可在任意BVH骨架上工作的通用动作tokenizer，通过结构感知编码与拓扑无关量化，将不同物种/形态的动作压缩为统一离散tokens，实现跨骨架高保真重建与多任务生成/检索。


<details>
  <summary>Details</summary>
Motivation: 现有动作tokenization方法多依赖特定物种/骨架（如人形），难以在异构骨架间泛化与迁移，限制了跨物种动作分析、合成与共享的能力。需要一种能直接处理任意BVH骨架并学习拓扑无关表示的通用方法与数据基座。

Method: 提出三部分框架：1）Ontology-aware Skeletal Graph Encoder（OwO）：从BVH解析关节语义、静态姿态偏移与骨架拓扑，构建结构先验的骨架嵌入；2）Topology-Agnostic Tokenizer（TAT）：将时序动作压缩为与拓扑无关的通用离散token空间，实现对动作与骨架结构的解耦；3）Unified BVH Universe（UvU）：聚合大量异构BVH骨架与动作的统一数据集，支撑训练与评测。

Result: 在高压缩率下仍实现高保真重建，并验证了动作与骨架结构的有效解耦。所得token空间支持跨物种动作迁移、动作组合、去噪、基于token的生成以及文本-动作检索等应用。

Conclusion: NEC在异构骨架上实现统一的动作token化与重建，构建了兼容多形态的动作分析与生成框架，为跨物种/跨拓扑的动作理解与合成提供了通用基础设施。

Abstract: Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/

</details>


### [53] [LIBERO-X: Robustness Litmus for Vision-Language-Action Models](https://arxiv.org/abs/2602.06556)
*Guodong Wang,Chenkai Zhang,Qingjie Liu,Jinjin Zhang,Jiancheng Cai,Junjie Liu,Xinmin Liu*

Main category: cs.CV

TL;DR: LIBERO-X 提出一套面向视觉-语言-动作（VLA）模型的更可靠基准：分层评测协议+高多样性人工遥操作训练数据，用于细粒度诊断空间泛化、物体识别与指令理解能力，并揭示在累积扰动下的显著性能下滑。


<details>
  <summary>Details</summary>
Motivation: 现有VLA基准难以真实反映模型的泛化、鲁棒与语义-操作对齐能力，主要因评测协议不足、难以覆盖现实分布偏移，导致对模型能力的误判。

Method: 从评测与数据两方面重构基准：1）设计分层评测协议，围绕空间泛化、物体识别、任务指令理解三大能力，设置逐级增难的场景与任务，用于跟踪性能随复杂度提升的退化曲线；2）构建高多样性训练集，基于人类遥操作采集，每个场景包含多种细粒度操作目标，以缩小训练-评测分布差距。

Result: 在典型VLA模型上的实验显示：在累积扰动（更复杂环境与任务）下性能显著下降，暴露出在场景理解与指令落地方面的持续短板；分层协议支持对具体失效因素的细粒度定位。

Conclusion: LIBERO-X 将分层评测与多样训练数据结合，提供更可信的VLA基准，能更精准诊断与推动模型在泛化、鲁棒与语义-动作对齐上的进展。

Abstract: Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.

</details>


### [54] [SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs](https://arxiv.org/abs/2602.06566)
*Niccolo Avogaro,Nayanika Debnath,Li Mi,Thomas Frick,Junling Wang,Zexue He,Hang Hua,Konrad Schindler,Mattia Rigotti*

Main category: cs.CV

TL;DR: SPARC 通过将“视觉感知”和“推理”解耦为两阶段（先找图像中与问题相关的区域，再基于这些区域推理），在测试时可灵活扩展计算并显著降低视觉token开销；在多项视觉推理基准上优于单体VLM与强对齐方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在测试时扩展token（test-time scaling）不稳健：无结构的图像CoT将感知与推理缠结，易产生冗长杂乱上下文，小小感知错误会级联成错误答案；且常需代价高昂、需手工奖励的RL才能取得好效果。

Method: 提出SPARC框架，受大脑感觉-认知顺序处理启发，将流程拆为两阶段：1) 显式视觉检索/搜索，定位与问题相关的图像区域；2) 仅基于这些被选中的区域进行推理与作答。该解耦允许：独立的测试时伸缩与非对称算力分配；可选择性优化（如只优化感知阶段）；利用分辨率分级与区域化高分辨率处理以压缩上下文与降低token数。

Result: 在多个视觉推理基准上优于单体基线与强视觉落地方法：例如，使Qwen3VL-4B在V* VQA基准上准确率提升6.7个百分点；在OOD任务上较“thinking with images”高4.6点，同时将token预算降低约200倍。

Conclusion: 将感知与推理解耦带来更稳健的测试时扩展、更低的视觉token与计算成本，并能在分布移位下灵活地把算力优先用于感知阶段，从而获得更强的总体性能与可优化性。

Abstract: Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.

</details>


### [55] [An Integer Linear Programming Approach to Geometrically Consistent Partial-Partial Shape Matching](https://arxiv.org/abs/2602.06590)
*Viktoria Ehm,Paul Roetzer,Florian Bernard,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出首个面向部分-部分3D形状匹配的整数线性规划(ILP)方法，利用几何一致性同时估计重叠区域并求解保持邻域的对应，在误差与光滑性上表现优异且更具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现实采集(如3D扫描)常得到不完整形状。现有研究多聚焦全-全或部分-全匹配，部分-部分匹配更贴近真实但更难：需在未知重叠区域下得到精确、平滑且一致的对应关系。

Method: 构建一个专为部分-部分情形设计的ILP模型：以几何一致性为强先验，将重叠区域选择与对应关系联合优化；约束旨在保持邻域/局部拓扑，确保匹配的平滑性与一致性；通过可扩展的求解策略实现鲁棒与高效的优化。

Result: 在实验中，该方法在匹配误差与平滑性指标上取得高质量结果；对比现有形式化方法具有更好的可扩展性(更大规模形状/点集上依然有效)。

Conclusion: 几何一致性驱动的ILP为部分-部分3D匹配提供了有效统一框架，能够同时识别重叠区域并得到邻域保持的对应，兼顾精度、平滑性与规模化能力。

Abstract: The task of establishing correspondences between two 3D shapes is a long-standing challenge in computer vision. While numerous studies address full-full and partial-full 3D shape matching, only a limited number of works have explored the partial-partial setting, very likely due to its unique challenges: we must compute accurate correspondences while at the same time find the unknown overlapping region. Nevertheless, partial-partial 3D shape matching reflects the most realistic setting, as in many real-world cases, such as 3D scanning, shapes are only partially observable. In this work, we introduce the first integer linear programming approach specifically designed to address the distinctive challenges of partial-partial shape matching. Our method leverages geometric consistency as a strong prior, enabling both robust estimation of the overlapping region and computation of neighbourhood-preserving correspondences. We empirically demonstrate that our approach achieves high-quality matching results both in terms of matching error and smoothness. Moreover, we show that our method is more scalable than previous formalisms.

</details>


### [56] [ProtoQuant: Quantization of Prototypical Parts For General and Fine-Grained Image Classification](https://arxiv.org/abs/2602.06592)
*Mikołaj Janusz,Adam Wróbel,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: ProtoQuant通过在潜表示空间中使用离散码本量化原型，稳定“原型—部件”解释并免微调骨干，在ImageNet与细粒度数据集上实现可解释且高效的分类，精度具竞争力、可解释性可比。


<details>
  <summary>Details</summary>
Motivation: 现有原型/部件可解释模型存在两大痛点：1）ImageNet级泛化差、常需代价高的主干微调；2）“原型漂移”，即原型不扎根训练分布、对微小扰动激活不稳定，削弱了“this looks like that”的可解释性。作者希望在不微调骨干的前提下，提升原型稳定性与可解释性并扩展到大规模数据。

Method: 提出ProtoQuant：在骨干冻结的前提下，于潜空间引入向量量化，将原型限制到一个可学习离散码本中。模型作为一个轻量可解释“分类头”，以码本条目作为原型，避免原型漂移并保证与训练数据对齐；无需更新骨干即可端到端训练头部与码本。

Result: 在ImageNet、CUB-200、Cars-196上，ProtoQuant获得与现有原型部件方法相当的可解释性指标，并在分类精度上具竞争力，同时成功扩展到ImageNet规模。

Conclusion: 通过离散码本量化约束，ProtoQuant稳定原型、提升解释的扎根性，并以无需微调骨干的高效头部实现大规模可解释分类，在精度与可解释性之间取得良好平衡。

Abstract: Prototypical parts-based models offer a "this looks like that" paradigm for intrinsic interpretability, yet they typically struggle with ImageNet-scale generalization and often require computationally expensive backbone finetuning. Furthermore, existing methods frequently suffer from "prototype drift," where learned prototypes lack tangible grounding in the training distribution and change their activation under small perturbations. We present ProtoQuant, a novel architecture that achieves prototype stability and grounded interpretability through latent vector quantization. By constraining prototypes to a discrete learned codebook within the latent space, we ensure they remain faithful representations of the training data without the need to update the backbone. This design allows ProtoQuant to function as an efficient, interpretable head that scales to large-scale datasets. We evaluate ProtoQuant on ImageNet and several fine-grained benchmarks (CUB-200, Cars-196). Our results demonstrate that ProtoQuant achieves competitive classification accuracy while generalizing to ImageNet and comparable interpretability metrics to other prototypical-parts-based methods.

</details>


### [57] [DAVE: Distribution-aware Attribution via ViT Gradient Decomposition](https://arxiv.org/abs/2602.06613)
*Adam Wróbel,Siddhartha Gairola,Jacek Tabor,Bernt Schiele,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: DAVE提出一种针对ViT的分布感知归因方法，通过对输入梯度进行结构化分解，生成更稳定、细粒度的高分辨率归因图，减少由补丁嵌入与注意力路由引入的结构性伪影。


<details>
  <summary>Details</summary>
Motivation: 现有ViT可解释性方法在像素级归因上不稳定、高分辨率不足，且受补丁化与注意力结构影响产生条纹/网格等伪影，因而常退回到粗粒度的patch级归因，限制了精细定位与可靠解释。

Method: 基于ViT架构特性，对输入-输出映射的梯度进行严格的结构化分解，分离出局部等变、稳定的梯度成分，与由架构（如patch embedding、注意力路由）诱导的伪影及其他不稳定源。该方法称为DAVE（Distribution-aware Attribution via ViT Gradient Decomposition），本质是对有效梯度进行分布感知的解耦与滤除。

Result: 得到稳定、高分辨率的像素级归因图，显著减少结构性伪影，相比现有方法在可视质量与稳定性上更优，支持更细致的目标边界与局部特征归因。

Conclusion: 通过利用ViT的结构性质进行梯度分解，DAVE能有效隔离并抑制架构诱导的伪影与不稳定性，提升ViT的像素级可解释性与归因可靠性，摆脱对粗粒度patch级归因的依赖。

Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \textit{(\underline{D}istribution-aware \underline{A}ttribution via \underline{V}iT Gradient D\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.

</details>


### [58] [CauCLIP: Bridging the Sim-to-Real Gap in Surgical Video Understanding via Causality-Inspired Vision-Language Modeling](https://arxiv.org/abs/2602.06619)
*Yuxin He,An Li,Cheng Xue*

Main category: cs.CV

TL;DR: 提出CauCLIP：一种因果启发的视觉-语言框架，在无目标域数据条件下实现手术阶段识别的跨域泛化；通过频域增强和因果抑制损失学习域不变、因果相关特征，在SurgVisDom硬适配基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 手术阶段识别对智能手术室中的情境感知决策支持至关重要，但标注临床视频稀缺且合成到真实之间存在大域间差异，导致模型泛化受限。需要一种无需访问目标域、仍能学习域不变且因果相关特征的方法。

Method: 基于CLIP的因果视觉-语言框架：1) 频率域增强：扰动域特定属性（如纹理/光照等高频或低频成分），在不破坏语义结构下生成多样分布；2) 因果抑制损失：惩罚模型依赖非因果偏差，强化与手术流程因果相关的稳定特征；3) 统一训练：将上述组件融入端到端学习，获得域不变表示进行阶段识别；训练时不访问目标域数据。

Result: 在SurgVisDom硬适配基准上，CauCLIP显著优于所有对比方法，显示其在跨域手术视频理解中的优越性（具体数值未在摘要中给出）。

Conclusion: 因果引导的视觉-语言模型可在不依赖目标域的前提下学习稳定的因果特征，实现对手术阶段识别的强域泛化能力；频域增强与因果抑制的结合是性能提升的关键。

Abstract: Surgical phase recognition is a critical component for context-aware decision support in intelligent operating rooms, yet training robust models is hindered by limited annotated clinical videos and large domain gaps between synthetic and real surgical data. To address this, we propose CauCLIP, a causality-inspired vision-language framework that leverages CLIP to learn domain-invariant representations for surgical phase recognition without access to target domain data. Our approach integrates a frequency-based augmentation strategy to perturb domain-specific attributes while preserving semantic structures, and a causal suppression loss that mitigates non-causal biases and reinforces causal surgical features. These components are combined in a unified training framework that enables the model to focus on stable causal factors underlying surgical workflows. Experiments on the SurgVisDom hard adaptation benchmark demonstrate that our method substantially outperforms all competing approaches, highlighting the effectiveness of causality-guided vision-language models for domain-generalizable surgical video understanding.

</details>


### [59] [PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks](https://arxiv.org/abs/2602.06663)
*Junxian Li,Kai Liu,Leyang Chen,Weida Wang,Zhixin Wang,Jiaqi Xu,Fan Li,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出PlanViz基准与PlanScore评分，用于评估统一多模态模型在与电脑使用相关的图像生成与编辑任务中的规划与执行能力，涵盖路线规划、工作流程图和网页/界面展示三类子任务，并通过人工标注与质控确保数据质量，实验揭示UMMs在空间与程序性推理方面的局限与机遇。


<details>
  <summary>Details</summary>
Motivation: UMMs在图像生成和多模态推理上表现强劲，但其在贴近日常“电脑使用”情境（需要空间推理与程序性理解）的规划式图像生成/编辑能力尚未系统评估；现有基准无法全面测试这类任务的正确性、视觉质量与效率。

Method: 构建PlanViz基准，选取与日常高频相关、需多步规划的三类子任务：路线规划、工作流程图、网页与UI展示；采用人工标注问题与参考图像并配套质控流程；提出任务自适应评分PlanScore，综合衡量正确性、视觉质量与效率。

Result: 通过在PlanViz上的实验，作者识别并量化了UMMs在处理空间布局、步骤依赖与界面约束等方面的关键短板，同时发现部分模型在特定子任务上具备潜力。

Conclusion: PlanViz与PlanScore为评测UMMs在电脑使用相关图像生成/编辑中的规划与执行能力提供了系统框架，当前模型仍存在明显差距，基准揭示了改进方向与研究机会。

Abstract: Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.

</details>


### [60] [CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis](https://arxiv.org/abs/2602.06674)
*Yonghao Si,Xingyuan Zeng,Zhao Chen,Libin Zheng,Caleb Chen Cao,Lei Chen,Jian Yin*

Main category: cs.CV

TL;DR: CytoCrowd 引入了含冲突标注与权威金标准并存的细胞学图像基准，用于常规视觉任务与标注聚合方法评测，填补了医疗图像多专家分歧与客观评估并存的数据缺口。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像数据集要么只有单一“干净”真值，掩盖专家分歧；要么提供多重标注却缺乏独立金标准，无法客观评价算法。需要一种既保留专家分歧、又提供权威评测基准的数据资源。

Method: 构建包含446张高分辨率细胞学图像的数据集，每张图像附带：(1) 四名病理专家的原始、可能互相冲突的标注；(2) 由资深专家建立的独立高质量金标准。并基于该数据集分别开展：a) 使用金标准进行检测/分类等常规视觉任务评测；b) 使用多专家冲突标注评测标注聚合算法。提供相应的基线实验与结果。

Result: 基线实验显示：在常规视觉任务与标注聚合任务上均存在显著挑战，模型与聚合方法在此数据集上表现受限，反映出真实世界专家分歧对算法性能的影响。

Conclusion: CytoCrowd 弥合了医疗影像数据集中“单真值”与“多标注无金标准”的断层，为发展能处理专家分歧的新一代医学影像模型与聚合算法提供了有价值的公开基准。

Abstract: High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.

</details>


### [61] [Can We Build a Monolithic Model for Fake Image Detection? SICA: Semantic-Induced Constrained Adaptation for Unified-Yet-Discriminative Artifact Feature Space Reconstruction](https://arxiv.org/abs/2602.06676)
*Bo Du,Xiaochen Ma,Xuekang Zhu,Zhe Yang,Chaogun Niu,Jian Liu,Ji-Zhe Zhou*

Main category: cs.CV

TL;DR: 提出SICA，用高层语义先验重建伪迹特征空间，实现单体式跨四类图像取证的统一检测，性能超越15个SOTA，并在OpenMMSec上验证近正交、统一且可判别的伪迹表示。


<details>
  <summary>Details</summary>
Motivation: 现实取证场景需一个模型统一检测多种伪造（四个子领域），但现有单体式模型实际表现不如集成方法。作者发现各子领域伪迹本质异质导致特征空间塌缩，是单体式失效根因，亟需既统一又可区分的表示。

Method: 提出以高层语义为结构先验的SICA（Semantic-Induced Constrained Adaptation）：通过语义引导的受约束自适应学习，重建伪迹特征空间，使跨子领域的伪迹表示既共享统一结构又保持判别性；目标是逼近子空间近正交。基于OpenMMSec数据训练与评估。

Result: 在OpenMMSec上，SICA显著优于15个SOTA基线；重建的伪迹特征子空间呈近正交结构，实现“统一且可判别”。

Conclusion: 异质伪迹导致单体式FID失败的关键在于特征空间塌缩；利用高层语义作为结构先验的SICA可有效重建统一-可判别的伪迹空间，解决单体式统一检测难题，并在基准上取得领先表现。

Abstract: Fake Image Detection (FID), aiming at unified detection across four image forensic subdomains, is critical in real-world forensic scenarios. Compared with ensemble approaches, monolithic FID models are theoretically more promising, but to date, consistently yield inferior performance in practice. In this work, by discovering the ``heterogeneous phenomenon'', which is the intrinsic distinctness of artifacts across subdomains, we diagnose the cause of this underperformance for the first time: the collapse of the artifact feature space driven by such phenomenon. The core challenge for developing a practical monolithic FID model thus boils down to the ``unified-yet-discriminative" reconstruction of the artifact feature space. To address this paradoxical challenge, we hypothesize that high-level semantics can serve as a structural prior for the reconstruction, and further propose Semantic-Induced Constrained Adaptation (SICA), the first monolithic FID paradigm. Extensive experiments on our OpenMMSec dataset demonstrate that SICA outperforms 15 state-of-the-art methods and reconstructs the target unified-yet-discriminative artifact feature space in a near-orthogonal manner, thus firmly validating our hypothesis. The code and dataset are available at:https: //github.com/scu-zjz/SICA_OpenMMSec.

</details>


### [62] [Clinical-Prior Guided Multi-Modal Learning with Latent Attention Pooling for Gait-Based Scoliosis Screening](https://arxiv.org/abs/2602.06743)
*Dong Chen,Zizhuang Wei,Jialei Xu,Xinyang Sun,Zonglin He,Meiru An,Huili Peng,Yong Hu,Kenneth MC Cheung*

Main category: cs.CV

TL;DR: 提出ScoliGait基准与多模态可解释模型，用独立受试者视频-文本-知识图谱融合，显著提升AIS早筛性能并避免数据泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有AIS筛查依赖主观、难扩展的临床评估；视频步态分析虽有潜力，但常存在数据泄漏（同一人重复片段导致虚高）与过于简化、缺乏临床可解释性的模型问题。需要一个无泄漏、可解释且可扩展的基准与方法。

Method: 1) 构建ScoliGait：含1,572训练步态视频与300独立测试视频，附放射学Cobb角与基于临床运动学先验的文本描述；2) 设计多模态框架：引入由临床先验指导的运动学知识图谱作可解释特征；3) 采用潜在注意力池化，将视频、文本与知识图谱进行融合，实现可解释表示与判别。

Result: 在真实的受试者独立测试集上达成新的SOTA，相比既有方法有显著性能提升，体现数据泄漏控制下的真实泛化能力。

Conclusion: ScoliGait与所提多模态可解释框架为可扩展、非侵入的AIS评估提供了稳健且具临床依据的基础，推动早期筛查与客观化评估。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a prevalent spinal deformity whose progression can be mitigated through early detection. Conventional screening methods are often subjective, difficult to scale, and reliant on specialized clinical expertise. Video-based gait analysis offers a promising alternative, but current datasets and methods frequently suffer from data leakage, where performance is inflated by repeated clips from the same individual, or employ oversimplified models that lack clinical interpretability. To address these limitations, we introduce ScoliGait, a new benchmark dataset comprising 1,572 gait video clips for training and 300 fully independent clips for testing. Each clip is annotated with radiographic Cobb angles and descriptive text based on clinical kinematic priors. We propose a multi-modal framework that integrates a clinical-prior-guided kinematic knowledge map for interpretable feature representation, alongside a latent attention pooling mechanism to fuse video, text, and knowledge map modalities. Our method establishes a new state-of-the-art, demonstrating a significant performance gap on a realistic, non-repeating subject benchmark. Our approach establishes a new state of the art, showing a significant performance gain on a realistic, subject-independent benchmark. This work provides a robust, interpretable, and clinically grounded foundation for scalable, non-invasive AIS assessment.

</details>


### [63] [Gold Exploration using Representations from a Multispectral Autoencoder](https://arxiv.org/abs/2602.06748)
*Argyro Tsandalidou,Konstantinos Dogeas,Eleftheria Tetoula Tsonga,Elisavet Parselia,Georgios Tsimiklis,George Arvanitakis*

Main category: cs.CV

TL;DR: 用多光谱Sentinel‑2影像+自监督生成表示（自编码器基础模型Isometric）+轻量XGBoost，在小样本标注下识别含金区；相比原始光谱基线，补丁层面精度0.51→0.68，影像层面0.55→0.73，显示生成嵌入可迁移，提升全局可扩展勘探效率。


<details>
  <summary>Details</summary>
Motivation: 实地矿产勘查数据昂贵且稀缺，限制大范围找矿；卫星遥感覆盖广但标注少。需要一种能在少量标注下从多光谱数据中抽取可迁移矿物学特征、提升金矿找矿可行性与效率的方法。

Method: 预训练：在大规模FalconSpace‑S2 v1.0上训练自编码器基础模型Isometric，学习致密的光谱‑空间生成表示。下游：将Isometric产生的嵌入输入轻量XGBoost分类器进行金/非金判别；与直接用原始光谱作为特征的基线比较。数据：来自已知金/非金位置的63幅Sentinel‑2影像；评估补丁级与影像级准确率。

Result: 采用生成嵌入的方案，补丁级准确率从0.51提升到0.68，影像级从0.55提升到0.73；表明所学表示在标注有限时仍能捕捉可迁移的矿物学模式。

Conclusion: 基础模型的生成表示能显著提升基于卫星的金矿找矿可行性，在标注稀缺条件下更高效、可扩展、具全球适用潜力；为基于遥感的矿产勘探提供了有前景的通用框架。

Abstract: Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.

</details>


### [64] [Revisiting Emotions Representation for Recognition in the Wild](https://arxiv.org/abs/2602.06778)
*Joao Baptista Cardia Neto,Claudio Ferrari,Stefano Berretti*

Main category: cs.CV

TL;DR: 将表情识别从单标签转为“情绪分布”学习：用VAD(愉悦-唤醒-支配)空间的情绪分布映射为桥梁，自动为现有单标签/或仅VAD标注的人脸数据生成多情绪概率标注，从而刻画混合和模糊情绪；初步实验显示有效并开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 单一基本情绪分类过于简化，难以表达自发表情的多样与混合；现有分布学习方法受限于缺乏多情绪分布标注，因为多数数据集仅有单标签。作者希望用可得的VAD标注把复杂情绪映射为多情绪概率分布，丰富标注并更贴近人类感知的不确定性。

Method: 基于一项将大量基本/复合情绪映射到VAD空间概率分布的研究：1) 收集带VAD标注的人脸图像；2) 计算该图像在各情绪原型的VAD分布下的似然，得到对各情绪的概率权重；3) 由此自动为样本生成“情绪混合”分布标签，用作分布学习训练/评估。并发布对AffectNet的再标注。

Result: 初步实验表明：用所生成的分布标签可更好表征模糊与多重情绪，相较单标签更具表达力，训练得到的模型在处理复杂/含糊表情时表现出优势；提供的注释资源可复用。

Conclusion: 将VAD作为中介实现从单标签到情绪分布的自动再标注是可行且有益，能更真实刻画多维、混合与不确定的情绪状态；该方向值得进一步在更大规模数据与任务上验证与扩展。

Abstract: Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at https://github.com/jbcnrlz/affectnet-b-annotation.

</details>


### [65] [Machine Learning for Detection and Severity Estimation of Sweetpotato Weevil Damage in Field and Lab Conditions](https://arxiv.org/abs/2602.06786)
*Doreen M. Chelangat,Sudi Murindanyi,Bruce Mugizi,Paul Musana,Benard Yada,Milton A. Otema,Florence Osaru,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CV

TL;DR: 该研究用计算机视觉自动评估红薯象甲为害：田间用分类模型判定根部损伤等级（测试准确率71.43%）；实验室用YOLO检测微小取食孔，配合根分割与切片策略，mAP达77.7%，为育种表型评估提供客观高效工具。


<details>
  <summary>Details</summary>
Motivation: 人工评分费时、主观且一致性差，限制了在撒哈拉以南非洲等地区提升抗虫红薯育种效率与规模化表型评估的需求。

Method: 提出田间与实验室两套计算机视觉流程：1) 田间：采集根样图像，训练分类模型预测损伤严重度；2) 实验室：构建带标注数据集，设计两阶段检测管线——先进行根部分割，再采用图像切片（tiling）以提升小目标可见性，基于YOLO系列实时目标检测器进行微孔检测。

Result: 田间分类模型在测试集上达到71.43%准确率；实验室检测模型在微小象甲取食孔识别上取得77.7% mAP。

Conclusion: 计算机视觉可在红薯育种中实现客观、可扩展、与工作流兼容的损伤评估，显著提升表型评估效率，有助于缓解象甲对粮食安全的负面影响。

Abstract: Sweetpotato weevils (Cylas spp.) are considered among the most destructive pests impacting sweetpotato production, particularly in sub-Saharan Africa. Traditional methods for assessing weevil damage, predominantly relying on manual scoring, are labour-intensive, subjective, and often yield inconsistent results. These challenges significantly hinder breeding programs aimed at developing resilient sweetpotato varieties. This study introduces a computer vision-based approach for the automated evaluation of weevil damage in both field and laboratory contexts. In the field settings, we collected data to train classification models to predict root-damage severity levels, achieving a test accuracy of 71.43%. Additionally, we established a laboratory dataset and designed an object detection pipeline employing YOLO12, a leading real-time detection model. This methodology incorporated a two-stage laboratory pipeline that combined root segmentation with a tiling strategy to improve the detectability of small objects. The resulting model demonstrated a mean average precision of 77.7% in identifying minute weevil feeding holes. Our findings indicate that computer vision technologies can provide efficient, objective, and scalable assessment tools that align seamlessly with contemporary breeding workflows. These advancements represent a significant improvement in enhancing phenotyping efficiency within sweetpotato breeding programs and play a crucial role in mitigating the detrimental effects of weevils on food security.

</details>


### [66] [A Unified Formula for Affine Transformations between Calibrated Cameras](https://arxiv.org/abs/2602.06805)
*Levente Hajder*

Main category: cs.CV

TL;DR: 给出两视图间局部图像块的仿射映射封闭解；该映射由相对位姿、像素位置与局部表面法向共同决定。


<details>
  <summary>Details</summary>
Motivation: 在多视图几何与特征匹配中，局部外观因视角和表面形状而发生仿射畸变。现有方法多用近似或数值估计。提供一个显式闭式公式可提升配准、跟踪与重建的准确性与效率。

Method: 在两台已标定相机的前提下，从投影几何出发，将三维局部曲面在像平面上的一阶近似建模为仿射变换。推导将仿射映射表示为相机相对位姿、图像坐标及局部表面法向的函数，得到封闭形式表达式。

Result: 得到在两幅校准图像间映射局部补丁的显式仿射变换公式，明确其依赖的参数为相对位姿、像素位置和表面法向。

Conclusion: 该闭式表达为局部畸变建模提供了精确高效的工具，可用于更稳健的特征匹配、配准与三维重建等任务。

Abstract: In this technical note, we derive a closed-form expression for the affine transformation mapping local image patches between two calibrated views. We show that the transformation is a function of the relative camera pose, the image coordinates, and the local surface normal.

</details>


### [67] [RAIGen: Rare Attribute Identification in Text-to-Image Generative Models](https://arxiv.org/abs/2602.06806)
*Silpa Vadakkeeveetil Sreelatha,Dan Wang,Serge Belongie,Muhammad Awais,Anjan Dutta*

Main category: cs.CV

TL;DR: RAIGen提出一种无监督方法在扩散模型中发现“稀有属性”，用稀疏自编码器与少数性度量定位可解释神经元，并据其最强激活样本揭示被训练数据忽略的社会、文化或风格特征；可用于审计与在生成中放大这些属性，超越固定公平性范畴，并在SD与SDXL上有效。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法要么限定在预定义的公平类别（性别、种族等），要么只识别主流偏置；两者都忽略了“发现并发掘”数据分布中罕见但已被模型编码的少数/稀有特征，这对模型审计、公平性与多样性提升很关键。

Method: 提出RAIGen：1) 使用Matryoshka Sparse Autoencoders（分层稀疏自编码器）从扩散模型内部表征中学习可解释稀疏因子/神经元；2) 设计新的少数性度量，结合“神经元激活频率”（越少见越可能少数）与“语义独特性”（与其他因子区分度高）；3) 通过最强激活的图像簇来语义化这些神经元，从而发现代表稀有社会、文化或风格属性的因子；4) 用于系统审计与在采样时有针对性地放大相关属性。

Result: 在Stable Diffusion与更大规模的SDXL上，RAIGen能：a) 发现超越固定公平标签的稀有属性；b) 横跨架构进行系统审计；c) 在生成过程中实现对稀有属性的定向增强。实验表明其可扩展性与有效性。

Conclusion: RAIGen首次提供无监督的稀有属性发现框架，能定位并解读扩散模型中被低覆盖的特征，既可用于偏差审计，也可在生成时提升多样性与少数属性的可见性，补足传统闭集与开集去偏方法的空白。

Abstract: Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.

</details>


### [68] [GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification](https://arxiv.org/abs/2602.06830)
*Soonbin Lee,Yeong-Gyu Kim,Simon Sasse,Tomas M. Borges,Yago Sanchez,Eun-Seok Ryu,Thomas Schierl,Cornelius Hellge*

Main category: cs.CV

TL;DR: GaussianPOP提出基于3DGS渲染方程的解析误差度量，对每个高斯对图像的贡献进行精确评估，以进行训练中与训练后剪枝/简化，在单次前向中高效计算误差，较现有基于启发式重要性分数的方法实现更优的紧凑度-质量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting简化多依赖混合权重、敏感度等启发式重要性指标，这些并非视觉误差驱动，常导致剪枝后画质退化或压缩率不足，缺乏稳定、可泛化的原则性准则。

Method: 从3DGS渲染方程出发推导单个高斯对渲染结果的解析误差标准，给出可在单次前向中计算的高效算法；在此基础上进行两种场景：1）训练中在线剪枝；2）训练后迭代式误差重估与简化，以提升稳定性与最终质量。

Result: 在训练内与训练后两种应用场景中，较当下SOTA剪枝方法持续取得更优的参数/高斯数量压缩与渲染质量（视觉误差）权衡。

Conclusion: 基于解析误差量化的GaussianPOP为3DGS提供了统一、准确且高效的简化框架，能够稳定提升紧凑度而保持高保真渲染，优于基于启发式的重要性评分方法。

Abstract: Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image. By introducing a highly efficient algorithm, our framework enables practical error calculation in a single forward pass. The framework is both accurate and flexible, supporting on-training pruning as well as post-training simplification via iterative error re-quantification for improved stability. Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.

</details>


### [69] [Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping](https://arxiv.org/abs/2602.06850)
*Chao Zhou,Tianyi Wei,Yiling Chen,Wenbo Zhou,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出PKA框架，在DiT中高效实现多条件控制，通过位置对齐与关键词范围化注意力减少无效跨模态交互，并结合条件敏感采样，在保持高保真条件控制下实现显著加速与省显存。


<details>
  <summary>Details</summary>
Motivation: 多条件（布局、主体外观等）控制在文本到图像生成中很重要，但现有在DiT中的“拼接并注意”策略随条件数增长呈二次复杂度，带来计算与显存瓶颈；作者观察到跨模态交互中存在大量空间或语义冗余，亟需一种既精确又高效的控制机制。

Method: 提出PKA（Position-aligned and Keyword-scoped Attention）：1）PAA（Position-Aligned Attention）通过局部patch对齐，将空间控制线性化，减少全局交互；2）KSA（Keyword-Scoped Attention）基于语义感知掩码裁剪无关的主体交互，仅在与关键词相关的区域/通道计算注意力；3）CSAS（Conditional Sensitivity-Aware Sampling）在训练中对关键去噪阶段重加权，提升收敛速度与条件一致性。

Result: 在多条件控制的DiT推理中，实现约10.0×的速度提升与5.1×的显存节省，同时提升条件保真度与收敛效率。

Conclusion: 通过位置对齐与关键词范围化的注意力设计，结合条件敏感采样，PKA有效去除跨模态冗余，提供可扩展、资源友好的高保真多条件生成方案，适合在受限算力环境中部署并扩展到更多控制信号。

Abstract: While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.

</details>


### [70] [Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing](https://arxiv.org/abs/2602.06862)
*Meng Lou,Stanley Yu,Yizhou Yu*

Main category: cs.CV

TL;DR: 提出AdaRoute：一种用于视觉模型PEFT的MoE式适配器，动态路由共享专家中心生成输入相关的低秩权重，跨层共享以提升特征多样性，在多种密集预测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT在复杂密集预测任务上存在两大问题：1）输入不可感知的静态适配，难以对不同图像/区域定制化；2）跨层存在冗余表示，缺乏有效的跨层交互与参数共享，导致效率与性能受限。

Method: 设计AdaRoute适配器：- 建立共享专家中心（若干可训练矩阵作为专家）；- 每一层的AdaRoute模块在前向时根据当前输入通过轻量动态参数路由，从对应专家中心选择并加权聚合，生成该层的动态权重矩阵；- 这些动态权重以低秩方式对主干进行适配，实现输入依赖的表示增强；- 多层模块共享同一专家中心，形成隐式跨层交互并减少冗余。

Result: 在语义分割、目标检测与实例分割、全景分割等多种视觉任务上，AdaRoute较现有PEFT方法取得更优性能（接近或超过全量微调基线）并保持较低可训练参数量。

Conclusion: 通过共享专家中心与动态参数路由，AdaRoute实现输入感知、低秩且跨层协同的适配，在多种密集预测任务上展现出优越的准确率-效率权衡，证明其为PEFT的有效方案。

Abstract: Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cross-layer representations. To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture. Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix. During a feedforward pass, each AdaRoute module in the network dynamically generates weight matrices tailored for the current module via a simple dynamic parameter routing mechanism, which selectively aggregates parameter matrices in the corresponding expert center. Dynamic weight matrices in AdaRoute modules facilitate low-rank adaptation in an input-dependent manner, thus generating more customized and powerful feature representations. Moreover, since AdaRoute modules across multiple network layers share the same expert center, they improve feature diversity by promoting implicit cross-layer feature interaction. Extensive experiments demonstrate the superiority of AdaRoute on diverse vision tasks, including semantic segmentation, object detection and instance segmentation, and panoptic segmentation. Code will be available at: https://bit.ly/3NZcr0H.

</details>


### [71] [RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing](https://arxiv.org/abs/2602.06871)
*Mohammadreza Salehi,Mehdi Noroozi,Luca Morreale,Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Ramos,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: 提出RFDM：一种自回归、因果且高效的文本驱动视频编辑模型，基于2D图像扩散模型逐帧编辑，预测相邻帧间残差，实现可变长度视频的低算力高一致性编辑，并在新基准上优于I2I方法、接近3D V2V。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动视频编辑多需固定长度输入与高算力；而自回归视频生成可高效处理可变长度，但少用于编辑。需要一种既能可变长度、又高效、还能保持时序一致的编辑方法。

Method: 从图像到图像(I2I)扩散模型出发，构建因果自回归视频到视频(V2V)编辑：第t帧条件依赖于对t-1帧的预测。提出Residual Flow Diffusion Model（RFDM）：重新设计I2I前向扩散，使模型学习预测“目标输出−前一预测”的残差，聚焦相邻帧变化，提升时序一致与效率；并构建新的视频编辑评测基准。

Result: 在全局/局部风格迁移与目标移除的成对视频数据上训练，RFDM在计算量与图像模型同级、推理随视频长度线性扩展，超过I2I基线，并与3D时空V2V模型性能相当；新基准能更好区分SOTA方法。

Conclusion: RFDM以低计算成本实现可变长度、逐帧因果视频编辑，通过残差预测强化时序一致与编辑聚焦，实证上兼具效率与质量，成为I2I与3D V2V之间的有效折中，并提供更合理的评测基准。

Abstract: Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/

</details>


### [72] [NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices](https://arxiv.org/abs/2602.06879)
*Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Couto Pimentel Ramos,Luca Morreale,Mehdi Noroozi,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: NanoFLUX 将 17B 级别的 FLUX.1-Schnell 文生图模型经由渐进式压缩蒸馏到 2.4B，并在移动端约 2.5 秒生成 512×512 图像，兼顾质量与延迟。


<details>
  <summary>Details</summary>
Motivation: SOTA 文生图扩散/流匹配模型视觉质量不断提升，但模型规模巨大，难以在端侧实时运行；需要在不显著牺牲生成质量的前提下降低参数量与推理延迟，实现高质量 on-device 生成。

Method: 提出 NanoFLUX：从 17B 教师模型进行蒸馏与结构压缩，核心包括三点：1) 基于扩散 Transformer 的冗余裁剪，将模型从 12B 压到约 2B 级；2) 引入 ResNet 风格的 token 下采样，使中间块在低分辨率 token 上计算、同时保留高分辨率路径以维持细节；3) 文本编码器蒸馏新法，在采样过程中利用去噪器早期层的视觉信号指导文本表征学习。整体采用渐进式压缩与蒸馏以守住质量。

Result: 在移动设备上约 2.5 秒生成 512×512 图像，显示端侧高质量文生图可行；在同等分辨率下显著降低延迟与模型体量，同时保持接近教师模型的生成质量（摘要未给出具体指标）。

Conclusion: 通过结构裁剪、token 下采样与跨模态蒸馏，NanoFLUX 在大幅压缩参数与延迟的同时维持生成质量，缩小了 SOTA 文生图与端侧部署之间的鸿沟，验证了高质量移动端文生图的可行性。

Abstract: While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.

</details>


### [73] [Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers](https://arxiv.org/abs/2602.06886)
*Yuxuan Yao,Yuxuan Chen,Hui Li,Kaihui Cheng,Qipeng Guo,Yuwei Sun,Zilong Dong,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: 论文发现多模态扩散Transformer（MMDiT）在去噪深层会“遗忘”文本提示语语义，并提出训练免改动的“提示再注入”方法，在多基准上显著提升指令跟随与生成质量。


<details>
  <summary>Details</summary>
Motivation: MMDiT采用文本与图像分支双向交互，但实际生成中常出现“跑题/不跟随指令”。作者怀疑根源在于随层深度加深，文本分支对提示语的语义表征被逐步弱化或遗忘，需要验证并寻找无需再训练即可缓解的方法。

Method: 1) 在三种代表性MMDiT（SD3、SD3.5、FLUX.1）上，对文本分支各层表示进行语言学属性探针，量化语义保持度随层数变化；2) 提出训练免改动的“提示再注入”：把早期层的提示语表示在后期层重新注入，以恢复/强化语义；3) 在GenEval、DPG、T2I-CompBench++等基准上评测包括指令跟随、偏好与美学等多维指标。

Result: 探针结果验证了“提示遗忘”现象：层越深，文本语义属性越弱。采用提示再注入后，各基准上指令跟随能力稳定提升，同时偏好度、美学分与整体文图质量也有一致增益。

Conclusion: MMDiT存在系统性的提示遗忘问题。无需再训练的提示再注入策略能有效缓解该问题并提升生成质量，为改进多模态扩散模型的对齐与可控性提供了简单实用的路径。

Abstract: Multimodal Diffusion Transformers (MMDiTs) for text-to-image generation maintain separate text and image branches, with bidirectional information flow between text tokens and visual latents throughout denoising. In this setting, we observe a prompt forgetting phenomenon: the semantics of the prompt representation in the text branch is progressively forgotten as depth increases. We further verify this effect on three representative MMDiTs--SD3, SD3.5, and FLUX.1 by probing linguistic attributes of the representations over the layers in the text branch. Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting. Experiments on GenEval, DPG, and T2I-CompBench++ show consistent gains in instruction-following capability, along with improvements on metrics capturing preference, aesthetics, and overall text--image generation quality.

</details>


### [74] [PANC: Prior-Aware Normalized Cut for Object Segmentation](https://arxiv.org/abs/2602.06912)
*Juan Gutiérrez,Victor Gutiérrez-Garcia,José Luis Blanco-Murillo*

Main category: cs.CV

TL;DR: 提出PANC：一种以少量标注token为锚的弱监督谱分割框架，稳定、可控、可复现，少量标注即可在多基准与细粒度/同质域取得SOTA或显著提升。


<details>
  <summary>Details</summary>
Motivation: 无监督分割常只找最显著目标，且对初始化、种子顺序、阈值等高度敏感，导致划分不稳定且不可控。需要一种在保持自监督全局分组优势的同时，引入最小人类先验以获得稳定、可控、可复现的分割。

Method: 基于TokenCut的token亲和图，引入少量带注释的“锚点”token和先验，将其耦合到图中以改变拓扑；通过偏置谱分解的特征空间，使生成的划分与标注一致。无需训练，使用5–30个注释/数据集即可；保留密集自监督特征的全局分组，同时获得用户可控性与稳定性。

Result: 在DUTS-TE、ECSSD、MS COCO等标准基准上，作为无/弱监督方法达到SOTA；在密集标注昂贵或类内差异微妙的领域表现尤佳：CrackForest mIoU 96.8%（较SOTA+14.43%）、CUB-200-2011 78.0%（+0.2%）、HAM10000 78.8%（+0.37%）；多目标基准上可进行显式、用户可控的语义分割。

Conclusion: 以极少标注token锚定谱分割，可在不训练的前提下实现稳定、可控且高质量的分割，显著提升可复现性与跨域适用性，并在多个基准和细粒度/同质域取得SOTA或大幅超越。

Abstract: Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics.
  We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality.
  Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.

</details>


### [75] [Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs](https://arxiv.org/abs/2602.06914)
*Darryl Hannan,John Cooper,Dylan White,Yijing Watkins*

Main category: cs.CV

TL;DR: 论文探究VLLM在细粒度与空间推理上的薄弱原因，提出合成基准与冗余度量，研究训练任务复杂度如何影响视觉表示的压缩与分布，发现高复杂度视觉数据比例对提升复杂任务表现至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管VLLM语言能力强，但在需要细粒度视觉信息与空间推理的任务上表现不佳，现有解释如“视觉冗余/丢失细节”尚不清晰。作者希望精确刻画模型如何处理不同类型的视觉信息、何种信息被压缩或丢弃，以指导更有效的训练策略。

Method: 1) 构建一个专门探测多类视觉特征的合成基准数据集；2) 设计测量视觉冗余与压缩的指标，量化高层信息分布与细节信息丢失；3) 在多种复杂视觉任务上微调VLLM，系统比较不同任务复杂度对冗余与压缩模式的影响。

Result: 实验表明任务复杂度与视觉压缩存在关联：当训练数据包含更高比例的高复杂度视觉样本时，模型会改变其视觉表示的分布（降低不必要的冗余、保留更多细粒度信息），从而在复杂视觉任务上的性能提升。

Conclusion: VLLM的细粒度与空间推理弱项与其默认的视觉冗余/压缩策略相关。通过在训练中引入足量高复杂度视觉数据，可重塑视觉表示分布，减少有害压缩，提升复杂任务表现；该工作为下一代VLLM的训练配方与数据构成提供了实践性指引。

Abstract: Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.

</details>


### [76] [Reliable Mislabel Detection for Video Capsule Endoscopy Data](https://arxiv.org/abs/2602.06938)
*Julia Werner,Julius Oexle,Oliver Bause,Maxime Le Floch,Franz Brinkmann,Hannah Tolle,Jochen Hampe,Oliver Bringmann*

Main category: cs.CV

TL;DR: 提出一个用于医疗影像（胶囊内镜）数据集的误标检测框架，经专家复核后清洗数据，可提升异常检测表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像标注昂贵且受限于专业医生；类边界常含歧义，导致训练集存在误标，削弱深度网络分类/检测性能，需要一种系统方法定位并纠正误标。

Method: 构建一条误标检测管线：在两个最大、公开的胶囊内镜数据集上运行，自动识别潜在误标样本；随后由三位资深胃肠科医生复核并重新标注。具体算法未在摘要中细述，但核心是基于模型表现或一致性信号筛出可疑样本。

Result: 管线能有效发现错误标注；对被识别样本经专家重标并清洗后，基于该数据训练的异常检测模型性能优于现有基线。

Conclusion: 在胶囊内镜大规模数据中，针对误标的检测与清洗可实质提升异常检测效果；该框架为医学影像数据质量控制提供可行方案。

Abstract: The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.

</details>


### [77] [CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation](https://arxiv.org/abs/2602.06959)
*Kaiyi Huang,Yukun Huang,Yu Li,Jianhong Bai,Xintao Wang,Zinan Lin,Xuefei Ning,Jiwen Yu,Pengfei Wan,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: 提出CineScene：在给定多张静态场景图与用户相机轨迹下，生成含动态主体、场景一致的电影感视频的新任务与方法。核心是将VGGT编码的3D感知场景特征以隐式上下文拼接注入到预训练文本到视频模型，实现可控相机与场景一致性；并以输入场景图随机打乱增强鲁棒性，配合UE5构建的场景解耦数据集，取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 真人实拍电影需要昂贵的实景搭建与拍摄；现有视频生成难以同时保证大幅相机运动下的场景一致性、动态主体生成与相机可控性。需要一种能在解耦静态场景与动态主体的前提下，依据多视图场景条件与用户相机轨迹生成高质量电影视频的方法。

Method: 1) 任务设定：输入多张静态环境图和用户指定相机轨迹，输出含动态主体的场景一致视频。2) 表征：用VGGT将场景图编码为3D感知视觉特征/空间先验。3) 上下文条件注入：将上述特征以“隐式”方式通过额外上下文拼接注入到预训练文本到视频生成模型，实现相机可控的视频合成。4) 训练技巧：对输入场景图进行随机顺序打乱以提升鲁棒性。5) 数据：使用UE5合成场景解耦数据集，包含有/无动态主体的视频对、静态场景全景图及相机轨迹。

Result: 在大幅相机运动与多样环境中实现更好的场景一致性与视频质量，达成SOTA；展示对不同环境的泛化与对相机控制的顺从。

Conclusion: CineScene通过隐式3D上下文条件与预训练T2V模型结合，实现解耦场景条件下的电影级视频生成，兼顾场景一致性、动态主体与相机控制；数据与训练策略进一步提升鲁棒性与泛化。

Abstract: Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.

</details>


### [78] [MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images](https://arxiv.org/abs/2602.06965)
*Ankan Deria,Komal Kumar,Adinath Madhavrao Dukre,Eran Segal,Salman Khan,Imran Razzak*

Main category: cs.CV

TL;DR: MedMO 是面向医学多模态的基础模型，基于通用 MLLM 架构，用大规模医学专属数据分三阶段训练，通过跨模态对齐、指令微调与可验证奖励的强化学习，显著提升视觉问答、文本问答、报告生成与病灶定位等任务，跨放射、眼科、病理等多模态泛化良好，并发布 4B/8B 两个版本。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在医学场景中受限于领域覆盖不足、模态对齐不佳与缺乏有据可依的推理与空间定位能力，难以满足临床复杂任务需求。作者希望构建一个以医学数据为核心训练、具备强事实性与空间落地能力的通用医学多模态模型。

Method: 采用三阶段训练流程：1) 跨模态预训练，使异构视觉编码器与医学语言骨干对齐；2) 指令微调，覆盖图像字幕、VQA、报告生成、检索与带框的病灶定位等多任务；3) 结合事实性校验与框级 GIoU 奖励的可验证强化学习，强调空间定位与逐步推理。

Result: 在多种基准上超越强开源医学 MLLM：VQA 平均准确率较基线+13.7%，距 SOTA Fleming-VL 仅差1.9%；文本问答较基线+6.9%，且较 Fleming-VL +14.5%；报告生成在语义与临床一致性显著提升；定位任务 IoU 较基线+40.4、较 Fleming-VL +37.0%，显示强空间推理与定位能力；在放射、眼科、病理显微等多模态上具备良好泛化。

Conclusion: MedMO 通过医学专属数据与多阶段训练，将事实性校验与空间定位纳入可验证奖励，系统性提升了医学多模态理解、推理与定位能力，提供 4B/8B 版本，展示了在多学科与多任务上的强大通用性与实用潜力。

Abstract: Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page

</details>
