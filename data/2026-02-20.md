<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 提出一种基于Gaussian Splatting (GS) 的数字孪生方法，实现土木基础设施震后损伤的高效三维可视化与更新。其核心：用GS重建三维并映射二维分割，采用多尺度重建兼顾效率与细节，并支持随时间演化的孪生更新。实验在开源合成数据集上验证可行。


<details>
  <summary>Details</summary>
Motivation: 二维图像检测难以在数字孪生中精准呈现三维损伤形态与位置；传统摄影测量在无纹理区域、渲染质量与效率方面受限。需要一种既高质量又高效率、适合数字孪生持续更新的三维表示方法。

Method: 以GS为核心的离散各向异性3D高斯来表示场景辐射场：1) 首先进行GS三维重建；2) 将二维损伤分割结果映射/融合到GS点-高斯上，实现三维损伤可视化并抑制分割误差；3) 采用多尺度GS重建策略，在全局-局部层次间权衡效率与细节；4) 设计随时间采样的增量更新机制，使数字孪生可随着损伤演化迭代。

Result: 在开源的震后检查合成数据集上展示了方法能力：较传统方法更优的三维可视化质量，对无特征区域更稳健，效率高于NeRF类方法，并能更准确地呈现与对齐损伤位置；支持多时相数据的增量更新。

Conclusion: 基于GS的数字孪生框架能在土木基础设施场景中提供高效、高保真、可更新的三维损伤可视化，相比传统摄影测量与NeRF更适用于工程级数字孪生应用。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: 提出UltraVQA多维UGC视频质量数据集与ASO后训练优化，利用多维标注与解析式优化对齐人类序序偏好，在多基线之上提升VQA性能并降低MAE。


<details>
  <summary>Details</summary>
Motivation: 现有VQA多以单一MOS分数衡量，难以覆盖运动、审美、内容、清晰度等多维感知要素；同时模型难与人类序偏好与可解释性对齐。

Method: 1) 构建UltraVQA：涵盖多样UGC视频，按运动质量/幅度、审美、内容、清晰度五维打分，含细粒度子属性与基于汇总人评的GPT解释理由；每段视频≥3名标注者。2) 提出Analytic Score Optimization (ASO)：将质量评估重构为带正则的决策过程，推导闭式解，显式刻画人类评分的序数属性，使预测与人类排序偏好对齐，用作后训练目标提升离散质量打分。

Result: 在实验中，相比多数闭源API与开源模型，方法取得更优性能，并在质量预测MAE上显著下降；多维与可解释标注带来泛化与对齐收益。

Conclusion: 多维、可解释的人评标注结合基于强化/决策视角的解析式后训练，有助于提升VQA的准确性与人类偏好对齐；UltraVQA与ASO为推进多维VQA提供了数据与方法基础。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: 提出DODO：一种基于块式离散扩散的VLM，用并行解码加速OCR，在保持接近SOTA准确率的同时将推理速度提升至约3倍。


<details>
  <summary>Details</summary>
Motivation: 现有VLM做OCR多采用自回归解码，长文档需逐token前向，计算昂贵且慢；而OCR输出高度确定，理论上可用并行的扩散式解码提升效率，但现有掩码/全局扩散在严格序列匹配任务中会出现结构不稳定与同步误差。

Method: 提出DODO：采用块式（block）离散扩散，将序列分解为多个块并行/分阶段生成，缓解全局扩散的同步误差；作为VLM进行端到端视觉—文本建模，利用离散扩散进行并行解码以适配OCR的确定性输出。

Result: 在OCR任务上达到接近SOTA的准确率；相较自回归基线推理最高加速约3倍。

Conclusion: 块式离散扩散可在OCR这种确定性、严格匹配任务中实现并行快速解码且不显著牺牲精度；DODO验证了扩散替代自回归在该场景的可行性与效率优势。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 提出StereoAdapter-2：以ConvSS2D选择性状态空间更新器替代ConvGRU，实现四向扫描一次性长程视差传播；并构建8万张规模的合成水下立体数据集UW-StereoDepth-80K；在TartanAir-UW提升17%、SQUID提升7.2%，并在BlueROV2实测验证。


<details>
  <summary>Details</summary>
Motivation: 水下成像受波长相关衰减、散射与折射影响，导致与常规空中域存在强域差，现有利用单目基础模型+ConvGRU迭代细化的方法依赖序列门控与局部卷积，长程视差传播需多次迭代，难以覆盖大视差与低纹理区域。

Method: 1) 以基于选择性状态空间模型的ConvSS2D替代ConvGRU，采用左右/上下四向扫描，符合极线几何同时建模垂直一致性，一步实现长距离信息传播，线性复杂度；2) 构建UW-StereoDepth-80K：两阶段生成管线，语义感知风格迁移+几何一致的新视角合成，覆盖多基线、衰减与散射参数；3) 继承StereoAdapter的动态LoRA做域自适应。

Result: 在零样本设置下达SOTA：TartanAir-UW相对提升17%，SQUID提升7.2%；并在BlueROV2真实平台验证鲁棒性；代码与网站公开。

Conclusion: 以ConvSS2D实现高效一次性全局传播，结合大规模合成水下数据与动态LoRA，显著提升水下立体深度的跨域泛化与实际可用性。

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: 论文提出SemCovNet来缓解视觉语义表示中的“语义覆盖不均衡”(SCI)偏差。通过语义描述图(SDM)、描述符注意调制(DAM)与描述符-视觉对齐损失(DVA)，并用覆盖差异指数(CDI)衡量公平性。实验表明SemCovNet提升可靠性并显著降低CDI，实现更公平可解释的视觉学习。


<details>
  <summary>Details</summary>
Motivation: 现代视觉任务不只依赖类别标签，还需要丰富的可解释语义与上下文属性。然而现实数据在语义概念上呈长尾分布，导致模型对稀有但重要的语义学得差、推理偏颇。现有研究多关注类别不平衡，忽视了语义层面的覆盖不均，缺乏可度量与可校正的方法。

Method: 提出SemCovNet：1) 语义描述图（SDM）用于学习并组织语义描述子；2) 描述符注意调制（DAM）动态加权视觉与概念特征，聚焦欠覆盖语义；3) 描述符-视觉对齐损失（DVA）拉近视觉特征与语义描述的表征；4) 定义覆盖差异指数（CDI）以量化“覆盖度-错误率”对齐程度，作为公平性评价。

Result: 在多数据集上，SemCovNet相较基线提升鲁棒性与可靠性，并显著降低CDI，表现出更均衡的语义层面性能与更公平的误差分布。

Conclusion: SCI是可测量且可纠正的偏差。SemCovNet通过结构化语义表示、动态注意与对齐损失实现对SCI的系统缓解，并以CDI作为公平性指标，为推进语义公平与可解释视觉学习提供基础。

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual 是一个统一的图像与视频大模型架构，基于数十亿规模的社媒多模态数据，采用三阶段训练（MAE自监督、半监督标签分类、CLIP式对比学习），结合EViT提效与LLM2CLIP文本编码器，在多项图像/视频/检索基准上达SOTA，兼具鲁棒性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型要么单一模态、要么数据与训练策略有限，难以同时在图像与视频任务上扩展到超大规模并保持鲁棒性与效率；社媒海量数据噪声重、分布偏；传统文本编码器限制了跨模态检索与泛化能力。

Method: 1) 数据：从FB/IG获取>150亿图文对与100亿视频-标签对，进行平衡与去噪的精细化数据策展。2) 训练：三阶段流水线——(a) MAE自监督预训练；(b) 利用半监督方式进行话题/标签分类；(c) CLIP式图文/视频-文本对比学习，联合优化图像与视频。3) 架构：ViT骨干+EViT高效token重组以降算力；集成LLM作为文本编码器(LLM2CLIP)。

Result: 在ImageNet、Kinetics、HMDB51、MSCOCO等基准上达到或刷新SOTA；表现出对域移与对抗扰动的强鲁棒性；LLM2CLIP显著提升跨模态检索与泛化。

Conclusion: Xray-Visual 证明了在超大规模、多模态数据上，通过精心的数据策展、分阶段训练与高效架构设计，可以同时实现高精度、强鲁棒与高效率；集成LLM文本编码是进一步提升检索和泛化的关键方向。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 提出HSI-SC-NeRF：在固定相机+旋转物体条件下，实现多通道（可见-近红外）高光谱3D重建，兼顾高空间准确度与高光谱保真度，适配自动化农产品表型/质检流程。


<details>
  <summary>Details</summary>
Motivation: HSI能表征生化成分，3D几何提升形态分析，但两者大规模融合难，传统硬件复杂且不适配自动化表型平台；NeRF高效但多依赖移动相机，室内农环难以高通量与可复现。

Method: 设计固定相机、多视角高光谱采集：物体在特氟龙漫反射腔内旋转，均匀照明；用ArUco标定获取物体位姿，并通过模拟变换统一到相机坐标，转化为可用于标准NeRF训练的数据。提出多通道NeRF：跨全部光谱波段联合优化，采用复合光谱损失；两阶段训练：先几何初始化，后辐射度细化。

Result: 在三个农产品样本上，跨可见-近红外实现高空间重建精度与强光谱一致性，验证方法在后处理质检与表型分析中的可用性。

Conclusion: HSI-SC-NeRF在固定相机设置下实现高通量高光谱3D重建，兼顾几何与光谱质量，硬件简单、易于集成到自动化农业工作流，提升产线与表型平台的效率与可重复性。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 提出DiT的动态分块(tokenization)推理策略：早期用大块抓全局，后期用小块补细节；在保证感知质量与提示一致性的同时显著加速（最高约3.5×）。


<details>
  <summary>Details</summary>
Motivation: 固定大小patch在整个扩散去噪过程中恒定不变，忽视了不同时刻与内容复杂度的需求，导致计算冗余与低效。

Method: 在推理阶段根据去噪时间步与内容复杂度自适应调整patch尺寸：前期采用更粗的patch建模全局结构，后期逐步细化为更小patch捕获局部细节；在图像与视频生成中动态分配不同步的patch大小以减少token数与注意力计算。

Result: 在FLUX-1.Dev与万2.1上分别实现最高约3.52×与3.2×的速度提升，同时保持生成的感知质量与提示遵循度无显著下降。

Conclusion: 动态tokenization是对DiT推理阶段的有效加速方案，可在不牺牲质量的前提下降低计算成本；揭示了去噪早晚阶段对空间分辨率需求的差异，为后续自适应多尺度推理提供方向。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: 提出PRIMO：在多模态数据不完整场景下，通过监督式潜变量完成缺失模态并量化其对预测的影响；在缺失/完整两端均达基线级性能，并提供实例级影响度量。


<details>
  <summary>Details</summary>
Motivation: 现实中多模态数据常缺失或不同步，现有MLLM/多模态方法多数假设训练与推理时模态齐全，导致样本浪费与不确定性无法量化，因此需要既能利用不完整数据又能评估缺失模态对预测影响的方法。

Method: 构建监督式潜变量模型PRIMO：以潜变量表示缺失模态与观测模态在预测任务中的关系；训练时用完全与部分样本共同学习缺失模态的条件分布；推理时从学得的分布对缺失模态进行多次采样，得到边缘化预测分布以做预测，并基于跨采样预测的方差度量缺失模态对实例级预测的影响。

Result: 在合成XOR、Audio-Vision MNIST、以及MIMIC-III（死亡率与ICD-9预测）上，若某模态完全缺失，性能接近对应单模态基线；当所有模态可用时，性能接近多模态基线；可视化显示不同潜在补全对应一组合理标签。

Conclusion: PRIMO能在训练与推理中统一处理模态缺失，既不浪费不完整样本，又给出实例级影响量化；在多数据集上表现稳定并提供可解释的不确定性分析。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [10] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: 提出一种基于图像小块（patch）的空间作者归属框架，用于在人机协作抽象绘画中定位“人”与“机器人”的笔触来源；在单人-单机、15幅作品的数据集上，以扫描仪采集并采用留一画交叉验证，达到88.8%小块准确率、86.7%整画多数投票准确率，优于纹理与预训练特征基线。对协作画作，用条件香农熵衡量风格重叠，人工标注的混合区域熵显著更高（+64%，p=0.003），表明模型在识别混合作者而非简单失误。方法样本效率高，尽管当前特定于该人机组合，但为数据稀缺的人机创作流程取证与署名提供了方法学基础，并具备推广潜力。


<details>
  <summary>Details</summary>
Motivation: 随着具有能动性的AI参与创作，艺术家、藏家与法律场景对可验证的作者署名需求激增；现有作者归属多在整幅层面，难以在同一作品内区分不同作者贡献，尤其是人机协作绘画中“谁画了哪里”的取证空白仍待填补。

Method: 提出“基于patch的空间作者归属”流程：用平板扫描仪获取高分辨率图像；将图像切分为小块；在一名人类画家与一台机器人共15幅抽象画的数据集上，采用留一幅交叉验证训练分类器；并以多数投票汇聚到整画预测。与纹理特征和预训练特征（作为基线）比较。对协作作品，引入条件香农熵度量局部归属不确定性，以检验模型对混合风格区域的敏感性。

Result: 在patch层面准确率88.8%，整画多数投票86.7%，优于68.0%-84.7%的基线方法。人工标注的“人机混合”区域熵值比纯人或纯机作品高64%，差异显著（p=0.003），说明模型捕捉到混合作者信号而非分类失败。

Conclusion: 该方法在数据稀缺设定下对人机协作绘画实现了高精度、可解释的空间作者归属；通过熵度量能够揭示风格重叠与混合作者区域。虽当前模型对特定人机组合具特异性，但为未来扩展到更广泛的人机协作与法律取证应用提供了可复用的技术路线。

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

</details>


### [11] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG是一个将外部零件数据库与扩散Transformer结合的检索增强式单图3D生成与编辑框架，通过层级对比检索注入真实零件先验，并提供在规范空间中的遮罩级零件编辑；在Objaverse/ShapeNet/ABO上兼具精度提升与高效推理与交互编辑。


<details>
  <summary>Details</summary>
Motivation: 单图3D生成在零件层面存在两大难题：1) 学到的先验难以覆盖长尾零件几何并保持多视角一致性；2) 现有系统对精确且局部的可编辑性支持不足，往往需要整体重生成且易破坏非目标部分与一致性。

Method: 提出PartRAG：
- 检索增强：构建包含1,236个带零件标注资产的外部数据库；设计层级对比检索（Hierarchical Contrastive Retrieval, HCR），在零件与整体两种粒度上，将密集图像patch与3D零件潜变量对齐，在去噪过程中检索并注入多样且物理合理的零件样例。
- 可编辑表示：在共享规范（canonical）空间中引入遮罩式零件级编辑器，支持零件替换、属性细化、组合式更新；在不重生成整体的前提下保持非目标区域与多视角一致性。

Result: 在Objaverse上将Chamfer Distance从0.1726降至0.1528，F-Score从0.7472升至0.844；在ShapeNet与ABO上取得有竞争力结果。推理约38秒，交互式编辑5–8秒；定性上具备更清晰的零件边界、更好的细薄结构保真与对可动结构的鲁棒性。

Conclusion: 检索增强与规范空间的零件级可编辑性相结合，有效缓解长尾先验与多视角一致性难题，实现高质量、可交互的单图3D生成与编辑；PartRAG在精度与效率上均表现出色，并易于支持局部迭代编辑工作流。

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [12] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 提出Amber-Image压缩框架，将60层双流MMDiT的Qwen-Image无须从零训练压缩为轻量T2I模型（10B→6B），显著降参与算力成本，同时保持高保真与强文本渲染，评测媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: DiT在文生图上表现强，但计算昂贵、部署困难；需要在不重头训练的前提下，将大模型压缩为高效、低成本、可部署的模型，同时维持生成质量与文本对齐。

Method: 提出无须从零训练的压缩流程：1）对60层双流MMDiT采用与时间步相关的深度剪枝，得到Amber-Image-10B；保留层用局部权重平均重初始化，并进行分层蒸馏与全参微调。2）在此基础上将深层双流转为单流混合架构，单流以图像分支初始化，进而通过渐进式蒸馏与轻量微调得到Amber-Image-6B。

Result: 参数规模减少约70%；从10B到6B的压缩与训练全流程耗时<2000 GPU小时；在DPG-Bench与LongText-Bench等基准上，合成保真度与文本渲染优异，性能可与更大模型匹敌。

Conclusion: 该压缩框架以极低成本将大型双流DiT转化为轻量高效的T2I模型系列，显著降低部署门槛且基本不损失质量，为大规模T2I模型的实用化提供有效路径。

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [13] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: 提出StructCore：一种训练免、结构感知的图像级异常评分方法，替代UAD中常用的max pooling，通过结构化描述与对角马氏校准显著提升图像级检测性能。


<details>
  <summary>Details</summary>
Motivation: max pooling只依赖单个极值，忽略异常证据的分布与空间结构，导致正常与异常图像的得分重叠，限制了无监督异常检测的图像级判别能力。

Method: 对给定的异常分数图S，计算低维结构描述符φ(S)，编码分布与空间特征；基于仅来自良品训练集的对角马氏距离进行校准，得到图像级分数；不改变像素级定位流程，且无需额外训练。

Result: 在MVTec AD上图像级AUROC=99.6%，在VisA上=98.4%，显示对图像级异常检测的鲁棒提升，相比max pooling能利用被其忽略的结构线索。

Conclusion: 利用结构化签名与简单的马氏校准即可在无训练改动下超越max pooling，实现更稳健的图像级异常检测，同时保持像素级定位不变。

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [14] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 提出并发布Cholec80-port：基于Cholec80的高保真穿刺套管（trocar port）分割数据集与统一SOP，显式排除中心开口；并清洗统一其他公开集。几何一致的标注显著提升跨数据集鲁棒性，优于仅扩大数据量。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜中的trocar端口是相机固定、近似静态的高反光纹理结构，长期遮挡画面并吸引特征点，给基于几何的方法（拼接、重建、SLAM）带来外点干扰与跟踪不稳。然而公开手术数据很少有端口标签，且已有标注常把中央开口也遮住，破坏几何一致性（开口处可见的解剖区域被错误标成非解剖物）。

Method: 1) 从Cholec80构建高保真端口分割数据集；2) 制定严格SOP，仅标注端口套管/袖口区域，明确排除中央开口；3) 按同一SOP清洗并统一现有公开数据集；4) 在多数据集上做实验，比较几何一致标注与仅增大数据量对跨域泛化的影响。

Result: 采用几何一致（排除开口）的标注，在跨数据集测试中显著提升鲁棒性与性能；这一提升超过仅靠扩大数据规模所带来的增益。

Conclusion: 为端口这一对几何任务有害但常被忽视的类别提供标准化高质量标注与统一协议。遵循几何一致的SOP可有效减少动态/非解剖外点对下游几何管线的干扰，提升跨域稳定性与泛化能力，价值超过单纯扩大数据量。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [15] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: 提出CPL-VAD：基于跨伪标签的双分支弱监督视频异常检测方法，兼顾时间定位与语义分类，在XD-Violence与UCF-Crime上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 弱监督仅有视频级标签，难以同时实现精确的时间段异常定位与细粒度异常类别识别；现有方法在二者之间权衡且语义判别力不足。

Method: 构建双分支框架：1) 二分类异常检测分支进行片段级(anomaly vs. normal)定位；2) 类别分类分支利用视觉-语言对齐进行异常类别识别。两分支通过跨伪标签互相监督：检测分支为分类分支提供时间精确的伪标签，分类分支为检测分支提供语义区分的伪标签，从而融合时间与语义优势。

Result: 在XD-Violence与UCF-Crime数据集上，异常检测与异常类别分类均达到最新最优（SOTA），优于现有基线。

Conclusion: 跨伪标签的双分支协同有效提升弱监督异常检测：检测更精确、类别更可靠，验证了时序与语义互补的价值，可作为弱监督VAD的通用范式。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [16] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: 提出ComptonUNet，一种结合原始计数域与成像域的混合深度学习框架，在低光子统计与强背景下显著提升弱伽马暴（GRB）的定位精度。


<details>
  <summary>Details</summary>
Motivation: 远距离、微弱GRB可揭示早期恒星形成与高能过程，但受限于低光子数与强背景噪声，现有方法在统计稳健性与去噪之间难以平衡，导致弱源探测与定位准确性不足。

Method: 设计ComptonUNet混合框架：并行/联合处理原始探测计数（直接重建，具统计效率）与重建图像（U-Net类图像去噪与特征提取），在端到端训练中融合两域信息以鲁棒定位。通过低地轨道背景条件下的逼真仿真，生成含GRB事件与背景的训练/测试数据，评估定位性能。

Result: 在广泛的低统计、高背景场景中，ComptonUNet较现有方法显著降低定位误差，提升定位精度与稳健性，优于仅直接重建或仅图像网络的基线。

Conclusion: 混合计数-图像联合学习有效缓解弱信号与强噪声矛盾，提升微弱GRB定位能力；对未来低地轨道任务的弱源科学产出具有潜在价值。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [17] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 提出将射频（RF，如车载雷达）与3D Gaussian Splatting融合，用稀疏RF深度辅助初始化与训练，提升在少视角与恶劣条件下的3D重建与渲染质量与效率。


<details>
  <summary>Details</summary>
Motivation: 纯视觉GS依赖足够多视角进行高质量初始化与训练，在恶劣天气、弱光、遮挡等视觉线索匮乏时性能下降且初始化代价高；RF信号对天气/光照/遮挡更鲁棒，若能与GS结合可弥补视觉不足。

Method: 构建多模态框架：利用雷达等RF传感获取稀疏深度测量，通过高效深度预测生成高质量3D点云，用于在多种GS架构中初始化高斯原语（位置/尺度/方向等），并在训练中以RF信息约束结构一致性，从而实现更稳健的渲染。

Result: 数值实验表明，引入RF测量可在少视角与不利条件下显著改善结构准确性与渲染保真度，同时降低初始化与训练的计算/内存成本。

Conclusion: 将RF感知与GS渲染有机融合可在复杂环境中实现高保真、高效率的3D场景重建与渲染，优于仅依赖视觉的GS管线。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [18] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: 提出B^3-Seg：在无相机预设与零训练条件下，实现开放词汇的3D高斯点渲分割；以Beta–Bernoulli贝叶斯更新建模并用解析期望信息增益(EIG)主动选视角，具备自适应单调与次模性质，从而用贪心达到(1−1/e)近似最优取景；在多数据集上以秒级端到端推理取得接近监督方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS分割需预设相机、真值标签或代价高的再训练，难以满足电影与游戏资产实时编辑对低时延与交互性的需求；缺乏在开放词汇与训练/相机自由设定下的高效、可证明的信息利用框架。

Method: 把分割视为对每个高斯的前景概率进行序贯Beta–Bernoulli贝叶斯更新；基于解析形式的期望信息增益选择下一最佳观察视角，实现主动视角采样；证明EIG的自适应单调与次模性，从而使用贪心策略获得(1−1/e)近似最优的取景策略；全流程无需再训练与预设相机。

Result: 在多个数据集上，秒级完成端到端开放词汇分割，精度与昂贵监督方法相当；交互式流程下稳定提升，验证了信息利用效率与实际可用性。

Conclusion: B^3-Seg在3DGS中实现了快速、训练/相机自由且可证明高效的开放词汇分割，通过贝叶斯更新与EIG驱动的主动取景，在保证理论近似最优性的同时达到接近监督方案的性能，适用于实用的交互式编辑场景。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [19] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出BadCLIP++，在仅0.3%投毒下对多模态对比学习模型实现极高、持久且隐蔽的后门（数字环境ASR≈99.99%），在连续微调与多种防御下仍保持>99.90% ASR且干净准确率几乎不降，并在物理场景与水印移除防御下仍具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对比学习（如CLIP）后门攻击在强检测与持续微调下易失效，原因在于跨模态触发不一致暴露模式、低投毒率导致梯度稀释与后门遗忘。这两类耦合因素未被系统建模与解决，亟需同时提升隐蔽性与持久性的统一框架。

Method: 统一框架BadCLIP++：
- 隐蔽性：语义融合的QR微触发，贴近任务相关区域、几乎不可感，同时保持干净数据统计并形成紧凑触发分布；目标对齐的子集选择，在低注入率下放大有效信号。
- 持久性：触发嵌入稳定化（半径收缩+质心对齐）；参数稳定化（曲率控制+弹性权重固化EWC），使解停留在低曲率宽盆地以抗微调遗忘。
- 理论分析：在可信区间内，干净微调与后门目标的梯度共向，给出攻击成功率退化的非增上界。
- 实验：0.3%投毒下ASR≈99.99%，跨19种防御ASR>99.90%，干净准确率下降<0.8%；物理攻击成功率≈65.03%，对水印移除防御鲁棒。

Result: 在极低投毒率（0.3%）下数字场景ASR 99.99%，较基线提升约11.4个百分点；跨19种防御后ASR仍>99.90%，干净准确率下降<0.8%；物理世界攻击成功率约65.03%，且对水印移除型防御仍有效。

Conclusion: BadCLIP++通过微触发设计、数据子集选择与表示/参数两层稳定化，兼顾隐蔽与持久，理论上证实与实证上均显示在低投毒率、强防御与持续微调下仍具极高ASR与鲁棒性，为多模态对比学习后门攻击设立新基线。

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [20] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 提出NRGS-SLAM：一种基于3D Gaussian Splatting的单目非刚性内窥SLAM，通过可学习“形变概率”与贝叶斯自监督解耦相机运动与组织形变，实现更稳健跟踪与高保真重建，在多数据集上显著优于SOTA（姿态RMSE最高降50%）。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景软组织持续变形，破坏刚性假设，造成相机自运动与场景内在形变耦合，传统非刚性单目SLAM多用稀疏/低保真表示、解耦不足，导致跟踪漂移与重建质量差。

Method: 1) 变形感知3D高斯地图：为每个高斯增加可学习“形变概率”，并用贝叶斯自监督优化，无需外部非刚性标签；2) 可形变跟踪：先基于低形变区域做粗到细位姿估计，再进行逐帧高效形变更新；3) 可形变建图：渐进扩展与细化高斯地图，在表达能力与计算开销间折中；4) 统一鲁棒几何损失：融合外部几何先验，缓解单目非刚性问题的病态性。

Result: 在多内窥公用数据集上，相机位姿估计更准（RMSE最高降低50%），并获得更高质量的照片级重建；消融实验证明关键设计有效。

Conclusion: NRGS-SLAM通过基于3D高斯的变形建图与贝叶斯自监督形变解耦，实现稳健的单目非刚性内窥SLAM，兼顾跟踪精度与重建质量，优于当前方法；代码将公开。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [21] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 提出VIG(Visual Information Gain)度量图像对LVLM预测不确定性降低的贡献，并据此进行选择性训练，提升视觉扎根、减轻语言偏置，用更少监督获得更优表现。


<details>
  <summary>Details</summary>
Motivation: LVLM常因语言偏置而忽视图像，现有方法缺乏对“哪些样本/词真正受益于视觉”的量化指标，难以精确改进与高效用数据选择。

Method: 提出基于困惑度变化的VIG：比较含图像与不含图像条件下的预测不确定性差异，细化到样本级与token级，识别颜色、空间关系、属性等视觉扎根要素；据此设计VIG引导的选择性训练，仅优先使用高VIG的样本与token进行训练。

Result: 在相同或更少监督下，选择高VIG样本/词进行训练可更好对齐视觉证据，减轻语言偏置并在相关评测上取得更优性能。

Conclusion: VIG为LVLM提供可解释、可量化的视觉增益评估，并作为数据与训练选择信号显著提升视觉扎根与效率，证明聚焦“视觉信息量高”的数据能以更少监督获得更好效果。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [22] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: 提出EntropyPrune：用矩阵熵视角在MLLM中选择“熵坍塌层”并按令牌信息量裁剪，显著加速推理（FLOPs降68.2%）且保持96%性能；通过双Gram矩阵光谱等价将熵计算复杂度大幅降低，适配高分辨率与视频模型。


<details>
  <summary>Details</summary>
Motivation: 视觉令牌数量巨大导致MLLM推理成本高；现有令牌裁剪通常在经验性固定层进行，缺乏可解释性与可迁移性；注意力图驱动的方法稳定性与普适性有限，需要一种有理论依据且高效的裁剪准则。

Method: 从矩阵熵角度分析视觉表示的信息量随层深变化，发现信息骤降且一致的“熵坍塌层”（ECL），据此确定裁剪阶段；定义基于矩阵熵的单令牌信息价值度量，在ECL处对低信息令牌进行裁剪而不依赖注意力；利用双Gram矩阵的谱等价性近似/加速熵计算，将复杂度显著降低（理论最高64倍）。

Result: 在多种多模态基准上优于SOTA裁剪方法的精度-效率；在LLaVA-1.5-7B上FLOPs减少68.2%，保持原始性能的96.0%；在高分辨率与视频模型上同样有效，显示良好鲁棒性与可扩展性。

Conclusion: 矩阵熵提供了可解释、可迁移的裁剪阶段判据（ECL），EntropyPrune能在不依赖注意力图的前提下高效裁剪冗余视觉令牌，并通过谱等价实现大幅加速，在不同MLLM及设置中实现优异的效率与性能权衡。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [23] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: 论文提出通过几何视角提升文本到图像（T2I）生成的多样性：在CLIP嵌入空间中，将多样性分解为与提示相关（语义）和与提示无关（背景/风格）两个正交方向，并通过几何感知的球面采样与扩展预测在生成轨迹中同时扩大两轴投影分布，实现更高多样性且几乎不损伤保真度与对齐。


<details>
  <summary>Details</summary>
Motivation: 现有T2I虽语义对齐高，但生成样本往往缺乏多样性，限制用户选择并可能放大社会偏见。多数方法用熵或去重损失来鼓励差异，但缺乏对“何种差异”的控制，容易牺牲图像质量或语义对齐。作者希望用可解释的几何分解，分别控制提示相关与无关的变化来源，达成可控且稳健的多样性提升。

Method: 提出Geometry-Aware Spherical Sampling（GASS）。在CLIP图像嵌入空间中：1）将多样性分解为两条正交轴：文本嵌入方向（提示相关语义变化）与其正交方向（提示无关变化，如背景/风格）。2）在这两个方向上显式控制投影分布的“张开度”（projection spread），通过球面采样构造目标分布，并在扩散/流模型的采样过程中使用“扩展预测”（expanded predictions）引导生成轨迹，使生成图像嵌入在两轴上均匀分散。方法可插拔，适用于不同冻结骨干（U-Net或DiT，扩散或流）。

Result: 在多种基准与多种冻结T2I骨干上，GASS在不显著降低图像保真度与文本对齐的前提下，显著提高了生成多样性；同时实现了对提示相关与无关多样性的解耦与可控提升。

Conclusion: 通过在CLIP嵌入空间进行正交几何分解并在生成过程中施加球面采样与扩展预测引导，能够解耦并同时增强提示相关与无关的多样性，兼顾质量与对齐，具有通用、可插拔的实用价值。

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [24] [HiMAP: History-aware Map-occupancy Prediction with Fallback](https://arxiv.org/abs/2602.17231)
*Yiming Xu,Yi Yang,Hao Cheng,Monika Sester*

Main category: cs.CV

TL;DR: HiMAP是一个无需跟踪ID的轨迹预测框架，通过将历史检测转为时空不变的历史占用图并用历史查询模块检索特定体素历史，结合DETR式解码器输出多模态未来轨迹；在无跟踪场景显著优于强基线，并在Argoverse2上接近基于跟踪的方法。


<details>
  <summary>Details</summary>
Motivation: 多数预测器依赖MOT与ID一致性，当发生遮挡、身份切换、漏检时，历史关联断裂，预测劣化并带来安全风险。需要一种对跟踪失败鲁棒、可在无ID条件下稳定输出的预测框架，作为实际系统的稳健后备方案。

Method: 1) 将过去的检测转化为时空不变的历史占用图（历史栅格表示，去除了ID依赖）。2) 设计历史查询模块：依据当前智能体状态，迭代地从无标签占用表示中检索与该智能体对应的历史片段，得到时间地图嵌入。3) 将时间嵌入与最终查询以及地图上下文一起输入DETR风格解码器，生成多模态未来轨迹。4) 支持流式推理与可复用编码。

Result: 在Argoverse 2数据集上，无需ID的情况下达到与跟踪基线相当的整体性能；在无跟踪设定下显著领先强基线（相对提升：FDE 11%、ADE 12%、MR 降低4%，相较于微调的QCNet）。能为所有目标同时输出稳定预测，无需等待跟踪恢复。

Conclusion: HiMAP通过历史占用图与历史查询机制摆脱对ID的依赖，实现对MOT失败的鲁棒轨迹预测，适合作为自动驾驶中安全关键的后备预测模块，并在标准数据集上验证了有效性与效率。

Abstract: Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.

</details>


### [25] [Inferring Height from Earth Embeddings: First insights using Google AlphaEarth](https://arxiv.org/abs/2602.17250)
*Alireza Hamoudzadeh,Valeria Belloni,Roberta Ravanelli*

Main category: cs.CV

TL;DR: 评估AlphaEarth地球嵌入在10 m尺度上指导深度学习回归以推断区域地表高程；用U-Net/U‑Net++作轻量解码器，训练R^2≈0.97，测试受分布偏移影响但U‑Net++泛化更好（R^2=0.84，RMSE≈16 m，偏差更小）；嵌入捕捉可迁移地形模式，但需校正偏差以提升区域可迁移性。


<details>
  <summary>Details</summary>
Motivation: 传统高程制图依赖昂贵或稀缺数据（例如LiDAR/高精度DSM）与复杂建模，难以在大范围、异质区域稳定泛化。地理多模态预训练“地球嵌入”或可在缺标或低成本情境下提供可解码的地形信息，但其用于连续高程回归的有效性与可迁移性尚不清楚。

Method: 采用10 m分辨率AlphaEarth Embeddings作为输入特征，以高质量DSM为真值；使用U‑Net与U‑Net++作为轻量卷积解码器进行端到端回归；在存在训练/测试区域高程分布差异的设定下评估训练与测试表现，并比较泛化与偏差。

Result: 两种网络训练性能均高（R^2=0.97），显示嵌入包含可解码的高程相关信号；测试集上受分布移位影响性能下降，但U‑Net++优于U‑Net（R^2=0.84 vs 0.78，测试中位差-2.62 m vs -7.22 m，RMSE约16 m），提示其对分布不匹配更鲁棒；仍存在残余偏差与误差。

Conclusion: AlphaEarth Embeddings能有效引导基于卷积的高程回归并捕获可迁移地形模式；U‑Net++在泛化上更优。但区域迁移时仍有系统偏差与RMSE需改进，建议加入偏差校正/域自适应等策略以提升可移植性。

Abstract: This study investigates whether the geospatial and multimodal features encoded in \textit{Earth Embeddings} can effectively guide deep learning (DL) regression models for regional surface height mapping. In particular, we focused on AlphaEarth Embeddings at 10 m spatial resolution and evaluated their capability to support terrain height inference using a high-quality Digital Surface Model (DSM) as reference. U-Net and U-Net++ architectures were thus employed as lightweight convolutional decoders to assess how well the geospatial information distilled in the embeddings can be translated into accurate surface height estimates. Both architectures achieved strong training performance (both with $R^2 = 0.97$), confirming that the embeddings encode informative and decodable height-related signals. On the test set, performance decreased due to distribution shifts in height frequency between training and testing areas. Nevertheless, U-Net++ shows better generalization ($R^2 = 0.84$, median difference = -2.62 m) compared with the standard U-Net ($R^2 = 0.78$, median difference = -7.22 m), suggesting enhanced robustness to distribution mismatch. While the testing RMSE (approximately 16 m for U-Net++) and residual bias highlight remaining challenges in generalization, strong correlations indicate that the embeddings capture transferable topographic patterns. Overall, the results demonstrate the promising potential of AlphaEarth Embeddings to guide DL-based height mapping workflows, particularly when combined with spatially aware convolutional architectures, while emphasizing the need to address bias for improved regional transferability.

</details>


### [26] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 提出并实测一套面向货运优先（FSP）的路侧多模态感知系统，融合路口与路段两套子系统的LiDAR+相机，通过无线同步、聚类/深度检测与卡尔曼跟踪及地理坐标配准，实现货车类型/位置/速度的高精度实时感知。


<details>
  <summary>Details</summary>
Motivation: FSP需要对接近信号交叉口的货运车辆进行可靠、及时的检测与运动估计（类型、位置、速度），以便精准触发和优化优先控制；现有感知在稳定性、空间对齐和车道级定位方面不足，亟需可部署的基础设施感知方案。

Method: 构建基础设施侧多模态感知架构：在路口与中段分别部署LiDAR与相机子系统，并通过无线通信实现时钟同步与数据联动；感知管线结合基于聚类与深度学习的检测方法，配合卡尔曼滤波跟踪以获得稳定实时轨迹；将LiDAR测量注册到大地参考坐标，实现车道级定位与跨设备一致跟踪。

Result: 现场测试显示系统可在高时空分辨率下稳定监测货运车辆运行，准确输出类型、位置与速度，并在跨子系统范围内保持一致追踪与定位。

Conclusion: 所提系统在真实道路环境中具备可靠性与实时性，为FSP提供可落地的基础设施感知能力与部署实践经验，支持更有效的优先控制策略设计与实施。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [27] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: 提出EA-Swin与EA-Video，显著提升AI生成视频检测的准确率与泛化性（0.97-0.99），通过嵌入无关的Swin式时空建模和大规模跨分布基准，优于现有方法5-20%。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在面对Sora2、Veo3等高保真生成视频时失效：依赖浅层嵌入轨迹、图像级自适应或昂贵的多模态大模型，导致鲁棒性差、泛化弱、计算重。需要一种既能直接利用通用视频嵌入、又能高效捕捉时空依赖且可跨生成器泛化的方法与基准。

Method: 提出Embedding-Agnostic Swin Transformer（EA-Swin）：在预训练视频嵌入上进行直接时空建模，采用因子化的窗口注意力（空间/时间维分解+滑动窗口）以兼容ViT式patch编码器并提升效率与可扩展性。同时构建EA-Video数据集（13万视频），融合新采集与整理的多源数据，覆盖商业与开源生成器，并设立“未见生成器”划分进行跨分布评测。

Result: 在主要生成器上准确率0.97-0.99，较SoTA（通常0.8-0.9）提升5-20%；在未见分布上仍保持强泛化。计算开销相较使用MLLM的方法更低，且对不同嵌入/编码器保持兼容。

Conclusion: EA-Swin结合EA-Video提供了可扩展、鲁棒的现代AI视频检测方案，在跨生成器、跨分布场景下显著优于现有方法，展示了嵌入无关的时空Transformer对高保真生成视频检测的有效性。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [28] [Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution](https://arxiv.org/abs/2602.17277)
*Ruoyi Zhang,Jiawei Yuan,Lujia Ye,Runling Yu,Liling Zhao*

Main category: cs.CV

TL;DR: 提出PESTGAN，用物理先验与时空对抗判别提升台风卫星序列4×超分，结构与感知质量更佳，并保持像素精度可比。


<details>
  <summary>Details</summary>
Motivation: 现有深度SR多把卫星云图序列当通用视频，忽视云系受大气动力约束（如涡度方程），导致运动不一致、结构失真、物理不合理。需要在SR中显式/隐式编码物理规律以提升气象可信度。

Method: 构建PESTGAN：1) 采用解耦式生成器，将“物理解”与“纹理解”分离；其中引入PhyCell模块，用受约束卷积近似涡度方程，将物理动力学编码为隐式潜表征；2) 采用双判别器：空间判别器保真度与纹理真实感，时间判别器约束跨帧运动一致性；3) 在Digital Typhoon数据集上做4×超分训练与评估。

Result: 在Digital Typhoon上，PESTGAN在结构保真与感知质量上优于现有方法，同时像素级指标（如PSNR/SSIM等）保持竞争力；重建的云系形态更符合气象学，物理一致性更强。

Conclusion: 将物理约束嵌入时空GAN可显著改善台风卫星图像SR的物理可信度与视觉质量；PESTGAN在不牺牲像素指标的前提下，重建出更具气象合理性的云结构，适合用于TC生成、增强与路径监测等应用。

Abstract: High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.

</details>


### [29] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: 提出“附着锚（attachment anchors）”作为结直肠腹腔镜手术中组织与解剖附着关系的结构化中间表示，用以标准化局部参考系，从而显著提升基于学习的抓取点预测，尤其在分布外场景中优于仅基于图像的方法。


<details>
  <summary>Details</summary>
Motivation: 结直肠手术场景复杂、时长长且在研究中代表性不足，但存在大量重复的组织牵引操作，是自动化支持的理想对象。现有方法在变换多样的视角与解剖差异下，抓取点预测不稳定且不具鲁棒性，需要一种能减少不确定性、可泛化的表示。

Method: 提出“附着锚”作为编码组织与其解剖附着之间局部几何与力学关系的表示，将手术场景归一化到一致的局部参考系；从腹腔镜图像中预测该表示，并将其集成到一个基于机器学习的抓取框架中，与仅图像输入的基线进行比较。

Result: 在90例结直肠手术数据集上，使用附着锚的模型在抓取点预测上优于仅图像基线；在分布外条件（未见过的手术类型与术者）中提升尤为明显。

Conclusion: 附着锚作为中间表示能有效降低抓取点预测的不确定性，提升泛化与鲁棒性，是实现学习驱动的结直肠组织操作的有前景路径。

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [30] [Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline](https://arxiv.org/abs/2602.17322)
*Mohamed Dhouib,Davide Buscaldi,Sonia Vanier,Aymen Shabou*

Main category: cs.CV

TL;DR: 提出一种生成高质量篡改文档图像的数据合成框架，通过两个辅助网络与精心设计的管线，生成多样且逼真的伪造文本样本，用以缓解训练数据稀缺并显著提升篡改检测模型在多架构与多数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的文档篡改合成数据种类单一、视觉伪影明显，与真实攻击分布不匹配，导致检测模型泛化性差、对真实数据表现不佳。需要一种更逼真、更多样的篡改数据生成方法来缩小合成与现实之间的差距。

Method: 训练两个辅助网络并嵌入到数据生成管线中：1) 文本裁块对比网络：基于对比学习，提出新的正负样本定义来学习判别文本裁块相似性的嵌入；2) 裁块质量评估网络：判断裁块是否紧致包围目标字符而不截断或包含邻近字符。结合两者，设计自动化合成流程，生成高质量、多样化的篡改文档图像数据。随后在相同源图像和一致训练协议下，用本方法与既有方法生成的数据分别训练多种模型进行对比。

Result: 在多个开源数据集上的评测表明，用该管线生成的数据训练的模型在不同架构与数据集上都取得一致的性能提升，相比现有基于规则的生成方法更具鲁棒性与泛化性。

Conclusion: 通过对比学习与裁块质量控制的双辅助网络合成框架，可显著提升文档篡改检测训练数据的真实性与多样性，进而稳定提升下游检测模型跨数据集、跨架构的表现。

Abstract: Detecting tampered text in document images is a challenging task due to data scarcity. To address this, previous work has attempted to generate tampered documents using rule-based methods. However, the resulting documents often suffer from limited variety and poor visual quality, typically leaving highly visible artifacts that are rarely observed in real-world manipulations. This undermines the model's ability to learn robust, generalizable features and results in poor performance on real-world data. Motivated by this discrepancy, we propose a novel method for generating high-quality tampered document images. We first train an auxiliary network to compare text crops, leveraging contrastive learning with a novel strategy for defining positive pairs and their corresponding negatives. We also train a second auxiliary network to evaluate whether a crop tightly encloses the intended characters, without cutting off parts of characters or including parts of adjacent ones. Using a carefully designed generation pipeline that leverages both networks, we introduce a framework capable of producing diverse, high-quality tampered document images. We assess the effectiveness of our data generation pipeline by training multiple models on datasets derived from the same source images, generated using our method and existing approaches, under identical training protocols. Evaluating these models on various open-source datasets shows that our pipeline yields consistent performance improvements across architectures and datasets.

</details>


### [31] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: Polaffini利用分割得到的解剖结构质心作为一一对应特征点，闭式求解全局/局部仿射并在log-Euclidean框架下组合为可微分的多仿射变换，实现快速、稳健、精确的解剖驱动配准，优于常见基于强度的方法且能改进后续非线性配准初始化。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像配准多依赖强度相似性等替代指标，难以保证解剖一致性；基于特征的方法虽更符合解剖语义，但过去因特征提取不稳而式微。随着深度学习分割的成熟，可即时得到可靠细粒度解剖分割，促使重新探索解剖驱动的配准方法。

Method: 利用预训练分割网络获得解剖区域；从每个分割区域提取质心作为具有1-1对应的特征点；先通过闭式解求全局仿射匹配，再进行局部仿射（多仿射）匹配；在log-Euclidean框架中对这些仿射进行平滑融合，确保可微分（diffeomorphic）与平滑性可调；可作为独立配准或非线性配准的预对齐。

Result: 在与主流强度驱动配准的对比中，Polaffini在结构对齐精度上更佳，作为非线性配准的初始化也显著提升后续效果；方法运行快速、鲁棒性强。

Conclusion: 借助现代分割提供的稳定解剖特征，Polaffini以质心驱动的多仿射与log-Euclidean融合实现高效、稳健且可微分的解剖一致配准，适合嵌入医学影像处理流程，既可独立使用也可提升下游非线性配准性能。

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [32] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: 研究提出首个覆盖南美、空间分辨率10米的树作物（tree crops）制图，利用S1/S2多模态时序深度学习，识别约1100万公顷树作物，其中23%与2000–2020年森林损失关联；现有支持EUDR的监管地图常将既有农业（尤其小农复合经营）误判为“森林”，导致潜在误报与不公平惩罚；本成果提供高分辨率基线以降低误报并支撑更公平的保护政策。


<details>
  <summary>Details</summary>
Motivation: 零砍伐政策（如欧盟无砍伐法规EUDR）需要精确区分森林与各类树作物/农林复合系统；但缺乏高分辨率、能区分多样农业系统与森林的地图，导致监管误报与小农受罚风险。

Method: 构建多模态、时空深度学习模型，训练于Sentinel-1雷达与Sentinel-2光学影像时间序列，在南美生成10米分辨率树作物分类图，并与2000–2020年森林覆盖损失数据及现有监管地图进行对比分析。

Result: 生成首张南美10米树作物图，识别约1100万公顷树作物；其中约23%与2000–2020森林损失相联系。发现当前用于EUDR的监管地图经常把既有农业用地（尤其小农农林复合/小规模种植园）归为“森林”。

Conclusion: 提供高分辨率、面向政策执行的树作物基线可减少将农业误判为森林导致的虚假砍伐警报，降低对小农的不公平处罚，提升保护政策的有效性、包容性与公平性。

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [33] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 提出基于RetNet的解码器式HTR模型DRetHTR，替代Transformer注意力以线性时间/内存解码，在不降精度下实现1.6–1.9倍更快、38–42%更省内存，并在多数据集上达到SOTA或具竞争力的CER。


<details>
  <summary>Details</summary>
Motivation: 现有HTR多用Transformer，但解码需维护随输出增长的KV缓存，导致推理慢、占内存大；希望在不牺牲精度下提升速度与内存效率。

Method: 用RetNet替代软最大注意力为“软最大免”的retention机制，并加入多尺度顺序先验，避免增长型KV缓存，使时间与内存均线性于输出长度；提出逐层gamma缩放以在更深层扩大有效保留视野，前层关注短程、后层建模长程，从而弥补去掉softmax后的灵活性不足。

Result: 与等参量的Transformer解码器基线相比，推理加速1.6–1.9倍、内存减少38–42%，精度无损；在IAM-A(英)2.26% CER、RIMES(法)1.81%、Bentham(英)3.46%达当前最佳；READ-2016(德)4.21%具竞争力。

Conclusion: 基于RetNet的解码器模型在保持Transformer级HTR精度的同时显著提升解码速度和内存效率，验证了无需KV缓存的retention机制与层级gamma缩放对局部-全局建模的有效性。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [34] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: SpectralGCD 用 CLIP 的图像-概念相似度作为统一跨模态表示，并通过谱过滤与双向蒸馏，在保持语义对齐的同时高效发现新类别，达到与 SOTA 相当或更优的精度且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有 GCD 方法若仅基于图像特征训练参数化分类器，易过拟合已知类；多模态方法虽能引入文本语义改善泛化，但通常将模态独立处理、代价高且对视觉伪线索敏感。需要一种既高效又能显式依托语义的跨模态方法来稳健发现新类别。

Method: 1) 将每张图像表示为来自大型、与任务无关的“语义概念字典”的混合，使用 CLIP 提供的图像-概念相似度作为统一跨模态表示；2) 提出“谱过滤”：基于强教师模型对软化相似度的跨模态协方差矩阵，自动选择并保留与任务相关的概念，保证学生表示的语义质量；3) 设计正向与反向的同源知识蒸馏，确保学生的跨模态表示既语义充分又与教师良好对齐；4) 在此基础上进行类别发现与分类。

Result: 在六个基准上达到与 SOTA 可比或显著更优的准确率，同时计算成本大幅降低。代码已开源。

Conclusion: 统一的跨模态相似度表示结合谱过滤与双向蒸馏，能在不牺牲效率的前提下增强 GCD 的语义对齐与新类发现能力，提供高效、可扩展的多模态 GCD 方案。

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [35] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本文是针对配备RGB相机的无人机与光学遥感领域的综述，系统汇总任务、方法、能力、数据集与实用见解，面向入门与跨领域研究者提供高层次导航与重点指引。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉的飞速发展与无人机应用的普及推动了光学遥感的进步，但现有综述多聚焦单一任务或方法，缺乏覆盖任务谱系、能力边界、数据集与实践要点的整体视角，给新入门者带来信息碎片化与选题迷茫的问题。

Method: 以RGB相机为主的光学遥感为范围，梳理无人机及相关应用的主要任务与方法；汇编公开数据集与评价维度；总结各任务的发展趋势与经验洞见，形成面向新手的高层综述与指引。

Result: 形成一份全面的领域概览：列举并比较多种任务与方法，整理关键数据集与评测信息，并提炼实践建议与研究洞察，展示RGB光学遥感在多种应用中的能力版图。

Conclusion: 该综述弥补了现有文献缺少整体视角的问题，为研究者提供高层次导航，帮助快速定位兴趣方向与资源；作者声称目前尚无同类从全局角度覆盖该领域的综述。

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [36] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: 提出EAGLE：一种无需微调的专家增强注意力引导框架，将专业异常检测模型的输出用于引导多模态大模型，实现更准确的异常检测与可解释文本描述。实验在MVTec-AD与VisA上验证，对多种MLLM均有提升，接近微调方法表现。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测常被深度学习方法简化为二分类，缺乏语义可解释性；而MLLM能生成细粒度语言分析，但现有方法往往需高成本微调，且检测准确性不稳定，难超越轻量级专家模型。需要一种既提升检测性能又保留可解释性的、低成本的MLLM方案。

Method: 提出EAGLE（Expert-Augmented Attention Guidance）：不更新参数，利用专家模型（specialist detector）的输出作为引导信号，影响MLLM对图像异常区域的关注与推理，从而获得准确检测与可解释描述；并通过分析中间层注意力分布，量化MLLM对异常区域的注意力集中度与对齐程度。

Result: 在MVTec-AD与VisA数据集上，EAGLE在多种MLLM上均提升异常检测性能，同时生成可解释的异常文本描述；其指标与需要微调的方案相当。注意力分析显示成功检测与异常区域注意力集中增强相关，EAGLE能促进这种对齐。

Conclusion: EAGLE无需微调即可把专家模型知识注入MLLM，实现性能与可解释性的兼得，并在标准工业异常检测基准上取得接近微调方法的效果；注意力研究揭示其通过提升对异常区域的注意力集中来发挥作用。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [37] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 提出Local-EndoGS：面向单目内镜、任意相机运动的高质量4D重建框架，通过窗口化的局部可变形表示与粗到细初始化，在无可靠双目/精确SfM时仍稳健；并引入长程像素轨迹与物理先验，三数据集上在外观与几何上均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有隐式表示或3D Gaussian方法多假设固定视角，且依赖双目深度或高精度SfM做初始化，难以应对临床中单目、相机大幅运动的变形场景；需要一种既能扩展到长序列又能在缺乏可靠初始化时稳定优化的方案。

Method: 1) 窗口化全局表示：为每个时间窗口分配局部可变形场（local deformable scene models），以渐进方式覆盖整段视频，兼顾可扩展性与大运动；2) 无可靠SfM/双目时的粗到细策略：融合多视几何、跨窗口信息与单目深度先验，提供鲁棒初始对齐与优化基础；3) 约束与先验：引入长程2D像素轨迹一致性约束与物理运动先验，提升形变的时空连贯性与物理合理性；4) 基于Gaussian/神经表示的高质量外观与几何重建（与EndoGS思路相承）。

Result: 在三个公开内镜数据集、含不同相机运动与组织变形的设置下，Local-EndoGS在外观质量与几何精度上稳定超越当下SOTA；消融实验证明窗口化表示、粗到细初始化、跨窗口/轨迹与物理先验等组件均有显著贡献。

Conclusion: Local-EndoGS为单目、任意相机运动下的内镜4D重建提供了稳健高质的方案，突破对双目深度或精确SfM的依赖，并具备长序列可扩展性；代码将开源，具备实际临床场景的潜在应用价值。

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [38] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出面向二维量子材料显微图像的物理感知多模态框架：用物理仿真数据生成器Synthia合成薄膜干涉下的片层图像，构建物理指导的多模态指令数据集QMat-Instruct，并通过带物理先验注意力模块的QuPAINT进行指令微调，同时发布跨材料/基底/成像条件的基准QF-Bench，以缓解标注稀缺与跨域泛化差。


<details>
  <summary>Details</summary>
Motivation: 二维量子材料在光学显微下层数相关对比度微弱、跨实验室与设备差异大且标注数据稀缺；通用视觉模型缺乏光学物理先验，难以泛化到新材料或成像条件。需要数据与模型两方面注入物理先验并降低对专家标注的依赖。

Method: 1) Synthia：基于薄膜干涉物理的合成数据引擎，生成多样、逼真的量子材料薄片光学响应图像；2) QMat-Instruct：大规模、物理知情的多模态指令问答数据，教MLLM理解薄片外观与厚度；3) QuPAINT：在多模态架构中加入Physics-Informed Attention，将视觉特征与光学先验融合，学习更鲁棒的片层表征；4) QF-Bench：覆盖多材料、基底与成像设置的统一评测基准与协议。

Result: 合成数据提升数据效率与多域鲁棒性；物理知情指令微调与注意力融合使模型在片层识别、厚度估计等任务上相对现有方法取得更好、可复现的跨材料与跨设备泛化表现（文中建立了标准化评测并显示优于基线）。

Conclusion: 在二维量子材料视觉表征中，引入基于物理的合成数据、物理指导的多模态指令学习与物理先验注意力，可显著缓解标注稀缺与域转移问题；QF-Bench为后续研究提供统一比较平台，方法对新材料与成像条件具有更强泛化潜力。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [39] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 提出PixTrace像素坐标追踪与CopyNCE几何引导对比损失，利用可验证的像素级映射来提升图像拷贝检测中的细粒度对应学习，在DISC21上实现SOTA并具可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 现有自监督的视图级对比方法无法充分学习细粒度对应，遇到复杂编辑（裁剪、仿射、非刚性、复合编辑等）时易失配，导致监督噪声和性能瓶颈。作者观察到编辑内容通常保留可追踪的几何关系，可被用来缓解上述问题。

Method: 1) PixTrace：在数据增广/编辑流程中显式维护像素坐标的前后映射，提供被验证的像素-像素对应；2) CopyNCE：利用PixTrace得到的重叠比例与可验证映射，约束patch级相似性，将重叠高的patch对作为强正样本、低重叠为负或弱正，并对对比损失做几何加权，降低错误监督；3) 将像素级可追踪性与patch级表征学习结合，以更鲁棒地训练matcher与descriptor。

Result: 在DISC21数据集上取得新的SOTA：matcher达到88.7% uAP / 83.9% RP90，descriptor达到72.6% uAP / 68.4% RP90；同时在可解释性方面优于现有方法（如更清晰的对应热力图/匹配可视化）。

Conclusion: 通过引入可验证的几何追踪与几何引导对比损失，显著提升ICD对复杂编辑的鲁棒性与可解释性，证明将像素级可追踪性融入自监督对比学习是有效的方向。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [40] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 用深度图+基础姿态估计替代传统基于器官轮廓与FE形变模型的流程，并以NICP完成非刚性配准；在真实病例中获得约9.9 mm平均误差，刚性+NICP优于仅刚性，提供轻量、工程友好的临床可用方案。


<details>
  <summary>Details</summary>
Motivation: 现有腹腔镜肝手术的AR配准多依赖器官轮廓与有限元（FE）形变模型，实施复杂、需要专门建模与参数整定，工程门槛高；希望在不牺牲临床精度的前提下降低系统复杂度与专家依赖。

Method: 1) 将腹腔镜深度图与“基础”通用姿态估计器结合，进行相机-肝脏的初始刚性位姿估计；2) 以非刚性ICP（NICP）替代FE形变，进行稠密点云到模型的非刚性对齐；3) 在真实患者数据上评估刚性与刚性+NICP配准精度。

Result: 在3例真实患者数据上，深度增强的基础姿态估计得到平均配准误差9.91 mm；联合刚性+NICP的方案优于仅刚性注册，显示NICP能有效替代FE形变模型。

Conclusion: 该流程在保持临床相关精度的同时，显著降低了实现与建模复杂度；NICP为FE形变的高效替代，整体方案工程友好、便于实际部署。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [41] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出LATA：一种无需训练与标签的转导式后处理，将零样本概率在图上平滑并结合失效感知分数，以在保证SCP覆盖率前提下缩小预测集合、降低类间覆盖差；在多模型多任务上高效稳定提升效率与公平性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在域移下需要有保证的校准不确定性。SCP虽有有限样本覆盖保证，但在少样本与类别不平衡时预测集过大、类条件覆盖差大，且直接用校准标签自适应会破坏交换性与保证。

Method: 提出LATA：对联合的校准+测试池构建图（图像-图像kNN），对零样本类概率做拉普拉斯/CCCP均值场平滑（少量迭代），作为确定性变换以保留SCP有效性；并在ViLU框架中引入失效感知的保序分数，结合实例难度与标签可然性，提升在固定覆盖下的集合效率与类间平衡。可选使用一次性校准边缘先验，仍保持黑盒、低计算、无反向传播。

Result: 在3个医疗VLM与9个下游任务上，LATA稳定地在匹配或收紧目标覆盖率的同时，显著减小预测集大小与类条件覆盖差，优于现有转导基线，并逼近需标签的方法，计算开销更低。

Conclusion: LATA在不破坏交换性的前提下，锐化零样本预测、提升SCP效率与公平性，提供实用的、黑盒且轻量的医疗VLM不确定性后处理方案。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [42] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker 通过在视频层面构建事件场景图并结合强化微调的视觉注意奖励，提升因果结构理解与视觉落地，显著降低视频推理幻觉，在 RexTime 与 VidHalluc 上表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 多依赖密集字幕/摘要进行视频推理，缺乏对事件内外显式因果与关系结构的建模，导致对对象、事件关系与定位不准并产生幻觉；人工标注因果关系成本高，需一种能自动构建结构并增强落地性的方案。

Method: 提出 GraphThinker：1) 用 MLLM 自动生成事件级视频场景图（EVSG），显式编码事件内（对象、属性、交互）与事件间（时间、因果、语义）关系，并作为中间“思维链”输入回 MLLM；2) 在强化微调中引入视觉注意奖励，鼓励模型回答时关注与视频帧/区域对齐，从而提升视觉落地与减少幻觉；整体采用奖励驱动的策略优化。

Result: 在 RexTime 与 VidHalluc 数据集上，GraphThinker 更好捕获对象与事件关系，事件定位更精确，视频推理问题上的幻觉显著降低，整体指标优于现有方法。

Conclusion: 显式的事件级结构建模结合视觉注意奖励能有效提升视频因果推理与视觉落地性，减少幻觉；将场景图作为中间推理步骤是提升 MLLM 视频理解的有效范式。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [43] [RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward](https://arxiv.org/abs/2602.17558)
*Qiucheng Wu,Jing Shi,Simon Jenni,Kushal Kafle,Tianyu Wang,Shiyu Chang,Handong Zhao*

Main category: cs.CV

TL;DR: RetouchIQ提出用通用奖励模型指导MLLM做可执行的专业图像编辑，通过RL将高层审美指令转化为具体参数操作，显著提升语义一致性与感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM图像编辑虽能调用专业工具，但难以用可靠、可验证的奖励来训练，尤其创意编辑主观性强、规则/参考图相似度指标不足以反映用户意图与美学质量。

Method: 构建RetouchIQ框架：1) 由MLLM理解用户编辑意图，生成在专业软件中可执行的参数化调整步骤；2) 设计通用奖励模型——经RL微调的MLLM，按案例生成多维评估指标并通过多模态推理输出标量奖励；3) 用该奖励进行强化学习，获得高质量、与指令一致的梯度；4) 汇集19万条指令—推理对，并建立新的指令式图像编辑基准。

Result: 与既有MLLM与扩散式编辑方法相比，RetouchIQ在语义一致性与感知质量上大幅提升；能产出可解释的评估与更稳定的可执行操作序列。

Conclusion: 通用奖励驱动的MLLM代理可成为灵活、可解释、可执行的专业图像编辑助手，有效弥补基于规则或固定参考的奖励缺陷，推动指令驱动的创意编辑。

Abstract: Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.

</details>


### [44] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: 提出ArtSound数据集与ArtToMus框架，实现无需图像转文本的直接“艺术作品→音乐”生成，利用视觉嵌入条件的潜变量扩散模型，产出与视觉风格一致、音乐连贯的音频，尽管与文本条件相比对齐分数较低，但感知质量与跨模态对应具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有图像条件音乐生成多基于自然照片与图像转文本，两点限制：难涵盖艺术作品的丰富语义/风格/文化维度；依赖语言作为语义捷径，阻碍直接视觉到音频的学习。作者希望构建覆盖艺术领域的大规模配对数据，并探索去语言监督的直接跨模态生成路径。

Method: 1) 构建ArtSound数据集：10.59万对艺术品-音乐配对，含双模态字幕，来源于扩展ArtGraph与FMA。2) 提出ArtToMus：不做图像→文本转换，提取艺术图像视觉嵌入，投影到潜变量扩散模型的条件空间，直接以视觉信号引导音乐合成；以感知与对齐指标评估。

Result: ArtToMus能生成在音乐上连贯、风格上与源艺术品一致的音频；在跨模态对应上取得有意义的匹配。虽然绝对对齐分数低于文本条件系统（因缺少语言监督），但总体感知质量具竞争力。

Conclusion: 开创“直接视觉到音乐”的研究方向与基准资源，证明去语言监督的可行性与挑战性；为多媒体艺术、文化遗产与AI创作提供数据与代码支持（接受后公开）。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [45] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 提出一个统一的地理空间目标发现框架，结合主动学习、在线元学习与概念引导推理，通过“概念相关性”加权不确定性采样与相关性感知的元批次形成，提高在稀疏偏置标注与动态环境下的高效取样与泛化；在PFAS污染数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实任务（环境监测、灾害响应、公共卫生）下数据采集昂贵且环境动态，需在资源受限下从未观测区域高效取样来发现隐藏目标。现有强化学习等方法依赖密集、均衡的地面真值，难以适用稀疏且偏置的地理空间数据，因此需要能在少量、偏置数据和分布变化中仍具鲁棒性的发现与采样方法。

Method: 提出以“概念相关性”为核心的统一框架：1) 概念加权的不确定性采样：将不确定性与由领域概念（如土地覆盖、污染源距离）学习得到的相关性相结合，优先采样高相关且高不确定的区域；2) 相关性感知的元批次构造：在线元学习阶段按概念语义多样性构建meta-batches，增强在动态环境中的泛化；并将主动学习、在线元学习与概念引导推理整合用于地理空间发现。

Result: 在真实PFAS（全氟和多氟烷基物质）污染数据集上进行实验，方法在有限数据与环境变化条件下更可靠地发现目标（污染点），相较基线提升采样效率与发现率。

Conclusion: 通过引入概念相关性并在采样与在线元学习中显式利用，框架在稀疏、偏置与动态的地理空间场景中提升了目标发现效率与泛化能力，适用于资源受限的环境监测与公共健康应用。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [46] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出CORAL框架，在DiT的全3D注意力中显式对齐人-衣对应，通过蒸馏外部对应与熵最小化，显著提升VTON的形状迁移与细节保真，并给出VLM评测协议。


<details>
  <summary>Details</summary>
Motivation: 现有VTON方法（尤其是unpaired场景）难以保留衣物细节，且未显式约束人与衣的对应关系；DiT内部对应关系的形成机制也缺乏解释，导致对齐不稳、细节丢失。

Method: 分析DiT的全3D注意力，发现人-衣对应依赖于精确的query-key匹配；据此提出CORAL：1）对应蒸馏损失，将来自稳健外部模块的可靠对应对齐到人-衣注意力；2）熵最小化损失，收紧并锐化注意力分布；并提出基于VLM的人偏好一致性评测协议。

Result: 在基线之上稳定提升，全局形状迁移更准确、本地细节更清晰；消融实验证实两种损失和对齐策略的有效性与互补性。

Conclusion: 显式对齐DiT中的query-key匹配是提升VTON人-衣对应与细节保真的关键；CORAL有效、通用，并在新VLM评测下与人类偏好更一致。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [47] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: IntRec提出交互式目标检索：用用户反馈迭代细化检索结果，通过正负记忆和对比对齐快速消歧，在LVIS及其含歧义基准上显著提升AP且交互延迟低。


<details>
  <summary>Details</summary>
Motivation: 开放词汇检测在复杂场景中遇到歧义、多相似目标时，一次性预测难以满足用户真实意图，缺乏利用用户反馈进行细化的机制。需要一种能把用户的确认与否定信息纳入推理、实现交互式消歧与可靠检索的方法。

Method: 提出IntRec交互式框架：核心是Intent State（IS），维护两类记忆——正锚（用户确认的线索）与负约束（被否定的假设）。通过对比式对齐函数，对候选目标进行打分：最大化与正线索的相似度，同时惩罚与负约束的相似度；多轮交互中动态更新IS以逐步细化预测。无需额外监督，可与开放词汇检测器结合。

Result: 在LVIS上达到35.4 AP，分别较OVMR、CoDet、CAKE提升+2.3、+3.7、+0.5。于更具歧义的LVIS-Ambiguous基准上，相比其一次性基线，仅一次纠错反馈即可提升+7.9 AP，且每次交互新增延迟<30毫秒。

Conclusion: 通过显式维护用户意图状态并进行正负对比对齐，IntRec能在杂乱、歧义场景中高效消歧与提升检索准确率，代价极低、无需额外监督，适合实际交互式开放词汇检索应用。

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [48] [Human-level 3D shape perception emerges from multi-view learning](https://arxiv.org/abs/2602.17650)
*Tyler Bonnen,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 提出一种多视角视觉-空间训练的神经网络框架，在零样本条件下可匹配人类在3D形状推断任务上的准确率，并能预测人类的误差模式与反应时。


<details>
  <summary>Details</summary>
Motivation: 长期以来，计算模型难以在从2D图像推断3D结构方面达到人类水平；现有方法常依赖对象先验或任务特定训练，泛化与人类一致性不足。作者希望构建一个无需对象偏置、可直接从自然多视角感知中学得空间线索，并能与人类行为紧密对应的模型。

Method: 训练“多视角”神经网络：输入为来自自然场景、从不同位置拍摄的一组图像；以视觉-空间目标进行自监督学习，预测与这些图像相关的空间信息（如相机位置、深度），而不引入对象相关先验。之后采用零样本评估：将训练好的模型直接用于经典的3D感知任务，比较模型与人类的选择、错误型式与反应时间。并通过独立读出层解码模型表征来预测细粒度人类行为。

Result: 模型在无任务特定训练或微调的前提下，首次在3D形状推断准确率上达到人类水平；模型表征的独立读出可高精度预测人类的误差分布与反应时间，显示模型动力学与人类知觉之间的自然对应关系。

Conclusion: 人类级3D感知可从对自然多视角视觉-空间数据的简单且可扩展的学习目标中自发涌现；该框架为理解与工程化人类样式3D知觉提供了可复现、可扩展的途径，资源已开源。

Abstract: Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.

</details>


### [49] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: 论文提出并系统揭示“反事实失效”问题：VLA在缺少强场景特定监督时，会凭视觉捷径而无视语言，重复训练中常见动作与物体选择。作者构建首个反事实基准LIBERO-CF，并提出无需改模型/数据的即插即用推理方案CAG，通过与语言无关的VA分支做对比，显著提升语言遵循与任务成功率，且在仿真与真实机器人上均有效。


<details>
  <summary>Details</summary>
Motivation: 现有VLA常把语言当次要信号，受数据偏差诱导，遇到视觉上合理但与指令不匹配的情形会选错对象/行为。社区缺乏系统 benchmark 去量化和比较这类“反事实”错误，也缺少低成本、可广泛适配的方法来抑制视觉捷径、增强语言条件依赖。

Method: 1) 基准：提出LIBERO-CF，在与训练分布视觉上相符但语言意图相反/替换的布局中，给出替代性指令，专测语言遵循能力。2) 方法：提出Counterfactual Action Guidance (CAG)，在推理时并联两条分支：VLA(语言条件)与VA(无语言)。通过对两分支的动作偏好做反事实比较与正则，引导选择更依赖语言信号的动作，减少对视觉捷径的依赖。无需额外示范、无需改架构或预训练参数，可即插即用。

Result: 在LIBERO-CF上，采用无训练策略即可将π_{0.5}语言遵循准确率提升9.7%，在“观察不足”任务上的成功率提升3.6%；若配合VA模型进一步提升至+15.5%语言遵循与+8.5%成功率。真实机器人实验中，平均减少9.4%的反事实失败，并提升17.2%的任务成功率。

Conclusion: 反事实失败在SOTA VLA中普遍存在且被低估。LIBERO-CF提供了系统评测手段，CAG以简单、通用、训练友好（可免训练）方式强化语言条件，显著提升鲁棒性与语言跟随能力，为VLA在现实机器人中的可靠部署提供有效路径。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


### [50] [OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents](https://arxiv.org/abs/2602.17665)
*Akashah Shabbir,Muhammad Umer Sheikh,Muhammad Akhtar Munir,Hiyam Debary,Mustansar Fiaz,Muhammad Zaigham Zaheer,Paolo Fraccaro,Fahad Shahbaz Khan,Muhammad Haris Khan,Xiao Xiang Zhu,Salman Khan*

Main category: cs.CV

TL;DR: OpenEarthAgent提出一个统一框架，利用工具增强的多模态地理空间智能体，通过含推理轨迹的监督微调，在遥感场景中实现稳定、可解释的多步空间与指数分析，覆盖多域任务，并优于强基线、接近最新开源/闭源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态推理在通用图文任务上进展迅速，但在遥感领域仍受限：需跨尺度、地理结构、多光谱指数（如NDVI/NBR/NDBI）进行一致的多步逻辑推理，同时与GIS操作结合，现有方法难以保证稳定空间理解与可解释性。

Method: 构建OpenEarthAgent：1）统一的工具增强地理空间智能体框架；2）基于卫星影像、自然语言查询与详细推理轨迹的数据集；3）以“结构化推理轨迹”的有监督微调对齐模型，使其学习经验证的多步工具交互；4）工具涵盖GIS操作与多光谱指数分析。数据规模：训练14,538例、评测1,169例；训练>100K步推理、评测>7K步。

Result: 模型在多域（城市、环境、灾害、基础设施）条件下展现结构化推理、稳定空间理解与可解释的工具驱动交互；在基准上持续优于强基线，并与最新开源/闭源模型表现具有竞争力。

Conclusion: 通过显式推理轨迹的SFT与统一工具化框架，OpenEarthAgent在遥感多步地理空间推理中实现性能与可解释性的兼顾，证明“工具+轨迹监督”对复杂地理空间任务有效。

Abstract: Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.

</details>
