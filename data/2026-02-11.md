<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 97]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UI-Venus-1.5 Technical Report](https://arxiv.org/abs/2602.09082)
*Veuns-Team,:,Changlong Gao,Zhangxuan Gu,Yulin Liu,Xinyu Qiu,Shuheng Shen,Yue Wen,Tianyu Xia,Zhenyu Xu,Zhengwen Zeng,Beitong Zhou,Xingran Zhou,Weizhi Chen,Sunhao Dai,Jingya Dou,Yichen Gong,Yuan Guo,Zhenlin Guo,Feng Li,Qian Li,Jinzhen Lin,Yuqi Zhou,Linchao Zhu,Liang Chen,Zhenyu Guo,Changhua Meng,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus-1.5 是一个统一的端到端 GUI 智能体家族（2B、8B、30B-MoE），通过中期训练、在线强化学习与模型合并三项关键改进，在多项 GUI 基准上取得 SOTA，并在中文移动应用中展现稳健导航与执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有 GUI 智能体难以同时兼顾通用性与稳定高性能，尤其在长程、动态、多平台（网页/移动/跨应用）环境中表现受限。作者希望构建一个单一、鲁棒且可落地的通用 GUI Agent。

Method: 1) 中期训练（Mid-Training）：利用超 100 亿 token、30+ 数据集建立 GUI 语义与跨域能力；2) 在线强化学习（全轨迹 rollout）：将目标与长程导航对齐，提升决策的时序一致性与鲁棒性；3) 模型合并（Model Merging）：将领域专家模型（grounding、web、mobile）融合为统一 checkpoint；提供 2B/8B 稠密版与 30B-A3B MoE 版。

Result: 在 ScreenSpot-Pro 69.6%、VenusBench-GD 75.0%、AndroidWorld 77.6% 等基准上达 SOTA，显著超越强基线；在真实中文移动 App 上实现稳健的指令执行与导航。

Conclusion: 通过统一架构与从预训练到在线 RL 的多阶段优化，UI-Venus-1.5 实现通用 GUI 交互的强鲁棒性与高性能，证明模型合并与全轨迹 RL 对长程、多域 GUI 任务有效；为实际应用部署提供可行路径。

Abstract: GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus

</details>


### [2] [Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling](https://arxiv.org/abs/2602.09084)
*Ruijie Ye,Jiayi Zhang,Zhuoxin Liu,Zihao Zhu,Siyuan Yang,Li Li,Tianfu Fu,Franck Dernoncourt,Yue Zhao,Jiacheng Zhu,Ryan Rossi,Wenhao Chai,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 提出“Agent Banana”，用于高保真、面向对象、可多轮协作的指令式图像编辑；通过上下文折叠与图层分解在4K等超高清下稳定编辑，并发布对话式高分基准HDD-Bench；在多轮一致性与背景保真上SOTA，同时单轮表现亦强。


<details>
  <summary>Details</summary>
Motivation: 专业工作流中的指令式图像编辑面临三大痛点：过度编辑、模型多为单轮导致多轮编辑失真、评测分辨率与实际超高清工作流不匹配。需要能在长时程对话中保持对象忠实与背景不破坏，并原生支持超高分辨率输出的方法与基准。

Method: 提出层级式“规划-执行”代理框架Agent Banana，包含两大机制：1) Context Folding：将长对话/历史压缩为结构化记忆，提升长时程稳定控制；2) Image Layer Decomposition：进行局部图层化编辑，锁定非目标区域并支持原生分辨率输出。并构建HDD-Bench：对话式、逐步可验证目标、原生4K（11.8MP）的评测集，用于诊断长时程失败。

Result: 在HDD-Bench上，Agent Banana取得最佳多轮一致性与背景保真（如IC 0.871、SSIM-OM 0.84、LPIPS-OM 0.12），且指令跟随能力具竞争力；在标准单轮编辑基准上也表现强劲。

Conclusion: Agent Banana通过上下文折叠与图层分解，实现面向真实专业工作流的可靠、多轮、高分辨率图像编辑；HDD-Bench为该方向提供严格评测基座，促进专业级代理式图像编辑的落地。

Abstract: We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.

</details>


### [3] [SemanticMoments: Training-Free Motion Similarity via Third Moment Features](https://arxiv.org/abs/2602.09146)
*Saar Huberman,Kfir Goldberg,Or Patashnik,Sagie Benaim,Ron Mokady*

Main category: cs.CV

TL;DR: 提出SimMotion基准揭示现有视频表示对外观/场景偏置，提出无需训练的SemanticMoments通过对预训练语义特征计算时间高阶矩来做运动语义检索并显著优于RGB/光流/文本监督方法。


<details>
  <summary>Details</summary>
Motivation: 视频检索与理解需要基于“运动语义”而非仅静态外观；现有模型因训练数据与目标导致对外观与场景依赖强、对运动动态不敏感；传统光流虽运动导向但缺乏高层语义锚定。作者为量化并揭示这种偏置，构建专门的评测基准。

Method: 1) 构建SimMotion基准：受控合成数据+人工标注的真实数据，专为区分运动与外观而设计。2) 提出SemanticMoments：在预训练语义特征（如来自大规模语义模型）上计算时间统计量，特别是高阶矩（均值之外的方差、偏度、峰度等），以无训练方式编码运动随时间变化的语义动态，用于检索/理解任务。

Result: 在SimMotion等基准上，现有基于RGB、光流及文本监督的方法表现不佳，常无法将运动与外观解耦；SemanticMoments在各基准上一致优于这些方法，显示其对运动中心任务更有效。

Conclusion: 在语义特征空间中引入时间高阶统计可作为可扩展且知觉上扎实的基础，提升以运动为中心的视频理解；揭示现有方法的外观偏置并给出简单、免训练的有效替代。

Abstract: Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>


### [4] [A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video](https://arxiv.org/abs/2602.09154)
*Andrea Filiberto Lucas,Dylan Seychell*

Main category: cs.CV

TL;DR: 论文提出一个可审计、可解释的自动从新闻视频中检测与提取人物姓名的框架，并构建多样化标注数据集；在定位图形元素上取得95.8% mAP@0.5，整体F1略低于生成式方法但具备更高可追溯性、无幻觉与审计友好性，提供混合多模态信息抽取的严谨基线。


<details>
  <summary>Details</summary>
Motivation: 视频新闻量激增，屏幕上姓名等信息对新闻核实与索引重要，但版式、字体、平台风格多变，人工标注不可扩展；现有生成式多模态方法虽准确率高，却缺乏可审计性与可追溯性，不适合新闻与分析场景。

Method: 1) 构建覆盖当代新闻图形多样性的均衡标注帧数据集；2) 设计模块化、可解释、可审计的确定性信息抽取流水线，包括图形元素检测与姓名提取；3) 用传统确定性流水线与生成式多模态方法对比评测。

Result: 图形元素检测器达95.8% mAP@0.5，整体抽取性能Precision 79.9%、Recall 74.4%、F1 77.08%；生成式系统F1为84.18%，但存在不可追溯与潜在幻觉问题；用户研究显示59%受试者在快节奏新闻中难以读清姓名，证明任务现实意义。

Conclusion: 在新闻场景下，确定性、可审计的模块化流水线提供了稳健、可追溯且无幻觉的基线方案，虽准确略低于生成式方法，但更符合新闻与分析对透明度的需求，并为混合多模态信息抽取奠定方法学基线。

Abstract: The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.
  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.

</details>


### [5] [Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images](https://arxiv.org/abs/2602.09155)
*Ahmed Rahu,Brian Shula,Brandon Combs,Aqsa Sultana,Surendra P. Singh,Vijayan K. Asari,Derrick Forchetti*

Main category: cs.CV

TL;DR: 研究探讨用CNN从低级别管状腺瘤的全切片数字病理图像中捕捉细微形态学信号，预测个体未来发生结直肠癌的风险。


<details>
  <summary>Details</summary>
Motivation: 尽管筛查与息肉切除降低了CRC发生率，但部分最初仅有低级别腺瘤的患者仍在多年后进展为癌。传统组织学可能遗漏微妙的结构/细胞学风险信号，亟需更精准地分层“低风险”人群以优化随访与预防干预。

Method: 利用数字病理与机器学习，训练卷积神经网络在全视野切片（WSI）上自动学习低级别管状腺瘤中的细微组织学特征，并评估其对长期发生CRC风险的预测能力。

Result: CNN能够在WSI中识别传统评估难以察觉的形态学特征，并与患者日后CRC发生风险具有预测关联（摘要层面暗示阳性结果）。

Conclusion: 基于WSI的CNN模型有望作为传统病理评估的补充，实现对低级别腺瘤患者的精细化风险分层，从而指导个体化随访与预防策略。

Abstract: Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.

</details>


### [6] [All-in-One Conditioning for Text-to-Image Synthesis](https://arxiv.org/abs/2602.09165)
*Hirunima Jayasekara,Chuong Huynh,Yixuan Ren,Christabel Acquaye,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 提出一种基于场景图的零样本条件机制，通过ASQL（属性-尺寸-数量-位置）条件器在推理时为扩散模型提供软视觉引导，以提升复杂文本提示的组合理解与生成对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型虽然能生成逼真图片，但在多对象、属性与空间关系等复杂提示下，常出现语义偏差与结构失真；既有用预设布局图的方法过于刚性，牺牲组合灵活性与多样性，亟需一种既保持语义-结构一致又具多样性的条件机制。

Method: 以场景图为基础进行条件约束：提出零样本的场景图条件机制，在推理阶段提供“软”视觉引导。核心是ASQL条件器（Attribute/Size/Quantity/Location），使用轻量级语言模型从文本抽取并生成视觉条件，并通过推理时优化（inference-time optimization）与扩散生成耦合，维持文本-图像对齐。

Result: 方法在无需预定义硬布局的情况下，能够更好地保持语义一致与结构连贯，同时生成更轻量、连贯且多样的图像（相较于以往硬布局或无场景图引导的方案）。

Conclusion: 以场景图+ASQL软条件与推理时优化相结合的做法，提高了复杂提示的组合表达能力与对齐度，在不牺牲多样性的前提下实现了连贯与灵活的文本到图像合成。

Abstract: Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>


### [7] [Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain](https://arxiv.org/abs/2602.09209)
*Michael D. Murray,James Tung,Richard W. Nuckols*

Main category: cs.CV

TL;DR: 用穿戴式RGB-D相机与CNN-RNN，在脚落地前250ms内预测足底压力中心(AP方向)与触地时间；在50–150ms预测窗内COP MAE≈24–29mm，TOI MAE≈18–21ms；模型轻量、60FPS实时运行，显示基于视觉的前馈预测可用于辅具的预见性控制。


<details>
  <summary>Details</summary>
Motivation: 现有步态环境分类多用于辅助设备控制，但对即将发生的足部接触动力学（COP与触地时间）的直接预测研究不足。若能提前从视觉推断，将有助于实现前馈、预见性控制，提升楼梯过渡等情境下的安全与稳定。

Method: 8名受试者右小腿佩戴RGB-D相机并穿仪表化鞋垫，执行平地到上楼过渡。以连续视频与传感数据训练轻量CNN-RNN，在脚触地前250ms窗口内持续预测AP方向足底压力中心(COP)与触地时间(TOI)，并分析步态动力学因素（躯干速度、趾摆动速度、前向落脚位置）对误差的影响。

Result: 在预测视窗为150/100/50ms时，COP MAE分别为29.42/26.82/23.72mm；TOI MAE分别为21.14/20.08/17.73ms。躯干速度对误差无显著影响；较快趾摆动速度提升COP预测精度（对TOI无显著影响）；更前的落脚位置降低COP预测精度（对TOI无影响）。模型可在笔记本或边缘设备上以60FPS运行。

Conclusion: 基于视觉的轻量CNN-RNN可在脚落地前实时、可用精度地预测COP与TOI，验证了在楼梯上踏过渡中的可行性；结果支持在辅助系统中引入前馈、预见性控制以提升交互与安全性。

Abstract: Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>


### [8] [VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models](https://arxiv.org/abs/2602.09214)
*Chenyu Wang,Tianle Chen,H. M. Sabbir Ahmad,Kayhan Batmanghelich,Wenchao Li*

Main category: cs.CV

TL;DR: 提出VLM-UQBench，用于评估视觉-语言模型在图像、文本与跨模态来源的不确定性；并给出两种灵敏度与幻觉相关性的指标，系统评测多种UQ方法，发现现有方法强依赖模型、易与幻觉共现、难以捕捉细粒度实例级不确定性。


<details>
  <summary>Details</summary>
Motivation: VLM在安全可靠部署中需要精确量化与定位不确定性来源（图像、文本或跨模态错配），现有UQ缺乏模态感知与细粒度诊断能力。

Method: 构建VLM-UQBench：从VizWiz筛选600真实样本，划分为干净/图像不确定/文本不确定/跨模态不确定子集；设计可扩展扰动流水线（8种视觉、5种文本、3种跨模态扰动）；提出两种指标，用于度量UQ分数对扰动的敏感度及其与幻觉的相关性；在四个VLM与三个数据集上评测多种UQ方法。

Result: (i) 现有UQ方法呈现强模态专化并且对底层VLM高度依赖；(ii) 模态特定不确定性常与幻觉共现，但当前UQ分数作为风险信号弱且不稳定；(iii) 尽管在显性、群体级歧义上能与推理型CoT基线相当，但在本基准引入的细粒度、实例级歧义检测上大多失败。

Conclusion: 当前UQ实践与可靠VLM部署所需的细粒度、模态感知不确定性之间存在显著差距；需要面向模态源定位、稳健且与幻觉强相关的UQ指标与方法。

Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>


### [9] [VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models](https://arxiv.org/abs/2602.09252)
*Ange Lou,Yamin Li,Qi Chang,Nan Xi,Luyuan Xie,Zichao Li,Tianyu Luan*

Main category: cs.CV

TL;DR: 提出IR-SIS：可用自然语言驱动、可自我迭代精修的外科图像分割系统，结合SAM3初分割、视觉语言模型质检与器械检测、以及智能体式自适应策略选择，支持临床交互，达成SOTA并具OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有外科分割方法受限于预定义类别、一次性预测且难以根据反馈迭代优化，缺乏让临床医生通过自然语言参与修正的机制，难以满足实际术中指导与机器人辅助手术的需求。

Method: 1) 以微调的SAM3执行初始分割；2) 采用视觉-语言模型（VLM）识别手术器械并评估分割质量；3) 通过具备决策能力的agent式工作流，根据VLM与用户反馈自适应选择细化策略并迭代优化；4) 支持基于自然语言的人在回路交互；5) 构建来自EndoVis2017/2018的多粒度语言标注数据集以训练与评测。

Result: 在域内（EndoVis基准）与域外数据上均达成SOTA表现；引入临床交互的自然语言反馈可进一步提升分割精度与稳健性。

Conclusion: IR-SIS实现首个基于语言的外科分割与自适应自我细化框架，填补交互式、可迭代外科分割空白，兼具实用交互性与OOD泛化潜力。

Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>


### [10] [Rethinking Global Text Conditioning in Diffusion Transformers](https://arxiv.org/abs/2602.09268)
*Nikita Starodubcev,Daniil Pakhomov,Zongze Wu,Ilya Drobyshevskiy,Yuchen Liu,Zhonghao Wang,Yuqian Zhou,Zhe Lin,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: 论文探讨扩散Transformer中文本条件的两种途径：注意力与基于池化文本嵌入的调制。作者发现传统调制几乎无益，但若将池化嵌入作为“引导”信号可在无需再训练的前提下带来显著改进，且泛化到图像/视频生成与编辑。


<details>
  <summary>Details</summary>
Motivation: 近年来一些方法完全放弃调制，仅依赖注意力进行文本条件，暗示调制可能是冗余的。作者希望系统性回答：调制是否必要？在什么情形下能带来优势？

Method: 对比分析扩散Transformer中传统的池化文本嵌入调制与仅注意力方案的效果；提出将池化嵌入从“调制”转为“引导（guidance）”的新用法，实现可控属性偏移；方法训练无关、实现简单、推理开销可忽略，并可套用于多种扩散模型与任务。

Result: 实验证明：常规用法下，池化嵌入对总体性能贡献很小，注意力足以传递提示信息；而作为引导信号时，池化嵌入带来显著性能提升与可控性增强，适用于文本到图像/视频生成与图像编辑多种任务。

Conclusion: 调制式文本条件在常规形态下非必要，但池化嵌入若转而充当训练无关的引导，可在广泛任务中提升质量并提供可控性，且实现简便、开销极低。

Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>


### [11] [X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging](https://arxiv.org/abs/2602.09284)
*Pranav Kulkarni,Junfeng Guo,Heng Huang*

Main category: cs.CV

TL;DR: 提出X-Mark：面向胸部X光的样本特异、干净标签水印，用于数据版权保护；在黑盒模型上可验证所有权，WSR达100%，并在独立模型情形降低误报12%，且对自适应攻击具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据昂贵且敏感，未经授权使用引发版权与伦理问题。现有针对自然图像的水印方法对医学影像不适用：固定尺度、静态图案在高分辨率、动态缩放及低视觉多样性、细微结构的场景中易失效且会影响诊断质量。需要一种既能保留诊断质量、又可在黑盒模型上验证数据所有权的水印技术。

Method: 提出X-Mark：1) 使用条件U-Net，为每个样本在其显著区域生成唯一的微扰（样本特异、干净标签）；2) 多组件训练目标兼顾水印可验证性、对动态尺度变化的鲁棒性、诊断质量和可视可区分性；3) 在损失中引入Laplacian正则，抑制高频扰动，实现尺度不变性；4) 黑盒验证，通过触发受水印模型的特征反应来判定可疑模型是否使用了被水印的数据。

Result: 在CheXpert数据集上，X-Mark实现WSR（水印成功率）100%，在独立模型（Ind-M）情境下降低误报率12%，并对潜在自适应攻击表现出抗性。

Conclusion: X-Mark能在不牺牲诊断质量的前提下，为医学影像数据提供样本级干净标签水印，并可在黑盒环境下可靠验证所有权，兼具尺度鲁棒性与对抗适应性。

Abstract: High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>


### [12] [A Deep Multi-Modal Method for Patient Wound Healing Assessment](https://arxiv.org/abs/2602.09315)
*Subba Reddy Oota,Vijay Rowtula,Shahid Mohammed,Jeffrey Galitz,Minghsun Liu,Manish Gupta*

Main category: cs.CV

TL;DR: 提出一个基于深度学习的多模态方法，用伤口图像与临床变量联合预测住院风险，并通过迁移学习从图像推断变量及愈合轨迹，以便早期发现并发症、降低诊断时间与成本。


<details>
  <summary>Details</summary>
Motivation: 住院是伤口护理成本高企的重要因素；多数伤口本可门诊管理，但因治疗延误、不依从或合并症导致恶化而需住院。需要一种能及早、可靠评估住院风险与愈合进程的工具，以支持及时干预、降低成本与负担。

Method: 构建深度多模态模型：同时利用伤口图像与伤口变量进行风险预测；采用迁移学习，使模型可从图像中预测关键伤口变量，并进一步建模伤口愈合轨迹，从而实现端到端的住院风险评估与愈合预测。

Result: 模型能够从图像中推断伤口变量，并联合变量与图像对住院风险和愈合轨迹进行预测；与仅基于单一模态或仅分类型伤口轨迹的方法相比，表现更优（摘要未给出具体数值）。

Conclusion: 多模态、迁移学习驱动的伤口评估可早期识别影响愈合的复杂因素，提升对住院风险与愈合过程的预测能力，减少临床评估时间与潜在成本。

Abstract: Hospitalization of patients is one of the major factors for high wound care costs. Most patients do not acquire a wound which needs immediate hospitalization. However, due to factors such as delay in treatment, patient's non-compliance or existing co-morbid conditions, an injury can deteriorate and ultimately lead to patient hospitalization. In this paper, we propose a deep multi-modal method to predict the patient's risk of hospitalization. Our goal is to predict the risk confidently by collectively using the wound variables and wound images of the patient. Existing works in this domain have mainly focused on healing trajectories based on distinct wound types. We developed a transfer learning-based wound assessment solution, which can predict both wound variables from wound images and their healing trajectories, which is our primary contribution. We argue that the development of a novel model can help in early detection of the complexities in the wound, which might affect the healing process and also reduce the time spent by a clinician to diagnose the wound.

</details>


### [13] [GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification](https://arxiv.org/abs/2602.09318)
*Lin-Guo Gao,Suxing Liu*

Main category: cs.CV

TL;DR: 提出GAFRNet：结合图注意力与可微模糊规则的弱监督乳腺组织病理图像分类模型，兼顾性能与可解释性，并在三大数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN/Transformer在标注稀缺时性能下降，且“黑箱”难以临床落地；需要一种既能利用样本间关系、又能提供可解释诊断逻辑的模型。

Method: 1) 基于样本相似度构建图，建模不同组织结构间的关系；2) 采用多头图注意力捕获复杂关系特征；3) 设计可微模糊规则模块，将节点度、聚类系数、标签一致性等拓扑描述编码为人类可理解的IF-THEN规则，实现端到端训练与解释；4) 弱监督设置下进行分类。

Result: 在BreakHis、Mini-DDSM、ICIAR2018上，于多倍率与多任务设置中，GAFRNet稳定超越多种SOTA方法，表现出更强的泛化性。

Conclusion: GAFRNet在稀缺标注的病理图像分类中实现高精度且具备透明推理过程，较适合作为临床决策支持工具，减少对事后可视化归因的依赖。

Abstract: Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a "blackbox" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent "IF-THEN" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.

</details>


### [14] [Deep Modeling and Interpretation for Bladder Cancer Classification](https://arxiv.org/abs/2602.09324)
*Ahmad Chaddad,Yihang Wu,Xianrui Chen*

Main category: cs.CV

TL;DR: 比较ViT与CNN在膀胱癌影像分类中的性能、校准与可解释性；结论：无一刀切模型，ConvNeXt更适合分布内，ViT更适合分布外与校准更好。


<details>
  <summary>Details</summary>
Motivation: 医学影像中病灶区域往往很小，主流在自然图像上成功的CNN/ViT模型在此场景可能失效；需要系统评估这些模型在膀胱癌分类上的准确性、校准性与可解释性以指导临床应用。

Method: 在公开多中心膀胱癌数据集上，使用13个深度模型（4个CNN与8个Transformer/ViT家族）进行：1）标准分类评估；2）模型校准分析；3）基于Grad-CAM++的可解释性评估；并进行约300次实验与测试时增强（TTA）以提升解释性。

Result: ConvNeXt系列在泛化到膀胱癌图像上表现有限（约60%准确率）；相比ConvNeXt与Swin，ViT家族具有更好的校准；TTA可提升可解释性指标。

Conclusion: 不存在通用最优模型：ConvNeXt更适合分布内样本；ViT及其变体在分布外样本解释性更佳且校准更好；实际部署需根据场景在准确性、校准与解释性之间权衡。

Abstract: Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>


### [15] [Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents](https://arxiv.org/abs/2602.09337)
*Michail S. Alexiou,Nikolaos G. Bourbakis*

Main category: cs.CV

TL;DR: 提出Kyrtos方法，自动识别与解析技术文档中的曲线图：以聚类识别曲线线段的中点，重建曲线并提取方向/趋势等行为特征，转为属性图与自然语言描述，并进一步映射到随机Petri网；实验显示对多函数曲线图有较高结构相似度。


<details>
  <summary>Details</summary>
Motivation: 技术文档蕴含大量图形化知识，图表（尤其含曲线的图）对系统行为与关系表达关键，但跨模态理解困难。需要一种能从图像中准确识别曲线并将其结构与语义转化为可分析形式（文本、图模型）的自动化方法，以支撑整体文档理解与下游建模。

Method: 提出Kyrtos：1) 识别阶段以基于聚类的方法，定位并识别划分曲线的线段中点，从而重构由线段组成的曲线；2) 解析阶段对提取的线段进行语义解析，捕捉方向、趋势等行为特征；3) 将线段与其关系转换为带属性的图以保留曲线结构；4) 将图关系生成自然语言句子，丰富文档文本；5) 将这些关系进一步映射为随机Petri网图，刻画图表中的内部功能动态。

Result: 通过大量评测，基于输入曲线图与Kyrtos生成的曲线近似之间的结构相似度度量，显示其在含多条函数曲线的图表上具有较高的识别与解析准确性。

Conclusion: Kyrtos能从技术文档的曲线图中自动提取并语义化曲线结构，将视觉信息桥接到文本与形式化模型（SPN），为技术文档的深度理解与后续分析提供有效支撑，且在多函数图表上表现稳定、准确。

Abstract: Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.

</details>


### [16] [Impact of domain adaptation in deep learning for medical image classifications](https://arxiv.org/abs/2602.09355)
*Yihang Wu,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 论文评估10种深度学习DA方法在四类医学影像任务中的有效性：总体上，特征对齐能小幅至中幅提升性能、抗噪、可解释性与校准，但在联邦学习中增益有限。


<details>
  <summary>Details</summary>
Motivation: DA通过将源/目标域对齐到共享特征空间，以利用有标注源域帮助目标域（低标注）训练。医学影像跨模态、噪声、设备与中心差异显著，临床部署面临域偏移；亟需系统性评估DA在多情境（多模态、噪声、联邦、可解释性、校准）下的实际收益与局限。

Method: 基于四个医学影像数据集，复现并比较10种深度学习DA模型/策略，围绕ResNet34等骨干；设置多情境实验：多模态、添加高斯噪声、联邦学习框架嵌入、Grad-CAM++可解释性评估、分类器校准（ECE）。以准确率等指标衡量，并与不使用DA的CNN基线对比。

Result: 在脑肿瘤数据集上，ResNet34+DA提升约4.7%准确率；在加入高斯噪声情形下，DA带来约3%增益；在皮肤癌联邦学习中，仅约0.3%微弱提升；引入DA后，Grad-CAM++热力图更贴合病灶（定性改进）；在多模态数据上，校准更佳，ECE较CNN基线降低约2%。

Conclusion: 特征对齐类DA在医学影像多数设定下能带来稳健但中等幅度的性能与鲁棒性收益，并改善模型解释与校准；然而在联邦学习中直接套用DA增益有限。实践上应结合具体场景挑选DA策略，并与噪声鲁棒、校准与可解释性目标联合优化；在FL中需开发更贴合分布异质性的DA-FL协同方法。

Abstract: Domain adaptation (DA) is a quickly expanding area in machine learning that involves adjusting a model trained in one domain to perform well in another domain. While there have been notable progressions, the fundamental concept of numerous DA methodologies has persisted: aligning the data from various domains into a shared feature space. In this space, knowledge acquired from labeled source data can improve the model training on target data that lacks sufficient labels. In this study, we demonstrate the use of 10 deep learning models to simulate common DA techniques and explore their application in four medical image datasets. We have considered various situations such as multi-modality, noisy data, federated learning (FL), interpretability analysis, and classifier calibration. The experimental results indicate that using DA with ResNet34 in a brain tumor (BT) data set results in an enhancement of 4.7\% in model performance. Similarly, the use of DA can reduce the impact of Gaussian noise, as it provides $\sim 3\%$ accuracy increase using ResNet34 on a BT dataset. Furthermore, simply introducing DA into FL framework shows limited potential (e.g., $\sim 0.3\%$ increase in performance) for skin cancer classification. In addition, the DA method can improve the interpretability of the models using the gradcam++ technique, which offers clinical values. Calibration analysis also demonstrates that using DA provides a lower expected calibration error (ECE) value $\sim 2\%$ compared to CNN alone on a multi-modality dataset.

</details>


### [17] [Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation](https://arxiv.org/abs/2602.09378)
*Jun Li*

Main category: cs.CV

TL;DR: 提出DBiSL：一种全可微、双向协同的半监督分割框架，联合监督、一致性、伪监督与不确定性估计，支持在线跨任务双向协作，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割标注昂贵、数据稀缺；现有半监督方法多依赖伪标签与一致性，但双任务协作通常是单向（回归→分割），分割到回归只能离线转换，限制了在线双向协作与性能提升。

Method: 构建可微的双向协同学习框架DBiSL：在相关任务（如回归与分割）间建立在线、端到端的双向一致性约束；同时整合四要素——有监督损失、一致性正则、伪监督学习与不确定性估计；通过可微映射实现分割↔回归的互相指导，动态筛选高置信伪标签以稳定训练。

Result: 在两个基准数据集上取得SOTA性能（定量指标优于现有方法），验证了双向在线协同和统一SSL组件整合的有效性与鲁棒性。

Conclusion: DBiSL为双任务驱动的半监督学习提供了新的统一与可微架构，克服单向协作与离线转换的局限，提升医疗图像分割性能，并具备推广到更广泛视觉多任务场景的潜力。

Abstract: Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>


### [18] [Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D](https://arxiv.org/abs/2602.09407)
*Yan Luo,Advaith Ravishankar,Serena Liu,Yutong Yang,Mengyu Wang*

Main category: cs.CV

TL;DR: 对五个最先进图像到3D模型在单张医学切片重建上的零样本基准评测：体素重叠普遍一般，深度/体积推断易失败；全局距离指标区分度更强，SAM3D整体拓扑最相似，其它模型更易过度简化。结论：单切片重建受固有深度歧义限制，需多视角方法以实现可靠医学3D推断。


<details>
  <summary>Details</summary>
Motivation: 3D解剖理解对临床诊断与治疗规划至关重要，但体积成像昂贵且等待时间长；现有大模型多在自然图像上训练，能否将几何先验迁移到医学数据尚不明确，缺乏系统、可控的零样本评测。

Method: 构建受控的零样本基准：选择五个SOTA图像到3D模型（SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, TripoSG），在六个医学数据集（含解剖与病理结构）与两个自然数据集上评测；采用体素重叠类指标与点云距离类全局拓扑指标进行比较分析。

Result: 在医学数据上，各模型体素重叠仅中等，显示从单张切片推断体积时的深度重建失败模式；全局距离指标能更好区分方法：SAM3D与真实3D在整体拓扑上最相似，其他模型更易产生过度简化的重建。

Conclusion: 单切片医学图像到3D重建受二维平面导致的深度歧义所限，当前方法在体素一致性上受阻。建议采用多视角/多切片的图像到3D重建策略，以提升医学3D推断的可靠性。

Abstract: A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>


### [19] [K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge](https://arxiv.org/abs/2602.09411)
*Zhikai Li,Jiatong Li,Xuewen Liu,Wangbo Zhao,Pan Du,Kaicheng Zhou,Qingyi Gu,Yang You,Zhen Dong,Kurt Keutzer*

Main category: cs.CV

TL;DR: 提出K-Sort Eval：基于VLM的生成模型评测框架，通过后验校正与动态匹配，实现与人类偏好一致且高效的排序评估，通常少于90次模型运行即可达到与众包Arena一致结果。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型评测依赖人工投票（Arena）成本高、扩展差；直接用VLM代替人工易受幻觉与偏见影响且静态评估低效，亟需一种既人类对齐又高效可扩展的自动化评测方法。

Method: 1) 构建K-Sort Arena数据集：从上千条人工投票中提炼，样本含K个模型的输出与排名；2) 评估新模型时与已有K模型进行(K+1)-wise自由混战比较，由VLM给出排序；3) 后验校正：在贝叶斯更新中根据VLM预测与人类监督一致性自适应调整后验概率，减轻VLM偏差并提升对齐；4) 动态匹配：在对局选择上平衡不确定性与多样性，最大化单次比较的期望信息增益/效益，提高评估效率。

Result: 在多组实验中，K-Sort Eval与K-Sort Arena的人类偏好结果高度一致，且通常少于90次模型运行即可完成可靠评估，显示出显著的效率与稳定性。

Conclusion: VLM可在经过后验校正与动态匹配策略加持下，成为可靠、可扩展、与人偏好一致的生成模型评测器；K-Sort Eval在效率与一致性上优于现有静态或纯人工方案，可作为实用的人类对齐评测替代。

Abstract: The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>


### [20] [LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging](https://arxiv.org/abs/2602.09413)
*Xinyu Wang,Ke Deng,Fei Dou,Jinbo Bi,Jin Lu*

Main category: cs.CV

TL;DR: 提出LARV：在不需数据/训练的前提下，为任务向量合并方法提供逐层自适应缩放，抑制浅层干扰、增强深层对齐，显著提升多种合并规则在ViT上的多任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量合并（如TIES、TSV-M、Iso-C/CTS）基本对各层一视同仁，忽略ViT中“浅层易受干扰、深层更稳健且任务特定”的层异质性，导致合并后性能受损与不稳定。

Method: 提出LARV（Layer-wise Adaptive Rescaling Veneer），作为与合并器无关的“贴层式”模块：在聚合前为每个任务向量的每一层分配缩放系数。缩放由数据无关的层代理指标计算，再经轻量规则映射为尺度；支持分层（两/三段固定值）与连续映射，多种实例无需重训、可直接插拔到任意合并规则中。核心思想是浅层缩小、深层放大。

Result: 在FusionBench的ViT模型（B/32、B/16、L/14）与8/14/20任务设置下，LARV稳定提升所有任务向量基线；如Iso-C+LARV达85.9%(ViT-B/32)、89.2%(ViT-B/16)、92.6%(ViT-L/14)。层分析与扰动实验显示：浅层干扰被抑制，深层任务稳定特征被适度放大。

Conclusion: 通过逐层自适应缩放，LARV将模型合并从“层统一”变为“层感知”，在零数据、零训练、合并器无关的条件下稳健提升多任务合并性能，分层（tiered）方案最稳健，连续映射作为消融参考。

Abstract: Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.

</details>


### [21] [Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting](https://arxiv.org/abs/2602.09415)
*Joe-Mei Feng,Hsin-Hsiung Kao*

Main category: cs.CV

TL;DR: 提出一种用于非线性逆问题（具有块结构参数）的算子理论框架，给出稳定性与统计集中性的统一结果，并以高斯Splatting渲染算子为实例，导出与分辨率相关的稳定—分辨率权衡与显式Lipschitz常数。


<details>
  <summary>Details</summary>
Motivation: 现代成像与可微渲染中的高维、非线性、块结构模型普遍存在，但缺乏与具体重建算法无关的“算子层面”稳定性与统计误差界。需要一个统一框架，能同时刻画噪声影响、可辨识性、几何光滑性，并给出非渐近概率保证。

Method: 基于算子理论，假设前向算子满足（1）块级Lipschitz几何，（2）局部可辨识性，（3）次高斯噪声。证明：—确定性稳定不等式；—最小二乘失配泛函的全局Lipschitz界；—非渐近集中不等式。由此得到与重建算法无关、仅依赖前向算子的高概率参数误差上界。进一步对Gaussian Splatting渲染算子验证上述假设，计算其Lipschitz常数与分辨率相关的可观测性常数。

Result: 获得内在于前向算子的误差上界与集中率，且不依赖具体算法；对Gaussian Splatting给出显式常数与可验证条件，证明其满足框架假设；推导出估计误差受图像分辨率与模型复杂度之比的根本限制。

Conclusion: 该框架刻画了现代高维非线性逆问题的算子级极限：存在稳定—分辨率不可避免的权衡。结果提供统一的稳定性与统计保证，并对可微渲染（如Gaussian Splatting）给出可计算常数与实践指导。

Abstract: We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are intrinsic to the forward operator and independent of any specific reconstruction algorithm. As a concrete instantiation, we verify that the Gaussian Splatting rendering operator satisfies the proposed assumptions and derive explicit constants governing its Lipschitz continuity and resolution-dependent observability. This leads to a fundamental stability--resolution tradeoff, showing that estimation error is inherently constrained by the ratio between image resolution and model complexity. Overall, the analysis characterizes operator-level limits for a broad class of high-dimensional nonlinear inverse problems arising in modern imaging and differentiable rendering.

</details>


### [22] [Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification](https://arxiv.org/abs/2602.09425)
*Yiqiao Li,Bo Shang,Jie Wei*

Main category: cs.CV

TL;DR: 提出一种将路侧稀疏LiDAR点云转化为深度编码2D图像代理并接入现成VLM、无需参数微调的卡车细粒度分类框架，在少样本(每类16–30)下取得与监督基线相当的准确率，并可冷启动标注以训练轻量模型。发现“语义锚”效应：极低样本(k<4)时文本引导有益，更多样本时因语义失配而有害。


<details>
  <summary>Details</summary>
Motivation: ITS需要对卡车进行细粒度识别，但基于LiDAR的现有方法依赖大量标注和监督深度学习，难以扩展。VLM具备少样本泛化能力，但点云与图像之间存在模态鸿沟，限制其在路侧LiDAR上的应用。

Method: 提出无微调框架：通过深度感知图像生成管线把稀疏、遮挡的LiDAR扫描变为2D深度编码可视代理，并接入现成VLM进行文本-图像匹配。管线包含降噪、时空配准、朝向校正、形态学操作、各向异性平滑等。评估在包含20类车辆的真实数据集上，少样本(16–30/类)进行推理；同时用VLM生成标签为轻量监督模型冷启动。

Result: 在20类车辆数据集上，在每类16–30样本的少样本条件下取得与数据密集型监督基线竞争的准确率；特定甩挂集装箱卡车(20ft/40ft/53ft)的正确分类率超过75%，且完全无需训练或微调。观察到“语义锚”效应：k<4时文本引导提升稳定性；在更多样本时因语义不匹配而降低准确率。

Conclusion: 无需微调即可将VLM用于路侧LiDAR的细粒度卡车分类，显著减少初始人工标注成本，并可作为冷启动策略为后续轻量监督模型提供标签；但文本引导在样本较多时可能带来语义失配，需要自适应策略或提示优化。

Abstract: Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery. We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies. Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a "Semantic Anchor" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch. Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models. Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.

</details>


### [23] [SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL](https://arxiv.org/abs/2602.09432)
*Yang Zhao,Shizhao Sun,Meisheng Zhang,Yingdong Shi,Xubo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: 提出SceneReVis：通过迭代“诊断-行动”自反环解决一遍式3D场景合成的空间幻觉（如碰撞），并以多模态反馈进行显式冲突拦截与修复；构建SceneChain-12k因果搭建轨迹数据集；采用SFT→Agent式强化学习两阶段训练；在高保真生成与目标导向优化上达SOTA，并能泛化至长尾域。


<details>
  <summary>Details</summary>
Motivation: 当前单次（one-pass）3D场景合成缺乏审慎推理，易出现空间冲突与幻觉（如对象重叠、穿模）。需要一种能在生成过程中识别并纠正空间问题、并具备因果/步骤化构建证据的数据与训练范式，以提升可靠性与可控性。

Method: 1) 提出SceneReVis：引入视觉落地的自反（self-reflection）Agent框架，循环执行“诊断（检测空间冲突/不一致）-行动（调整布局/关系）”，以多模态反馈（图像、文本、可能含布局元信息）显式拦截并修复冲突。2) 构建SceneChain-12k：通过逆向工程生成因果构建轨迹（step-wise、带中间状态与纠错信号）。3) 两阶段训练：先监督微调学习基本构建与纠错技能，再进行“Agent式强化学习”以在交互环境中优化规划与决策，进化为主动空间规划器。

Result: 在多项基准上取得SOTA：更高的场景真实性/一致性、更少碰撞与空间矛盾；在目标导向优化任务中更高成功率；对长尾类别与复杂组合关系表现出稳健泛化。

Conclusion: 通过把自反式迭代推理、因果轨迹数据与Agent强化学习结合，SceneReVis显著缓解了3D场景合成中的空间幻觉问题，提升了高保真与可控生成能力，并具备更强的跨域泛化潜力。

Abstract: Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>


### [24] [Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning](https://arxiv.org/abs/2602.09439)
*Xu Ma,Yitian Zhang,Qihua Dong,Yun Fu*

Main category: cs.CV

TL;DR: 提出Fine-T2I，一个开放高质量的大规模文本生成图像微调数据集，含600万对高质图文对，显著提升多类预训练模型的生成质量与指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 开源社区用于T2I微调的数据集普遍存在分辨率低、文图对齐差、多样性不足等问题，导致与企业级模型存在明显性能差距；需要一个既大规模又高质量、可开放使用的数据集来弥合数据鸿沟。

Method: 构建Fine-T2I：覆盖10种任务组合、32类提示类别、11种视觉风格与5种提示模板；数据来源包括强大现代模型生成的合成图像与专业摄影师精筛的真实图像；通过严格的多阶段过滤确保文图对齐、视觉保真与提示质量，淘汰>95%初始候选，形成约600万图文对（约2TB）。在多种扩散与自回归预训练模型上进行微调评测。

Result: 在多样的预训练模型上，使用Fine-T2I微调后，生成质量和指令遵循能力均一致提升；通过人工评估、可视化对比与自动指标验证。

Conclusion: Fine-T2I作为开放许可的数据集，在规模上接近预训练数据，同时保持微调级质量，可有效提升T2I模型表现，有望缩小开源模型与企业级模型的差距。

Abstract: High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>


### [25] [A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index](https://arxiv.org/abs/2602.09446)
*Mohammad Masudur Rahman,Md. Rashedur Rahman,Ashraful Islam,Saadia B Alam,M Ashraful Amin*

Main category: cs.CV

TL;DR: 综述深度学习在城市视觉污染（UVP）检测/分类与应用的研究版图，揭示数据与方法碎片化，提出含视觉污染指数的监测框架与统一管理体系需求。


<details>
  <summary>Details</summary>
Motivation: UVP影响城市美学与居民福祉，但自动化检测与实际应用零散、标准不一，难以支撑城市治理决策，需要系统梳理方法、数据与应用缺口并提出统一框架。

Method: 按PRISMA-ScR流程在7大学术数据库系统检索与筛选，共纳入26篇涉及深度学习检测与分类的研究；比较模型（YOLO、Faster R-CNN、EfficientDet等）、数据集特征、类别体系、实时集成与地域分布；并提出包含视觉污染指数的应用框架。

Result: 现有研究多聚焦单一或少数污染子类，常用目标检测架构的变体；数据集地域受限、类别体系缺乏统一；少量工作落地为实时系统且地理分布失衡。本文提出监测框架，融入视觉污染指数用于区域严重度评估。

Conclusion: 呼吁建立统一的UVP管理系统：标准化污染物分类法、跨城市基准数据集、可泛化的深度学习模型与可操作的评估指数，以支持可持续城市美学与居民福祉提升。

Abstract: Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management. Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures. Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area. This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.

</details>


### [26] [Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing](https://arxiv.org/abs/2602.09449)
*Yan Luo,Henry Huang,Todd Y. Zhou,Mengyu Wang*

Main category: cs.CV

TL;DR: 论文提出无需再训练的潜变量轨迹平滑方法（Look-Ahead与Look-Back）来改进基于流匹配的扩散/ODE生成，直接在潜空间调整z而非修改速度场v，从而减少误差累积，并在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 训练自由的流匹配改进方法通常通过修改速度场v，但这会使误差沿生成全路径传播；相反，调整潜变量轨迹z可由已训练的速度网络自校正，潜在地更稳健、成本更低且更易泛化。

Method: 基于流匹配将扩散过程视为确定性ODE。提出两种无需再训练的潜空间轨迹平滑：1) Look-Ahead：用曲率门控的权重对当前与下一步潜变量做加权平均；2) Look-Back：对潜变量做指数滑动平均（EMA）以抑制噪声与高频摆动。两者利用过去/未来的v与z信息直接在潜空间精炼生成路径。

Result: 在COCO17、CUB-200、Flickr30K等数据集上，通过多种评测指标进行广泛实验，所提方法显著优于多种SOTA训练自由方法，生成质量全面提升。

Conclusion: 在流匹配生成框架中，直接平滑潜变量轨迹比改动速度场更稳健、误差更少。Look-Ahead与Look-Back互补、简单高效、无需再训练，并在多数据集上取得一致增益。

Abstract: Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.

</details>


### [27] [ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs](https://arxiv.org/abs/2602.09475)
*James Burgess,Rameen Abdal,Dan Stoddart,Sergey Tulyakov,Serena Yeung-Levy,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 预训练VLM已蕴含检测合成图像伪影的能力；通过少量样本与“脚手架”式提示/架构即可解锁，ArtifactLens以极少标注数据在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 生成模型图像越来越逼真，但仍存在手部畸形、物体扭曲等伪影。若无法有效检测：无法客观评估生成器、也难以训练改进用的奖励模型。现有方法需在数万标注上微调VLM，代价高且难以适应新伪影与模型迭代。

Method: 提出ArtifactLens：利用预训练VLM的潜在能力，结合“脚手架”式多组件架构，包括：少样本/上下文学习、文本指令优化，以及各模块的新改进。每类伪影仅需数百标注样本即可训练/适配；并进行跨数据集评测。

Result: 在五个人工伪影基准上（首次跨多数据集统一评测）取得SOTA，同时相比现有方法减少数个数量级的标注需求；并能泛化到其他伪影类型与AIGC检测任务。

Conclusion: 不必重度微调VLM即可高效检测生成图像伪影；通过合理的提示与架构设计，用少量标注即可获得强泛化与SOTA性能，为快速适配新伪影与生成器迭代提供实用方案。

Abstract: Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.

</details>


### [28] [FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation](https://arxiv.org/abs/2602.09476)
*Chuanhai Zang,Jiabao Hu,XW Song*

Main category: cs.CV

TL;DR: 提出FD-DB：频率解耦的双分支合成到真实风格迁移模型，低频用可解释编辑， 高频用残差补偿，并通过门控融合与两阶段训练在不破坏结构的前提下显著提升真实域一致性与下游分割表现。


<details>
  <summary>Details</summary>
Motivation: 合成数据标注便宜但与真实域存在外观与成像差异，导致严重域偏移并损害几何敏感任务效果。无配对的合成到真实翻译能缩小差距，但现有方法在“逼真度—结构稳定”之间两难：自由生成会扭曲结构或引入伪纹理，过强约束又抑制对真实统计的适配。

Method: 提出频率解耦双分支（FD-DB）：1）可解释编辑分支预测物理可解释参数（白平衡、曝光、对比度、饱和度、模糊、颗粒）构建稳定的低频外观基底，强化内容保持；2）自由分支生成高频残差补充细节；3）门控融合在显式频率约束下整合两分支，限制低频漂移；4）两阶段训练：先稳定编辑分支，再释放残差分支，提升优化稳定性。

Result: 在YCB-V数据集上，FD-DB提升了与真实域外观的一致性，并在保持几何与语义结构的同时，显著提升下游语义分割性能。

Conclusion: 频率解耦与可解释编辑结合的双分支与门控融合、两阶段训练，有效解决合成到真实翻译中逼真度与结构保持的权衡，改善域一致性并带来下游任务显著收益。

Abstract: Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>


### [29] [Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings](https://arxiv.org/abs/2602.09477)
*Bodong Zhang,Xiwen Li,Hamid Manoochehri,Xiaoya Tang,Deepika Sirohi,Beatrice S. Knudsen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: 提出WeakSupCon：在弱监督MIL场景下，用包级标签参与对比学习预训练编码器，生成更可分离的补丁特征，从而提升下游MIL性能。


<details>
  <summary>Details</summary>
Motivation: WSI为吉像素级，标注昂贵且多为切片级标签。现有MIL多用固定的补丁特征并专注聚合策略，忽视在弱监督条件下的特征表示预训练，导致实例区分度不足、下游性能受限。

Method: 设计弱监督对比学习框架WeakSupCon：在预训练阶段利用包（切片）级标签构造正负对，避免实例级伪标签；通过对比损失将来自同类包的补丁拉近、异类包的补丁拉远，学习到在特征空间中按类别分离的补丁嵌入；之后将该编码器特征输给标准MIL聚合器进行分类。

Result: 在三个数据集上，弱监督对比预训练生成的图像特征相较自监督对比学习特征，能显著提升下游MIL分类表现（文中称均有提升）。

Conclusion: 在MIL场景中，结合包级标签进行对比式特征预训练能不依赖实例伪标签却获得更判别的补丁表示，进而稳定提升WSI弱监督诊断性能；方法通用，可与现有MIL聚合器兼容。

Abstract: Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.
  In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>


### [30] [Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions](https://arxiv.org/abs/2602.09483)
*Lin Chen,Xiaoke Zhao,Kun Ding,Weiwei Feng,Changtao Miao,Zili Wang,Wenxuan Guo,Ying Wang,Kaiyuan Zheng,Bo Zhang,Zhe Li,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出Align-TI，一个面向“Token交互”的多模态大模型蒸馏框架，通过对视觉-指令交互与生成过程中token转移动态的对齐，在更小模型上保留理解与生成能力，实验优于传统KD，并以2B规模超过更大7B模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs体量大、部署困难；主流知识蒸馏多做静态下一词对齐，忽视动态token间交互（跨模态信息抽取与响应内连贯生成的关键）。需要一种能捕获并迁移“交互过程”的蒸馏方法。

Method: 提出Align-TI，从两类交互出发：1) 视觉-指令交互(IVA)：让学生在与指令相关的显著视觉区域上与教师对齐，模仿从图像中抽取关键信息的能力；2) 响应内token交互(TPA)：对齐教师的序列生成动态，通过token到token的转移概率分布进行学习，捕获生成逻辑。整体作为KD框架替代或补充vanilla KD的next-token对齐。

Result: 大量实验显示优于基线：相对Vanilla KD提升2.6%；蒸馏得到的Align-TI-2B在若干评测上超越体量更大的LLaVA-1.5-7B 7.0%，达到新的SOTA参数高效多模态蒸馏效果。

Conclusion: 对齐“Token交互”是高效蒸馏MLLM的关键。Align-TI有效迁移教师的跨模态信息抽取与动态生成能力，显著提升小模型性能并刷新参数高效MLLM蒸馏SOTA；代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>


### [31] [OSI: One-step Inversion Excels in Extracting Diffusion Watermarks](https://arxiv.org/abs/2602.09494)
*Yuwei Chen,Zhenliang He,Jia Tang,Meina Kan,Shiguang Shan*

Main category: cs.CV

TL;DR: 提出One-step Inversion (OSI) 方法，用一次前向推理高效提取“Gaussian Shading”风格的扩散水印，比多步反演快约20倍、更准且承载量翻倍。


<details>
  <summary>Details</summary>
Motivation: 现有将水印嵌入扩散初始噪声的训练免方法对图像质量影响小，但提取需多步扩散反演以重构精确初始噪声，计算昂贵、耗时且易受误差累积影响。需要一种更快、更稳健的提取方式。

Method: 将水印提取重构问题转化为“噪声符号(sign)分类”任务：不再回归连续初始噪声，而是判别每个噪声维度/像素的符号位。以扩散模型为骨干进行初始化，在合成的噪声-图像对上，使用符号分类损失微调，使模型能一跳从生成图像预测噪声符号，从而在一次前向中完成提取。方法适配不同调度器、扩散骨干与加密方案。

Result: 与多步扩散反演相比：速度提升约20倍；水印提取准确率更高；可承载的水印比特数约翻倍。实验覆盖多种scheduler、扩散骨干与密码学方案，均显示一致增益。

Conclusion: 通过将连续反演转化为符号分类并以扩散骨干微调的单步推理框架，OSI实现了对训练免“Gaussian Shading”水印的高效、通用且更可靠的提取，显著降低计算成本并提升容量与准确率。

Abstract: Watermarking is an important mechanism for provenance and copyright protection of diffusion-generated images. Training-free methods, exemplified by Gaussian Shading, embed watermarks into the initial noise of diffusion models with negligible impact on the quality of generated images. However, extracting this type of watermark typically requires multi-step diffusion inversion to obtain precise initial noise, which is computationally expensive and time-consuming. To address this issue, we propose One-step Inversion (OSI), a significantly faster and more accurate method for extracting Gaussian Shading style watermarks. OSI reformulates watermark extraction as a learnable sign classification problem, which eliminates the need for precise regression of the initial noise. Then, we initialize the OSI model from the diffusion backbone and finetune it on synthesized noise-image pairs with a sign classification objective. In this manner, the OSI model is able to accomplish the watermark extraction efficiently in only one step. Our OSI substantially outperforms the multi-step diffusion inversion method: it is 20x faster, achieves higher extraction accuracy, and doubles the watermark payload capacity. Extensive experiments across diverse schedulers, diffusion backbones, and cryptographic schemes consistently show improvements, demonstrating the generality of our OSI framework.

</details>


### [32] [Equilibrium contrastive learning for imbalanced image classification](https://arxiv.org/abs/2602.09506)
*Sumin Roh,Harim Kim,Ho Yun Lee,Il Yong Chun*

Main category: cs.CV

TL;DR: 提出一种名为ECL的监督式对比学习框架，旨在在类别不平衡下实现表示与分类器的几何均衡，通过同时规整类内塌缩、类间等距和分类器-原型对齐，显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督式对比学习在长尾/不平衡数据上性能受限：1) 未显式对齐类均值/原型与分类器权重，影响泛化；2) 原型仅作为每类一个额外样本，受批内实例数影响，导致各类贡献不均衡。作者希望在不平衡条件下实现“规则单纯形”几何与分类器一致性，以提升鲁棒性与泛化。

Method: 提出Equilibrium Contrastive Learning (ECL)：1) 表示几何均衡：通过损失设计促使类内特征塌缩、类间类中心均匀分布（规则单纯形），并显式平衡“类平均特征”与“类原型”在对比损失中的权重，以消除批次不均衡带来的影响；2) 分类器-类中心几何均衡：引入对齐项将分类器权重向量与类原型对齐，保证决策边界与表示空间结构一致。

Result: 在三类长尾数据集（CIFAR-10(0)-LT、ImageNet-LT）及两类不平衡医学数据集（ISIC 2019、LCCT）上实验，ECL均超越现有面向不平衡分类的SOTA监督式对比学习方法。

Conclusion: 通过同时约束表示空间的规则单纯形结构与分类器-原型对齐，并平衡原型与类平均特征的贡献，ECL在不平衡场景下实现几何与决策的一致，带来稳定且优于SOTA的性能提升。

Abstract: Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>


### [33] [Robust Depth Super-Resolution via Adaptive Diffusion Sampling](https://arxiv.org/abs/2602.09510)
*Kun Wang,Yun Zhu,Pan Zhou,Na Zhao*

Main category: cs.CV

TL;DR: AdaDS 是一个适用于深度超分辨率的通用框架，通过自适应选择扩散逆过程的起始步并注入定制噪声，使预训练扩散模型的生成先验在未知/严重退化下主导重建，实现对多样退化的零样本鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 传统方法直接回归深度值，遇到未知或强退化会产生伪影与失真；需要一种在退化不确定、噪声模式多变时仍能稳健恢复高质量深度图的方法。

Method: 基于高斯平滑的收缩性质：前向加噪会减小退化输入与理想分布的差异并收敛到各向同性高斯先验。AdaDS先估计重建不确定度，自适应选择扩散逆过程的起始时间步，并注入匹配的不同比例噪声，使中间样本落入目标后验的高概率区域，从而让预训练扩散模型的生成先验在后续迭代中主导恢复。

Result: 在真实与合成基准上进行大量实验，AdaDS在零样本泛化与对多样退化模式的鲁棒性方面优于现有SOTA方法。

Conclusion: 利用收缩性质与自适应起点/噪声注入，将退化对齐到扩散先验的高概率区域，使扩散生成先验主导恢复，从而在未知或严重退化下实现稳健、通用的深度超分辨率。

Abstract: We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.

</details>


### [34] [Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems](https://arxiv.org/abs/2602.09515)
*Mas Nurul Achmadiah,Afaroj Ahamad,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: 提出一种基于帧间差分的轻量级快速目标检测IoT方案，在多种边缘设备与模型上验证，相比端到端方法显著提升准确率与能效并降低时延。


<details>
  <summary>Details</summary>
Motivation: IoT场景对能耗、时延和计算资源极其敏感，传统端到端检测在快速目标（如火车、飞机）下易失效且耗能高，需一种更适合边缘侧的高效检测策略。

Method: 以帧间差分作为前端触发与过滤，减少需送入AI分类/检测模型的帧与区域；在AMD Alveo U50、Jetson Orin Nano、Hailo-8 AI加速器上，结合ANN与Transformer模型（含MobileNet、YOLOX等）评估多类别（鸟、汽车、火车、飞机）。

Result: 在使用帧间差分的方案中，MobileNet表现出较高准确率、低时延与高能效；YOLOX准确率、时延与效率均为最低。总体较端到端方法：平均准确率提升28.314%，能效提升3.6倍，时延降低39.305%。快速目标（火车、飞机）准确率仍相对较低。

Conclusion: 帧间差分+轻量级模型的检测算法适合IoT边缘快速运动目标场景，能在保证较高准确率的同时显著降低能耗与时延；端到端方法在此类任务上可能失效。

Abstract: This paper presents an Internet of Things (IoT) application that utilizes an AI classifier for fast-object detection using the frame difference method. This method, with its shorter duration, is the most efficient and suitable for fast-object detection in IoT systems, which require energy-efficient applications compared to end-to-end methods. We have implemented this technique on three edge devices: AMD AlveoT M U50, Jetson Orin Nano, and Hailo-8T M AI Accelerator, and four models with artificial neural networks and transformer models. We examined various classes, including birds, cars, trains, and airplanes. Using the frame difference method, the MobileNet model consistently has high accuracy, low latency, and is highly energy-efficient. YOLOX consistently shows the lowest accuracy, lowest latency, and lowest efficiency. The experimental results show that the proposed algorithm has improved the average accuracy gain by 28.314%, the average efficiency gain by 3.6 times, and the average latency reduction by 39.305% compared to the end-to-end method. Of all these classes, the faster objects are trains and airplanes. Experiments show that the accuracy percentage for trains and airplanes is lower than other categories. So, in tasks that require fast detection and accurate results, end-to-end methods can be a disaster because they cannot handle fast object detection. To improve computational efficiency, we designed our proposed method as a lightweight detection algorithm. It is well suited for applications in IoT systems, especially those that require fast-moving object detection and higher accuracy.

</details>


### [35] [A Universal Action Space for General Behavior Analysis](https://arxiv.org/abs/2602.09518)
*Hung-Shuo Chang,Yue-Cheng Yang,Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,James C. Liao,Chien-Chang Chen,Hen-Hsen Huang,Hong-Yuan Mark Liao*

Main category: cs.CV

TL;DR: 提出一个通用动作空间（UAS），用人类动作大规模数据训练，迁移用于哺乳动物与黑猩猩行为分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于手工特征与轨迹建模的方法鲁棒性与泛化差，难以可靠识别与分析动物/人类行为。深度学习与ImageNet式大规模监督带来高层表征学习机会，因此希望构建一个可泛化的“动作词典”，统一表征并促进跨物种行为理解。

Method: 汇集并整合现有标注的人类动作数据集，训练出一个大规模“通用动作空间”（UAS）作为高层表示；随后将该UAS迁移/应用到哺乳动物和黑猩猩行为数据上，用于行为分析与分类；提供代码实现。

Result: UAS能够在非人类（哺乳类、黑猩猩）数据上用于行为分析与分类，显示跨域可用性；（摘要未给出具体数值或基准对比）。

Conclusion: 利用人类动作大数据学得的通用动作表征可以作为跨物种行为分析的基础，有望替代脆弱的低层手工流程并提升泛化能力；代码已开源以便复现与拓展。

Abstract: Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>


### [36] [Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs](https://arxiv.org/abs/2602.09521)
*Jingyi Wang,Fei Li,Rujie Liu*

Main category: cs.CV

TL;DR: 提出一种无需训练的注意力干预算法，通过提升与任务相关的视觉token注意力并在解码中偏好高视觉注意路径，显著减少LVLM幻觉且不损害准确性与连贯性。


<details>
  <summary>Details</summary>
Motivation: LVLM常出现视觉注意力不足，导致将不相关或不存在的视觉信息写入文本（幻觉）。以往做法整体放大视觉注意，连带放大与任务无关token，副作用大。因此需要一种能有选择地提升与任务相关视觉token关注度的方法，降低幻觉同时保持生成质量。

Method: 1) 依据“任务相关token通常与文本具有较高视觉-文本相似度”的假设，从跨模态（vision-text）交叉注意力中提取子矩阵，度量视觉-文本关联；2) 以此构建重加权矩阵，仅提升高关联（任务相关）视觉token的权重，抑制无关token；3) 在beam search中注入视觉注意值作为额外打分项，优先选择整体视觉注意更高的候选，增强视觉证据对解码的影响；4) 全流程无需额外训练，可直接作用于主流LVLM。

Result: 在多种主流LVLM与多个评测上显著降低幻觉发生率，同时保持或提升答案的准确性与连贯性。

Conclusion: 选择性地增强任务相关视觉注意与在解码中引入视觉注意偏置，可在无需训练的前提下有效缓解LVLM幻觉问题，并具备良好的通用性与可移植性。

Abstract: Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>


### [37] [Singpath-VL Technical Report](https://arxiv.org/abs/2602.09523)
*Zhen Qiu,Kaiwen Xiao,Zhengwei Lu,Xiangyu Liu,Lei Zhao,Hao Zhang*

Main category: cs.CV

TL;DR: 提出Singpath-VL：面向宫颈细胞学的视觉-语言大模型，利用三阶段合成标注数据与多阶段微调，实现细粒度形态理解与细胞级诊断优于通用模型，并计划开源部分数据与基准。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在计算病理学进展迅速，但在细胞病理尤其宫颈细胞学缺乏大规模高质量标注数据，导致应用受限，需要专用模型与数据构建方案。

Method: 设计三阶段数据合成管线：以通用MLLM作弱标注→共识融合+专家知识注入精炼→生成高保真细胞形态描述；基于该百万级图文数据，对Qwen3-VL-4B进行多阶段微调，得到专用细胞病理MLLM（Singpath-VL）。

Result: Singpath-VL在细粒度形态感知与细胞级诊断分类上表现优越，超过通用模型；提供部分合成数据与基准以促进研究。

Conclusion: 弱标注融合与专家注入的合成数据可有效弥补标注稀缺，结合多阶段微调能打造在宫颈细胞学任务上强大的专用MLLM，具备研究与临床辅助潜力，并通过开源推动社区发展。

Abstract: We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>


### [38] [HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.09524)
*Han Zhou,Yuxuan Gao,Yinchao Du,Xuezhe Zheng*

Main category: cs.CV

TL;DR: 提出HLGFA：通过高低分辨率跨尺度特征一致性学习“正常”，无需像素级重建；以结构/细节先验引导对齐，并用噪声感知增强抑制干扰；在MVTec AD上实现97.9%像素AUROC与97.5%图像AUROC，优于代表性方法。


<details>
  <summary>Details</summary>
Motivation: 工业外观检测中缺陷样本稀缺、环境噪声多且需高可靠无监督检测。重建式/特征式UAD常受像素重建伪影或域偏移影响，难以稳健刻画“正常性”。作者动机是利用同一正常样本在高/低分辨率表征间应保持一致的先验，用跨分辨率一致性替代重建误差作为异常判据。

Method: - 双分辨率输入，经共享且冻结的主干提取多层特征。
- 将高分辨率表征分解为结构先验与细节先验，作为条件信号调制低分辨率特征：条件调制+门控残差校正，逐级细化低分辨率特征以与高分辨率对齐。
- 训练时仅用正常数据，最小化跨分辨率对齐损失；并引入噪声感知数据增强以抑制工业噪声诱发的伪响应。
- 推理时以跨分辨率对齐的崩溃/偏差作为异常分数，生成像素级热图与图像级分数。

Result: 在MVTec AD上达97.9%像素级AUROC、97.5%图像级AUROC；相对典型重建式与特征式方法取得更优性能，并在多基准上验证有效性。

Conclusion: 跨分辨率特征一致性可作为稳健的正常性建模信号，避免像素重建缺陷；结构/细节先验引导与噪声感知增强共同提升对噪声与纹理干扰的鲁棒性。HLGFA在标准基准上实现SOTA或接近SOTA表现。

Abstract: Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required. In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction. Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down. In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

</details>


### [39] [SchröMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schrödinger Bridge Problem](https://arxiv.org/abs/2602.09528)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 提出SchröMind框架，通过解Schrödinger bridge在token层面将“幻觉”激活映射到“真实”激活，显著降低MLLM幻觉，保持原能力且计算开销小，在POPE与MME达SOTA。


<details>
  <summary>Details</summary>
Motivation: MLLM在通用任务上表现突出，但在医疗等高风险领域受制于幻觉问题：生成文本与视觉证据不符。作者认为模型能理解图像，但自回归生成易被小扰动带入不真实状态，且难自我纠错，亟需一种在不破坏原能力的前提下降低幻觉的方法。

Method: 将幻觉视为从真实分布漂移的状态，构建Schrödinger bridge以最小传输代价在token级别学习从“幻觉激活”到“真实激活”的映射。通过轻量训练对模型内部激活进行校正，保持主干参数与能力；推理时以极小额外开销应用该映射。

Result: 在POPE和MME基准上取得SOTA，显著降低视觉不一致的生成，同时仅引入最小的计算开销；原有通用能力基本保持。

Conclusion: 通过SB最优传输视角对MLLM自回归幻觉进行校正是有效且高效的。SchröMind在不牺牲模型通用性的情况下减少幻觉，适合向高风险场景扩展。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchröMind-a novel framework reducing hallucinations via solving the Schrödinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schrödinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>


### [40] [SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection](https://arxiv.org/abs/2602.09529)
*Emad Gholibeigi,Abbas Koochari,Azadeh ZamaniFar*

Main category: cs.CV

TL;DR: 提出SCA-Net用于遥感双时相建筑与道路变化检测，相比现有方法在小目标、效率与稳定性上显著提升：mIoU在LEVIR-MCI上提升2.64%，小建筑IoU提升57.9%，训练时间降低61%。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习变化检测模型对小目标敏感性低、计算开销大、训练不稳定，难以满足城市管理、环境监测和灾害评估对高精度且高效率的需求。

Method: 在Change-Agent框架上增强：1) 差异金字塔块（Difference Pyramid Block）进行多尺度变化表征；2) 自适应多尺度处理模块，融合形状感知块与高分辨率增强块；3) 多级注意力（PPM与CSAGate）联合建模上下文与细节；4) 动态复合损失与四阶段训练策略以稳定训练、加速收敛。

Result: 在LEVIR-CD与LEVIR-MCI上全面评估，较Change-Agent与其他SOTA取得更优结果：LEVIR-MCI上mIoU提升2.64%，小建筑IoU提升57.9%，且训练时间减少61%。

Conclusion: SCA-Net在精度、对小目标的鲁棒性与训练效率间实现良好平衡，为实际变化检测提供高效、准确、稳健的解决方案。

Abstract: Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>


### [41] [DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment](https://arxiv.org/abs/2602.09531)
*Bohan Fu,Guanyi Qin,Fazhan Zhang,Zihao Huang,Mingxuan Li,Runze Hu*

Main category: cs.CV

TL;DR: 提出DR.Experts：以失真先验为核心的无参考图像质量评价框架，通过失真感知VLM获取先验、差异化显著性模块净化先验、动态加权专家模块融合与加权失真特征，显著提升与人类主观一致性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA多基于统一特征与评分的浅层关联，缺乏可靠的“失真先验”，对细微失真不敏感，导致与人主观感知不一致与性能瓶颈。

Method: 1) 利用退化感知的视觉-语言模型提取失真类型相关的先验表示；2) 通过“失真-显著性差分模块”(Distortion-Saliency Differential) 将失真注意与语义显著性区分并净化先验，保证先验真正表征失真；3) 设计“动态失真加权模块”(Mixture-of-Experts风格) 融合 refined 先验、语义与桥接表示，根据各失真对感知质量的影响进行动态加权，输出质量分数。

Result: 在五个具有挑战性的BIQA基准上优于现有方法，并在泛化与数据效率方面表现突出。

Conclusion: 引入显式、可区分的失真先验并以MoE式动态加权融合，可显著提升无参考图像质量评价与人类感知的一致性与跨数据集鲁棒性。

Abstract: Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>


### [42] [RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes](https://arxiv.org/abs/2602.09532)
*Michael Baltaxe,Dan Levi,Sagie Benaim*

Main category: cs.CV

TL;DR: 提出RAD：一种检索增强的单目度量深度估计框架，通过不确定性感知检索相似RGB-D样本、双流网络处理、匹配式交叉注意力在可靠对应处传递几何，显著提升复杂场景中稀有/低频类别的深度精度，并在NYU/KITTI/Cityscapes上对这些类别相对绝对误差分别降29.2%/13.3%/7.2%，常规基准保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有单目度量深度模型在复杂场景的长尾/弱代表类别上误差大，缺少多视几何约束；希望在仅单目输入下近似获得多视几何的结构先验，提高这类区域的深度可靠性且不牺牲整体性能。

Method: 提出RAD框架：1) 不确定性感知检索：定位输入图像低置信度区域，并从数据库检索语义相似的RGB-D上下文样本；2) 双流网络：分别编码输入与检索到的上下文特征；3) 匹配式交叉注意力（matched cross-attention）：仅在可靠点对应处进行几何信息传递与融合，抑制错误迁移；最终输出改进的度量深度。

Result: 在NYU Depth v2、KITTI、Cityscapes上，对弱代表类别显著优于SOTA：相对绝对误差减少29.2%、13.3%、7.2%；同时在常规的域内基准上保持有竞争力的整体表现。

Conclusion: 通过检索增强与匹配约束的跨注意力，将外部RGB-D先验的结构信息有效注入单目深度估计，尤其改善长尾类别的度量深度精度，兼顾提升与稳健性，显示出近似多视几何收益的潜力。

Abstract: Monocular Metric Depth Estimation (MMDE) is essential for physically intelligent systems, yet accurate depth estimation for underrepresented classes in complex scenes remains a persistent challenge. To address this, we propose RAD, a retrieval-augmented framework that approximates the benefits of multi-view stereo by utilizing retrieved neighbors as structural geometric proxies. Our method first employs an uncertainty-aware retrieval mechanism to identify low-confidence regions in the input and retrieve RGB-D context samples containing semantically similar content. We then process both the input and retrieved context via a dual-stream network and fuse them using a matched cross-attention module, which transfers geometric information only at reliable point correspondences. Evaluations on NYU Depth v2, KITTI, and Cityscapes demonstrate that RAD significantly outperforms state-of-the-art baselines on underrepresented classes, reducing relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.

</details>


### [43] [AUHead: Realistic Emotional Talking Head Generation via Action Units Control](https://arxiv.org/abs/2602.09534)
*Jiayi Lyu,Leigang Qu,Wenjing Zhang,Hanyu Jiang,Kai Liu,Zhenglin Zhou,Xiaobo Xia,Jian Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出AUHead：两阶段、可控的说话人脸生成。先用音频-语言大模型经“情感→AU”链式推理生成细粒度AU；再以AU驱动扩散模型合成视频，通过AU→2D人脸结构映射与跨注意建模，并在推理时用AU解耦引导平衡AU质量与身份一致性。实验在情感真实感、对口与视觉一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有谈话人脸生成难以表达细腻情绪，缺乏可控、可解释的细粒度情感控制。人脸表情学中的Action Units可精确刻画表情，但如何从语音中稳健提取并用于可控生成仍未解决。

Method: 两阶段：1) AU生成阶段：提出时空AU标记化，并利用音频-语言大模型的“先情感后AU”的链式思维，从语音中解耦并预测细粒度AU序列。2) 生成阶段：AU驱动的可控扩散模型：将AU序列映射为结构化2D面部表示以增强空间保真；在扩散网络中通过跨注意建模AU与视觉的交互；推理时引入AU解耦引导策略以在情绪表现与身份一致性间灵活权衡。

Result: 在基准数据集上实现更高的情感真实感、唇形同步准确度与视觉一致性，显著超过现有方法；展示了细粒度AU控制与可调AU质量-身份一致性折中能力。

Conclusion: 将AUs作为中间可控表示，并结合ALM的链式推理与AU驱动扩散生成，可有效捕获语音中的细腻情绪并产生高保真、可控的说话人脸视频，是提升情感可控性与真实性的有力方案。

Abstract: Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>


### [44] [Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination](https://arxiv.org/abs/2602.09541)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 提出Scalpel，在LVLM中通过在推理阶段重构多头注意力的激活分布，显著降低视觉幻觉；基于高斯混合与熵正则最优传输精确映射“幻觉-可信”注意模式，动态干预强度与方向，零额外训练与几乎零开销，SOTA。


<details>
  <summary>Details</summary>
Motivation: LVLM常受LLM强先验与跨模态注意错配影响，导致输出与图像不一致（幻觉）。现有方法要么需额外训练/数据，要么通用性差或代价高，缺乏在推理时对注意力进行细粒度、可信导向的校正机制。

Method: 在Transformer多头注意力层，预测每个head可信的注意方向；用高斯混合模型分别拟合“可信/幻觉”注意激活的多峰分布；利用熵正则最优传输（等价Schrödinger bridge）在两流形上精确对齐对应高斯分量；在解码时依据样本对各分量的归属与映射关系，动态调整（强度与方向）注意激活分布，实现对幻觉头/区域的抑制与对可信区域的增强。无需额外训练，仅单步解码干预。

Result: 在多数据集与基准上，Scalpel显著降低LVLM幻觉，优于现有方法并达SOTA。

Conclusion: 通过以GMM+熵OT为核心的注意力分布重塑，Scalpel在不依赖特定模型或数据、且几乎无额外计算的前提下，有效缓解LVLM视觉幻觉，为推理期可解释与可控的跨模态注意校正提供通用方案。

Abstract: Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>


### [45] [Delving into Spectral Clustering with Vision-Language Representations](https://arxiv.org/abs/2602.09586)
*Bo Peng,Yuanwei Hu,Bo Liu,Ling Chen,Jie Lu,Zhen Fang*

Main category: cs.CV

TL;DR: 提出一种利用预训练视觉-语言模型（VLM）对齐的多模态谱聚类方法，通过神经切线核和提示词正名词锚定构建亲和矩阵，并加入正则化的亲和扩散以自适应融合多提示，显著提升16个基准上的聚类表现。


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类多依赖单模态相似度，忽视多模态表示中的跨模态语义信息。随着VLM在跨模态对齐上的成功，作者希望把视觉语义对齐引入谱聚类以增强类内连通、抑制类间噪声。

Method: 1) 以预训练VLM的对齐空间为基础，用与目标图像语义相近的“正名词”作为锚；2) 构造神经切线核（NTK）并将视觉接近度与语义重叠耦合成新的图像亲和度；3) 理论与经验上表明该亲和能放大块对角结构；4) 设计正则化的亲和扩散机制，自适应集成由不同提示词诱导的多个亲和矩阵。

Result: 在16个数据集（包含经典、大规模、细粒度、域移场景）上，方法在无监督聚类指标上较现有SOTA实现显著领先。

Conclusion: 跨模态对齐+NTK的亲和建模与正则化亲和扩散可系统性增强谱聚类，提供从单模态到多模态的有效升级路径，并在多种基准上取得稳健优势。

Abstract: Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>


### [46] [MieDB-100k: A Comprehensive Dataset for Medical Image Editing](https://arxiv.org/abs/2602.09587)
*Yongfan Lai,Wen Qian,Bo Liu,Hongyan Li,Hao Luo,Fan Wang,Bohan Zhuang,Shenda Hong*

Main category: cs.CV

TL;DR: 提出MieDB-100k：一个10万规模、文本引导的医疗图像编辑数据集，通过专家模型与规则合成并经人工严审，覆盖感知、修改、变换三类编辑任务，显著提升模型性能与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有医疗图像编辑数据稀缺、种类单一、忽视医学理解，且难以在质量与规模间平衡，制约多模态生成模型在医疗编辑场景的适配与泛化。

Method: 构建MieDB-100k数据集：将任务按感知（Perception）、修改（Modification）、变换（Transformation）三维度组织；采用模态特定专家模型与规则驱动的数据合成流水线进行筛选与生成，并辅以严格的人工临床一致性核验；随后用该数据训练并评测文本引导的医疗图像编辑模型。

Result: 基于MieDB-100k训练的模型在多项实验中稳定优于开源与闭源对比基线，并展现出较强的跨任务与跨分布泛化能力。

Conclusion: MieDB-100k在规模、质量与多样性上填补了医疗图像编辑数据的空缺，可作为后续专科化医疗图像编辑研究与应用的基础设施与催化剂。

Abstract: The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.

</details>


### [47] [Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures](https://arxiv.org/abs/2602.09600)
*Yuxi Wang,Wenqi Ouyang,Tianyi Wei,Yi Dong,Zhiqi Shen,Xingang Pan*

Main category: cs.CV

TL;DR: 提出Hand2World：一种从单幅场景图像与自由空间手势生成长时程自我中心交互视频的自回归框架，兼顾低延迟、几何一致与稳定相机视角。关键做法：用投影3D手部网格做遮挡不变条件、用每像素Plücker射线嵌入解耦相机与手运动，并将双向扩散蒸馏为因果生成器以支持任意长度合成；在三大基准上显著提升感知质量与3D一致性并支持相机控制。


<details>
  <summary>Details</summary>
Motivation: 增强现实与具身智能需要能对用户手势快速响应且几何稳定的自我中心交互视频生成。然而仅凭单目视角与自由手势存在：训练/测试分布偏移（自由手势 vs 接触密集数据）、手与相机运动歧义、以及需要任意长度视频生成的挑战。现有方法在遮挡处理、相机漂移与长时程一致性上不足。

Method: 提出Hand2World统一自回归框架：1) 遮挡不变手部条件：将估计的3D手部网格投影到图像作为控制信号，让可见性/遮挡由场景上下文推断，而非硬编码；2) 相机几何注入：使用逐像素Plücker射线嵌入，显式提供相机内外参以解耦手/相机运动并抑制背景漂移；3) 自动单目标注流水线：用于生成训练所需的手-物体交互与相机信息；4) 模型蒸馏：把双向扩散模型蒸馏为因果（单向）生成器，实现低延迟与任意长度视频合成；5) 统一自回归生成支持头动与交互动作。

Result: 在三个自我中心交互基准上，相比现有方法显著提升：- 感知质量（更真实的外观与纹理）；- 3D一致性（手/物体/背景的几何稳定）；- 相机可控性（头动轨迹可指定）；- 长时程交互生成（无明显漂移、可持续生成）。

Conclusion: 通过遮挡不变的3D手部条件与Plücker射线几何注入，并将扩散蒸馏为因果生成，Hand2World在单目自我中心交互视频生成中同时解决了分布偏移、运动歧义与长时程稳定性问题，实现可控、稳定、真实的任意长度交互合成并在多基准上验证有效。

Abstract: Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>


### [48] [Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing](https://arxiv.org/abs/2602.09609)
*Jialun Liu,Yukuo Ma,Xiao Cao,Tian Li,Gonghu Shang,Haibin Huang,Chi Zhang,Xuelong Li,Cong Liu,Junqi Liu,Jiakui Hu,Robby T. Tan,Shiwen Zhang,Liying Yang,Xiaoyan Yang,Qizhen Weng,Xiangzhen Chang,Yuanzhi Liang,Yifan Xu,Zhiyong Huang,Zuoxin Li,Xuelong Li*

Main category: cs.CV

TL;DR: Tele-Omni提出一个统一的多模态视频生成与编辑框架，用LLM解析多模态指令并输出结构化意图，再用扩散模型按结构化信号合成高质量、时序一致的视频，覆盖多种任务并在多基准上具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式视频方法多为任务专用，主要依赖文本指令，难以统一处理多模态输入、上下文引用与多样化生成/编辑；许多编辑方案流水线繁琐、难以扩展与组合。需要一个能统一多任务、多模态控制且保持时序一致性的框架。

Method: 提出Tele-Omni：将预训练多模态大语言模型用于解析异构指令（文本、图像、参考视频），推断结构化的生成/编辑意图；再以扩散式视频生成器在这些结构化条件下进行视频合成。为实现跨异构任务的联合训练，设计任务感知的数据处理流程，将多模态输入统一为结构化指令格式并保留任务约束。支持文本生成视频、图像生成视频、首末帧生成、in-context视频生成与编辑等。

Result: 在多种视频生成与编辑任务上取得有竞争力的效果，表现出良好的时序一致性与视觉一致性，并实现灵活的多模态控制。

Conclusion: 通过将指令解析与视频合成解耦，并结合任务感知的数据设计，Tele-Omni在单一模型内统一多模态视频生成与编辑，提升可扩展性、可组合性与控制灵活性，同时维持高质量与稳定的时序表现。

Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>


### [49] [AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Linlin Wang*

Main category: cs.CV

TL;DR: AGMark 是一种针对大型视觉语言模型的自适应动态水印方法，利用注意力与不确定性信号在解码过程中逐步选择并水印“语义关键”词，既保证可检测性与抗攻击性，又显著提升后期生成的视觉语义一致性与文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有水印方案要么与视觉无关，强行注入伪随机偏置，破坏视觉对齐；要么一次性静态估计视觉权重且忽略权重分布密度，无法跟随生成过程中的视觉依赖动态变化，还可能在长尾引入低质量词。需要一种既可检测、又不损伤视觉语义与生成质量的多模态水印。

Method: 提出 Attention-Guided Dynamic Watermarking (AGMark)：在每个解码步，基于注意力权重识别与视觉相关的“语义关键证据”，并结合上下文连贯线索得到校准的证据权重分布；再依据不确定性（token 熵）与证据密度共同决定受保护词比例，进行自适应词表划分与水印嵌入，避免引入无关词。

Result: 与传统方法相比，AGMark 在生成质量上有可观提升，尤其在生成后期的视觉语义一致性更强；在检测性能上保持至少 99.36% AUC，在多种攻击下保持至少 88.61% AUC，且不增加推理开销。

Conclusion: AGMark 在不牺牲效率的前提下兼顾可检测性与鲁棒性，并显著提升视觉对齐与文本质量，为多模态水印设定了新的可靠性基线。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>


### [50] [Towards Training-free Multimodal Hate Localisation with Large Language Models](https://arxiv.org/abs/2602.09637)
*Yueming Sun,Long Yang,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出LELA：首个免训练、基于LLM的仇恨视频定位框架，通过多模态字幕与多阶段提示在帧级打分并进行时间定位，效果在HateMM与MultiHateClip上优于所有免训练基线。


<details>
  <summary>Details</summary>
Motivation: 现有视频仇恨检测要么高度依赖大规模人工标注的监督训练，要么难以做到细粒度的时间定位，难以兼顾可扩展性与精细度。

Method: 将视频分解为图像、语音、OCR、音乐、视频上下文五种模态；对各模态进行自动字幕/描述；设计多阶段提示让LLM对每帧计算仇恨分；提出组合匹配机制以强化跨模态推理；全流程免训练。

Result: 在HateMM与MultiHateClip两个数据集上，LELA显著超越所有免训练基线；并给出消融实验与可视化验证其有效性与可解释性。

Conclusion: LELA证明了利用LLM与模态特定字幕的免训练方案可实现精细的仇恨内容时间定位，具备可扩展、可解释的优势，为后续多模态安全检测奠定基础。

Abstract: The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>


### [51] [VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model](https://arxiv.org/abs/2602.09638)
*Hanqing Wang,Mingyu Liu,Xiaoyu Chen,Chengwei MA,Yiming Zhong,Wenti Yin,Yuhao Liu,Zhiqing Cui,Jiahao Yuan,Lu Dai,Zhiyuan Ma,Hui Xiong*

Main category: cs.CV

TL;DR: 提出VIDA视频式3D可供性数据集与VideoAfford基线，结合多模态大模型、潜在动作编码与空间感知损失，实现更强的3D可供性分割与开放域推理，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往3D可供性研究多依赖静态图像/语言，缺乏动态交互带来的时序与因果线索，限制了对可操作区域的准确定位与推理能力。

Method: 1) 构建VIDA：38K HOI视频、16种可供性、38类物体、22K点云。2) 提出VideoAfford：在多模态大语言模型框架中加入可供性分割头，统一进行世界知识推理与细粒度3D可供性定位。3) 设计潜在动作编码器，从HOI视频提取动态交互先验以增强动作理解。4) 引入空间感知损失，提升对3D空间关系的建模与分割质量。

Result: 在多项基准上显著超越现有方法；展现出良好的开放域泛化和可供性推理能力。

Conclusion: 利用视频中的动态时序与因果信息、结合多模态LLM与专用分割与损失设计，可显著提升3D可供性落地能力；数据与代码将公开，促进后续研究。

Abstract: 3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.

</details>


### [52] [Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation](https://arxiv.org/abs/2602.09648)
*Siyu Chen,Ting Han,Haoling Huang,Chaolei Wang,Chengzheng Fu,Duxin Zhu,Guorong Cai,Jinhe Su*

Main category: cs.CV

TL;DR: 提出Time2General用于域泛化视频语义分割，在未知域与不同采样率下仍保持时序一致与高精度。核心是时空记忆解码器与掩码化时序一致性损失，避免显式对应传播并抑制闪烁，实验在多数据集上显著优于DGSS/VSS基线，最高18 FPS。


<details>
  <summary>Details</summary>
Motivation: DGVSS需在单一标注驾驶域训练、零样本部署到未知域且不能做测试时自适应，同时保持视频逐帧预测的时序稳定。然而域移与采样率变化会破坏基于对应的传播与固定步长聚合，导致即使语义稳定区域也出现明显帧间闪烁。需要一种对域与时间采样变化都稳健、无需显式对应的时序一致方案。

Method: 1) 稳定性查询（Stability Queries）为核心范式。2) 设计时空记忆解码器：将多帧上下文聚合为片段级时空记忆，在不进行显式像素/特征对应传播的情况下，逐帧解码时序一致的分割掩码。3) 掩码化时序一致性损失：在不同时间步长（stride）上的预测间施加一致性正则，同时对训练步长进行随机化，增强对变采样率的鲁棒性并抑制闪烁。

Result: 在多种驾驶基准上显著提升跨域精度与时序稳定性，相较现有DGSS与VSS基线均有明显优势；推理速度最高可达18 FPS。

Conclusion: 通过时空记忆解码与跨步长一致性正则，Time2General在无需测试时自适应的前提下实现对域移与采样率变化的稳健DGVSS，减少帧间闪烁并提升跨域性能与实时性。

Abstract: Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>


### [53] [TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution](https://arxiv.org/abs/2602.09662)
*Deyang Jiang,Jing Huang,Xuanle Zhao,Lei Chen,Liming Zheng,Fanfan Liu,Haibo Qiu,Peng Shi,Zhixiong Zeng*

Main category: cs.CV

TL;DR: TreeCUA 通过“树结构可验证演化”来高效扩展 GUI 自动化数据与训练：用多智能体协作探索/验证/总结/评估，借助树拓扑去重复用节点、深广自适应探索、世界知识引导与全局记忆回溯，并提出利用树节点分支信息的 TreeCUA-DPO 强化 GUI 规划；在域内与 OOD 上均显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有 CUA 研究更偏重 GUI grounding，而真正限制可扩展性的关键在于 GUI 规划数据的高成本与难采集；CUA 在应用/网页中的探索天然呈树形，早期入口被高频访问。若把大规模轨迹组织为树，可降低重复、复用节点、降低成本并提升数据质量与覆盖。

Method: 1) 多智能体协作：探索环境、动作验证、轨迹总结与质量评估，生成高质量可扩展 GUI 轨迹；2) 树式拓扑：存储与复用重复探索节点，支持回放；3) 自适应探索：在深度(难度)与广度(多样性)间动态权衡；4) 世界知识引导与全局记忆回溯：抑制低质生成与死胡同；5) TreeCUA-DPO：基于丰富的树节点与相邻分支信息进行偏好优化，提升 GUI 规划能力。

Result: 在多项实验中 TreeCUA 与 TreeCUA-DPO 显著优于基线；跨域(OOD)实验显示出强泛化能力。

Conclusion: 将 GUI 轨迹组织为可验证的树结构并结合多智能体与分支感知偏好优化，可高效扩展高质量 GUI 规划数据与能力，带来显著性能与泛化提升；代码与轨迹节点信息将开源。

Abstract: Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>


### [54] [Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI](https://arxiv.org/abs/2602.09686)
*Boya Wang,Ruizhe Li,Chao Chen,Xin Chen*

Main category: cs.CV

TL;DR: 提出一个多任务深度学习框架，实现肝脏分割与肝纤维化分期，在多参数MRI（含3通道与7通道）上通过半监督分割-配准联合建模与基于patch的分期分类，兼顾ID与OOD泛化，并由挑战方在独立测试集验证；代码开源。


<details>
  <summary>Details</summary>
Motivation: 临床上肝纤维化评估困难，需要精准肝脏分割与可靠分期；多参数MRI存在标注稀缺、模态差异与域偏移，常规监督或单模态方法难以稳健泛化。

Method: 两阶段多任务框架：1）LiSeg：半监督学习，将分割与影像配准联合训练，利用有标注与无标注数据，对抗模态差异与域偏移；2）LiFS：基于patch的分类方法，生成可视化的分期地图。数据涵盖3通道（T1、T2、DWI）与7通道（T1、T2、DWI、GED1–GED4）。

Result: 在CARE Liver 2025 Track 4独立测试集上（含ID与OOD病例）完成评测，能处理多模态与域偏移情景；具体数值未给，但报告显示方法有效且泛化良好。

Conclusion: 联合分割-配准的半监督策略与patch级分期可在多参数MRI上稳健进行肝分割与纤维化分期，并对ID与OOD均具鲁棒性；代码已开源，便于复现与扩展。

Abstract: Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>


### [55] [GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation](https://arxiv.org/abs/2602.09701)
*Sandesh Hegde,Jaison Saji Chacko,Debarshi Banerjee,Uma Mahesh*

Main category: cs.CV

TL;DR: 提出GenSeg-R1：先“推理再分割”的细粒度指代分割框架，VLM输出结构化空间提示（框+关键点），SAM2将其转为高质量掩码；用GRPO对Qwen3-VL(4B/8B)强化微调，无需显式推理链标注，在RefCOCOg/GRefCOCO/ReasonSeg上显著超越Seg-Zero/Seg-R1等现有方法，并具备无目标检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有指代图像分割往往直接从文本到像素，难以解释、泛化与负例处理；同时高质量掩码生成可由强大的可提示分割器完成，若能把VLM的理解转化为空间提示并联动优化，有望提升精度、鲁棒性与可解释性。

Method: 采用解耦流水线：VLM接收图像与查询，先进行场景推理并输出结构化空间提示（每个目标的边界框+两个内部关键点）；冻结的可提示分割器SAM2将提示转为掩码。使用GRPO对Qwen3-VL(4B/8B)进行强化学习式微调，无需监督推理链标注。提出GenSeg-R1-G：在GRefCOCO上加入“环中SAM2”奖励，直接以掩码质量为回报优化。

Result: RefCOCOg验证：GenSeg-R1-8B达0.7127 cIoU与0.7382 mIoU，较Qwen3-VL Instruct基线分别+15.3/+21.9，且较Seg-Zero-7B在同设下+3.3 cIoU。GRefCOCO验证：GenSeg-R1-G目标mIoU 76.69%，无目标准确率82.40%，显著优于不具备无目标检测的Seg-R1-7B与Seg-Zero-7B。ReasonSeg测试：GenSeg-R1-4B mIoU 68.40%，较Seg-Zero-7B +7.0、Seg-R1-7B +10.7。

Conclusion: 将“推理”与“分割”解耦，用VLM生成结构化空间提示并由SAM2提掩码，再配合GRPO与掩码质量奖励，能在多基准上显著提升指代分割性能，并带来无目标识别与更强的可泛化性。

Abstract: We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.
  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.
  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>


### [56] [Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models](https://arxiv.org/abs/2602.09713)
*Ruisi Zhao,Haoren Zheng,Zongxin Yang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: Stroke3D从用户2D手绘笔划与文本描述直接生成可绑定动画的3D网格，先生成可控骨架，再基于骨架合成带纹理的网格，并通过数据与偏好优化提升对齐与质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成难以产生可动画化几何；传统rigging难以细粒度控制骨架结构。需要一种既能按语义、又能按用户结构意图控制的端到端可绑定3D生成方案。

Method: 两阶段：1) 可控骨架生成：提出Sk-VAE编码骨架图结构到潜空间，Sk-DiT在文本与2D笔划条件下生成骨架嵌入，再由VAE解码得到高质量3D骨架；2) 网格合成：以生成骨架为条件训练/微调骨架到网格模型。构建TextuRig数据集（来自Objaverse-XL的带纹理、绑定与字幕网格）扩充训练；提出SKA-DPO基于骨架-网格对齐分数的偏好优化以提升几何保真度与对齐。

Result: 方法首次实现基于用户2D笔划条件的绑定3D网格生成，实验显示能产生合理骨架与高质量、与骨架和文本一致的网格，几何保真度提高。

Conclusion: Stroke3D提供直观工作流：以笔划+文本控制骨架与网格的生成，填补可动画3D生成与可控rigging间的空白；数据增强与偏好优化进一步提升质量与对齐。

Abstract: Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.

</details>


### [57] [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](https://arxiv.org/abs/2602.09717)
*Radib Bin Kabir,Tawsif Tashwar Dipto,Mehedi Ahamed,Sabbir Ahmed,Md Hasanul Kabir*

Main category: cs.CV

TL;DR: 提出轻量级CNN→SNN系统化基准：用LIF与代理梯度训练，将ShuffleNet/SqueezeNet/MnasNet/MixNet转为SNN；在CIFAR-10/100与TinyImageNet评测，SNN在保持可比精度下最高节能15.7×。SNN‑SqueezeNet最佳，并通过结构化剪枝得SNN‑SqueezeNet‑P：CIFAR‑10精度+6%、参数‑19%，与CNN‑SqueezeNet精度仅差1%，能耗降88.1%。


<details>
  <summary>Details</summary>
Motivation: 以往关注大规模模型，忽视边缘设备适用的轻量级SNN；缺少统一设定下对紧凑CNN转SNN的系统比较与能效评估。

Method: 统一管线：将多种轻量级CNN转换为SNN，采用LIF神经元与代理梯度直接训练；在CIFAR-10/100、TinyImageNet上评估准确率、F1、参数量、计算复杂度与能耗；对表现最优的SNN‑SqueezeNet施加模块级结构化剪枝，形成稀疏脉冲计算模型SNN‑SqueezeNet‑P。

Result: 轻量级SNN总体在精度可比的同时能效最高达15.7×；SNN‑SqueezeNet在各数据集上最稳定优；剪枝后模型在CIFAR‑10上精度提升6%、参数减少19%，与CNN‑SqueezeNet精度差距缩至1%。

Conclusion: 轻量级SNN在边缘侧具备实用价值；通过结构化剪枝与脉冲稀疏性可进一步降低能耗（达88.1%）且保持高性能，为低功耗边缘智能提供可行路径。

Abstract: Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.

</details>


### [58] [Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings](https://arxiv.org/abs/2602.09730)
*Laura Paul,Holger Rauhut,Martin Burger,Samira Kabri,Tim Roith*

Main category: cs.CV

TL;DR: 提出一种用于绘画裂纹自动检测的混合方法：把图像分解为无裂纹图与裂纹成分，结合深度生成先验与Mumford–Shah变分模型，联合优化得到像素级裂纹定位。


<details>
  <summary>Details</summary>
Motivation: 非侵入成像与深度学习推动艺术品数字分析，但裂纹检测仍难：场景复杂且裂纹与笔触、发丝等相似，易误检。需要一种既能理解艺术品内容又能鲁棒分离裂纹的模型，服务于文物劣化评估与修复指导。

Method: 将裂纹检测表述为逆问题的图像分解：观测图=无裂纹绘画+裂纹。对“无裂纹绘画”使用深度生成模型作为强先验；对“裂纹”使用带裂纹先验的Mumford–Shah型变分泛函刻画其分段光滑、细长不连续结构。通过联合优化（数据项+先验项）求解，输出裂纹的像素级概率/指示图。

Result: 在联合优化下得到高分辨、像素级的裂纹定位图，并同时重建无裂纹版本的绘画。相较仅基于纹理/边缘或仅深度分割的方法，能更好地区分裂纹与类似艺术笔触结构（摘要暗示的性能提升）。

Conclusion: 混合生成先验与变分裂纹模型的逆问题框架，有效提升数字化绘画裂纹检测的准确性，可用于文物记录与修复决策；方法提供可解释的分解（无裂纹图+裂纹层）与可扩展的优化框架。

Abstract: Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>


### [59] [Toward Fine-Grained Facial Control in 3D Talking Head Generation](https://arxiv.org/abs/2602.09736)
*Shaoyang Xie,Xiaofeng Cong,Baosheng Yu,Zhipeng Gui,Jie Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TL;DR: 提出FG-3DGS，通过频率感知的区域解耦与后处理对齐机制，提升音频驱动说话人头的精细控制、时序一致性与唇形同步，超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽能实时高保真渲染，但细粒度表情控制差、唇形不同步与抖动导致“恐怖谷”。需要能区分不同面部区域运动频谱，并稳定时间一致性的生成方法。

Method: 1) 频率感知解耦：按运动频率将脸部分区；低频区（脸颊/鼻/额）用标准MLP建模，共享表示；高频区（眼/口）用独立网络并由区域掩码引导。2) 动态表示：预测高低频区域的Gaussian位姿/形状“增量”(Gaussian deltas)，施加到静态Gaussians上，结合逐帧相机参数，经栅格化渲染得到图像。3) 后渲染高频对齐：利用大规模音视频预训练模型，学习基于音频的逐帧细化与对齐，提升唇动精度、减少抖动。

Result: 在主流说话人头数据集上，生成视频在清晰度、时序稳定性与唇形同步方面优于近期SOTA，实验广泛验证其有效性。

Conclusion: 频率感知区域解耦+Gaussian增量+预训练引导的后渲染对齐，可在保持实时性的同时显著改进细粒度表情控制与唇形同步，为音频驱动的高保真、时序一致说话人头像提供新范式。

Abstract: Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>


### [60] [Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors](https://arxiv.org/abs/2602.09740)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: 论文分析CAV视觉系统的鲁棒性与安全性，提出参考架构并系统梳理攻击面与攻击向量，按CIA三性评估其影响，以指导安全防护设计。


<details>
  <summary>Details</summary>
Motivation: Level-5自动驾驶依赖可靠视觉感知；现有工作多关注性能与单点安全，缺乏从体系架构出发的全面攻击面刻画与CIA影响评估。

Method: 1) 解析CAV关键视觉传感与组件，抽象出CAVVS参考架构；2) 基于架构标注潜在攻击面；3) 针对每个攻击面枚举与细化攻击向量；4) 以CIA机密性、完整性、可用性为度量，评估各向量影响与风险。

Result: 形成一套CAV视觉系统参考架构与对应攻击面清单；为每类攻击给出对CIA的影响分析，揭示视觉链路在完整性与可用性方面的脆弱点更为突出。

Conclusion: 体系化理解CAV视觉系统的攻击动态有助于制定更稳健的防护策略；应围绕CIA三性强化感知安全，支撑Level-5自动驾驶的安全与可靠。

Abstract: This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>


### [61] [Self-Supervised Learning as Discrete Communication](https://arxiv.org/abs/2602.09764)
*Kawtar Zaher,Ilyass Moummad,Olivier Buisson,Alexis Joly*

Main category: cs.CV

TL;DR: 提出一种把视觉自监督学习视为“离散通信”的方法：教师网络通过固定容量的二进制信道发送多标签比特信息，学生去预测这些比特；用BCE对齐离散信息，并以编码率正则化促使充分使用信道与结构化表征；定期重置投影头可进一步增强可迁移的离散编码。实验在分类、检索、密集预测和域适应上优于连续对齐基线，学到的二进制码形成紧凑、可复用的“离散语言”。


<details>
  <summary>Details</summary>
Motivation: 现有SSL多以对齐连续特征为主，难以控制各维度信息的组织与利用效率；希望通过容量受限的离散表征，获得更可控、更结构化且可泛化的语义编码。

Method: 把SSL建模为教师-学生间的二进制通信：教师产出多标签二进制消息，学生预测这些比特。用逐元素二元交叉熵实现离散一致性；加入编码率（coding-rate）正则化，鼓励充分、均衡地使用比特信道；并定期重置投影头，使主干嵌入在多次不同离散编码下仍具可预测性，从而加强语义稳定性与结构化。

Result: 在图像分类、图像检索、密集预测等任务上，相较于连续特征对齐的基线实现一致提升；在域移位场景下通过自监督适配同样取得更好表现。分析发现学到的二进制码紧凑且信息量高，能捕捉跨类别可复用的语义因素。

Conclusion: 将SSL转化为固定容量的离散通信并配合编码率正则与投影头周期重置，可学到结构化、可复用、具泛化性的表示和二进制“语言”，在多任务与域适应中优于连续对齐方法。

Abstract: Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.

</details>


### [62] [Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets](https://arxiv.org/abs/2602.09775)
*Abhipsa Basu,Yugam Bahl,Kirti Bhagat,Preethi Seshadri,R. Venkatesh Babu,Danish Pruthi*

Main category: cs.CV

TL;DR: 论文通过从图像-文本数据集中用LLM抽取地理位置信息，刻画样本的国家分布，揭示主流多模态训练数据严重偏向英语国家，并与GDP强相关；非英语子集也偏向主要语种国家；高代表性不等于更高的视觉/语义多样性；基于此数据训练的扩散模型在国家层面的生成覆盖贫乏。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在生成具有地理代表性的图像方面表现不佳，暗示训练数据地域偏置；社区缺乏对这些大规模多模态数据的地理来源与分布的系统量化。作者希望回答：训练样本来自世界哪些地区？偏置有多强？是否随语言/经济水平变化？对生成模型有何影响？

Method: 提出一种地理画像流程：对三大英文数据集（Re-LAION、DataComp1B、Conceptual Captions）中与20类常见实体相关的图文对，用LLM从英文（及多语言子集）标题抽取地点线索并映射到国家；统计各国样本占比、与GDP的斯皮尔曼相关（ρ=0.82），并评估非英语子集的语言-地域偏向；分析代表性与视觉/语义多样性的关系；进一步用在Re-LAION上训练的Stable Diffusion v1.3按国家生成图像，对比真实世界图像覆盖度。

Result: 美国、英国、加拿大合计占48.0%；南美与非洲分别仅占1.8%与3.8%；国家代表性与GDP高度正相关（ρ=0.82）。在Re-LAION的4种非英语子集中，样本显著集中于该语言主要国家。更高的国家代表性并未带来更高的视觉或语义多样性。Stable Diffusion v1.3 的国家特定生成尽管看似真实，但相较真实世界图像，题材覆盖严重不足。

Conclusion: 主流多模态训练数据存在显著的全球南方与低GDP国家的代表性不足，且语言生态加剧地域偏差；数据量并不等价于多样性。这些偏置传导至生成模型，导致地理覆盖贫乏。应改进数据收集与标注策略，增强全球与跨语言多样性，并在评测与生成中纳入地理公平性考量。

Abstract: Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($ρ= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>


### [63] [SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing](https://arxiv.org/abs/2602.09809)
*Tong Zhang,Honglin Lin,Zhou Liu,Chong Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出SciFlow-Bench：一个从像素级输出评估科学图示生成结构正确性的基准，通过闭环逆解析将生成图还原为结构图并比较。结果显示复杂拓扑下结构保持仍很困难。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型对科学图示常视觉合理但结构错误；现有评测或偏图像相似/主观、或只评估中间符号表示，缺少对最终像素级图示结构性的评价。

Method: 从真实科学PDF构建数据集：每个框架图与规范化的真值图配对。将模型视为黑盒图像生成器，采用闭环“往返”协议：对生成的图示进行逆解析为结构化图，再与真值图比较。该流程由一个层级化多智能体系统实现，协同规划、感知与结构推理，以保证结构可恢复性为评估核心。

Result: 实验发现当前模型在保持结构正确性方面表现不佳，尤其在复杂拓扑的图示上，结构恢复率/一致性显著下降。

Conclusion: 需要结构感知的评测与方法；SciFlow-Bench提供了直接面向像素输出、以结构可恢复性为标准的基准，揭示了结构正确性是科学图示生成的核心挑战。

Abstract: Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>


### [64] [CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video](https://arxiv.org/abs/2602.09816)
*Hojun Song,Heejung Choi,Aro Kim,Chae-yeong Song,Gahyeon Kim,Soo Ye Kim,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: CompSplat是一种针对真实视频压缩失真的新视角合成训练框架，通过显式建模逐帧压缩特性，缓解长序列与无位姿场景中的姿态漂移、特征错配与几何畸变，显著提升在重压缩条件下的重建质量与位姿精度。


<details>
  <summary>Details</summary>
Motivation: 真实世界视频往往很长、相机轨迹不规则且位姿未知；有损压缩进一步带来跨帧不一致与几何误差累积，现有方法要么处理长序列/无位姿，要么只针对特定压缩伪影，难以覆盖长视频中多样的压缩模式。

Method: 提出CompSplat：在训练中显式估计并利用逐帧压缩特征进行“压缩感知”建模；通过压缩感知的帧加权策略降低受压缩影响严重帧的负面作用；并引入自适应剪枝以在重压缩下提升鲁棒性与几何一致性。

Result: 在Tanks and Temples、Free、Hike等挑战性基准上，CompSplat在严重压缩情形下取得领先的渲染质量和位姿精度，超过多数近期SOTA NVS方法。

Conclusion: 面向受压缩影响的长视频NVS，显式建模帧级压缩特性并配合加权与剪枝策略，可有效缓解跨帧不一致和几何误差累积，带来稳定且高质量的重建。

Abstract: High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.

</details>


### [65] [SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding](https://arxiv.org/abs/2602.09825)
*Zhaoxu Li,Chenqi Kong,Peijun Bao,Song Xia,Yi Tu,Yi Yu,Xinghao Jiang,Xudong Jiang*

Main category: cs.CV

TL;DR: 论文研究LVLM的幻觉来源与稳定性关联，并提出无需训练的稳定性感知解码方法SAKED，通过层间知识稳定度评分选择更可靠内部知识，显著降低各类模型与任务上的幻觉。


<details>
  <summary>Details</summary>
Motivation: 现实应用中LVLM幻觉带来安全与可靠性风险。作者观察到人类在不确定时更易出错，推测模型内部知识不稳定可能诱发幻觉，因而系统性分析不同粒度（注意头、层、解码token）的不稳定性如何导致幻觉。

Method: 三方面实证分析：1）注意头层面：度量视觉激活在不同注意头间的漂移；2）层级层面：度量层间知识波动；3）token层面：相邻输出token的视觉聚焦分散。基于发现，提出SAKED：定义逐层的知识稳定度评分KSS，比较最“稳定感知”的层与“稳定冷漠”的层，抑制解码噪声，动态利用最可靠内部知识进行token生成；方法为训练免费、可即插即用、跨架构兼容。

Result: 在多种LVLM、任务与基准上，SAKED在缓解幻觉方面达到SOTA；在不改动训练的前提下显著降低幻觉，提升输出忠实度与稳健性（具体数值未在摘要给出）。

Conclusion: LVLM的幻觉与内部知识不稳定性密切相关。通过引入层级稳定度并在解码时偏向稳定知识，可有效抑制幻觉。SAKED作为训练无关、通用的推理策略，为提升LVLM可靠性提供了高效实用的路径。

Abstract: Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>


### [66] [ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge](https://arxiv.org/abs/2602.09839)
*Yijie Lin,Guofeng Ding,Haochen Zhou,Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: ARK是一个面向专业知识与复杂推理的多模态检索基准，覆盖多领域与多类型视觉数据，通过难负样本与多步推理评估，揭示现有方法在细粒度视觉与空间推理上的显著瓶颈，并表明重排序与重写可带来稳健提升但仍有巨大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索基准偏重日常语义匹配，难以诊断涉及专业知识与复杂推理的能力；缺少能系统刻画知识领域与推理技能的评测，导致模型在更具挑战的实际场景中表现不明。

Method: 构建ARK基准，从两维分析检索：(1) 知识领域：5大领域、17子类，刻画检索依赖的内容与专业度；(2) 推理技能：6类推理类型，刻画识别正确候选所需的多模态推理。ARK支持单模与多模态的查询与候选，涵盖16种异构可视数据。为避免捷径匹配，大多数查询配有针对性的困难负样本，需要多步推理才能排除。对23个代表性文本与多模态检索器进行评测。

Result: 发现知识密集型与推理密集型检索存在明显性能差距；细粒度视觉与空间推理是持续性瓶颈。简单增强（如重排序、查询重写）在ARK上带来一致改进。

Conclusion: ARK系统性揭示当前检索模型在专业知识与复杂推理方面的短板，并提供可量化的诊断维度。尽管轻量策略能改进表现，但仍有显著提升空间，未来需面向细粒度视觉与空间推理能力与知识整合的改进。

Abstract: Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>


### [67] [Kelix Technique Report](https://arxiv.org/abs/2602.09843)
*Boyang Ding,Chenglong Chu,Dunju Zang,Han Li,Jiangxia Cao,Kun Gai,Muhao Wei,Ruiming Tang,Shiyao Wang,Siyang Mao,Xinchen Luo,Yahui Liu,Zhixin Ling,Zhuoran Yang,Ziming Li,Chengru Song,Guorui Zhou,Guowang Zhang,Hao Peng,Hao Wang,Jiaxin Deng,Jin Ouyang,Jinghao Zhang,Lejian Ren,Qianqian Wang,Qigen Hu,Tao Wang,Xingmei Wang,Yiping Yang,Zixing Zhang,Ziqi Wang*

Main category: cs.CV

TL;DR: Kelix提出一种全离散、自回归的多模态模型，通过改进视觉离散化与自回归训练，弥合离散视觉token在理解任务上相较连续特征VLM的性能差距，同时保留统一的理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多采用文本离散token+视觉连续特征的混合接口，监督偏向文本，限制了在大规模非文本数据上的自监督能力。尽管离散视觉token化有望实现统一的多模态自回归建模，但受限于码书容量等问题，现有离散视觉表示信息损失较大，理解性能劣于连续特征。因此需要一种能在不牺牲理解能力的前提下实现全离散统一建模的方法。

Method: 提出Kelix：采用改进的高容量离散视觉token化方案（更大的码书/分层或多粒度编码、信息保持与重建损失、多目标对齐），将视觉与文本统一为离散token序列；以自回归LLM为骨干，端到端使用下一个token预测进行训练，结合多模态指令/对比/重建混合目标，支持统一的理解与生成。

Result: Kelix在多种视觉理解基准上达到与甚至逼近使用连续ViT特征的VLM性能，同时在图像到文本、文本到图像等生成任务上表现优异，显示出全离散自回归框架的可行性与优势。

Conclusion: 通过增强离散视觉表示的容量与保真度，并在统一自回归框架下训练，Kelix基本消除了离散视觉token在理解上的劣势，实现了多模态理解与生成的一体化建模路径。

Abstract: Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>


### [68] [Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection](https://arxiv.org/abs/2602.09850)
*Peng Chen,Chao Huang,Yunkang Cao,Chengliang Liu,Wenqiang Wang,Mingbo Yang,Li Shen,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出Reason-IAD：结合领域知识检索与熵驱动潜在推理的可解释工业异常检测框架，动态注入关键信息图像块，在多个基准上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 通用MLLM在工业场景难以识别类别特定、细粒度缺陷，导致检测精度与可解释性不足；需要一种能够引入领域知识并在视觉细节上进行有针对性推理的机制。

Method: 1) 检索增强知识模块：为输入检索并注入类别特定文本描述，提供域内上下文；2) 熵驱动潜在推理：在紧凑潜在空间中通过可优化的think tokens迭代探索，以熵奖励促使预测更自信稳定；3) 动态视觉注入：从图像中选取最具信息量的patch注入潜在序列，聚焦异常关键区域；整体形成知识指导的动态潜在推理流程。

Result: 在多项实验中，Reason-IAD在检测性能上持续超过现有SOTA方法，并提供更具可解释性的推理过程。

Conclusion: 结合领域知识检索、熵奖励下的潜在推理与动态视觉注入，可显著提升工业异常检测的准确性与可解释性；该框架为将MLLM适配到工业缺陷检测提供有效路径，代码将开源。

Abstract: Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>


### [69] [Code2World: A GUI World Model via Renderable Code Generation](https://arxiv.org/abs/2602.09856)
*Yuhao Zheng,Li'an Zhong,Yi Wang,Rui Dai,Kaikui Liu,Xiangxiang Chu,Linyuan Lv,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 提出Code2World：把GUI下一帧视觉状态转化为“可渲染代码”的生成任务，通过渲染校验实现高保真、可控的UI预测，并显著提升下游GUI导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI World模型要实现“动作条件预测”，但文本或像素方式难兼顾高视觉保真与细粒度结构可控性；同时缺少高质量训练数据。

Method: 1) 数据：将Android GUI轨迹翻译为高保真的HTML，配合“视觉反馈修订”机制迭代优化合成代码，构建>80K屏幕-动作对(AndroidCode)。2) 模型：将VLM适配为代码预测器，先用SFT作为冷启动学习格式与布局，再用“可渲染感知强化学习”（RARL），以渲染结果为奖励，约束视觉语义保真与动作一致性。

Result: Code2World-8B在下一步UI预测上达到SOTA，表现接近GPT-5与Gemini-3-Pro-Image；在下游导航上大幅提升，例：使Gemini-2.5-Flash在AndroidWorld导航提升+9.5%。

Conclusion: 以“可渲染代码”作为中间表征，可在保证视觉保真同时获得结构可控性；数据与RARL联合带来强泛化与实用收益，显著增强多种代理的GUI推理与导航能力。

Abstract: Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>


### [70] [Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence](https://arxiv.org/abs/2602.09868)
*Xiaoyue Ling,Chuqin Zhou,Chunyi Li,Yunuo Chen,Yuan Tian,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 提出Free-GVC：一种无需训练的生成式视频压缩，将视频编码为扩散轨迹上的潜域压缩，并通过自适应质量控制与跨GOP对齐，显著提升超低码率下的时序一致性与感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频压缩虽能在极低码率下保外观，但对时域相关性的利用不足，导致闪烁与时序不一致；且跨GOP感知质量波动，缺乏在极低码率下稳健的率-感知控制。

Method: 1) 将视频分段为GOP，在紧凑潜空间中按扩散模型的轨迹逐步压缩与重建（以视频扩散先验引导）。2) 自适应质量控制（AQC）：在线构建率-感知替代模型，为每个GOP预测最优扩散步数，实现感知一致与码率受控。3) 跨GOP对齐（Inter-GOP Alignment）：通过帧重叠与相邻GOP潜特征融合，缓解闪烁并增强时序一致性。

Result: 在超低码率下，相比最新神经编解码器DCVC-RT，于DISTS指标的BD-Rate平均降低93.29%；主观用户研究也验证了更优的感知质量与时序一致性。

Conclusion: 将视频压缩重构为扩散先验指导的潜域轨迹压缩，并结合自适应质量控制与跨GOP潜融合，可在无需训练的前提下于超低码率显著提升感知质量与时序稳定性，优于现有神经编解码器。

Abstract: Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>


### [71] [BabyMamba-HAR: Lightweight Selective State Space Models for Efficient Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.09872)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: 提出BabyMamba-HAR，两种轻量Mamba风格SSM用于TinyML下的可穿戴/移动端人体行为识别，在8个基准上以约27K参数、2.21M MACs达到86.52%宏F1，性能匹配TinyHAR但在高通道数据上算力需求降至约1/11。


<details>
  <summary>Details</summary>
Motivation: 可穿戴/移动端HAR受存储与算力限制且传感器异构，现有注意力机制复杂度高（平方级），而选择性状态空间模型（SSM）线性时序处理具潜力，但在TinyML部署上的设计空间尚未被系统探索。

Method: 提出BabyMamba-HAR框架，含两种轻量架构：1) CI-BabyMamba-HAR：通道独立stem，对每个传感器通道用共享权重但实例独立变换，抑制跨通道噪声传播；2) Crossover-BiDir-BabyMamba-HAR：早期融合stem，使计算复杂度与通道数无关。两者均采用权重共享的双向扫描与轻量级时序注意力池化；进行系统消融以量化各组件贡献。

Result: 在8个多样基准上，Crossover-BiDir-BabyMamba-HAR以约27K参数和2.21M MACs获得86.52%平均宏F1，匹配TinyHAR的86.16%，并在高通道数据集上将MACs降低约11倍。消融显示：双向扫描带来最高+8.42%的F1提升；门控时序注意力相对均值池化最高+8.94%提升。

Conclusion: 选择性SSM可作为TinyML-HAR高效主干。通过通道独立或通道数无关的早融合stem、权重绑定的双向扫描与门控时序注意力，可在严格资源预算下保持与SOTA相当的准确度并显著降低计算开销，给出可操作的TinyML SSM设计准则。

Abstract: Human activity recognition (HAR) on wearable and mobile devices is constrained by memory footprint and computational budget, yet competitive accuracy must be maintained across heterogeneous sensor configurations. Selective state space models (SSMs) offer linear time sequence processing with input dependent gating, presenting a compelling alternative to quadratic complexity attention mechanisms. However, the design space for deploying SSMs in the TinyML regime remains largely unexplored. In this paper, BabyMamba-HAR is introduced, a framework comprising two novel lightweight Mamba inspired architectures optimized for resource constrained HAR: (1) CI-BabyMamba-HAR, using a channel independent stem that processes each sensor channel through shared weight, but instance independent transformations to prevent cross channel noise propagation, and (2) Crossover-BiDir-BabyMamba-HAR, using an early fusion stem that achieves channel count independent computational complexity. Both variants incorporate weight tied bidirectional scanning and lightweight temporal attention pooling. Through evaluation across eight diverse benchmarks, it is demonstrated that Crossover-BiDir-BabyMamba-HAR achieves 86.52% average macro F1-score with approximately 27K parameters and 2.21M MACs, matching TinyHAR (86.16%) while requiring 11x fewer MACs on high channel datasets. Systematic ablation studies reveal that bidirectional scanning contributes up to 8.42% F1-score improvement, and gated temporal attention provides up to 8.94% F1-score gain over mean pooling. These findings establish practical design principles for deploying selective state space models as efficient TinyML backbones for HAR.

</details>


### [72] [MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation](https://arxiv.org/abs/2602.09878)
*Jiaxu Wang,Yicheng Jiang,Tianlun He,Jingkai Sun,Qiang Zhang,Junhao He,Jiahang Cao,Zesen Gan,Mingyuan Sun,Qiming Shao,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出一种具备几何一致性的具身4D世界模型：单视角RGBD输入，生成任意视角、多步RGBD并融合为随时间演化的3D结构，用于更好地“想象-再行动”；同时引入测试时动作优化与残差逆动力学以提升操控精度。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型多仅做纯图像预测或只推理部分3D几何，难以完整预测随时间变化的4D场景；且由生成的未来转为具体动作常依赖逆动力学，易不适定（多解）。需要一种既能进行跨视角、跨模态一致的4D生成，又能稳健从预测未来推断可执行动作的方法。

Method: 1) 设计具身4D世界模型：从单视角RGBD出发，进行任意视角RGBD生成；通过回投影与融合重建更完整的时序3D。2) 构建跨视角与跨模态（RGB-深度）特征融合模块，联合约束几何对齐与模态一致性。3) 提出测试时动作优化：通过对生成模型反向传播，优化轨迹级潜变量以匹配预测的未来。4) 残差逆动力学：以优化得到的轨迹先验为输入，输出准确可执行动作。

Result: 在三个数据集上，4D场景生成质量与下游操控任务表现优于现有方法；消融实验证明跨视角/跨模态融合与测试时动作优化、残差逆动力学是性能提升的关键。

Conclusion: 跨视角、跨模态一致的4D世界模型结合测试时动作优化与残差逆动力学，能从单视角观测高效想象完整4D动态并转化为有效动作，显著提升机器人操控能力。

Abstract: World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>


### [73] [AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization](https://arxiv.org/abs/2602.09883)
*Shaoqiu Zhang,Zizhong Ding,Kaicheng Yang,Junyi Wu,Xianglong Yan,Xi Li,Bingnan Duan,Jianping Fang,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出AdaTSQ：面向扩散Transformer（DiT）的时序感知PTQ框架，通过动态比特分配与Fisher引导的时序校准，大幅提升在同等或更低位宽下的生成质量与效率，优于SVDQuant与ViDiT-Q。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法在LLM上有效，但直接套用到DiT效果不佳，原因是忽视扩散过程的时序动态与不同timestep对误差的敏感性，导致量化策略不匹配、重建误差大、端侧部署困难。

Method: 1) Pareto感知的timestep动态位宽分配：将量化策略搜索建模为受约束的路径搜索问题，用束搜索在不同timestep上按层自适应分配位宽，搜索目标由端到端重建误差与资源约束共同引导。2) Fisher引导的时序校准：用时序Fisher信息评估各timestep敏感度，优先选取高敏时刻的校准数据，并与基于Hessian的权重要素优化无缝结合，降低量化误差。

Result: 在四个先进DiT模型（Flux-Dev、Flux-Schnell、Z-Image、Wan2.1）上，AdaTSQ显著优于SOTA（SVDQuant、ViDiT-Q），在同等/更低位宽下取得更好的重建与生成质量指标，并提升推理效率与内存占用；推动效率-质量帕累托前沿。

Conclusion: 考虑扩散时序敏感性的PTQ至关重要。AdaTSQ通过动态位宽与Fisher引导校准，有效缓解DiT量化的时序误差放大问题，实现更低开销下的高保真生成，适合边缘端部署；方法通用且与Hessian优化兼容。

Abstract: Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>


### [74] [SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models](https://arxiv.org/abs/2602.09918)
*Gulraiz Khan,Kenneth Y. Wertheim,Kevin Pimbblet,Waqas Ahmed*

Main category: cs.CV

TL;DR: 提出SARS：从单张图像重建全身3D模型的形状与外观感知3DMM系统。


<details>
  <summary>Details</summary>
Motivation: 以往3D人脸/人体重建多聚焦于整体几何结构，忽略语义属性（年龄、性别、面部关键点、皱纹等）与外观因素，导致重建细节与身份一致性不足。需要一个能同时考虑形状与外观高层语义的重建方案。

Method: 基于3DMM框架，结合身份与表情blendshape及基础面部网格；引入可调高层参数（形状、纹理、光照、相机）并显式建模语义特征；构建模块化流水线，从单张图像中解析身体与面部信息，联合优化形状与外观以重建全身3D。

Result: 从单张2D图像生成兼顾全身与面部细节的3D模型，较仅用全局几何的方法更好地适配年龄、性别与面部边界/曲线/褶皱等语义外观变化。

Conclusion: SARS通过形状与外观感知的模块化3D重建，弥补了传统方法对语义与外观忽视的问题，实现了单图像的人体全身高保真3D重建。

Abstract: Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.

</details>


### [75] [A benchmark for video-based laparoscopic skill analysis and assessment](https://arxiv.org/abs/2602.09927)
*Isabel Funke,Sebastian Bodenstedt,Felix von Bechtolsheim,Florian Oehme,Michael Maruschke,Stefanie Herrlich,Jürgen Weitz,Marius Distler,Sören Torge Mees,Stefanie Speidel*

Main category: cs.CV

TL;DR: 提出LASANA腹腔镜技能数据集：1270段立体视频、4项基础任务、三名评审汇总的结构化技能评分与任务特异错误二元标注，含预定义数据划分与基线模型结果，用于推动视频化技能评估与错误识别研究。


<details>
  <summary>Details</summary>
Motivation: 深度学习在外科技能视频评估上有潜力，但受限于小规模、缺少高质量标注的数据集，难以客观开发与公平比较模型。

Method: 构建并发布LASANA数据集：收集来自培训课程的真实分布学员操作的1270段立体视频；为每段视频提供三名独立评审聚合的结构化技能评分与任务特定错误的有无标注；为每项任务提供固定数据划分；并给出一个深度学习基线模型及其结果以便基准测试。

Result: 得到覆盖4项基础腹腔镜任务的大规模、带结构化评分与错误标注的数据集及预定义划分；提供基线模型性能作为比较参照。

Conclusion: LASANA为视频化外科技能评估与错误识别提供了标准化基准与充足数据，有望缓解数据稀缺、促进方法开发与可重复比较。

Abstract: Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.

</details>


### [76] [Monocular Normal Estimation via Shading Sequence Estimation](https://arxiv.org/abs/2602.09929)
*Zongrui Li,Xinhua Ma,Minghui Hu,Yunqing Zhao,Yingchen Yu,Qian Zheng,Chang Liu,Xudong Jiang,Song Bai*

Main category: cs.CV

TL;DR: 提出RoSE：将单目法线估计从“直接回归法线图”改为“预测明暗序列（shading sequence）再求解法线”，显著缓解3D错位并在真实基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 直接用深度网络回归法线图易出现3D错位：法线外观看似正确，但重建表面与几何细节不对齐，因为不同几何只在法线颜色上呈现细微变化，模型难以区分与重建。需要一种对几何更敏感的中间表征。

Method: 将任务重构为明暗序列估计：利用图像到视频生成模型预测在不同光照下的物体明暗序列；随后通过一个简单的普通最小二乘（OLS）从这些明暗帧求解每像素法线。为鲁棒性与复杂物体适配，构建并用合成数据集MultiShade训练（多形状、材质、光照）。

Result: 在真实世界的物体级单目法线估计基准上取得SOTA性能；生成的明暗序列对几何变化更敏感，降低3D错位问题。

Conclusion: 以“明暗序列→法线”的新范式替代“直接回归法线”，借助生成视频模型与OLS解算，并以多样化合成数据训练，能显著提升单目法线估计的几何一致性与精度。

Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>


### [77] [GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery](https://arxiv.org/abs/2602.09932)
*Han Jinzhen,JinByeong Lee,JiSung Kim,MinKyung Cho,DaHee Kim,HongSik Yun*

Main category: cs.CV

TL;DR: GeoFormer利用开源Swin Transformer与多源遥感（Sentinel-1/2与开放DEM），在100 m格网内同时预测城市建筑高度与建筑占地，跨54城取得显著优于CNN的精度，并提供代码、权重与全球产品。


<details>
  <summary>Details</summary>
Motivation: 高精度三维城市数据对气候、灾害与规划至关重要，但现有方法依赖专有传感器或跨城泛化差，亟需一种仅依赖开源数据、具备强泛化能力的方案。

Method: 提出GeoFormer：基于Swin Transformer的多模态融合框架，输入为Sentinel-1（SAR）、Sentinel-2（光学）与开放DEM；在100 m格上联合回归建筑高度（BH）与回归/估计建筑占地（BF）；采用地理阻断式数据划分以确保训练/测试空间独立；进行消融以评估各模态贡献。

Result: 在54座多样化城市上，BH RMSE=3.19 m、BF RMSE=0.05，分别较最强CNN提升7.5%与15.3%；跨洲迁移时BH RMSE仍<3.5 m。消融显示DEM对高度估计不可或缺，光学信息贡献大于SAR，多源融合最佳。

Conclusion: GeoFormer用完全开源数据与模型在全球范围实现稳健的建筑高度与占地估计，具备优良跨域泛化并公开代码、权重与全球产品，适用于气候、风险与城市应用场景。

Abstract: Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.

</details>


### [78] [Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors](https://arxiv.org/abs/2602.09933)
*Melika Qahqaie,Dominik Neumann,Tobias Heimann,Andreas Maier,Veronika A. Zimmer*

Main category: cs.CV

TL;DR: 提出一种基于不平衡最优传输（UOT）的“配准感知”病灶匹配方法，能在纵向CT中稳健处理病灶出现/消失/合并/分裂，并在多项图匹配指标上优于仅用距离的基线。


<details>
  <summary>Details</summary>
Motivation: 纵向CT评估肿瘤疗效需跨时间点正确对应病灶，但传统基于几何接近的二部图匹配在病灶数量与形态变化（出现、消失、合并、分裂）时易失效；需要一种能处理质量不守恒、利用配准可靠性并适应患者级肿瘤负荷变化的方法。

Method: 构建基于不平衡最优传输的匹配框架：1) 代价函数融合三项信息——尺寸归一化的几何距离、由形变场Jacobian导出的局部配准可信度、可选的块级外观一致性；2) 通过UOT自然处理病灶“质量”不相等并在患者级自适应先验以反映肿瘤负荷变化；3) 对得到的传输计划施行相对剪枝以稀疏化，产生一对一匹配并显式标注新生、消失、合并、分裂，无需重新训练或启发式规则。

Result: 在纵向CT数据上，相比仅基于距离的基线，方法在边检测的Precision/Recall、病灶状态召回率，以及病灶图联通分量F1等指标上均取得一致性更优表现。

Conclusion: 将配准可信度与外观信息融入UOT的匹配策略，能够稳健处理纵向病灶数量与形态变化，减少对启发式与再训练的依赖，并提升多维度匹配与图级评估指标，适用于治疗反应评估中的病灶对应任务。

Abstract: Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, merging, and splitting lesions without retraining or heuristic rules. On longitudinal CT data, our approach achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores versus distance-only baselines.

</details>


### [79] [VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization](https://arxiv.org/abs/2602.09934)
*Yikun Liu,Yuan Liu,Shangzhe Di,Haicheng Wang,Zhongyin Zhao,Le Tian,Xiao Zhou,Jie Zhou,Jiangchao Yao,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 论文提出VersaViT，通过多任务协作后训练提升MLLM视觉编码器在密集预测任务上的能力，实现同时胜任语言推理与像素级理解。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在视觉-语言高层语义对齐上表现突出，但其视觉编码器在密集特征表征方面不足，导致语义分割、深度估计等密集预测任务表现不佳；作者希望让这些编码器成为通用视觉骨干。

Method: 提出VersaViT：在现有MLLM视觉编码器上，引入轻量级任务头，构建多粒度监督的多任务协作后训练框架；通过联合若干经典视觉任务对骨干进行优化，提升密集表征能力，同时保持语言相关能力。

Result: 在多种下游任务上进行广泛实验，显示该方法显著提升密集预测性能，并维持/提升语言介导推理能力，得到更通用的视觉骨干。

Conclusion: MLLM视觉编码器原生不擅长密集预测；通过VersaViT的多任务协作后训练，可将其打造成既适用于语言推理又擅长像素级理解的通用视觉骨干。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.

</details>


### [80] [Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework](https://arxiv.org/abs/2602.09949)
*Franziska Krauß,Matthias Ege,Zoltan Lovasz,Albrecht Bartz-Schmidt,Igor Tsaur,Oliver Sawodny,Carina Veil*

Main category: cs.CV

TL;DR: 提出一种结合Transformer与CNN的混合注意-卷积(HAC)网络，用于膀胱镜血管分割，解决数据稀缺与伪阳性问题，在BlaVeS数据集上取得更高精度与连通性指标。


<details>
  <summary>Details</summary>
Motivation: 膀胱肿瘤术后监测需在多次干预中定位相同病灶部位，但膀胱为空腔且可变形，缺乏稳定标志；血管纹理可作为个体化“指纹”，然而内镜图像存在噪声、光照变化、气泡与皱襞等伪像、形变与稀疏标注，导致现有血管分割方法难以可靠应用于导航。

Method: 提出HAC架构：Transformer分支先学习全局血管拓扑先验，CNN分支学习残差细化以恢复细小血管；为强调结构连通性，Transformer以优化后的标注（去除短/终末分支）进行训练；为缓解数据匮乏，引入物理感知的自监督预训练，基于临床合理的增强对未标注数据进行预训练；在BlaVeS内镜帧数据集上进行评估。

Result: 在BlaVeS上达到高准确率0.94、优于SOTA的精确率0.61与clDice 0.66，较好抑制由动态出现/消失的黏膜皱襞导致的假阳性，保持结构连通性。

Conclusion: HAC通过融合全局拓扑与局部细节、并结合物理感知预训练与连通性导向标注，有效提升膀胱镜血管分割的可靠性，为临床内镜导航提供稳定的结构参考。

Abstract: Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>


### [81] [Learning to Detect Baked Goods with Limited Supervision](https://arxiv.org/abs/2602.09979)
*Thomas H. Schmitt,Maximilian Bundscherer,Tobias Bocklet*

Main category: cs.CV

TL;DR: 在德式烘焙门店的剩余品监测场景中，作者用少标注/弱监督训练流程打造面包点心检测系统：融合开放词汇定位与图像级标签、再用视频伪标注增强视角鲁棒性，最终在非理想部署条件下超越全监督基线。


<details>
  <summary>Details</summary>
Motivation: 德国面包房现烤产品保质期短，需监测剩余品以优化生产；人工盘点昂贵且易错。行业场景往往任务专门化、标注数据稀缺，开放词汇检测虽灵活但不足以直接解决细粒度、多样化烘焙品识别与定位问题。

Method: 构建含19类烘焙品、不同监督强度的数据划分；提出两条训练工作流：1) 以OWLv2与Grounding DINO提供定位，再配合图像级标签进行弱监督训练；2) 使用Segment Anything 2进行视频帧伪标注传播，对YOLOv11微调以提升视角与部署鲁棒性。最终选用YOLOv11因速度-精度权衡佳。

Result: 仅用图像级监督训练的模型mAP=0.91；在非理想部署条件下，通过伪标签微调使性能提升19.3%；两流程结合后，在非理想条件下的表现超过了全监督基线。

Conclusion: 通过将开放词汇定位、图像级监督与视频伪标签传播结合，可在标注极少的行业场景中训练出鲁棒的目标检测器，在实际部署条件下甚至优于全监督基线，展示了可扩展、低成本的工业计算机视觉落地路径。

Abstract: Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>


### [82] [Coupled Inference in Diffusion Models for Semantic Decomposition](https://arxiv.org/abs/2602.09983)
*Calvin Yeung,Ali Zakeri,Zhuowen Zou,Mohsen Imani*

Main category: cs.CV

TL;DR: 提出一种在扩散模型中通过耦合推理实现“绑定表示”的语义分解框架，用重建驱动的指导项将多个因子扩散过程耦合，并配合新的迭代采样策略；该框架涵盖注意力型共振网络作为特例，在多种合成分解任务上优于共振网络。


<details>
  <summary>Details</summary>
Motivation: 视觉场景往往由潜在因子组成，许多任务既要能用“绑定”构造可组合表示，也要能从绑定向量中反解出各因子（分解问题）。共振网络可做分解，但其与霍普菲尔德网络/扩散模型的联系启发：能否用扩散模型的推理过程更稳健地做语义分解，并统一解释共振网络？

Method: 将语义分解表述为逆问题：给定绑定向量，针对每个潜在因子各自运行扩散过程，并通过一个重建驱动的指导项将这些扩散过程耦合，使得当前因子估计的组合能重建原绑定向量；提出一种改进的迭代采样方案以提升收敛与精度；理论上证明注意力型共振网络是该框架的特例。

Result: 在多种合成的语义分解任务上，耦合推理的扩散框架优于（注意力型）共振网络，表现为更高的分解准确率与更稳健的收敛（细节未给出）。

Conclusion: 扩散模型中的耦合推理为绑定表示的语义分解提供了统一且更强的框架，既能涵盖共振网络作为特例，又在实验上取得更好效果；新迭代采样进一步提升了性能。

Abstract: Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

</details>


### [83] [Efficient Special Stain Classification](https://arxiv.org/abs/2602.09989)
*Oskar Thaeter,Christian Grashei,Anette Haas,Elisa Schmoeckel,Han Li,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 论文比较两种全视野病理切片染色自动分类方法：MIL与缩略图法。MIL在内部数据上更准，缩略图法在外部数据上更泛化，且快两个数量级，适合规模化质控。


<details>
  <summary>Details</summary>
Motivation: 临床与计算病理需要准确的切片染色元数据以保证档案质控与数据集完整性。H&E虽为标准，但特殊染色多样，人工标注与维护易错且成本高，亟需自动化分类。

Method: 在包含16类（14种常用特殊染色+常规与冰冻H&E）的全视野图像上，对比：1）多实例学习（MIL）基线：基于切块/实例汇聚的WSI级分类；2）提出的轻量级缩略图法：直接用低分辨率缩略图进行分类。评估内部测试集与外部TCGA数据的性能与吞吐量。

Result: 内部测试：MIL宏F1=0.941（16类），合并为14类宏F1=0.969；缩略图法分别为0.897与0.953。外部TCGA：缩略图法加权F1=0.843，优于MIL的0.807。速度：缩略图法5.635张/秒，MIL 0.018张/秒（全patch），提升约两个数量级。

Conclusion: 缩略图法在保持较高准确度的同时具备更好的跨域泛化与显著更高的吞吐量，是数字病理常规视觉质控的可扩展、稳健方案；MIL在内部数据上最优，但计算代价大、泛化略逊。

Abstract: Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently
  utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for
  the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly
  used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.
  On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and
  0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of
  magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control
  in digital pathology workflows.

</details>


### [84] [Faster-GS: Analyzing and Improving Gaussian Splatting Optimization](https://arxiv.org/abs/2602.09999)
*Florian Hahlbohm,Linus Franke,Martin Eisemann,Marcus Magnor*

Main category: cs.CV

TL;DR: 论文提出Faster-GS：在不损失视觉质量的前提下，将3D Gaussian Splatting训练加速至最高5倍，并可推广到4D（非刚体）场景。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS加速研究碎片化：很多工作将工程实现细节与核心算法改动交织，或以质量换速度，导致难以公平比较与复用。需要一个系统性、可复现、成本效益高的加速基线。

Method: 系统梳理并整合既有3DGS中最有效、可广泛适配的加速策略，加入若干新优化；重点研究数值稳定性、高斯截断策略、梯度近似等被忽视环节；在统一框架下实现端到端优化与评测，形成Faster-GS。

Result: 在多项权威基准上验证：Faster-GS训练速度最高提升至5×，同时保持重建的视觉保真度；并进一步将优化策略迁移到4D高斯重建，实现高效的非刚体场景优化。

Conclusion: Faster-GS确立了一个资源高效、可对比、可复用的3DGS加速基线；其优化原则与实现对3D与4D场景均适用，为后续研究提供了稳健的参考框架。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>


### [85] [Perception with Guarantees: Certified Pose Estimation via Reachability Analysis](https://arxiv.org/abs/2602.10032)
*Tobias Ladner,Yasser Shoukry,Matthias Althoff*

Main category: cs.CV

TL;DR: 提出一种仅基于单幅相机图像与已知目标几何体，在3D中给出可证明安全的位姿有界估计的方法；通过可达性分析与神经网络形式化验证，得到经认证的姿态界并在合成与真实实验中高效准确。


<details>
  <summary>Details</summary>
Motivation: 安全关键的网络物理系统需要精确且可证明的位姿信息以进行后续决策。传统依赖多传感器融合或外部服务（如GPS）的方案要么仅给出粗略估计、难以满足最坏情形安全性证明，要么存在可信度问题。为此需要一种无需不可信外部服务、且能给出形式化安全保证的相机仅视觉位姿估计。

Method: 利用已知目标几何与单幅图像，通过将位姿估计问题转化为带不确定性的几何匹配/投影约束问题；结合可达性分析对可能位姿集合进行系统性界定，并借助形式化神经网络验证来严格上/下界解空间，从而输出经认证的三维位姿区间（有界集合），而非点估计。

Result: 在合成与真实世界数据上实验表明，该方法能高效计算并提供精确且保守（安全）的位姿边界，用时与精度表现良好，支持在安全关键任务中使用。

Conclusion: 在无需外部服务、仅依赖单目视觉与已知目标几何的前提下，可获得经形式化验证的3D位姿有界估计，为安全关键系统提供可证明的感知基础；方法有效且具备实际可行性。

Abstract: Agents in cyber-physical systems are increasingly entrusted with safety-critical tasks. Ensuring safety of these agents often requires localizing the pose for subsequent actions. Pose estimates can, e.g., be obtained from various combinations of lidar sensors, cameras, and external services such as GPS. Crucially, in safety-critical domains, a rough estimate is insufficient to formally determine safety, i.e., guaranteeing safety even in the worst-case scenario, and external services might additionally not be trustworthy. We address this problem by presenting a certified pose estimation in 3D solely from a camera image and a well-known target geometry. This is realized by formally bounding the pose, which is computed by leveraging recent results from reachability analysis and formal neural network verification. Our experiments demonstrate that our approach efficiently and accurately localizes agents in both synthetic and real-world experiments.

</details>


### [86] [Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection](https://arxiv.org/abs/2602.10042)
*Changjiang Jiang,Xinkuan Sha,Fengchang Yu,Jingjing Liu,Jian Liu,Mingqi Fang,Chenfeng Zhang,Wei Lu*

Main category: cs.CV

TL;DR: 提出Fake-HR1：一种能自适应决定是否进行推理（CoT）的混合推理大模型，用于合成图像检测，在保持或提升检测与推理能力的同时显著降低开销与时延。


<details>
  <summary>Details</summary>
Motivation: CoT可提升合成图像检测效果，但对明显伪造样本进行冗长推理会带来高token与时延成本，需要一种能按需启用推理的机制以兼顾精度与效率。

Method: 提出混合推理（Hybrid-Reasoning）范式与两阶段训练框架：（1）混合微调（HFT）实现冷启动；（2）在线强化学习HGRPO，使模型隐式学习在不同查询类型下选择是否/何种程度进行推理，从而自适应切换推理模式。

Result: 在多种查询/任务上，Fake-HR1能自适应地使用或跳过推理，较现有LLMs在推理能力与生成检测性能上更优，同时显著降低响应延迟与token消耗。

Conclusion: 自适应混合推理可在保持检测与推理强度的同时显著提升效率；Fake-HR1验证了该策略在生成检测任务中的有效性并优于现有基线。

Abstract: Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>


### [87] [Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI](https://arxiv.org/abs/2602.10043)
*Gaurang Sharma,Harri Polonen,Juha Pajula,Jutta Suksi,Jussi Tohka*

Main category: cs.CV

TL;DR: 论文表明：即使去颅骨的T1头部MRI，仅用标准预处理与简单图像相似度计算，也能几乎完美地把同一受试者在不同数据库/时间/扫描条件下的图像匹配起来，存在链接与再识别隐私风险。


<details>
  <summary>Details</summary>
Motivation: 头部MRI在研究共享前通常去标识化（含去颅骨）。然而脑实质本身或含有可唯一识别的“指纹”，可能跨数据库将同一人的数据链接起来，若再有其他外部标识则可被再识别。既有工作多依赖训练模型或高算力手段，监管亦常以“合理性”评估风险，尚缺乏用简单流程即可实现链接的系统性证据。

Method: 对去颅骨的T1加权MRI实施标准预处理（如配准、标准化、重采样等），随后采用直接的图像相似度度量进行样本间匹配；在不同时间间隔、扫描仪厂商与型号、空间分辨率与成像协议等条件下，模拟跨数据库匹配情境并评估链接准确率。

Result: 无需训练或复杂模型，仅凭预处理+相似度计算即可在广泛条件变化下实现近乎完美的跨扫描、跨时间、跨协议的个体匹配（接近100%的链接准确率），即使存在潜在的认知衰退。

Conclusion: 去颅骨并不足以消除头部MRI的可链接性；脑实质蕴含稳定的个体特征，易被简单方法捕捉。该发现提示：现行医疗数据共享的隐私保护与“合理性”风险评估需更新，需制定更前瞻的政策与技术（如受控访问、更强的去识别或合成数据/隐私保护学习）以降低再识别风险。

Abstract: Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.
  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.

</details>


### [88] [Conformal Prediction Sets for Instance Segmentation](https://arxiv.org/abs/2602.10045)
*Kerri Lu,Dan M. Kluger,Stephen Bates,Sherrie Wang*

Main category: cs.CV

TL;DR: 提出一种用于实例分割的不确定性量化与校准的新方法：在给定图像和像素查询下，输出具备覆盖保证的“置信实例集合”，确保至少一个预测与真值具有高IoU。方法在多领域验证，达到目标覆盖并优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 现有实例分割模型虽有较高平均性能，但缺乏可验证的不确定性与校准能力；无法保证输出掩码与真值足够接近。需要一种具备统计保证的机制，为每个像素/实例提供可信的置信集合。

Method: 基于保序/分位思想的保序预测（conformal prediction）框架：对给定图像与像素坐标查询，生成包含若干实例掩码的自适应“置信集合”，使其以预设概率包含至少一个与真值高IoU的预测。提供大样本与有限样本版本；与Learn-Then-Test、Conformal Risk Control、形态学膨胀等作比较。

Result: 在农业地块划分、细胞分割与车辆检测数据上，置信集合大小随查询难度而自适应变化，实证达到目标覆盖，并在覆盖/集合大小等指标上优于上述基线方法。

Conclusion: 所提算法为实例分割提供了具备理论覆盖保证的像素级置信集合，实现了校准的不确定性量化；具有可扩展性与实用性，并在多任务上验证有效，提供了渐近与有限样本的理论保证。

Abstract: Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.

</details>


### [89] [Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving](https://arxiv.org/abs/2602.10052)
*Serin Varghese,Kevin Ross,Fabian Hueger,Kira Maag*

Main category: cs.CV

TL;DR: 提出一种时空注意力（STA）机制，将Transformer注意力从单帧扩展到多帧，以提升视频语义分割的准确性与时序稳定性；在Cityscapes与BDD100k上提升时序一致性约9.2个百分点、mIoU最高+1.76。


<details>
  <summary>Details</summary>
Motivation: 现有视频语义分割多将每帧独立处理，未充分利用跨帧的时序一致性，导致动态场景中稳定性与精度受限。需要一种能在不大幅增加计算与改动架构的前提下，融合多帧信息的通用方法。

Method: 提出Spatio-Temporal Attention（STA），在Transformer自注意力中引入时空序列建模：将特征从单帧扩展为多帧序列，设计计算高效的时空注意力以捕获跨帧关联，同时尽量保持与原有注意力模块接口兼容，便于插拔到轻量或大规模Transformer分割模型中。

Result: 在Cityscapes与BDD100k上，较单帧基线显著提升：时序一致性指标提升约9.20个百分点，mIoU最高提升1.76个百分点，且对不同规模与架构的Transformer均有效。

Conclusion: STA是对视频语义分割的有效结构增强模块，可广泛适配各类Transformer模型，在基本不增加太多计算成本与改动的情况下，显著提升跨帧一致性与分割性能。

Abstract: Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>


### [90] [Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach](https://arxiv.org/abs/2602.10079)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Forensim 是一个用于图像伪造取证的注意力式状态空间框架，可同时精准定位被篡改（target）与其来源（source）区域，并输出三类掩膜（原始/来源/目标），在拼接与复制-移动任务上达到了SOTA，并发布了新的CMFD-Anything数据集。


<details>
  <summary>Details</summary>
Motivation: 仅依赖伪造伪迹的传统方法往往只能检测到被篡改区域，忽略其与来源区域的对应关系，容易导致错误语义解读（如抗议现场将暴力事件复制进人群）。因此需要一个能联合定位来源与目标区域、揭示内部重复关系的统一方法。

Method: 提出视觉状态空间模型：利用归一化注意力图挖掘图像内部相似性（捕获复制/重复模式），并配合基于区域的块级注意力模块区分被操纵区域；端到端训练，输出三类掩膜（pristine/source/target），统一处理拼接与复制-移动伪造。

Result: 在标准基准上取得SOTA的检测与定位性能；方法可精确联合定位source/target；并构建并发布新的复制-移动取证数据集CMFD-Anything，弥补现有数据集不足。

Conclusion: 联合来源-目标定位对于语义可靠的图像取证至关重要。Forensim 的注意力驱动状态空间设计在统一框架下实现精确定位与跨任务适用性，并以SOTA结果与新数据集验证其实用价值。

Abstract: We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>


### [91] [4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere](https://arxiv.org/abs/2602.10094)
*Yihang Luo,Shangchen Zhou,Yushi Lan,Xingang Pan,Chen Change Loy*

Main category: cs.CV

TL;DR: 4RC提出统一的前馈式4D重建框架：一次编码整段单目视频，在任意时间对任意帧高效查询稠密几何与运动，实验全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目4D方法往往将运动与几何割裂处理，或仅提供稀疏轨迹/两帧场景流等受限属性，难以获得统一且稠密的时空表征与高效推理。需要一个能够整体建模几何与动态、并可灵活查询的通用框架。

Method: 采用“encode-once, query-anywhere/anytime”的Transformer骨干：把整段视频编码为紧凑的时空潜表示；利用条件解码器，对任意查询帧与目标时间戳解码3D几何与运动。为便于学习，将每视角4D属性做最小因子分解：基几何（base geometry）+ 随时间变化的相对运动（time-dependent relative motion）。

Result: 在多种4D重建任务与基准上，相比现有与同期方法均取得更优表现，能输出联合的稠密几何与运动。

Conclusion: 4RC实现了统一的、前馈式的4D重建：以一次视频编码支撑任意时空位置的高效查询，最小因子化表征提升了学习与泛化能力，推动单目视频4D重建的准确性与适用性。

Abstract: We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

</details>


### [92] [Causality in Video Diffusers is Separable from Denoising](https://arxiv.org/abs/2602.10095)
*Xingjian Bai,Guande He,Zhengqi Li,Eli Shechtman,Xun Huang,Zongze Wu*

Main category: cs.CV

TL;DR: 提出可分离因果扩散（SCD）：将一次性的时序因果推理与多步逐帧去噪解耦，以提高速度并保持/提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有因果扩散模型在每个去噪步、所有层、全上下文中施加因果注意，导致时序推理与去噪迭代纠缠，计算冗余、延迟高。作者怀疑时序推理可与多步去噪分离，从而提升效率。

Method: 系统探测自回归视频扩散器：发现(1) 早期层跨去噪步特征高度相似（冗余）；(2) 深层跨帧注意稀疏，主要做帧内渲染。基于此，设计SCD架构：用因果Transformer编码器做“每帧一次”的时间因果推理；用轻量扩散解码器进行多步逐帧渲染去噪，显式解耦两阶段。

Result: 在合成与真实基准上的预训练与后训练任务中，SCD显著提升吞吐与单帧时延，并在生成质量上匹配或优于强基线因果扩散模型。

Conclusion: 因果推理与扩散去噪可有效解耦。SCD用单次时序编码+逐帧扩散解码实现更高效的视频等因果生成，在不牺牲甚至提升质量的同时大幅提高效率。

Abstract: Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>


### [93] [VideoWorld 2: Learning Transferable Knowledge from Real-world Videos](https://arxiv.org/abs/2602.10102)
*Zhongwei Ren,Yunchao Wei,Xiao Yu,Guixun Luo,Yao Zhao,Bingyi Kang,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: VideoWorld 2 通过将动作动力学与视觉外观解耦，利用视频扩散模型建模外观、用自回归潜变量建模任务动力学，从原始真实视频中学习可迁移知识；在手工制作与机器人任务上显著提升成功率和长序列一致性。


<details>
  <summary>Details</summary>
Motivation: 智能体需要从未标注的真实视频中学习可迁移的世界知识，并在新环境/任务中复用。现有视频生成或潜在动力学方法在真实、长时程、复杂任务上不够稳定与可迁移，亟需一种能专注于任务相关动力学、减少外观干扰的学习框架。

Method: 提出动态增强的潜在动力学模型（dLDM）：1）用预训练视频扩散模型专注建模视觉外观；2）学习紧凑、任务相关的潜在动力学编码（与外观解耦）；3）对潜在代码进行自回归建模以进行策略学习与长时程推理；4）在真实视频上端到端训练与评估。

Result: 在真实手工制作任务上，相比以往视频生成与潜在动力学模型，任务成功率最高提升约70%，并能生成连贯的长执行视频；在机器人领域，从Open-X视频中习得操控知识，显著提升CALVIN基准上的任务表现。

Conclusion: 从原始真实视频直接学习可迁移世界知识是可行且有效的；通过将外观与动力学解耦并在潜在空间自回归建模，可提升长时程一致性与跨任务迁移性能。代码、数据与模型将开源以促进后续研究。

Abstract: Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>


### [94] [Olaf-World: Orienting Latent Actions for Video World Modeling](https://arxiv.org/abs/2602.10104)
*Yuxin Jiang,Yuchao Gu,Ivor W. Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出SeqΔ-REPA与Olaf-World，从无标注视频中学习可迁移的潜在动作空间，通过对齐“动作造成的可观测效果”实现跨场景一致的控制接口，显著提升零样本迁移与数据高效适配能力。


<details>
  <summary>Details</summary>
Motivation: 可控世界模型需要动作标签，但动作标注稀缺且昂贵。无监督/弱监督的潜在动作学习虽能从视频提取“控制接口”，但常因只在单段视频内训练而导致潜在空间纠缠场景线索、缺乏统一坐标系，跨场景不可迁移。作者洞察：虽然动作本身不可见，但其语义效果在视频表征的时间差分中是可观测和可对齐的。

Method: 1) 提出SeqΔ-REPA：以冻结的自监督视频编码器提取时间步特征，计算序列级的时序特征差分，作为“动作效果”的锚点；将集成的潜在动作与这些差分进行对齐，形成跨片段的一致语义参照。2) 基于此构建Olaf-World：从大规模被动视频预训练动作条件的视频世界模型，利用学得的潜在动作空间进行生成与控制。3) 训练仅在无动作标签的视频上进行，对齐目标跨片段/跨场景共享。

Result: 在多项实验中，学得的潜在动作空间更结构化，能更强的零样本动作迁移，并在适配到新控制接口时显著更数据高效，优于现有SOTA基线。

Conclusion: 通过将“动作效果”作为共享参照进行序列级对齐，可在无标签视频上学习可泛化的潜在动作坐标系；该框架（SeqΔ-REPA+Olaf-World）提升了可控世界模型的迁移性与样本效率。

Abstract: Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>


### [95] [ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation](https://arxiv.org/abs/2602.10113)
*Mingyang Wu,Ashirbad Mishra,Soumik Dey,Shuo Xing,Naveen Ravipati,Hansi Wu,Binbin Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 提出ConsID-Gen：通过多视图辅助与双流编码器提升图像到视频(I2V)在身份一致性与时序连贯上的表现，并发布大规模数据集与基准；在多项指标上优于主流模型。


<details>
  <summary>Details</summary>
Motivation: I2V常出现身份外观漂移与几何失真，源于单视图观测稀疏与跨模态对齐不足；缺少针对多视图一致性的高质量数据和细粒度评测框架。

Method: (1) 数据：构建大规模面向身份一致性的对象中心视频数据集ConsIDVid，并提出多视图一致性评测基准ConsIDVid-Bench，包含对细微几何与外观偏差敏感的指标；(2) 模型：ConsID-Gen在首帧之外引入未定姿(auxiliary)多视图，采用视觉-几何双流编码器融合语义与结构线索，并通过文本-视觉连接器统一条件，驱动Diffusion Transformer骨干进行视频生成。

Result: 在ConsIDVid-Bench上，多项指标全面领先，整体性能超过Wan2.1与HunyuanVideo，显著提升身份保真与时序一致性，尤其在复杂真实场景中表现更优。

Conclusion: 结合高质量多视图数据与视图增强的双流条件建模，可有效缓解I2V的外观漂移与几何失真；ConsID-Gen与ConsIDVid/Bench为评测与研究身份一致性提供了新的标准与强基线。

Abstract: Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>


### [96] [Quantum Multiple Rotation Averaging](https://arxiv.org/abs/2602.10115)
*Shuteng Wang,Natacha Kuete Meli,Michael Möller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 提出IQARS：将多重旋转平均(MRA)转化为一系列本地二次非凸子问题，经二值化后在量子退火器上求解，较Shonan在现有硬件上已达约12%精度提升。


<details>
  <summary>Details</summary>
Motivation: 经典MRA方法（如L1-IRLS、Shonan）要么易陷入局部极小，要么依赖凸松弛而偏离SO(3)流形几何，噪声高时精度受限。作者希望去除凸松弛依赖、保留旋转流形结构，并利用量子退火的并行与量子隧穿改进全局搜索能力。

Method: 将MRA重构为一串局部的二次非凸优化子问题；对子问题进行二值化映射成可在量子退火器（如D-Wave）上求解的形式；采用迭代框架（IQARS）在量子硬件上求解并回传更新，重复直至收敛，从而不依赖凸松弛并更贴合SO(3)几何。

Result: 在合成和真实数据上评测；尽管现有量子退火器规模与性能受限，IQARS在D-Wave上已优于最佳对比Shonan，准确率提升约12%。

Conclusion: IQARS证明了在MRA中使用量子退火的可行性与潜在优势：更好地保持非欧几里得几何并获得更高精度。受当前硬件限制，规模与速度尚受约束，但结果表明未来更强量子硬件有望进一步提升表现。

Abstract: Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>


### [97] [SAGE: Scalable Agentic 3D Scene Generation for Embodied AI](https://arxiv.org/abs/2602.10116)
*Hongchi Xia,Xuan Li,Zhaoshuo Li,Qianli Ma,Jiashu Xu,Ming-Yu Liu,Yin Cui,Tsung-Yi Lin,Wei-Chiu Ma,Shenlong Wang,Shuran Song,Fangyin Wei*

Main category: cs.CV

TL;DR: SAGE提出一个能根据任务自动生成可仿真的3D场景的代理式框架，结合多种生成器与评估器迭代优化，产出真实、多样、物理可行的环境，用于训练能泛化的具身策略。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据采集昂贵且有安全隐患，现有场景生成依赖规则或特定任务，易产生伪影与物理不一致，缺乏可规模化、可直接用于仿真的高质量3D环境。

Method: 提出SAGE：给定具身任务指令，使用多种布局与物体组合生成器产生候选场景；再用多重“批评者”从语义合理性、视觉真实度与物理稳定性评估；通过迭代推理与自适应工具选择，自我修正直至满足用户意图与物理有效性，输出可直接用于现代模拟器的场景。

Result: 生成的环境真实、多样、物理有效，并能规模化生产（SAGE-10k）。仅用该数据训练的策略表现随数据规模清晰上升，并能泛化到未见过的物体与布局。

Conclusion: 代理式、多评估器联合的生成-自我修正框架可高效产出仿真就绪的3D环境，支持具身策略的可扩展训练与泛化，验证了依赖高质量模拟数据进行规模化学习的可行性与前景。

Abstract: Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>
