<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出从单目图像恢复真实尺度的3D食物模型，用于更准确的摄入量估计，在两个公开数据集上将体积估计MAE降低近30%。


<details>
  <summary>Details</summary>
Motivation: 饮食相关慢病（肥胖、糖尿病）上升，需要准确监测食物摄入量。现有单目图像的AI膳食评估难以可靠恢复份量/尺度；一些3D重建虽几何精细但缺乏真实世界尺度，限制在精准营养中的应用。

Method: 先用单目3D重建得到几何，再利用在大规模数据上训练的模型提取丰富视觉特征，学习估计对象的真实尺度；将估计尺度用于对重建结果进行物理尺寸校准，得到真实比例的3D模型。包含消融实验验证各组成的作用。

Result: 在两个公开数据集上，相比现有方法有一致优势，体积估计的平均绝对误差降低约30%。

Conclusion: 通过从单目图像学习尺度并校准重建，可将单景3D结果转化为真实物理意义的模型，显著提升食物体积/份量估计的准确性，具有推动精准营养应用的潜力。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 提出DiSa框架，通过显著性感知的前景-背景解耦与层次化细化模块，缓解VLM在开放词汇语义分割中的前景偏置与定位不准问题，在六个基准上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 基于图文对预训练的VLM（如CLIP）用于密集预测时倾向显著、以物体为中心的区域，导致忽视背景（前景偏置）与边界模糊（空间定位受限）。需要一种能显式处理前景/背景并提升像素级定位与边界质量的方法。

Method: 提出DiSa框架，包括：1) 显著性感知解耦模块（SDM），显式引入显著性线索，将特征按前景与背景分别建模，形成前后景集成特征，分而治之；2) 层次化细化模块（HRM），利用像素级空间上下文并在通道维进行多级更新，以逐级细化与校正特征，实现更精确的边界与区域一致性。

Result: 在六个公开基准上，DiSa在开放词汇语义分割任务上持续超过现有SOTA方法（摘要未给出具体数值）。

Conclusion: 显著性驱动的前后景解耦结合层次细化能有效缓解VLM的前景偏置与定位不准，提升开放词汇语义分割的精度与边界质量，并在多基准上验证其通用性与优越性。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: 提出SSMAE：在少标注多无标注场景下，把掩码自编码重建与分类联合训练，并用验证驱动的门控机制在模型足够可靠时才启用跨弱/强增广一致的伪标签，从而提升ViT的半监督性能。


<details>
  <summary>Details</summary>
Motivation: ViT在标注稀缺时训练困难；现有MAE自监督预训或半监督伪标签方法要么忽视分类目标、要么早期伪标签不可靠易致确认偏差。需要一个能高效利用大量无标注数据、同时避免伪标签时机不当导致误导的框架。

Method: 提出Semi-Supervised Masked Autoencoder：1）同时优化掩码图像重建（利用无标注）与分类（利用有标注与被筛选的伪标签）；2）设计验证集驱动的门控机制，仅当模型在验证上达到可靠度阈值且对同一图像的弱/强增广预测一致、置信度高时才启用伪标签；3）动态选择伪标签并联合训练，减少确认偏差。

Result: 在CIFAR-10/100上均优于监督ViT与微调MAE，尤其在低标注比例场景有显著提升：CIFAR-10仅10%标注时较ViT提升+9.24%。

Conclusion: 伪标签“何时引入”与“如何生成”同等关键。通过门控启用与一致性约束，SSMAE能更数据高效地训练ViT，缓解确认偏差并在少标注条件下显著提升性能。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [4] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 论文提出在CLIP训练中直接引入稀疏性，得到既可解释又高性能的稀疏表征，优于事后稀疏自编码器（SAE），并保留多模态能力；基于这些稀疏表征还能实现语义对齐、训练动态洞察和可解释的视觉 steering。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP潜表征致密且难以解释；普遍观点认为可解释性与性能存在权衡。事后方法（如SAE）在下游性能与多模态能力上常有退化，且学到的特征多为单模态，限制了可解释应用。

Method: 在CLIP预训练过程中显式融入稀疏性正则/约束，得到稀疏多模态特征；与SAE进行系统对比评估，包括下游性能、特征可解释性与多模态性；分析稀疏多模态特征的语义对齐与训练过程中跨模态知识涌现的动态；基于稀疏CLIP表征训练一个小型视觉-语言模型，展示可解释的视觉驱动控制能力。

Result: 与SAE相比，稀疏CLIP在下游任务上保持强性能，同时显著提升可解释性并保留/增强多模态特征；稀疏多模态特征可直接实现语义概念对齐，揭示跨模态知识形成的训练轨迹；基于其表征的VLM实现了可解释的视觉steering示例。

Conclusion: 可解释性与性能并非必然冲突；通过在训练中一体化引入稀疏性，可同时优化两者，为未来多模态模型提供了可解释且高效的设计范式。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [5] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 该论文不开发新模型，而是系统梳理并统一多个人工标注的公开H&E核实例分割数据集，基于两种SOTA模型对数据集进行跨数据集评测与排名，并发布统一测试集（NucFuse-test）与统一训练集（NucFuse-train）以提供公平评测和更强泛化的训练基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究多偏重提出新分割算法，且只在少量任意选取的数据集上评测，缺乏统一标准与系统性基准，导致结果不可直接对比与泛化不明。

Method: 1) 文献调研收集并筛选公开H&E核分割数据集；2) 统一输入与标注格式；3) 选用两种代表性SOTA模型（CNN与CNN+ViT混合）在各数据集与跨数据集场景下系统训练与评测；4) 依据性能对数据集进行排名；5) 构建统一测试集NucFuse-test用于公平评测，及融合训练集NucFuse-train以提升性能；6) 开源代码与流程。

Result: 给出了多数据集的系统性评测与排名，证明融合训练（NucFuse-train）可提升分割性能与泛化；统一测试集（NucFuse-test）实现跨数据集的公平比较；并通过外部验证支持结论。

Conclusion: 该工作提供了标准化数据、统一评测协议与公开实现，成为H&E核实例分割任务的实用基准，有助于更公平地比较模型并推动更具泛化性的研究。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [6] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 提出一种训练免调的结构锚点剪枝（SAP）方法，在视觉文档检索中能把索引向量压缩90%+且保持检索效果，借助新评测协议OSR揭示中间层保留关键语义结构信号，比末层更适合剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型支持细粒度文档检索，但索引向量巨大；无训练剪枝（如基于EOS注意力）在高压缩比时常劣于随机，学界认为因为视觉token重要性强依赖查询，导致训练免调剪枝被质疑可行性。

Method: 提出Structural Anchor Pruning（SAP）：在模型中间层识别并保留“语义结构锚点”patch，进行训练免调剪枝；并提出Oracle Score Retention（OSR）协议，量化不同层信息对压缩效率的贡献，用以指导/评估剪枝层位选择。

Result: 在ViDoRe基准上，SAP在保持鲁棒检索效果的同时将索引向量减少90%以上，显著优于以往训练免调方法，特别是在高压缩场景。

Conclusion: 视觉文档的关键结构语义在中间层更稳定可迁移，末层结构信号趋于消散；因此无需再训练也能通过中层“结构锚点”实现高效压缩与高保真检索，为可扩展的Visual RAG提供可行路径。

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [7] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 论文提出在扩散式大规模多模态模型（如 LLaDA‑V）中进行结构化视觉token剪枝，在保持性能的同时显著降低计算量（FLOPs），最高降至35%剩余计算、保留约95%任务表现。


<details>
  <summary>Details</summary>
Motivation: 扩散式LMM采用双向注意力与多步去噪，导致视觉token在所有层与所有步反复参与计算，计算开销巨大。注意力分析显示，与自回归解码器不同，LLaDA‑V的跨模态信息主要在中后层聚合，语义对齐偏晚，这为更高效的剪枝时机提供了依据。

Method: 受FastV启发，提出结构化token剪枝：在首个去噪步的中后层，按比例有选择地移除部分视觉token，使后续所有去噪步都受益于减少的序列长度，从而降低FLOPs。与FastV在浅层剪枝不同，本方法匹配LLaDA‑V的“延迟注意力聚合”特性以维持输出质量。

Result: 在多项基准上，最优配置可削减最高约65%的计算成本，同时平均保留约95%的任务性能。

Conclusion: 证明了在扩散式多模态模型中进行“视觉感知”结构化剪枝的有效性与通用潜力，为高效的LLaDA‑V推理提供了经验依据，并指出在中后层与首步去噪处进行剪枝是兼顾效率与质量的关键。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [8] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle提出一种轻量的图像与视频风格迁移方法，基于Qwen-Image-Edit，通过精心构建的风格三元组数据与课程式持续学习，在保持内容一致的同时提升风格相似度与审美质量，并引入视频到视频模块增强时序一致性，达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: DiT在风格迁移中内容与风格特征高度耦合，导致难以同时保持内容与风格。需要一种既能泛化到未见风格、又能精准保持内容结构，并在视频场景中保证时序一致性的轻量方案。

Method: 1) 以Qwen-Image-Edit为基座，利用其内容保持与风格定制能力；2) 构建高质量“特定风格”数据并合成大规模多风格三元组（内容、风格、目标）；3) 提出“课程式持续学习”（先干净再噪声、逐步难化）在混合数据上训练，实现对未见风格的泛化；4) 设计视频到视频风格化模块，增强时序一致与视觉质量。

Result: 在风格相似度、内容一致性、审美质量三项核心指标上达到SOTA；模型在图像与视频任务上均表现优异，具备对未见风格的泛化能力。

Conclusion: TeleStyle通过课程式持续学习与视频时序模块，在不牺牲内容保真的前提下实现强风格化效果并具备良好泛化，成为轻量且实用的DiT风格迁移方案；代码与预训练权重已开源。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [9] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: 研究比较传统计算机视觉与大多模态LLM在船体生物污损严重度（LoF尺度）自动判别上的效果，发现二者各有优势，建议融合分割覆盖率与LLM推理以实现可扩展、可解释评估。


<details>
  <summary>Details</summary>
Motivation: 潜水员人工检查既危险又难以规模化，且船体生物污损带来生态、经济与生物安全风险，亟需自动化、可解释且可扩展的评估方法。

Method: 基于新西兰初级产业部专家标注数据，评估卷积网络分类、Transformer分割等计算机视觉模型与零样本多模态LLM；为LLM设计结构化提示与检索；比较在LoF各等级的表现并分析类别不均衡与取景问题。

Result: 传统视觉模型在LoF极端等级上准确率高，但在中间等级受数据不平衡与图像取景影响而下降；零样本LLM在无训练下获得有竞争力的性能，并能给出可解释输出。

Conclusion: 计算机视觉与LLM具有互补性；将分割得到的覆盖率等量化特征与LLM推理结合的混合方法，有望提升可扩展性与可解释性，用于实际的生物污损评估。

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [10] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO 为基于流匹配（flow matching）的文本生成图像模型引入“稠密奖励”，按去噪时间步评估与分配偏好信号，并用奖励感知的采样噪声注入自适应校准探索空间，从而缓解稀疏奖励与探索失配问题，在多基准上显著提升对人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的偏好对齐方法将整条去噪轨迹的终端奖励均摊到所有中间步骤，导致全局反馈与各步实际贡献不匹配（稀疏且粗粒度），训练信号噪声大、效率低。同时，统一的探索策略忽视了不同时间步噪声强度差异，造成探索空间设置不当。

Method: 1) 稠密奖励：通过ODE式中间清晰图像重建，在每个去噪时间步上用奖励模型评估“增益”，得到逐步的奖励分配，确保反馈与贡献对齐；2) 奖励感知探索：基于估计的稠密奖励，发现统一探索与时变噪声不匹配，据此在SDE采样器中自适应调整各时间步的随机性注入，校准探索空间。整体嵌入GRPO框架以进行偏好对齐训练。

Result: 在多项标准数据集/基准上实验，DenseGRPO优于现有GRPO式对齐方法，验证了稠密奖励与奖励感知探索对提升对齐质量与训练有效性的关键作用。

Conclusion: 稠密化偏好信号并按时间步自适应探索可有效缓解稀疏奖励与探索失配难题，是提升流匹配文本生成图像模型人类偏好对齐的有效途径。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [11] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出FPL，将分类转化为特征投影重构问题：用类原型投影并重构查询特征图，以重构误差作为分数，并与原始CLIP预测融合；在准确率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP等视觉-语言对比预训练模型在下游高效适配时面临三难：性能受限、可训练参数多、训练耗时长，需要一种参数高效、训练简便且效果更强的适配策略。

Method: 构建投影模型：将每个类别的原型特征投射到查询图像的特征空间以重构查询特征图；以负的平均平方重构误差作为该类得分，将分类任务转化为特征投影/重构问题；最终将投影模型的预测与预训练CLIP的原始预测进行融合。

Result: 在多项实证评测中，FPL在准确率上显著超越当前最先进方法，显示更强的泛化与适配性能。

Conclusion: FPL以简单高效的特征投影重构机制，实现低监督、低参数代价的CLIP适配，并在多个基准上取得显著领先，说明该范式是适配VLP模型的有效方向。

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [12] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: 提出Prompt-Agnostic Evolution（PAE）以稳定并加速视觉提示微调（VPT），通过频域初始化、共享Koopman算子建模跨层演化、以及Lyapunov风格正则，缓解浅层停滞与深层振荡问题，实现更快收敛与更高精度。


<details>
  <summary>Details</summary>
Motivation: 现有VPT在冻结ViT上插入提示token进行适配，但训练过程中常出现梯度/动态不稳定：浅层提示早期停滞，深层提示高方差振荡，跨层不协调，导致收敛慢与性能下降。需要一种能显式建模提示演化、稳定训练并提升泛化的方法。

Method: 1) 频域视角初始化：挖掘并传播主干网络在识别中利用的“频率捷径”模式，将提示初始化到任务相关方向。2) 共享Koopman算子：以全局线性变换统一建模各层提示的演化，替代层内各自为政的更新，保证跨层一致性。3) Lyapunov稳定性启发的正则：约束演化过程中的误差放大，抑制振荡。方法对提示类型无关、无需改主干与推理改动，可无缝接入多种VPT变体。

Result: 在25个数据集、多个下游任务上，平均收敛加速1.41×；最终精度提升约1–3%。训练更稳定、层间演化更协调。

Conclusion: 通过PAE对提示动态进行显式建模，可在不改动主干和推理流程的前提下，稳定并加速VPT训练、提升性能；方法通用、轻量、可与不同VPT变体结合。

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [13] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出BLenDeR，一种用于度量学习的扩散采样方法，通过对去噪残差执行“并/交”操作，受集合论启发，以可控方式提升类内多样性，从而显著提高Recall@1。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型可用于数据增强，但在DML中难以细粒度、可控地增加类内属性多样性，且不同属性组合难以系统合成，限制了下游检索/聚类性能。

Method: 在扩散模型采样过程中，对来自多个条件/提示的去噪残差实施集合论式操作：并运算促进“任一提示中存在的属性”出现；交运算通过主成分方向近似共同属性，提取共有特征。由此可控制在同一类别内生成多样但相关的属性组合，用作DML训练数据增强。

Result: 在标准DML基准上，BLenDeR在多数据集和骨干网络上优于SOTA：CUB-200的Recall@1提升约3.7%，Cars-196提升约1.8%，并保持稳定一致的增益。

Conclusion: 通过在扩散去噪残差上施加集合论式并/交操作，BLenDeR实现了可控的类内多样性合成，克服现有生成方法的局限，显著提升DML表现。

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [14] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出RED可逆高效扩散模型，用显式监督训练，避免扩散估计带来的噪声累积与细节丢失，在多模态图像融合中兼顾细节与效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于图像融合时因马尔可夫噪声累积导致细节缺失与不一致；而端到端引入显式监督又计算开销大，需一种既保留扩散生成力又高效且细节友好的方法。

Method: 设计RED：基于扩散思想的可逆、高效训练框架，抛弃分布估计，引入显式监督信号；通过可逆结构与高效反演减少噪声误差累积，并在融合过程中直接优化细节保真与一致性。

Result: 在多模态融合任务中，相比传统扩散融合方法，RED实现更好的细节保留与视觉一致性，同时显著降低训练/推理的计算成本。

Conclusion: RED在不依赖分布估计的前提下，将显式监督融入扩散式框架，缓解细节丢失和不稳定问题，为高保真、高效率的多模态图像融合提供了有效方案。

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [15] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: 提出LVLMs-Saliency：融合注意力权重与梯度的“显著性”度量来评估每个输出token的视觉扎根强度，并基于此在推理时用拒绝采样与局部一致性强化双机制降低幻觉。


<details>
  <summary>Details</summary>
Motivation: 仅用前向注意力模式难以可靠区分幻觉与事实扎根输出，忽视了梯度能揭示影响如何在网络中传播；需要更稳健、可解释的LVLM幻觉检测与缓解方法。

Method: 1) 梯度感知诊断：将注意力权重与相应输入梯度融合，得到每个输出token的视觉扎根显著性；分析发现当先前输出token对下一token预测的显著性低时更易产生幻觉。2) 双机制推理框架：a) 显著性引导拒绝采样(SGRS)：自回归解码时动态过滤显著性低于上下文自适应阈值的候选token；b) 局部一致性强化(LocoRE)：轻量可插拔模块，增强当前token对最近前序token的注意力，抵消上下文遗忘。

Result: 在多种LVLM上显著降低幻觉率，同时保持流畅度与任务性能；方法稳健且具有可解释性。

Conclusion: 融合梯度与注意力的显著性度量可有效诊断并缓解LVLM幻觉；结合SGRS与LocoRE的推理时干预为提升模型可靠性提供了通用、可解释的方案。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [16] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出一种无需源数据的域自适应方法，通过多视图增强与潜空间一致性学习，仅用目标域数据即可学习可迁移表示，避免对齐与伪标签，在Office-31/Office-Home/Office-Caltech上取得SOTA或显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有DA常需访问源域、对抗训练或复杂伪标签，计算代价高且在隐私/合规受限场景不可行。亟需一种仅依赖目标域、训练稳定且高效的源自由（source-free）域自适应方案。

Method: 在目标域上生成多种数据增强视图，使用ConvNeXt编码器提取特征；在潜空间中对同一样本不同视图的表示施加一致性约束，结合分类损失与一致性损失进行联合优化，无需源-目标对齐或伪标签迭代。

Result: 在Office-31、Office-Home、Office-Caltech分别达到90.72%、84%、97.12%平均准确率；相对现有方法平均提升+1.23%、+7.26%、+1.77%。

Conclusion: 多视图增强+潜空间一致性可在完全源自由设定下有效学习域不变特征，带来稳定且高性能的跨域分类；证明无需访问源域即可达到或超过现有方法。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [17] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 提出视频生成评测新协议（外观/运动/机位三轴+10类伪影分类），构建80k带标注数据集GenVID，并基于此训练细粒度伪影识别框架DVAR，显著提升伪影检测与低质内容筛除效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评估多给出整体质量分，缺乏对具体伪影的定位与类别判别，难以满足审计与实际应用中的精细质量控制需求。

Method: 1) 定义人感知相关的三大评测轴：Appearance、Motion、Camera，并据此建立涵盖10类常见生成失败的伪影分类体系；2) 构建GenVID：收集多个SOTA视频生成模型产生的80k视频，按10类伪影细致标注；3) 提出DVAR框架：进行密集视频伪影识别与分类，实现细粒度定位与多类别判别；4) 通过广泛实验验证。

Result: 在大规模实验中，DVAR基于GenVID训练后，在伪影检测准确率上显著优于现有方法，并能有效过滤低质量生成视频。

Conclusion: 细粒度的三轴-十类评测协议结合大规模标注数据与DVAR模型，可更准确地定位与分类生成伪影，为视频生成系统的质量评估与内容审计提供有效工具。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [18] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: 提出C-SAM：在剪枝掩码空间进行“锐度感知”训练，使剪枝后模型同时更紧凑且对输入扰动更稳健。实验在多数据集与架构上显著提升经认证的鲁棒性（最高+42%），精度接近未剪枝模型。


<details>
  <summary>Details</summary>
Motivation: SAM提升对输入变化的鲁棒性，但与设备端部署的模型压缩（剪枝）存在冲突：1) 先用SAM训练再剪枝，连续参数空间的平坦性不等于对离散结构改变（剪枝）的稳健，鲁棒性会下降；2) 先剪枝再用SAM，早期与鲁棒性无关的剪枝结构限制了后续优化。需要一种在压缩过程中就考虑鲁棒性的学习方法。

Method: 提出Compression-aware SAM (C-SAM)：将传统SAM的参数扰动转为对剪枝掩码（结构）进行显式扰动，最小化对掩码扰动的最坏损失，从而在“模型结构”维度获得更平坦的损失地形；可同时搜索/学习有利于紧凑性与输入鲁棒性的剪枝模式。

Result: 在CelebA-HQ、Flowers-102、CIFAR-10-C上，ResNet-18、GoogLeNet、MobileNet-V2等架构中，C-SAM相对强基线获得更高的认证鲁棒性，最高提升达42%，同时保持与未剪枝模型相当的任务精度。

Conclusion: 在剪枝过程中进行面向结构的锐度最小化可弥合鲁棒性与压缩之间的张力。C-SAM能学习到对掩码扰动平坦的结构，因而在紧凑性与输入鲁棒性上实现兼得，优于“先SAM后剪枝”或“先剪枝后SAM”的范式。

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [19] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 该研究针对CT引导妇科近距离治疗中膀胱分割的域偏移问题，提出将“无器械(NA)”与“有器械(WA)”CT数据联合训练的双域学习策略。仅用NA训练难以泛化到WA，但在以NA为主的数据中掺入10%–30%的WA样本，即可将分割性能提升到接近只用WA训练的水平，最高Dice达0.94、IoU达0.92。


<details>
  <summary>Details</summary>
Motivation: 临床场景中WA图像稀缺且存在解剖变形与成像伪影，导致模型在域偏移下性能骤降；同时NA图像充足但分布不同。如何在目标域数据有限时，利用分布相近但有偏移的数据来提升分割鲁棒性与临床可用性，是待解的关键问题。

Method: 提出“双域学习”策略：以NA为主体训练集，系统性地按不同比例掺入少量WA样本（10%–30%），在轴向、冠状、矢状三个切面上、使用多种深度分割架构进行训练与评估，比较不同WA掺入比例下的泛化与性能。

Result: 仅用NA数据训练无法捕捉WA图像的解剖与伪影特征；当训练集中加入10%–30%的WA样本时，模型在WA测试集上的分割性能显著提升，达到与仅用WA训练模型相当的水平。最高Dice=0.94、IoU=0.92，体现出有效的域适应。

Conclusion: 在目标域数据稀缺的情况下，利用与其解剖相似但存在分布偏移的丰富数据，通过少量目标域样本“掺入”即可实现接近目标域专训的性能。该策略提高了膀胱分割在近距离治疗计划中的可靠性，对医疗影像分割中应对协变量偏移具有普适借鉴意义。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [20] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 本文提出从单幅RGB图像估计物体质量的方法，通过显式对齐体积与密度两大物理因素，结合几何重建与材料语义，引入实例自适应融合与双分支潜变量回归，在两个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 质量由体积与密度共同决定，但二者均不可直接从RGB外观观测，导致从图像估计质量高度不适定。需要引入物理可解释表示来约束解空间、提高泛化与可解释性。

Method: 从单张图像出发：（1）用单目深度估计恢复以目标为中心的3D几何，支撑体积相关信息；（2）利用视觉-语言模型提取粗粒度材料语义，指导密度相关推断；（3）将几何、语义与外观表示通过实例自适应门控机制融合；（4）设置两个物理引导的潜在因子分支，分别回归体积相关与密度相关表征，最终在仅有质量标注的监督下联合学习并预测质量。

Result: 在 image2mass 与 ABO-500 数据集上，方法在质量估计上持续优于最先进基线，取得更低误差/更高相关性（文中未给出具体数值）。

Conclusion: 将物理结构先验融入从图像估计质量的任务，显式分解体积与密度因素并结合实例自适应融合，可在仅质量监督下实现更准确、可解释的质量预测，优于现有方法。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [21] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 提出SLDM，将低碘对比剂CTA合成为等效常规剂量，兼顾结构一致与语义空间精度，并通过减影增强提高造影可视性，定量定性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低剂量碘造影剂降低肾损伤与过敏风险，但现有从低剂量到常规剂量的深度重建方法在配对不完整场景下结构识别能力弱，导致增强不准确，需要一种既保结构一致又能精确定位造影区域的生成模型。

Method: 提出结构约束+语言/语义空间监督的扩散模型SLDM：1) 提取并注入结构先验，约束扩散推理，保证血管等解剖结构一致；2) 语义监督策略引入空间智能，将视觉感知与空间推理结合，指导精准增强；3) 设计减影血管增强模块，提升ICM区域对比度至可观察区间。

Result: 在低剂量CTA到常规剂量的重建任务上，经多种指标定量评估与可视化对比，SLDM取得更好结构保持与造影增强效果，整体优于现有方法。

Conclusion: SLDM实现了在不完全配对数据下的高保真造影增强，既降低对比剂用量又维持诊断质量，为低剂量CTA提供可行方案。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [22] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: 提出TPGDiff三重先验引导扩散网络，用退化/结构/语义三类先验分层引导扩散过程，实现统一多任务图像复原并在多基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有“一体化”图像复原依赖退化先验但在重度退化区域难以重建；把语义信息塞到扩散模型浅层易破坏空间结构（如模糊）。需要一种既保结构又能在重度退化下生成合理内容的分层先验引导策略。

Method: 构建Triple-Prior Guided Diffusion：1）沿全扩散轨迹注入退化先验（由退化提取器学习），实现按时间步自适应控制；2）在浅层引入多源结构线索作为结构先验，捕获细节与几何结构；3）在深层引入经蒸馏训练的语义提取器输出鲁棒语义先验，为高层内容生成提供稳定指导；三类先验形成分层互补引导。

Result: 在单一与多重退化基准上取得更优的客观指标和更强泛化能力，优于现有统一复原方法；对严重退化区域的重建质量提升并减少模糊伪影。

Conclusion: 分层、互补的三重先验（退化-结构-语义）与全时步扩散引导可在统一框架下兼顾结构保真与内容补全，实现更稳健的多场景图像复原，具有良好泛化。

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [23] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: 提出OSDEnhancer：首个面向真实场景的一步扩散式时空视频超分框架，通过线性预插值初始化时空结构，结合时序细化与空间增强MoE以及双向可变形VAE解码器，实现高保真且时序一致的升采样，达到SOTA并具备强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在视频超分上能生成细节，但对同时提升分辨率与帧率的时空视频超分(STVSR)研究不足；多数STVSR方法假设简化退化模型，难以适应真实复杂退化；在保证重建保真与时序一致性的同时实现鲁棒STVSR具有挑战。

Method: 提出OSDEnhancer：1) 线性预插值初始化关键时空结构；2) 训练时序细化-空间增强混合专家（TR-SE MoE），分别学习时序一致性与空间细节，并在推理时协同强化；3) 设计双向可变形VAE解码器，进行递归式时空聚合与传播，提高跨帧重建质量；4) 采用高效“一步扩散”流程实现端到端的真实场景STVSR。

Result: 在多项实验中取得SOTA性能；在真实复杂退化场景下表现出优越的泛化与稳健性，能够恢复高保真细节并保持时序连贯。

Conclusion: 一步扩散+TR-SE MoE+双向可变形VAE的协同，使OSDEnhancer在真实场景STVSR中兼顾细节、保真与时序一致性，达到SOTA并具备良好泛化，验证了该框架的有效性。

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [24] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: 提出CPiRi：一种通道置换不变的多变量时间序列预测框架，通过冻结时间编码器、轻量空间模块与通道打乱正则，实现SOTA性能、对通道顺序稳健，并可对未见通道归纳泛化。


<details>
  <summary>Details</summary>
Motivation: 现有通道相关模型易过拟合通道顺序、在通道增删/重排时适应性差；通道独立模型忽视跨通道依赖、性能受限。需要一种既能捕获跨通道结构又能对通道顺序与分布共漂移鲁棒的方法。

Method: 提出CPiRi框架与理论分析：1) 架构上时空解耦——冻结预训练的时间编码器抽取高质量时间特征；轻量空间模块基于内容学习跨通道关系；2) 训练上引入通道洗牌的置换不变正则，强制模型学习与通道排列无关的表示；3) 从理论上分析多元时间序列预测中的置换等变性/不变性。

Result: 在多项基准上达到SOTA；在通道顺序打乱时性能稳定；仅用一半通道训练即可对未见通道有强归纳泛化；在大规模数据上保持高效。

Conclusion: 通过CPI正则与时空解耦，CPiRi在不重新训练的情况下适应通道结构与分布共漂移，兼顾性能、稳健性与效率，为多元时间序列预测提供通道置换不变的新范式。

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [25] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 论文提出两种约束来提升3D Gaussian Splatting的表面重建精度：基于可见性的多视角几何一致性与逐级四叉树校准的单目深度约束，显著减轻深度监督误差并在DTU、TNT上取得更优几何表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian Splatting的重建在表面几何上不稳定：多视几何在大几何差异时不可靠，单目深度先验存在尺度歧义与局部不一致，导致高质量渲染但几何精度不足。

Method: 1) 高斯可见性感知的多视几何一致性：聚合跨视角共享高斯原语的可见性信息，提供更稳健的几何监督。2) 渐进式四叉树校准的单目深度约束：从粗到细对图像块执行仿射（尺度/偏移）校准，逐步缓解尺度歧义并保留细节。两者结合以联合优化高斯深度并提升表面重建。

Result: 在DTU与TNT数据集上较以往高斯方法与隐式表面重建方法均有一致的几何精度提升。

Conclusion: 通过可见性感知的多视一致性与分层块状仿射校准的单目深度监督，显著提高高斯渲染框架中的几何准确性与稳定性，验证了方法在标准数据集上的有效性。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [26] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: 提出TopoOT：在异常分割中结合拓扑数据分析与最优传输，通过多重滤波持久性图的OT级联对齐，生成跨尺度稳定性分数与伪标签，并在测试时自适应训练轻量头，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的二值化在分布偏移下易碎，难以稳健捕捉异常；需要能表征跨尺度的全局结构破坏的方法。TDA可提取连通性与环等结构不变量；结合测试时自适应有望在域移下保持鲁棒。

Method: 构建多滤波的持久性图，提出“最优传输级联”顺序对齐不同阈值与滤波下的PD，得到跨尺度的测地稳定性分数；基于此生成稳定性感知的伪标签；在线训练一个轻量解码头，以OT一致性损失与对比损失进行TTA。

Result: 在标准2D与3D异常检测/分割基准上取得SOTA：2D平均F1最高提升达+24.1%，3D基准提升达+10.2%。

Conclusion: 以拓扑结构稳定性为核心的OT-TDA联合框架能在域移下稳健适应，显著提升异常分割性能，验证了跨尺度拓扑对齐与稳定伪标签的有效性。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [27] [MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis](https://arxiv.org/abs/2601.20347)
*Chengying She,Chengwei Chen,Xinran Zhang,Ben Wang,Lizhuang Liu,Chengwei Shao,Yun Bian*

Main category: cs.CV

TL;DR: 提出MMSF：基于线性复杂度MIL的多任务多模态监督框架，显式分解并融合病理WSI与临床特征，通过图特征、临床嵌入、共享/特异表示对齐与Mamba-MIL编码器，实现在分类与生存任务上显著优于强基线。


<details>
  <summary>Details</summary>
Motivation: 计算病理需要同时利用WSI肿瘤形态与患者临床上下文，但两种模态统计分布与尺度差异大，融合困难。现有方法要么仅用单模态，要么融合不足，难以捕捉跨模态共享与特异信息并保持高效。

Method: 构建MMSF框架：1) 图特征提取模块在patch层面编码组织拓扑；2) 临床数据嵌入模块标准化并嵌入患者属性；3) 特征融合模块显式对齐并分解模态共享与模态特异表示；4) 基于Mamba的线性复杂度MIL编码器，接多任务预测头（分类、AUC/生存C-index）。整体为监督训练。

Result: 在CAMELYON16与TCGA-NSCLC分类任务上，相比强基线提升准确率2.1–6.6%、AUC提升2.2–6.9%；在5个TCGA生存队列上，相对单模态提升C-index 7.1–9.8%，相对其他多模态方法提升5.6–7.1%。

Conclusion: 显式分解共享/特异信息并在线性复杂度MIL上融合WSI与临床数据，可在分类与生存分析中稳定优于单模态与现有多模态方法，证明MMSF的有效与高效。

Abstract: Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\% accuracy and 2.2--6.9\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\% C-index improvements compared with unimodal methods and 5.6--7.1\% over multimodal alternatives.

</details>


### [28] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: PalmBridge提出一种基于向量量化的特征空间对齐与融合框架，通过将特征映射并与代表向量混合，抑制域偏移带来的无关变化，提升开放集掌纹验证在同域与跨域下的泛化，且开销小、EER显著降低。


<details>
  <summary>Details</summary>
Motivation: 现实部署中采集设备、光照、皮肤状态等差异导致掌纹特征分布迁移，现有深度模型多在封闭、平稳分布假设下训练，易过拟合数据集特定纹理。数据增强常被用于缓解，但默认增强样本能近似目标分布，这在显著域不匹配时并不成立，因此需要一种不依赖数据级匹配、能在特征空间对齐并保持身份判别性的方案。

Method: 提出PalmBridge：1) 在训练特征上学习一组紧凑的代表向量（codebook），与主干网络联合优化；2) 注册与验证时，将每个特征映射到最近代表向量，并按最小距离准则选择，再将映射向量与原向量按权重混合，以压制域移引入的干扰；3) 通过任务监督（如验证损失）、特征一致性目标及正交正则，构建稳定、结构化的共享嵌入空间；4) 通过分配一致性与碰撞率分析映射与权重敏感性；适配多骨干与数据集，开销可控。

Result: 在多掌纹数据集与不同骨干上，PalmBridge在开放集同数据集评测中持续降低EER，并在跨数据集评测中显著提升泛化性能，同时推理时仅带来可忽略到中等的运行时开销。

Conclusion: 特征空间的向量量化对齐与混合可在不依赖强假设的数据增强前提下，有效抑制域偏移导致的无关变化并保持身份判别性；PalmBridge作为即插即用模块，提升了开放集掌纹验证的鲁棒性与跨域泛化。

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [29] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出SpatialGenEval基准与SpatialT2I数据集，系统评测并提升T2I模型的空间智能，显示高阶空间推理是当前瓶颈，数据微调带来稳定提升。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型虽能生成高保真图像，但在复杂空间关系（位置、遮挡、因果、交互等）的感知与推理上表现欠佳；现有基准提示短、信息稀疏，难以覆盖并诊断这些能力缺陷。

Method: 1) 构建SpatialGenEval：包含25个真实场景、共1,230条长且信息密集的提示，每条提示涵盖10个空间子领域并配套10个多选QA，用于系统评测空间智能；评测21个SOTA模型以定位瓶颈。2) 构建SpatialT2I：基于信息密集提示重写以保证图像一致性，同时保留信息密度，形成15,400对图文数据；在SD-XL、Uniworld-V1、OmniGen2上微调并验证效益。

Result: 大规模评测显示高阶空间推理是主要瓶颈；使用SpatialT2I微调在多个基础模型上带来一致性能提升（约+4.2%、+5.7%、+4.4%），并改善空间关系的真实感。

Conclusion: 信息密集型提示可作为既能评测也能促进学习的手段；数据中心范式（构建高质量、空间约束强的数据）能显著增强T2I模型的空间智能，未来应进一步针对高阶推理与复杂交互设计数据与模型。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [30] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: 提出CURVE：结合变分不确定性建模与不确定性引导的结构正则以抑制环境特定高方差关系，从而让场景图具备跨分布泛化；在零样本迁移与低数据仿真到真实任务上验证其学习稀疏、域稳定拓扑与可靠不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统场景图容易对训练域的偶然相关性过拟合，导致分布外泛化差；需要一种能区分域不变因果交互与域相关噪声/偏差，并在分布漂移下仍稳健的建模方式。

Method: 基于因果启发：1) 变分不确定性建模以量化关系预测的不确定性；2) 不确定性引导的结构正则化，抑制高方差、环境特定的关系边；3) 原型条件去偏（prototype-conditioned debiasing），将交互中的域不变动态与域相关变化解耦，促成稀疏、域稳定的图拓扑。

Result: 在零样本跨域迁移与低样本的仿真到真实适配实验中，CURVE学到更稀疏且域稳定的场景图结构，具备更可靠的不确定性估计，可用于分布移位下的风险预测；整体优于基线。

Conclusion: 通过将不确定性建模与因果化的结构正则结合，CURVE有效抑制环境特定关系，提升场景图的OOD泛化与风险感知能力。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [31] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 提出RAW-Flow：用流匹配在潜空间做确定性“RGB→RAW”传输，结合跨尺度上下文引导与双域潜编码器，实现更一致的细节与色彩重建，并在多项指标上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 直接回归的RGB→RAW重建受逆ISP病态性与RGB量化信息缺失影响，易出现细节不一致与色偏；需要一种能在保持确定性前提下更好建模RGB与RAW关系的框架。

Method: 将RGB→RAW重建重构为确定性潜在传输问题：1) 采用flow matching学习潜空间中的确定性向量场，将RGB潜表示映射到RAW潜表示；2) 设计跨尺度上下文引导模块，在流估计中融合分层RGB特征；3) 构建带特征对齐约束的双域潜自编码器，共同编码RGB与RAW以稳定训练并提升保真。

Result: 在广泛实验中，RAW-Flow在定量指标与视觉质量上均优于现有方法，能更准确重建结构细节与颜色。

Conclusion: 以生成式的确定性潜传输替代直接回归可更好解决逆ISP带来的细节与色彩缺失问题；跨尺度引导与双域潜编码器进一步提升稳定性与保真，方法达到当前最优性能。

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [32] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: 提出一种将RFID门禁与环境安全监测融合的双模态IoT架构，基于ESP32与云端，低成本却达到接近商用品质的准确率、响应和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统安防与环境安全系统各自为政，导致运维低效、应急延迟、管理复杂且成本高，需要一个统一、协同、可负担的集成方案。

Method: 设计统一云架构下的双子系统：子系统1用RFID认证+舵机闸控+Google表格实时记录；子系统2集成火焰检测、水流测量、LCD状态显示与人员识别。两者均由ESP32负责边缘计算与无线通信，并采用本地缓存以应对断网。

Result: 45天实验显示：RFID认证准确率99.2%，平均响应0.82秒；火焰检测在5米范围可靠性98.5%；云端数据记录成功率99.8%；在断网时仍保持运行；总成本约5400 BDT（≈$48），较商用品降本82%。

Conclusion: 通过架构优化与元件选型，可在低成本下实现安防-安全一体化的专业级性能，具备高可靠性、快速响应与可扩展性，适用于多种场景。

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [33] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: 提出RepSFNet：轻量、实时的人群计数网络，基于RepLK-ViT大核特征提取，ASPP+CAN上下文自适应融合，拼接保持分辨率，MSE+OT联合损失；在多个数据集上以更低延迟达成与SOTA相当的精度，适合边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有人群计数在密度变化、遮挡与尺度不一致下精度受限；多分支/注意力模型参数量大、推理慢，不利于实时与低功耗部署。需要一种既准确又高效、对变化密度鲁棒的模型。

Method: 1) 主干：RepLK-ViT，利用重参数化大卷积核实现高效多尺度特征提取；2) 特征融合：ASPP捕获多尺度上下文，CAN进行密度自适应的上下文建模；3) 拼接融合模块保留空间分辨率，生成高质量密度图；4) 损失：MSE + 最优传输(OT)联合，兼顾总人数误差与空间分布对齐；5) 结构上避免注意力与多分支以降参降耗。

Result: 在ShanghaiTech、NWPU、UCF-QNRF上取得与最新方法相当的计数精度，同时相比近期SOTA推理时延最高降低34%，显著减少参数与计算复杂度。

Conclusion: RepSFNet在保证精度的同时大幅提升速度与轻量性，能在变密度、遮挡等复杂场景下提供可靠的人群计数，适用于实时与低功耗边缘部署。

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [34] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出HINT：首个用于多人的文本驱动动作生成的自回归扩散框架，能随人数与文本长短变化在线生成长序列，并在InterHuman上取得SOTA（FID 3.100）。


<details>
  <summary>Details</summary>
Motivation: 现有离线方法通常固定序列长度与参与者数量，难以处理长或可变文本与动态人数；需要一种能逐步生成、同时建模多人互动并保持长时一致性的方案。

Method: 1) 采用分解的运动表示并在规范化潜空间中解耦个体局部动作语义与人际交互，从而无需额外微调即可适配不同人数；2) 采用滑动窗口式在线自回归扩散生成，将窗口内的局部条件与跨窗口的全局条件聚合，以捕获过去历史、人物依赖并与文本对齐，既细粒度建模窗口内互动，又保证长程一致性。

Result: 在公共基准上与强离线模型相当并优于自回归基线；在InterHuman数据集上FID达到3.100，显著优于先前SOTA的5.154。

Conclusion: 分层互动建模的自回归扩散框架能够灵活适配人数与文本长度，在线生成长时一致、互动细致的多人动作序列，并在基准上取得领先表现。

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [35] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: 提出BiFTA：通过对图像视图和文本描述的双重去冗余精炼，提高CLIP等模型的细粒度文本-视觉对齐，从而在零样本任务上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法将细粒度文本描述与局部图像patch对齐能提升零样本性能，但细粒粒度描述与局部patch本身常含冗余，互相对齐时被噪声/重复信息稀释，影响效果。需要系统性地剔除冗余以获得更判别性的对齐数据。

Method: BiFTA包含两条并行精炼：1) 视图精炼（View refinement）：对候选局部图像patch进行IoU去重，移除IoU高的重叠patch，保留更具差异性的视觉样本。2) 描述精炼（Description refinement）：计算文本描述两两余弦相似度，删除高相似度的冗余描述，提升语义多样性。随后用精炼后的patch-文本对进行对齐训练/适配。

Result: 在六个基准上，对ViT版与ResNet版CLIP均取得更优的零样本性能，相比未精炼或单侧精炼方案有稳定提升，验证去冗余策略的有效性。

Conclusion: 从视觉与文本双通道同时去冗余能增强细粒度对齐质量，从而改善零样本泛化；简单的IoU与相似度筛选即可带来显著增益，表明冗余是影响细粒度对齐效果的关键因素。

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [36] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 提出“扩散四重奏”，一种结构感知的点云生成框架，通过四个协同扩散模型分别学习全局潜变量、对称性、语义部件及其装配，实现保证对称、连贯装配和高质量多样生成，并支持对部件的细粒度可控编辑，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云生成多将形状视为整体建模，难以同时显式处理部件组合与对称性，导致结构不一致、对称性缺失、可控性差。需要一种能在生成过程中全面引入并强制结构先验（对称、部件）的框架。

Method: 构建四个协同的扩散模型（“四重奏”）：1）全局形状潜变量扩散，捕获整体结构与风格；2）对称性扩散，建模并约束全局/局部对称规则；3）语义部件扩散，生成各部件的点云；4）装配扩散，学习部件在全局潜变量与对称约束下的空间布局；通过可解释的解耦管线实现可控编辑与一致性。

Result: 在基准数据集上达到或超越现有方法的SOTA，生成结果在对称性保证、部件摆放连贯性、质量与多样性方面显著提升，并展示对单个部件的目标化操控能力同时保持全局一致。

Conclusion: 通过将生成过程解耦为全局潜变量、对称性、部件与装配四个扩散模块，方法在保证对称与部件先验的同时提供细粒度控制与高质量生成，是首个全流程整合并强制对称与部件先验的3D点云生成框架。

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [37] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing 是一套以ViT视觉编码器+小型LLM解码器为核心的高效文档解析模型，采用特征可复用与区域提示解码，并通过“token并行”和“query并行”实现显著加速，同时在多元素与多语言/手写等复杂场景达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析（文本、表格、公式、图表等）在速度与精度间权衡明显，传统自回归解码慢且难以在结构化场景高效扩展；同时需要一个能跨多元素、多语言、罕见字符并具鲁棒性的统一解析框架，适用于大规模文档智能应用。

Method: 采用原生ViT和动态分辨率视觉编码器提取共享文档特征；解码端用提示引导的Youtu-LLM-2B进行版面分析与区域提示解码。提出解耦、可复用的特征框架，并设计两类并行解码：1) token并行：每步同时生成最多64个候选token并经验证机制筛选；2) query并行：对多个候选区域/框（最多5个）同时进行内容预测。

Result: token并行带来约5–11倍解码加速，尤其适合高度结构化任务（如表格识别）；在区域提示场景下，query并行可再带来约2倍加速，且质量与标准解码相当。模型覆盖文本、公式、表格、图表、印章、层级结构等元素，并在稀有字符、多语言与手写上表现稳健。

Conclusion: Youtu-Parsing 在OmniDocBench与olmOCR-bench取得SOTA，证明其在大规模文档解析中的效率、泛化性与实用价值；并行解码与特征复用是关键，使其在复杂文档场景兼顾速度与质量。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [38] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: 提出MARE方法：结合多模态对齐与强化学习（含RLHF）提升VLM在Deepfake检测中的准确性、可解释性与空间对齐能力，并通过伪迹解缠模块从高层人脸语义中提取内在伪造痕迹，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测多为分类或空间定位，难以应对快速迭代的生成模型，且缺乏可信、可解释的推理与文本-空间一致性。因此需要让VLM具备更强的检测、定位与人类偏好一致的可解释能力。

Method: 提出MARE框架：1）设计综合奖励函数，引入RLHF，鼓励模型生成与人类偏好一致、在文本与空间上对齐的推理内容；2）加入伪造解缠模块，从高层人脸语义中分离并捕获内在伪造痕迹以提升真伪判别；3）对生成的推理内容进行系统评估。

Result: 在定量与定性实验中，MARE在准确性与可靠性上达到SOTA，并展示更好的文本-空间对齐与可解释推理质量。

Conclusion: 通过多模态对齐与RLHF的强化，再结合伪造解缠，MARE显著增强VLM在Deepfake检测与可解释推理方面的性能与可信度，适应新型生成模型带来的挑战。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [39] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: 提出利用生成器的最终组件“污染”真实图像并训练检测器，从而在未见生成器上实现强泛化，平均准确率98.83%。


<details>
  <summary>Details</summary>
Motivation: 现有深伪图像检测器在未见生成器上泛化差。尽管生成器训练范式不同（扩散、自回归等），许多现代生成器共享相似的最后图像投射/解码组件。若能针对这一共享“末端”特性学习判别信号，或可提升跨生成器泛化。

Method: 1）提出“末端组件污染”（end-layer contamination）：将真实图像经过各类生成器的最终图像生成组件（如卷积解码头、上采样-投影层、色彩空间映射等）处理，得到“被污染”图像；2）以原始真实图像为正样本、污染图像为负样本，训练二分类检测器；3）构建基于生成器最终组件的分类法，将21个主流生成器按末端组件类型分组；4）使用DINOv3作为骨干并进行少量数据微调；5）仅用来自三个代表类别、各100张样本进行训练/微调。

Result: 在22个未见生成器测试集上实现平均98.83%的准确率，显示强跨生成器泛化能力。

Conclusion: 关注生成器共享的最终组件并用其“污染”真实图像进行对比式训练，可大幅提升对未见生成器的检测泛化；基于末端组件的分类法有助于系统评估与扩展该方法。

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [40] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出“Dummy Forcing”以减少自回归视频扩散模型中注意力头对历史上下文的冗余访问，通过异质内存分配、动态头类型编程与上下文打包，在无需再训练的情况下实现最高2.0倍加速，画质下降<0.5%，24.3 FPS。


<details>
  <summary>Details</summary>
Motivation: 发现多头自注意在自回归视频扩散中对历史帧利用不足：约25%的注意力头几乎只看当前帧，即使丢弃其KV缓存性能也仅轻微下降，说明存在显著的上下文冗余与可压缩性。

Method: 1) Dummy Forcing：控制不同注意力头的上下文可达性；2) 异质内存分配：为不同头分配不同长度/粒度的历史上下文以降低冗余；3) 动态头编程：自适应识别并分类头类型（如当前帧头与历史帧头），相应施加约束；4) 上下文打包：对可共享或低用度的KV缓存进行更激进的压缩与复用；整体为推理期方法，无需额外训练。

Result: 在无需再训练的前提下，相比基线实现最多2.0x推理速度提升；在24.3 FPS下生成视频，画质指标下降小于0.5%；移除约25%仅关注当前帧的头的KV缓存几乎不影响性能。

Conclusion: 自回归视频扩散模型的注意力上下文存在系统性冗余。通过Dummy Forcing等推理期机制可有效压缩KV缓存、调度头的上下文访问，在保持几乎不变的质量下显著提速，表明未来可通过更细粒度的头级内存管理进一步提升效率。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [41] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 论文探索用部分标注数据训练同时分割WMH与ISL的模型，比较六种策略，伪标签方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: WMH与ISL在FLAIR上视觉相似且常共存，导致分割与区分困难；完全标注数据稀缺、昂贵，需要利用部分标注与公开数据提升模型性能。

Method: 整合私有与公开MRI数据共2052例，其中WMH有1341例标注、ISL有1152例标注；设计并比较六种利用部分标注的训练策略（含伪标签等），训练联合分割模型，对比其性能。

Result: 多种方法都能有效利用部分标注提高性能，其中基于伪标签训练获得最佳结果。

Conclusion: 在WMH与ISL联合分割任务中，借助部分标注数据可显著提升模型表现；伪标签是最有效策略之一，适用于SVD相关病灶的实际场景。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [42] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 提出一种基于潜空间时间差异(LTD)的损失加权策略，按帧间变化自适应加大动态区域惩罚、保持静态区常规优化，从而稳定训练、提升高频运动重建；在VBench与VMBench上分别提升约3.3%与3.6%。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型多用对所有场景一视同仁的静态损失，难以在剧烈动态变化下维持时序一致性，噪声使动态区域学习更困难，导致运动质量退化。需要一种能感知运动并在训练中区别对待动态/静态区域的机制。

Method: 提出Latent Temporal Discrepancy (LTD) 作为运动先验：在潜空间度量相邻帧差异，据此对损失进行空间-时间位置的自适应加权。帧间差异高的区域赋予更大惩罚，静稳区域保持常规定义，从而引导扩散模型更关注高频动态并稳定优化。

Result: 在通用基准VBench与运动聚焦的VMBench上取得一致增益：较强基线分别提升3.31%与3.58%，体现运动质量显著改进。

Conclusion: 利用LTD进行运动感知的损失加权可稳定训练并更好地重建复杂动态，相比静态统一损失的扩散模型在运动视频生成上有显著优势。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [43] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 提出PCG任务与数据集CHEESE及方法SCheese：通过自然语言编辑单张人像，生成多样且一致的人像合集；构建57.3万样本数据集并用层级身份与细节保持的生成框架取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 社媒用户需要便捷地批量生成多样且高质量的人像合集；现有方法难以同时处理多属性复杂编辑（姿态、布局、视角）和高保真细节保持（身份、服饰、配件）。

Method: 1) 构建CHEESE数据集：24K人像合集、573K样本，基于大视觉语言模型管线生成高质量文本修改标注，并用基于反演的验证保障一致性。2) 提出SCheese框架：文本引导生成+分层的身份与细节保持；采用自适应特征融合维持身份一致性；通过ConsistencyNet注入细粒度特征以保证细节一致。

Result: 在CHEESE上进行全面实验，证明该数据集推动了PCG研究；SCheese在PCG任务上取得了当前最优的性能（对比多个基线）。

Conclusion: PCG作为新任务可通过高质量数据与专门设计的层级一致性机制有效解决多属性编辑与细节保持难题；CHEESE与SCheese共同为生成一致且多样的人像合集提供了SOTA方案。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [44] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: dMLLM因扩散式解码慢而依赖缓存提速，但引入“重复诅咒”——易产生重复文本。作者从信息流角度剖析成因，并提出CoTA：在注意力与解码置信度上做调控，显著缓解重复且不伤整体性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态大模型推理延迟高，普遍使用缓存（KV cache等）加速。然而实际应用中出现明显的重复生成问题，影响可用性与质量。现有解释与缓解方法不足，需要从机制层面理解并提出简单可插拔的改进方案。

Method: 信息流分析：观察上下文token在层间的信息聚合与熵变化，识别重复与信息流中断、熵不收敛的关联；据此设计CoTA：1) 强化上下文token注意力，保持其信息流模式；2) 在解码时对置信度加入惩罚，抑制由不确定上下文驱动的输出。方法为即插即用，无需重训。

Result: 实验显示CoTA显著降低重复生成，在通用任务上获得稳定提升。作者报告广泛实验验证有效性。

Conclusion: 重复问题源于上下文信息流受扰与熵未在深层收敛。CoTA通过注意力增强与置信度惩罚双管齐下，缓解重复且提升整体表现，具有实用性与通用性。

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [45] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: 提出AnomalyVFM，将任意预训练视觉基础模型(VFM)转为强大的零样本异常检测器；通过三阶段合成数据生成与参数高效适配（低秩适配器+置信加权像素损），在9个数据集上以RADIO为骨干实现94.1%图像级AUROC，较SOTA提升3.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 纯视觉基础模型（如DINOv2）在零样本异常检测上落后于利用VLM（如CLIP）的方法，主要因辅助异常数据集多样性不足与对VFM的适配过浅，限制了知识迁移与泛化能力。

Method: 提出通用框架AnomalyVFM：1) 三阶段鲁棒合成数据生成流程，丰富异常形态与背景多样性；2) 参数高效适配机制，在预训练VFM上加入低秩特征适配器（LoRA式）并采用置信加权的像素级损函数进行微调；3) 以任意VFM作为骨干（示例用RADIO），实现零样本异常检测与定位。

Result: 在9个多样数据集上，使用RADIO骨干达到图像级AUROC 94.1%，比现有最佳方法提升3.3个百分点，并显著优于基于VFM的既有方案。

Conclusion: 通过丰富的合成数据和更深但参数高效的适配策略，可充分释放VFM在零样本异常检测中的潜力，超越依赖VLM的SOTA；框架通用、可移植性强。

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [46] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: 提出IOTA：将黑箱数据驱动与白箱知识驱动结合的提示学习框架，用于高效下游适配，显著优于现有PET方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法把预训练模型当作黑箱，仅依赖数据优化，忽视模型蕴含的先验与可解释知识，导致下游适配不充分。

Method: 设计黑白盒结合的提示学习框架IOTA。白盒模块通过对比错误预测与正确认知，抽取“纠错知识”，将其口语化为可解释的人类提示，并采用纠错知识引导的提示选择策略，引导黑盒模块做出更准确预测；黑盒模块仍进行数据驱动优化。两者联合学习，实现知识与数据的共同驱动。

Result: 在12个图像分类基准上、少样本与从易到难适配设置中，IOTA显著提升性能，超过当前SOTA，验证纠错知识与框架有效性。

Conclusion: 结合白盒（知识驱动）与黑盒（数据驱动）的提示学习能更好利用预训练模型先验，通过纠错知识引导选择提示，有效提升下游任务适配与准确率。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [47] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World是一款开源、以视频生成为核心的世界模型，强调高保真与稳健物理、分钟级长时一致性和亚秒级交互延迟，并面向内容创作、游戏与机器人学习等应用。


<details>
  <summary>Details</summary>
Motivation: 缩小开源与闭源世界模型/视频生成技术的差距，提供一个可公开使用、性能强的通用世界模拟器，满足长时一致性、跨风格环境与实时交互的需求。

Method: 构建基于视频生成的世界模拟框架，针对多风格环境训练以提升泛化和动态稳健性；优化长时间序列一致性的建模（“长程记忆”）以支持分钟级生成；工程上压低推理延迟，实现16 FPS下<1秒的交互响应；开源代码与模型。

Result: 模型在多种环境（真实、科学、卡通等）保持高保真与稳健动态；可在分钟级时间范围内保持上下文一致；实现实时交互，16帧/秒条件下端到端延迟<1秒；代码与模型公开可用。

Conclusion: LingBot-World作为高水平开源世界模型，综合兼顾画面保真、物理与语义一致性、长时记忆与实时交互，可用于内容生成、游戏与机器人学习等实际场景，有望推动开源社区发展。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [48] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: DeepSeek-OCR 2 提出一种可按语义动态重排视觉token的编码器（DeepEncoder V2），以更贴近人类在复杂版面中的因果与逻辑扫描方式，从而提升二维图像理解与OCR的效能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在送入LLM前采用固定光栅顺序与位置编码，不符合人类在复杂布局中基于语义与因果的灵活浏览，限制了二维推理与版面理解能力。

Method: 设计DeepEncoder V2：在进入LLM前，利用具因果推理能力的模块对视觉token进行智能重排；将二维理解转化为两级串联的一维因果推理结构，以探索“通过1D因果结构实现真实2D推理”的可行性。

Result: 实验表明，动态重排的策略在复杂布局和因果关系驱动的阅读场景中更有效，验证了二维理解可通过级联的一维因果结构取得良好表现（论文声称验证，具体指标未在摘要中给出）。代码与权重已开源。

Conclusion: 通过在编码阶段引入因果驱动的token重排，DeepSeek-OCR 2 为二维图像/版面理解提供了新范式，潜在实现更接近人类的2D推理流程，并对VLM架构设计给出新的方向。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [49] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 提出DiffVC-RT，实现实时扩散式感知型神经视频压缩，通过高效架构、时序一致性建模与异步并行解码，在HEVC上相较VTM-17.0以LPIPS指标节省80.1%码率，并在H800上达720p编码/解码206/30 fps。


<details>
  <summary>Details</summary>
Motivation: 扩散式NVC在实际部署中面临信息损失大、推理延迟高、时序一致性差（闪烁）的问题，需要一种既保感知质量又可实时落地的方案。

Method: 1) 高效且信息保真的模型：通过模块替换与剪枝，降低计算量同时减轻结构信息丢失；2) 显式+隐式一致性建模：在U-Net中加入零开销的在线Temporal Shift Module，并引入混合隐式一致性约束以抑制生成式闪烁；3) 异步并行解码管线：结合混合半精度，采用批维度Temporal Shift，实现潜空间解码异步化与帧级并行重建。

Result: 在HEVC数据集上，相比VTM-17.0按LPIPS指标实现80.1%码率节省；在NVIDIA H800上，720p视频可实时编码/解码达206/30 fps。

Conclusion: DiffVC-RT将扩散式视频压缩推进至可实时部署阶段，兼顾感知质量、时序稳定性与计算效率，为生成式视频编解码提供了可落地的系统化方案。

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [50] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出StructAlign用于持续文本-视频检索，通过简单xETF几何先验与两种损失（跨模态ETF对齐、跨模态关系保持）同时缓解跨模态与模态内特征漂移，显著降低灾难性遗忘并优于SOTA。


<details>
  <summary>Details</summary>
Motivation: CTVR需要在不断学习新类别时保持已学类别的文本-视频对齐，易发生灾难性遗忘。核心难点是特征漂移：模态内的连续学习引起漂移，以及跨模态的不协同漂移导致对齐失衡。现有方法难以同时稳住两种漂移并维持跨模态结构。

Method: 1) 以单纯形等角紧框架（simplex ETF）作为统一几何先验，构造类别级ETF原型；2) 设计跨模态ETF对齐损失，将文本与视频特征对齐到对应ETF原型，促使表示近似形成simplex ETF几何；3) 设计跨模态关系保持损失，利用另一模态的关系作为稳定监督，保持跨模态相似性结构，抑制模态内漂移；4) 联合优化两类损失以同时应对跨模态与模态内漂移。

Result: 在多个基准数据集上的持续检索实验中，StructAlign在指标上稳定超过现有SOTA方法，展示更少遗忘与更强的类增量泛化与对齐稳定性。

Conclusion: 将ETF几何先验与跨模态关系保持相结合，可在CTVR中有效缓解特征漂移与灾难性遗忘；结构化对齐为多模态持续学习提供了通用而稳健的途径。

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [51] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 论文评测了监督、自监督与语言对齐（基础模型）三种训练范式在跨域行人重识别上的鲁棒性。结论：监督模型域内最强但跨域崩溃；语言对齐模型（如SigLIP2家族）虽未针对ReID训练，却在跨域泛化上显著更稳健。


<details>
  <summary>Details</summary>
Motivation: ReID在实际落地常遇到域偏移（相机、场景、天气、时间等变化），现有SOTA多在单一数据集上报告高分，但跨域性能不明；同时基础模型以更丰富的视觉-语言表示被认为可迁移，需系统检验其在ReID中的价值与短板。

Method: 对比三种范式：监督（传统ReID模型）、自监督、语言对齐/基础模型（如SigLIP2）。在9个数据集上评测11个模型，关注域内与跨域设置下的识别性能与鲁棒性，分析泛化差异与失败模式。

Result: 形成明显分化：监督模型在训练域内表现最佳，但在跨域测试时性能大幅下降；语言对齐模型在未专门针对ReID训练的前提下，跨域鲁棒性出人意料地强；自监督未被强调为最优。

Conclusion: 传统监督ReID对域偏移敏感，难以泛化；基础模型的语言对齐表示有助于跨域ReID泛化。建议未来工作：结合基础模型的可迁移表征与ReID细粒度判别学习，改进跨域适应与数据高效训练；代码与数据已开放以促进可复现研究。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [52] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: 提出CLEAR-Mamba：在眼底荧光造影（FFA/ICGA）多疾病分类中，结合超网络自适应条件层（HaC）与基于证据不确定性的可靠性预测训练（RaP），并构建大规模造影数据集。相较多种基线（含MedMamba），在泛化与可靠性上均显著提升。


<details>
  <summary>Details</summary>
Motivation: 单模态限制、病灶细微、设备域差异大导致现有方法泛化差、置信度不可靠；眼科造影（FFA/ICGA）包含关键血流与病灶结构信息，需要兼顾跨域适应与预测可靠性的方法。

Method: 在MedMamba上增强：1) 结构上引入HaC（超网络驱动的自适应条件层），根据输入特征分布动态生成参数，提升跨域适应；2) 训练上提出RaP（可靠性感知预测），基于证据不确定性学习，聚焦低置信样本以提高稳定性与可靠性；3) 构建覆盖FFA与ICGA、多疾病的大规模数据集用于训练与评估。

Result: 在多项指标上持续优于多种基线（含原始MedMamba），在多疾病分类及可靠性相关评测中优势尤为明显。

Conclusion: CLEAR-Mamba在造影特定的医学图像分类中实现了泛化性与可靠性的平衡，提供了跨设备/域稳健与高可信预测的有效方案。

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [53] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: 提出GDCNet，用MLLM生成客观图像描述作为“语义锚”，计算与原文本的语义/情感差异及图文一致性，再通过门控融合多模态特征，实现更稳健的多模态讽刺检测，并在MMSD2.0上刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MSD方法多依赖跨模态嵌入对齐的“错位”来判讽刺，但当图文关系松散或间接时失效；用LLM生成讽刺线索会引入主观噪声。需要一种稳定、客观的参考来衡量跨模态冲突。

Method: 用多模态大模型生成客观、可核验的图像描述（caption）作为稳定语义锚；计算三类差异特征：1）描述与原文本的语义差异；2）描述与原文本的情感差异；3）视觉-文本的忠实度（图文一致性）。随后将这些差异特征与原始视觉/文本表示通过门控模块自适应融合，动态调节各模态贡献。

Result: 在多模态讽刺检测基准上进行大量实验，准确率与鲁棒性优于现有方法，在MMSD2.0基准上达到新的SOTA。

Conclusion: 通过以MLLM生成的客观描述为锚，GDCNet更可靠地刻画跨模态讽刺中的语义与情感不一致，缓解了松散配对与主观噪声问题，取得显著性能提升。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [54] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: 提出OS-Marathon基准，涵盖两个领域242个长周期、重复性任务，并给出一种基于少量示例的低成本“浓缩演示”方法，帮助CUA在大规模、未见数据上执行相似工作流；实验显示任务困难且方法有效。


<details>
  <summary>Details</summary>
Motivation: 专业场景中大量长周期、重复性的人机界面工作（如报销单据处理、成绩录入）对人类乏味冗长，但非常适合计算机使用代理（CUA）。然而缺少系统基准阻碍了方法比较与进步，因此需要一个评测套件与可泛化的教学方式。

Method: 1) 构建OS-Marathon：覆盖2个领域、共242个长流程重复任务，用于评测SOTA代理。2) 提出“浓缩演示”策略：用极少的few-shot示例提炼并传授工作流逻辑，使代理能够在更大、未见的数据集合上复用该逻辑。3) 大规模实验评测不同代理与所提方法。

Result: 实验表明：这些长周期重复任务对现有代理具有显著挑战；所提基于少量示例的浓缩演示能有效提升代理在大规模、未见数据上的执行能力，同时具备成本优势。

Conclusion: OS-Marathon为长流程重复任务提供了首个或关键评测标准之一；少样本的浓缩演示是一种高性价比的教学与泛化方案，可显著增强CUA在真实大规模场景中的可用性。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [55] [FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection](https://arxiv.org/abs/2601.20656)
*Diogo J. Paulo,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 提出一种区域感知的频域残差方法，用于单样本人脸变形攻击检测（S-MAD），在跨数据集/跨变形方法上显著优于强基线，且模型轻量。


<details>
  <summary>Details</summary>
Motivation: S-MAD场景缺少可信参考，现有方法在跨数据集泛化差，易过拟合数据集偏差。需要一种对不同数据库与变形技术具有稳健性的检测策略。作者观察到不同面部区域在频域上真样本与变形样本可分离。

Method: (1) 在傅里叶域引入“残差频域”：将图像频谱与自然谱能量衰减规律解耦，得到对变形伪迹更敏感的残差信号；(2) 区域化处理：提取多面部区域（全局与局部，如眼、鼻、口等）的频域谱特征；(3) 以马尔可夫随机场（MRF）融合区域证据，进行结构化、一致性的全局判决；(4) 使用轻量的光谱特征与分类器，训练仅基于SMDD合成数据集。

Result: 在跨数据集评测中：FRLL-Morph平均EER达1.85%；MAD22平均EER为6.12%（排名第二）；同时在低APCER下保持较好的BPCER，仅用光谱特征即可取得竞争性能。

Conclusion: 频域残差建模结合结构化的区域级证据融合，为S-MAD提供了强泛化、低复杂度的替代方案，相比深度架构在跨数据集/跨变形设置中更具竞争力。

Abstract: Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

</details>


### [56] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 提出ProSkill数据集：面向流程性任务的动作级技能评估，结合瑞士轮配对标注与ELO聚合得到连续绝对分数；对现有方法基准测试，结果不佳表明任务具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有技能评估研究多集中体育场景、数据规模小、动作类别少，且多为二元或成对比较，缺乏复杂流程任务的动作级“绝对”技能评估数据。

Method: 提出可扩展标注协议：采用瑞士轮进行高效成对比较，再以ELO评分系统将配对结果聚合为一致的连续全局分数，从而获得绝对排名与成对标注；构建ProSkill数据集并提供基准评测。

Result: 用ProSkill对当前主流（排序式与成对式）技能评估算法进行基准测试，表现均不理想，显示现有方法难以处理该任务。

Conclusion: ProSkill为流程性视频的动作级技能评估提供首个含绝对分数的基准与标注协议，揭示任务挑战并为未来方法改进提供数据与评测平台。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [57] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: 提出BiMoRS：一种为遥感任务定制的轻量级双模态提示学习框架，利用冻结的图像字幕模型生成文本语义，并与CLIP视觉特征融合，通过跨注意力生成上下文化提示，在多个DG任务与数据集上平均提升约2%。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的VLM适配方法多在自然图像上有效，但在遥感场景中面临多标签、高类内差异、分辨率多样等挑战，难以抓住主导语义并泛化到新类别，亟需面向RS特性的提示学习方案。

Method: 冻结的图像字幕模型（如BLIP-2）从RS图像生成文本摘要；用BERT分词，将其与CLIP编码器的高层视觉特征融合；设计轻量级跨注意力模块，以融合的文-视表示来调制一个可学习的查询提示，生成上下文化提示；保持CLIP骨干冻结不变进行下游适配。

Result: 在4个遥感数据集、3种域泛化任务上，BiMoRS相对强基线取得稳定提升，平均最高约+2%。

Conclusion: 利用外部字幕模型提供的文本语义与视觉特征跨模态融合，可在不改动CLIP骨干的前提下，生成对RS更友好的上下文提示，从而提升遥感场景的泛化表现。

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [58] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出LEAF：从MLLM教师蒸馏质量感知先验到轻量学生回归器，用极少MOS标注完成标定，显著降低IQA标注成本且保持与MOS的高相关。


<details>
  <summary>Details</summary>
Motivation: MLLM在IQA上感知能力强，但微调成本高且仍依赖大量MOS标注。瓶颈在于将感知质量映射到MOS标度（标定），而非感知本身，需要一种高效的标定方案以减少人工标注。

Method: 构建教师-学生框架：以MLLM为教师，通过点对（像素/位置）判断与成对（图像对）偏好提供稠密监督，并估计决策可靠性；学生为轻量回归器，通过联合蒸馏学习教师的质量感知模式；再用少量带MOS的样本对学生进行标定，使其输出与人类MOS对齐。

Result: 在用户生成与AI生成的IQA基准上，使用很少的MOS标注即可获得与人类MOS高度相关的结果，性能接近或优于需要大量标注的方案，实现显著的标注成本下降。

Conclusion: 通过从MLLM蒸馏质量先验并在少量MOS上校准，LEAF能以低标注成本实现轻量IQA且保持MOS一致性，为资源受限场景下的IQA提供可行方案。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [59] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: LEMON 是一个面向长时程 STEM 课堂视频的多模态理解评测基准，覆盖多任务/多子任务，强调跨模态、时间结构与教学语境，实验证明当前顶尖 MLLMs 在时间推理与教学预测上仍有明显短板。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在通用视觉/音频/语言任务上进展显著，但对长时、知识密集且具有时间与教学结构的教育内容理解不足，现有视频基准难以全面衡量这类能力。

Method: 构建 LEMON 基准：收集5学科29门课程的2,277个讲座视频片段（平均196.1秒），标注4,181条高质量问答（3,413选择题、768开放题）；设计六大任务与十二子任务，覆盖从感知、推理到生成；问题设置强调多模态紧耦合、显式时间与教学结构、以及多轮上下文关联。

Result: 在 LEMON 上开展广泛实验，发现多个任务上存在性能缺口；包括 GPT-4o 在内的先进 MLLMs 在时间性推理与教学步骤预测等方面表现不佳。

Conclusion: LEMON 提供了一个可扩展且具挑战性的长时教学视频多模态评测基准，有望推动模型在长程多模态感知、推理与生成方面的研究进展。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [60] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: 提出Li-ViP3D++：一种在查询空间中融合相机与激光雷达的端到端感知与预测框架，通过可微的Query-Gated Deformable Fusion实现跨模态信息整合，联合优化检测、跟踪与多假设轨迹预测，在nuScenes上提升EPA与mAP、显著降低FP并更快。


<details>
  <summary>Details</summary>
Motivation: 现有端到端PnP虽缓解模块化流水线的信息割裂与误差放大，但对相机与LiDAR在查询空间的互补性利用不足；主流融合依赖启发式对齐与离散选择，限制信息利用并引入偏差，缺乏全可微、无启发式的融合机制。

Method: 构建查询式多模态PnP框架Li-ViP3D++，核心为Query-Gated Deformable Fusion（QGDF）：(i) 通过masked attention在多相机、多尺度特征上汇聚图像证据；(ii) 以学习到的每查询偏移在BEV上进行全可微的LiDAR采样获取几何上下文；(iii) 采用查询条件化门控，自适应赋权视觉与几何线索。模型端到端联合优化检测、跟踪和多假设轨迹预测。

Result: 在nuScenes上，端到端行为与检测质量提升：EPA 0.335、mAP 0.502；误检显著降低（FP ratio 0.147）；推理更快（139.82ms vs. 145.91ms，相比先前Li-ViP3D）。

Conclusion: 在查询空间进行全可微的相机-LiDAR融合能在不牺牲可部署性的前提下提升端到端PnP的鲁棒性与性能；QGDF实现了对视觉与几何信息的细粒度、自适应融合，带来更好的检测、跟踪与预测表现。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [61] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 论文综述并统一视角：将传统视觉编码与新兴视觉Token技术从优化与压缩效率的角度桥接，阐明性能-成本权衡，给出双向启发与未来标准化路线，并在MLLM、AIGC、具身智能等任务上展示任务导向Token的潜力。


<details>
  <summary>Details</summary>
Motivation: 在多模态大模型时代，模型能力与压缩效率呈相关；传统视觉编码与生成式多模态模型中的视觉Token学习目标相近，但两者社区割裂，缺少统一理论框架来理解语义保真与计算/比特开销的权衡以及对下游智能任务的影响。

Method: 1) 系统性综述两大技术族：传统视觉编码（基于信息论的图像/视频编解码标准）与视觉Token技术（生成式多模态模型中的表示/离散化/量化方案）；2) 从优化视角提出统一表述，将“最大语义信息保真、最小计算与码率/Token开销”纳入同一目标；3) 基于该统一框架提炼双向启发，预测下一代编解码器与Token方法；4) 通过实验在MLLM、AIGC、具身AI等任务验证任务导向Token的价值。

Result: 实证表明任务导向的视觉Token在实际智能任务中具有显著潜力；统一视角揭示了压缩效率与模型性能之间可量化的权衡关系，并给出改进方向。

Conclusion: 视觉编码与视觉Token可在统一优化框架下融合：以语义保真为核心、受限于码率/Token与计算成本。面向任务的Token设计有望推动下一代高效视觉编解码与多模态系统，并为类似H.264/265的通用Token标准化奠定基础。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [62] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: FairT2V 是一个无需训练的文本到视频去偏框架，通过几何方式中和文本编码产生的性别倾向，并在早期去噪步骤动态施用，显著降低职业场景中的性别偏见，同时基本不损伤视频质量。


<details>
  <summary>Details</summary>
Motivation: T2V 扩散模型快速发展，但其人口统计学偏见（尤其性别偏见）鲜少被系统研究。作者发现偏见主要来自预训练文本编码器：即便是中性提示词也被编码出隐式性别联想，需要一种不依赖微调、可直接用于现有模型的去偏手段。

Method: 1) 提出性别倾向分数以量化文本编码对性别的偏置，并与生成视频的偏见相关联；2) 基于锚点的球面测地变换对提示词嵌入进行“中和”，在保持语义的同时消减性别方向；3) 为保持时间一致性，仅在早期“身份成形”的去噪阶段动态施用去偏；4) 设计结合 VideoLLM 推理与人工校验的视频层面公平性评估协议。

Result: 在 Open-Sora 等现代 T2V 模型上，FairT2V 在多种职业场景中显著降低性别偏见，且对视频质量影响很小。

Conclusion: T2V 模型的性别偏见主要源自文本编码器。通过对提示嵌入进行几何中和并在早期去噪阶段动态应用，可在无需微调的情况下有效减偏并保持生成质量；该方法还配套了可操作的视频公平性评估方案。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [63] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: FunHSI 是一个无需训练、以功能为导向的框架，从开放词汇任务指令生成与场景功能元素正确交互的3D人体，并通过接触推理、图像生成与3D姿态估计以及分阶段优化，确保物理可行与功能正确。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对场景中物体功能与人—物接触关系的显式推理，导致生成的人—场景交互不合理或功能错误；需要一个能从任务语义出发、实现功能感知交互的通用方法。

Method: 提出 FunHSI：1) 基于任务指令进行功能感知的接触推理，定位功能元素、重建其3D几何，并以接触图建模高层交互；2) 利用视觉-语言模型合成执行任务的人体图像并估计初始3D身体与手部姿态；3) 通过分阶段优化细化3D人体配置，保证物理合理与功能正确。方法为训练自由（training-free）。

Result: 在多样的室内外场景中，FunHSI 相较现有方法生成更合理的通用交互（如坐沙发），还能处理细粒度、功能明确的交互（如提高室温），整体生成的交互在功能正确性与物理可行性上均表现更优。

Conclusion: 功能驱动、无需训练的 FunHSI 能从开放任务指令生成功能正确、物理可行的人—场景3D交互，优于现有方法并具备更强的泛化与细粒度交互能力。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [64] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 提出融合相机与IMU的轻量双向跨模态注意力与自适应门控的RSC方法，并发布涵盖真实、视觉-only与合成三部分的ROAD数据集；在PVS与ROAD上显著提升，尤其提升少数类与极端环境鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有道路路面分类方法感知模态单一、数据集环境多样性不足，导致跨域泛化弱，难以在不同光照/天气/表面条件下稳定工作。

Method: 构建多模态框架，将RGB图像与IMU信号通过轻量级双向跨注意力进行特征交互，再用自适应门控在域移条件下动态调节各模态贡献；同时引入ROAD数据集：1) 真实多模态RGB-IMU同步数据，涵盖多光照、天气、路面；2) 大规模仅视觉子集，检验不良光照与异构采集鲁棒性；3) 合成子集，用于研究难以实际采集的OOD场景。

Result: +1.4个百分点优于SOTA（PVS基准），在ROAD多模态子集上+11.6个百分点；在少数类上F1更高；夜间、暴雨、混合路面过渡等困难条件下表现稳定。

Conclusion: 廉价相机+IMU结合多模态注意与门控能在多变环境下提供可扩展、鲁棒的路面理解能力，适用于受成本与环境限制的地区，推动面向维护的环境感知系统落地。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [65] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix提出一种无需微调的生成监督框架，用预训练图像扩散模型在2D-3D间交替细化NeRF/3DGS渲染，改善外插视角与多帧一致性，达到或超过需微调方法，同时保持泛化。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF/3DGS在输入稠密度不足和外插视角时退化。引入扩散模型可提供先验，但存在“泛化-保真”权衡：微调可去伪影但易过拟合，不微调泛化好但保真度不足。需要一种既不微调又能提升外插质量与一致性的方法。

Method: 提出FreeFix：1) 交替的2D-3D细化策略——在3D重建与2D图像扩散引导之间循环，利用预训练图像扩散模型而非昂贵的视频扩散模型实现跨视角一致的改进；2) 在2D阶段引入逐像素置信度掩膜，定位不确定区域进行定向修复；3) 以生成监督增强外插渲染并维持多帧一致性。

Result: 在多数据集上，FreeFix提升了多帧一致性与外插视角质量，其性能可与或优于需微调扩散模型的方法，同时保持强泛化能力。

Conclusion: 无需微调的FreeFix通过2D-3D交替细化与置信度掩膜有效缓解泛化-保真权衡，提升外插新视角与一致性，成为较具实用性的生成监督方案。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>
