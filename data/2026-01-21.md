<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 266]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study](https://arxiv.org/abs/2601.11612)
*Arnav S. Sonavane*

Main category: cs.CV

TL;DR: 在农作物病害分类任务中，用仅约3000张无标注农业图像进行SimCLR领域自监督预训练，比单纯更换为分层Transformer结构带来更大收益：准确率平均提升约+4.57%，而架构带来的提升为+3.70%。该SSL增益对架构无关：在Swin-Base与ViT-Base上分别带来+4.08%与+4.20%的提升。提出的HierarchicalViT（HVT）在三数据集上取得强性能，并具备更好校准（ECE 3.56%，温标后1.52%）。


<details>
  <summary>Details</summary>
Motivation: 农业病害识别常受标注数据稀缺与分布偏移影响，通用预训练难以充分适配领域特征。作者希望验证：小规模但同域的无标注数据能否通过自监督显著提升下游分类，且该收益是否独立于具体Transformer架构设计。

Method: 1) 构建Swin风格的层级Transformer（HVT）；2) 使用仅约3000张无标注农业图像进行SimCLR自监督预训练，再在有标注病害数据上微调；3) 在相同参数规模下，与Swin-Base、ViT-Base等进行对比；4) 在三套数据集（Cotton Leaf Disease、PlantVillage、PlantDoc）上评测并做不确定性校准分析（ECE与温度缩放）。

Result: - 领域SSL预训练带来更大收益：平均+4.57%，超过层级架构带来的+3.70%。在Swin-Base与ViT-Base上也分别获得+4.08%与+4.20%，显示架构无关性。- HVT-Base（78M）优于Swin-Base（88M）：88.91% vs 87.23%（+1.68%）。- 数据集成绩：棉叶病害（7类）90.24%，PlantVillage（38类）96.3%，PlantDoc（27类）87.1%。- 校准：ECE 3.56%，温度缩放后1.52%。

Conclusion: 在农业病害分类中，优先收集同域无标注数据并进行自监督预训练，比单纯更换为更复杂的层级Transformer架构更有效且对架构普适。HVT在相同参数量下略优于Swin，并在预测校准上表现良好，适合面向部署的可靠应用。

Abstract: We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT

</details>


### [2] [Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning](https://arxiv.org/abs/2601.11614)
*Jason Qiu*

Main category: cs.CV

TL;DR: 提出用3D TransUNet从T1w MRI合成dMRI的FA/MD图，实现早期微结构信息的替代获取，提升AD/MCI分类性能，同时减少扫描时间。


<details>
  <summary>Details</summary>
Motivation: AD早期微结构变化早于结构性改变；dMRI能捕捉但耗时且易运动伪影，临床难以常规获取。需要从常规T1w中推断dMRI信息，保留多模态优势、提高可及性。

Method: 构建3D TransUNet图像合成框架，端到端由T1w输入预测FA与MD图；评估与真实dMRI的一致性（SSIM、皮尔逊相关），并将合成特征融入多模态诊断模型评估分类性能。

Result: 合成FA/MD与真值高度一致：SSIM>0.93、相关>0.94；在诊断模型中加入合成特征后，AD分类准确率由78.75%提升到83.75%（+5%），MCI检测提高12.5%。

Conclusion: 从常规T1w可靠推断高质量扩散微结构指标，可在无dMRI场景下转移多模态优势，减少扫描时间并提升AD/MCI诊断可及性与准确性，具备临床转化潜力。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.

</details>


### [3] [PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM](https://arxiv.org/abs/2601.11617)
*Xu Wang,Boyao Han,Xiaojun Chen,Ying Liu,Ruihui Li*

Main category: cs.CV

TL;DR: 提出PointSLAM++：以分层约束的神经高斯表示+渐进式位姿优化+动态神经表示图，实现在深度噪声下结构一致、定位稳健、可真实感渲染的实时RGB-D SLAM，并在精度与质量上优于现有3DGS类方法。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM在深度噪声与复杂场景下易破坏结构一致性、位姿估计不稳，难以兼顾高精度重建与真实感渲染，尤其在大规模AR与机器人应用中更为突出。

Method: 1) 分层约束的神经高斯表示：以层级结构编码与约束高斯原语间的结构关系，提升地图结构一致性；2) 渐进式位姿优化：逐步引入深度与几何/外观约束以抑制传感器噪声，提升定位鲁棒性与精度；3) 动态神经表示图：依据局部几何复杂度自适应调整高斯节点分布，实时细化复杂区域并保持整体高效；4) 基于RGB-D输入的在线建图与渲染。

Result: 在多组实验中，相比现有基于3D Gaussian Splatting的SLAM，PointSLAM++在重建精度与渲染质量上显著提升，并提供更稳健的位姿估计。

Conclusion: 通过结构感知的神经高斯表示、抗噪位姿优化与自适应节点调度，PointSLAM++实现高精度、真实感、实时的RGB-D SLAM，适用于大规模AR与机器人场景。

Abstract: Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.

</details>


### [4] [Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings](https://arxiv.org/abs/2601.11627)
*Hassan Ugail,Jan Ritch-Frel,Irina Matuzava*

Main category: cs.CV

TL;DR: 该论文提出一种用于历史素描真伪鉴定的验证式框架，使用仅正类（一类）自编码器和少量可解释手工特征，对多位艺术家的作品进行生物识别风格的验证评测，在数据稀缺场景下提供可复现的量化证据，整体在选定阈值下达到83.3%真接受率与9.5%误接受率，并揭示不同艺术家间的可混淆性结构。


<details>
  <summary>Details</summary>
Motivation: 纸上作品的鉴定与归属在可用参照样本少、风格主要体现在线条与有限色调时尤其困难。传统鉴定依赖鉴赏家经验，缺乏可复验的量化依据，且历史素描的数字化差异与艺术家风格近邻会导致混淆。作者希望在数据稀缺条件下提供一种可解释、可量化、可与鉴赏结合的计算方法。

Method: 提出验证式（verification-based）管线：为每位艺术家训练一个一类自编码器（只用真品训练），输入为紧凑的手工特征向量而非原始图像。特征包括：傅里叶域能量、香农熵、全局对比度、GLCM同质性、盒计数分形复杂度。数据来自多家博物馆开源与目录。按生物识别协议构建“真（genuine）”“伪（impostor）”试验，通过重构误差/相似度与阈值做接受/拒绝决策，并进行阈值设定与跨艺术家错误路径分析。

Result: 在900次验证（90真、810伪）上，整体在所选工作点取得TAR=83.3%、FAR=9.5%。不同艺术家差异显著：一些验证器几乎零误受；另一些对风格近似者混淆较高。对误受进行成对分析显示结构化的错误路径，与风格相近与共享绘画习惯一致，同时也暴露了数字化伪影与阈值设定对性能的影响。

Conclusion: 该方法在数据稀缺的历史素描鉴定中可作为鉴赏（connoisseurship）的量化补充而非替代，能提供可复现的证据；需关注按艺术家定制阈值、控制数字化差异，并进一步研究风格相近导致的系统性混淆。

Abstract: Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.

</details>


### [5] [A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow](https://arxiv.org/abs/2601.11630)
*Haonan Wei,Linyuan Wang,Nuolin Sun,Zhizhong Zheng,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: 提出SLT：把FreeFlow的28层DiT教师蒸馏为单层共享Transformer，在保持一步生成范式下极大压缩参数并提升平均生成质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 一步生成(Flow matching/ODE)速度快但对初始噪声敏感，且像FreeFlow使用深层(28层)Transformer，参数大、成本高；需要在不牺牲质量下进一步压缩、并缓解有限采样下因劣质初始噪声导致的质量波动。

Method: 将FreeFlow的深度方向视作ODE时间，28层Transformer对应欧拉离散步。提出单层共享DiT块(SLT)近似深度演化：以若干“深度patch”匹配教师的中间特征、融合这些patch表示，并同时对齐教师最终速度预测。通过蒸馏把DiT-XL/2的28个独立块压缩为1个共享块(4.3M参数)。利用其极快采样，对噪声空间进行>100次候选筛选，挑选高质量起点再交由教师FreeFlow生成。

Result: 参数从675M降至4.3M；在与教师两次随机采样等价的时间预算内，可完成>100次噪声筛选并用最佳点通过教师生成高质量样本；显著降低由差初始噪声引起的质量波动，提高一步生成的稳定性与平均质量。

Conclusion: 将FreeFlow的深度离散化解释用于结构蒸馏，成功用单层共享Transformer近似28层演化，并通过快速噪声筛选提升一步生成性能与可靠性，为高效高质的单步生成提供了实用范式。

Abstract: Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.

</details>


### [6] [Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents](https://arxiv.org/abs/2601.11631)
*Yurun Song,Jiong Yin,Rongjunchen Zhang,Ian G. Harris*

Main category: cs.CV

TL;DR: 提出CCPO，将坐标感知的视觉压缩与策略优化联合，缓解多轮GUI智能体的历史膨胀，兼顾效率与性能，在多基准上达SOTA并显著提速与降Token。


<details>
  <summary>Details</summary>
Motivation: 多轮GUI交互需要长历史上下文，但累积的视觉与行为轨迹导致上下文爆炸；现有截断丢失长期信息、Token剪枝破坏空间结构，导致决策与定位变差，亟需既保空间结构又控上下文长度的高效方法。

Method: CCPO：在策略优化框架中引入CASC（Coordinate-Aware Spatial Compression），跨多次rollout聚合坐标信号，构造自适应的注意力边界，逐步聚焦在与目标相关的关键视觉区域，从而对历史视觉信息进行空间压缩；同时设计基于距离的优势函数（Distance-Based Advantage），以连续距离反馈替代二元正确性，提供更细粒度的学习信号，提升目标落点的对齐与压缩质量。

Result: 在四个基准上达到SOTA，最多实现约55%的token压缩与3.8倍训练加速，同时保持或提升任务完成率与 grounding 准确度。

Conclusion: 将坐标感知的空间压缩与策略优化深度耦合能有效抑制上下文膨胀并提升多轮GUI智能体的效率与精度；基于距离的优势设计进一步改善学习信号，使得在显著压缩的前提下仍能取得更好的性能与更快训练。

Abstract: Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\times$ training speedup.

</details>


### [7] [KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering](https://arxiv.org/abs/2601.11632)
*Zhiyang Li,Ao Ke,Yukun Cao,Xike Xie*

Main category: cs.CV

TL;DR: 提出KG-ViP：将场景图与常识图统一检索融合，提升多模态VQA在知识幻觉与细粒度感知上的能力，并在FVQA 2.0+与MVQA上显著领先。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在VQA中常出现两大问题：对外部世界知识不足导致的知识幻觉，以及视觉细节理解不够；而常识图能提供知识，场景图能刻画细粒度视觉，但以往方法将二者割裂使用，未发挥协同作用。

Method: 提出统一框架KG-ViP：以查询为语义桥，通过新颖的“检索-融合”流水线，先从常识图与场景图中分别检索相关子图，再逐步对齐并融合为统一的结构化上下文，供MLLM进行可靠的多模态推理。

Result: 在FVQA 2.0+与MVQA基准上显著超越现有VQA方法，显示出更强的知识利用与细粒度视觉理解能力。

Conclusion: 融合场景图与常识图能有效缓解MLLM在VQA中的知识幻觉与细粒度感知不足问题；KG-ViP证明了两类图的互补与协同价值，并提供通用的检索融合范式。

Abstract: Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.

</details>


### [8] [Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images](https://arxiv.org/abs/2601.11633)
*Xuchen Li,Xuzhao Li,Renjie Pi,Shiyu Hu,Jian Zhao,Jiahui Gao*

Main category: cs.CV

TL;DR: 提出ViEBench，一个可验证过程的视觉推理基准，用高分辨率图像与证据标注评估VLM是否真正利用细粒度线索进行多步推理，并以双轴难度矩阵与四象限诊断指标全面分析模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有VLM评测多以最终答案准确率为导向，无法判断模型推理是否忠实依赖图像中的细粒度证据，尤其在多步骤、需结合先验知识与局部细节的任务中，缺乏过程可验证与可诊断的评估。

Method: 构建ViEBench：包含200张多场景高分辨率图像，配有专家标注的可定位视觉证据；根据感知与推理两条难度轴对任务进行分层；提出“双轴矩阵+四象限诊断”评测框架，细粒度度量模型在不同复杂度下的证据定位与推理正确性，并对模型行为进行透明诊断。

Result: 实验发现：1) 模型有时在错误或不相关的区域进行对齐仍能给出正确答案；2) 也可能准确定位正确证据却无法用其得出正确结论。由此展示现有VLM在证据利用与推理一致性上的缺口。

Conclusion: ViEBench提供了更可解释、可操作的评测途径，能够系统检验与诊断VLM在真实视觉证据上的忠实推理能力，并作为实践性基准推动具代理能力的VLM发展。

Abstract: Despite the remarkable progress of Vision-Language Models (VLMs) in adopting "Thinking-with-Images" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.

</details>


### [9] [When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms](https://arxiv.org/abs/2601.11634)
*Chenghui Yu,Hongwei Wang,Junwen Chen,Zixuan Wang,Bingfeng Deng,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: 提出一种基于多模态大模型代理的自动“新问题”发现与策略更新方法，在短视频平台上显著提升新兴内容问题的发现效率与治理效果。


<details>
  <summary>Details</summary>
Motivation: 短视频趋势变化快，新的内容风险不断出现，既有标注/审核政策覆盖不到，人工发现更新滞后，导致治理失效与成本高。需要自动、快速地发现并纳入策略。

Method: 构建多模态LLM代理：1）自动召回可能包含新问题的短视频；2）两阶段聚类将召回样本按语义/视觉等特征分组，每个簇代表一个潜在新问题；3）代理基于各簇生成并扩展相应的标注政策；4）在真实系统中部署并进行离线与在线评估。

Result: 与人工或传统方法相比，新问题发现F1提升>20%，后续治理效果提升，问题视频观看量约下降15%，同时显著降低时间成本、加速政策迭代。

Conclusion: 多模态LLM代理能自动发现并抽象新兴问题，生成可执行的策略，实证显示可在生产环境提升治理效率与效果，优于纯人工流程。

Abstract: Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.

</details>


### [10] [Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos](https://arxiv.org/abs/2601.11635)
*Anil Egin,Andrea Tangherloni,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 提出Anon-NET：用扩散模型+属性引导与表情迁移进行人脸视频去标识，同时保持年龄/性别/种族/姿态/表情与时空一致性；实验证明在多数据集上有效。


<details>
  <summary>Details</summary>
Motivation: 视频分析任务需要隐私保护的人脸去标识，同时尽量保留下游任务所需语义（表情、姿态等）与时间一致性。现有方法在身份去除、保真度、属性保持和时序稳定之间难以兼顾。

Method: 统一两阶段框架：1）基于扩散模型的人脸修复/重绘，通过高层属性识别（年龄、性别、种族、姿态、表情）进行条件引导，并结合“motion-aware”的表情迁移以脱敏且保持动态表情；2）视频驱动的动画模块，将去标识的人脸与原视频结合，实现时序一致的驱动渲染。

Result: 在VoxCeleb2、CelebV-HQ、HDTF上做了大量实验，显示能有效模糊身份，同时维持视觉真实感与时间一致性；保留年龄、性别、种族、姿态、表情等属性。

Conclusion: Anon-NET在人脸视频匿名化上实现身份去除与属性/时序保持的平衡，具备较强的视觉质量和通用性，代码将开源。

Abstract: Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.

</details>


### [11] [Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics](https://arxiv.org/abs/2601.11637)
*Aradhya Dixit*

Main category: cs.CV

TL;DR: 提出一个用于评估视觉-语言代理（VLA）自我纠错能力的诊断型微基准，发现任务成功与纠错成功脱钩，纠错收益在三次后饱和，语义漂移是主要失败源。


<details>
  <summary>Details</summary>
Motivation: 现有VLA能把复杂视觉任务拆解为工具计划，但对其迭代自纠的上限、瓶颈与误差类型缺乏细粒度量化。需要一个可复现实验框架来诊断并推动“有状态、可信”的多模态代理。

Method: 构建诊断型微基准，独立度量任务成功率（TSR）与纠错成功率（CSR），并在多次重试条件下统计纠错收益曲线；建立失败分类法（含语义漂移等），以分离并量化主导推理瓶颈。

Result: 在评测中，TSR≈62%，而CSR仅≈25–33%，表明初始能力与修复能力不相关；纠错收益在约三次重试后趋于饱和；失败案例中约28%归因于语义漂移（上下文状态丢失）。

Conclusion: 自纠并非万能，且存在明显的边际递减与状态管理瓶颈。该微基准提供可复现的诊断框架，有助于面向有状态与可信的多模态代理的研究与改进（尤其是缓解语义漂移）。

Abstract: Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR = 62 percent) from Correction Success Rate (CSR = 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.

</details>


### [12] [Confident Learning for Object Detection under Model Constraints](https://arxiv.org/abs/2601.11640)
*Yingda Yu,Jiaqi Xuan,Shuhui Shi,Xuanyu Teng,Shuyang Xu,Guanchao Tong*

Main category: cs.CV

TL;DR: 提出MDDC数据驱动纠错框架，在固定轻量模型和边缘设备约束下，通过迭代诊断并修复数据问题显著提升杂草检测mAP。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的杂草检测受模型容量、算力与实时性限制，无法通过加大模型或集成提升性能，亟需不改模型而提升检测效果的途径。

Method: 构建“模型驱动的数据校正”流程：利用自动化错误分析将检测失败分为假阴性、假阳性、类别混淆和定位误差；针对各类错误制定数据修复策略，并采用版本化数据管理执行“训练-修复-再训练”的迭代管线；始终使用固定的轻量检测器（如YOLOv8n）评估改进。

Result: 在多个杂草检测数据集上，基于固定YOLOv8n模型，mAP@0.5稳定提升约5–25个百分点。

Conclusion: 系统性的数据质量优化可在不增加模型容量的情况下显著缓解性能瓶颈，适合资源受限的边缘场景。

Abstract: Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.

</details>


### [13] [Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers](https://arxiv.org/abs/2601.11641)
*Yuxi Liu,Yipeng Hu,Zekun Zhang,Kunze Jiang,Kun Yuan*

Main category: cs.CV

TL;DR: 提出MOD-DiT：一种无需采样的动态稀疏注意力框架，为视频扩散Transformer加速并提升质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成需长序列建模，DiT的自注意力复杂度为二次方，导致训练/推理成本高。现有稀疏注意力要么使用静态、过于简化的模式损失精度，要么依赖代价高的采样实现动态稀疏，带来模式预测不准与生成质量下降。

Method: 两阶段、无采样的动态注意力：1) 从早期去噪步骤的先验出发，采用“分布式混合（mixture-of-distribution）”建模得到线性近似模型，用于在给定去噪区间预测注意力掩码模式；2) 在线块级掩码（online block masking）在推理中按区间动态应用上述预测掩码，并保留历史稀疏信息，无需重复采样。

Result: 在多项基准与不同模型架构上实现稳定加速与画质提升，相较传统稀疏注意力更准确地刻画随时间演化的注意模式。

Conclusion: MOD-DiT有效突破自注意力二次复杂度在视频生成中的瓶颈，以采样-free的动态稀疏注意力实现高效且高质量的视频生成。

Abstract: While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution \textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.

</details>


### [14] [PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models](https://arxiv.org/abs/2601.11642)
*Abbas Alzubaidi,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: 提出一个物理驱动的合成仿真框架（PSSF），从参数化膝关节解剖模型生成可控的X线片，用于训练/评估放射组学与ML模型在OA（KL 0与2）分级上的性能与稳健性，并检验多协议与采集变化下的特征稳定性。


<details>
  <summary>Details</summary>
Motivation: 真实膝关节X线数据受隐私、治理与资源限制，难以获取且标注成本高；现有OA评估依赖主观KL分级。需要一种不涉患者隐私、可控且可扩展的数据来源来支持量化评估与算法稳健性研究。

Method: 构建2D X线投影模拟器，从可参数化的股骨远端与胫骨近端模型合成前后位膝关节影像。生成180名虚拟受试者（260膝），在三种采集协议（基准、低剂量、几何位移）下成像；自动定位内侧关节区，按IBSI流程预处理并提取放射组学特征。使用LR、RF、GB三种模型，进行二分类（KL样0 vs 2）与三分类（0–2）训练与评估；在IBSI内协议、跨协议与多协议场景下评估鲁棒性；以ICC评估采集变化下特征稳定性。

Result: PSSF成功生成可控的膝关节X线数据集并支持放射组学与ML训练；模型在二分类和三分类任务上实现可观性能，并在多协议设定下保持一定鲁棒性；部分放射组学特征在不同采集条件下表现出较高ICC（稳定），也有特征对剂量或几何变化敏感。

Conclusion: 物理驱动的合成X线仿真可在不牵涉患者隐私的前提下为OA量化评估提供可控数据来源，支持对模型跨协议稳健性与特征稳定性研究；为开发与验证更泛化的OA AI工具铺路，并可扩展到更多采集配置与更丰富的解剖/病理变化。

Abstract: Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like "0" vs. "2") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.

</details>


### [15] [Predicting When to Trust Vision-Language Models for Spatial Reasoning](https://arxiv.org/abs/2601.11644)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: 论文提出一种基于视觉的置信度估计框架，用目标检测与几何一致性验证VLM的空间关系预测，显著提升不确定性评估与选择性预测表现，优于仅靠模型自评的方法。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态任务强大，但在基础空间关系（如方向、相对位置）上常出错，且现有依赖文本自评的置信度估计不可靠。为在机器人/自动驾驶等安全关键场景中安全部署，需要能判断何时信任VLM的空间预测。

Method: 构建独立于VLM的视觉验证器：使用目标检测获取物体框与坐标，计算四类信号并用梯度提升融合：(1) VLM声明与检测坐标的几何对齐度；(2) 由框重叠引起的空间歧义；(3) 检测质量（如置信度/IoU等）；(4) VLM内部不确定性（自信度）。该框架适配生成式与分类式VLM（如 BLIP-2、CLIP）。

Result: 在BLIP-2上AUROC=0.674（较文本基线提升34.0%），在CLIP上AUROC=0.583（提升16.1%）；在选择性预测下，目标准确率60%时覆盖率61.9%，较基线27.6%提升2.2倍；特征重要性分析显示视觉信号占87.4%，VLM自信度占12.7%。

Conclusion: 外部几何验证显著优于VLM自评，可可靠筛选可信的空间关系预测，支持更精准的场景图构建（精度从52.1%升至78.3%，保留68.2%边），为安全关键应用中的VLM部署提供有效的置信度估计方案。

Abstract: Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.

</details>


### [16] [IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation](https://arxiv.org/abs/2601.11645)
*Ujjwal Jain,Oshin Misra,Roshni Chakraborty,Mahua Bhattacharya*

Main category: cs.CV

TL;DR: 提出IMSAHLO框架：多尺度密集块+分层注意力+混合损失（Tversky/Focal+clDice+轮廓加权边界），在FNC数据集上优于SOTA，显著提升密集与稀疏神经元分割的精度与拓扑完整性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在荧光显微神经元分割中易受细胞密度变化、形态重叠与严重类别不平衡影响，导致边界不清、拓扑破坏与小目标漏检，需要一种既能捕获多尺度特征又能关注关键形态细节并缓解类不平衡的模型。

Method: 构建IMSAHLO：1) 多尺度密集块（MSDB）融合不同感受野特征，适配密集/稀疏场景；2) 分层注意力（HA）自适应强调显著形态与ROI边界；3) 混合损失：Tversky+Focal应对不平衡，clDice保持中心线与拓扑连续，轮廓加权边界损失强化相邻细胞分割；并在FNC数据集上进行大规模实验与消融。

Result: 在FNC数据集困难场景中取得Precision 81.4%、Macro F1 82.7%、Micro F1 83.3%、Balanced Accuracy 99.5%，整体优于多种SOTA基线；消融表明多尺度注意力与混合损失具有叠加增益。

Conclusion: IMSAHLO能在复杂密度与形态重叠下保持精细边界与拓扑一致性，具备良好泛化潜力，可推广至多种生物医学成像分割任务，支持高通量神经生物学分析流程。

Abstract: Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.

</details>


### [17] [Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification](https://arxiv.org/abs/2601.11651)
*Miriam Doh,Aditya Gulati,Corina Canali,Nuria Oliver*

Main category: cs.CV

TL;DR: 论文发现文本到图像生成模型与下游性别分类系统共同表现出“算法外貌歧视”：把“好看=好属性”的刻板关联系统化编码，并在性别识别上对女性（尤以带负面属性脸）误判更高；新模型还强化了年龄同质化、性别化曝光与地理缩减，放大不平等。


<details>
  <summary>Details</summary>
Motivation: 生成式AI与视觉识别系统是否把社会中对外貌与性别的偏见固化甚至放大，尚缺系统证据。作者想量化T2I模型如何把“吸引力—正面属性”绑定，并检验下游性别分类在不同外貌属性下的偏差与错误，从而揭示跨系统的结构性危害。

Method: 用Stable Diffusion 2.1与3.5 Medium生成26,400张合成面孔，操控提示词注入不同属性（正负面、年龄、性别、地域等），再分析生成图像里吸引力与属性的系统性关联。随后将这些图像输入三种性别分类算法，比较在不同外貌与文本属性条件下的误分类率与偏差。

Result: 1) T2I模型将“吸引力”与正面特质系统性绑定，反之亦然，反映社会刻板而非证据关联；2) 三个性别分类器对女性面孔误判更高，尤其当输入带有负面属性时显著放大；3) 新版本模型显示更强的审美约束：年龄被同质化、性别化曝光更强、地域呈现更单一。

Conclusion: 算法外貌歧视在生成与识别两端共同运作，既影响再现（生成偏差）也影响识别（分类偏差），从而复合放大社会不平等。呼吁对T2I与下游识别模型进行偏见审计、数据与提示治理，以及多样性与公平性的系统干预。

Abstract: This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.
  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.

</details>


### [18] [PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation](https://arxiv.org/abs/2601.11654)
*Kaustubh Shivshankar Shejole,Gaurav Mishra*

Main category: cs.CV

TL;DR: 提出基于像素-超像素图的新相似性度量PSSI，并结合MeanShift与最大生成树进行交互式分割，在GrabCut与Images250上优于AMOE、OneCut、SSNCut，且更快、更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有交互式图割方法在计算开销高、对用户交互敏感、前景背景颜色分布相似时性能下降。核心瓶颈在于图边权的相似性度量不足以同时刻画颜色相似与空间平滑等特性。

Method: 1) 提出Pixel Segment Similarity Index（PSSI）：融合像素强度与空间平滑特征，对多通道相似性取调和平均，能对任一通道的不一致性进行惩罚，复杂度为O(B)（B为直方图区间数）。2) 先用MeanShift做低层分割，得到颜色/纹理/形状一致的像素段；3) 构建像素段图，以PSSI作为边权；4) 用最大生成树（MaxST）进行划分，强化强连通的局部邻域，有助于精确边界。

Result: 在GrabCut与Images250数据集上，较AMOE、OneCut、SSNCut取得更高的IoU与F1，较低的Mean Error，并具有更快的执行时间，表现稳定且对用户交互更鲁棒。

Conclusion: 将PSSI与MeanShift、MaxST结合，能同时利用颜色相似、空间平滑、纹理与形状及强局部连通性，显著提升交互式分割的准确性与效率。代码已开源。

Abstract: Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.

</details>


### [19] [Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores](https://arxiv.org/abs/2601.11660)
*Chunshu Wu,Ruibing Song,Sushant Kondguli,Tong Geng,Ang Li*

Main category: cs.CV

TL;DR: 提出 MBU-Net：通过“掩码二值化+GPU 张量核映射”的方法，在保持接近全精度分割精度（平均下降约3%）的同时，实现2.04倍速度和3.54倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要在严格的延迟与能耗预算下进行高分辨率实时图像分割。U-Net 相对高效，但在算力/内存/功耗受限场景仍难达实时。极端量化（尤其二值化）硬件友好，却面临精度大幅下降与缺乏在通用 GPU 上端到端高效实现两大痛点。

Method: 两条经验发现驱动设计：(1) 对二值 U-Net 权重进行“零掩码”训练可引入显著稀疏，显式零态是必要的；(2) 各层量化敏感度较为均匀。基于此提出 Masked Binary U-Net（MBU-Net）：采用“代价感知的掩码策略”在最具性价比的位置引入掩码，使网络在接近二值化的同时兼顾精度。为落地效率，提出 GPU 执行框架：采用减法式位编码，将“掩码二值权重×二值激活”映射到 Tensor Core 的原生二值 BMMA 指令，实现高吞吐与能耗优势。

Result: 在3个分割基准上，MBU-Net 相比16-bit FP U-Net 达到：接近全精度的分割精度（平均仅下降约3%）、2.04×推理加速、3.54×能耗降低。

Conclusion: 通过在架构层引入“掩码二值化”并在系统层将其高效映射到通用 GPU 张量核，MBU-Net 在精度、速度、能耗之间取得新的折衷点，为边缘实时分割提供可落地方案。

Abstract: Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.
  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.
  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.

</details>


### [20] [LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions](https://arxiv.org/abs/2601.11662)
*Abdullah Jirjees,Ryan Myers,Muhammad Haris Ikram,Mohamed H. Zaki*

Main category: cs.CV

TL;DR: 论文提出LTV‑YOLO：一种面向边缘设备、用于在低光/恶劣天气条件下检测儿童与远距离成人等小体型行人的轻量级热成像目标检测模型。基于YOLO11，结合深度可分离卷积与FPN，专为LWIR热图像、小目标与部分遮挡场景优化，强调实时性与高效性。


<details>
  <summary>Details</summary>
Motivation: VRU（尤其儿童与青少年）在夜间、逆光、雨雪雾等条件下难以被RGB视觉可靠检测，影响交通安全与自动驾驶。现有热成像检测器多为通用或重型模型，缺乏面向小/远/遮挡VRU的、可在边缘侧实时运行的热成像专用方案。

Method: 以YOLO11为骨干，定制热成像单模态管线：引入深度可分离卷积以降算力开销；集成FPN提升小目标与多尺度特征融合；针对LWIR数据与短/遮挡目标做结构与超参优化，形成轻量化的LTV‑YOLO。

Result: 在热成像场景下，模型在检测小尺度、部分遮挡、热特征明显的行人方面表现强，兼顾精度、速度与模型体积，满足边缘设备实时推理。

Conclusion: LTV‑YOLO为智能交通与智慧城市（如校园周边、自动驾驶）提供了实用可扩展的热成像行人检测方案。虽然FPN与深度可分离卷积常见，但其在“仅热成像+面向短/遮挡VRU+边缘实时”这一任务设定中的集成具有新颖性。

Abstract: Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions is, to the best of our knowledge, novel.

</details>


### [21] [UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM](https://arxiv.org/abs/2601.11665)
*Amir Farzin Nikkhah,Dong Chen,Bradford Campbell,Somayeh Asadi,Arsalan Heydarian*

Main category: cs.CV

TL;DR: 综述150+研究，总结UAV在AEC/FM中的数据采集、三维建模、缺陷检测与决策支持；提出融合RGB/LiDAR/热像与Transformer的工作流，优化航线与多模态融合，指出实时处理与泛化性难题，并给出未来方向（轻量AI、自适应飞行、合成数据与更丰富模态）。


<details>
  <summary>Details</summary>
Motivation: 基础设施巡检传统方法成本高、风险大、覆盖与时效有限；UAV与ML快速发展但碎片化，需系统梳理方法、评估成效与痛点，并提出可操作框架指导实际部署。

Method: 系统性文献综述（150+篇），归纳数据采集、摄影测量建模、缺陷检测、决策支持等主题与关键技术（路径优化、热像融合、YOLO/Faster R-CNN等）；结合一案例提出工作流：融合RGB、LiDAR、热像，采用Transformer架构与动态航线规划实现多模态缺陷/异常检测。

Result: 证明UAV已在结构健康监测、灾害响应、城市基础设施、能效评估与文化遗产方面展现价值；总结主要瓶颈为实时处理、多模态融合与模型泛化；提出的工作流在检测结构缺陷、热异常与几何不一致性方面可提升精度与可靠性（基于文献与案例佐证）。

Conclusion: 给出一套分步的UAV巡检框架以融合多模态并适应复杂环境；未来应聚焦轻量化AI、自适应飞行规划、合成数据增强与更丰富模态融合，以提升效率、准确性与可推广性。

Abstract: Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.

</details>


### [22] [MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models](https://arxiv.org/abs/2601.11666)
*Muhammad Imran,Chi Lee,Yugyung Lee*

Main category: cs.CV

TL;DR: MATEX是一种面向医学视觉-语言模型的可解释性框架，融合多尺度注意力回滚、文本引导的空间先验与层间一致性分析，生成更精准、稳定且具解剖学依据的归因图，并在MS-CXR数据上优于SOTA的M2IB方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLM可解释方法存在空间不精确、缺乏解剖学约束、注意力粒度不足与稳定性欠佳等问题，影响临床可用性与信任。作者希望提供解剖学驱动、细粒度、稳定且与文本一致的解释。

Method: 提出MATEX框架：1) 多层注意力回滚（multi-layer attention rollout）以聚合不同层级的注意力信息；2) 文本引导的空间先验，将放射学文本提示与解剖结构对齐以限制与引导归因区域；3) 层一致性分析，约束不同层与尺度的归因图一致与稳定；整体生成梯度归因图，并进行空间精度与临床语义对齐评估。

Result: 在MS-CXR数据集上，MATEX在空间精度与与专家标注发现的一致性方面均优于当前SOTA的M2IB方法，生成的归因图更精准、稳定且具临床意义。

Conclusion: MATEX通过解剖学先验+多尺度注意力+一致性约束，提升医学VLM的解释可信度与透明度，为放射影像AI提供更可靠的可解释性支持。

Abstract: We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.

</details>


### [23] [Generating metamers of human scene understanding](https://arxiv.org/abs/2601.11675)
*Ritik Raina,Abe Leite,Alexandros Graikos,Seoyoung Ahn,Dimitris Samaras,Gregory J. Zelinsky*

Main category: cs.CV

TL;DR: 提出MetamerGen：一种结合周边“粗略要旨”和凝视高分辨信息的潜在扩散模型，生成与人类潜在场景表征感知等效的图像（视觉变形体）。通过DINOv2双流表征将凝视区细节与周边退化上下文融合，并用同异判断实验验证其与人类表征的一致性。结果显示在以真实凝视区域为条件时，高层语义对“同一性”判断贡献最大。


<details>
  <summary>Details</summary>
Motivation: 人类视觉是非均匀采样：中央凹高分辨、周边低分辨，但能形成连贯场景理解。现有生成/重建方法难以同时整合周边要旨与有限高分辨凝视信息，且缺少与人类潜在场景表征对齐的评估工具。因此需要一种既能从“凹+周边”输入合成图像、又可行为学验证与人类表征一致性的模型与范式。

Method: 提出MetamerGen：一种潜在扩散模型的新型“凹化（foveated）图像到图像”合成框架。核心是双流DINOv2 token 表征：一支编码凝视区域的高细节特征，另一支编码周边退化后的场景要旨与上下文；在潜空间融合并进行条件生成。通过同/异行为实验（比较生成图与原图）评估是否构成“视觉变形体（metamer）”，并分析多水平特征对判断的贡献。

Result: MetamerGen可在给定真实或随机凝视的条件下生成与人类潜在场景表征对齐的图像；行为实验中部分生成被判为“相同”，表明达成感知等效。特征贡献分析显示多层次（低到高层）特征均有作用，其中当条件使用观察者自身的凝视区域时，高层语义一致性最能预测变形体效应。

Conclusion: MetamerGen验证了以凹化输入驱动的潜在扩散可与人类场景表征对齐，为研究“我们看到并理解了什么”提供了新工具。与真实凝视结合能更好捕捉语义，从而更易产生感知变形体；该范式可用于解析多层视觉特征在场景理解中的作用。

Abstract: Human vision combines low-resolution "gist" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. "foveated") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a "same" or "different" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.

</details>


### [24] [Conformal Point and the Calibrated Conic](https://arxiv.org/abs/2601.11679)
*Richard Hartley*

Main category: cs.CV

TL;DR: 论文讨论“共形点”和“校准圆锥曲线（calibrating conic）”及其相互关系，用于直观刻画图像几何，并由此提供在图像中计算角度与方向等几何量的直观方法。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉/投影几何中，如何在图像平面上方便而稳健地表达与计算几何量（如角度、方向）是关键问题。传统做法依赖相机内参或繁琐代数推导，不利于直观理解与快速计算。作者引入共形点与校准圆锥曲线的关系，作为统一、几何化的工具来辅助可视化和计算。

Method: 基于投影几何与共形几何框架，定义并阐释图像平面的共形点与校准圆锥曲线，分析它们在射影变换下的不变/半不变性质，并给出用这些实体来表示和计算图像中的角度与方向的构造性步骤（例如，通过与校准圆锥曲线的极-对偶关系或接触条件来恢复欧氏度量信息）。

Result: 建立了共形点与校准圆锥曲线之间的明确联系，说明如何借助它们在图像中直观地获取角度与方向等度量信息，减少对繁琐代数和显式内参的依赖。

Conclusion: 共形点与校准圆锥曲线为图像几何提供了直观、统一的表达与计算手段，可用于计算图像中的角度和方向等度量量，在视觉几何推理与相机标定相关任务中具有应用价值。

Abstract: This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.

</details>


### [25] [Telling Human and Machine Handwriting Apart](https://arxiv.org/abs/2601.11700)
*Luis A. Leiva,Moises Diaz,Nuwan T. Attygalle,Miguel A. Ferrer,Rejean Plamondon*

Main category: cs.CV

TL;DR: 研究使用手写轨迹作为行为生物特征来区分“人类生成”与“人工合成”输入，在10个公开数据集与7类合成器上，基于浅层RNN的非特征化端到端方法取得平均AUC 98.3%、EER 1.4%的强性能，并在少样本与跨域测试中仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 移动与人机交互场景需要验证是否由真实用户操作，手写轨迹蕴含人类运动特性，可作为对抗机器人/合成器的“反图灵测试”。现有合成器（如Kinematic Theory、GAN、Transformer、Diffusion等）能逼真生成手写数据，亟需稳健的鉴别方法评估与提升系统安全性。

Method: 收集10个手写符号/数字/手势/签名与指点轨迹数据集；用7种不同合成器合成对应数据；采用不做手工特征的原始轨迹输入，训练浅层循环神经网络（RNN）二分类器（人类 vs 合成），并评估总体、少样本（只用10%训练）与跨域（out-of-domain）表现，指标为AUC与EER。

Result: 在整体上平均AUC达98.3%，平均EER为1.4%；少样本下用10%训练数据在其余90%测试上仍保持“优秀”水平；跨域测试也表现“非常有竞争力”。

Conclusion: 简单的浅层RNN即可在多数据集、多合成器下高准确区分真实与合成手写轨迹，为需验证人类存在的系统提供有效防护层，并对抗不断进步的合成手写生成器。

Abstract: Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.

</details>


### [26] [SemAlign: Language Guided Semi-supervised Domain Generalization](https://arxiv.org/abs/2601.11724)
*Muditha Fernando,Kajhanan Kailainathan,Krishnakanth Nagaratnam,Isuranga Udaravi Bandara Senavirathne,Ranga Rodrigo*

Main category: cs.CV

TL;DR: 提出一种用于半监督域泛化（SSDG）的新方法：将模型的中间特征对齐到视觉语言模型（VLM）语义丰富、具泛化性的特征空间，并结合图像级增强与输出级正则化，提高数据利用率、减轻过拟合，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SSDG方法过度强调提高伪标签（PL）准确率与防止过拟合，却忽视训练过程中“最大化数据利用率”。这种偏重限制了进一步性能提升；作者希望在兼顾稳健性的同时，充分挖掘有限标注与大量未标注数据的价值。

Method: 1) 特征对齐：将模型中间层特征与VLM的语义通用特征空间对齐，促进域不变表示学习；2) 数据利用：采用有效的图像级增强策略扩展覆盖；3) 正则化：在输出层施加正则化，缓解过拟合与伪标签噪声影响；整体在SSDG框架内联合训练。

Result: 在四个基准上与主流SSDG方法比较，定性与定量均达SOTA表现，显示更强的跨域泛化与稳定性。

Conclusion: 通过借助VLM的通用语义空间进行中间特征对齐，并配合增强与输出正则化，可在不依赖极高伪标签精度的情况下提升SSDG的泛化能力与数据利用效率，达到先进水平；代码将开源。

Abstract: Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.

</details>


### [27] [SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models](https://arxiv.org/abs/2601.11729)
*Turhan Can Kargin,Wojciech Jasiński,Adam Pardyl,Bartosz Zieliński,Marcin Przewięźlikowski*

Main category: cs.CV

TL;DR: 提出SpaRRTa基准，用于评估视觉基础模型在图像中识别物体相对位置的空间关系能力，发现现有模型在空间推理上差异显著，并给出影响空间意识的机制洞见。


<details>
  <summary>Details</summary>
Motivation: 现有如DINO、CLIP等VFM擅长语义理解但空间推理弱；即便加入部分3D任务（如深度估计）训练，跨空间任务表现不一致，难以判断是否具备真正的空间意识还是对特定3D目标过拟合。因此需要一个能直接检验相对位置理解的基准。

Method: 构建SpaRRTa：可生成任意数量的高逼真、多样场景图片，完全可控的物体布局，并提供可自由获取的空间标注；以识别对象间相对位置为核心任务，对多种最先进VFM进行评测与比较分析。

Result: 在该基准上，不同VFM的空间推理能力差异显著，显示部分模型并未从3D任务中学到稳健的空间意识。

Conclusion: SpaRRTa为评估与分析VFM空间意识提供了新工具，可用于揭示支持或阻碍空间推理的机制，并指导未来更具空间感知能力的视觉模型发展。

Abstract: Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.

</details>


### [28] [From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce](https://arxiv.org/abs/2601.11769)
*Cheng Lyu,Jingyue Zhang,Ryan Maunu,Mengwei Li,Vinny DeGenova,Yuanli Pei*

Main category: cs.CV

TL;DR: 提出一种解耦于分类体系的电商视觉搜索与基于LLM的零样本评测框架，在大规模落地中提升召回质量与用户参与度。


<details>
  <summary>Details</summary>
Motivation: 现有工业视觉搜索依赖检测+基于类目的分类与目录数据评估，目录噪声与主观开放式意图导致鲁棒性与可扩展性受限，评估成本高且不可靠。

Method: 1) 采用与类目解耦的架构：使用不依赖分类的区域提议（classification-free region proposals）发现候选局部，并用统一嵌入进行相似度检索，实现更灵活的跨类泛化。2) 评测方面提出“LLM-as-a-Judge”：让大模型在零样本条件下评估查询-结果对的细粒度视觉相似性与类目相关性，摆脱人工标注或噪声目录的依赖。3) 在全球家居平台部署并进行线上线下联动验证。

Result: 线上大规模部署后，检索质量显著提升，并带来可测量的用户参与度提升；线下指标与真实业务效果高度相关。

Conclusion: 解耦分类体系的统一嵌入检索配合LLM裁判的零样本评测，能在风格驱动的电商场景中提升视觉搜索的泛化性、鲁棒性与评估可行性，并与业务指标良好对齐。

Abstract: Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.

</details>


### [29] [studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting](https://arxiv.org/abs/2601.11772)
*Yimu Pan,Hongda Mao,Qingshuang Chen,Yelin Kim*

Main category: cs.CV

TL;DR: studentSplat：用教师-学生框架与外推网络，实现单视图输入下的3D高斯溅射场景重建，缓解尺度歧义并提升视角外推质量，达到SOTA并兼作自监督深度估计。


<details>
  <summary>Details</summary>
Motivation: 单视图3D场景重建因尺度不确定与视角外推缺失而远落后于多视图；而近期前馈式3D高斯溅射在多/单视图物体重建上表现突出，却未充分解决单视图场景的歧义与外推问题。

Method: 提出studentSplat：1) 教师-学生架构——用多视图教师模型在训练时提供几何监督，缓解单视图尺度歧义并保证几何合理性；2) 外推网络——补全缺失场景上下文，提升新视角的外推能力与质量；基于3D Gaussian Splatting的前馈式管线。

Result: 在单视图新视角重建质量上达SOTA；在场景级别接近多视图方法；作为自监督单视图深度估计器也具竞争力。

Conclusion: 通过教师几何监督与外推补全，studentSplat有效解决单视图场景重建中的尺度与外推难题，兼具高质量新视角合成与深度估计潜力，具备通用单视图3D理解应用前景。

Abstract: Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.

</details>


### [30] [Cross-Domain Object Detection Using Unsupervised Image Translation](https://arxiv.org/abs/2601.11779)
*Vinicius F. Arruda,Rodrigo F. Berriel,Thiago M. Paixão,Claudine Badue,Alberto F. De Souza,Nicu Sebe,Thiago Oliveira-Santos*

Main category: cs.CV

TL;DR: 提出用无监督图像翻译（CycleGAN与基于AdaIN的模型）把源域标注图像转成目标域风格，生成目标域的人工数据来直接训练检测器，简化流程并提升可解释性，在自动驾驶跨域检测上优于主流特征对齐方法并缩小与上界差距。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应检测多做中间特征对齐，虽有效但实现复杂、解释性差，且与使用目标域标注训练的上界仍有差距，需更简单、可解释且效果更好的方法。

Method: 仅用源域有标注数据与目标域无标注数据，训练两个无监督图像翻译器（CycleGAN与AdaIN风格迁移），把源域图像翻成目标域风格，产出“目标域风格+源域标注”的人工数据集，然后用常规目标检测器在该数据上直接监督训练。

Result: 在自动驾驶真实场景的跨域检测任务中，相比最新特征对齐类方法，在多数设置上取得显著提升，表现更优，并进一步缩小与目标域有标注训练的上界性能差距。

Conclusion: 通过风格迁移生成目标域风格的标注数据，可用更低复杂度和更强可解释性的流程实现更好的跨域检测性能，是特征对齐之外有效且实用的替代方案。

Abstract: Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.

</details>


### [31] [Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening](https://arxiv.org/abs/2601.11896)
*Ngoc-Khai Hoang,Thi-Nhu-Mai Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 提出一个用于院前快速卒中筛查的多模态深度学习框架，融合面部表情、语音和上肢动作，基于自建数据集验证，显著优于单模态，准确率95.83%、F1 96.00%，对测试集所有卒中样本均检出，但需更大、更具代表性的临床数据以落地。


<details>
  <summary>Details</summary>
Motivation: 卒中院前早识别能显著改善预后，但现有方法多依赖人工F.A.S.T.量表、单模态信号与主观判断，鲁棒性与自动化程度不足。需要一种快速、非侵入、可在复杂院前环境工作的自动筛查方法，并提升对不同表现与噪声条件的适应性。

Method: 构建多模态二分类框架：1) 面部：基于人脸关键点提取时序动态特征，用Transformer捕捉时序依赖；2) 语音：将语音转为mel谱图，采用Audio Spectrogram Transformer提取声学表征；3) 上肢：用姿态关键点序列，采用MLP-Mixer建模时空运动模式；4) 融合：注意力式跨模态融合学习交互；5) 训练：在自采集222段、37名受试者数据上训练与评估，并与各单模态基线对比；引入迁移学习以增强泛化。

Result: 多模态模型在自建测试集上获得95.83%准确率、96.00% F1，兼顾敏感性与特异性，并对测试集中所有卒中病例实现检出，显著优于任一单模态基线。

Conclusion: 多模态与迁移学习可显著提升院前卒中自动筛查性能，具备快速、非侵入的应用潜力。但当前样本量小、外部有效性与临床代表性不足，需更大规模、真实世界数据与前瞻性验证以支持可靠部署。

Abstract: Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.

</details>


### [32] [RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection](https://arxiv.org/abs/2601.11898)
*Yilmaz Korkmaz,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出RemoteVAR：一种将视觉自回归模型用于遥感变化检测的新框架，通过跨尺度双时相特征条件化和专门的自回归训练策略，显著提升像素级变化图预测，超越扩散与Transformer强基线。


<details>
  <summary>Details</summary>
Motivation: VAR在生成方面强，但在像素级判别任务上可控性弱、密集预测欠佳、且存在曝光偏差；遥感变化检测需要精确、可控的双时相像素级推断，因此需要一种能把VAR优势与判别任务需求结合的方法。

Method: 构建RemoteVAR：以多分辨率融合的双时相特征作为条件，通过跨注意力引导自回归像素（或块）预测，配合为变化图预测定制的自回归训练（缓解曝光偏差、提升密集预测稳定性）。

Result: 在标准遥感变化检测基准上，相比强扩散模型与Transformer基线，RemoteVAR取得稳定且显著的性能提升，展现出竞争力的自回归替代方案。

Conclusion: 经专门设计的条件化与训练策略使VAR适用于变化检测，RemoteVAR在多基准上验证有效，为遥感像素级判别任务提供了可行的自回归路径。

Abstract: Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\underline{here}}.

</details>


### [33] [Towards Airborne Object Detection: A Deep Learning Analysis](https://arxiv.org/abs/2601.11907)
*Prosenjit Chatterjee,ANK Zaman*

Main category: cs.CV

TL;DR: 提出基于EfficientNetB4的双任务模型，同时进行空中目标类别分类与威胁等级预测；自建并清洗AODTA数据集，性能在AODTA与AVD上均优于ResNet-50，分类准确96%、威胁预测90%。


<details>
  <summary>Details</summary>
Motivation: 空中平台（民航机、无人机、UAV）激增，人工监控难以实时、可扩展地评估威胁，需要自动化、实时的威胁评估模型；现有数据集稀疏且不均衡，限制了模型训练效果。

Method: 以EfficientNetB4为骨干，设计多任务学习框架：共享特征提取干路，分支同时输出目标类别与威胁等级；为缓解数据问题，聚合多公开源并清洗构建AODTA数据集；在AVD与AODTA上训练与评测，并与ResNet-50基线对比。

Result: EfficientNetB4双任务模型在目标分类上达96%准确率、威胁等级预测达90%准确率；在两数据集上的表现均优于ResNet-50基线，后者持续落后。

Conclusion: 该双任务模型能有效提升空中目标识别与威胁推断的准确性，适用于监视、防务与空域管理场景；尽管题目提到“检测”，本文工作限定于对数据集中已定位目标的分类与威胁推断。

Abstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.

</details>


### [34] [Effects of the retina-inspired light intensity encoding on color discrimination performance](https://arxiv.org/abs/2601.11909)
*Io Yamada,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: 研究比较了中心/周围Retinex模型在不同光强编码函数下的颜色恒常性表现，发现采用Naka-Rushton函数并在双对手色彩平面表示下能更好区分不同照明下的目标颜色。


<details>
  <summary>Details</summary>
Motivation: 颜色恒常性使视觉系统能在不同照明颜色下保持目标颜色稳定，是物体识别等任务的关键；经典C/S Retinex依赖光强编码函数，但不同编码函数对恒常性表现的影响尚不清楚。

Method: 在C/S Retinex框架中替换光强编码函数，比较原始的对数函数与模拟视网膜感受器响应的Naka-Rushton函数。使用可变色LED在多种照明下照明视觉目标，计算模型输出的颜色信息，并在HSV空间与基于经典对手色理论的颜色平面（含双对手表示）中评估不同照明下目标颜色的可分辨性。

Result: 采用Naka-Rushton函数并结合双对手色彩平面表示时，模型对不同照明条件下的目标颜色区分度最佳，优于对数函数与其他表示。

Conclusion: 在C/S Retinex中使用更符合生理的Naka-Rushton光强编码，并配合双对手色彩表征，可显著提升颜色恒常性的判别性能；这为构建鲁棒的基于颜色信息的视觉系统提供了有效策略。

Abstract: Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.

</details>


### [35] [A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection](https://arxiv.org/abs/2601.11910)
*Guiying Zhu,Bowen Yang,Yin Zhuang,Tong Zhang,Guanqun Wang,Zhihao Che,He Chen,Lianlin Li*

Main category: cs.CV

TL;DR: 提出无需训练的GW-VLM，通过多尺度视觉-语言搜索(MS-VLS)与情境概念提示(CCP)协同VLM与LLM进行“猜物体”互动，在COCO、VOC、DIOR、NWPU-10等上实现优于现有方法的开放词汇目标检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有OVOD多依赖大规模预训练模型的零样本能力，但忽视了在“任何物体认知”上建立通用理解范式的必要性；作者希望在不再训练的前提下，更好地利用现成VLM/LLM实现泛化检测。

Method: 提出训练免调的GW-VLM框架：1) MS-VLS进行多尺度视觉-语言软对齐，基于类无关检测生成候选并产出“片段”(snippets)；2) CCP根据MS-VLS的结果构建情境化概念流，促使LLM理解这些片段并完成开放词汇检测；整体以VLM与LLM的“猜物体”博弈式协同完成识别与定位。

Result: 在COCO val、Pascal VOC、DIOR与NWPU-10上做广泛实验，GW-VLM在无需任何训练的条件下，OVOD性能优于现有最先进方法。

Conclusion: 通过MS-VLS与CCP实现VLM和LLM协同的“猜物体”范式，可在零训练下建立通用理解并提升OVOD性能，展现跨自然与遥感场景的强泛化。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of "guess what". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.

</details>


### [36] [Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh](https://arxiv.org/abs/2601.11911)
*Muhammad Ibrahim,Alfe Suny,MD Sakib Ul Islam,Md. Imran Hossain*

Main category: cs.CV

TL;DR: 评估一款紧凑型CNN在孟加拉真实小类图像任务上的表现，取得高准确、快收敛、低计算开销，并具良好泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 主流CNN在小数据集上易过拟合且架构复杂、计算成本高，需要验证更精简的网络能否在真实世界小类图像任务中保持性能与泛化。

Method: 在五个来自孟加拉的公开数据集（含城市侵占、车辆检测、道路破损、农作物等）上，采用一套紧凑CNN进行训练与评测；报告分类准确率、收敛效率、计算开销，并用显著性图分析判别特征。

Result: 该紧凑CNN实现高分类准确率、快速收敛、低计算成本；显著性分析表明模型聚焦于关键判别区域，显示稳健的跨场景泛化能力。

Conclusion: 精简化CNN对小类图像分类任务是合适的选择，能在资源受限和小样本条件下兼顾准确性、效率与泛化。

Abstract: Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.

</details>


### [37] [From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection](https://arxiv.org/abs/2601.11915)
*Chi Wang,Xinjue Hu,Boyu Wang,Ziwen He,Zhangjie Fu*

Main category: cs.CV

TL;DR: 论文提出在表示空间中统一建模并干预“伪相关”因素，通过低秩正交投影剔除它们，使分类仅依赖真实伪造线索，显著提升跨数据集泛化，且参数量仅0.43M即达SOTA。


<details>
  <summary>Details</summary>
Motivation: 人脸伪造检测在跨数据分布时泛化差。已有工作发现表示中存在由与伪造无关信息到标签的“后门路径”，导致模型学到偏置。此前方法多针对具体伪相关逐一处理，但伪相关由不可观测混淆因素引起，难以穷尽识别与逐个消除。

Method: 提出“表示空间干预”范式：将所有实例级伪相关统一建模为低秩子空间。通过正交低秩投影把伪相关特征分解到该子空间，并从原表示中移除该子空间分量，仅在其正交补上训练分类器或特征提取分支，以捕获与伪造相关的真实线索。核心是低秩投影与子空间移除（orthogonal low-rank projection + subspace removal），参数开销极小（0.43M）。

Result: 在多个基准上取得SOTA，证明对跨数据集、分布偏移的鲁棒性与泛化能力明显提升。

Conclusion: 将伪相关作为低秩子空间统一处理并在表示层面进行干预，能有效切断后门路径，使决策依赖真实伪造证据；方法轻量、泛化强，适合实际部署。

Abstract: The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.

</details>


### [38] [Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions](https://arxiv.org/abs/2601.11918)
*Akito Morita,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: 用类VNS的Gabor滤波作为CNN预处理，在小数据、边缘设备场景下提升泛化并缩小模型。


<details>
  <summary>Details</summary>
Motivation: 边缘机器人视觉需要小而高效的CNN，且在受限条件与少样本下训练仍要具备跨条件泛化能力；受生物视觉启发，VNS能以少量经验学习，提示可借助其特征提取机制。

Method: 将Gabor滤波作为图像预处理，随后训练多种小型CNN架构；构建含不同相机位置/距离的自制数据集，用在某一距离上训练，在其他距离上测试；比较使用与不使用Gabor预处理的准确率与模型尺寸。

Result: 在多种CNN上，加入Gabor预处理的模型在跨距离测试时准确率更高，且可在较小网络规模下达到或超过基线性能。

Conclusion: Gabor预处理能提升小数据条件下CNN的泛化能力，并允许减小网络规模，适合边缘机器人视觉部署。

Abstract: In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.

</details>


### [39] [SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM](https://arxiv.org/abs/2601.11930)
*Xulei Shi,Maoyu Wang,Yuning Peng,Guanbo Wang,Xin Wang,Qi Chen,Pengjie Tao*

Main category: cs.CV

TL;DR: 提出SupScene：面向SfM重建中“几何可匹配性”而非语义相似性的图像检索框架，含软监督对比学习与DINO注意力驱动的VLAD聚合（DiVLAD），在GL3D上显著超越NetVLAD且参数增量很小，并对多种聚合器均带来稳健收益。


<details>
  <summary>Details</summary>
Motivation: 传统SfM中的图像检索多用二元标签（重叠/不重叠）训练全局描述子，强调语义相似而忽略几何可匹配性细粒度差异，难以准确发现真正可配准的重叠视图，导致匹配开销与鲁棒性受限。

Method: 1) 子图级训练：基于真实几何重叠关系构建子图，并用连续权重刻画共视程度；提出软监督对比损失，用细粒度监督替代二元标签。2) DiVLAD聚合：以ViT最后一层多头注意力图作为“语义显著性”引导，将DINO思想融入VLAD；再通过可学习门控，将注意力线索与视觉特征自适应融合，生成更判别的全局描述子。3) 训练策略可与不同聚合器兼容。

Result: 在GL3D基准上取得SOTA，明显优于NetVLAD，同时仅增加极少可训练参数；所提子图+软对比训练在多种聚合器上均带来一致增益。

Conclusion: 针对SfM的检索需求，SupScene通过细粒度几何重叠监督与注意力增强的VLAD聚合，显著提升可匹配对的检索质量与效率，方法轻量、通用、可移植。

Abstract: Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.

</details>


### [40] [Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition](https://arxiv.org/abs/2601.11931)
*Zhengxian Wu,Chuanrui Zhang,Shenao Jiang,Hangrui Xu,Zirui Liao,Luyuan Zhang,Huaqiu Li,Peng Jiao,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出LMGait：用语言引导与运动感知机制提升步态识别，缓解对静态噪声过拟合并更好捕获动态区域。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法多用复杂网络从图像直接提特征并池化到序列级表示，容易对衣着等静态噪声过拟合，且难以有效聚焦随时间变化的运动区域，限制了泛化与鲁棒性。

Method: 构建语言引导与运动感知的框架LMGait：设计与步态相关的语言提示/线索，作为高层先验，引导模型从序列中对关键运动模式与部位进行关注，从而在特征学习阶段突出动态信息、抑制静态干扰。

Result: 相较传统直接池化的方案，所提方法能够更好地捕获关键运动特征并减轻静态噪声影响；（摘要未给出具体数值与基准，但暗示性能与鲁棒性有所提升）。

Conclusion: 将语言先验融入步态表示学习并强调运动区域，有助于提升步态识别对服饰等静态变化的鲁棒性与对动态信息的敏感度，体现了多模态/语言指导对时空表征学习的价值。

Abstract: Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.

</details>


### [41] [Deep learning-based neurodevelopmental assessment in preterm infants](https://arxiv.org/abs/2601.11944)
*Lexin Ren,Jiamiao Lu,Weichuan Zhang,Benqing Wu,Tuo Wang,Yi Liao,Jiapan Guo,Changming Sun,Liang Guo*

Main category: cs.CV

TL;DR: 提出一种用于早产儿脑MRI体数据的层次化稠密注意力分割网络，通过3D空间-通道注意与注意引导的稠密上采样，在等信号对比（WM/GM不易区分）下实现更优WM/GM分割，并证实早产儿WM/GM体积显著低于足月儿。


<details>
  <summary>Details</summary>
Motivation: 早产儿存在较高神经发育迟缓风险，需早期评估。MRI体素分割可量化脑组织，但早期脑发育阶段WM与GM在MRI上等信号，导致分割困难。因此需要一种能在低对比度体数据中增强特征判别力的新方法。

Method: 提出Hierarchical Dense Attention Network（HDAN）：在3D分割网络中引入空间-通道联合注意力模块，配合注意力引导的稠密上采样策略，以分层方式加强上下文聚合与细节恢复，从而提升对等信号组织的区分能力。

Result: 在定量实验中优于多种SOTA基线，在WM/GM等组织分割任务上取得更高性能；应用于临床数据表明早产儿WM与GM体积显著低于足月儿。代码开源于GitHub。

Conclusion: HDAN能够有效缓解早期脑MRI中WM/GM等信号带来的分割难题，提升分割精度，并为早产儿神经发育迟缓提供成像学证据与可复现的工具。

Abstract: Preterm infants (born between 28 and 37 weeks of gestation) face elevated risks of neurodevelopmental delays, making early identification crucial for timely intervention. While deep learning-based volumetric segmentation of brain MRI scans offers a promising avenue for assessing neonatal neurodevelopment, achieving accurate segmentation of white matter (WM) and gray matter (GM) in preterm infants remains challenging due to their comparable signal intensities (isointense appearance) on MRI during early brain development. To address this, we propose a novel segmentation neural network, named Hierarchical Dense Attention Network. Our architecture incorporates a 3D spatial-channel attention mechanism combined with an attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric data. Quantitative experiments demonstrate that our method achieves superior segmentation performance compared to state-of-the-art baselines, effectively tackling the challenge of isointense tissue differentiation. Furthermore, application of our algorithm confirms that WM and GM volumes in preterm infants are significantly lower than those in term infants, providing additional imaging evidence of the neurodevelopmental delays associated with preterm birth. The code is available at: https://github.com/ICL-SUST/HDAN.

</details>


### [42] [Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal](https://arxiv.org/abs/2601.11952)
*Haonan An,Guang Hua,Wei Du,Hangcheng Cao,Yihang Tao,Guowen Xu,Susanto Rahardja,Yuguang Fang*

Main category: cs.CV

TL;DR: 论文提出“解码器梯度护盾”（DGS）以防止黑盒盒外水印方案的解码器被基于梯度的攻击训练出水印移除器；通过在解码器输入、层内或输出端对泄露梯度进行重定向与缩放，阻断攻击者训练收敛，同时保持解码器生成图像质量，在多任务上实现100%防御成功率。


<details>
  <summary>Details</summary>
Motivation: 盒外水印（box-free）因模型无关、能处理高熵生成图像而受关注，通常采用编码器-解码器并以黑盒方式工作。已有工作多强调编码器的鲁棒性，忽视了解码器安全：攻击者可利用查询响应获得反向传播梯度，训练出水印移除器，导致水印失效。

Method: 提出Decoder Gradient Shields（DGS）家族：在解码器输出端（DGS-O）、输入端（DGS-I）与层内（DGS-L）对从查询中泄露的“水印通道梯度”进行联合“重定向+缩放”。DGS-O给出闭式解；全部DGS提供可证明性能保证。目标是在不显著影响解码器输出图像质量的前提下，使攻击者的训练目标无法收敛到低损失。

Result: 在去雨与图像生成等多种应用场景、基于最先进盒外水印框架的实验中，DGS在所有设置下实现100%防御成功率，并保持解码器输出图像质量。

Conclusion: 解码器是盒外水印系统的关键攻击面。通过在不同位置注入可控的梯度“护盾”，可有效阻断基于梯度的水印移除训练而不牺牲生成质量；DGS提供闭式与可证明机制，通用于多任务并表现稳健。

Abstract: Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.

</details>


### [43] [Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms](https://arxiv.org/abs/2601.11970)
*S. M. Khalid Bin Zahid,Md. Rakibul Hasan Nishat,Abdul Hasib,Md. Rakibul Hasan,Md. Ashiqussalehin,Md. Sahadat Hossen Sajib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 提出在树莓派5上运行的实时多模态视觉框架，通过情境触发的自适应调度统一调配目标检测、人脸识别与情绪识别，整体算力负载降约65%，在5.6 FPS下仍保持较高精度。


<details>
  <summary>Details</summary>
Motivation: 现有监控系统多将目标检测、脸部与情绪分析割裂处理，缺乏能依据场景上下文动态分配算力的统一运行时调度器，难以在低功耗边缘设备上实现高效、整体的智能感知与隐私友好处理。

Method: 在树莓派5上构建统一管线：以YOLOv8n进行目标检测；在检测到“主人/关心对象”相关上下文时调用基于FaceNet的定制人脸嵌入进行身份识别；必要时调用DeepFace CNN进行情绪分类。核心是情境感知的自适应调度器，按触发条件选择性激活模块，从而降低持续全时处理的开销。

Result: 自适应调度使较“连续全开”基线计算负载降低约65%；目标检测AP=0.861；人脸识别准确率88%；情绪识别对部分情绪AUC最高达0.97；整机实时性约5.6 FPS。

Conclusion: 情境感知调度是让成本可控的边缘设备也能承载复杂多模态AI的关键路径，可在保障隐私的同时实现较高精度与可用实时性。

Abstract: Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.

</details>


### [44] [AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering](https://arxiv.org/abs/2601.11976)
*Zongmin Li,Yachuan Li,Lei Kang,Dimosthenis Karatzas,Wenkang Ma*

Main category: cs.CV

TL;DR: 提出AVIR自适应视觉文内检索框架：先用轻量检索按页评估相关性并自适应聚类筛页，再Top-K精简或阈值选择短文档，最后仅将选中页送入冻结LVLM回答。在MP-DocVQA上以更低算力将页数减少70%，ANLS达84.58%，并在SlideVQA、DUDE验证有效。


<details>
  <summary>Details</summary>
Motivation: 多页文档VQA中，长文档导致算力消耗大、LVLM注意力被稀释，端到端大模型或全页处理低效；需要在不微调大模型的前提下降低上下文冗余、提升推理质量与效率。

Method: 1) 轻量级页级检索为每页打问题相关分；2) 根据分布进行自适应聚类，选出候选页；3) 对候选再Top-K精简以保持上下文紧凑；4) 若文档较短、聚类不可靠，改用相关概率阈值筛选；5) 仅将选中页面输入冻结的LVLM生成答案。

Result: 在MP-DocVQA上平均答题页数减少约70%，ANLS达84.58%，超过以往方法，并显著降低计算成本；在SlideVQA与DUDE基准上同样验证了有效性。

Conclusion: AVIR通过自适应检索与精简上下文，在无需微调LVLM的情况下，兼顾精度与效率，适合多页文档VQA并具良好跨数据集泛化。

Abstract: Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.

</details>


### [45] [Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection](https://arxiv.org/abs/2601.11981)
*Jian Lang,Rongpei Hong,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.CV

TL;DR: 提出RADAR：一种在测试时对未见主题的假新闻视频进行自适应的方法，通过检索稳定样本、对齐分布与目标域自训练，实现对新事件与不平衡标签的鲁棒适配，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FNVD方法假设训练与测试话题一致，难以适应突发事件/新话题导致的领域偏移与类别分布变化；需要一种在测试时无需源数据即可快速适配未见主题的视频方法。

Method: 提出检索引导的测试时自适应框架RADAR：1) 基于熵的选择式检索，挑选目标域中低熵（稳定）、语义相关的视频作为参考；2) 稳定锚点引导的对齐模块，将不稳定样本的表示与其稳定参考在分布层面对齐，缩小与源域差异；3) 目标域感知的自训练，利用由稳定参考增强的伪标签，处理目标域类别分布变化与不平衡。

Result: 在多组实验中，RADAR在测试时FNVD任务上取得显著优于现有方法的性能，能对未见新闻主题实现强健的在线适配。

Conclusion: 检索稳定参考并进行锚点对齐与目标域感知自训练，可有效缓解领域与分布漂移，实现对新事件/未见主题的假新闻视频检测的高效测试时适配。

Abstract: Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.

</details>


### [46] [An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System](https://arxiv.org/abs/2601.11983)
*Md. Asiful Islam,Abdul Hasib,Tousif Mahmud Emon,Khandaker Tabin Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 论文提出一种低成本、模块化的AI+IoT智能轮椅，支持手套手势控制、YOLOv8视觉与超声波多模态避障，并持续监测生命体征、云端告警，实验取得较高导航与检测准确度。


<details>
  <summary>Details</summary>
Motivation: 传统轮椅缺乏智能与安全功能，现有“智能”方案往往价格高、单一模态、与健康监测整合不足。老年与障碍人群增长带来对可负担、个性化且能保障安全与健康的助行设备的迫切需求。

Method: 构建一套AI-IoT一体化系统：1) 手套式手势识别实现免手操作导航；2) 视觉端采用YOLOv8进行目标/障碍物检测，并以语音反馈辅助避障；3) 超声波传感器用于近距/即刻防撞；4) 生命体征（心率、血氧、ECG、体温）连续采集并上传ThingSpeak，关键阈值触发邮件告警；5) 低成本、模块化硬件与软件架构。

Result: 手势控制成功率95.5%；超声避障准确率94%；YOLOv8检测Precision 91.5%、Recall 90.2%、F1 90.8%。

Conclusion: 多模态融合的低成本智能轮椅在安全导航与健康监测上取得良好表现，具备实用性与可扩展性，可提升用户自主性与独立性，并缩小研究到落地的差距。

Abstract: The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\% success rate, ultrasonic obstacle detection reached 94\% accuracy, and YOLOv8-based object detection delivered 91.5\% Precision, 90.2\% Recall, and a 90.8\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.

</details>


### [47] [Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis](https://arxiv.org/abs/2601.11987)
*Khaled Berkani*

Main category: cs.CV

TL;DR: 提出一种结合显式解剖先验的结构化图推理框架，将卷积特征图转为补丁级图，用自定义的空间关系传播实现可解释的病灶与诊断预测，并在胸片案例中验证可解释性与推理效果，方法通用且可迁移。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的诊断方法可解释性不足，传统GNN多依赖通用消息传递，难显式建模解剖结构与相对空间关系，导致推理过程缺乏结构约束与可解释性。

Method: 将CNN特征图重解释为补丁级图：节点同时编码外观与空间坐标，边反映局部结构邻接；提出自定义的结构化传播机制，将相对空间关系显式纳入消息传递/推理；联合进行节点级（病灶感知）预测与图级诊断推理，并通过学习到的节点重要性得出内生可解释性，无需事后可视化。

Result: 在胸部X光案例研究中展示：结构先验指导关系推理，提升解释性；模型能输出节点重要性并进行节点/图级预测；效果与可解释性较基线更优（摘要未给出具体指标）。

Conclusion: 图作为结构化归纳偏置能增强可解释的视觉推理；该框架虽在医学影像上评估，但具领域无关性，契合更广泛的图推理愿景，并为面向结构与可解释学习的研究提供支持。

Abstract: We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.

</details>


### [48] [DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset](https://arxiv.org/abs/2601.11990)
*Yiming Li,Chen Cai,Tianyi Liu,Dan Lin,Wenqian Wang,Wenfei Liang,Bingbing Li,Kim-Hui Yap*

Main category: cs.CV

TL;DR: 提出DAOS驾驶员动作-物体协同数据集与AOR-Net模型，通过建模动作-物体-关系多层推理与链式提示、并用Mixture of Thoughts动态选择知识，在多模态多视角条件下显著提升驾驶动作识别效果。


<details>
  <summary>Details</summary>
Motivation: 驾驶舱内上半身动作相似度高，单靠肢体轨迹难区分；人类识别常借助所交互物体（如手机、方向盘），但现有数据集缺少精确物体位置与动作-物体关联标注，影响可靠、细粒度动作识别。

Method: 1) 构建DAOS数据集：9787段视频、36类细粒度动作、15类物体、超250万物体实例，含RGB/IR/深度多模态与前/面部/左/右多视角；提供动作与相关物体关系标注。2) 提出AOR-Net：进行动作-物体-关系的多层推理，并设计“链式动作提示”机制建模动作与物体及其关系的逻辑链。3) 引入Mixture of Thoughts模块，在不同阶段动态选择关键知识，适应物体丰富或稀缺场景。

Result: 在多个数据集上广泛实验，AOR-Net优于现有SOTA方法；在对象密集与稀疏条件下均表现稳健。

Conclusion: 结合精细标注的DAOS与具关系推理和动态知识选择的AOR-Net，可有效缓解驾驶员上半身动作易混淆问题，显著提升细粒度驾驶动作识别的准确性与鲁棒性。

Abstract: In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.

</details>


### [49] [SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine](https://arxiv.org/abs/2601.12010)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: 论文提出SMc2f：一种用于自动驾驶场景挖掘的“粗到细”管线，结合VLM粗检索、基于RefAV的成功案例数据库与自动示例检索以few-shot提示LLM，以及文本-轨迹对比学习进行细粒度匹配，显著提升检索准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现实道路日志中安全关键场景稀有且长尾，但对自动驾驶计划与控制栈的安全验证至关重要。现有RefAV依赖轨迹标签做检索、与原始图像脱节且受上游3D检测跟踪误差影响，导致空间与时间定位不准，需要一种更鲁棒、贴近视觉信号的场景挖掘方法。

Method: 提出SMc2f三阶段：1) 用视觉-语言模型进行粗粒度图文筛选，直接利用图像与文本对齐；2) 基于RefAV构建成功挖掘案例库，并自动检索相似示例，对LLM进行few-shot条件化以提升鲁棒检索；3) 设计文本-轨迹对比学习，将匹配对拉近、非匹配对拉远，学习共享嵌入用于细粒度匹配，进一步对LLM候选轨迹进行精炼。

Result: 在公共数据集上，相比基线（含RefAV），在检索质量（空间/时间定位准确率等）与效率（检索速度、筛选开销）上均获得显著提升。

Conclusion: 结合VLM的粗筛、案例库驱动的few-shot LLM检索，以及对比学习的细匹配，可更鲁棒地从海量驾驶日志中挖掘安全关键场景，为回放仿真、回归测试与失效分析提供高质量、效率更高的场景检索。

Abstract: The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.

</details>


### [50] [SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture](https://arxiv.org/abs/2601.12015)
*Pavan Kumar Yata,Pediredla Pradeep,Goli Himanish,Swathi M*

Main category: cs.CV

TL;DR: 提出DeepSegFusion混合深度学习模型用于SAR图像溢油分割，融合SegNet与DeepLabV3+并加入注意力特征融合，在多个数据集上显著降低误报并提升边界与上下文理解，达成94.85%准确率、IoU 0.5685、ROC-AUC 0.9330，误检减少64.4%，适用于近实时监测。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的SAR溢油检测易受类油膜（风致薄膜、船舶尾迹等）干扰，误报高、稳健性差，需要一种能兼顾边界精度与全局语义的自动化方法以满足环境监管与海事安全的近实时需求。

Method: 提出DeepSegFusion：以SegNet和DeepLabV3+作为双分支主干，分别侧重精细边界与上下文语义；通过注意力驱动的多尺度特征融合模块在编码器/解码器阶段对齐并加权聚合特征；在SAR油污数据集（含ALOS PALSAR）上进行训练与评估。

Result: 在SAR油污数据集上取得准确率94.85%、IoU 0.5685、ROC-AUC 0.9330；与单一基线和非分割传统法相比，误检减少超过三倍，总体误报下降64.4%。

Conclusion: DeepSegFusion在多种海况下表现稳定，显著降低类油膜导致的误报，兼顾边界与语义信息，可用于近实时的SAR溢油监测场景。

Abstract: Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.

</details>


### [51] [DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering](https://arxiv.org/abs/2601.12020)
*Guillermo Figueroa-Araneda,Iris Diana Jimenez,Florian Hofherr,Manny Ko,Hector Andrade-Loarca,Daniel Cremers*

Main category: cs.CV

TL;DR: DIAMOND-SSS 用极少视角/光照数据，通过微调扩散模型生成高保真视角合成与重光照数据，并配合与光照无关的几何先验，实现对半透明材质的高质量重建与可重光渲染，显著降低真实采集需求。


<details>
  <summary>Details</summary>
Motivation: 半透明材质的次表面散射导致复杂光输运，神经渲染需要密集多视角与单光源(OLAT)数据，采集成本高且不易获得；现有方法在稀疏监督下质量和稳定性不足。

Method: 1) 以极少量(≤7%)真实数据估计几何并微调扩散模型，进行新视角与重光照合成，生成可替代高达95%缺失采集的照片级增强数据；2) 在稀疏或合成监督下，为稳定重建，引入与光照无关的几何先验：多视角轮廓一致性损失与多视角深度一致性损失；3) 基于高斯渲染实现可重光的SSS重建流程。

Result: 在各类数据稀疏度下，达到当前最优的可重光高斯渲染质量；与SSS-3DGS相比，真实采集需求减少最高可达90%。

Conclusion: 通过扩散模型的数据增强与几何一致性先验，DIAMOND-SSS在极稀疏监督下仍能实现高保真次表面散射重建与重光照，显著降低数据采集成本并提升实用性。

Abstract: Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).
  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.
  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.

</details>


### [52] [\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions](https://arxiv.org/abs/2601.12049)
*Chenchen Zhao,Muxi Chen,Qiang Xu*

Main category: cs.CV

TL;DR: FocaLogic是一种与模型无关的视觉可解释性框架，通过寻找最小关键视觉区域并用精炼逻辑表达式表征其对预测的决定性作用，同时提出量化评估指标来系统评估模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有视觉可解释方法要么依赖白盒访问、要么缺乏可量化与可比较性，难以在高风险场景中提供可信、可审计的解释。

Method: 在黑盒设置下定位对模型预测最关键的最小视觉子集（visual focuses），并将这些子集转写为紧凑的逻辑表达式以给出结构化解释；同时定义焦点精度、召回、散度等指标，用于跨场景量化模型的注意力与决策一致性。

Result: 实证显示该方法能揭示：训练过程导致注意力集中趋势；随泛化提升焦点准确度提高；在偏置与对抗攻击下会出现异常焦点；整体可提供透明、可比较的解释与诊断信号。

Conclusion: FocaLogic为视觉模型提供系统化、可扩展且可量化的解释路径，既能生成可读的逻辑规则，又能以指标客观评估与监测模型行为。

Abstract: Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.

</details>


### [53] [A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models](https://arxiv.org/abs/2601.12051)
*Weixin Ye,Wei Wang,Yahui Liu,Yue Song,Bin Ren,Wei Bi,Rita Cucchiara,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出一种用于联邦学习中Transformer的Masked Jigsaw Puzzle (MJP) 框架：通过随机打乱token并用可学习的unk位置嵌入屏蔽位置信息，既削弱梯度攻击对位置嵌入的利用，也提升CV与NLP任务性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在联邦学习中易受梯度攻击，特别是PE的梯度可泄露充分的输入重建信息；同时希望在不牺牲性能的前提下提升隐私与鲁棒性，且能统一适用于CV与NLP模型。

Method: 训练时对输入序列进行随机token打乱（jigsaw），并将被打乱token的PE以可学习的unk位置嵌入统一替换/掩蔽，从而破坏局部空间/顺序信息，使模型学习更依赖全局与内容特征；在不同Transformer（视觉与文本）上作为通用训练策略集成。

Result: 在ImageNet-1K图像分类、以及Yelp/Amazon情感分析等任务上，MJP提升了性能；对基于梯度的输入重建攻击显著增强鲁棒性（尤其针对PE梯度泄露）。

Conclusion: MJP通过“打乱+PE掩蔽”的简单统一策略，削弱位置嵌入泄露风险并提升泛化，成为适用于视觉与语言Transformer的通用隐私增强与性能增益框架；代码已开源。

Abstract: In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack

</details>


### [54] [Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation](https://arxiv.org/abs/2601.12052)
*Zaiyan Zhang,Jie Li,Shaowei Shi,Qiangqiang Yuan*

Main category: cs.CV

TL;DR: 提出TDP-CR：面向任务的多模态云去除与地物分割联合框架；用可学习“退化提示”引导光学与SAR自适应融合，仅在云遮挡区域引入SAR；两阶段、小参数训练；在LuojiaSET-OSFCR上同时提升重建质量与分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统云去除方法强调低层次像素保真，易过度平滑纹理与边界，导致与分析就绪数据（ARD）的语义需求不匹配；需要在保证视觉与重建质量的同时，提升下游语义任务（如分割）的可用性，并有效利用SAR在云遮挡下的补充信息。

Method: 提出TDP-CR联合优化云去除与土地覆盖分割。核心为提示引导融合（PGF）：学习“退化提示”表征云厚度与空间不确定性，结合全局通道上下文与局部、提示调制的空间偏置，仅在光学受损处自适应引入SAR信息。并提出参数高效的两阶段训练：先重建学习，后语义表示学习解耦，减少干扰与参数量。

Result: 在LuojiaSET-OSFCR数据集上，较重量级SOTA提升PSNR 0.18 dB，仅用其约15%的参数；对多任务对比方法，mIoU稳定提升1.4%。

Conclusion: TDP-CR通过提示引导的多模态自适应融合与解耦训练，同时改善重建保真与语义分割性能，生成更符合ARD需求的产品，并以较小参数量实现SOTA级综合表现。

Abstract: Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\% of the parameters, and achieves a 1.4\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.

</details>


### [55] [Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer](https://arxiv.org/abs/2601.12055)
*Lina Meyer,Felix Wissel,Tobias Knopp,Susanne Pfefferle,Ralf Fliegert,Maximilian Sandmann,Liana Uebler,Franziska Möckl,Björn-Philipp Diercks,David Lohr,René Werner*

Main category: cs.CV

TL;DR: 该论文提出AUTO-DIP：在荧光显微成像中通过“参数迁移”实现免优化的深度图像先验（DIP）降噪，利用校准集学习最优U-net结构与停止时刻，并按元数据相似度将参数转移到新图像，整体优于原始DIP与变分降噪，尤其在强噪声下。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习需要大规模标注数据且泛化受限；DIP虽免监督但需为每张图像反复搜索网络结构与早停点，计算耗时，难以在批量荧光显微图像中实用。作者猜想：相似显微图像的最优DIP参数也相似，可通过参数迁移避免逐图优化。

Method: 从开源荧光显微数据构建校准集(n=110)与验证集(n=55)；在校准集上进行面向U-net的架构搜索与最佳迭代停止点估计；在验证集中比较多种图像相似性准则，选择最能预测最优参数的准则；实现AUTO-DIP流水线：根据待测图像与校准库的相似性（尤其是元数据：显微镜类型、样本类型等）检索并转移对应的U-net与停止点，无需再对新图像优化；与原始DIP配置和图像特定的变分降噪基线比较。

Result: 基于仅元数据相似性的参数迁移，效果与基于定量图像相似性指标的迁移相当或更优；AUTO-DIP在多个复杂度不同的公开数据集上优于原始DIP与先进变分降噪方法，噪声越强优势越明显；在本地采集的荧光显微数据上同样取得领先。

Conclusion: 相似荧光显微图像共享可迁移的最优DIP参数，利用元数据驱动的参数转移可将DIP从“逐图像优化”变为“即用型”推理，显著降低计算成本并提升在强噪声条件下的表现，对显微成像批量处理具有实用价值。

Abstract: Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.

</details>


### [56] [Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2601.12062)
*Xiaomei Yang,Xizhan Gao,Antai Liu,Kang Wei,Fa Zhu,Guang Feng,Xiaofeng Qu,Sijie Niu*

Main category: cs.CV

TL;DR: 提出LSMRL，用语言提示驱动的视频可见光-红外行人再识别，结合高效时空建模、语义扩散与跨模态交互，并配以模态级损失，在多数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用CLIP语言提示的VVI-ReID虽准确但仍存在：时空建模效率不足、跨模态交互不充分、缺少显式模态级损失以指导模态不变表征学习。

Method: 提出LSMRL框架，含三大模块：1) STFL基于CLIP最小改动实现参数/计算高效的时空特征学习；2) SD将模态共享语言提示扩散到可见光与红外特征中，以建立初步模态一致性；3) CMI用双向跨模态自注意力进一步对齐并细化模态不变表征；同时设计两种模态级损失提升判别性与对未见类别的泛化。

Result: 在大规模VVI-ReID数据集上进行大量实验，LSMRL性能优于所有当前最先进方法（SOTA/AOTA）。

Conclusion: 语言驱动的序列级模态不变表示可通过高效时空建模、语义扩散与双向跨模态交互得到有效学习，结合模态级损失可显著提升VVI-ReID的准确性与泛化能力。

Abstract: The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features' discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.

</details>


### [57] [Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation](https://arxiv.org/abs/2601.12066)
*Zijie Lou,Xiangwei Feng,Jiaxin Wang,Xiaochao Qu,Luoqi Liu,Ting Liu*

Main category: cs.CV

TL;DR: 论文提出用随机桥模型将含目标的视频直接转译为去目标视频，避免从高斯噪声生成带来的结构先验丢失，并以自适应掩码调制在大目标去除与背景保真间取得平衡，显著提升画质与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 噪声到数据的扩散方法忽视输入视频的结构与上下文先验，导致去除不彻底或生成违背物理逻辑的内容，需要一种能充分利用原视频先验、且具备物理与时序一致性的解决方案。

Method: 将视频目标移除表述为视频到视频的随机桥（stochastic bridge）翻译：从源视频（含目标）到目标视频（去目标）建立随机路径，以输入视频作为强结构先验引导生成；并提出自适应掩码调制，根据掩码大小/形态动态调制输入嵌入，权衡背景忠实度与生成灵活性，特别提升大目标去除能力。

Result: 在广泛实验中，相较现有方法，本方法在视觉质量和时间一致性上显著优于对比基线，能够更精确地擦除目标并生成与环境物理逻辑相符的填充内容。

Conclusion: 随机桥视频到视频框架有效利用输入先验，缓解扩散法指导不足问题；配合自适应掩码调制，可在大目标场景中仍保持去除效果与背景一致性，整体显著优于现有方法。

Abstract: Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.

</details>


### [58] [ARMARecon: An ARMA Convolutional Filter based Graph Neural Network for Neurodegenerative Dementias Classification](https://arxiv.org/abs/2601.12067)
*VSS Tejaswi Abburi,Ananya Singhal,Saurabh J. Shigwan,Nitin Kumar*

Main category: cs.CV

TL;DR: 提出ARMARecon，一种结合ARMA图滤波与重构驱动目标的统一图学习框架，用白质FA直方图特征建模局部与全局连接，缓解过平滑，并在ADNI与NIFD上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AD与FTD沿白质网络以图依赖方式扩散，早期检测关键；现有图神经网络易过平滑、难兼顾局部与全局连接，且特征表达有限，需要新的框架提升判别与泛化能力。

Method: 在白质区域提取20-bin FA直方图作为节点特征；采用ARMA图滤波器以并行自回归-滑动平均分支刻画多尺度局部/全局连通；引入重构驱动（reconstruction-driven）目标，联合分类任务优化以增强表示并减轻过平滑；在多站点dMRI（ADNI、NIFD）上训练与评估。

Result: 在ADNI与NIFD数据集上，相较最新方法取得更高分类准确率（具体数值未给出），显示更强的判别能力与跨站点鲁棒性。

Conclusion: ARMARecon通过ARMA图滤波+重构目标有效整合局部/全局连接并缓解过平滑，提升白质基础的AD/FTD早期检测性能，优于现有基线。

Abstract: Early detection of neurodegenerative diseases such as Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is essential for reducing the risk of progression to severe disease stages. As AD and FTD propagate along white-matter regions in a global, graph-dependent manner, graph-based neural networks are well suited to capture these patterns. Hence, we introduce ARMARecon, a unified graph learning framework that integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation and improve classification accuracy. ARMARecon effectively models both local and global connectivity by leveraging 20-bin Fractional Anisotropy (FA) histogram features extracted from white-matter regions, while mitigating over-smoothing. Overall, ARMARecon achieves superior performance compared to state-of-the-art methods on the multi-site dMRI datasets ADNI and NIFD.

</details>


### [59] [CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation](https://arxiv.org/abs/2601.12076)
*H. Jiang,Y. Sun,Z. Dong,T. Liu,Y. Gu*

Main category: cs.CV

TL;DR: 提出RS-RVOS领域首个大规模因果标注基准RS-RVOS Bench，并提出记忆质量感知的在线指代分割框架MQC-SAM，通过运动一致性校准与动态记忆质量评估，缓解弱显著性与记忆误差累积，取得SOTA。


<details>
  <summary>Details</summary>
Motivation: RS视频中的目标显著性弱、遮挡与视野截断严重，导致分割中难以保持判别性表示；缺少专用大规模基准限制发展；现有方法存在初始记忆偏置与无差别记忆累积，引入噪声并导致错误传播。

Method: 数据：构建RS-RVOS Bench（111序列、约25k帧、213k时序指代标注），采用因果感知策略，仅基于初始帧状态撰写语言表达，避免利用未来信息。方法：提出MQC-SAM框架——1) 时间运动一致性模块进行初始记忆校准，利用短期运动轨迹先验校正结构偏差、建立可靠记忆锚点；2) 解耦注意力的记忆融合与动态质量评估，仅选择高置信语义特征更新，过滤遮挡/误分的低质量记忆，抑制错误累积。并以SAM作为分割基础。

Result: 在RS-RVOS Bench上进行大量实验，MQC-SAM获得SOTA表现，相比现有模型在定位与时序一致性上显著提升。

Conclusion: 构建了严格因果标注的大规模RS-RVOS基准，并提出记忆质量控制的在线分割框架，通过运动一致性与动态记忆筛选显著缓解错误传播问题，推动RS-RVOS研究发展。

Abstract: Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.

</details>


### [60] [EmoLat: Text-driven Image Sentiment Transfer via Emotion Latent Space](https://arxiv.org/abs/2601.12079)
*Jing Zhang,Bingjie Fan,Jixiang Zhu,Zhe Wang*

Main category: cs.CV

TL;DR: 提出 EmoLat：一种跨模态情感潜变量空间，结合文本语义与视觉情感，实现细粒度文本驱动的图像情感迁移；并发布含密集标注的新数据集 EmoSpace Set，方法在多指标上优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像情感编辑缺乏可控性与细粒度操作，难以将文本语义与视觉情感有效对齐，且缺少大规模、密集标注的数据支撑。

Method: 1) 构建 EmoLat 跨模态情感潜空间，并建立“情感-物体-视觉属性”的情感语义图以捕捉关系结构；2) 通过对抗式正则化对齐文本与图像的情感潜分布，提升判别性与可迁移性；3) 基于 EmoLat 设计跨模态情感迁移框架，联合嵌入文本与 EmoLat 特征；4) 采用多目标损失（语义一致性、情感对齐、对抗正则）端到端优化；5) 构建包含情感、物体、属性密集标注的大规模数据集 EmoSpace Set。

Result: 在 EmoSpace Set 上的广泛实验显示，相比现有方法，在定量指标和主观迁移保真度上显著提升，达成新的SOTA。

Conclusion: EmoLat 提供了可控、细粒度的文本驱动图像情感编辑新范式；对抗式跨模态对齐与情感语义图是关键；配套的 EmoSpace Set 数据集与代码开源，为后续研究提供基准与工具。

Abstract: We propose EmoLat, a novel emotion latent space that enables fine-grained, text-driven image sentiment transfer by modeling cross-modal correlations between textual semantics and visual emotion features. Within EmoLat, an emotion semantic graph is constructed to capture the relational structure among emotions, objects, and visual attributes. To enhance the discriminability and transferability of emotion representations, we employ adversarial regularization, aligning the latent emotion distributions across modalities. Building upon EmoLat, a cross-modal sentiment transfer framework is proposed to manipulate image sentiment via joint embedding of text and EmoLat features. The network is optimized using a multi-objective loss incorporating semantic consistency, emotion alignment, and adversarial regularization. To support effective modeling, we construct EmoSpace Set, a large-scale benchmark dataset comprising images with dense annotations on emotions, object semantics, and visual attributes. Extensive experiments on EmoSpace Set demonstrate that our approach significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing guided by textual input. The EmoSpace Set and all the code are available at http://github.com/JingVIPLab/EmoLat.

</details>


### [61] [Toward Real-World High-Precision Image Matting and Segmentation](https://arxiv.org/abs/2601.12080)
*Haipeng Zhou,Zhaohu Xing,Hongqiu Wang,Jun Ma,Ping Li,Lei Zhu*

Main category: cs.CV

TL;DR: 提出FCLM用于高精度前景解析（抠图/二元分割），通过深度感知蒸馏、域不变学习与面向对象解码器（支持视觉+语言提示）提升细节与跨域泛化，实验超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦显著的单前景且类无关，难以泛化到多类别目标；高质量标注稀缺导致依赖合成数据，域差异严重、对真实场景泛化差；交互式方法虽可调目标，但受限于类无关设计与数据问题。

Method: 1) 深度感知蒸馏：从深度/几何先验迁移深度相关知识，增强前景表示与边界细节。
2) 域不变学习：将合成数据处理为域自适应问题，学习对域变化鲁棒、聚焦前景的特征（如对齐分布/对抗或正则约束）。
3) 面向对象解码器：可接收视觉与语言提示（文本/点/框等），实现指代目标的交互式预测，输出高细节掩码。

Result: 在多个基准上，定量（指标提升）与定性（细节更佳、泛化更强）均优于现有SOTA；对真实场景与不同类别目标具有更好鲁棒性与可控交互表现。

Conclusion: FCLM通过深度知识蒸馏与域不变学习缓解数据与泛化难题，并以多模态交互解码器实现可指代的高精度前景解析，整体效果优于SOTA，适用于真实复杂场景与多类别对象。

Abstract: High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.

</details>


### [62] [Conditional Random Fields for Interactive Refinement of Histopathological Predictions](https://arxiv.org/abs/2601.12082)
*Tiffanie Godelaine,Maxime Zanella,Karim El Khoury,Saïd Mahmoudi,Benoît Macq,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 提出HistoCRF：在不重新训练模型的前提下，用条件随机场（CRF）后处理提升病理图像VLM零样本分类；通过新颖的成对势定义促进标签多样性并可利用专家标注/人机协作，跨5个数据集显著提高准确率（+16.0%无标注，+27.5%仅100条标注，人机循环达+32.6%）。


<details>
  <summary>Details</summary>
Motivation: VLM在病理图像的零样本分类虽强但仍不稳定、易误判；临床场景希望在无需再训练大模型、标注成本低的前提下提升一致性与精度，同时能利用少量专家反馈与人机协作迭代改进。

Method: 提出HistoCRF框架：将VLM的patch级预测作为CRF的单点势；重新设计成对势，既考虑空间邻近一致性，又引入“标签多样性”约束以避免过度平滑；框架可在三种设置运行——无标注、有限专家标注（用于引导或锚定标签/参数）、以及人机在环迭代修正误分patch；无需对VLM或CRF进行额外训练，只做后处理推断。

Result: 在5个器官/疾病的patch级分类数据集上，相比VLM零样本基线：无标注平均准确率提升16.0%；加入100条专家标注提升27.5%；同样标注量下，人机在环进一步提升至32.6%。

Conclusion: HistoCRF以零训练成本为病理VLM提供通用后处理增益，能有效结合少量专家标注与交互，显著提升不同数据集的Patch分类精度；适合在临床工作流中作为轻量、可迭代优化的组件使用。

Abstract: Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.

</details>


### [63] [Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data](https://arxiv.org/abs/2601.12090)
*Matej Mok,Lukáš Gajdošech,Michal Mesároš,Martin Madaras,Viktor Kocur*

Main category: cs.CV

TL;DR: 提出一种针对工业箱体的6DoF位姿估计方法：在点云上检测箱体上沿3D线段，再通过几何求解位姿；用合成+真实数据训练，精度优于现SOTA，且无需实例CAD。


<details>
  <summary>Details</summary>
Motivation: 工业场景中6DoF位姿估计常受限于：需要大量标注或CAD模型、数据稀缺、实例多变。针对广泛存在的箱/料箱（具有规则长方体几何）这一关键对象，现有方法泛化性与数据需求矛盾明显，需开发更鲁棒、低数据依赖的方法。

Method: 1) 利用料箱为长方体的先验，聚焦其顶部边缘。2) 将2D线段检测网络LeTR扩展到结构化点云输入，检测上沿对应的3D线段。3) 以检测到的3D线段为中间表示，采用简洁稳健的几何过程恢复箱体6DoF位姿。4) 扩充现有数据集并新采集标注一套公开数据；引入合成数据以增强训练。

Result: 在真实扫描上，使用合成数据训练能显著提升精度；整体方法在无需实例级CAD的前提下，位姿精度达到约3 cm平移误差与8.2°旋转误差，并显著优于当前SOTA方法。

Conclusion: 面向工业箱体，基于3D线段中间表示与几何求解的方案，在数据需求与泛化间取得良好平衡：无需实例CAD、可用合成数据强化训练，并在真实场景中实现更高精度。

Abstract: The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\circ$ rotation error) while not requiring instance-specific CAD models during inference.

</details>


### [64] [Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification](https://arxiv.org/abs/2601.12109)
*Larissa Ferreira Rodrigues Moreira,Rodrigo Moreira,Leonardo Gabriel Ferreira Rodrigues*

Main category: cs.CV

TL;DR: 通过知识蒸馏与小模型集成，在受限设备上实现高精度、低能耗的咖叶病害诊断。


<details>
  <summary>Details</summary>
Motivation: 田间咖啡叶病诊断困难且设备受限、网络不稳定；现有高精度AI模型算力与能耗高，不利于在IoT端部署，需探索可持续的端侧诊断方案。

Method: 以高容量CNN为教师，在数据中心训练后，通过知识蒸馏向紧凑CNN学生模型迁移；再利用集成学习（包含简单与优化的集成策略）构建“致密微型模型对（dense tiny pairs）”的轻量级集成，以满足算力与能耗约束。

Result: 在自建咖啡叶数据集上，蒸馏后的微型集成在精度上与已有工作相当，同时显著降低能耗与碳足迹。

Conclusion: 恰当的知识蒸馏与轻量级模型集成可在严格资源约束下提供实用的IoT场景病害诊断方案，实现精度-能耗的兼顾。

Abstract: Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.

</details>


### [65] [RCDN: Real-Centered Detection Network for Robust Face Forgery Identification](https://arxiv.org/abs/2601.12111)
*Wyatt McCurdy,Xin Zhang,Yuqi Song,Min Gao*

Main category: cs.CV

TL;DR: 提出RCDN：以“真实中心”为核心的频域-空间双分支+Xception骨干的人脸伪造检测网络，优先建模真实图像一致性而非伪造多样性，在DiFF数据集三类伪造（FE/I2I/T2I）上同时取得SOTA域内精度与显著更强的跨域泛化，缩小泛化鸿沟并提升跨/域内稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有人脸伪造检测器在同域可达近乎完美，但跨域显著退化；新伪造方法不断涌现，要求检测器对未见分布具备鲁棒性。与其追逐不断变化的伪造痕迹，不如聚焦“真实”的稳定统计特征，围绕真实样本构建更稳健的表示空间。

Method: - 架构：频率-空间CNN框架，Xception为骨干，双分支（频域分支提取频谱伪迹/稳定性信号，空间分支捕捉纹理与结构）。
- 训练目标：引入Real-Centered Loss，使特征分布以真实图像为中心聚集，并与伪造样本分离；强调真实一致性而非枚举伪造模式。
- 表征策略：通过对齐真实分布、抑制域偏移，提升在分布变化（跨数据集、跨伪造类型）下的鲁棒性。

Result: 在DiFF数据集三类代表性伪造（FE、I2I、T2I）上：
- 域内（in-domain）性能达SOTA；
- 跨域（cross-domain）显著优于主流基线；
- 明显缩小泛化性能差距（generalization gap），并获得最高的跨/域内稳定性比。

Conclusion: 以“真实为中心”的学习范式能在不追逐特定伪造痕迹的情况下，带来更好的跨域泛化与稳定性。RCDN作为实用方案，有望更好抵御不断演化与未见的图像伪造技术。

Abstract: Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.

</details>


### [66] [CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction](https://arxiv.org/abs/2601.12119)
*Xiaotong Zhou,Zhenhui Yuan,Yi Han,Tianhua Xu,Laurence T. Yang*

Main category: cs.CV

TL;DR: CARLA-Round提出一个系统化的环岛车辆轨迹预测仿真数据集，控制气象与交通密度两大因素（5×5=25场景），并用标准基线验证不同因素对预测难度的影响，展示可量化的结论与良好的仿真到真实迁移效果。


<details>
  <summary>Details</summary>
Motivation: 环岛场景预测难，因为几何环形结构、持续并线/让行交互、无信号灯导致行为复杂；现实数据稀缺且受遮挡与混杂因素影响，难以隔离因素来评估算法。需要一个可控、可复现实验平台与带显式标注的数据集。

Method: 基于CARLA构建CARLA-Round：系统化变化天气（5类）与交通密度（LoS A–E）形成25个受控场景；注入多样且逼真的驾驶行为并提供显式标注。以LSTM、GCN、GRU+GCN等标准基线进行评测，分析不同条件对ADE等指标的影响，并在真实rounD数据上验证迁移。

Result: 实验显示交通密度对预测难度具有主导且单调增强的影响；天气影响呈非线性。最佳模型在真实rounD数据集上达到0.312 m ADE，显示有效的仿真到真实迁移。

Conclusion: 系统化受控的数据设计能定量拆解影响因素（密度、天气）对环岛轨迹预测的作用，克服现实数据混杂难题；CARLA-Round为评测与方法开发提供了可复现实验基准，并具备良好的sim-to-real潜力。

Abstract: Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.

</details>


### [67] [Segment and Matte Anything in a Unified Model](https://arxiv.org/abs/2601.12147)
*Zezhong Fan,Xiaohan Li,Topojoy Biswas,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: SAMA在SAM基础上做轻量扩展，引入多视角定位编码器与本地适配器，联合输出分割与抠图，显著提升边界精度与交互式质量，在多项基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: SAM虽具零样本与灵活提示，但精度不足，特别是边界细节；现有后处理/精炼模块分散且难以统一，同时交互式图像抠图尚未与SAM打通。分割与抠图具强相关性，促成统一模型需求。

Method: 在SAM上增加少量参数：1) 多视角定位编码器（MVLE）从局部视图提取细粒度特征以捕获细节；2) 本地化适配器（Local-Adapter）对掩码进行细化以恢复微小边界；3) 双任务头，分别预测分割掩码与alpha抠图，同时训练。使用聚合的公开多源数据进行训练。

Result: 在多项分割与抠图基准上获得SOTA，表现出较强的适配性与泛化能力；在交互式场景下生成高质量掩码与alpha。

Conclusion: 通过最小增参，SAMA实现分割与抠图的统一与高精度边界刻画，适合广泛下游任务，验证分割-抠图一体化的有效性。

Abstract: Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.

</details>


### [68] [Principal Component Analysis-Based Terahertz Self-Supervised Denoising and Deblurring Deep Neural Networks](https://arxiv.org/abs/2601.12149)
*Pengfei Zhu,Xavier Maldague*

Main category: cs.CV

TL;DR: 提出一种针对太赫兹(THz)幅值图像的自监督去噪去模糊网络THz-SSDD，利用“再污染到再污染”策略与PCA分解/重建，同时抑制高频噪声与低频模糊，在少量无标注数据上跨多样本泛化并提升画质且保持物理一致性。


<details>
  <summary>Details</summary>
Motivation: THz成像存在频率相关退化：低频端模糊、高清端噪声强，传统方法难以同时处理且需手工调参，因为去噪与去模糊的边界未知。需要一种无需干净真值、能自动权衡两者的学习方法。

Method: 采用Recorrupted-to-Recorrupted自监督框架：对同一噪声观测施加不同再污染，学习对噪声扰动不变的表征；结合PCA对频域或特征域进行主成分分解与重建，将低/高频成分分别约束，实现联合去噪与去模糊；仅用少量无标注噪声图训练。

Result: 在四类不同材料与测量模式的样本上测试，网络实现有效去噪与去模糊；定量指标提升，图像质量改善，同时保持原始信号的物理特性。

Conclusion: THz-SSDD无需干净标签即可联合处理THz图像的高频噪声与低频模糊，具备跨样本泛化能力与物理一致性，为THz成像质量提升提供实用方案。

Abstract: Terahertz (THz) systems inherently introduce frequency-dependent degradation effects, resulting in low-frequency blurring and high-frequency noise in amplitude images. Conventional image processing techniques cannot simultaneously address both issues, and manual intervention is often required due to the unknown boundary between denoising and deblurring. To tackle this challenge, we propose a principal component analysis (PCA)-based THz self-supervised denoising and deblurring network (THz-SSDD). The network employs a Recorrupted-to-Recorrupted self-supervised learning strategy to capture the intrinsic features of noise by exploiting invariance under repeated corruption. PCA decomposition and reconstruction are then applied to restore images across both low and high frequencies. The performance of the THz-SSDD network was evaluated on four types of samples. Training requires only a small set of unlabeled noisy images, and testing across samples with different material properties and measurement modes demonstrates effective denoising and deblurring. Quantitative analysis further validates the network feasibility, showing improvements in image quality while preserving the physical characteristics of the original signals.

</details>


### [69] [Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models](https://arxiv.org/abs/2601.12150)
*Mengxuan Hu,Zihan Guan,John Kang,Sheng Li,Zhongliang Zhou*

Main category: cs.CV

TL;DR: 提出一种针对病理全视野图像（WSI）高分辨率推理的高效注意力稀疏化策略，降低显存与时延，同时保持或提升下游分类/分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型多固定输入尺寸（如224×224），在覆盖多尺度、超高分辨率的WSI上推理低效。简单放大输入耗费GPU内存，下采样又改变像素分辨率、损失关键形态学细节，因此需要在不牺牲分辨率的前提下提升推理效率。

Method: 在推理阶段引入空间感知的邻域块稀疏注意力，并基于全局注意力分数过滤非信息性token；以此降低注意力计算与内存占用，实现对高分辨率WSI更高效的处理。

Result: 在保持甚至提升性能的同时显著降低GPU内存与推理时间：ROI分类最多提升7.67%，分割结果与基线相当；能够在相同GPU预算下进行更高分辨率推理。

Conclusion: 所提稀疏注意力与token过滤策略在WSI高分辨率推理中实现空间与时间效率的统一，避免放大输入或下采样带来的问题，提升分类性能并维持分割表现。

Abstract: Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.

</details>


### [70] [Inverse Rendering for High-Genus 3D Surface Meshes from Multi-view Images with Persistent Homology Priors](https://arxiv.org/abs/2601.12155)
*Xiang Gao,Xinmu Wang,Yuanpeng Liu,Yue Wang,Junqi Huang,Wei Chen,Xianfeng Gu*

Main category: cs.CV

TL;DR: 提出一种结合多视图光度一致性与持久同调先验的协同逆渲染方法，用拓扑约束稳定重建高亏格（三环、把手等）几何，显著降低CD并提升Volume IoU。


<details>
  <summary>Details</summary>
Motivation: 多视图到3D的逆问题因几何、外观与拓扑歧义而病态，尤其高亏格结构（隧道、把手）易坍塌或丢失。现有网格/神经方法缺少显式拓扑约束，导致灾难性拓扑失败。需要把拓扑信息纳入优化以提高稳健性与精度。

Method: 在网格逆渲染框架中进行基于梯度的优化，不使用神经网络。目标函数由两部分协同：1) 多视图光度一致性项驱动几何贴合影像；2) 持久同调先验（通过计算持久条形码/Betti数相关特征）对隧道、把手等拓扑特征施加约束，引导保持或形成目标拓扑。通过同调引导避免隧道坍塌与高亏格结构丢失。

Result: 在与最新网格方法比较中，加入持久同调先验后，Chamfer Distance更低、Volume IoU更高，表现为更准确的几何与更少的拓扑失败，特别是在高亏格对象上优势明显。

Conclusion: 将持久同调先验融入逆渲染优化能有效缓解图像到3D重建的拓扑与几何歧义，稳健重建高亏格结构，较SOTA网格法在精度与鲁棒性上更优。

Abstract: Reconstructing 3D objects from images is inherently an ill-posed problem due to ambiguities in geometry, appearance, and topology. This paper introduces collaborative inverse rendering with persistent homology priors, a novel strategy that leverages topological constraints to resolve these ambiguities. By incorporating priors that capture critical features such as tunnel loops and handle loops, our approach directly addresses the difficulty of reconstructing high-genus surfaces. The collaboration between photometric consistency from multi-view images and homology-based guidance enables recovery of complex high-genus geometry while circumventing catastrophic failures such as collapsing tunnels or losing high-genus structure. Instead of neural networks, our method relies on gradient-based optimization within a mesh-based inverse rendering framework to highlight the role of topological priors. Experimental results show that incorporating persistent homology priors leads to lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failure.

</details>


### [71] [VIRTUE: Versatile Video Retrieval Through Unified Embeddings](https://arxiv.org/abs/2601.12193)
*Shaunak Halbe,Bhagyashree Puranik,Jayakrishnan Unnikrishnan,Kushan Thakkar,Vimal Bhat,Toufiq Parag*

Main category: cs.CV

TL;DR: 提出VIRTUE：一种以MLLM为骨干、统一支持语料级检索、片段定位与多模态组合查询的多才视频检索框架；通过共享MLLM生成的视觉/文本嵌入进行对比对齐与高效候选检索，并辅以重排序训练，在零样本与组合检索上达SOTA或接近专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有专用视频检索模型虽性能强，但仅限单一模态或任务，难以处理灵活的多模态组合查询；而MLLM方法虽灵活，但检索精度显著落后。需要一种兼具通用多模态表达与高性能检索的统一框架。

Method: 以共享的MLLM作为视觉与文本编码器，通过对比学习对齐两者嵌入，先做嵌入式候选召回；嵌入模型用LoRA在70万对视听-文本样本上高效微调；同一模型可零样本迁移到片段（moment）检索与组合检索；在候选上再训练一个重排序模块以提升最终排名。

Result: 在零样本视频检索上优于其他MLLM方法；无需额外训练即可在零样本片段检索达到有竞争力表现，并在零样本组合视频检索上达SOTA；加入重排序训练后总体性能大幅超越现有MLLM检索系统，并接近或可比使用数量级更大数据训练的专用模型。

Conclusion: VIRTUE证明了基于MLLM的统一视频检索在保持多模态灵活性的同时，可通过对比对齐与高效微调获得接近专用系统的性能；同一模型可覆盖检索与定位等多任务，并通过轻量重排序进一步提升，减少对海量专用数据与多模型管线的依赖。

Abstract: Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.

</details>


### [72] [Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion](https://arxiv.org/abs/2601.12224)
*Meng Wei,Kun Yuan,Shi Li,Yue Zhou,Long Bai,Nassir Navab,Hongliang Ren,Hong Joo Lee,Tom Vercauteren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SurgRef提出以运动为导向的语言指引外科器械指代分割方法，并配套Ref-IMotion视频数据集，实现跨术式、跨机构的更强泛化与SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有外科视频中的指代分割依赖静态外观与预定义器械名，难以在遮挡、外观变化与非标准术语下泛化；需要一种能利用时序运动与交互信息来对自然语言描述进行稳健定位。

Method: 提出SurgRef：将自由文本表达与器械运动表征对齐，围绕“如何移动与交互”而非“长什么样”进行时序建模与指代分割；并构建Ref-IMotion多机构、多样化视频数据集，提供稠密时空掩码与以运动为核心的语言标注，用于训练与评测。

Result: 在多种外科手术场景中取得SOTA精度与更强的跨域泛化能力，能在遮挡、术语不一致或不熟悉器械时仍准确分割定位。

Conclusion: 以运动驱动的语言—视觉对齐是提升外科视频指代分割鲁棒性的关键；SurgRef与Ref-IMotion为智能手术室与自主外科机器人的人机自然交互奠定基线与数据基础。

Abstract: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.

</details>


### [73] [DiffusionQC: Artifact Detection in Histopathology via Diffusion Model](https://arxiv.org/abs/2601.12233)
*Zhenzhen Wang,Zhongliang Zhou,Zhuoyu Wen,Jeong Hwan Kook,John B Wojcik,John Kang*

Main category: cs.CV

TL;DR: 提出DiffusionQC：用扩散模型把组织病理图像中的伪影视为离群点进行检测，仅需干净图像训练，无需伪影标注；再引入对比学习扩大干净/伪影分布间隔，跨染色泛化、数据与标注需求更少、性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 病理切片在制片与数字化过程中易引入各类伪影，影响诊断与下游分析。现有监督方法依赖大量标注、对新型伪影泛化差、成本高，因此需要一种无需伪影标注、能识别未知伪影且具跨染色泛化的质量控制方法。

Method: 训练阶段：仅用“干净”图像训练扩散模型，使其学习干净分布；同时加入对比学习模块，构造正/负对比以显式拉大干净与伪影（离群）在表示空间的间隔。推理阶段：对输入图像计算与干净分布的偏离度（如重建误差/似然或表示距离）并将高偏离样本判为伪影；方法不需要像素级伪影掩码与预定义伪影类型。

Result: 在多数据集上优于现有SOTA伪影检测/质量控制方法，并在跨染色设置下保持良好泛化；与之相比，所需数据量与人工标注显著减少。

Conclusion: DiffusionQC可将病理图像伪影作为离群点有效检测，具备少标注、强泛化与跨染色能力；加入对比学习后进一步提升干净/伪影可分性与检测性能，适用于实际病理质控流程。

Abstract: Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.

</details>


### [74] [Less is More: Label-Guided Summarization of Procedural and Instructional Videos](https://arxiv.org/abs/2601.12243)
*Shreya Rajpal,Michal Golovanesky,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 提出PRISM三阶段框架，通过多模态语义理解与LLM校验，生成面向流程的关键帧摘要；在采样<5%帧的情况下保留84%语义内容，较基线提升最高33%。


<details>
  <summary>Details</summary>
Motivation: 传统视频摘要多依赖低层视觉特征，难以把握语义与时序脉络；即便使用视觉-语言模型，也可能产生泛化差、幻觉与流程不连贯的问题。在手术训练等高风险场景，需要既语义准确又流程一致的摘要。

Method: PRISM三阶段：1) 自适应视觉采样，减少冗余并覆盖潜在转场；2) 基于标签的关键帧锚定，以任务/步骤标签驱动选择语义关键帧；3) 利用LLM做上下文一致性校验与过滤，剔除泛化或幻觉内容，保证步骤间过渡合理。适用于通用教学与领域特定视频。

Result: 在教学与活动数据集上评估；教学集有参考摘要。尽管只采样<5%帧，摘要保留84%语义信息；相较基线在若干指标上最高提升33%，在语义对齐与精确度上表现强。

Conclusion: PRISM能在极低采样率下生成语义扎实、流程一致的摘要，跨程序化与领域特定任务具备良好泛化，显著优于基线。

Abstract: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.

</details>


### [75] [An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion](https://arxiv.org/abs/2601.12249)
*Ehsan Sadeghi Pour,Mahdi Esmaeili,Morteza Romoozi*

Main category: cs.CV

TL;DR: 提出一种结合PAAC与Transformer的乳腺X线恶性肿块检测框架，采用多尺度特征融合与Dice+Focal损失，在INbreast、MIAS、DDSM上达98.5%准确率并优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早诊对预后至关重要，但传统方法在复杂场景下对良恶性区分与多尺度病灶表征不足，且容易受类不平衡影响。需要一个能捕获长程依赖、多尺度细节并提升困难样本学习的模型。

Method: 构建融合Pyramid Adaptive Atrous Convolution（PAAC）与Transformer的架构：通过多尺度特征融合增强良恶性组织特征提取；引入自注意力处理长程依赖；采用Dice Loss与Focal Loss的组合以缓解类不平衡并提升边界与难例学习。对INbreast、MIAS、DDSM数据进行增强、对比度提升，并统一至227×227训练。与BreastNet、DeepMammo、Multi-Scale CNN、Swin-Unet、SegFormer对比。

Result: 在综合数据集上取得Accuracy 98.5%、Sensitivity 97.8%、Specificity 96.3%、F1 98.2%、Precision 97.9%，整体优于所列基线模型。

Conclusion: PAAC+Transformer结合多尺度融合与Dice+Focal损失能在复杂乳腺X线场景中高效准确检测恶性肿块，优于传统与部分SOTA方法，具备集成到临床诊断系统的潜力。

Abstract: Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%, specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.

</details>


### [76] [Federated Joint Learning for Domain and Class Generalization](https://arxiv.org/abs/2601.12253)
*Haoran Xu,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: 提出FedDCG：在联邦学习中同时处理类泛化与域泛化，通过域分组+类泛化网络训练与推理时按域相似性加权聚合，结合可学习增强与解耦机制，在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型参数巨大，微调成本高；且多数方法只解决未见类或未见域其中之一，缺乏统一框架在联邦环境中同时提升两者的泛化能力。

Method: 1) 联邦框架下先进行域分组；2) 在每个域组内训练“类泛化网络”，以减少跨域导致的决策边界混淆；3) 推理阶段根据与目标样本的域相似性对各组的类泛化输出进行聚合；4) 采用可学习网络模块增强类泛化能力；5) 通过解耦机制分离通用与域特定表征，提升对未见域的适应性。

Result: 在多种数据集与设置上，FedDCG在准确率与鲁棒性方面均超过当前最先进基线。

Conclusion: 在联邦学习场景下，联合考虑类与域泛化是有效的；通过域分组、类泛化训练与域相似性聚合，以及表征解耦，能够显著提升对未见类与未见域的泛化性能。

Abstract: Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.

</details>


### [77] [Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy](https://arxiv.org/abs/2601.12257)
*Fadlullah Raji,John Murray-Bruce*

Main category: cs.CV

TL;DR: 提出一种从普通被动NLOS照片重建隐藏场景3D结构的方法，通过新的光传输重构将场景分解为遮光与非遮光成分，形成可分离的非线性最小二乘问题，并给出基于梯度优化与物理启发的SSD神经网络两种解法，实验与仿真均有效且对噪声/环境光鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统成像需视线直达，许多场景下不可行；被动NLOS利用墙上微弱阴影进行重建，但目前局限于1D/低分辨率2D或仅定位已知形状目标，缺乏高维、通用的三维重建能力。

Method: 重新表述光传输模型，将隐藏场景分解为“遮光(occluding)”与“非遮光(non-occluding)”两部分，得到可分离的非线性最小二乘（SNLLS）逆问题；提出两种解法：(1) 基于梯度的优化求解；(2) 物理启发的神经网络Soft Shadow Diffusion（SSD），在模拟数据训练。

Result: 在多个真实实验3D场景上成功重建隐藏三维结构；SSD在只用仿真训练的情况下对未见类别和真实NLOS场景具有良好泛化，并对噪声与环境光表现出显著鲁棒性。

Conclusion: 通过SNLLS建模与SSD/梯度优化两条路径，实现从单张被动NLOS照片进行三维重建，突破了以往被动NLOS仅限低维/先验强的限制，展示了实用性与鲁棒性。

Abstract: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.

</details>


### [78] [AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search](https://arxiv.org/abs/2601.12272)
*Shahrzad Esmat,Mahdi Banisharif,Ali Jannesari*

Main category: cs.CV

TL;DR: 提出AgenticPruner：用多智能体+大模型迭代学习，在满足严格MAC预算的前提下进行剪枝，稳定收敛并保持/提升精度，适配CNN与ViT，实测达到容差带内的MAC控制与速度收益。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝多聚焦参数量，难以直接约束计算量（MAC），导致部署时延不可预测；需要一种能在给定MAC预算下自动收敛、并兼顾精度与实际速度的策略。

Method: 多智能体框架：Profiling Agent统计并分析模型结构与各层MAC分布；Master Agent编排流程并监控发散；Analysis Agent（基于Claude 3.5 Sonnet）通过历史尝试和上下文学习策略，结合同构剪枝的图结构分组，在多轮迭代中自适应调整各层剪枝率以逼近目标MAC，并在用户容差带内收敛。对比网格搜索，利用in-context learning提升收敛成功率。

Result: 在ImageNet-1K上验证：ResNet-50在1.77G MAC下达77.04%（较基线+0.91%）；ResNet-101在4.22G MAC下达78.94%（+1.56%）；ConvNeXt-S剪到8.17G MAC，GPU加速1.41x、CPU 1.07x、参数-45%；ViT族在用户设定的容差带内（+1%~+5%超、-5%~-15%欠）达成MAC预算。

Conclusion: AgenticPruner能在严格MAC预算下实现稳定收敛的结构化剪枝，较传统搜索更高效，并在CNN/ViT上维持或提升精度与推理效率，具备面向部署的可行性。

Abstract: Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning's graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.
  We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.

</details>


### [79] [CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training](https://arxiv.org/abs/2601.12282)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 提出CytoCLIP，以视觉-语言对比学习自动识别胚胎期人脑NISSL切片中的细胞结构区域；两种分辨率两条分支，显著优于现有方法（F1：全区域0.87，高分辨率瓦片0.91）。


<details>
  <summary>Details</summary>
Motivation: 手工在脑组织学切片上依据细胞结构边界划分脑区既耗时又需专家经验，限制了大规模和跨年龄/切面的一致分析。需要自动化方法在不同发育阶段与成像条件下泛化地识别脑区细胞架构。

Method: 基于CLIP的视觉-语言框架，构建CytoCLIP：低分辨率分支学习整体区域模式，高分辨率瓦片分支学习细胞级细节；以胚胎期不同孕周的人脑NISSL切片构建配对图像-文本数据集（低分辨率86类，高分辨率384类）；通过对比学习获得联合表征，并在区域分类与跨模态检索上评估泛化（跨年龄、切面）。

Result: 在多种数据设定（不同年龄与切面）下，CytoCLIP在区域分类与跨模态检索均优于现有方法；F1：全区域分类0.87，高分辨率瓦片分类0.91。

Conclusion: 视觉-语言对比学习能有效捕捉脑组织细胞架构的区域特征，CytoCLIP在不同尺度上实现准确、可泛化的脑区识别，为自动化神经组织学标注与检索提供新工具。

Abstract: The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.

</details>


### [80] [SDiT: Semantic Region-Adaptive for Diffusion Transformers](https://arxiv.org/abs/2601.12283)
*Bowen Lin,Fanjiang Ye,Yihua Liu,Zhenghui Guo,Boyuan Zhang,Weijian Zheng,Yufan Xu,Tiancheng Xing,Yuke Wang,Chengming Zhang*

Main category: cs.CV

TL;DR: 提出SDiT：在不改模型与不再训练的前提下，用语义区域自适应的调度与精炼，让DiT按区域难度分配算力，最高加速约3倍且质量几乎不变。


<details>
  <summary>Details</summary>
Motivation: DiT在文本生成图像上表现卓越，但推理代价高：扩散去噪迭代多步且全局注意力为二次复杂度。作者观察到去噪的空间动态不均匀——背景收敛快、边缘与纹理区域变化活跃，因此有望按区域差异做精细化计算以节省成本。

Method: 提出训练免（training-free）的SDiT框架：1）用快速Quickshift分割做语义感知聚类，得到区域。2）基于区域复杂度（活跃度）进行调度，只对信息量高的区域频繁更新，低复杂度区域少更新或跳步。3）边界感知精炼策略，保证跨区域的一致性与边界细节不被破坏。整个流程无需模型改动或再训练。

Result: 在标准DiT推理上直接应用即可，得到最高约3.0×的速度提升，同时在感知质量与语义一致性上与全注意力推理几乎无差异。

Conclusion: 空间非均匀的去噪动态可被有效利用。通过语义分割、复杂度驱动调度与边界精炼，SDiT在零改动、零再训练的条件下显著加速DiT且保持质量，为扩散模型的高效推理提供了通用方案。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.

</details>


### [81] [LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines](https://arxiv.org/abs/2601.12285)
*Safa C. Medin,Gengyan Li,Ziqian Bai,Ruofei Du,Leonhard Helminger,Yinda Zhang,Stephan J. Garbin,Philip L. Davidson,Gregory W. Wornell,Thabo Beeler,Abhimitra Meka*

Main category: cs.CV

TL;DR: 提出一种基于参数化人脸模型锚定的辐射场的新表示，把高写实3D人脸头像转为可在传统图形管线高效渲染的显式层叠网格与纹理；训练时学习3D辐射流形并提取网格与外观/形变纹理，部署时用线性混合与alpha合成在静态网格上可控重现头发、皮肤、眼睛等细节。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF等隐式表示虽写实且可控，但推理慢、难以在传统GPU/移动端实时渲染与在线传输，且常需定制渲染与工程集成。需要一种既保留写实度与可控性的表示，又能在传统网格-着色器平台高效、即插即用地运行与流式化。

Method: 在注册（enrollment）阶段：以参数化人脸模型为锚，学习一组位于3D空间的辐射流形（radiance manifolds），从中提取显式的分层网格（layered mesh），并联合估计外观纹理与形变（warp）纹理。部署阶段：保持静态网格，利用简单的线性混合和alpha合成对多层纹理进行控制与动画驱动，实现包含头发、皮肤、眼睛等的体渲染效果，但通过经典网格/着色器流水线实现。

Result: 得到的头像可以可控地重现复杂面部要素，支持通过纹理混合进行动画；相比隐式体渲染，显著提高渲染与传输效率，可在传统平台上实时或高效运行，无需特殊集成。

Conclusion: 通过把锚定辐射场蒸馏为显式层叠网格与纹理，本方法在写实度、可控性与系统可部署性之间取得平衡，使高写实3D人脸头像可在传统图形管线中高效流式化与渲染。

Abstract: We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.

</details>


### [82] [Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations](https://arxiv.org/abs/2601.12303)
*Shizhan Gong,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: 提出PCBM-ReD：在不改动已训练模型的前提下，通过表示分解把“概念瓶颈”结构后置到黑箱模型上，实现可解释且高精度的图像分类。


<details>
  <summary>Details</summary>
Motivation: 现有概念解释方法要么后验相关性不可靠、概念定义非视觉或标注成本高，要么CBM需在训练时介入且常假设与模型/数据无关，导致在关键场景部署受限。需要一种能在预训练黑箱上自动发现、命名、筛选并独立化视觉概念，同时保持性能的方案。

Method: 提出PCBM-ReD流水线：1) 从预训练编码器自动提取候选视觉概念（表示原子/方向）。2) 借助多模态大语言模型为概念生成/过滤标签，依据可视可辨性与任务相关性筛选。3) 通过重构引导的优化选择近独立子集。4) 利用CLIP的图文对齐，将图像表示分解为概念嵌入的线性组合，从而适配概念瓶颈模型。

Result: 在11个图像分类任务上达到SOTA或接近端到端模型的准确率，显著缩小性能差距；同时提供更好的可解释性（概念可视、可命名、与预测相关且更独立）。

Conclusion: PCBM-ReD可在现成黑箱模型上“后置”概念瓶颈，以自动化、可扩展的概念提炼与表示分解获得强性能与高可解释性，为关键领域部署提供可行途径。

Abstract: Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.

</details>


### [83] [A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models](https://arxiv.org/abs/2601.12304)
*Wutao Chen,Huaqin Zou,Chen Wan,Lifeng Huang*

Main category: cs.CV

TL;DR: 提出2S-GDA，两阶段全局多样化攻击框架，在黑盒下显著提升对VLP模型的攻击成功率，提升最高达11.17%，且模块化、可与现有方法结合以增强可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对抗攻击在黑盒场景下效果受限，主要因为扰动多样性不足与多阶段流程不稳定，导致对VLP模型的迁移性与成功率不高。

Method: 两阶段全局多样化：1) 文本阶段通过“候选文本扩展 + 全局感知替换”注入多样化文本扰动，提高跨模型迁移；2) 视觉阶段利用多尺度重采样与块洗牌旋转生成图像级扰动，增强视觉多样性与鲁棒性。框架为模块化，可与现有攻击策略组合。

Result: 在多种VLP模型与黑盒设置下，较SOTA方法稳定提升攻击成功率，最高提升11.17%，表现出更强的可迁移性与稳定性。

Conclusion: 2S-GDA能在无需白盒信息的情况下，通过全局多样化的文本与图像扰动，稳定提升对VLP模型的黑盒对抗攻击效果，且作为模块化组件可进一步增强现有方法。

Abstract: Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.

</details>


### [84] [Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification](https://arxiv.org/abs/2601.12308)
*Anurag Kaushish,Ayan Sar,Sampurna Roy,Sudeshna Chakraborty,Prashant Trivedi,Tanupriya Choudhury,Kanav Gupta*

Main category: cs.CV

TL;DR: 提出AMC-MetaNet，用相关性引导的多尺度与元学习，在无需大型预训练的情况下，实现高效少样本遥感分类，参数仅约60万、推理<50ms，并在多数据集5-way 5-shot上达86.65%准确率。


<details>
  <summary>Details</summary>
Motivation: 遥感少样本学习受限于标注稀缺、跨域分布偏移以及地物多尺度变化，现有方法多依赖大模型/Transformer与重型预训练，计算成本高且泛化不足，亟需轻量、可扩展、具多尺度感知与良好跨域能力的方案。

Method: 提出AMC-MetaNet：1) 相关性引导的特征金字塔，显式建模尺度不变模式；2) 自适应通道相关模块（ACCM），动态学习跨尺度通道关系；3) 相关性引导的元学习，以相关模式替代传统原型均值作为度量/匹配基础。模型从零训练，参数约600K，无需外部预训练或Transformer。

Result: 在EuroSAT、NWPU-RESISC45、UC Merced、AID等上进行5-way 5-shot评测，最高达86.65%准确率；效率方面单图推理<50ms，参数量约为ResNet-18的1/20。

Conclusion: AMC-MetaNet兼具轻量、高效与多尺度感知，能在标注稀缺与域偏移场景下取得强少样本性能，适用于真实遥感应用。

Abstract: Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.

</details>


### [85] [CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding](https://arxiv.org/abs/2601.12312)
*Yongjun Jeon,Jongmin Shin,Kanggil Park,Seonmin Park,Soyoung Lim,Jung Yong Kim,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: 提出CurConMix/CurConMix+框架与新数据集LLS48，用课程引导对比学习与多分辨率时序Transformer来识别手术动作三元组，并在CholecT45与LLS48上超越SOTA且具跨层泛化。


<details>
  <summary>Details</summary>
Motivation: 手术流程理解与技能评估需要精细到“器械-动作-解剖目标”的三元组识别，但受类别极不平衡、细微视觉差异、以及三元组组分语义相互依赖的挑战；现有方法多各自解决部分问题，缺乏统一整体建模。

Method: 基于空间表示框架CurConMix：以课程引导的对比学习逐步学习更判别且相关的特征；引入结构化“困难样本对”采样与特征级mixup增强。时间扩展为CurConMix+：加入多分辨率时序Transformer（MRTT），自适应融合多尺度时间特征，动态平衡时空线索；并发布具层级（阶段/任务/动作）标注的新数据集LLS48。

Result: 在CholecT45与新LLS48上，三元组识别超过现有最优；同时细粒度特征能迁移至更高层级的phase/step识别，表现出强跨层泛化与稳健的上下文理解。

Conclusion: CurConMix+结合课程对比学习与多尺度时序建模，统一解决三元组识别中的不平衡、细微差异与语义依赖问题；配合LLS48数据集，为层级化、可复现、可解释的手术流程理解提供统一基础；代码与数据将开源。

Abstract: Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.

</details>


### [86] [S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection](https://arxiv.org/abs/2601.12313)
*Xiangyu Hu,Yicheng Hong,Hongchuang Zheng,Wenjun Zeng,Bingyao Liu*

Main category: cs.CV

TL;DR: 提出S2F-Net跨模型检测框架，利用频域差异和可学习频率注意力，在多类生成模型上实现强泛化与高准确率（90.49%）。


<details>
  <summary>Details</summary>
Motivation: 现有生成内容检测方法易对特定源模型过拟合，跨架构/跨域时性能明显下降，迫切需要能在未知生成模型下仍保持鲁棒的检测方案。

Method: 构建S2F-Net，围绕“真实vs.合成”在频域上的内在差异：1) 观察上采样在纹理贫乏与纹理丰富区域都会留下独特频率指纹；2) 设计可学习的频率注意力模块，将空间纹理线索与频谱依赖联合建模，自适应加权判别性频带，从而强化频域伪迹；3) 以频域伪迹检测为核心提升跨模型泛化。

Result: 在包含17类生成模型的AIGCDetectBenchmark上，S2F-Net跨域检测准确率达90.49%，显著优于多种现有基线方法。

Conclusion: 关注上采样导致的频域伪迹并通过可学习频率注意力融合空间-频谱信息，可显著提升对未知生成架构的检测泛化；S2F-Net在大规模基准上验证了有效性。

Abstract: The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.

</details>


### [87] [GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer](https://arxiv.org/abs/2601.12316)
*Xinyuan Zhao,Xianrui Chen,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 提出一种语义调制的多尺度Transformer用于3D注视估计，通过原型条件、跨尺度融合与MoE提升性能，在四大数据集上取得SOTA并显著降低角度误差。


<details>
  <summary>Details</summary>
Motivation: 现有3D注视估计受光照、头部姿态、背景与视线方向等域移/语义因素影响大，多尺度信息与条件特征融合不足，模型容量受限导致对复杂条件泛化弱。

Method: 1) 以CLIP全局特征为基础，引入可学习原型库（光照、头姿、背景、方向）对全局特征进行条件化；2) 将经原型增强的CLIP全局向量、CLIP patch token与高分辨率CNN token在统一注意力空间中跨尺度融合；3) 用路由/共享式Mixture of Experts替换部分FFN以提升条件表达容量；4) 在MPIIFaceGaze、EYEDIAP、Gaze360、ETH-XGaze上训练与评估并做消融。

Result: 在MPIIFaceGaze、EYEDIAP、Gaze360、ETH-XGaze上分别达到2.49°、3.22°、10.16°、1.44°的平均角误差，较先前方法最高相对提升达64%。消融显示原型条件、跨尺度融合、MoE与超参选择均有显著贡献。

Conclusion: 语义原型调制与跨尺度统一注意力结合MoE能有效增强3D注视估计的稳健性与精度，达成SOTA；代码已开源，方法具有可扩展与可复用性。

Abstract: We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.

</details>


### [88] [Multi-Sensor Matching with HyperNetworks](https://arxiv.org/abs/2601.12325)
*Eli Passov,Nathan S. Netanyahu,Yosi Keller*

Main category: cs.CV

TL;DR: 提出一种利用超网络与条件实例归一化增强的轻量级Siamese描述子学习框架，用于多模态（如可见光-红外）补丁匹配，在保持推理效率的同时显著提升对外观域移的鲁棒性，并在多个VIS-IR基准上达SOTA；同时发布跨平台VIS-IR补丁数据集GAP-VIR（50万对）以促进跨域研究。


<details>
  <summary>Details</summary>
Motivation: 多模态（尤其VIS-IR、VIS-NIR）图像之间存在显著外观差异，导致传统描述子在跨模态补丁匹配中鲁棒性差；现有强方法往往推理开销大。需要一种在不显著增加参数与计算的前提下，能自适应不同模态与上下文、提升跨域泛化的匹配方法与数据基准。

Method: 在Siamese CNN中引入：1）超网络模块，生成对特征通道的自适应缩放与平移（per-channel scale/shift）；2）在浅层使用条件实例归一化，实现模态特定（如VIS与IR）适配。采用三元组损失与hard-negative mining进行训练，学习判别性跨模态补丁描述子。

Result: 在VIS-NIR及其他VIS-IR基准上取得SOTA或相当/更优的性能；在更多数据集上与更高推理成本的方法相比也能匹配或超越。

Conclusion: 超网络结合条件实例归一化可在不增加显著推理成本的前提下显著提升多模态补丁匹配的鲁棒性与泛化；发布的GAP-VIR（50万对，地面/航拍跨平台）数据集为评估跨域泛化与自适应提供了新的标准。

Abstract: Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.

</details>


### [89] [EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation](https://arxiv.org/abs/2601.12326)
*Jing Zhang,Bingjie Fan*

Main category: cs.CV

TL;DR: 提出EmoKGEdit：一个免训练、结构保持的图像情感编辑框架，通过外部情感知识图谱与潜空间解耦，实现强情感注入且不破坏内容布局，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像情感编辑难以将情感线索与内容潜表示解耦，常导致情感表达弱、结构失真。需要一种既能精确注入目标情感，又能保持图像空间一致性的方案。

Method: 1) 构建多模态情感关联知识图谱（MSA-KG），显式编码对象-属性-情感的因果链，并作为外部知识支持“链式推理”，引导多模态大模型推断情感相关视觉线索并生成一致性指令；2) 基于MSA-KG设计解耦的结构-情感编辑模块，在潜空间显式分离情感属性与布局特征，确保在注入情感的同时严格保持空间结构。框架为免训练。

Result: 在广泛实验中，相比现有方法，EmoKGEdit在情感保真度与内容保留上均取得更好表现，综合指标超越SOTA，并展示更强的结构保持与情感显著性。

Conclusion: 利用外部知识图谱与潜空间解耦，可在无需训练的前提下实现精确且结构稳定的图像情感编辑；该范式为多模态可控编辑提供了通用思路。

Abstract: Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.

</details>


### [90] [FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching](https://arxiv.org/abs/2601.12329)
*Mithlesh Singla,Seema Kumari,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 提出FlowIID：基于潜变量流匹配的轻量级本征图像分解模型，单步推理、参数高效，在多基准上达SOTA或具竞争力，适合资源受限与实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有IID模型效果好但参数庞大，难以与其他系统集成且推理成本高；需要一种在保证质量的同时更高效、可实时部署的方案。

Method: 引入基于latent flow matching的架构FlowIID：以VAE引导构建稳定的潜在空间，再用流匹配模块在潜空间中学习从图像到反照率与阴影的映射，实现单步、稳定的分解；整体设计紧凑、参数量小。

Result: 在多项基准数据集上，FlowIID以更少参数实现与现有方法相当或更优的分解质量，并支持单次前向推理得到结果。

Conclusion: FlowIID在保证精度的同时显著降低模型规模与推理开销，适合资源受限及实时视觉场景中的本征分解任务。

Abstract: Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.

</details>


### [91] [Turbo-GoDec: Exploiting the Cluster Sparsity Prior for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12337)
*Jiahui Sheng,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: 提出在GoDec框架中显式建模异常的“簇稀疏”先验，通过MRF与消息传递估计像素为异常的概率，并将其作为稀疏项，形成Turbo-GoDec，在多组真实HSI上对小目标检测优于LSMAD与多种SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱异常检测多依赖“低秩背景+稀疏异常”的先验，并常对背景加空间约束，但对异常仅用独立稀疏假设，忽略异常在空间上常成小团簇的事实。为弥补这一不足，需把异常的簇结构纳入模型以提升小尺寸目标检测。

Method: 在经典GoDec的S步中引入异常的簇稀疏先验：用马尔可夫随机场(MRF)在空间上对异常标签建模，在因子图上进行消息传递以得到每个像素的异常边缘概率，将概率高的像素作为稀疏成分更新；与GoDec的低秩更新(L步)交替迭代，形成Turbo-GoDec。

Result: 在三个真实HSI数据集上进行实验，Turbo-GoDec在小尺寸异常检测上明显优于原始GoDec(LSMAD)与多种最新方法，表现出更高的检测精度与鲁棒性（文中未给出具体数值，但宣称SOTA或接近SOTA）。

Conclusion: 显式建模异常的簇稀疏先验并与低秩分解融合，能有效提升HSI小目标异常检测性能；MRF+消息传递提供了可行的概率化异常估计，与GoDec交替更新形成高效框架，具有推广潜力。

Abstract: As a key task in hyperspectral image processing, hyperspectral anomaly detection has garnered significant attention and undergone extensive research. Existing methods primarily relt on two prior assumption: low-rank background and sparse anomaly, along with additional spatial assumptions of the background. However, most methods only utilize the sparsity prior assumption for anomalies and rarely expand on this hypothesis. From observations of hyperspectral images, we find that anomalous pixels exhibit certain spatial distribution characteristics: they often manifest as small, clustered groups in space, which we refer to as cluster sparsity of anomalies. Then, we combined the cluster sparsity prior with the classical GoDec algorithm, incorporating the cluster sparsity prior into the S-step of GoDec. This resulted in a new hyperspectral anomaly detection method, which we called Turbo-GoDec. In this approach, we modeled the cluster sparsity prior of anomalies using a Markov random field and computed the marginal probabilities of anomalies through message passing on a factor graph. Locations with high anomalous probabilities were treated as the sparse component in the Turbo-GoDec. Experiments are conducted on three real hyperspectral image (HSI) datasets which demonstrate the superior performance of the proposed Turbo-GoDec method in detecting small-size anomalies comparing with the vanilla GoDec (LSMAD) and state-of-the-art anomaly detection methods. The code is available at https://github.com/jiahuisheng/Turbo-GoDec.

</details>


### [92] [MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents](https://arxiv.org/abs/2601.12346)
*Peizhou Huang,Zixuan Zhong,Zhongwei Wan,Donghao Zhou,Samiul Alam,Xin Wang,Zexin Li,Zhihao Dou,Li Zhu,Jing Xiong,Chaofan Tao,Yan Xu,Dimitrios Dimitriadis,Tuo Zhang,Mi Zhang*

Main category: cs.CV

TL;DR: 提出MMDeepResearch-Bench：一个涵盖140个跨21领域任务的多模态深度研究基准，评测模型在图文证据上的报告生成、引用对齐与视觉一致性，并给出可解释的三部分评价框架（FLAE、TRACE、MOSAIC）。实验显示当前SOTA在文采、引用纪律和多模态落地之间存在权衡，多模态完整性是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有DRA评测多聚焦文本或短答式多模态问答，缺乏端到端利用多模态证据的基准，难以衡量报告体裁中从检索到证据对齐与叙述一致性的全过程与可信度。

Method: 构建MMDR-Bench：每个任务提供图文包，要求生成带来源的报告并显式引用视觉证据；并提出三段可解释评估：1) FLAE用于报告质量的自适应公式-LLM评估；2) TRACE衡量检索与引用的一致性与可信度；3) MOSAIC检查文本-视觉支持与完整性，输出细粒度诊断信号。对25个SOTA模型进行对比实验。

Result: 在25个模型上观察到：生成质量、引用纪律与多模态证据落地存在系统性trade-off；仅有强叙述能力并不保证忠实证据使用；多模态完整性（视觉-文本对齐与一致性）普遍较弱。

Conclusion: MMDR-Bench与FLAE/TRACE/MOSAIC提供了端到端、可解释的多模态研究评测框架，揭示现有模型在证据忠实与多模态一致性上的瓶颈，可作为推动多模态深度研究代理进展的标准平台。

Abstract: Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.

</details>


### [93] [SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence](https://arxiv.org/abs/2601.12357)
*Hailing Jin,Huiying Li*

Main category: cs.CV

TL;DR: 提出SimpleMatch，在低分辨率下也能实现强语义对应，核心是上采样解码器与多尺度监督，辅以稀疏匹配与窗口定位以降内存；在252x252上达84.1% PCK@0.1，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有依赖大规模预训练模型的方法需要高分辨率输入以避免深层下采样造成的关键点特征不可逆融合，导致计算开销大，且当不同语义关键点落入同一下采样感受野时会混淆。

Method: 1) 轻量级上采样解码器，将深层特征逐步上采样恢复到1/4分辨率以保留空间细节；2) 多尺度监督损失，确保上采样后的特征在不同空间尺度上保持判别性；3) 训练时采用稀疏匹配和基于窗口的定位策略以优化显存/内存使用，降低约51%。

Result: 在SPair-71k基准上，以252x252输入（较SOTA小3.3倍）取得84.1% PCK@0.1的成绩，性能超越现有方法，并显著降低资源开销。

Conclusion: 通过针对下采样导致的特征融合问题，SimpleMatch在低分辨率下实现高质量语义对应，兼顾精度与效率，为未来研究提供了实用高效的基线。

Abstract: Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.

</details>


### [94] [From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles](https://arxiv.org/abs/2601.12358)
*Omar Y. Goba,Ahmed Y. Gado,Catherine M. Elias,Ahmed Hussein*

Main category: cs.CV

TL;DR: 提出一种利用LLM与多模态视觉模型动态生成与自适应行为树的AV行为规划框架，在CARLA+Nav2中当基线BT失效时触发，能无人工干预绕行突发障碍，展示优于静态BT的可扩展性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统行为树虽结构清晰、可解释，但静态且需大量人工调参，难以应对复杂、不可预测场景并达成L5自动驾驶需求。因此需要一种能在运行时理解场景、生成并调整策略的自适应规划方法。

Method: 构建多代理“代理式”框架：1) Descriptor代理用链式符号（chain-of-symbols）提示配合LVM评估场景关键性；2) Planner代理基于上下文学习生成高层子目标；3) Generator代理将子目标合成为可执行的XML行为树子树。该系统与现有BT共存，仅在基线BT失败时触发。以CARLA+Nav2为平台评测。

Result: 在仿真中，当出现街道封堵等非常规障碍时，系统能在线生成替代BT子树并成功完成绕行，无需人工介入；相较静态BT基线，表现出更强的适应性与任务完成率（定性为主，作为概念验证）。

Conclusion: 将LLM与LVM引入BT在线生成与适配，可在无需人工干预的情况下处理突发情境，验证了该思路对多样驾驶场景的可扩展潜力，为迈向更高自动化级别提供可行路径。

Abstract: Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.

</details>


### [95] [DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data](https://arxiv.org/abs/2601.12366)
*Jiafei Zhang,Songliang Cao,Binghui Xu,Yanan Li,Weiwei Jia,Tingting Wu,Hao Lu,Weijuan Hu,Zhiguo Han*

Main category: cs.CV

TL;DR: 提出DepthCropSeg++，一个用于开放田间环境下跨物种作物分割的基础模型，在大规模多样数据与自训练范式下显著优于监督基线与通用视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有作物分割受限于昂贵的像素级标注和有限数据，往往只在特定作物或受控环境中有效；需要一个能跨物种、跨场景、在真实田间条件下具备强泛化能力的分割模型。

Method: 1) 扩展跨物种跨场景数据集：28,406张图像，覆盖30+作物、15种环境；2) 基于ViT-Adapter的语义分割骨干，加入动态上采样以增强细节感知；3) 两阶段自训练（几乎无监督）以充分利用无标注数据；4) 系统化实验评估泛化。

Result: 在综合测试集上取得93.11% mIoU：超越监督基线+0.36%，显著优于SAM等通用模型+48.57%。在夜间（86.90% mIoU）、高密度冠层（90.09% mIoU）及未见品种（90.09% mIoU）等挑战场景表现突出。

Conclusion: DepthCropSeg++在开放田间环境中实现跨物种作物分割新SOTA，证明了大规模多样数据+ViT-Adapter改进+自训练对提升真实世界泛化的有效性。

Abstract: DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.

</details>


### [96] [CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology](https://arxiv.org/abs/2601.12373)
*Amro Khaled,Farah Khaled,Omar Riad,Catherine M. Elias*

Main category: cs.CV

TL;DR: 提出CD-TWINSAFE：一种基于V2I的自动驾驶数字孪生架构，车端感知定位→经ROS2/UDP/4G上报；基础设施侧UE5数字孪生实时复刻场景并回传安全告警，实测具备实时性与有效性。


<details>
  <summary>Details</summary>
Motivation: 车端仅依赖本车传感与算力在复杂场景中难以及时、全局地评估安全风险；需要一种利用基础设施与数字孪生增强感知、预测与安全告警的体系，从而提高实时性与安全性。

Method: 设计双栈并行架构：1) 车载驾驶栈含定位与感知模块，利用立体相机以20 fps进行目标检测与特征提取（速度、偏航）并计算TTC与TH等安全指标；2) 基础设施侧数字孪生栈使用UE5重建与更新场景。通过ROS2自定义消息在UDP/4G链路上传输V2I数据，基础设施侧根据实时定位与感知信息驱动孪生并向座舱回传安全告警。

Result: 在多种驾驶场景下完成了联通与协同验证，数字孪生能够根据车端数据实时更新场景并输出安全告警，架构展现出实时响应能力与有效性。

Conclusion: V2I驱动的CD-TWINSAFE能将车端感知与基础设施侧UE5数字孪生协同，实现实时场景复刻与安全告警，验证了其可行性与实时性，为增强自动驾驶安全提供了一种可实现的系统架构。

Abstract: In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.

</details>


### [97] [Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12379)
*Jiahui Sheng,Yidan Shi,Shu Xiang,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: 提出ScoreAD：利用分数基生成模型（SGM）学习HSI数据分布的时间相关梯度场（score），通过与低维流形背景的偏离来检测异常；在四个数据集上有效。


<details>
  <summary>Details</summary>
Motivation: HSI光谱维度高但由少数物理因素决定，满足流形假设；异常与背景在流形上分布不同。现有方法难以准确刻画复杂背景分布，需要能学习真实数据分布几何的模型来区分异常。

Method: 1) 在整幅HSI的所有像素光谱上训练SGM以学习数据分布的score场；2) 测试时对每个光谱施加扰动核得到扰动样本；3) 将扰动样本输入SGM估计score；4) 基于score与流形对齐程度（与背景流形的一致性）构造异常分数，实现无监督检测。

Result: 在四个公开HSI数据集上实验，ScoreAD显示出优于或具竞争力的异常检测性能（文摘未给出具体数值）。代码已开源。

Conclusion: 利用SGM学习的score可刻画HSI背景低维流形结构，异常因不服从该流形而被识别；方法通用、无监督，并在多数据集上验证有效。

Abstract: Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.

</details>


### [98] [A Hierarchical Benchmark of Foundation Models for Dermatology](https://arxiv.org/abs/2601.12382)
*Furkan Yuceyalcin,Abdurrahim Yilmaz,Burak Temelkuran*

Main category: cs.CV

TL;DR: 论文评估10个基础模型在皮肤病变分层诊断中的表现，提出分层评测框架，发现“粒度落差”：通用医学模型在高层二分类最强，但在细粒度40类明显下滑；而皮肤科专用或多模态对细粒度更优，但在粗粒度不及通用模型。结论：高层筛查用通用模型，临床级细分需专门化策略与适配器。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤科基准多简化为二分类，无法反映临床所需的层级、细粒度鉴别能力，影响模型向真实工作流过渡。作者希望系统评估不同领域基础模型的嵌入在分层（从恶性良性到40亚类）任务上的能力差异。

Method: - 数据：DERM12345，含40个病变子类，并可映射为15主类、4/2超类与二分类恶性。
- 模型：10个基础模型（通用视觉、通用医学影像、皮肤科特定）。
- 流程：冻结特征，提取嵌入；训练轻量适配器；五折交叉验证。
- 评测：提出分层评估框架，在四个临床粒度层级用加权F1评分比较。

Result: - 发现“粒度鸿沟”：
  • MedImageInsights 在二分类恶性检测表现最佳（加权F1=97.52%），但在40类细分降至65.50%。
  • MedSigLip 与皮肤科特定模型（Derm Foundation、MONET）在40类细分更强（如MedSigLip 69.79%），但在粗粒度/高层任务落后于MedImageInsights。

Conclusion: 通用医学基础模型适合高层次筛查；临床所需的细粒度鉴别需专业化建模（如领域特化模型或任务特定适配器）。应采用分层评估基准来全面衡量模型在真实临床流程中的适用性。

Abstract: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a "granularity gap" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.

</details>


### [99] [Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation](https://arxiv.org/abs/2601.12391)
*Dasith de Silva Edirimuni,Ajmal Saeed Mian*

Main category: cs.CV

TL;DR: 提出CPVQ-VAE结合LFMM，实现无需外部数据库的纯点云3D场景生成，通过类分区码本和类感知更新稳定学习、按类解码潜变量，在复杂客厅场景中显著降低几何误差。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成要么只产出包围盒，要么依赖扩散生成的类与潜特征再从数据库检索对象；复杂多类别场景下，现有自编码器难以把扩散潜变量还原为与目标类别一致的点云，且码本训练易塌缩，制约了端到端的点云生成。

Method: 1) 设计类分区向量量化VAE（CPVQ-VAE）：码本按类别分区，码向量带类别标签；推理时用“类感知反向查找”只在对应类别分区内量化并解码。2) 提出类感知的运行均值更新策略：监测各分区的“死码向量”并在分区内重初始化，缓解码本塌缩。3) 以专为场景生成的潜空间Flow Matching模型（LFMM）联合生成物体的类别与潜特征，作为CPVQ-VAE的输入，从而直接解码为类别一致的点云形状。

Result: 在复杂客厅场景实验中，相比基线方法，Chamfer距离与Point2Mesh误差分别最多降低70.4%与72.3%，生成场景更合理、一致性更强，并摆脱外部对象库检索。

Conclusion: 类分区码本与类感知训练使VQ-VAE能稳定、按类解码扩散/流模型生成的潜变量，结合LFMM实现端到端的纯点云场景生成，并在复杂多类场景上显著优于现有方法。

Abstract: Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\textit{codebook collapse}$, we propose a $\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.

</details>


### [100] [Weaknesses of Facial Emotion Recognition Systems](https://arxiv.org/abs/2601.12402)
*Aleksandra Jamróz,Patrycja Wysocka,Piotr Garbat*

Main category: cs.CV

TL;DR: 论文综述并复现实验：选择3种最佳情绪识别网络与3个大型多样数据集，交叉测试揭示跨数据集泛化差、某些情绪更难、相近情绪易混淆等弱点。


<details>
  <summary>Details</summary>
Motivation: 情绪识别是人机交互的重要能力，方法繁多但缺乏系统比较与跨数据集一致性评估，需梳理代表性方案与数据资源并量化其实际泛化与难点。

Method: 从文献中筛选3个表现突出的神经网络模型与3个具有多样性与规模优势的数据集；对每个模型在指定数据集上训练；设计一系列对比实验，包括在未参与训练的数据集上进行交叉测试；分析类别间差异与混淆模式。

Result: 三种模型在各自训练数据集上表现良好，但在跨数据集测试时性能显著下降，显示出数据分布偏移的影响；不同情绪类别识别难度不均，一些细微或相近的情绪（如相邻情绪）混淆度高。

Conclusion: 现有解决方案在泛化与细粒度区分上存在不足；数据集间差异和类别不均衡导致性能波动。需要改进的方向包括更强的域泛化/自适应、类别再平衡与细粒度判别建模，以及更一致、更具代表性的数据集标准化。

Abstract: Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.

</details>


### [101] [HOT-POT: Optimal Transport for Sparse Stereo Matching](https://arxiv.org/abs/2601.12423)
*Antonin Clerc,Michael Quellmalz,Moritz Piening,Philipp Flotho,Gregor Kornhardt,Gabriele Steidl*

Main category: cs.CV

TL;DR: 提出一种将相机几何的线性约束与最优传输相结合的无监督稀疏立体匹配与对象匹配方法，基于对极/3D射线距离构造(部分)OT代价，转化为高效的指派问题，并在面部关键点与标注规范对齐中验证有效。


<details>
  <summary>Details</summary>
Motivation: 稀疏特征（如人脸关键点）在立体匹配中对参数与噪声敏感，且遮挡、运动、相机畸变等使问题病态；缺乏监督时更难。作者希望利用相机几何的线约束和OT的鲁棒匹配能力来稳定、无监督地进行稀疏匹配与对象层级匹配。

Method: 从OT视角建模：将相机投影点表示为（半）直线/射线，引入对极距离与3D射线距离作为匹配代价；在(部分)OT框架中求解，从而化为可高效求解的指派问题；进一步把无监督对象匹配建成分层（层级）OT问题以同时进行特征级与对象级匹配。

Result: 得到一套高效的特征与对象匹配算法。数值实验显示在面部分析任务中能有效匹配不同的人脸关键点标注规范，并在存在遮挡/畸变等困难条件下表现稳健。

Conclusion: 基于相机几何线约束的(部分、层级)OT建模可在无监督设置下实现稳健、高效的稀疏立体与对象匹配，适用于人脸等应用场景。

Abstract: Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.

</details>


### [102] [SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition](https://arxiv.org/abs/2601.12432)
*Shunyu Huang,Yunjiao Zhou,Jianfei Yang*

Main category: cs.CV

TL;DR: SkeFi通过跨模态知识迁移，把RGB的人体骨架知识迁到无线传感（LiDAR/mmWave），并结合改进的时序图卷积与多尺度时域建模，在噪声较大的无线骨架上实现SOTA动作识别。


<details>
  <summary>Details</summary>
Motivation: RGB标注骨架在暗光/隐私环境下受限；无线传感具备非侵入与鲁棒优势，但数据稀缺且骨架噪声大，导致骨架估计与后续动作识别困难。需要一种能利用RGB数据优势、同时提升无线骨架鲁棒性的方案。

Method: 提出SkeFi跨模态知识迁移框架：从数据丰富的RGB模态向无线模态迁移骨架与动作表征；核心是TC-AGC（Temporal Correlation Adaptive Graph Convolution）并加入帧间交互增强以应对缺失/不连续帧噪声；同时引入双重时域卷积以增强多尺度时序建模；将上述与跨模态迁移整合，实现从无线传感中稳定提取姿态与动作。

Result: 在mmWave与LiDAR数据上取得SOTA性能，验证了在高噪声无线骨架上的有效性与泛化能力；代码开源于GitHub。

Conclusion: 跨模态知识迁移结合TC-AGC与双时域卷积可有效缓解无线传感骨架估计的噪声与数据稀缺问题，实现对暗光与隐私敏感场景的鲁棒动作识别。

Abstract: Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.

</details>


### [103] [Adversarial Defense in Vision-Language Models: An Overview](https://arxiv.org/abs/2601.12443)
*Xiaowei Fu,Lei Zhang*

Main category: cs.CV

TL;DR: 本文综述VLM对抗防御：训练期、防测试自适应、免训练三类方案的动机、方法与权衡，并指出仍存鲁棒性与效率等挑战。


<details>
  <summary>Details</summary>
Motivation: VLM（如CLIP）在跨模态任务中广泛应用，但对不可感知且复杂的对抗攻击脆弱，威胁性能与系统安全，亟需系统性梳理与评估防御策略。

Method: 将现有防御按范式分为三类：1) 训练期防御：通过对抗微调等修改训练流程以提升鲁棒性；2) 测试时自适应：在推理时更新参数以适应无标签对抗样本；3) 免训练防御：不改模型，改输入或特征嵌入以抑制攻击影响。综述各自代表性方法、机制与开销。

Result: 总结三类防御的效用与代价：训练期防御通常最稳健但计算昂贵、泛化有限；测试时自适应灵活但复杂度与开销高；免训练防御最轻量但提升有限、依赖攻击特性。

Conclusion: 当前VLM防御在鲁棒性-效率-泛化间存在权衡。需发展更通用、低开销、可组合的防御，以及标准化评测、跨攻击与跨数据集泛化研究以提升VLM鲁棒性。

Abstract: The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.

</details>


### [104] [Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild](https://arxiv.org/abs/2601.12464)
*Yanrui Lu,Danyang Chen,Haowen Xiao,Jiarui Zhu,Fukang Ge,Binqian Zou,Jiali Guan,Jiayin Liang,Yuting Wang,Ziqian Guan,Xiangcheng Bao,Jinhao Bi,Lin Gu,Jun He,Yingying Zhu*

Main category: cs.CV

TL;DR: 提出一个大规模、多来源的EM细胞器实例分割基准与半自动标注流程，发现现有局部上下文模型难以泛化，尤其对全局分布形态如ER表现差。


<details>
  <summary>Details</summary>
Motivation: 现有基准以小且精心挑选的数据为主，无法体现真实EM数据的异质性与大范围上下文需求，使得补丁式方法存在根本局限。

Method: 构建包含10万+张2D EM图像、覆盖多种细胞类型与五类细胞器的大规模多源数据集；利用设计的三维连通感知标签传播算法（3D LPA）生成标注并由专家修订；评测多种SOTA模型（U-Net、SAM变体、Mask2Former）。

Result: 在该更具异质性与长程结构需求的基准上，现有模型普遍泛化不佳；对具有全局分布、长程连通形态的细胞器（如内质网）分割性能尤其低下。

Conclusion: 当前依赖局部上下文的实例分割范式与真实EM数据中的长程结构连续性需求存在根本不匹配；需要能够建模长距离依赖与全局形态的新的方法与评测。

Abstract: Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.

</details>


### [105] [DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors](https://arxiv.org/abs/2601.12468)
*Yanqi Wu,Qichao Chen,Runhe Lai,Xinhua Lu,Jia-Xin Zhuang,Zhilin Zhao,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出DCAC：一种训练-free、测试时校准模块，利用按类划分的缓存与轻量两层网络缓解OOD过自信，提高OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度网络在测试时对未见的OOD样本常产生过度自信预测。作者观察到：被模型判为同一类（或对该类给高概率）的OOD样本在视觉上彼此更相似，而与真实ID样本差异更大，提示可按类别利用这种相似性进行校准。

Method: 提出动态按类缓存（DCAC）：为每个ID类别维护独立缓存，收集高熵样本的视觉特征与预测概率；在测试时用一个轻量两层模块，基于缓存特征与输入的预测分布对原始预测进行校准，从而抑制OOD过自信。该模块无需训练、测试时即可插入，适配单模态与视觉-语言模型，并与多种OOD方法组合。

Result: 在多种OOD基准上广泛实验，整合到现有方法后显著提升性能；例如在ImageNet OOD基准与ASH-S结合时，将FPR95降低6.55%。

Conclusion: 按类感知的动态缓存与轻量校准在不增加显著开销、无需训练的前提下，有效缓解OOD过自信，并广泛提升各类OOD检测方法。

Abstract: Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.

</details>


### [106] [NeuralFur: Animal Fur Reconstruction From Multi-View Images](https://arxiv.org/abs/2601.12481)
*Vanessa Sklyarova,Berna Kabadayi,Anastasios Yiannakidis,Giorgio Becherini,Michael J. Black,Justus Thies*

Main category: cs.CV

TL;DR: 提出首个多视图驱动的动物毛发三维重建方法：用VLM提取部位级毛长与方向先验，结合多视图几何/光度监督，生成高保真条束级毛发。


<details>
  <summary>Details</summary>
Motivation: 动物毛发重建受细粒度结构、自遮挡、视角相关外观影响，且缺乏可学习的跨物种数据集与先验；现有方法多聚焦人类发型，难以泛化到多样动物毛型。

Method: 1) 由多视图RGB重建粗网格（传统MVS）；2) 调用视觉语言模型查询不同身体部位的真实毛长与结构，构建“无毛”基几何；3) 在其上生长条束级毛发；4) 以多视图几何与光度损失监督；5) 用Gabor滤波提取纹理方向但存在方向歧义，借助VLM提供生长方向与与重力方向关系作为额外损失以消歧。

Result: 在多种动物与毛型上实现高保真、可泛化的条束级3D毛发重建，显示VLM引导可有效缓解方向歧义并提升细节与一致性。

Conclusion: 将VLM知识注入多视图3D重建流程，可在无专用数据集的情况下取得逼真动物毛发建模；方法具备跨物种泛化能力并公开更多结果与代码。

Abstract: Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.

</details>


### [107] [Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation](https://arxiv.org/abs/2601.12493)
*Mehrdad Noori,Gustavo Adolfo Vargas Hakim,David Osowiechi,Fereshteh Shakeri,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出Histopath-C基准与LATTE低秩转导式自适应方法，提升病理领域VLM在多种真实仿真扰动下的鲁棒性与TTA表现。


<details>
  <summary>Details</summary>
Motivation: 病理图像在实际采集中常出现显著域偏移（染色差异、污染、模糊、噪声等），导致预训练对比式医疗VLM在下游任务性能下降；现有鲁棒性与TTA方法多针对自然图像，难直接适配病理场景，缺少系统化评测基准。

Method: 1) 构建Histopath-C：为任意病理数据集动态施加逼真的合成腐扰动，在线评估测试时自适应（TTA）机制；2) 提出LATTE：一种转导式、低秩适配策略，结合多文本模板以减轻VLM对文本提示敏感性，并在测试时对模型进行低秩更新/适配。

Result: 在多种病理数据集与广泛腐扰动设定下，所提LATTE优于面向自然图像的SOTA TTA方法，展现更强鲁棒性与泛化能力。

Conclusion: 针对病理VLM的域偏移问题，Histopath-C提供统一评测平台，LATTE实现稳健的测试时自适应，显著提升在真实分布偏移下的性能；代码与数据已开源。

Abstract: Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.

</details>


### [108] [Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods](https://arxiv.org/abs/2601.12500)
*Yaowu Fan,Jia Wan,Tao Han,Andy J. Ma,Antoni B. Chan*

Main category: cs.CV

TL;DR: 提出使用移动无人机拍摄的大规模密集人群视频数据集MovingDroneCrowd++，并基于全局密度图分解与描述子关联（GD3A）以及描述子投票跟踪（DVTrack）的方法，实现无需显式定位的个体计数与跟踪，在复杂运动与高密度场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有密集人群计数与跟踪方法多依赖固定摄像机数据，空间覆盖有限且难以适应大场景与复杂运动；在移动平台（如无人机）场景下，传统方法表现不佳，缺乏大规模、视频级标注的数据集与相应的鲁棒算法。

Method: 1) 构建MovingDroneCrowd++：大规模、视频级、由移动无人机采集，涵盖多高度、视角、光照与复杂人群密度。2) 提出GD3A：以密度图为基础，通过最优传输并引入自适应dustbin分数，建立相邻帧之间的像素级行人描述子对应关系，将全局密度图分解为共享、流入与流出三部分，实现视频级个体计数。3) 提出DVTrack：将描述子级匹配通过“描述子投票”机制聚合为实例级关联，实现行人跟踪。

Result: 在MovingDroneCrowd++上，现有方法表现不理想；所提GD3A与DVTrack在高密度、复杂运动情况下显著领先：计数误差降低47.4%，跟踪性能提升39.2%。

Conclusion: 移动无人机视频下的密集人群计数与跟踪可通过密度图分解与描述子关联有效实现；新数据集推动该方向研究，所提方法在复杂动态场景中明显优于现有方案。

Abstract: Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.

</details>


### [109] [SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection](https://arxiv.org/abs/2601.12507)
*Ruo Qi,Linhui Dai,Yusong Qin,Chaolei Yang,Yanshan Li*

Main category: cs.CV

TL;DR: 提出SDCoNet，将超分辨率与目标检测在同一框架内协同优化，通过共享编码器、显著性引导与梯度路由，实现对弱小目标的更精准检测，尤其适用于低质遥感图像。


<details>
  <summary>Details</summary>
Motivation: 串联“先超分后检测”的管线存在目标不一致、冗余特征与弱交互等问题，且低质成像下小目标信号弱、背景复杂，使检测困难。需要一种能在保证任务特异性的同时促进SR与检测协同、并显式抑制背景干扰与优化冲突的方法。

Method: - 设计SDCoNet：以Swin Transformer为共享编码器，实现分层、窗口移位自注意力促成跨任务特征协作，同时保留任务专属性。
- 多尺度显著性预测模块：为token分配重要性分数，选择关键token，聚焦弱目标区域，抑制背景杂波与多任务耦合引入的不利特征。
- 梯度路由策略：先稳定检测语义，再将SR分支的梯度沿“有利于检测”的方向路由，引导SR生成对检测显式有益的高频细节。
- 在NWPU VHR-10-Split、DOTAv1.5-Split、HRSSD-Split上进行实验与效率评估。

Result: 在上述公开数据集上，保持竞争性计算开销的同时，小目标检测性能显著优于主流方法，尤其在低质量遥感图像场景下表现突出。

Conclusion: 通过显式的显著性引导与隐式特征共享，并配合梯度路由以缓解优化冲突，SDCoNet有效耦合SR与检测两任务，提升低质遥感图像小目标检测精度，且具备良好的效率与泛化性。

Abstract: In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.

</details>


### [110] [Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images](https://arxiv.org/abs/2601.12512)
*Mohd Usama,Belal Ahmad,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: 提出一种基于CycleGAN的无监督医学影像域适配方法，通过学习源域与目标域的双向映射，在不需要成对标注的情况下减轻MRI跨设备/机构的域偏移，并显著提升下游模型在目标域的性能与一致性。


<details>
  <summary>Details</summary>
Motivation: 不同扫描仪、协议与采集参数导致MRI图像在不同机构间存在显著域差异，致使在源域训练的深度模型在目标域上性能下降。需要一种无需配对数据与标注、又能保持解剖结构一致性的域适配方案，以提升跨域泛化与临床可用性。

Method: 采用CycleGAN学习源域与目标域之间的双向映射；通过循环一致性保持内容，结合内容保持损失与“disparity（差异/视差）”相关损失以约束域转换，确保外观风格迁移同时维持解剖结构完整；训练过程中不依赖成对数据与标签。

Result: 在多组MRI数据上完成双向域适配实验，统计结果显示该方法在无标签条件下提升了目标域模型性能，降低了由域差异带来的变异性，表现出更稳定、可泛化的推理结果。

Conclusion: 基于CycleGAN的无监督域适配可在不损伤解剖信息前提下缓解MRI跨域偏移，改善诊断相关模型的准确性与一致性，为临床落地提供可行途径，并为进一步提升医疗影像分析的可靠性提供方向。

Abstract: Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.

</details>


### [111] [Deep Feature Deformation Weights](https://arxiv.org/abs/2601.12527)
*Richard Liu,Itai Lang,Rana Hanocka*

Main category: cs.CV

TL;DR: 提出一种将深度特征先验与传统把手式网格变形融合的方法，利用深度特征邻近性直接生成平滑且语义一致的变形权重，实时计算、无需优化，并支持对称与语义共变形，高分辨率网格分钟级预处理、实时编辑。


<details>
  <summary>Details</summary>
Motivation: 传统把手式变形精确高效但需预先合理布置控制点，映射非直观且缺乏语义；数据驱动方法具语义但慢且不精确。需要一种同时具备语义先验与传统方法可控性和速度的方案。

Method: 以深度网络提取的视觉/几何深度特征为先验：1) 基于“深度特征距离”的邻近性直接定义变形权重场，天然平滑、无需额外正则；2) 提出“重心特征蒸馏（barycentric feature distillation）”，利用渲染的视觉信号，以重心插值高效将特征蒸馏到网格顶点/面上，降低蒸馏成本；3) 通过特征空间约束与局部性加权，保留并扩展经典方法性质；4) 利用场表示自动检测语义对称，进行对称保持的变形；5) 权重可对任意表面点实时计算，新把手不需再优化。

Result: - 权重计算对高分辨率网格在1分钟内完成；- 运行时可对多达百万面片网格实现实时编辑；- 相比经典与神经方法，从小时级预处理降至分钟级；- 实现语义部件的共变形与对称保持；- 变形精度与控制性接近传统方法，同时具备语义一致性。

Conclusion: 深度特征驱动的权重场为把手式变形提供了语义一致、可控且高效的方案。通过高效蒸馏与特征空间约束，实现了实时、对称与共变形能力，兼具传统方法的速度/精度和数据驱动的语义先验。

Abstract: Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.

</details>


### [112] [XRefine: Attention-Guided Keypoint Match Refinement](https://arxiv.org/abs/2601.12530)
*Jan Fabian Schmid,Annika Hagemann*

Main category: cs.CV

TL;DR: 提出XRefine，一个与检测器无关的亚像素关键点精炼方法，仅用匹配关键点周围的图像patch，通过交叉注意力预测更精确坐标；在MegaDepth、KITTI、ScanNet上显著提升几何估计并兼顾效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏关键点匹配对3D视觉至关重要，但现有关键点检测器产生的匹配位置常有空间偏差。已有精炼方法多需依赖特定检测器内部表征并为每个检测器重新训练，泛化差、成本高。

Method: 设计XRefine：输入为已匹配关键点处的图像patch（可扩展到多视角轨迹），采用基于交叉注意力的网络，直接回归精炼后的关键点亚像素坐标，不依赖任何特定检测器的内部特征或表示。

Result: 在MegaDepth、KITTI、ScanNet数据集上，XRefine在不显著增加推理时间的前提下，持续提升几何估计精度，优于现有精炼方法；对不同检测器具有良好泛化能力。

Conclusion: 与检测器无关的交叉注意力精炼框架能利用局部图像信息实现稳健亚像素关键点修正，通用于多检测器与多视角场景，在效率与精度上取得更优折中。

Abstract: Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.

</details>


### [113] [BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images](https://arxiv.org/abs/2601.12533)
*Md. Ahanaf Arif Khan,Ariful Islam,Sangeeta Biswas,Md. Iqbal Aziz Khan,Subrata Pramanik,Sanjoy Kumar Chakrabarty,Bimal Kumar Pramanik*

Main category: cs.CV

TL;DR: 提出BirdsEye-RU：面向俯视场景的小目标人脸数据集（2978张图、8000+标注），涵盖无人机与高空手机图像，聚焦极端尺度变化与复杂环境；数据已开源于Kaggle。


<details>
  <summary>Details</summary>
Motivation: 俯视视角的人脸检测受小目标、尺度跨度大、背景杂乱等因素影响，现有数据集与方法泛化不足，缺乏专门针对高空/俯拍场景的人脸数据。

Method: 构建并标注BirdsEye-RU数据集：收集多环境下的无人机与高空手机图像，聚焦远距与小尺寸人脸；提供详细标注与数据描述，并开放下载。

Result: 得到2978张图像、8000+人脸标注的数据集，覆盖多类俯视与复杂环境；为评测与研究俯视人脸检测提供基准。

Conclusion: BirdsEye-RU填补俯视小目标人脸数据空白，支持算法在极端尺度和杂乱背景下的研究与比较；数据已公开可获取。

Abstract: Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.

</details>


### [114] [Encoding Emotion Through Self-Supervised Eye Movement Reconstruction](https://arxiv.org/abs/2601.12534)
*Marcus Ma,Jordan Prescott,Emily Zhou,Tiantian Feng,Kleanthis Avramidis,Gabor Mihaly Toth,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: 研究提出一种自监督的凝视检测与眼动重建方法，在低分辨率、自然场景视频中用眼动预测情绪相关信号，并在下游任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有文献表明注视模式与情绪相关，但多依赖昂贵高分辨率眼动仪，限制了应用范围。作者希望在更自然、低分辨率的视频中，从眼动中预测情绪表达的多模态标记，扩展可及性与生态效度。

Method: 以大规模未标注访谈视频为素材（犹太人大屠杀幸存者口述史），提出受语言模型预训练启发的自监督眼动重建框架：通过重建眼动信号来学习凝视表征；随后用编码器嵌入微调两个下游任务：(1) 将眼动与语音方向性情绪估计对齐；(2) 仅用眼动预测三类瞬时情绪行为（笑、哭/啜泣、叹气）。

Result: 新模型的眼动表征对情绪结果具有预测力；并且预训练重建性能与下游情绪处理任务表现呈正相关，在两个实验中均得到验证。

Conclusion: 自监督的眼动重建能有效编码眼动所承载的情感信号，可在低分辨率、自然场景视频中支持情绪相关任务，拓展了无需专业眼动仪的情感计算方法。

Abstract: The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.

</details>


### [115] [PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception](https://arxiv.org/abs/2601.12551)
*Tong Wu*

Main category: cs.CV

TL;DR: 提出PISE：一种物理先验的深度鬼成像框架，用于低带宽边缘感知；通过伴随算子初始化与语义引导，在5%采样下将分类准确率提升2.57%、方差降低9倍。


<details>
  <summary>Details</summary>
Motivation: 鬼成像在极低采样/带宽条件下能获取场景信息，但深度学习重建/识别在稀疏测量下易不稳且泛化差；需要把物理模型与任务语义结合，提升低采样下的鲁棒感知与可用性。

Method: 提出PISE框架：1) 物理一致的伴随算子初始化，将网络与前向测量模型对齐；2) 语义引导（如分类/特征监督或先验）贯穿训练以引导重建/表征；面向鬼成像测量，在端到端管线中联合优化，提高在低采样（5%）下的感知性能与稳定性。

Result: 在5%采样率下，相比基线，分类准确率提高2.57%，结果方差降低约9倍，显示出显著更稳定与更可靠的性能。

Conclusion: 物理先验与语义引导的结合可在低带宽鬼成像感知中有效提升准确性与稳定性；PISE为边缘设备在极低采样场景下的鲁棒感知提供了可行路径。

Abstract: We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.

</details>


### [116] [Camera Pose Revisited](https://arxiv.org/abs/2601.12567)
*Władysław Skarbek,Michał Salomonowicz,Michał Król*

Main category: cs.CV

TL;DR: 提出PnP-ProCay78，用Cayley旋转参数化+二次重建误差与最小二乘，配以确定性初始点选择，实现平面PnP位姿初估；精度近似最优SQPnP、略优于IPPE，但更简洁高效。


<details>
  <summary>Details</summary>
Motivation: 平面PnP在相机标定与多传感器系统中关键，但现有方法要么需复杂的全局搜索/初始化，要么算法结构复杂。作者希望在不牺牲投影精度的前提下，提供简单、可解释、对低分辨率传感器也稳健的位姿初估方法。

Method: 将旋转用Cayley参数化，使用经典二次形式的重建误差；通过分析两个规范向量的重建误差，确定性地选取优化起点，避免代价高的解空间搜索；最小二乘优化在Cayley空间进行，同时将平移用解析方式从重建误差中消去，形成结合投影误差与翻译代理的混合代价。

Result: 在RGB与低分辨率热红外的RGB–IR实验中，PnP-ProCay78的投影精度与最优SQPnP几乎一致，略优于IPPE；同时算法结构更简单，优化轨迹在Cayley空间中呈现出直观的收敛行为。

Conclusion: PnP-ProCay78以简洁结构实现与先进PnP方法相当的精度，并提供可解释的收敛过程；其将投影误差最小化与解析消去的平移重建误差代理相结合，得到几何透明、计算高效且具教学价值的平面PnP解法。

Abstract: Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \texttt{SQPnP} and slightly higher than \texttt{IPPE}, both prominent \texttt{PnP-OpenCV} procedures. However, \texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.

</details>


### [117] [Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models](https://arxiv.org/abs/2601.12626)
*Raphi Kang,Hongqiao Chen,Georgia Gkioxari,Pietro Perona*

Main category: cs.CV

TL;DR: 论文发现并验证：许多视觉语言模型通过对语言激活线性绑定“空间/时间ID”来表示物体位置与时间顺序，并在中间层以可因果干预的方式影响推理，从而揭示了一种可解释的线性机制。


<details>
  <summary>Details</summary>
Motivation: VLM 展现出跨空间与时间的推理能力，但其内部表征与计算机制不透明。作者假设模型在某处将视觉/几何信息与文本表征融合，并希望找到能以线性形式因果解释输入输出行为的“汇合点”。

Method: 提出并搜索“空间ID/时间ID”（spatial/temporal IDs）的线性表征：1) 在多种 VLM 与视频 VLM 中训练/探测线性读出器，检测对象位置或时间步是否可由语言侧激活线性恢复；2) 通过因果干预（编辑、注入、消融这些ID）测试其对模型中间层与最终回答的影响；3) 以这些ID作诊断信号分析模型失效模式，并作为训练信号改进学习。

Result: 实证表明：VLM 会以线性方式将位置/时间信息绑定到语言 token 激活，并主要通过语言通道执行推理；这些ID在模型多层广泛存在，且对其进行系统性编辑能可控地改变模型信念与输出；空间ID还能识别现有模型的局限，并在训练中提供有效监督；在视频 VLM 中观察到对应的线性“时间ID”机制。

Conclusion: VLM 内部存在可线性读出与干预的时空ID机制，它在中间层介导空间与时间信息并驱动语言化推理。该机制提升了解释性，并为更对齐、更强大的模型设计与训练提供了可操作的信号与工具。

Abstract: Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual/geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: https://github.com/Raphoo/linear-mech-vlms.

</details>


### [118] [From Bands to Depth: Understanding Bathymetry Decisions on Sentinel-2](https://arxiv.org/abs/2601.12636)
*Satyaki Roy Chowdhury,Aswathnarayan Radhakrishnan,Hsiao Jou Hsu,Hari Subramoni,Joachim Moortgat*

Main category: cs.CV

TL;DR: 研究提出并剖析一种基于 Swin-Transformer 的U-Net（Swin-BathyUNet）用于哨兵2卫星反演水深，并评估其可靠性与可迁移性；通过波段、显著性与注意力消融，揭示影响因素与稳健性提升路径，并给出跨区域应用实践建议。


<details>
  <summary>Details</summary>
Motivation: 卫星导出水深（SDB）在不同海域间稳健部署困难：光学、水体/底质、表面干扰（浪沫/阳光闪斑）与传感器辐射差异导致模型迁移性能不稳定。需要理解深度模型的判别依据、预测何时可信，并形成可操作的跨区域部署策略。

Method: 1) 提出并分析 Swin-Transformer 融合U-Net（Swin-BathyUNet）；2) 留一波段消融评估各光谱带重要性；3) 将消融式类激活映射扩展到回归任务（A-CAM-R），并通过“保留前p%显著像素、将其余中和”的性能保留测试验证解释可靠性；4) 注意力消融，比较解码器端条件交叉注意力与跳连；5) 跨区域训练/测试评估深度相关退化与数据分布影响。

Result: - 波段重要性排序与浅水光学一致（绿/蓝通道最关键）。- A-CAM-R 的保留测试表现为随着保留比例下降，RMSE单调大幅增加，说明解释集中在模型真实依赖的证据上。- 在解码器中引入对跳连的条件交叉注意力显著提升对闪斑/浪沫等高亮干扰的鲁棒性。- 跨区域时误差随深度近线性升高，且当目标区域深度分布呈双峰时，中深水错误加剧。

Conclusion: 为稳健SDB：保持较大感受野；确保绿/蓝通道的辐射定标与保真；对近岸高亮高方差区域先行预滤；跨区域迁移时结合小样本目标域微调与“随深度”校准，从而缓解深度相关退化与分布偏移。

Abstract: Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across sites remains challenging. We analyze a Swin-Transformer based U-Net model (Swin-BathyUNet) to understand how it infers depth and when its predictions are trustworthy. A leave-one-band out study ranks spectral importance to the different bands consistent with shallow water optics. We adapt ablation-based CAM to regression (A-CAM-R) and validate the reliability via a performance retention test: keeping only the top-p% salient pixels while neutralizing the rest causes large, monotonic RMSE increase, indicating explanations localize on evidence the model relies on. Attention ablations show decoder conditioned cross attention on skips is an effective upgrade, improving robustness to glint/foam. Cross-region inference (train on one site, test on another) reveals depth-dependent degradation: MAE rises nearly linearly with depth, and bimodal depth distributions exacerbate mid/deep errors. Practical guidance follows: maintain wide receptive fields, preserve radiometric fidelity in green/blue channels, pre-filter bright high variance near shore, and pair light target site fine tuning with depth aware calibration to transfer across regions.

</details>


### [119] [Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT](https://arxiv.org/abs/2601.12638)
*Ninnart Fuengfusin,Keisuke Yoneda,Naoki Suganuma*

Main category: cs.CV

TL;DR: 提出针对PointPillars的混合精度量化与小样本校准策略，在保证精度的同时显著降低延迟与模型大小（最高约2.35×更快、2.26×更小）。


<details>
  <summary>Details</summary>
Motivation: 激光雷达3D目标检测在自动驾驶中需满足实时性；直接整模量化常因数值分布宽与极端离群值导致精度下降，因此需要既高效又稳健的量化方案。

Method: 1) 混合精度框架：以PTQ逐层INT8量化并评估AP，选出top-k敏感层保留FP；对这些层做贪心组合搜索得到候选混合精度模型；最终用PTQ或QAT定型。2) 小样本校准：采用极少量校准数据以降低遇到离群值概率，从而提升PTQ稳定性与性能。3) 基于PointPillars并部署于TensorRT。

Result: PTQ管线下无需再训练即可得到混合精度模型；QAT管线达到与全精度模型相近的性能；在TensorRT上推理延迟最高降低约2.35×，模型大小最高缩小约2.26×。

Conclusion: 通过逐层敏感性分析的混合精度与小样本校准，有效解决LIDAR数值分布宽与离群值问题，在保持精度的前提下显著加速并压缩模型，适合实时自动驾驶部署。

Abstract: LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.

</details>


### [120] [Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images](https://arxiv.org/abs/2601.12664)
*Elisa Gonçalves Ribeiro,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: 研究探讨在非IID联邦学习场景下，是否可将单一数据集上经贝叶斯优化得到的超参数泛化到不同癌症病理任务；并提出一个跨数据集的简单聚合启发式（学习率取平均、优化器与批大小取众数），该组合在分类上取得有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 临床隐私限制难以集中训练深度学习模型，联邦学习虽可缓解但在非IID客户端下对超参数极其敏感。实际部署往往缺乏逐站点调参资源，因此需要评估：在一个数据集上得到的超参数能否迁移到其他数据集/任务与联邦场景，并寻找简单可行的跨数据集超参数组合策略。

Method: 针对卵巢与结直肠癌的二分类病理任务：1) 在集中式设置上对每个数据集进行贝叶斯超参数优化；2) 将每个数据集的最优超参数迁移到非IID的联邦学习设置中评估；3) 设计跨数据集聚合启发式：学习率取平均，优化器与批大小取众数，形成联合配置；4) 比较单数据集迁移与联合配置在联邦场景的分类性能。

Result: 跨数据集的简单聚合配置在非IID联邦学习中达到与最佳单数据集迁移相当的、具有竞争力的分类性能，显示一定的稳健性与泛化能力。

Conclusion: 单数据集上优化的超参数在非IID联邦场景具有一定可迁移性；更重要的是，采用学习率平均与优化器/批大小众数的简易聚合规则，可在不同癌症病理任务中取得稳健、接近最优的联邦分类效果，为实际临床部署提供低成本的超参数选择方案。

Abstract: Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.

</details>


### [121] [Near-Light Color Photometric Stereo for mono-Chromaticity non-lambertian surface](https://arxiv.org/abs/2601.12666)
*Zonglin Li,Jieji Ren,Shuangfan Zhou,Heng Guo,Jinnuo Zhang,Jiang Zhou,Boxin Shi,Zhanyu Ma,Guoying Gu*

Main category: cs.CV

TL;DR: 提出一种在单色性假设下，利用神经隐式表示联合建模深度与BRDF的单张彩色光度立体方法，并配套小型光学触觉传感器验证，在近距离光源与非朗伯表面条件下实现高精度重建。


<details>
  <summary>Details</summary>
Motivation: 现有彩色光度立体多假设远距光源与朗伯反射，不适用于更实际的近光与非朗伯表面；单张图像问题病态，需要新的先验与表征来缓解不适定性。

Method: 在单色性（统一色度与均质材料）假设下，引入神经隐式表示同时学习几何（深度）与BRDF；利用单张彩色图像在近光条件下进行联合优化/重建；并设计紧凑的光学触觉传感器用于实验验证。

Result: 在合成与真实数据上，相比现有方法能在近光与非朗伯情况下实现准确、鲁棒的表面重建，仅需单张图像。

Conclusion: 单色性约束结合神经隐式几何与BRDF建模有效缓解彩色光度立体的不适定性，使单次采样在更实际的照明与反射条件下也能得到高质量重建，并具有实际传感器验证。

Abstract: Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.

</details>


### [122] [Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification](https://arxiv.org/abs/2601.12671)
*Thamara Leandra de Deus Melo,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: 在联邦学习下评估CNN用于脑肿瘤MRI分类：单独预处理几乎无益；加入测试时增强（TTA）后得到稳定且显著提升（p<0.001）。TTA应作为默认推理策略，若算力允许，再配合轻量预处理可进一步受益。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤早诊依赖准确的影像分类，但病灶异质性强、MRI复杂，且医疗数据常被孤岛化。联邦学习允许多中心协作训练而不共享数据，需明确在这种设置下预处理与TTA是否真正带来可重复的性能提升。

Method: 在联邦学习框架中训练CNN分类器，比较直接使用原始MRI与经过一系列常见轻量预处理（重采样/缩放、灰度化、归一化、滤波、直方图均衡）两种训练策略的效果；并在推理阶段是否采用测试时增强（多视角/扰动集成）进行对照，统计检验性能差异。

Result: 仅应用预处理的提升可以忽略；当与TTA结合时，在联邦MRI分类任务上获得一致且统计学显著的性能提升（p<0.001）。

Conclusion: 在FL医疗影像推理中应默认启用TTA；在算力允许的情况下，TTA配合轻量预处理能带来额外、稳定的收益。

Abstract: Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.

</details>


### [123] [VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness](https://arxiv.org/abs/2601.12672)
*Qimao Chen,Fang Li,Shaoqing Xu,Zhiyi Lai,Zixun Xie,Yuechen Luo,Shengyin Jiang,Hanbing Li,Long Chen,Bing Wang,Yi Zhang,Zhi-Xin Yang*

Main category: cs.CV

TL;DR: 提出VILTA：将视觉语言模型（VLM）直接纳入自动驾驶闭环训练，通过直接编辑周围交通参与者未来轨迹，生成多样且具挑战性的长尾场景，从而显著提升策略的安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的长尾问题导致关键罕见场景数据稀缺，现有基于规则、重采样或离线生成的方案难以产生真正新颖多样的危险场景；两阶段“VLM描述→下游生成器”也受下游算法上限限制，无法充分发挥VLM的泛化能力。

Method: 提出VILTA框架，把VLM置于训练闭环中：VLM理解动态环境并“直接细粒度编辑”周边交通体的未来轨迹，在线生成合理但具有对抗性的情景，形成多样化课程以挑战并改进自动驾驶策略。

Result: 与传统方法相比，VILTA能产生更丰富且可信的危险场景，训练后的自动驾驶策略在应对长尾关键事件上的安全性与鲁棒性显著提升。

Conclusion: 将VLM纳入闭环并直接编辑轨迹可打破两阶段框架的瓶颈，充分利用VLM泛化能力，构建多样对抗情景，实证表明能有效提升自动驾驶在长尾场景下的表现。

Abstract: The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories. This direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.

</details>


### [124] [Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement](https://arxiv.org/abs/2601.12682)
*Banglei Guan,Dongcai Tan,Jing Tao,Ang Su,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种融合-复原图像处理方法，抑制高温环境下热辐射与热流扰动对DIC测量的干扰，显著提升可用区域与应变测量精度。


<details>
  <summary>Details</summary>
Motivation: 高温结构变形测量中，热辐射造成成像退化、热流导致的“热晕/热扰动”产生随机高频误差，严重降低DIC的有效性与精度，亟需稳健的图像处理策略以提升图像质量并降低测量误差。

Method: 1) 针对热辐射引起的退化：采用图像分层表示，将图像分解为正负通道并行处理，再通过多曝光图像融合进行质量优化；2) 针对热晕引入的随机误差：以FSIM作为目标函数迭代优化模型参数；并结合灰度均值算法均衡异常灰度，抑制高频噪声，降低DIC误差。

Result: 多曝光融合在复杂光照下有效抑制退化：欠曝图像有效计算区域由26%提升至50%，过曝由32%提升至40%，且不降低测量精度；融合与灰度均值联合的复原流程显著降低静态热变形应变误差：ε_xx误差降85.3%，ε_yy降36.0%，γ_xy降36.4%。

Conclusion: 所提融合-复原方法能在高温环境中显著提升图像质量与DIC变形测量精度，扩大可用计算区域，具备在热变形测量中的应用潜力。

Abstract: In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in ε_xx is reduced by 85.3%, while the errors in ε_yy and γ_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.

</details>


### [125] [GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation](https://arxiv.org/abs/2601.12683)
*Liwei Liao,Ronggang Wang*

Main category: cs.CV

TL;DR: 提出GaussianTrimmer：一种在线边界修剪的即插即用后处理方法，通过虚拟相机生成与基于2D分割的原语级裁剪，缓解3D Gaussian分割因大尺度高斯跨越前景/背景导致的锯齿边界问题，显著提升分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian的场景分割以高斯原语为单位直接分割。由于高斯尺度变化大，大尺寸高斯常跨越物体边界，造成前景与背景混杂、分割轮廓锯齿且不精细，影响应用效果。

Method: 提出GaussianTrimmer作为后处理：1) 生成覆盖均匀、视角充分的虚拟相机；2) 在这些视角下进行2D分割，将结果回投到3D，对跨界的高斯原语进行基于2D证据的原语级切割/修剪，从而细化边界。该流程在线、效率高、可与任意现有3D Gaussian分割方法无缝对接。

Result: 大量定量与定性实验表明，作为即插即用模块，GaussianTrimmer能稳定提升现有3D Gaussian分割方法的边界质量与整体指标，减少锯齿与前景-背景混淆。

Conclusion: 通过虚拟相机+2D分割指导的原语级修剪，可有效补足3D Gaussian分割在大尺度高斯边界处理上的短板，方法简单高效、通用性强，可直接集成以提升分割精度。

Abstract: With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.

</details>


### [126] [Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation](https://arxiv.org/abs/2601.12697)
*Chao Yang,Deshui Miao,Chao Tian,Guoqing Zhu,Yameng Gu,Zhenyu He*

Main category: cs.CV

TL;DR: 提出IVGF框架：用多模态2D输入重建场景几何并直接渲染融合图像，引入跨模态不透明度调制(CMA)与融合损失，缓解红外/可见冲突并保留各自关键特征，实验显示有效。


<details>
  <summary>Details</summary>
Motivation: 现有红外-可见图像融合多为2D、固定视角，难以全面理解复杂场景，导致关键信息丢失与跨模态冲突。需要能利用多视角/几何的表示来提高融合质量与一致性。

Method: 构建Infrared-Visible Gaussian Fusion(IVGF)：从红外与可见2D输入重建场景几何（基于高斯表示/渲染），并直接渲染融合图。核心是跨模态调整(CMA)模块，通过调制高斯的不透明度解决跨模态冲突；配合设计的融合损失，约束优化以保留两模态的独特与关键特征。

Result: 在定性与定量实验上优于或至少竞争，能更好保留红外显著目标与可见细节；在复杂场景和不同视角下呈现更稳健的融合效果。

Conclusion: 重建3D几何并在高斯表示中进行跨模态不透明度调制与融合损失优化，可以有效解决红外-可见融合中的跨模态冲突并保留关键信息，适合复杂场景与多视角渲染的融合应用。

Abstract: Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.

</details>


### [127] [P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2601.12714)
*Songlin Dong,Jiangyang Li,Chenhao Ding,Zhiheng Ma,Haoyu Luo,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: 提出P2L-CA：通过提示到标签与连续适配器，实现多标签增量学习的高效训练与强泛化，无需记忆缓冲且参数开销低，在MS-COCO与VOC上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多标签类增量学习方法要么需全参数微调和较大样本回放导致计算与存储成本高，要么难以解决多目标场景中的特征混淆与预训练到下游的域偏差问题。

Method: 提出参数高效框架P2L-CA：1）P2L模块以类特定prompt解耦多标签表示，并引入语言先验稳定语义-视觉对齐；2）CA模块以轻量adapter缓解预训练模型与下游任务的域间隙，提升可塑性；整体训练无需记忆缓冲且仅调整少量参数。

Result: 在MS-COCO与PASCAL VOC上、包含标准与更具挑战的MLCIL设置中，较现有方法取得显著性能提升，并在通用增量学习场景中表现出强泛化能力，同时训练参数量极少且无需存储回放样本。

Conclusion: P2L-CA有效解决多标签增量学习中的表示混淆与域偏差问题，以极低参数代价和零缓冲实现SOTA级性能与良好泛化，适合资源受限与实际部署场景。

Abstract: Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.

</details>


### [128] [RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels](https://arxiv.org/abs/2601.12715)
*Chengzhou Li,Ping Guo,Guanchen Meng,Qi Jia,Jinyuan Liu,Zhu Liu,Xiaokang Liu,Yu Liu,Zhongxuan Luo,Xin Fan*

Main category: cs.CV

TL;DR: 提出RSOD教师-学生框架，在极少标注的声呐图像上通过可靠度评分与目标混合伪标签、可靠度引导的自适应约束，充分利用未标注数据，5%标注即可接近/匹配全标注基线；并发布新数据集。


<details>
  <summary>Details</summary>
Motivation: 声呐图像纹理少、噪声多、类间细微差异难以由非专家准确标注，导致高质量标注稀缺；需要能在极少标注下仍能有效检测目标的方法。

Method: 半监督教师-学生架构RSOD：1) 多视角一致性估计教师预测的可靠度分数；2) 基于可靠度的“目标混合”伪标签策略，缓解标注不足；3) 可靠度引导的自适应约束优化学生，使其更好利用未标注数据。

Result: 在UATD数据集上，仅用5%标注即可取得可与100%标注训练的基线相当的检测性能；此外收集并发布了新的声呐数据集。

Conclusion: 通过可靠度建模与伪标签策略，RSOD能在极少标注的声呐检测任务中充分挖掘未标注数据的价值，显著减少对昂贵标注的依赖，并具备推广潜力。

Abstract: Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.

</details>


### [129] [S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation](https://arxiv.org/abs/2601.12719)
*Lin Zhao,Yushu Wu,Aleksei Lebedev,Dishani Lahiri,Meng Dong,Arpit Sahni,Michael Vasilkovsky,Hao Chen,Ju Hu,Aliaksandr Siarohin,Sergey Tulyakov,Yanzhi Wang,Anil Kag,Yanyu Li*

Main category: cs.CV

TL;DR: S2DiT是一种为移动端流式视频生成优化的“夹心式”扩散Transformer，通过高效注意力与蒸馏实现接近服务器端SOTA的质量，并在iPhone上达10+ FPS。


<details>
  <summary>Details</summary>
Motivation: 现有DiT虽能提升视频生成质量，但计算成本高，难以实时或在设备端运行。需要一种既高保真又高效率、还能流式生成的视频模型，适配移动硬件。

Method: 提出S2DiT：1) 架构上采用“夹心(sandwich)”设计，在层/模块配置上通过预算感知的动态规划搜索得到最佳质量-效率折中；2) 注意力机制上混合两种高效注意力：LinConv Hybrid Attention (LCHA) 与 Stride Self-Attention (SSA)，在生成更多token的同时控制复杂度；3) 训练上采用2-in-1蒸馏框架，将大型教师（如Wan 2.2-14B）的能力迁移到少步数、紧凑的夹心模型中；4) 支持流式生成以实现在线播放。

Result: 在移动设备上实现>10 FPS的流式视频生成；质量与服务器端最先进模型相当，同时显著降低计算与延迟。

Conclusion: S2DiT通过结构搜索的夹心设计、高效注意力与双重蒸馏，将高保真视频生成带到移动端，兼顾实时性与质量，为端侧流式生成提供可行方案。

Abstract: Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.

</details>


### [130] [DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition](https://arxiv.org/abs/2601.12729)
*Hanyu Zhu,Zhihao Zhan,Yuhang Ming,Liang Li,Dibo Hou,Javier Civera,Wanzeng Kong*

Main category: cs.CV

TL;DR: 提出DC-VLAQ框架，通过融合DINOv2与CLIP的互补特征并引入VLAQ全局聚合，提升VPR在视角、光照与域迁移下的稳健性，取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有VPR在大视角变化、光照变化与严重域迁移下难以保持全局表征的判别性；多数方法仅用单一VFM，忽视模型间互补信息；多模型融合会改变token分布，破坏现有基于查询的全局聚合稳定性。

Method: 1) 残差引导的互补融合：以DINOv2特征为空间锚，通过可学习残差注入CLIP的补充语义，实现轻量融合且保持DINOv2表征稳定性。2) VLAQ（Vector of Local Aggregated Queries）：基于“查询—残差”的全局聚合，将局部token对可学习查询的残差响应进行编码，以增强聚合稳定性并保留细粒度判别线索。

Result: 在Pitts30k、Tokyo24/7、MSLS、Nordland、SPED、AmsterTime等VPR基准上，较强基线均有稳定提升，并在复杂域迁移与长期外观变化场景取得SOTA性能。

Conclusion: 融合互补VFM并采用残差式查询聚合能在改变token分布的情况下保持全局表征稳定、充分利用细粒度线索，从而显著提升VPR鲁棒性与跨域泛化。

Abstract: One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.

</details>


### [131] [KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction](https://arxiv.org/abs/2601.12736)
*Qingtian Zhu,Xu Cao,Zhixiang Wang,Yinqiang Zheng,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 提出KaoLRM，把大型重建模型LRM的已学3D先验重定向到单视图参数化人脸重建，显著提升跨视角一致性和精度。


<details>
  <summary>Details</summary>
Motivation: 3DMM参数化人脸重建可解释且紧凑，但现有回归器在不同视角下不稳定、跨视角一致性差。作者希望借助大模型的3D先验克服自遮挡与视角变化带来的退化。

Method: 在LRM中引入基于FLAME的2D Gaussian Splatting渲染：将LRM预训练的triplane特征投射到FLAME参数空间以回归几何；外观用与FLAME网格紧耦合的2D高斯基元建模。这样让FLAME回归器“感知”3D结构，实现稳健跨视角重建。

Result: 在受控与真实场景基准上优于现有方法，重建精度和跨视角一致性更好，尤其在自遮挡与多视角条件下表现稳健。

Conclusion: 利用LRM的丰富3D先验并与FLAME+2D Gaussian Splatting融合，可实现更准确、跨视角一致的单视图参数化人脸重建；现有方法对视角仍敏感。

Abstract: We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.

</details>


### [132] [SSPFormer: Self-Supervised Pretrained Transformer for MRI Images](https://arxiv.org/abs/2601.12747)
*Jingkai Li,Xiaoze Tian,Yuhang Shen,Jia Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 提出SSPFormer，一种面向MRI的自监督预训练Transformer，通过频域掩码与噪声增强，学习结构感知且对伪影鲁棒的特征，在分割、超分辨与去噪上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 通用视觉Transformer在自然图像上泛化强，但迁移到MRI受制于（1）医学解剖结构的特异性导致域差距；（2）医疗数据隐私与稀缺导致标注不足。需要在无标注原始数据上学习MRI特定、对伪影鲁棒的表示。

Method: 提出自监督预训练框架SSPFormer：1）逆频率投影掩码，优先遮蔽并重建高频解剖区域，强化结构感知表示；2）频率加权FFT噪声增强，在k-space中注入符合生理特性的噪声，提升对真实MRI伪影的鲁棒性；在无标注原始扫描上预训练，再迁移至下游任务。

Result: 在分割、超分辨、去噪三类任务上取得SOTA性能，展示出对细粒度MRI保真度的捕获与更强临床适配性。

Conclusion: 利用频域感知的自监督预训练能有效弥合自然图像到MRI的域差、缓解数据稀缺，并学得对伪影鲁棒的通用MRI表示，推动多任务临床应用表现提升。

Abstract: The pre-trained transformer demonstrates remarkable generalization ability in natural image processing. However, directly transferring it to magnetic resonance images faces two key challenges: the inability to adapt to the specificity of medical anatomical structures and the limitations brought about by the privacy and scarcity of medical data. To address these issues, this paper proposes a Self-Supervised Pretrained Transformer (SSPFormer) for MRI images, which effectively learns domain-specific feature representations of medical images by leveraging unlabeled raw imaging data. To tackle the domain gap and data scarcity, we introduce inverse frequency projection masking, which prioritizes the reconstruction of high-frequency anatomical regions to enforce structure-aware representation learning. Simultaneously, to enhance robustness against real-world MRI artifacts, we employ frequency-weighted FFT noise enhancement that injects physiologically realistic noise into the Fourier domain. Together, these strategies enable the model to learn domain-invariant and artifact-robust features directly from raw scans. Through extensive experiments on segmentation, super-resolution, and denoising tasks, the proposed SSPFormer achieves state-of-the-art performance, fully verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical application requirements.

</details>


### [133] [Moaw: Unleashing Motion Awareness for Video Diffusion Models](https://arxiv.org/abs/2601.12761)
*Tianqi Zhang,Ziyi Wang,Wenzhao Zheng,Weiliang Chen,Yuanhui Huang,Zhengyang Huang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出Moaw：将视频扩散模型从生成转为稠密跟踪学习，并把学到的最强运动特征零样本注入同构生成模型，实现运动感知→运动迁移的桥接。


<details>
  <summary>Details</summary>
Motivation: 无监督/零样本地利用视频扩散模型的隐式对应性已能做光流、跟踪，但能力受限；若进行监督训练，或可更充分挖掘其运动表征，并把这种运动理解用于可控视频生成与运动迁移。

Method: 两阶段框架Moaw：1) 训练一个“视频→稠密跟踪”的扩散模型用于运动感知（从原本图像→视频生成的模态转为视频→光流/轨迹预测），获得表征运动的中间特征；2) 构建带运动标注的数据集，度量并选择编码最强运动信息的通道/层特征；3) 将这些特征无缝注入到结构完全同构的视频生成扩散模型中，利用网络同质性实现零样本适配，无需额外adapter，完成运动迁移与可控生成。

Result: 经由与生成模型同构的感知模型抽取并注入的运动特征，可在零样本条件下为视频生成模型提供更强的运动控制与迁移能力；在跟踪/光流等任务中达到更好的性能（摘要未给具体数值）。

Conclusion: Moaw展示了以监督方式激发视频扩散模型的运动感知能力，并将其无缝转移到生成模型，建立了生成与运动理解之间的新桥梁，为统一、可控的视频学习框架铺路。

Abstract: Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.

</details>


### [134] [Towards Unbiased Source-Free Object Detection via Vision Foundation Models](https://arxiv.org/abs/2601.12765)
*Zhi Cai,Yingjie Gao,Yanan Zhang,Xinzhu Ma,Di Huang*

Main category: cs.CV

TL;DR: 提出DSOD，一个借助VFM缓解源偏置的无源域目标检测框架；通过UFI(含SSE与DAAW)注入多尺度与域自适应特征，并用SAFR正则防止过拟合；另给出无需VFM的DSOD-distill（双教师蒸馏）。在多项基准上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Source-Free Object Detection在目标域自训练时仍受源域偏置影响，导致泛化差与误差累积；需要借助更强先验（如VFM）和正则化来打破对源域的依赖。

Method: 1) 引入VFM辅助：Unified Feature Injection（UFI）把VFM特征注入CNN骨干，含Simple-Scale Extension（SSE）进行多尺度扩展，和Domain-aware Adaptive Weighting（DAAW）按域特性自适应融合权重。2) 语义感知特征正则（SAFR）约束特征学习，避免过拟合源域。3) 资源受限场景下提出DSOD-distill：通过双教师蒸馏（VFM教师与自训练教师）实现无VFM推理。

Result: 在多基准显著提升：Normal→Foggy 48.1% AP，跨场景39.3% AP，合成→真实61.4% AP，均优于现有SFOD方法。

Conclusion: 借助VFM特征注入与语义正则可有效缓解源偏置，提升SFOD在多域迁移上的泛化；通过双教师蒸馏可在无需VFM的条件下保持性能，具有实用性。

Abstract: Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.

</details>


### [135] [Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration](https://arxiv.org/abs/2601.12766)
*Lu Yue,Yue Fan,Shiwei Lian,Yu Zhao,Jiaxin Yu,Liang Xie,Feitian Zhang*

Main category: cs.CV

TL;DR: 提出Spatial-VLN：通过增强空间感知与多专家推理，显著提升零样本VLN在连续复杂环境中的成功率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 零样本VLN依赖LLM具备强泛化，但在连续环境中空间感知不足，尤其在开门交互、多房间穿行、含糊指令执行三类场景下易失败，亟需提升跨视角一致、区域层级的空间表征与决策能力。

Method: 提出感知引导的探索框架Spatial-VLN：1) 空间感知增强（SPE）：全景过滤+门/区域专家，生成空间一致、跨视角一致的感知表示；2) 探索式多专家推理（EMR）：并行LLM专家分别处理航点语义与区域级空间转移；若专家预测不一致，触发“查询-探索”机制，主动探查关键区域以消除感知歧义。另引入基于价值的航点采样策略以缩小Sim2Real鸿沟。

Result: 在VLN-CE上以低成本LLM达到SOTA性能；在真实世界中通过价值式航点采样实现更好的泛化与稳健性。

Conclusion: 通过将空间一致的感知表示与多专家并行推理及主动探索相结合，Spatial-VLN有效解决门交互、多房间导航与含糊指令等空间瓶颈，推进零样本VLN在模拟与真实环境中的实用性与鲁棒性。

Abstract: Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/.

</details>


### [136] [Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval](https://arxiv.org/abs/2601.12768)
*Zequn Xie,Boyun Zhang,Yuxiao Lin,Tao Jin*

Main category: cs.CV

TL;DR: 提出HVP-Net，通过挖掘视觉编码器中多个中间层的层级特征，去冗余并保留关键语义，从而显著提升视频-文本检索的匹配精度，在MSRVTT、DiDeMo、ActivityNet上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VTR多依赖CLIP等预训练模型的最终层全局特征，无法充分利用视频的层级语义且易受视频时间与空间冗余影响，导致匹配不精确。需要一种方法在多语义层次上提取并筛选关键信息，提升跨模态对齐。

Method: 构建HVP-Net（Hierarchical Visual Perception Network）：从视觉编码器的多个中间层提取patch-token级特征；以层级式（由浅入深）逐步提炼显著视觉概念，进行去冗余与细粒度保留；将不同语义层级的表示融合为更鲁棒的视频表征，以与文本对齐进行检索。

Result: 在MSRVTT、DiDeMo、ActivityNet等基准上取得新的SOTA检索性能，表明多层级特征的挖掘和融合有效提升匹配准确率与鲁棒性。

Conclusion: 利用视觉编码器的层级中间特征并进行逐级蒸馏与去冗余，可构建更强的视频表示，显著改进视频-文本检索。代码开源于GitHub（HVP-Net）。

Abstract: Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.

</details>


### [137] [Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image](https://arxiv.org/abs/2601.12770)
*Shuling Zhao,Dan Xu*

Main category: cs.CV

TL;DR: 单张图像生成可动画的3D全头头像：以UV空间高斯原语绑定到参数化人脸模型，一次前向即可重建并实时动画与360°渲染；结合3D全头GAN先验做全局监督，再与输入局部细节融合，显著提升真实感与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单张头像重建在大视角/姿态变化下易崩溃，难以同时兼顾全头几何、纹理与实时动画控制；需要一种既能一-shot重建、又能360°视图与高保真动画的方案。

Method: 1) 在参数化人脸模型的UV空间上嵌入高斯基元表示3D头部表面；2) 利用预训练的3D全头GAN作为先验，提取全局全头特征并提供多视角监督；3) 利用人脸与UV空间对称性，将输入图像的局部细节与全局全头纹理融合；4) 单次前向推理获得可动画全头模型，支持实时渲染。

Result: 在多项实验中，实现高质量的3D全头重建与实时动画，对大幅相机姿态变化具有更强鲁棒性，可进行同时的360°视图渲染和自然说话头像驱动。

Conclusion: 提出的框架在单张输入下即可获得高保真、可控、可实时渲染的3D全头头像，改进了说话头像的真实感与稳定性。

Abstract: Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.

</details>


### [138] [Open Vocabulary Panoptic Segmentation With Retrieval Augmentation](https://arxiv.org/abs/2601.12779)
*Nafis Sadeq,Qingfeng Liu,Mostafa El-Khamy*

Main category: cs.CV

TL;DR: 提出RetCLIP：在开放词汇全景分割中，用检索增强的方式结合CLIP与特征库，提高未见类别性能。


<details>
  <summary>Details</summary>
Motivation: 传统全景分割在训练集外类别泛化差；开放词汇场景需要根据任意文本类别进行分割与识别，如何提升未见类的识别能力是难点。

Method: 利用成对图文数据构建“遮罩片段（masked segment）特征库”。在推理时，将输入图像的遮罩片段特征作为查询，从库中检索相似片段及其类别；将检索相似度转为分类分数，并与CLIP文本-图像相似度分数融合，集成到SOTA方法FC-CLIP框架中产生最终全景输出。

Result: 在COCO训练、ADE20K评测条件下，方法达成30.9 PQ、19.3 mAP、44.0 mIoU，相比基线提升+4.5 PQ、+2.5 mAP、+10.0 mIoU。

Conclusion: 检索增强能有效补充CLIP的开放词汇能力，显著提升未见类全景分割性能，并可无缝集成到现有框架（如FC-CLIP）。

Abstract: Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.

</details>


### [139] [SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification](https://arxiv.org/abs/2601.12791)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Hongyuan Shu,Junchu Zhao,Yanjun Huang,Yue Xiu,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: 提出SKANet：融合时频图(TFI)与功率谱密度(PSD)的双流网络，用选择性卷积核与非对称卷积自适应感受野，并在融合处用SE重标定，多模态捕捉瞬时与全局特征，实现复杂复合干扰高鲁棒分类，准确率96.99%，低JNR下仍稳健。


<details>
  <summary>Details</summary>
Motivation: GNSS在复杂电磁环境下面临复合干扰（多源叠加），传统单域或固定感受野方法难以同时提取瞬时脉冲与连续谱特征，导致分类性能在低JNR及复合场景下下降。需要一种能跨模态、跨尺度自适应提取特征的深度学习框架。

Method: 构建双流认知深度学习架构SKANet：输入为TFI与PSD。引入多分支Selective Kernel模块+Asymmetric Convolution Blocks，使网络可动态调整感受野，兼顾微尺度瞬时与宏尺度谱趋势；在融合阶段加入Squeeze-and-Excitation机制，对两模态与通道特征进行自适应重加权，从而实现空间-时间-频率联合自适应。

Result: 在包含405,000样本的数据集上整体准确率96.99%，对复合干扰分类优于现有方法，尤其在低JNR条件下表现更稳健。

Conclusion: 动态可调感受野与多模态自适应融合是解决复合干扰识别的关键；SKANet有效弥合瞬时与全局特征提取的尺度冲突，在复杂与低信噪环境下显著提升GNSS干扰分类性能。

Abstract: As the electromagnetic environment becomes increasingly complex, Global Navigation Satellite Systems (GNSS) face growing threats from sophisticated jamming interference. Although Deep Learning (DL) effectively identifies basic interference, classifying compound interference remains difficult due to the superposition of diverse jamming sources. Existing single-domain approaches often suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales. We propose the Selective Kernel and Asymmetric convolution Network(SKANet), a cognitive deep learning framework built upon a dual-stream architecture that integrates Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Distinct from conventional fusion methods that rely on static receptive fields, the proposed architecture incorporates a Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs). This mechanism enables the network to dynamically adjust its receptive fields, acting as an adaptive filter that simultaneously captures micro-scale transient features and macro-scale spectral trends within entangled compound signals. To complement this spatial-temporal adaptation, a Squeeze-and-Excitation (SE) mechanism is integrated at the fusion stage to adaptively recalibrate the contribution of heterogeneous features from each modality. Evaluations on a dataset of 405,000 samples demonstrate that SKANet achieves an overall accuracy of 96.99\%, exhibiting superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.

</details>


### [140] [Combating Noisy Labels through Fostering Self- and Neighbor-Consistency](https://arxiv.org/abs/2601.12795)
*Zeren Sun,Yazhou Yao,Tongliang Liu,Zechao Li,Fumin Shen,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出Jo-SNC：联合样本选择与模型正则化，在自一致与邻居一致性上鲁棒处理标签噪声与OOD噪声，含自适应阈、部分标签学习、负学习与三重一致性正则，SOTA 实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实数据常含标签噪声，深度网络受记忆效应影响易拟合噪声；现有方法多只挑“干净”样本，忽视小批次间噪声不均与OOD噪声，导致鲁棒性与泛化不足。

Method: 1) 用Jensen–Shannon散度度量样本为干净/ID噪声/OOD噪声的可能性，并结合最近邻信息提升判别稳定性；2) 基于数据驱动的按类自适应阈值进行样本选择；3) 训练策略分治：对干净样本常规监督；对识别为ID噪声样本用部分标签学习（弱化不确定标签）；对OOD噪声样本用负学习（显式排斥错误标签）；4) 提出三重一致性正则：自预测一致性、邻居预测一致性与特征一致性，联合提升稳健性。

Result: 在多项基准数据集上（未给出具体名称）进行大量实验与消融，整体优于已有SOTA方法，验证方法在不同噪声设置下的有效性与稳定性。

Conclusion: 结合自/邻居一致性的联合样本选择与正则化，可同时处理类内标签噪声与OOD噪声；自适应阈与分治训练策略提升鲁棒性，三重一致性进一步增强性能，整体实现对噪声标签场景的SOTA表现。

Abstract: Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (\textbf{Jo}int sample selection and model regularization based on \textbf{S}elf- and \textbf{N}eighbor-\textbf{C}onsistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the ``likelihood'' of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods.

</details>


### [141] [PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition](https://arxiv.org/abs/2601.12798)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Yue Xiu,Lu Chen,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: 提出PhyG-MoE：基于物理引导的专家混合模型，通过频谱门控按信号复杂度动态分配算力，在21类干扰识别上达97.58%准确率，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: GNSS在SAGIN中易受复杂电磁干扰影响；现有深度学习识别模型为静态拓扑，无法根据输入物理熵调整算力，导致简单样本和饱和混合干扰消耗相同计算资源，出现资源错配与时延问题。

Method: 提出物理引导的MoE框架：以频谱特征纠缠度为依据的门控机制对输入路由；复杂、饱和场景启用高容量TransNeXt专家进行特征解缠；基础、简单信号由轻量专家处理以降低延迟。核心在于动态对齐模型容量与信号复杂度。

Result: 在21类干扰数据集上总体准确率97.58%；在保持或提升识别性能的同时显著降低计算开销（文中强调无性能劣化），实现更高的资源利用效率。

Conclusion: 通过解决静态计算与动态电磁环境的内在冲突，PhyG-MoE为资源受限的认知接收机提供了可行方案：按需启用高容量专家处理复杂干扰，常态使用轻量专家以节省算力与时延。

Abstract: Complex electromagnetic interference increasingly compromises Global Navigation Satellite Systems (GNSS), threatening the reliability of Space-Air-Ground Integrated Networks (SAGIN). Although deep learning has advanced interference recognition, current static models suffer from a \textbf{fundamental limitation}: they impose a fixed computational topology regardless of the input's physical entropy. This rigidity leads to severe resource mismatch, where simple primitives consume the same processing cost as chaotic, saturated mixtures. To resolve this, this paper introduces PhyG-MoE (Physics-Guided Mixture-of-Experts), a framework designed to \textbf{dynamically align model capacity with signal complexity}. Unlike static architectures, the proposed system employs a spectrum-based gating mechanism that routes signals based on their spectral feature entanglement. A high-capacity TransNeXt expert is activated on-demand to disentangle complex features in saturated scenarios, while lightweight experts handle fundamental signals to minimize latency. Evaluations on 21 jamming categories demonstrate that PhyG-MoE achieves an overall accuracy of 97.58\%. By resolving the intrinsic conflict between static computing and dynamic electromagnetic environments, the proposed framework significantly reduces computational overhead without performance degradation, offering a viable solution for resource-constrained cognitive receivers.

</details>


### [142] [Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data](https://arxiv.org/abs/2601.12809)
*Takaki Yamamoto,Chihiro Noguchi,Toshihiro Tanizawa*

Main category: cs.CV

TL;DR: 论文构建一个可控的一维图文测试平台，研究CLIP式对比学习中左-右关系是如何出现的，并用注意力分解揭示位置与词元嵌入交互产生水平注意力梯度，从而打破左右对称并实现关系判别。


<details>
  <summary>Details</summary>
Motivation: 空间关系（尤其左右）一直是视觉-语言模型的薄弱点，社区不清楚模型是否真正学到、以及通过什么机制学到。现有复杂数据和模型使因果归因困难，因而需要一个可控、可解释的设置来隔离机制。

Method: 搭建一维（1D）图像-文本场景：包含一物体与两物体配置，使用轻量Transformer视觉与文本编码器，在CLIP式对比目标下端到端训练。系统控制并独立操纵标签多样性与版式（布局）多样性，评测对未见物体配对的泛化。对训练好的编码器做注意力分解，分析位置嵌入与词元嵌入的交互如何影响注意力分布；进行消融以验证因果。

Result: 对比学习能学到左右关系；在泛化到未见物体对时，标签多样性比布局多样性更关键。注意力分解显示位置与词元嵌入的交互诱发水平注意力梯度，打破左右对称；移除该贡献显著削弱左右判别能力。

Conclusion: CLIP式模型在受控设置下可获得左-右关系能力，其泛化主要受标签多样性驱动；其机制在于位置与词元嵌入交互形成的注意力梯度。该工作为何时、如何获得关系能力提供了机械层面的证据。

Abstract: Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.

</details>


### [143] [CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting](https://arxiv.org/abs/2601.12814)
*Yu-Jen Tseng,Chia-Hao Kao,Jing-Zhong Chen,Alessandro Gnutti,Shao-Yuan Lo,Yen-Yu Lin,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 提出首个统一框架，将3D高斯泼溅(3DGS)的有损压缩与语义分割在率失真优化下联合建模，实现低码率、高渲染质量并支持解码端的语义应用。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS分别在实时渲染与语义理解上表现突出，但压缩与分割多被分离处理，难以同时满足传输效率与解码端语义操作（如编辑、操控）的需求；已有压缩方法多依赖网格型超先验，复杂且开销大，缺乏对语义属性的高效编码与训练协同。

Method: 构建统一的率失真优化管线，将语义学习嵌入压缩流程；设计轻量的隐式神经表示超先验，用于颜色与语义属性的高效熵编码，避免昂贵的网格超先验；提出压缩引导的分割学习：量化感知训练提升特征可分性，以及质量感知加权抑制低可靠高斯基元对分割的干扰。

Result: 在LERF与3D-OVS数据集上显著降低传输码率，同时保持高保真渲染质量并维持/提升语义分割性能，验证了联合优化的有效性。

Conclusion: 联合的率失真优化压缩-分割框架可在不牺牲渲染质量的前提下，显著节省带宽并支持解码端语义应用；轻量隐式超先验与压缩引导的训练策略是关键。

Abstract: We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.

</details>


### [144] [A Generalist Foundation Model for Total-body PET/CT Enables Diagnostic Reporting and System-wide Metabolic Profiling](https://arxiv.org/abs/2601.12820)
*Wei Chen,Liang Wu,Shuyi Lu,Yuanyuan Sun,Wenkai Bi,Zilong Yuan,Yaoyao He,Feng Wang,Junchi Ma,Shuyong Liu,Zhaoping Cheng,Xiaoyan Hu,Jianfeng Qiu*

Main category: cs.CV

TL;DR: 提出SDF-HOLO，多模态基础模型用于全身PET/CT，采用双流编码、跨模态交互、分层上下文与“体素-掩膜-文本”对齐，在分割、低剂量检测与多语种报告生成上优于强基线，并能进行系统级代谢网络分析。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI多假设单模态、局部视野、粗糙图文对齐，难以处理全身PET/CT的异质解剖/代谢信号、长轴覆盖及结构化放射语义，亟需能实现全身范围、细粒度对齐与跨模态推理的基础模型。

Method: 构建SDF-HOLO：1) 双流编码器分别学习CT与PET表征；2) 跨模态交互模块使解剖上下文优化PET聚合、代谢显著性反哺形态细节推理；3) 分层上下文建模将高效局部窗口与全局注意力结合以覆盖全身长程依赖；4) 以解剖分割掩膜为显式语义锚点，在预训练中进行体素-掩膜-文本三方对齐。

Result: 在肿瘤分割、低剂量病灶检测、多语种诊断报告生成任务上，超越任务特定与临床参考基线，降低定位误差与幻觉；可进行系统范围代谢特征描绘，发现肿瘤相关的跨器官代谢网络指纹。

Conclusion: SDF-HOLO为全身PET/CT提供可扩展的多模态基础模型框架，提升多任务临床性能并支持系统层精准肿瘤学与代谢网络解析。

Abstract: Total-body PET/CT enables system-wide molecular imaging, but heterogeneous anatomical and metabolic signals, approximately 2 m axial coverage, and structured radiology semantics challenge existing medical AI models that assume single-modality inputs, localized fields of view, and coarse image-text alignment. We introduce SDF-HOLO (Systemic Dual-stream Fusion Holo Model), a multimodal foundation model for holistic total-body PET/CT, pre-trained on more than 10,000 patients. SDF-HOLO decouples CT and PET representation learning with dual-stream encoders and couples them through a cross-modal interaction module, allowing anatomical context to refine PET aggregation while metabolic saliency guides subtle morphological reasoning. To model long-range dependencies across the body, hierarchical context modeling combines efficient local windows with global attention. To bridge voxels and clinical language, we use anatomical segmentation masks as explicit semantic anchors and perform voxel-mask-text alignment during pre-training. Across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation, SDF-HOLO outperforms strong task-specific and clinical-reference baselines while reducing localization errors and hallucinated findings. Beyond focal interpretation, the model enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions, providing a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology.

</details>


### [145] [TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement](https://arxiv.org/abs/2601.12823)
*Belal Shaheen,Minh-Hieu Nguyen,Bach-Thuan Bui,Shubham,Tim Wu,Michael Fairley,Matthew David Zane,Michael Wu,James Tompkin*

Main category: cs.CV

TL;DR: TreeDGS利用3D Gaussian Splatting从航拍图像中稠密化树干几何并进行不依赖激光雷达的胸径（DBH）测量，达成4.79cm RMSE，优于LiDAR基线。


<details>
  <summary>Details</summary>
Motivation: 航拍遥感覆盖广但自然场景中目标级精确测量困难，尤其林木胸径；常规NeRF/GS虽提升重建保真与可稠密化几何，但树干在高空成像中仅数像素、视角稀疏，导致胸高部位几何约束弱，难以直接测量DBH。

Method: 提出TreeDGS：以SfM-MVS初始化与GS优化得到高保真高斯场；利用RaDe-GS的深度感知累计不透明度积分从高斯场提取稠密点集，并为每个采样点赋予多视角不透明度可靠度；对树干分割后的点进行不透明度加权的实心圆拟合以估计DBH。

Result: 在10个样地与地面实测DBH对比，TreeDGS达成4.79 cm RMSE（约等于该GSD下2.6像素），优于SOTA LiDAR基线的7.91 cm RMSE。

Conclusion: 稠密化的基于splat的几何表示能在航拍条件下实现准确、低成本的树木DBH测量；TreeDGS证明了3D Gaussian Splatting结合可靠度加权拟合可克服视角稀疏与像素尺度受限的问题。

Abstract: Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.

</details>


### [146] [Seeing Isn't Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification](https://arxiv.org/abs/2601.12826)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 论文评估Grad-CAM在肺癌医学影像分类中的解释可靠性，提出量化框架并比较多种架构，发现对CNN较有效但对ViT显著失真，跨模型一致性差，提示需要模型感知的解释方法。


<details>
  <summary>Details</summary>
Motivation: 热图型XAI（如Grad-CAM）广泛用于医疗影像，但其“忠实性”与“可靠性”饱受质疑；在临床高风险场景中，错误解释可能误导诊断与模型选择，因此需要系统量化检验不同模型下Grad-CAM的可信度。

Method: 使用IQ-OTH/NCCD肺癌数据集，选取ResNet-50/101、DenseNet-161、EfficientNet-B0与ViT-Base-Patch16-224五种架构；构建包含三部分的量化评估框架：定位准确度（与肿瘤区域标注对比）、基于扰动的忠实性（遮挡或替换重要区域对预测影响）、解释一致性（不同模型/同模型不同运行的热图相似度）。

Result: Grad-CAM在多数卷积网络上能突出肿瘤显著区域，但在ViT上因非局部注意导致解释忠实性显著下降；跨模型比较显示显著的显著性定位差异与不一致性，表明热图并不总与模型真实诊断证据对应。

Conclusion: 现有基于显著图的XAI在医疗影像存在关键局限：对不同架构敏感、对ViT失真、跨模型一致性差。应发展具备模型感知、兼顾计算合理性与临床意义的解释方法，并以更谨慎、严谨的态度采用可视化解释工具与“可解释性即可信”的假设。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), have become indispensable for visualizing the reasoning process of deep neural networks in medical image analysis. Despite their popularity, the faithfulness and reliability of these heatmap-based explanations remain under scrutiny. This study critically investigates whether Grad-CAM truly represents the internal decision-making of deep models trained for lung cancer image classification. Using the publicly available IQ-OTH/NCCD dataset, we evaluate five representative architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base-Patch16-224, to explore model-dependent variations in Grad-CAM interpretability. We introduce a quantitative evaluation framework that combines localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across architectures. Experimental findings reveal that while Grad-CAM effectively highlights salient tumor regions in most convolutional networks, its interpretive fidelity significantly degrades for Vision Transformer models due to non-local attention behavior. Furthermore, cross-model comparisons indicate substantial variability in saliency localization, implying that Grad-CAM explanations may not always correspond to the true diagnostic evidence used by the networks. This work exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful. Our findings aim to inspire a more cautious and rigorous adoption of visual explanation tools in medical AI, urging the community to rethink what it truly means to "trust" a model's explanation.

</details>


### [147] [FGTBT: Frequency-Guided Task-Balancing Transformer for Unified Facial Landmark Detection](https://arxiv.org/abs/2601.12863)
*Jun Wan,Xinyu Xiong,Ning Chen,Zhihui Lai,Jie Zhou,Wenwen Min*

Main category: cs.CV

TL;DR: 提出FGTBT框架，通过频域建模与多数据集统一训练，结合细粒度多任务平衡损失和频率引导的结构感知模块，在多基准上达到SOTA可比性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习的人脸关键点检测在大姿态、光照、表情变化下难以准确刻画面部几何结构；同时现有数据集规模与多样性不足，导致训练不稳与精度下降。

Method: 1) 频率引导任务平衡Transformer（FGTBT）总体框架。2) 提出细粒度多任务平衡损失FMB-loss：不只在任务层面，而是按关键点在各数据集中的出现频次分配权重，统一多数据集训练并缓解梯度不一致。3) 频率引导结构感知（FGSA）模型：通过频域引导的结构注入与正则化，显式编码/约束面部结构。

Result: 在多个主流基准上，融合FMB-loss与FGSA的FGTBT取得与最新方法相当的性能（SOTA可比），显示出对极端姿态、光照、表情等挑战场景的鲁棒性提升。

Conclusion: 细粒度的关键点级权重平衡与频域结构约束是提升多数据集统一训练下FLD鲁棒性的关键；所提FGTBT在不增加过多复杂度的情况下达到了接近SOTA的效果，并公开了代码以便复现与扩展。

Abstract: Recently, deep learning based facial landmark detection (FLD) methods have achieved considerable success. However, in challenging scenarios such as large pose variations, illumination changes, and facial expression variations, they still struggle to accurately capture the geometric structure of the face, resulting in performance degradation. Moreover, the limited size and diversity of existing FLD datasets hinder robust model training, leading to reduced detection accuracy. To address these challenges, we propose a Frequency-Guided Task-Balancing Transformer (FGTBT), which enhances facial structure perception through frequency-domain modeling and multi-dataset unified training. Specifically, we propose a novel Fine-Grained Multi-Task Balancing loss (FMB-loss), which moves beyond coarse task-level balancing by assigning weights to individual landmarks based on their occurrence across datasets. This enables more effective unified training and mitigates the issue of inconsistent gradient magnitudes. Additionally, a Frequency-Guided Structure-Aware (FGSA) model is designed to utilize frequency-guided structure injection and regularization to help learn facial structure constraints. Extensive experimental results on popular benchmark datasets demonstrate that the integration of the proposed FMB-loss and FGSA model into our FGTBT framework achieves performance comparable to state-of-the-art methods. The code is available at https://github.com/Xi0ngxinyu/FGTBT.

</details>


### [148] [Proxy Robustness in Vision Language Models is Effortlessly Transferable](https://arxiv.org/abs/2601.12865)
*Xiaowei Fu,Fuxiang Huang,Lei Zhang*

Main category: cs.CV

TL;DR: 提出HPT-GPD：利用“代理对抗鲁棒性”在不同架构的CLIP间进行鲁棒性蒸馏，并通过学习率调度的解耦策略在鲁棒性与零样本泛化间取得平衡，跨15个数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: VLM如CLIP在大规模多模态上进行对抗训练成本极高，难以直接构造鲁棒教师；同时鲁棒性蒸馏常导致零样本自然泛化显著下降。

Method: 1) 观察到未对抗训练的CLIP对来自异构CLIP生成的对抗样本具备一定免疫性，定义为“代理对抗鲁棒性”。2) 提出异构代理迁移(HPT)：在不同架构CLIP间建立跨架构鲁棒性蒸馏通道，让目标模型从代理模型继承鲁棒性。3) 为缓解过拟合导致的零样本性能下滑，设计GPD：通过差异化学习率调度将训练解耦为两阶段——(a) 泛化锚定的warm-up阶段保持自然泛化；(b) 泛化牵引的HPT阶段提升对抗鲁棒性，并在两者间寻求平衡。

Result: 在15个零样本数据集上进行大量实验，表明HPT-GPD能在不进行昂贵对抗教师训练的前提下显著提升VLM的对抗鲁棒性，同时维持较好的零样本自然泛化。

Conclusion: 无需高成本对抗教师即可通过代理鲁棒性进行跨架构蒸馏；结合GPD的调度解耦可在鲁棒性与泛化间取得更优权衡，实验证实方法有效。

Abstract: As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.

</details>


### [149] [Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation](https://arxiv.org/abs/2601.12876)
*Zhenxuan Lu,Zhihua Xu,Zhijing Yang,Feng Gao,Yongyi Lu,Keze Wang,Tianshui Chen*

Main category: cs.CV

TL;DR: 提出THFEM框架，将音频驱动说话人头像生成(AD-THG)与保语音的表情编辑(SPFEM)融合，实现在表情操控中精确保留口型同步。


<details>
  <summary>Details</summary>
Motivation: SPFEM在改变表情同时保留嘴部运动，但受表情与口型耦合影响，口唇同步仍不准确；AD-THG在从音频合成精确口型方面表现突出，因此希望融合两者弥补SPFEM的口型缺陷。

Method: 提出THFEM：先用SPFEM对图像进行表情操控，再用AD-THG根据音频生成具精准口型的帧。为缓解AD-THG生成多帧时真实感与表情保真度下降的问题，设计相邻帧学习策略，对AD-THG进行微调，使其预测连续帧并利用相邻帧信息提升质量。

Result: 实验显示，所提框架在表情操控中显著提升口型保持与同步，同时通过相邻帧学习提高生成图像的真实感与表情保真度。

Conclusion: 将AD-THG与SPFEM融合，并结合相邻帧学习，可在表情编辑中有效保留嘴部形状与语音同步，整体画质与表情一致性得到提升。

Abstract: Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.

</details>


### [150] [YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection](https://arxiv.org/abs/2601.12882)
*Sudip Chakrabarty*

Main category: cs.CV

TL;DR: 论文评述：YOLO26 通过取消 NMS、引入 MuSGD、STAL 与 ProgLoss，实现端到端检测，在速度与精度上建立新帕累托前沿，优于既有 YOLO 系列与同类 SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统 YOLO（v1–v11）依赖 NMS 后处理，带来推理延迟、超参敏感与工程复杂度，限制边缘端实时检测的效率与稳定性。作者旨在摆脱启发式 NMS，将检测学习端到端化，解决延迟-精度长期权衡。

Method: 提出三项关键改进：1）MuSGD 优化器：针对轻量骨干的稳定训练与收敛；2）STAL 小目标感知分配策略：改进样本指派以提升小目标召回；3）ProgLoss 渐进式损失：动态监督以对齐端到端学习，无需 NMS。整体架构删除 NMS，直接输出去重后的目标集合。

Result: 基于官方基准的系统性对比，YOLO26 在推理速度与精度上同时超越前代 YOLO 与竞争者（RTMDet、DAMO-YOLO），形成新的帕累托前沿。

Conclusion: 取消 NMS、以端到端学习替代启发式后处理，可同时降低延迟并提升精度，YOLO26 有望成为边缘视觉下一代范式。

Abstract: The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.

</details>


### [151] [Simultaneous Detection of LSD and FMD in Cattle Using Ensemble Deep Learning](https://arxiv.org/abs/2601.12889)
*Nazibul Basar Ayon,Abdul Hasib,Md. Faishal Ahmed,Md. Sadiqur Rahman,Kamrul Islam,T. M. Mehrab Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 提出一个集成深度学习模型（VGG16、ResNet50、InceptionV3加权融合）用于同时检测牛的结节性皮肤病（LSD）和口蹄疫（FMD），在10,516张跨国标注图像上达到约98%的准确率与99.5% AUC，能缓解症状重叠导致的误诊并适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: LSD与FMD高度传染且经济影响巨大，但其皮损、口鼻病变等表征与彼此及良性病变（如虫咬、化学灼伤）高度重叠，人工目检难以及时准确判别，延误防控。迫切需要一个鲁棒、自动化、多病种的视觉诊断工具。

Method: 构建涵盖印度、巴西、美国18个牧场的10,516张专家标注图像数据集；采用VGG16、ResNet50、InceptionV3三模型并进行加权平均的集成学习；针对多疾病重叠场景优化加权策略，实现同时识别LSD与FMD；以准确率、宏平均Precision/Recall/F1和AUC-ROC评估。

Result: 集成模型获得98.2%准确率、宏平均Precision 98.2%、Recall 98.1%、F1 98.1%、AUC-ROC 99.5%，为多疾病视觉诊断达到当前最优水平。

Conclusion: 该加权集成框架有效缓解症状重叠带来的诊断困难，能够实现早期、精准、自动化的LSD与FMD检测；具备在资源有限环境中部署潜力，有望提升畜牧疾病管理并支持农业可持续性。

Abstract: Lumpy Skin Disease (LSD) and Foot-and-Mouth Disease (FMD) are highly contagious viral diseases affecting cattle, causing significant economic losses and welfare challenges. Their visual diagnosis is complicated by significant symptom overlap with each other and with benign conditions like insect bites or chemical burns, hindering timely control measures. Leveraging a comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA, this study presents a novel Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging for simultaneous LSD and FMD detection. The model achieves a state-of-the-art accuracy of 98.2\%, with macro-averaged precision of 98.2\%, recall of 98.1\%, F1-score of 98.1\%, and an AUC-ROC of 99.5\%. This approach uniquely addresses the critical challenge of symptom overlap in multi-disease detection, enabling early, precise, and automated diagnosis. This tool has the potential to enhance disease management, support global agricultural sustainability, and is designed for future deployment in resource-limited settings.

</details>


### [152] [TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation, Detection and Localization in Identity Documents](https://arxiv.org/abs/2601.12895)
*Chan Naseeb,Adeel Ashraf Cheema,Hassan Sami,Tayyab Afzal,Muhammad Omair,Usman Habib*

Main category: cs.CV

TL;DR: 提出TwoHead-SwinFPN：一体化检测与定位身份证件合成篡改（人脸替换与文字修补），在FantasyIDiap上达84.31%准确率、AUC 90.78%、定位Dice 57.24%，分类F1 88.61%，具备FastAPI实用部署。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使身份证件更易被以人脸替换与文本补画方式伪造，现有方法往往仅做二分类或缺乏精细定位，且跨设备与多语言场景鲁棒性不足，难以实用化部署。

Method: 采用Swin Transformer骨干+FPN+UNet式解码器，结合CBAM注意力增强表征；双头架构同时进行篡改二分类与篡改区域分割，并用不确定性加权的多任务学习进行联合优化；提供FastAPI推理；进行消融、跨设备泛化和多语言（10种）与多设备（3种）评估。

Result: 在FantasyIDiap数据集上：分类准确率84.31%、AUC 90.78%、F1 88.61%；篡改区域定位的平均Dice 57.24%；在跨设备与多语言场景下表现稳定（文中详述），计算效率满足实际部署需求。

Conclusion: 双头SwinFPN通过注意力增强与多任务不确定性加权，实现对证件篡改的高效检测与定位，兼顾精度与效率，具备实际落地潜力；消融与泛化实验支持其设计有效性。

Abstract: The proliferation of sophisticated generative AI models has significantly escalated the threat of synthetic manipulations in identity documents, particularly through face swapping and text inpainting attacks. This paper presents TwoHead-SwinFPN, a unified deep learning architecture that simultaneously performs binary classification and precise localization of manipulated regions in ID documents. Our approach integrates a Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM) for improved feature representation. The model employs a dual-head architecture for joint optimization of detection and segmentation tasks, utilizing uncertainty-weighted multi-task learning. Extensive experiments on the FantasyIDiap dataset demonstrate superior performance with 84.31\% accuracy, 90.78\% AUC for classification, and 57.24\% mean Dice score for localization. The proposed method achieves an F1-score of 88.61\% for binary classification while maintaining computational efficiency suitable for real-world deployment through FastAPI implementation. Our comprehensive evaluation includes ablation studies, cross-device generalization analysis, and detailed performance assessment across 10 languages and 3 acquisition devices.

</details>


### [153] [Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection](https://arxiv.org/abs/2601.12919)
*Jun Wan,Yuanzhi Yao,Zhihui Lai,Jie Zhou,Xianxu Hou,Wenwen Min*

Main category: cs.CV

TL;DR: 提出弱监督的“幻觉+迁移”框架SHT，把超分辨(人脸幻觉)与人脸姿态迁移联合进FLD，以在低分辨/标注不足下提升关键点精度，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 低分辨图像或下采样导致深层高分辨特征缺失，影响关键点精度；训练数据不足与标注噪声进一步拉低性能，需要能在弱监督、低分辨条件下恢复细节并提升鲁棒性的方案。

Method: 提出SHT框架，包含两个互促模块：1) 双重幻觉学习网络DHLN：联合人脸关键点检测与人脸幻觉任务，从低分辨输入学习高分辨表征，恢复全局结构与局部细节，生成更有效的关键点热力图；2) 人脸姿态迁移网络FPTN：将人脸从一种姿态变换到另一种姿态，利用姿态变化进一步优化DHLN生成的热力图与幻觉人脸，从而提升定位精度。整体为弱监督训练。

Result: 在面部超分与关键点检测两个任务上，所提方法均超过当时的多项SOTA基线，显示在低分辨与弱监督情境下的明显优势。

Conclusion: 将人脸幻觉与姿态迁移纳入弱监督FLD是有效的；两模块相互促进，可学得高分辨判别表征并提升关键点检测精度，具有通用性与鲁棒性。

Abstract: High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.

</details>


### [154] [Dual-Stream Collaborative Transformer for Image Captioning](https://arxiv.org/abs/2601.12926)
*Jun Wan,Jun Liu,Zhihui lai,Jie Zhou*

Main category: cs.CV

TL;DR: 论文提出一种用于图像描述的双流协同Transformer（DSCT），将目标区域特征与分割特征进行巩固与动态融合，通过互注意与动态解码提升语义相关性与细粒度表达，在多基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于区域特征的图像描述方法虽强，但易因缺乏上下文、以及生成过程中过度依赖已生成片段而产生偏离图像内容的描述；同时，不同模态/表征（区域 vs. 分割）存在语义不一致与空间错配，亟需更有效的融合策略。

Method: 提出DSCT框架，包含：1) 模式特定互注意编码器（PSMAE）：区域与分割两种表征相互查询，突出各自“私有”且互补的信息并进行巩固；2) 动态提名解码器（DND）：根据当前文本表示动态选择最相关的学习块（子模块），并利用经过巩固的区域与分割之间的同质特征来指导词生成；整体流程先分别巩固两路特征，再进行协同融合，用于指导 caption 生成。

Result: 在多个主流数据集上，DSCT取得优于现有最先进图像描述模型的性能（具体指标未给出，但整体领先）。

Conclusion: 动态、模式特定的双流融合可缓解语义不一致与空间错配，提升描述准确性与细粒度度；这是首次在图像描述中以动态方式融合区域与分割特征的工作，并在基准上验证了有效性。

Abstract: Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.

</details>


### [155] [Membership Inference Test: Auditing Training Data in Object Classification Models](https://arxiv.org/abs/2601.12929)
*Gonzalo Mancera,Daniel DeAlcala,Aythami Morales,Ruben Tolosana,Julian Fierrez*

Main category: cs.CV

TL;DR: 研究提出针对目标识别场景的MINT（成员推断测试）架构，结合检测模型与嵌入提取器，通过卷积层建模激活模式，以70%–80%精度判断样本是否参与训练；并分析影响MINT性能的因素以提升训练透明度。


<details>
  <summary>Details</summary>
Motivation: 成员推断攻击/测试在隐私与模型透明度方面重要，但现有方法在目标识别任务（检测、嵌入）上的适配与效率不足，缺乏针对该领域特性的体系化架构与影响因素分析。

Method: 构建由对象检测模型、嵌入提取器与MINT模块组成的流水线；从不同深度的检测模块层获取特征，使用卷积层在MINT中捕捉训练期间的激活模式；在三个公共数据集（总计17.4万+图像）上评估，比较不同输入层深度对判别精度的影响。

Result: MINT在识别样本是否被用于训练与测试时达到约70%–80%精度，性能受选择作为MINT输入的检测模块层深度显著影响。

Conclusion: 面向目标识别的定制MINT架构有效且高效，可通过合适的特征层选择与卷积建模获得较高精度；对影响因素的分析有助于更透明的训练过程与更稳健的隐私评估。

Abstract: In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.

</details>


### [156] [QASA: Quality-Guided K-Adaptive Slot Attention for Unsupervised Object-Centric Learning](https://arxiv.org/abs/2601.12936)
*Tianran Ouyang,Xingping Dong,Jing Zhang,Mang Ye,Jun Chen,Bo Du*

Main category: cs.CV

TL;DR: 提出QASA，一种质量引导的K自适应Slot Attention，通过解耦槽选择与重建、引入无监督槽质量度量与门控解码器，实现动态选择高质量槽并在推理时自适应K，显著优于现有K自适应方法，并在真实数据上超越固定K。


<details>
  <summary>Details</summary>
Motivation: 现有K自适应槽注意力方法仍有两大痛点：1) 未显式约束槽绑定质量，导致特征归因含糊；2) 在重建目标上叠加槽数量惩罚，引发减少槽数与保持重建精度的目标冲突，性能仍落后于强力的固定K基线。

Method: 提出QASA：a) 将槽选择与重建目标解耦；b) 设计无监督的Slot-Quality度量，用于衡量每个槽的绑定质量；c) 基于质量度量进行质量引导的槽选择，仅选高质量槽进入训练时的新型门控解码器参与重建；d) 推理阶段通过token级竞争机制实现K自适应输出。

Result: 在真实与合成数据集上显著优于现有K自适应方法；在真实数据集上超过固定K方法。

Conclusion: 通过质量度量与解耦的训练架构，QASA实现稳定且高保真的K自适应对象绑定与重建，缓解目标冲突与槽质量不稳问题，推动无监督对象中心学习在更动态场景中的表现。

Abstract: Slot Attention, an approach that binds different objects in a scene to a set of "slots", has become a leading method in unsupervised object-centric learning. Most methods assume a fixed slot count K, and to better accommodate the dynamic nature of object cardinality, a few works have explored K-adaptive variants. However, existing K-adaptive methods still suffer from two limitations. First, they do not explicitly constrain slot-binding quality, so low-quality slots lead to ambiguous feature attribution. Second, adding a slot-count penalty to the reconstruction objective creates conflicting optimization goals between reducing the number of active slots and maintaining reconstruction fidelity. As a result, they still lag significantly behind strong K-fixed baselines. To address these challenges, we propose Quality-Guided K-Adaptive Slot Attention (QASA). First, we decouple slot selection from reconstruction, eliminating the mutual constraints between the two objectives. Then, we propose an unsupervised Slot-Quality metric to assess per-slot quality, providing a principled signal for fine-grained slot--object binding. Based on this metric, we design a Quality-Guided Slot Selection scheme that dynamically selects a subset of high-quality slots and feeds them into our newly designed gated decoder for reconstruction during training. At inference, token-wise competition on slot attention yields a K-adaptive outcome. Experiments show that QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. Moreover, on real-world datasets QASA surpasses K-fixed methods.

</details>


### [157] [GazeD: Context-Aware Diffusion for Accurate 3D Gaze Estimation](https://arxiv.org/abs/2601.12948)
*Riccardo Catalini,Davide Di Nucci,Guido Borghi,Davide Davoli,Lorenzo Garattoni,Giampiero Francesca,Yuki Kawana,Roberto Vezzani*

Main category: cs.CV

TL;DR: GazeD是一种从单张RGB图像同时估计3D注视与人体姿态的扩散模型方法，利用2D上下文生成多种合理的3D假设，并在多数据集上达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D注视估计在单帧、存在不确定性与模糊性的场景下表现受限，且通常与人体姿态分离建模，难以充分利用二者的相关性；需要一种能处理不确定性并联合建模姿态与注视的单图像方法。

Method: 以扩散模型为核心，去噪过程条件化于：2D人体姿态、主体周边区域与场景上下文；提出将3D注视点视作固定距离于双眼的“额外身体关节点”，与人体3D姿态关节一起在扩散过程中联合去噪，生成多组可行的3D注视与姿态假设。

Result: 在三个基准数据集上取得3D注视估计的SOTA，甚至优于依赖时序信息的方法；能输出多样且合理的3D注视/姿态假设。

Conclusion: 联合建模姿态与注视并利用扩散模型处理不确定性，可在单帧RGB条件下显著提升3D注视估计效果；所提出的“注视关节”表示与条件化去噪策略有效。

Abstract: We introduce GazeD, a new 3D gaze estimation method that jointly provides 3D gaze and human pose from a single RGB image. Leveraging the ability of diffusion models to deal with uncertainty, it generates multiple plausible 3D gaze and pose hypotheses based on the 2D context information extracted from the input image. Specifically, we condition the denoising process on the 2D pose, the surroundings of the subject, and the context of the scene. With GazeD we also introduce a novel way of representing the 3D gaze by positioning it as an additional body joint at a fixed distance from the eyes. The rationale is that the gaze is usually closely related to the pose, and thus it can benefit from being jointly denoised during the diffusion process. Evaluations across three benchmark datasets demonstrate that GazeD achieves state-of-the-art performance in 3D gaze estimation, even surpassing methods that rely on temporal information. Project details will be available at https://aimagelab.ing.unimore.it/go/gazed.

</details>


### [158] [StyMam: A Mamba-Based Generator for Artistic Style Transfer](https://arxiv.org/abs/2601.12954)
*Zhou Hong,Rongsheng Hu,Yicheng Di,Xiaolong Xu,Ning Dong,Yihua Shao,Run Ling,Yun Wang,Juqin Wang,Zhanjie Zhang,Ao Ma*

Main category: cs.CV

TL;DR: 提出StyMam：一种基于Mamba的生成器用于风格迁移，在保证内容结构的同时生成高质量、无伪影的风格化图像，并兼顾速度优势，优于现有GAN/SD方法。


<details>
  <summary>Details</summary>
Motivation: 现有GAN方法（CNN/Transformer）难以同时建模局部与全局依赖，导致伪影与不和谐纹理；SD方法虽缓解此问题，但内容保持较差且推理慢。需要一种兼顾质量、结构保持与速度的风格迁移框架。

Method: 重访GAN范式，设计基于Mamba的生成器StyMam，包含：1）残差双路径条带扫描机制，高效提取局部纹理；2）通道重加权的空间注意力模块，建模全局依赖。二者协同实现稳健的局部-全局特征融合。

Result: 通过大量定性与定量实验，StyMam在图像质量与推理速度上均优于最新方法，生成图像无明显伪影与不和谐模式，内容结构保持更好。

Conclusion: Mamba驱动的生成器结合条带扫描与通道重加权空间注意力，有效解决GAN/SD在风格迁移中的痛点，实现高质量、快速、结构友好的风格化。

Abstract: Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.

</details>


### [159] [Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation](https://arxiv.org/abs/2601.12964)
*John Waithaka,Gustave Bwirayesu,Moise Busogi*

Main category: cs.CV

TL;DR: 提出一种将高分辨率（HR）数据融入自监督预训练以提升中分辨率（MR）遥感表征与分割性能的方法：在现有自监督框架中加入“空间亲和”组件，实验证明优于仅用HR或仅用MR预训练。


<details>
  <summary>Details</summary>
Motivation: MR数据量大但细节有限；HR数据近期可用但直接用来预训练未必能提升MR任务。需要一种机制把HR的空间细节有效地迁移到MR表征中，从而改进MR下游分割。

Method: 在通用自监督学习框架中引入“空间亲和（spatial affinity）”组件，利用HR影像与MR影像之间的空间对应关系/亲和约束，指导MR表示学习。该组件可插件式加入不同SSL框架，并在预训练阶段同时使用HR与MR数据，学习更具空间一致性的MR特征。

Result: 在两种主流自监督框架上验证，带空间亲和组件的模型在MR下游分割任务上优于仅用HR或仅用MR预训练的基线。

Conclusion: 融合HR信息的空间亲和自监督预训练能显著提升MR图像表征与分割效果，且方法通用、可移植，优于单一分辨率的预训练策略。

Abstract: Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.

</details>


### [160] [Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers](https://arxiv.org/abs/2601.12981)
*Sulaiman Khan,Md. Rafiul Biswas,Zubair Shah*

Main category: cs.CV

TL;DR: 提出基于表格Transformer（TabTrans）的早期2型糖尿病（T2DM）风险预测方法，融合纵向EHR与DXA骨密度相关数据，在不平衡数据上经SMOTE/SMOTE-ENN处理，较传统ML与多家大模型取得更高AUC（≥79.7%），并解释出VAT与多项骨密度指标为关键特征。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以从纵向、异质的表格医疗数据中捕捉长期依赖与复杂交互，且T2DM早筛需要更准确、可解释的个体化风险预测工具；在卡塔尔人群背景下，DXA骨指标与代谢风险的关系亦值得验证。

Method: 构建TabTrans以处理纵向患者记录与DXA表格特征；对类别不平衡使用SMOTE与SMOTE-ENN重采样；在Qatar BioBank回顾性队列（n=1,382）上训练评估，并与传统ML及三种生成式大模型（Claude 3.5 Sonnet、GPT-4、Gemini Pro）比较；进行特征重要性解释分析。

Result: TabTrans在T2DM预测上取得ROC AUC≥79.7%，优于传统ML与所比较的生成式模型；关键预测因子包括内脏脂肪组织（VAT）质量/体积、Ward部位BMD/BMC、T/Z分数及L1-L4分数。

Conclusion: TabTrans能有效建模复杂纵向表格医疗数据，在卡塔尔成人的T2DM早期风险预测上表现优异并具可解释性，支持主动管理与个性化干预；DXA相关骨指标与VAT对糖尿病风险具有重要关联性。

Abstract: This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.
  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data

</details>


### [161] [AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection](https://arxiv.org/abs/2601.12994)
*Shiming Wang,Holger Caesar,Liangliang Nan,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 提出AsyncBEV模块，通过估计并应用BEV特征流来对齐不同模态在时间不同步下的特征，从而提升多模态3D检测在异步场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态自动驾驶感知通常假设训练与推理阶段跨传感器严格同步，但现实中频率不一致、网络延迟、硬件/处理瓶颈等会带来时间偏移，显著影响对动态目标的检测精度，需要一种对异步鲁棒的方法。

Method: 提出可训练、轻量、通用的AsyncBEV模块：给定两种模态（如LiDAR与Camera）的BEV特征及其已知时间偏移，学习估计2D特征流（受场景流思想启发）；随后用该特征流对一个模态的BEV特征进行扭曲（warp）以与另一模态在时空上对齐。该模块可无缝集成到多种BEV检测架构（基于网格的与基于token的，如UniBEV与CMT）。

Result: 在CMT（token-based）与UniBEV（grid-based）上做大量实验，面对小与大的跨模态异步（LiDAR/相机）均显著提升鲁棒性，尤其对动态目标；在最差0.5秒偏移下，相比仅做自车运动补偿的基线，动态目标NDS分别提升16.6%（CMT）与11.9%（UniBEV）。

Conclusion: AsyncBEV能有效缓解跨传感器时间不同步导致的性能退化，易集成且开销小，对动态目标检测尤为显著；代码将在论文接收后开源。

Abstract: In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors. Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds' Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation, AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements. The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based). Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5 s$ time offset. Code will be released upon acceptance.

</details>


### [162] [Think3D: Thinking with Space for Spatial Reasoning](https://arxiv.org/abs/2601.13029)
*Zaibin Zhang,Yuhan Wu,Lianjie Jia,Yifan Wang,Zhongbo Zhang,Yijiang Li,Binghao Ran,Fuxi Zhang,Zhuohan Sun,Zhenfei Yin,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: Think3D将3D重建工具接入VLM代理，使其通过相机位姿与视角切换在重建的点云空间中“互动思考”，显著提升多任务三维空间推理表现，且无需额外训练；小模型借助RL学会更有效的视角选择后收益更大。


<details>
  <summary>Details</summary>
Motivation: 现有VLM主要停留在2D感知，难以进行真正的三维几何、视角与空间关系推理；需要一种无需再训练、可泛化的方式，让代理具备在3D空间中探索与推理的能力。

Method: 提出Think3D框架：调用3D重建模型从图像/视频恢复点云与相机位姿；在此基础上提供相机操作（移动、旋转、缩放）与自我视角/全局视角切换等工具，将空间推理转化为交互式“3D链式思考”。对大模型直接零训练使用；对小模型进一步引入RL策略，学习选择信息量大的视点与操作。

Result: 在BLINK Multi-view与MindCube上平均+7.8%，在VSI-Bench上+4.7%；小模型原始使用工具仅+0.7%，加入RL后工具带来的提升达+6.8%。

Conclusion: 训练免、工具增强的3D空间探索与推理可以显著提升多模态代理的空间智能；通过3D重建与交互式视角操作，将2D VLM扩展为具备更人类式三维推理能力的代理；代码与权重已开源。

Abstract: Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.

</details>


### [163] [GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure](https://arxiv.org/abs/2601.13052)
*Antoine Carreaud,Shanci Li,Malo De Lacour,Digre Frinde,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: 提出GridNet-HD，多模态3D语义分割数据集，联合高密度LiDAR与高分辨率斜视影像，含2.5B点与7694幅图、11类标注，提供基线与划分；多模态融合较最佳单模态提升+5.55 mIoU，数据与代码已开源。


<details>
  <summary>Details</summary>
Motivation: 电力线路等架空电力设施的三维语义分割需要几何与外观信息，但现有公开数据集缺乏同时具备高密度LiDAR、高分辨率斜视影像与3D语义标注的多模态数据，限制了算法研究与公平对比。

Method: 构建GridNet-HD数据集：采集并配准高密度LiDAR点云与高分辨率斜视影像，对电力资产进行11类别3D语义标注；给出预定义训练/验证/测试划分和mIoU评测协议；实现并发布单模态（仅点云/仅图像）与多模态融合的基线模型，用于基准测试。

Result: 在该数据集上，多模态融合模型相较最佳单模态基线提升+5.55 mIoU，显示几何与外观的互补性；数据规模为约2.5亿×10的点（2.5B）与7694幅图，均带有11类标注，能稳定评估方法性能。

Conclusion: GridNet-HD填补了电力设施场景中高密度LiDAR与高分辨率斜视影像联合标注数据的空白，证明多模态融合在该任务上显著优于单模态；数据集与基线及代码已开源，可作为后续研究与比较的标准基准。

Abstract: This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.

</details>


### [164] [Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures](https://arxiv.org/abs/2601.13059)
*Yulun Guo*

Main category: cs.CV

TL;DR: 提出一种结合Retinex与小样本度量学习的双分支网络，实现低光裂缝分割，在多个基准上达SOTA，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 低光环境（隧道、桥底）使裂缝图像对比度低、噪声大，常规分割精度下降；像素级标注成本高，现有方法依赖大量高质量数据，难以适应低光少样本场景。

Method: 设计双分支原型学习框架：利用Retinex分解得到反射分量，作为照度不变的全局表示引导；结合度量/原型学习减少大规模标注需求。提出跨相似性先验掩码模块，计算query与support的高维相似以定位裂缝结构；提出多尺度特征增强模块，将多尺度特征与先验掩码融合，缓解空间不一致，提升边缘与细节。

Result: 在多套低光裂缝基准上进行大量实验，结果在一致性与精度上优于现有方法，达到SOTA。

Conclusion: 将Retinex引导的照度不变表示与小样本度量学习结合，能在低光且标注稀缺的裂缝分割中显著提升性能；提出的先验掩码与多尺度增强是关键组件，方法具备通用性与实用价值。

Abstract: Crack detection is critical for concrete infrastructure safety, but real-world cracks often appear in low-light environments like tunnels and bridge undersides, degrading computer vision segmentation accuracy. Pixel-level annotation of low-light crack images is extremely time-consuming, yet most deep learning methods require large, well-illuminated datasets. We propose a dual-branch prototype learning network integrating Retinex theory with few-shot learning for low-light crack segmentation. Retinex-based reflectance components guide illumination-invariant global representation learning, while metric learning reduces dependence on large annotated datasets. We introduce a cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and a multi-scale feature enhancement module that fuses multi-scale features with the prior mask to alleviate spatial inconsistency. Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions. Code: https://github.com/YulunGuo/CrackFSS.

</details>


### [165] [Patient-Conditioned Adaptive Offsets for Reliable Diagnosis across Subgroups](https://arxiv.org/abs/2601.13094)
*Gelei Xu,Yuying Duan,Jun Xia,Ruining Deng,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: 提出HyperAdapt：一种以患者属性为条件的超网络式轻量适配框架，在保持共享诊断骨干的同时提升不同亚群的可靠性与公平性。通过编码年龄/性别等临床属性，生成对部分层的小残差调制参数，低秩与瓶颈约束保证效率与稳健；在多数据集上提高亚群表现且不损害总体准确，在PAD-UFES-20上召回+4.1%、F1+4.4%，对稀缺人群提升更显著。


<details>
  <summary>Details</summary>
Motivation: 医疗AI在不同患者亚群上表现不均，常见“去敏”公平方法会丢失关键临床信息，影响高风险场景的诊断可靠性；临床实践则明确利用患者背景进行判读，启发设计一种既共享知识又能面向亚群自适应的模型。

Method: 将临床相关属性（如年龄、性别）编码为紧凑嵌入，用作条件信号驱动超网络模块，为共享骨干中选定层生成小规模的残差调制参数；通过低秩与瓶颈化参数化限制适配自由度与计算开销，既保存骨干的通用医学知识又实现面向患者的轻量适配。

Result: 在多个公开医学影像基准上，方法在不降低总体准确的前提下稳定提升亚群级指标；在PAD-UFES-20数据集上，相比最强基线召回提高4.1%、F1提高4.4%，对代表性不足人群的提升更大。

Conclusion: 以患者条件的超网络残差适配能在保持共享模型的同时改善亚群可靠性与公平性，兼顾性能与效率，适合实际医疗场景中存在人群异质性的诊断任务。

Abstract: AI models for medical diagnosis often exhibit uneven performance across patient populations due to heterogeneity in disease prevalence, imaging appearance, and clinical risk profiles. Existing algorithmic fairness approaches typically seek to reduce such disparities by suppressing sensitive attributes. However, in medical settings these attributes often carry essential diagnostic information, and removing them can degrade accuracy and reliability, particularly in high-stakes applications. In contrast, clinical decision making explicitly incorporates patient context when interpreting diagnostic evidence, suggesting a different design direction for subgroup-aware models. In this paper, we introduce HyperAdapt, a patient-conditioned adaptation framework that improves subgroup reliability while maintaining a shared diagnostic model. Clinically relevant attributes such as age and sex are encoded into a compact embedding and used to condition a hypernetwork-style module, which generates small residual modulation parameters for selected layers of a shared backbone. This design preserves the general medical knowledge learned by the backbone while enabling targeted adjustments that reflect patient-specific variability. To ensure efficiency and robustness, adaptations are constrained through low-rank and bottlenecked parameterizations, limiting both model complexity and computational overhead. Experiments across multiple public medical imaging benchmarks demonstrate that the proposed approach consistently improves subgroup-level performance without sacrificing overall accuracy. On the PAD-UFES-20 dataset, our method outperforms the strongest competing baseline by 4.1% in recall and 4.4% in F1 score, with larger gains observed for underrepresented patient populations.

</details>


### [166] [A Streamlined Attention-Based Network for Descriptor Extraction](https://arxiv.org/abs/2601.13126)
*Mattia D'Urso,Emanuele Santellani,Christian Sormann,Mattia Rossi,Andreas Kuhn,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: SANDesc 是一种轻量级注意力增强的描述子网络，在不改动检测器的前提下显著提升关键点匹配效果，并在多数据集与新建4K城市场景基准上验证了性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有关键点描述子在匹配准确率与计算效率之间存在权衡，并且常与检测器强耦合；需要一种在不更换检测器的情况下提升匹配质量、且参数量小、易部署的描述子模型。

Method: 提出经改造的 U-Net 框架，融合卷积块注意力模块（CBAM）与残差路径，形成“带注意力的残差 U-Net 模块”；训练采用改进的三元组损失并结合受课程学习启发的难负样本挖掘策略，以提升训练稳定性与判别力。

Result: 在 HPatches、MegaDepth-1500、IMC 2021 等上，基于现有检测器训练的 SANDesc 相比原始描述子在多种匹配任务上取得更好表现；模型仅约 2.4M 参数。在作者新构建的含4K图像与已标定内参的城市数据集上，在有限算力下亦获得显著增益。

Conclusion: SANDesc 以小模型实现更优匹配描述，能即插即用地替换现有描述子，兼具效果与效率；新数据集为评测高分辨率城市场景特征提供了有价值基准。

Abstract: We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.
  Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector. We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency. We refer to the building blocks of our model as Residual U-Net Blocks with Attention. The model is trained using a modified triplet loss in combination with a curriculum learning-inspired hard negative mining strategy, which improves training stability.
  Extensive experiments on HPatches, MegaDepth-1500, and the Image Matching Challenge 2021 show that training SANDesc on top of existing keypoint detectors leads to improved results on multiple matching tasks compared to the original keypoint descriptors. At the same time, SANDesc has a model complexity of just 2.4 million parameters.
  As a further contribution, we introduce a new urban dataset featuring 4K images and pre-calibrated intrinsics, designed to evaluate feature extractors. On this benchmark, SANDesc achieves substantial performance gains over the existing descriptors while operating with limited computational resources.

</details>


### [167] [PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain](https://arxiv.org/abs/2601.13128)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: PhaseMark是一种在VAE潜空间频域直接相位调制的单次水印方法，免优化、极快且在再生成等强攻击下保持SOTA鲁棒性，并兼顾画质；同时分析了四种调制变体的性能-质量权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的高逼真图像大量涌现，亟需鲁棒、快速的水印方案；现有事后水印多依赖迭代优化/反演，速度慢、成本高，限制实际部署。

Method: 提出PhaseMark：在LDM的VAE潜变量经频域变换后直接调制相位嵌入水印；为单次前向、无优化流程。设计并比较四种相位调制变体，考察鲁棒性与画质间的权衡，并评估其在再生成等强攻击场景下的表现。

Result: 相比优化型方法，速度提升达数千倍；在各类强攻击（含再生成）下达到或超越SOTA的识别鲁棒性，同时不降低图像质量。不同调制变体呈现清晰的性能-质量权衡曲线。

Conclusion: 利用VAE潜空间的内在相位属性，可实现高效且鲁棒的图像水印。PhaseMark为生成式模型水印提供了新的范式：单次、免优化、速度极快且具SOTA稳健性。

Abstract: The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.

</details>


### [168] [GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning](https://arxiv.org/abs/2601.13132)
*Kim Yu-Ji,Dahye Lee,Kim Jun-Seong,GeonU Kim,Nam Hyeon-Woo,Yongjin Kwon,Yu-Chiang Frank Wang,Jaesung Choe,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: GaussExplorer将3D高斯泼洒(3DGS)与视觉-语言模型(VLM)结合，用于在3D场景中基于问题的主动探索与推理；通过从已捕获图像中挑选与问题最相关的视角，并合成新视角，提升VLM推理效果，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言嵌入的3DGS方法仅能处理简单文本查询，难以理解组合性强的复杂语言；而基于对象中心的RGB-D记忆虽有空间对齐，但受限于预设视角，缺乏灵活探索能力。需要一种既能空间落地、又能面向复杂语言进行主动视角选择和推理的方案。

Method: 在3DGS表示的场景之上引入VLM进行问驱动探索：1) 从预采集图像中检索与问题最相关的帧；2) 基于3DGS从这些关键帧生成或调整到更优的新视角以捕获判别性视觉信息；3) 将新视角图像送入VLM进行多模态推理，实现语言到空间的对齐与决策。

Result: 在多个基准上超越现有方法，证明利用VLM推理与3DGS渲染生成新视角的结合可显著提升具身任务中的理解与问答表现。

Conclusion: 将VLM与3DGS紧密结合，通过问题相关视角选择与新视角合成，能够更好地解释复杂语言并提升具身探索与推理能力，克服了仅文本嵌入或固定视角记忆的局限。

Abstract: We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.

</details>


### [169] [CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks](https://arxiv.org/abs/2601.13133)
*Mingshuang Luo,Ruibing Hou,Bo Chao,Hong Chang,Zimo Liu,Yaowei Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 提出CLASP：利用CLIP生成多层次语义伪标签，并结合可由提示控制的MoE与多任务预训练，提升人类中心视觉任务的通用无监督预训练效果。


<details>
  <summary>Details</summary>
Motivation: 大规模无标注的人体图像数据增多，但下游任务多样（姿态、部件、属性、ReID等），缺乏一个能泛化到不同粒度语义需求的通用无监督预训练框架。

Method: 1) 以CLIP产生低层（人体部件）与高层（属性）伪标签；2) 通过提示可控的混合专家（MoE）模块，根据任务提示选择/融合合适的特征路径，缓解不同语义粒度间的冲突；3) 采用多任务预训练，用部件级与属性级伪标签联合监督表示学习，从而在预训练阶段对多层次语义进行对齐。

Result: 在多个人类中心视觉基准上，较现有无监督预训练方法取得一致领先（抽象未给具体数值），展示更强的迁移与泛化能力。

Conclusion: CLASP将CLIP的语义对齐能力引入无监督预训练，并通过提示可控MoE实现任务自适应特征提取，以多任务方式学习多层次语义，整体推进人类中心视觉分析性能。

Abstract: Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.

</details>


### [170] [TVWorld: Foundations for Remote-Control TV Agents](https://arxiv.org/abs/2601.13142)
*Zhantao Ma,Quanfeng Lu,Shuai Zhong,Dahai Yu,Ping Luo,Michael K. Ng*

Main category: cs.CV

TL;DR: 提出TVWorld框架与基准，发现LVLM在TV遥控式导航中的拓扑感知不足，并通过拓扑感知训练打造TVTheseus，在TVWorld-N上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM主要研究点按式交互，对日常电视使用中的遥控式交互研究不足；缺乏可复现实验环境与全面评测来检验长程导航与焦点落点能力。

Method: 1) 构建TVWorld：将真实电视导航抽象为离线图结构，可复现、无需部署；2) 设计两个互补基准：TVWorld-N（拓扑感知导航）与TVWorld-G（焦点感知定位/grounding）；3) 分析现有智能体的不足（拓扑意识不足）；4) 提出Topology-Aware Training，将拓扑信息注入LVLM；5) 基于该训练得到专门模型TVTheseus。

Result: TVTheseus在TVWorld-N上成功率68.3%，超过包括Gemini 3 Flash等强闭源基线，达SOTA；并通过额外分析提供开发高效TV使用智能体的见解。

Conclusion: 遥控式TV导航需要显式的拓扑意识；TVWorld提供标准化评测与研究平台；拓扑感知训练能显著提升LVLM在长程、焦点驱动的TV导航中的表现，TVTheseus验证了有效性。

Abstract: Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.

</details>


### [171] [ICo3D: An Interactive Conversational 3D Virtual Human](https://arxiv.org/abs/2601.13148)
*Richard Shaw,Youngkyoon Jang,Athanasios Papaioannou,Arthur Moreau,Helisa Dhamo,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ICo3D提出一种可交互、可对话、照片级逼真的3D虚拟人方法：以多视角数据重建可动画的头部与动态身体（基于高斯点渲染），融合后驱动实时人机对话；以语音驱动面部以实现精确对口型，同步接入LLM完成对话与多模态交互，并展示实时系统与应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟人往往难以同时满足：高写实度、可交互的实时性能、稳定的口型/表情同步与完整身体+面部的无缝融合。作者希望提供一个端到端系统，既有照片级外观、可靠动画控制，又支持对话交互与沉浸式应用。

Method: 1) 多视角采集目标人物；2) 分别构建两套改进的动态高斯模型：SWinGS++用于身体重建与动画、HeadGaS++用于面部重建与精细表情；3) 通过高斯splatting渲染实现高质量实时渲染；4) 设计无缝融合策略，将头部与身体模型合并避免边界伪影；5) 接入LLM与TTS/ASR管线，使用生成的语音作为驱动信号实现面部动画与口型精确对齐；6) 集成对话系统，支持口语与文本交互并在沉浸环境中实时运行。

Result: 得到可实时驱动的写实3D虚拟人，具有稳定的口型同步、细致面部表情与动态身体表现；系统演示显示可进行自然对话、即时渲染和多场景应用（游戏、助理、教育等）。

Conclusion: ICo3D实现了从采集、重建、渲染到对话交互的一体化虚拟人方案，关键在于改进的动态高斯模型（SWinGS++、HeadGaS++）与无缝融合策略，加上LLM驱动的对话与语音驱动的面部动画，同步性与写实度兼具，适用于多种实时交互场景。

Abstract: This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/

</details>


### [172] [From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models](https://arxiv.org/abs/2601.13166)
*Pedro M. Gordaliza,Jaume Banus,Benoît Gérin,Maxence Wynen,Nataliia Molchanova,Jonas Richiardi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 团队在MICCAI 2025的两个3D脑MRI基础模型挑战（SSL3D与FOMO25）均获第一，方案基于U-Net并融入解剖先验与神经影像知识，训练更快、模型更小，优于Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 医疗影像尤其是3D脑MRI的任务具有数据标注稀缺、计算成本高、任务多样等挑战，亟需能跨任务泛化且高效可训练的基础模型；现有Transformer系方法虽强但资源昂贵，促使寻找更轻量且融入领域知识的替代方案。

Method: 采用U-Net卷积架构为核心，结合解剖结构先验与神经影像领域知识的训练与建模策略（如对脑区结构约束、可能的预训练/自监督及数据归一化与增强策略），以满足SSL3D与FOMO25两赛道需求。

Result: 在MICCAI 2025的SSL3D和FOMO25两个赛道均获第一；相较Transformer方案，训练速度快1-2个数量级、参数规模约为其1/10。模型与代码已开放在GitHub。

Conclusion: 在3D脑MRI基础模型构建上，结合U-Net与解剖/领域先验可在精度、效率与资源占用上同时优于Transformer基线，显示经典CNN在配合领域知识后仍具强竞争力，且具有实际落地价值。

Abstract: Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.

</details>


### [173] [GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction](https://arxiv.org/abs/2601.13207)
*Jinnao Li,Zijian Chen,Tingzhu Chen,Changbo Wang*

Main category: cs.CV

TL;DR: 提出GTPred基准，面向图像的地理+时间联合预测；评估MLLM在“何地+何时”和推理链上的表现，发现时间线索能显著提升定位而现有模型在世界知识与时空推理上仍不足。


<details>
  <summary>Details</summary>
Motivation: 现有地理定位研究与基准多忽略图像所包含的时间信息（时代、年份等），而时间线索可强约束地理位置；随着多模态大模型用于地理定位，迫切需要一个同时覆盖空间与时间、并能评估推理过程的新基准。

Method: 构建GTPred：370张全球分布、跨度120+年的图像与精细标注的层级地理标签（国家/地区→城市→更细粒度）与年份，以及逐步推理链的黄金标注。设计联合评估：同时匹配年份与层级地点序列，并对模型生成的中间推理链与黄金推理对齐评分。对8个闭源与7个开源MLLM进行系统评测。

Result: 尽管模型具备较强视觉感知能力，但在世界知识覆盖与地—时联合推理方面表现欠佳；将时间信息纳入推理显著提升地理定位精度。

Conclusion: GTPred弥补了地时联合预测基准空缺，提供了同时衡量答案与推理过程的评测框架；实验强调时间维度对地理定位的重要性，并揭示当前MLLM在地理世界知识与时序推理方面的不足，指向未来改进方向。

Abstract: Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.

</details>


### [174] [Rethinking Skip Connections: Additive U-Net for Robust and Interpretable Denoising](https://arxiv.org/abs/2601.13208)
*Vikram R Lakkavalli*

Main category: cs.CV

TL;DR: 提出“加性U-Net”，用带门控的加性跳连替代传统拼接跳连，避免通道膨胀并可解释地控制编码器信息流，在Kodak-17去噪上以更轻量结构取得与标准方法竞争的PSNR/SSIM。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net的拼接式跳连会使通道数翻倍、信息流不透明，可能将噪声不受控地传递到解码端，缺乏对各尺度贡献的可解释与可控性；需要一种既轻量又能显式调控多尺度信息传输的跳连机制。

Method: 将跳连由拼接改为“加性+门控”：每条跳连乘以可学习的非负标量后与解码特征相加，实现对编码器贡献的显式缩放，避免通道膨胀；同时在不同深度/无强制上下采样的设置下评估网络学习到的频带进程。

Result: 在Kodak-17基准、噪声σ=15/25/50下取得与拼接式U-Net相当的PSNR/SSIM，且对不同卷积核调度与网络深度具有稳健性；即使不采用显式下/上采样或固定层级，模型也能自发形成从高频到带通再到低频的逐级表示。

Conclusion: 加性跳连是拼接的轻量、可解释替代方案，可在不牺牲性能的情况下减少通道与计算开销，并提供清晰的多尺度信息传递理解，有助于更高效的重建网络设计。

Abstract: Skip connections are central to U-Net architectures for image denoising, but standard concatenation doubles channel dimensionality and obscures information flow, allowing uncontrolled noise transfer. We propose the Additive U-Net, which replaces concatenative skips with gated additive connections. Each skip pathway is scaled by a learnable non-negative scalar, offering explicit and interpretable control over encoder contributions while avoiding channel inflation. Evaluations on the Kodak-17 denoising benchmark show that Additive U-Net achieves competitive PSNR/SSIM at noise levels σ = 15, 25, 50, with robustness across kernel schedules and depths. Notably, effective denoising is achieved even without explicit down/up-sampling or forced hierarchies, as the model naturally learns a progression from high-frequency to band-pass to low-frequency features. These results position additive skips as a lightweight and interpretable alternative to concatenation, enabling both efficient design and a clearer understanding of multi-scale information transfer in reconstruction networks.

</details>


### [175] [ObjectVisA-120: Object-based Visual Attention Prediction in Interactive Street-crossing Environments](https://arxiv.org/abs/2601.13218)
*Igor Vozniak,Philipp Mueller,Nils Lipp,Janis Sprenger,Konstantin Poddubnyy,Davit Hovhannisyan,Christian Mueller,Andreas Bulling,Philipp Slusallek*

Main category: cs.CV

TL;DR: 提出用于对象级注意力评估的VR街口过街凝视数据集、一个对象相似度评价指标oSIM，以及基于Mamba U-Net与图表示的SUMGraph模型，均显著提升对象级与常见注意力指标表现。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中的注意力预测多以像素/显著性为主，缺乏能反映“以对象为单位”的人类注意力评估与数据；现实场景采集对象级凝视数据存在伦理与安全难题。

Method: 1) 构建120名受试者在VR街口过街任务中的数据集：精确视线、完整对象状态空间、可变场景复杂度、全景分割、深度与车辆关键点等标注；2) 提出面向对象的相似度指标oSIM评估对象级注意力；3) 设计SUMGraph：在Mamba U-Net框架中显式以图表示关键场景对象（车辆）并进行注意力预测；4) 在多指标上与多种SOTA方法比较。

Result: 显式优化对象级注意力可显著提升oSIM，同时在常见显著性指标上也有增益；SUMGraph在多个基准与复杂度设置下优于多种SOTA视觉注意力预测模型。

Conclusion: 对象级注意力应成为注意力模型评估与建模的一等公民；VR数据集与oSIM为标准化评测奠定基础，SUMGraph展示了将对象图融入架构的效益。数据集、代码与模型将开源。

Abstract: The object-based nature of human visual attention is well-known in cognitive science, but has only played a minor role in computational visual attention models so far. This is mainly due to a lack of suitable datasets and evaluation metrics for object-based attention. To address these limitations, we present \dataset~ -- a novel 120-participant dataset of spatial street-crossing navigation in virtual reality specifically geared to object-based attention evaluations. The uniqueness of the presented dataset lies in the ethical and safety affiliated challenges that make collecting comparable data in real-world environments highly difficult. \dataset~ not only features accurate gaze data and a complete state-space representation of objects in the virtual environment, but it also offers variable scenario complexities and rich annotations, including panoptic segmentation, depth information, and vehicle keypoints. We further propose object-based similarity (oSIM) as a novel metric to evaluate the performance of object-based visual attention models, a previously unexplored performance characteristic. Our evaluations show that explicitly optimising for object-based attention not only improves oSIM performance but also leads to an improved model performance on common metrics. In addition, we present SUMGraph, a Mamba U-Net-based model, which explicitly encodes critical scene objects (vehicles) in a graph representation, leading to further performance improvements over several state-of-the-art visual attention prediction methods. The dataset, code and models will be publicly released.

</details>


### [176] [Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations](https://arxiv.org/abs/2601.13225)
*Tim Lachmann,Alexandra Israelsson,Christina Tornberg,Teimuraz Saghinadze,Michal Balazia,Philipp Müller,Petri Laukka*

Main category: cs.CV

TL;DR: 提出BLEMORE多模态（视频、音频）“情绪混合”数据集，含相对显著性标注，并基准评测两类任务（成分情绪存在与显著性预测）；多模态方法显著优于单模态，但整体准确率仍较低，揭示任务挑战性与数据价值。


<details>
  <summary>Details</summary>
Motivation: 现实中人类常呈现“情绪混合”而非单一基本情绪，但现有视频情绪识别多聚焦单情绪，少量混合情绪方法也难以判断各成分的相对显著性；主要瓶颈是缺乏带显著性标注的大规模数据集。

Method: 构建BLEMORE数据集：58位演员、3000+片段，包含6种基本情绪与10种混合情绪，每种混合情绪提供3种显著性配置（50/50、70/30、30/70），提供视频与音频模态。基于该数据进行两类任务评测：1) 混合中情绪成分的存在预测；2) 成分间相对显著性预测。比较多种SOTA视频/多模态分类器（如ImageBind+WavLM、VideoMAEv2+HuBERT、HiCMAE）。

Result: 在验证集：单模态最高达成分存在准确率29%、显著性准确率13%；多模态提升明显，ImageBind+WavLM达成分存在35%，HiCMAE达显著性18%。在独立测试集：最佳模型成分存在33%（VideoMAEv2+HuBERT），显著性18%（HiCMAE）。

Conclusion: BLEMORE为研究“混合情绪”及其显著性提供关键资源；多模态优于单模态但绝对性能仍低，表明任务困难且有较大改进空间，数据集可推动更能反映真实复杂情绪表达的识别系统发展。

Abstract: Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.

</details>


### [177] [ConvMambaNet: A Hybrid CNN-Mamba State Space Architecture for Accurate and Real-Time EEG Seizure Detection](https://arxiv.org/abs/2601.13234)
*Md. Nishan Khan,Kazi Shahriar Sanjid,Md. Tanzim Hossain,Asib Mostakim Fony,Istiak Ahmed,M. Monir Uddin*

Main category: cs.CV

TL;DR: 提出ConvMambaNet：将CNN与Mamba结构化状态空间模型结合，用于癫痫EEG发作检测；在CHB-MIT上达99%准确率，并能应对类别不平衡，具备实时临床监测潜力。


<details>
  <summary>Details</summary>
Motivation: EEG自动化发作检测因时间序列长程依赖与复杂动态而困难，传统CNN或RNN单独使用难以同时捕捉空间与长时序特征，因此需要新模型提升时序特征提取与鲁棒性。

Method: 在CNN框架中嵌入Mamba-SSM模块，形成混合深度网络ConvMambaNet：CNN负责局部空间特征提取，Mamba-SSM捕捉长程时序依赖；在CHB-MIT头皮EEG数据集上训练与评估，并在严重类别不平衡情形下测试其鲁棒性。

Result: 在CHB-MIT数据集上获得约99%准确率，且在类别极度不平衡条件下仍保持稳健性能。

Conclusion: ConvMambaNet能有效联合空间与长程时间特征，实现高精度、鲁棒的癫痫发作检测，具有向临床实时自动监测落地的可行性。

Abstract: Epilepsy is a chronic neurological disorder marked by recurrent seizures that can severely impact quality of life. Electroencephalography (EEG) remains the primary tool for monitoring neural activity and detecting seizures, yet automated analysis remains challenging due to the temporal complexity of EEG signals. This study introduces ConvMambaNet, a hybrid deep learning model that integrates Convolutional Neural Networks (CNNs) with the Mamba Structured State Space Model (SSM) to enhance temporal feature extraction. By embedding the Mamba-SSM block within a CNN framework, the model effectively captures both spatial and long-range temporal dynamics. Evaluated on the CHB-MIT Scalp EEG dataset, ConvMambaNet achieved a 99% accuracy and demonstrated robust performance under severe class imbalance. These results underscore the model's potential for precise and efficient seizure detection, offering a viable path toward real-time, automated epilepsy monitoring in clinical environments.

</details>


### [178] [A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models](https://arxiv.org/abs/2601.13238)
*Chengyin Hu,Xiang Chen,Zhe Jia,Weiwen Shi,Fengyu Zhang,Jiujiang Guo,Yiwei Wei*

Main category: cs.CV

TL;DR: 论文提出首个利用真实感雨天扰动来攻击视觉-语言模型（VLM）的对抗框架，展示在多任务上可诱发显著语义错配与安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多在理想视觉条件下训练与评测，对真实天气（如降雨）下的鲁棒性与跨模态对齐稳定性研究不足，亟需系统方法揭示结构化天气扰动对决策边界与语义对齐的影响。

Method: 提出两阶段、参数化且语义解耦的天气对抗框架，工作在非像素参数空间：阶段一以低维全局调制建模降雨的全局效应，对嵌入空间进行条件化以逐步削弱原有语义决策边界；阶段二显式建模多尺度雨滴外观与降雨引起的光照变化，在不可导的天气参数空间中优化，注入结构化雨变以稳定诱导语义偏移。

Result: 在多数据集与多任务上，物理合理且受限的雨天扰动即可造成主流VLM显著语义错配；消融显示光照建模与多尺度雨滴结构是导致语义偏移的关键因素。

Conclusion: 真实感、可解释的天气参数扰动能系统性削弱VLM跨模态对齐与决策稳定性，提示部署中的安全与可靠性风险；应关注天气建模、鲁棒训练与防御机制。

Abstract: Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.

</details>


### [179] [Deep Learning for Semantic Segmentation of 3D Ultrasound Data](https://arxiv.org/abs/2601.13263)
*Chenyu Liu,Marco Cecotti,Harikrishnan Vijayakumar,Patrick Robinson,James Barson,Mihai Caleap*

Main category: cs.CV

TL;DR: 论文提出利用Calyo Pulse固态3D超声传感器进行学习式3D语义分割，基于3D U-Net在体素化超声数据上训练，展示了在恶劣环境下具有鲁棒性的分割效果，并指出可通过更大数据集、改进标注与加权损失继续提升。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶感知主要依赖LiDAR与摄像头，但在成本、鲁棒性与恶劣工况表现存在权衡。需要一种在复杂、恶劣环境中仍可靠、且成本友好的新型传感与感知方案。

Method: 引入Calyo Pulse模块化固态3D超声传感器，采集空间超声体数据；构建并训练3D U-Net进行体素级语义分割；评估在超声数据上的分割性能，并讨论通过数据规模、标注质量与损失设计优化的方向。

Result: 基于Calyo Pulse的3D U-Net取得了稳健的语义分割结果，表明超声体数据在复杂场景下具备有效的可分割特性；尽管未给出具体数值，实验显示有进一步提升空间（更大数据集、精细真值、加权损失）。

Conclusion: 3D超声传感为自动驾驶提供了有前景的互补模态，在恶劣与杂乱环境下可增强可靠性；学习式3D分割框架可行且具备提升潜力，值得与现有LiDAR/相机方案融合。

Abstract: Developing cost-efficient and reliable perception systems remains a central challenge for automated vehicles. LiDAR and camera-based systems dominate, yet they present trade-offs in cost, robustness and performance under adverse conditions. This work introduces a novel framework for learning-based 3D semantic segmentation using Calyo Pulse, a modular, solid-state 3D ultrasound sensor system for use in harsh and cluttered environments. A 3D U-Net architecture is introduced and trained on the spatial ultrasound data for volumetric segmentation. Results demonstrate robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions. Importantly, this study highlights 3D ultrasound sensing as a promising complementary modality for reliable autonomy.

</details>


### [180] [Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams](https://arxiv.org/abs/2601.13299)
*Ethan Seefried,Prahitha Movva,Naga Harshita Marupaka,Tilak Kasturi,Tirthankar Ghosal*

Main category: cs.CV

TL;DR: Enginuity 是首个开放、规模大、跨多工程领域且具备完整结构标注的工程图示数据集，用于自动化图示解析与下游多模态任务。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型难以理解工程图中层级组件、连接关系与语义元素，限制了在科学与工程工作流中的参与与推理能力。需要一个高质量、结构化标注的数据集来突破图示理解这一关键瓶颈。

Method: 构建一个覆盖多工程领域的工程图数据集，提供层级组件关系、连线与语义元素等全面结构标注，使模型可进行结构化图示解析与跨模态检索等任务。

Result: 数据集的引入使多模态大模型能够处理结构化图示解析、跨模态信息检索与AI辅助工程仿真等关键下游任务。

Conclusion: Enginuity 将推动科学发现领域的AI，通过赋能AI理解并操作工程图中的视觉-结构知识，打破图示理解在科学工作流中的瓶颈，从而促进假设生成、实验设计与发现。

Abstract: We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.

</details>


### [181] [CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning](https://arxiv.org/abs/2601.13304)
*Wenxin Ma,Chenlong Wang,Ruisheng Yuan,Hao Chen,Nanru Dai,S. Kevin Zhou,Yijun Yang,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 提出CausalSpatial基准评测MLLM对3D场景“因果空间推理”（碰撞、兼容、遮挡、轨迹）的能力，发现与人类有巨大差距；归因于模型过度依赖文本链式推理而脱离视觉证据。为此提出COW，通过生成假设动态视频进行外化模拟，提供可视因果线索以提升推理。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多停留在静态空间感知，难以回答关于物体运动后果的“what-if”问题；需要一个专门基准来诊断，并探索改进路径以缩小与人类直觉物理推理的差距。

Method: 1) 构建CausalSpatial基准，涵盖四类任务：碰撞、兼容性、遮挡、轨迹，用于评估对物体运动后果的预测。2) 系统评测人类与MLLM（如GPT-5）表现并分析错误模式，发现文本式CoT与视觉证据脱钩。3) 提出COW（Causal Object World）：将因果推理外化为生成“假设动态视频”的模拟过程，把语言模型的推理锚定到可视动态证据上。

Result: 在人类84%对比GPT-5 54%的显著差距下，分析显示MLLM易产生空间上不着地的幻觉。采用COW后，模型可利用生成的动态视觉线索改进因果空间推理（文中暗示优于基线，具体数值未在摘要中给出）。

Conclusion: MLLM在因果空间推理上存在系统性短板，问题源于过度依赖文本推理且缺乏动态物理证据锚定。通过外化模拟并生成假设视频的COW框架，可为模型提供可视化因果线索，缓解“语言先验”导致的偏差；同时CausalSpatial为该方向提供标准化评测与开放资源。

Abstract: Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer "what-if" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial

</details>


### [182] [MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic](https://arxiv.org/abs/2601.13331)
*Wei Wang,Quoc-Toan Ly,Chong Yu,Jun Bai*

Main category: cs.CV

TL;DR: MultiST 是一个跨模态融合的空间转录组分析框架，通过跨注意力联合建模空间拓扑、基因表达与组织形态学，显著提高清晰、一致的空间域划分能力。


<details>
  <summary>Details</summary>
Motivation: 现有 ST 方法往往仅浅层融合或忽略组织图像，无法充分利用形态与分子信息，导致空间域边界模糊、下游推断（如伪时间、细胞互作）不稳定。

Method: 提出 MultiST：以图神经网络编码基因表达并结合对抗式对齐获取鲁棒空间表示；对苏木精-伊红（H&E）等组织图像做颜色归一化并提取形态特征；通过跨注意力机制在空间邻接拓扑上融合分子与形态信号，从而细化域边界与捕捉分子-形态依赖关系。

Result: 在跨两种器官的13个 ST 数据集（人脑皮层与乳腺癌等）上，MultiST 相较现有方法得到更清晰、连贯的空间域；并带来更稳定的伪时间轨迹与更具生物学解释性的细胞-细胞互作模式。

Conclusion: 跨注意力驱动的多模态联合建模能有效整合组织形态与分子数据，改进空间域分割并提升下游分析的稳定性与可解释性；MultiST 提供通用框架与开源实现，适用于多种 ST 场景。

Abstract: Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.

</details>


### [183] [Real-Time 4D Radar Perception for Robust Human Detection in Harsh Enclosed Environments](https://arxiv.org/abs/2601.13364)
*Zhenan Liu,Yaodong Cui,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: 提出在高杂波封闭环境中可控多级粉尘产生方法与4D毫米波雷达多传感器数据集；并以阈值噪声过滤+基于语义的聚类规则分类，实现在粉尘矿山场景下稳健行人检测。


<details>
  <summary>Details</summary>
Motivation: 严苛封闭场景（矿井、隧道、坍塌建筑）粉尘与强多径使毫米波感知产生虚警与漏检，缺乏可重复实验平台与公开数据支撑算法研究，需要系统性方法提升抗杂波与检测鲁棒性。

Method: 1) 搭建可控多级粉尘发生平台，在高度杂乱环境中进行可重复传播与感知实验；2) 采集并发布含相机与LiDAR辅助的4D毫米波雷达数据集；3) 在原始点云层面基于RCS、速度、方位、俯仰的阈值式滤波，抑制幽灵目标与强多径；4) 在聚类层面利用雷达语义（速度、RCS、体积扩展）进行规则式分类，实现实时行人检测，无需大量特定领域训练。

Result: 实验显示该集成流程显著降低杂波与多径引起的误检，提升检测稳健性与系统在粉尘矿区的整体抗扰能力。

Conclusion: 通过可控粉尘实验平台、数据集与两级雷达处理（阈值滤波+规则分类），可在粉尘密集、强多径环境中实现高鲁棒、实时的行人检测，为严苛环境下的毫米波感知提供可复现基准与工程可用方案。

Abstract: This paper introduces a novel methodology for generating controlled, multi-level dust concentrations in a highly cluttered environment representative of harsh, enclosed environments, such as underground mines, road tunnels, or collapsed buildings, enabling repeatable mm-wave propagation studies under severe electromagnetic constraints. We also present a new 4D mmWave radar dataset, augmented by camera and LiDAR, illustrating how dust particles and reflective surfaces jointly impact the sensing functionality. To address these challenges, we develop a threshold-based noise filtering framework leveraging key radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and mitigate strong multipath reflections at the raw data level. Building on the filtered point clouds, a cluster-level, rule-based classification pipeline exploits radar semantics-velocity, RCS, and volumetric spread-to achieve reliable, real-time pedestrian detection without extensive domainspecific training. Experimental results confirm that this integrated approach significantly enhances clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments.

</details>


### [184] [Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations](https://arxiv.org/abs/2601.13371)
*Junyi Zhang,Yiming Wang,Yunhong Lu,Qichao Wang,Wenzhe Qian,Xiaoyin Xu,David Gu,Min Zhang*

Main category: cs.CV

TL;DR: 提出一种将3D人脸几何约束到拓扑球面的表示，并在其2D展开图上用条件扩散模型联合生成几何与纹理，从而显著提升文本到3D人脸的几何质量、文本一致性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D方法难以获得高质量几何，根因在于3D空间顶点分布任意且复杂，难以形成干净连通性，导致网格劣化与重建不稳。需要一种能简化几何结构并与强大的2D生成模型协同的表示与生成框架。

Method: 1) 提出球面几何表示：把几何信号锚定到均匀球面坐标（拓扑球），确保规则点分布并可稳健重建网格连通性；球面可无缝展开成2D图。2) 在该2D展开图上构建“球面几何扩散”条件扩散框架：联合建模几何与纹理，几何显式条件化纹理合成，实现多样且可控的生成。

Result: 在文本到3D生成、三维人脸重建和文本驱动3D编辑等任务上取得成功；实验显示在几何质量、文本一致性与推理效率上显著优于现有方法。

Conclusion: 通过将复杂3D几何约束到规则球面流形并与2D扩散强力结合，可稳健重建网格、实现可控联合生成，全面提升文本到3D人脸生成体系的质量与效率。

Abstract: A fundamental challenge in text-to-3D face generation is achieving high-quality geometry. The core difficulty lies in the arbitrary and intricate distribution of vertices in 3D space, making it challenging for existing models to establish clean connectivity and resulting in suboptimal geometry. To address this, our core insight is to simplify the underlying geometric structure by constraining the distribution onto a simple and regular manifold, a topological sphere. Building on this, we first propose the Spherical Geometry Representation, a novel face representation that anchors geometric signals to uniform spherical coordinates. This guarantees a regular point distribution, from which the mesh connectivity can be robustly reconstructed. Critically, this canonical sphere can be seamlessly unwrapped into a 2D map, creating a perfect synergy with powerful 2D generative models. We then introduce Spherical Geometry Diffusion, a conditional diffusion framework built upon this 2D map. It enables diverse and controllable generation by jointly modeling geometry and texture, where the geometry explicitly conditions the texture synthesis process. Our method's effectiveness is demonstrated through its success in a wide range of tasks: text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show that our approach substantially outperforms existing methods in geometric quality, textual fidelity, and inference efficiency.

</details>


### [185] [A Lightweight Model-Driven 4D Radar Framework for Pervasive Human Detection in Harsh Conditions](https://arxiv.org/abs/2601.13373)
*Zhenan Liu,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: 提出一种完全模型驱动的4D毫米波雷达感知框架，在尘雾/烟雾和金属环境中实现实时、稳健的人体检测，优于相机与激光雷达在能见度极低场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 工业与地下环境中的粉尘、烟雾、狭窄空间和金属结构导致光学/激光雷达感知迅速退化；4D毫米波雷达虽具抗干扰优势，但其稀疏、各向异性的点云难以有效用于可靠的人体检测，尤其在封闭、低能见度空间中缺乏成熟处理方法。

Method: 构建仅依赖雷达的、可在嵌入式边缘设备实时运行的模型驱动管线：1) 领域知识驱动的多阈值滤波；2) 融合自运动补偿的时序累积；3) KD-tree欧式聚类并结合多普勒信息细化；4) 基于规则的3D分类器。

Result: 在注尘的封闭拖车与真实地下矿道中评测：在严重能见度退化时，相机与激光雷达检测失败，而所提雷达框架仍能稳定识别行人。

Conclusion: 该模型驱动方案在恶劣工业与地下场景中具备鲁棒、可解释、计算高效的感知能力，适用于安全关键应用。

Abstract: Pervasive sensing in industrial and underground environments is severely constrained by airborne dust, smoke, confined geometry, and metallic structures, which rapidly degrade optical and LiDAR based perception. Elevation resolved 4D mmWave radar offers strong resilience to such conditions, yet there remains a limited understanding of how to process its sparse and anisotropic point clouds for reliable human detection in enclosed, visibility degraded spaces. This paper presents a fully model-driven 4D radar perception framework designed for real-time execution on embedded edge hardware. The system uses radar as its sole perception modality and integrates domain aware multi threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and a rule based 3D classifier. The framework is evaluated in a dust filled enclosed trailer and in real underground mining tunnels, and in the tested scenarios the radar based detector maintains stable pedestrian identification as camera and LiDAR modalities fail under severe visibility degradation. These results suggest that the proposed model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial and subterranean environments.

</details>


### [186] [Practical Insights into Semi-Supervised Object Detection Approaches](https://arxiv.org/abs/2601.13380)
*Chaoxin Wang,Bharaneeshwar Balasubramaniyam,Anurag Sangem,Nicolais Guevara,Doina Caragea*

Main category: cs.CV

TL;DR: 对三种先进半监督目标检测方法（MixPL、Semi-DETR、Consistent-Teacher）在不同标注数量下的表现进行系统对比，并在COCO、VOC与自建甲虫数据集上评测，分析精度、模型规模与延迟的权衡，给出低数据场景的适配建议。


<details>
  <summary>Details</summary>
Motivation: 在标注样本稀缺（few-shot）但未标注数据丰富的现实场景中，如何有效利用未标注数据提升检测性能是关键难题。需要比较主流SSOD方法在不同标注规模与不同数据集（通用与小类目专用）上的鲁棒性与效率。

Method: 选取三种SOTA SSOD方法（MixPL、Semi-DETR、Consistent-Teacher），在MS-COCO与Pascal VOC两大基准以及一个小类目自建甲虫数据集上进行系统实验；改变标注图像数量以观察性能随标注量的变化；同时记录精度、模型大小与推理延迟，进行多维权衡分析。

Result: 在不同标注量下三种方法的性能存在差异，并伴随精度、模型规模与延迟的权衡；在小类目、数据较少的甲虫数据集上亦展现出方法间的行为差异，为专用数据集上的选择提供依据。

Conclusion: 不同SSOD方法在低数据 regime 中各有优劣：需要依据精度目标、计算资源与实时性要求综合选择。研究给出了在标注稀缺条件下方法选型的实证参考。

Abstract: Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.

</details>


### [187] [Organ-Aware Attention Improves CT Triage and Classification](https://arxiv.org/abs/2601.13385)
*Lavsen Dahal,Yubraj Bhandari,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: 论文提出ORACLE-CT，一个与编码器无关、基于器官感知的头部，用器官掩膜注意力和器官标量融合，在胸部与腹部CT分级任务上实现新的有监督SOTA，并提供可定位的证据与校准预测。


<details>
  <summary>Details</summary>
Motivation: 临床亟需对高通量CT进行分诊与分类，以减轻放射科负担并提升护理质量；但现成VLM在3D解剖、协议偏移与嘈杂报告监督下表现欠佳，且缺乏可定位证据与良好校准。

Method: 先构建并精调一个强有力的有监督基线（简单的GAP头），再提出ORACLE-CT头：1）Organ-Masked Attention：基于器官掩膜的注意力/池化，限制注意范围，生成每器官空间证据；2）Organ-Scalar Fusion：融合标准化的器官体积与平均HU两个轻量标量特征；方法对编码器无关，并在统一评估协议下验证。

Result: 在胸部CT（CT-RATE）上，ORACLE-CT的掩膜注意力模型AUROC达0.86；在腹部CT（MERLIN，30项发现）上，作者复现实验显示其有监督基线已优于零样本VLM，再加上掩膜注意与标量融合后AUROC提升至0.85；总体上在胸/腹部数据集上均达成新的有监督SOTA。

Conclusion: 器官感知的注意力与标量融合能在CT分级中提供更强的可解释、可校准的预测并提升性能；在统一协议下，该方法在胸部与腹部CT上均达到SOTA，适合作为高通量临床分诊与分类的实用方案。

Abstract: There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.

</details>


### [188] [Leveraging Transformer Decoder for Automotive Radar Object Detection](https://arxiv.org/abs/2601.13386)
*Changxu Zhang,Zhaoze Wang,Tai Fei,Christopher Grimm,Yi Jin,Claas Tebruegge,Ernst Warsitz,Markus Gardill*

Main category: cs.CV

TL;DR: 提出一种基于Transformer的雷达三维目标检测框架：用Transformer解码器直接从多尺度雷达特征回归3D框和类别分数；通过Pyramid Token Fusion将特征金字塔统一成尺度感知的token序列；以集合预测替代密集候选与NMS；在RADDet上显著超越雷达-only方法。


<details>
  <summary>Details</summary>
Motivation: 现有雷达检测常依赖密集proposal、启发式后处理（如大量NMS调参）且难以充分建模跨尺度与时空长程关系；需要一种端到端、能处理多尺度特征并减少后处理负担的方案以提升雷达-only 3D检测性能。

Method: 1) 设计以Transformer解码器为预测头，使用可学习的object queries与位置编码，将检测表述为集合预测；2) 提出Pyramid Token Fusion，将多尺度雷达特征金字塔转为统一、尺度感知的token序列，便于与解码器对齐；3) 建模长程时空相关与跨特征交互，端到端直接回归3D框和类别分数，去除密集proposal与繁琐NMS。

Result: 在RADDet数据集上，相比当下最佳雷达-only基线取得显著性能提升（文摘未给出具体数值，但表明全面优于SOTA）。

Conclusion: 将多尺度雷达特征通过PTF统一并用Transformer解码器做集合式端到端预测，可有效建模时空长程依赖并消除繁琐后处理，带来对雷达三维目标检测的显著改进。

Abstract: In this paper, we present a Transformer-based architecture for 3D radar object detection that uses a novel Transformer Decoder as the prediction head to directly regress 3D bounding boxes and class scores from radar feature representations. To bridge multi-scale radar features and the decoder, we propose Pyramid Token Fusion (PTF), a lightweight module that converts a feature pyramid into a unified, scale-aware token sequence. By formulating detection as a set prediction problem with learnable object queries and positional encodings, our design models long-range spatial-temporal correlations and cross-feature interactions. This approach eliminates dense proposal generation and heuristic post-processing such as extensive non-maximum suppression (NMS) tuning. We evaluate the proposed framework on the RADDet, where it achieves significant improvements over state-of-the-art radar-only baselines.

</details>


### [189] [Deep Image Prior with L0 Gradient Regularizer for Image Smoothing](https://arxiv.org/abs/2601.13400)
*Nhat Thanh Tran,Kevin Bui,Jack Xin*

Main category: cs.CV

TL;DR: 提出无需训练数据的深度图像先验（DIP）与ℓ0梯度正则化融合的图像平滑方法，通过ADMM并结合现成ℓ0梯度最小化求解器，实现保边去纹理与JPEG伪影去除，实验优于多种现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有图像平滑方法或依赖局部统计/优化，或依赖深度学习且需要精心构造的训练集；而构建高质量平滑训练数据困难，促使作者寻求无需监督数据也能获得高质量平滑的方案。

Method: 将深度图像先验（DIP）框架与ℓ0梯度正则化结合：在DIP的生成网络输出上施加ℓ0梯度“范数”作为先验，构建非凸非光滑的目标函数；为有效优化，设计ADMM分裂策略，并在ℓ0梯度子问题上调用现成的ℓ0梯度最小化求解器。

Result: 数值实验表明，所提DIP-ℓ0在保边图像平滑和JPEG伪影去除任务上优于多种经典与近期方法（含深度学习方法），生成高质量、结构保真的平滑结果。

Conclusion: 无需训练数据即可通过DIP与ℓ0梯度正则联合实现高质量保边平滑；提出的ADMM优化使非凸非光滑问题可解，方法在多任务上表现出色，验证了无数据先验与稀疏梯度先验的有效结合。

Abstract: Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\ell_0$, a deep image prior framework that incorporates the $\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\ell_0$ ``norm", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.

</details>


### [190] [Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics](https://arxiv.org/abs/2601.13401)
*Peter A. Massih,Eric Cosatto*

Main category: cs.CV

TL;DR: 论文提出用于定量空间推理的卫星图像数据集SQuID和一种代码生成式的定量视觉语言模型QVLM，通过分割掩码保持像素级索引，显著提升计数与测量等任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在定量空间推理（计数、测量）上表现不佳，因视觉编码将图像压缩为补丁嵌入而丢失像素级信息；需要既能评测该能力的基准，又需维持像素精度的架构。

Method: 1) 构建SQuID数据集：2,000个卫星图像问答对，含数值区间与类别答案，三种难度层级，基于人工标注及其学习到的变异性自动生成标注。2) 设计QVLM：将语言理解与视觉分析解耦，不将图像压缩为嵌入；模型生成可执行代码，先调用分割模型得到像素级掩码，再在掩码上进行运算以保持空间索引。

Result: 在SQuID上，使用GPT-5作为代码生成器的QVLM达到42.0%准确率，相比将图像与问题直接输入VLM的28.1%有明显提升。

Conclusion: 对定量空间推理任务，采用架构解耦并通过代码执行在像素级掩码上推理，可显著提升准确率；保持像素精度是关键。

Abstract: Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.

</details>


### [191] [Local-to-Global Logical Explanations for Deep Vision Models](https://arxiv.org/abs/2601.13404)
*Bhavan Vasu,Giuseppe Raffa,Prasad Tadepalli*

Main category: cs.CV

TL;DR: 提出以人类可识别原始概念为基础的本地与全局可解释方法，用MDNF逻辑公式与单调解释列表来解释黑盒图像分类器，并在视觉数据集上保持高保真与覆盖率。


<details>
  <summary>Details</summary>
Motivation: 深度网络尽管分类准确，但难以解释；需要把黑盒模型的决策用人可理解的概念表达，既能针对单样本也能针对样本集合，提升信任、调试与知识发现能力。

Method: 1) 定义人类可识别的“原始概念”作为解释基元；2) 对单样本与样本集合分别学习单调析取范式（MDNF）公式，使其满足时模型在给定类上得分高；3) 对多类别情形，学习基于原始概念的单调“解释列表”（类似决策列表）；4) 以黑盒访问（score/label）为前提，构造高保真、可覆盖的规则。

Result: 在具有挑战性的视觉数据集上，所得MDNF与解释列表在忠实度（对黑盒预测的一致性）与覆盖率（能解释的样本比例）上均表现良好，且解释简洁、可读。

Conclusion: 简单、可解释的MDNF与单调解释列表能有效解释黑盒图像分类决策，并在不牺牲太多性能的前提下提供局部与全局层面的高保真解释。

Abstract: While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.

</details>


### [192] [Using deep learning for predicting cleansing quality of colon capsule endoscopy images](https://arxiv.org/abs/2601.13412)
*Puneet Sharma,Kristian Dalsbø Hindberg,Benedicte Schelde-Olesen,Ulrik Deding,Esmaeil S. Nadimi,Jan-Matthias Braun*

Main category: cs.CV

TL;DR: 用ResNet-18对500张CCE图像的清洁度（四级Leighton-Rex量表）进行分类，并通过结构化剪枝提升效率且不降精度：在79%稀疏度下达到88%交叉验证准确率；采用多种CAM方法与ROAD评估可解释性，并用自适应温度缩放在外部数据上做校准。


<details>
  <summary>Details</summary>
Motivation: CCE图像清洁度直接影响病灶可见性与诊断可靠性；人工评估主观且一致性有限。需要一个高效、可解释且可泛化的自动化模型来辅助临床。

Method: - 数据：500张CCE图像，14位临床医师按Leighton-Rex四级标注。
- 模型：ResNet-18；使用分层K折交叉验证。
- 压缩：迭代式结构化剪枝以获得高稀疏度。
- 可解释性：Grad-CAM、Grad-CAM++、Eigen-CAM、Ablation-CAM、Random-CAM；统一用ROAD进行量化评估。
- 校准：对剪枝后模型采用变体的自适应温度缩放，在外部数据集上进行概率校准。

Result: 剪枝后模型在79%稀疏度下实现88%交叉验证准确率（未剪枝为84%），在不牺牲性能的情况下显著提升效率。可解释性方法表现存在差异，ROAD在该任务上应用面临挑战。

Conclusion: 结构化剪枝能在保持甚至提升CCE清洁度分类性能的同时显著提高模型效率；临床应用需重视可解释性与校准。评估清洁度与解释方法的一致性仍具挑战，ROAD的适配性需进一步研究，并需在更大、外部多中心数据上验证。

Abstract: In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.

</details>


### [193] [Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study](https://arxiv.org/abs/2601.13416)
*A. Nieto Juscafresa,Á. Mazcuñán Herreros,J. Sullivan*

Main category: cs.CV

TL;DR: 论文探索将扩散模型作为“冻结特征编码器”用于细粒度识别：直接探查扩散去噪过程不同层与时间步的中间特征，并为每个特征对训练线性分类器，结果在浮游生物监测任务上优于多种自监督基线、接近或匹配监督方法，并在分布移位下保持稳健。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成上已达SOTA，其作为通用表征学习器的潜力尚未系统验证。作者动机在于：1) 无标签去噪训练本质上是一种自监督学习，或可学习丰富的层级语义；2) 细粒度识别与长尾、分布移位场景需要强鲁棒的表征；3) 实际应用（浮游生物监测）需要可复用且无需微调的大模型特征。

Method: 冻结预训练扩散模型，不做微调；从多层多时间步的去噪特征中抽取表征（跨层×时间步的“特征对”）。对每个特征对训练一个线性分类器（probe），并比较其表现；训练和评估在受控条件下与监督/自监督基线对齐。场景覆盖平衡与自然长尾数据，并进行时空（时间与地理）分布移位的OOD测试。

Result: 在真实浮游生物分类任务中：冻结扩散特征与监督基线相当，且显著优于其他自监督方法；在平衡与自然长尾设置下均取得更高准确率与Macro F1；在时间和地域分布移位的OOD数据上，仍维持较强的准确率与Macro F1，表现出良好稳健性。

Conclusion: 扩散模型的中间去噪特征可作为强大的通用表征，尤其适用于细粒度与长尾场景；无需微调，仅用线性探针即可获得强性能，并在分布移位下具备鲁棒性。这提示扩散模型不仅是生成器，亦是竞争力的自监督特征编码器。

Abstract: Diffusion models have emerged as state-of-the-art generative methods for image synthesis, yet their potential as general-purpose feature encoders remains underexplored. Trained for denoising and generation without labels, they can be interpreted as self-supervised learners that capture both low- and high-level structure. We show that a frozen diffusion backbone enables strong fine-grained recognition by probing intermediate denoising features across layers and timesteps and training a linear classifier for each pair. We evaluate this in a real-world plankton-monitoring setting with practical impact, using controlled and comparable training setups against established supervised and self-supervised baselines. Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. Out-of-distribution evaluations on temporally and geographically shifted plankton datasets further show that frozen diffusion features maintain strong accuracy and Macro F1 under substantial distribution shift.

</details>


### [194] [SGW-GAN: Sliced Gromov-Wasserstein Guided GANs for Retinal Fundus Image Enhancement](https://arxiv.org/abs/2601.13417)
*Yujian Xiong,Xuanzhao Dong,Wenhui Zhu,Xin Li,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: 提出SGW-GAN，将切片Gromov-Wasserstein用于视网膜彩照增强，既提升视觉质量又保持类内结构，改进糖网分级并降低跨疾病GW差异。


<details>
  <summary>Details</summary>
Motivation: 现有GAN/扩散增强注重与高质分布对齐但易破坏类内几何，导致临床相关样本分散、类别边界模糊，影响分级/病灶检测；需要一种既提质又保结构的无配对医学图像增强方法。

Method: 以Sliced Gromov-Wasserstein（SGW）近似GW，通过随机投影对内部成对距离进行对齐，将其融入对抗式增强框架（SGW-GAN），在训练中最小化SGW以保持类内关系，同时实现无配对退化到高质域映射；计算成本显著低于原始GW。

Result: 在公开数据集上，SGW-GAN生成更具临床可信的增强结果；在糖尿病视网膜病变分级上优于现有方法；按疾病标签计算的GW差异最低，表明更好地保持了类内结构与跨类边界。

Conclusion: SGW-GAN以低成本保留结构关系，兼顾视觉与临床任务表现，为无配对医疗图像增强提供高效且具临床保真度的解决方案。

Abstract: Retinal fundus photography is indispensable for ophthalmic screening and diagnosis, yet image quality is often degraded by noise, artifacts, and uneven illumination. Recent GAN- and diffusion-based enhancement methods improve perceptual quality by aligning degraded images with high-quality distributions, but our analysis shows that this focus can distort intra-class geometry: clinically related samples become dispersed, disease-class boundaries blur, and downstream tasks such as grading or lesion detection are harmed. The Gromov Wasserstein (GW) discrepancy offers a principled solution by aligning distributions through internal pairwise distances, naturally preserving intra-class structure, but its high computational cost restricts practical use. To overcome this, we propose SGW-GAN, the first framework to incorporate Sliced GW (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing cost. Experiments on public datasets show that SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity for unpaired medical image enhancement.

</details>


### [195] [Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation](https://arxiv.org/abs/2601.13440)
*Mohit Kakda,Mirudula Shri Muthukumaran,Uttapreksha Patel,Lawrence Swaminathan Xavier Prince*

Main category: cs.CV

TL;DR: 论文综述VLM（如CLIP）在无/少样本异常检测中的方法与效果，比较WinCLIP滑窗密集特征、多阶段可学习对齐（AprilLab）与组合化提示集，对AC/AS在MVTec AD与VisA上的准确度、分割精度与效率进行系统评测，给出成功机理、方法选择建议与局限。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测常需大量标注或正常样本建模，对跨域泛化与细粒度缺陷定位困难。CLIP等VLM通过图文对齐与自然语言描述实现零/少样本识别，潜力巨大但方法众多、设计权衡复杂，亟需系统分析与实证对比以指导工业落地与研究方向。

Method: 从三类范式系统剖析：1) 滑窗式密集特征提取（WinCLIP）用于像素/块级AS；2) 多阶段特征对齐与可学习投影（AprilLab框架）以缩小视觉-文本域间隙；3) 组合/可组合提示集的提示工程以增强鲁棒性。按特征提取、对齐策略、提示设计、零/少样本折中、计算效率、跨域泛化等维度评估；在MVTec AD与VisA上比较AC准确率、AS精度与推理效率。

Result: 实验证明：VLM在零样本条件下即可取得有竞争力的AC/AS表现；滑窗密集特征在细粒度分割上更强但计算开销大；多阶段可学习对齐在少样本下显著提升；提示集组合能稳定跨类泛化；不同方法在效率与精度间存在明显权衡。

Conclusion: VLM通过自然语言先验和图文对齐为异常检测提供通用可迁移表征，能在零/少样本下有效完成AC/AS。实践中应依据精度-效率需求选择滑窗或轻量对齐方案，并结合提示集增强。当前局限包括细粒度定位的计算成本、对域移与长尾缺陷的敏感性、文本提示依赖及可解释性不足；未来方向为更高效的密集对齐、鲁棒提示自动化与跨域自适应。

Abstract: Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.

</details>


### [196] [Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging](https://arxiv.org/abs/2601.13498)
*Nimrod Kruger,Nicholas Owen Ralph,Gregory Cohen,Paul Hurley*

Main category: cs.CV

TL;DR: 提出将事件相机的稀疏ON/OFF事件映射为每像素对数强度及其时间导数，并把它们嵌入具有时变点扩散函数(PSF)的动态线性系统中，从而可直接对事件数据做频域维纳反卷积，实现在动态光学系统中的源定位与分离。


<details>
  <summary>Details</summary>
Motivation: 事件相机具备微秒级、宽动态范围与低带宽优势，但其非线性触发机制与线性前向模型难以耦合，限制了基于物理与反演的计算成像与光学设计应用。需要一个把事件表示与线性系统理论对接的框架，以利用成熟的频域反卷积与系统辨识工具。

Method: 1) 物理建模：把事件流解释为对数强度越阈产生的测量，估计每像素对数强度和强度时间导数；2) 动态成像模型：构建含时变PSF/传函的动态线性系统；3) 反演：已知或参数化的动态传递函数下，对事件导出的测量在频域执行维纳反卷积以恢复场景/源；4) 验证：仿真中对单点与重叠点光源在调制离焦下测试；真实数据上，用可调焦望远镜拍摄星场，进行定位与可分辨性实验。

Result: 仿真显示在调制离焦下，能从事件数据中准确恢复并分离重叠点源；真实星空事件数据上实现了稳定的星源定位与可分离性，证明了动态传函下的事件反卷积可行。

Conclusion: 该框架把事件相机的非线性事件表示转化为可用于线性系统反演的量，支持在时变光学系统中直接对事件数据进行频域去卷积，连接了事件感知与基于模型的计算成像，具有实际工程可用性。

Abstract: Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.

</details>


### [197] [DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities](https://arxiv.org/abs/2601.13502)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 该论文提出DIS2方法，面向遥感多模态缺失问题，通过重新设计解耦学习与知识蒸馏的协同（DLKD）、类内自适应特征学习（CFLM）与分层混合融合（HF），实现对缺失信息的有指导补偿，并在多基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感数据模态高度异质、尺度变化大，常规多模态方法在模态缺失时失效。传统解耦依赖模态共享特征，不适用于异质数据；传统蒸馏在缺失场景下变成盲目模仿，难以缩小语义鸿沟。因此需要面向RS特点的缺失信息补偿、类别自适应与多分辨率建模框架。

Method: 提出DIS2范式，核心为DLKD：将解耦学习与知识蒸馏重新表述为“主动、定向”的缺失特征补偿，显式学习可补偿特征，并与现有模态特征融合以逼近全模态理想表示。引入CFLM进行按类别建模，依据可用信号自适应提取判别证据；并采用分层混合融合（HF）结构，跨多分辨率进行特征融合和强化预测。

Result: 在多个遥感基准数据集上，DIS2显著优于当前SOTA方法（文中称“Extensive experiments validate...”，具体数值未在摘要中给出）。

Conclusion: 面向RS模态缺失，DIS2通过DLKD的定向补偿、CFLM的类别自适应，以及HF的多尺度融合，有效缓解异质性与尺度变化带来的困难，提升了鲁棒性与精度，对RS多模态学习提供了新的范式。

Abstract: The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.

</details>


### [198] [GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models](https://arxiv.org/abs/2601.13524)
*Yang Yu,Yunze Deng,Yige Zhang,Yanjie Xiao,Youkun Ou,Wenhao Hu,Mingchao Li,Bin Feng,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: 该论文提出首个面向多层服装试穿（ML-VTON）的系统GO-MLVTON，通过学习服装遮挡关系并用Stable Diffusion进行形变与贴合，生成逼真的多层试穿效果；同时发布MLG数据集与新指标LACD，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VTON多关注单层或多件同层穿搭，忽视真实场景中的多层穿搭（内外层叠穿）。难点在于正确建模内外衣之间的遮挡，避免内层冗余特征干扰，提升层次一致性与观感真实性。

Method: 1) Garment Occlusion Learning模块：显式/隐式学习内外衣之间的遮挡关系，抑制内层对外层的干扰；2) 基于Stable Diffusion的Garment Morphing & Fitting模块：利用扩散模型对服装进行形变与贴合，保持纹理细节并生成高质量层叠效果；3) 构建MLG数据集；4) 提出层次外观一致性差异指标LACD用于评测。

Result: 在新提出的MLG数据集与既有评测上取得领先，能生成高保真、遮挡关系正确的多层试穿图像；定量指标（含LACD）与可视化均优于现有方法。

Conclusion: GO-MLVTON首次系统性解决ML-VTON问题，通过遮挡学习与扩散式形变贴合实现逼真多层叠穿；配套数据集与指标促进该方向研究，并展现SOTA性能。

Abstract: Existing Image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.

</details>


### [199] [DiffFace-Edit: A Diffusion-Based Facial Dataset for Forgery-Semantic Driven Deepfake Detection Analysis](https://arxiv.org/abs/2601.13551)
*Feng Ding,Wenhui Yi,Xinan He,Mengyao Xiao,Jianfeng Xu,Jianqiang Du*

Main category: cs.CV

TL;DR: 提出DiffFace-Edit数据集：包含200万+细粒度人脸编辑假图，覆盖8个关键区域与多种组合编辑，并首次系统评估“检测器规避”拼接攻击对现有检测模型的影响，提供跨域评测与IMDL方法结合的分析。


<details>
  <summary>Details</summary>
Motivation: 现有生成脸部数据集缺乏对细粒度局部（如眼睛、鼻子等）编辑的系统覆盖，且忽视真实样本与篡改样本之间的区域拼接（splice）对检测器的真实影响；需要一个能刻画微小、区域级操控并用于研究检测器脆弱性的基准。

Method: 构建DiffFace-Edit：1) 生成200万+AI伪造人脸；2) 按8个面部区域进行单区域与多区域编辑，形成丰富组合；3) 特别构造并标注“检测器规避”样本（真实-伪造跨域拼接）；4) 对现有检测模型进行系统实验，设计结合IMDL（跨域/多源域学习）的方法与跨域评测协议；5) 全面统计与消融分析。

Result: 在包含细粒度区域编辑与拼接的设定下，主流人脸伪造检测器性能显著下降，尤其在“检测器规避”样本上表现脆弱；跨域评测显示域偏移显著，IMDL结合方案能一定程度提升鲁棒性但仍存在差距。

Conclusion: 细粒度区域编辑与真实-伪造拼接对现有深伪检测构成更强挑战；DiffFace-Edit为评估与提升检测鲁棒性提供了大规模基准与协议，推动针对局部操控与跨域泛化的研究。

Abstract: Generative models now produce imperceptible, fine-grained manipulated faces, posing significant privacy risks. However, existing AI-generated face datasets generally lack focus on samples with fine-grained regional manipulations. Furthermore, no researchers have yet studied the real impact of splice attacks, which occur between real and manipulated samples, on detectors. We refer to these as detector-evasive samples. Based on this, we introduce the DiffFace-Edit dataset, which has the following advantages: 1) It contains over two million AI-generated fake images. 2) It features edits across eight facial regions (e.g., eyes, nose) and includes a richer variety of editing combinations, such as single-region and multi-region edits. Additionally, we specifically analyze the impact of detector-evasive samples on detection models. We conduct a comprehensive analysis of the dataset and propose a cross-domain evaluation that combines IMDL methods. Dataset will be available at https://github.com/ywh1093/DiffFace-Edit.

</details>


### [200] [Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation](https://arxiv.org/abs/2601.13565)
*Yu Qin,Shimeng Fan,Fan Yang,Zixuan Xue,Zijie Mai,Wenrui Chen,Kailun Yang,Zhiyong Li*

Main category: cs.CV

TL;DR: 提出FiCoP，通过从全局匹配转向受空间约束的补丁级对应，缓解开放词汇6D位姿估计中的背景干扰问题；通过去噪、跨视角全局感知与补丁相关性预测，实现更稳健的开世界位姿估计，并在REAL275与Toyota-Light上显著提升AR。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇6D位姿估计依赖无约束的全局匹配，易将目标与背景干扰混淆，导致匹配歧义与位姿退化；需要一种能缩小匹配范围、利用结构先验抑制噪声的方案。

Method: FiCoP以“补丁到补丁”的相关性为结构先验，实施空间受限的细粒度对应。管线包括：1）对象中心的解缠预处理，分离语义目标与环境噪声；2）跨视角全局感知（CPGP）模块，融合双视角特征并进行显式上下文推理以达成结构共识；3）补丁相关性预测器（PCP），生成块级关联图作为空间滤波器，指导稳健匹配与位姿回归。

Result: 在REAL275与Toyota-Light数据集上，相较SOTA的Average Recall分别提升8.0%与6.1%，表明在复杂开放环境下的鲁棒性与泛化性更强。

Conclusion: 以补丁相关矩阵为先验的空间约束匹配能有效抑制干扰、提升开放词汇6D位姿估计性能；FiCoP在标准基准上取得显著收益，适用于开世界机器人感知。

Abstract: Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.

</details>


### [201] [ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch](https://arxiv.org/abs/2601.13606)
*Zheng Liu,Honglin Lin,Chonghan Qin,Xiaoyang Wang,Xin Gao,Yu Li,Mengzhang Cai,Yun Zhu,Zhanping Zhong,Qizhi Pei,Zhuoshi Pan,Xiaoran Shang,Bin Cui,Conghui He,Wentao Zhang,Lijun Wu*

Main category: cs.CV

TL;DR: 提出ChartVerse框架，面向图表推理的VLM数据合成：用RPE度量并驱动高复杂度图表生成；采用答案优先、真值锚定的逆向QA合成并严格一致性校验，结合失败率筛选与CoT蒸馏，构建大规模SFT与RL数据；据此训练的ChartVerse-8B在多项基准上达SOTA，超越其教师模型并接近更大模型。


<details>
  <summary>Details</summary>
Motivation: 开源VLM在图表推理上的进展受限于训练数据质量与复杂度：现有合成图表过于简单、模式单一，QA对往往存在幻觉、缺乏多步推理深度。因此需要一个能系统地产生高复杂度图表与可信、可验证推理监督的数据构建框架。

Method: 1) 复杂度度量与驱动：提出Rollout Posterior Entropy (RPE) 评估图表复杂度，并用复杂度感知的chart coder通过可执行代码自动合成多样高复杂图表。2) 真值锚定的逆向QA：采用“答案先行”，从源码确定性提取答案，再条件生成问题，并进行严格一致性校验。3) 难度提升与推理增强：基于模型失败率筛选样本，并蒸馏高质量CoT。4) 数据集构建：用Qwen3-VL-30B-A3B-Thinking作为教师，得到ChartVerse-SFT-600K与ChartVerse-RL-40K。

Result: 用上述数据训练的ChartVerse-8B在图表推理任务上取得SOTA，显著超越其教师模型，并可与更大规模的Qwen3-VL-32B-Thinking竞争。

Conclusion: 通过RPE驱动的复杂图表合成与真值锚定的逆向QA，ChartVerse能大规模、可验证地生成高质量图表推理数据，显著提升开源VLM在图表推理上的表现，具有可扩展与可迁移的潜力。

Abstract: Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.

</details>


### [202] [CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models](https://arxiv.org/abs/2601.13622)
*Donghee Lee,Rui Cai,Zhe Zhao*

Main category: cs.CV

TL;DR: CARPE 是一个面向开源 LVLM 的通用、可插拔框架，通过“视觉集成层+上下文感知的集成策略”在需要时优先使用图像表征，在需要推理时更依赖语言模型，从而提升图像分类与多模态基准上的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 现有 LVLM 在通用问答上强，但在以视觉为核心的任务（如图像分类）上往往不如其底座视觉编码器（多为 CLIP），说明多模态融合与决策时对视觉/语言信号的取舍不当，需要一种机制动态权衡与利用更丰富的图像表征以缩小性能差距。

Method: 提出 CARPE：1) 视觉集成层：为 LVLM 注入多样化的图像表征以扩展视觉信息通道；2) 上下文感知的集成策略：根据输入上下文判断当前应更重视图像特征还是语言模型的推理输出，自适应加权视觉与文本模态；3) 模型无关、可与多数由视觉编码器+语言模型构成的开源 LVLM 对接。

Result: 在大量实验中，CARPE 在图像分类基准上显著提升，并在多种视觉-语言基准上也带来一致改进，显示其增强了模型对不同任务与场景的泛化能力。

Conclusion: 通过在决策时动态优先级分配视觉与语言信息，CARPE 弥补了 LVLM 在视觉中心任务上的短板，同时保持甚至提升其在多模态任务上的表现，且具有良好的架构适配性与可集成性。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.

</details>


### [203] [Scaling Test-time Inference for Visual Grounding](https://arxiv.org/abs/2601.13633)
*Guanqi Zhan,Changye Li,Zhijian Liu,Yao Lu,Yi Wu,Song Han,Ligeng Zhu*

Main category: cs.CV

TL;DR: 提出EGM，通过在小型VLM上“扩展测试时计算”（增加生成token数/自回归推理步骤）来弥补语言能力差距，以更低延迟达到与大模型相当或更优的视觉指向定位效果；在RefCOCO上8B模型达91.4 IoU且737ms，优于235B的90.5 IoU与4320ms；还在“遮挡/非显式（amodal）”定位设置下验证了方法的通用性与显著增益。


<details>
  <summary>Details</summary>
Motivation: 小模型在视觉编码规模与大模型近似，定位能力差主要来自语言理解不足而非视觉表征；直接用大模型推理开销大、延迟高，需在保持部署友好的前提下提升小模型的定位效果与效率。

Method: 提出Efficient visual Grounding language Models (EGM)：不增大模型参数，而是扩大测试时计算量，具体为在小模型中增加生成token量（更长思维/多步推理、解码策略等），以充分发挥语言建模能力来辅助视觉 grounding；在标准与amodal定位任务上进行评估。

Result: 在RefCOCO基准上，EGM-Qwen3-VL-8B以平均737ms实现91.4 IoU，较Qwen3-VL-235B的90.5 IoU与4320ms延迟更快（约5.9倍）且更准；在新设定的amodal grounding任务中，小模型用EGM后稳定且显著提升，达到或超越大模型。

Conclusion: 通过扩大小模型测试时计算（生成token数）可显著提升视觉定位能力，以更低成本和延迟接近或超过大模型，方法对常规与amodal定位均通用、部署友好，提升视觉 grounding 效率。

Abstract: Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.

</details>


### [204] [Face-Voice Association with Inductive Bias for Maximum Class Separation](https://arxiv.org/abs/2601.13651)
*Marta Moscati,Oleksandr Kats,Mubashir Noman,Muhammad Zaigham Zaheer,Yufang Hou,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: 提出在人脸-语音关联中引入“最大类间分离”的归纳偏置，使不同说话者的多模态表征彼此尽量远，同类尽量近，从而在两种任务设定上达到SOTA，并显示与类间正交损失联合最有效。


<details>
  <summary>Details</summary>
Motivation: 现有人脸-语音关联多依赖度量学习/对比损失来拉近同人、拉远异人，但嵌入的判别性仍受限。分类领域近期进展表明，通过在训练中显式施加“最大类别分离”的归纳偏置，可显著提升判别能力；然而该思想尚未在多模态（尤其是人脸-语音关联）中验证。

Method: 构建一种多模态表征学习框架，将人脸与语音编码为共享嵌入空间，并在训练目标中引入“最大类间分离”的归纳偏置（如在类中心/分类器权向量上施加相互远离或正交约束），同时结合促进跨类正交的损失；通过该组合提升跨模态对齐与类别可分性。

Result: 在两种常见的人脸-语音关联任务设定上取得SOTA效果；消融实验证明：单独使用归纳偏置有效，但与类间正交损失联合时收益最大。

Conclusion: 首次在多模态学习中将“最大类间分离”作为归纳偏置成功应用于人脸-语音关联，显著增强嵌入判别性并达成SOTA，提示该思路可作为新的多模态学习范式。

Abstract: Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.

</details>


### [205] [VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement](https://arxiv.org/abs/2601.13664)
*Tiancheng Fang,Bowen Pan,Lingxi Chen,Jiangjing Lyu,Chengfei Lyu,Chaoyue Niu,Fan Wu*

Main category: cs.CV

TL;DR: VIAFormer 是一种用于多视图引导体素修复的Transformer，通过多视图图像对不完整、噪声体素进行精修，刷新合成与真实场景中的修复SOTA，并可无缝融入现实3D创作流程。


<details>
  <summary>Details</summary>
Motivation: 现有从多视图图像或大模型生成的体素常含严重缺失与伪影，跨模态（2D图像-3D体素）对齐不足、融合不稳健、以及优化目标间接，导致修复质量与鲁棒性受限。作者动机是提出一种能显式对齐2D与3D、学习直接修复轨迹并稳健融合多模态信息的通用体素精修框架。

Method: 提出 VIAFormer，包括三大关键设计：1) Image Index：为2D图像token提供显式3D空间定位与锚定，实现体素-图像在体素坐标系中的对齐；2) Correctional Flow 目标：直接学习体素从有缺陷到高质量的连续修复轨迹，减少间接监督带来的歧义；3) Hybrid Stream Transformer：融合体素流与图像流，采用稳健的跨模态注意力与交互机制以增强鲁棒性与信息互补。

Result: 在包含严重合成破坏和来自视觉基础模型生成体素的真实伪影两类设置中，均取得新的SOTA修复效果；实验表明其跨模态对齐与融合有效，能在多视图条件下显著提升体素质量。

Conclusion: VIAFormer 可作为现实3D内容创建流水线中的可靠桥梁，使体素方法在大模型与大数据背景下更具可用性与扩展性，并为多视图引导的体素精修树立新基线。

Abstract: We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.

</details>


### [206] [Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting](https://arxiv.org/abs/2601.13665)
*Mounika Kanulla,Rajasree Dadigi,Sailaja Thota,Vivek Yelleti*

Main category: cs.CV

TL;DR: 提出融合CNN与LSTM、DeiT的多任务模型，同时进行蔬菜分类、腐败检测与货架期预测，在自建从新鲜到完全腐败的图像数据集上，CNN+DeiT表现最佳（分类F1=0.98、腐败检测F1=0.61、预测MSE=3.58、SMAPE=41.66%），并通过噪声鲁棒性测试与LIME可视化验证可靠性。


<details>
  <summary>Details</summary>
Motivation: 农业供应链中食物浪费严重；需要既能准确检测食物腐败、又能提前预测保质/货架期的信息，以支持供应链调度与减损。现有方法多聚焦单任务或单一模型，难以兼顾分类、检测与预测的综合需求。

Method: 构建涵盖蔬菜从新鲜到完全腐败阶段的图像数据集；设计两种融合架构：1) CNN+CNN-LSTM（提取时空/序列腐败特征），2) CNN+DeiT Transformer（结合卷积的局部纹理与Transformer的全局注意）；以多任务学习框架同时优化蔬菜种类分类、腐败二分类、以及基于时间序列的货架期回归（MSE/SMAPE度量）；对比CNN、VGG16、ResNet50、CapsNet、DeiT等基线；在噪声扰动下评估鲁棒性，并用LIME做可解释性可视化。

Result: 两种融合模型均优于对比基线，其中CNN+DeiT最优：蔬菜分类F1=0.98；腐败检测F1=0.61；货架期预测MSE=3.58、SMAPE=41.66%。在加入噪声后仍保持较好性能；LIME显示决策集中于与腐败相关的区域。

Conclusion: 融合卷积与Transformer/时序建模的多任务框架能在同一系统中完成分类、腐败检测与货架期预测，并优于多种深度学习基线；模型具有一定鲁棒性与可解释性，适用于农业供应链中的减损与调度场景。

Abstract: Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.

</details>


### [207] [Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging](https://arxiv.org/abs/2601.13677)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jäger,Klaus Maier-Hein,Fabian Isensee*

Main category: cs.CV

TL;DR: 提出ClaSP PE主动学习策略，在3D生物医学分割中稳定优于改进的随机基线，兼顾精度与标注效率，并能零调参泛化到新数据集。


<details>
  <summary>Details</summary>
Motivation: 3D生物医学体数据标注昂贵且耗时，现有主动学习方法在3D场景下难以稳定超越经过加强的随机采样基线，缺乏可可靠部署的方案。

Method: 提出Class-stratified Scheduled Power Predictive Entropy（ClaSP PE）：(1) 类别分层查询，保证对少数类别/结构的覆盖，缓解类别不均衡；(2) 在预测熵上施加对数尺度的power噪声，并随迭代设置衰减调度，早期强化多样性、后期侧重利用，实现从探索到利用的平滑过渡；在nnActive基准框架上按统一指南设置参数，不做数据集特定调参。

Result: 在4个3D数据集、24个实验设置中，ClaSP PE是唯一在分割质量上普遍显著优于改进随机基线且标注效率高的方法；在4个未见过的新数据集上的零调参评估同样表现稳健，显示良好泛化。

Conclusion: 在贴近实际部署的场景下，ClaSP PE为3D分割提供了可靠的主动学习策略，能稳定超越强随机基线并保持标注成本友好，且具有跨数据集的鲁棒泛化；提供开源实现与明确部署指南，便于实践落地。

Abstract: Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.

</details>


### [208] [Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation](https://arxiv.org/abs/2601.13683)
*Boyuan Cao,Xingbo Yao,Chenhui Wang,Jiaxin Ye,Yujie Wei,Hongming Shan*

Main category: cs.CV

TL;DR: 提出DyDiLA，一种动态差分线性注意力，用于缓解线性注意在扩散Transformer中的过度平滑问题，提升生成质量，并在DyDi-LiT上实现SOTA表现。


<details>
  <summary>Details</summary>
Motivation: DiT生成质量高但自注意力二次复杂度限制扩展性；线性注意可降耗但常导致注意力过度平滑、表达力下降，影响生成效果。需要一种既保留线性复杂度又避免过度平滑的注意力机制。

Method: 设计DyDiLA线性注意，包括三点：1) 动态投影模块：为token动态分配知识并解耦表示；2) 动态测度核：为不同token动态选择核函数以更细粒度衡量相似度；3) token差分算子：基于动态测度核产出的信息，计算token与其冗余之间的差分，增强query-key检索鲁棒性。将其系统集成到改进的线性扩散Transformer——DyDi-LiT。

Result: 在多项指标与基准上，DyDi-LiT稳定优于现有SOTA线性注意/扩散Transformer方法，显示更好的生成质量与实用潜力（具体数据未在摘要中给出）。

Conclusion: DyDiLA缓解线性注意的过度平滑，提升LiT的表达与生成性能；DyDi-LiT验证其有效性，并在保持线性复杂度的同时实现SOTA。

Abstract: Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.

</details>


### [209] [Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles](https://arxiv.org/abs/2601.13705)
*Maria Lymperaiou,Vasileios Karampinis,Giorgos Filandrianos,Angelos Vlachos,Chrysoula Zerva,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 该综述把“视觉谜题”作为评估大规模视觉语言模型（LVLM）推理能力的诊断工具，按推理机制统一框架梳理基准，并总结当前模型在泛化、感知-推理解耦与解释-执行一致性上的共性缺陷，提出面向基准与系统设计的改进方向。


<details>
  <summary>Details</summary>
Motivation: 谜题能以少先验、高可控、可验证的方式分离并检验抽象化、规则发现与系统性推理；在开放式多模态评测噪声较大、可解释性不足的背景下，需要用更精确的“诊断仪器”刻画LVLM推理强弱与短板。

Method: 提出一个统一抽象，将视觉谜题按目标推理机制分为：归纳、类比、算法式、演绎、几何/空间；据此整理现有数据集与任务，并综合对比不同类别上的实证结果，以揭示跨任务的共性误差模式。

Result: 跨基准观察到：模型泛化脆弱；感知与推理强耦合，视觉识别微误差会放大到推理失败；模型能产出流利解释但与真实求解过程不一致（解释-执行鸿沟）。

Conclusion: 将视觉谜题定位为诊断工具而非一般任务格式，可更精确评估LVLM推理；未来应构建更可控、可分离感知与推理、能检验系统性泛化与过程一致性的基准，并开发面向推理感知耦合的模型与训练策略。

Abstract: Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.

</details>


### [210] [ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins](https://arxiv.org/abs/2601.13706)
*Xinhao Liu,Yu Wang,Xiansheng Guo,Gordon Owusu Boateng,Yu Cao,Haonan Si,Xingchen Guo,Nirwan Ansari*

Main category: cs.CV

TL;DR: ParkingTwin是一个无需训练、轻量化、可在线流式构建停车场数字孪生的系统，通过OSM先验直接构建度量一致TSDF、几何感知的动态过滤以及对照明鲁棒的CIELAB融合，实现实时(30+ FPS)重建，并在大规模实测数据上优于3DGS且资源占用更低。


<details>
  <summary>Details</summary>
Motivation: AVP场景需要高保真停车场数字孪生以做路径规划、碰撞检测与感知验证，但面临：前视稀疏视角导致几何不适定；动态遮挡与极端光照破坏纹理融合；神经渲染需昂贵离线优化，不符合边缘侧流式约束。

Method: 1) OSM先验驱动几何：利用OpenStreetMap语义拓扑直接生成度量一致的TSDF，避免盲目几何搜索与重优化。2) 几何感知动态过滤：基于法向/高度/深度一致性的四模态约束场，实时剔除移动车辆与瞬时遮挡。3) 照明鲁棒融合：在CIELAB空间中自适应L通道加权并结合深度梯度抑制，降低突变光照引起的接缝。系统在线流式运行，输出显式三角网格。

Result: 在68,000平方米真实数据集上：SSIM 0.87（+16%），端到端速度约15倍提升，GPU显存降低83.3%；在入门级GTX 1660上30+ FPS运行；相较需要高端GPU（RTX 4090D）的3DGS具备更高性价比与更好工程适配性。

Conclusion: 通过将地图先验、几何一致性过滤与光照解耦融合结合，ParkingTwin在资源受限的边缘侧实现高保真、鲁棒、实时的停车场数字孪生重建，便于接入Unity/Unreal等工业管线。

Abstract: High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/

</details>


### [211] [Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2601.13707)
*Yujin Jo,Sangyoon Bae,Taesup Kim*

Main category: cs.CV

TL;DR: 论文提出“注意力空间对比引导”（ACG）以缓解多模态大模型中的幻觉，通过在一次前向计算中对比视觉-语言与仅语言路径，降低语言先验主导，显著提升忠实度与效率（延迟最高降至2倍）。


<details>
  <summary>Details</summary>
Motivation: LVLM 常因语言先验覆盖视觉证据而产生对象误识与不一致描述；现有对比解码需多次前向传递，代价高。需要一种既能抑制语言先验、又高效的单次机制。

Method: 将幻觉缓解表述为“对比引导”：在自注意力层内显式构造两条注意力路径——视觉-语言路径与仅语言路径，并在单次前向中进行对比，引导生成更依赖视觉证据。同时加入正交化校正，去除与仅语言路径对齐的分量以放大视觉贡献，从而纠正单次近似带来的偏差。

Result: 在 CHAIR 与 POPE 基准上取得最先进的忠实度与字幕质量，同时计算成本显著降低，相比需多次前向的对比解码方法延迟最高降低约2倍。

Conclusion: ACG 在模型内部注意力空间直接施加高效的对比引导，既提升视觉忠实度又降低计算开销，提供了较以往多次前向对比解码更为原则且高效的替代方案。

Abstract: Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.

</details>


### [212] [MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network](https://arxiv.org/abs/2601.13715)
*Yiwei Lu,Hao Huang,Tao Yan*

Main category: cs.CV

TL;DR: 提出MVGD-Net，通过利用视频中的运动不一致性来检测玻璃表面，并配套构建大规模数据集，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 玻璃表面在现实中普遍存在，会对基于视觉的系统（如机器人与无人机导航）造成风险。现有工作对视频玻璃表面检测（VGSD）关注增加，但仍需更好地利用时序运动特征来区分玻璃反射/透射与真实场景。作者观察到反射/透射层的物体在视觉上更远，导致在视频中其表观运动速度更慢，这种运动不一致性可作为稳定线索。

Method: 提出MVGD-Net，核心利用运动不一致性线索：1）CMFM跨尺度多模态融合模块，将空间外观特征与光流估计进行融合；2）HGAM历史引导注意力模块，利用历史帧信息提升时序表征；3）TCAM时间交叉注意力模块，进一步强化跨时间的相关性；4）TSD时空解码器融合空间与时间特征生成玻璃区域掩码。同时构建包含312种玻璃场景、共19,268帧的大规模视频数据集用于训练与评测。

Result: 在作者构建的数据集及其他相关基准上，MVGD-Net在VGSD任务上优于现有最先进方法，显示更高的检测精度与鲁棒性。

Conclusion: 利用玻璃反射/透射导致的运动速度差异作为判别线索是有效的。MVGD-Net通过多模态融合与时序注意力设计显著提升了视频玻璃检测性能；新数据集为该方向提供了可靠训练与评测资源。

Abstract: Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.

</details>


### [213] [Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search](https://arxiv.org/abs/2601.13719)
*Xinlei Yin,Xiulian Peng,Xiao Li,Zhiwei Xiong,Yan Lu*

Main category: cs.CV

TL;DR: HAVEN提出一个面向长视频理解的统一框架，通过多模态实体凝聚与分层索引结合智能检索，实现连贯、全面的推理，在LVBench上达成SOTA（总准确率84.1%，推理任务80.1%）。


<details>
  <summary>Details</summary>
Motivation: 长视频对视觉-语言模型造成超长上下文挑战。现有基于简单切块+检索增强的方法易导致信息碎片化与全局连贯性丢失，尤其难以维持跨片段的叙事与实体一致性。

Method: 1) 融合视听流的实体级表示，跨模态保持语义一致；2) 构建分层视频索引（全局摘要-场景-片段-实体）；3) 采用代理式搜索在各层动态检索与推理，兼顾全局叙事重建与细粒度实体跟踪。

Result: 在LVBench上取得新的SOTA：总体准确率84.1%；在困难的推理类别达到80.1%。并展示更好的时间连贯性、实体一致性与检索效率。

Conclusion: 结构化、多模态、代理式检索与推理能显著提升长视频的上下文一致与全面理解，缓解长上下文导致的碎片化问题。

Abstract: Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.

</details>


### [214] [Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement](https://arxiv.org/abs/2601.13724)
*Sam Cantrill,David Ahmedt-Aristizabal,Lars Petersson,Hanna Suominen,Mohammad Ali Armin*

Main category: cs.CV

TL;DR: 提出将人脸rPPG的时空处理从2D像素移动到3D人脸表面：用人脸网格序列构建时空图（STGraph），并用轻量图时空网络MeshPhys进行生理信号估计，实现更稳健、可解释、具泛化的SOTA/竞品表现。


<details>
  <summary>Details</summary>
Motivation: 现有rPPG方法虽然利用面部颜色微变化，但其感受野未与真实的3D面部表面对齐，导致空间支持与信号来源不一致，易受姿态、表情、光照和背景干扰，泛化性与解释性不足。

Method: 1) 构建Facial Spatiotemporal Graph（STGraph）：以3D人脸网格序列为节点与边，节点特征包含对齐到表面的颜色与3D感知属性，时间上通过序列连接实现时空建模；2) 设计轻量级时空图卷积网络MeshPhys，在STGraph上进行表面对齐的时空卷积以回归生理信号；3) 在四个基准数据集上评测，并进行消融以检验表面对齐感受野与3D感知特征的作用。

Result: MeshPhys在四个基准数据集的组内与跨数据集设置中取得SOTA或具有竞争力的结果。消融表明：将感受野约束在面部表面提供强结构先验；使用表面对齐、3D感知的节点特征对于稳健编码肤色变化至关重要。

Conclusion: STGraph+MeshPhys提供了一种面向rPPG的表面对齐、3D感知、可解释的建模范式，显著提升稳健性与泛化。代码开源，便于复现与拓展。

Abstract: Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .

</details>


### [215] [HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection](https://arxiv.org/abs/2601.13751)
*Daniel Kyselica,Jonáš Herec,Oliver Kutis,Rado Pitoňák*

Main category: cs.CV

TL;DR: 提出HiT机制，将历史上下文注入Transformer以实现在小卫星上进行连续洪水变化检测，存储需求降至原图的不足1%，在保持精度的同时实现实时（Jetson Orin Nano上43 FPS）。


<details>
  <summary>Details</summary>
Motivation: 连续卫星观测需要在严格的计算与存储预算下处理多时相数据；洪水监测对时效性要求高，但现有方法常依赖地面处理或需要存储完整历史影像，难以在小卫星上部署。

Method: 提出History Injection for Transformer（HiT）：在Transformer中维护来自先前观测的紧凑历史表征，并在新观测时注入该历史，以进行变化检测；将HiT集成到小型基础模型Prithvi-tiny中；在STTORM-CD洪水数据集上评估，并在Jetson Orin Nano上做实时性测试。

Result: 在STTORM-CD上，HiT-Prithvi在精度上与双时相基线相当，同时将历史数据存储量减少>99%，并在Jetson Orin Nano上达到43 FPS。

Conclusion: HiT为在轨连续灾害监测提供可行方案：在极低存储/算力资源下保持检测准确并实现实时处理，减少对地面基础设施依赖；代码与模型已开源，便于复现与部署。

Abstract: Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection

</details>


### [216] [PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval](https://arxiv.org/abs/2601.13797)
*Gabriele Serussi,David Vainshtein,Jonathan Kouchly,Dotan Di Castro,Chaim Baskin*

Main category: cs.CV

TL;DR: 提出PREGEN：利用冻结的预训练VLM与轻量编码器，无需微调与生成即可从VLM各层最终token隐状态提取表示，训练小编码器得到用于检索的紧凑语义嵌入，在CoVR基准上显著提升R@1并强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有CoVR要么依赖过时结构、要么需要对VLM高成本微调或慢速字幕生成，未充分挖掘现代VLM潜力，亟需一种高效、免微调且可泛化的利用方式。

Method: 将查询视频与修改文本共同输入冻结的预训练VLM，从每一层提取最终token的隐藏状态并汇聚；基于这些多层表示训练一个轻量编码模型，生成紧凑、语义丰富的检索嵌入，无需对VLM微调或进行文本生成。

Result: 在标准CoVR基准上显著超越现有方法，Recall@1分别提升+27.23与+69.59；对不同VLM骨干具有鲁棒性，并在更复杂文本修改的零样本设定下表现优异。

Conclusion: PREGEN高效利用冻结VLM的分层语义，通过轻量编码器实现强大的视频-文本组合检索性能，兼具效率、可移植性与强泛化，代表CoVR的新SOTA范式。

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.

</details>


### [217] [Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders](https://arxiv.org/abs/2601.13798)
*Kai Wittenmayer,Sukrut Rao,Amin Parchami-Araghi,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: 提出Insight：一种语言对齐的概念级基础模型，可在图像中产生细粒度、可解释且具空间定位的概念表示，在分类与分割上保持与黑盒基础模型相当的性能，同时给出高质量概念解释。


<details>
  <summary>Details</summary>
Motivation: 现有语言对齐视觉基础模型虽在多任务上强，但内部表征不透明，难以解释；已有概念分解方法多局限于分类且空间定位差，无法给出细粒度、可对齐的人类可理解解释。

Method: 构建Insight：结合具强语义的基础模型与层次化稀疏自编码器，自动在多粒度上提取概念；利用局部共现依赖推导概念关系，并据此改进概念命名与解释质量；实现概念的空间落点以支持分割与解释。

Result: 在基准数据上，Insight在分类与分割任务的性能与黑盒基础模型具有竞争力，同时能提供细粒度、高质量、空间可定位的概念级解释。

Conclusion: Insight成功将可解释、可对齐且空间定位的概念引入视觉基础模型，实现可用性与可解释性的兼顾，为更丰富的概念关系与命名奠定基础。

Abstract: Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.

</details>


### [218] [Discriminant Learning-based Colorspace for Blade Segmentation](https://arxiv.org/abs/2601.13816)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一种用于图像分割前的颜色空间学习方法：Colorspace Discriminant Analysis（CSDA），通过可学习的判别颜色表示提升分割精度，并在风机叶片数据上显著增益。


<details>
  <summary>Details</summary>
Motivation: 许多分割算法忽视颜色表示这一关键预处理，导致类间混淆、类内差异大，尤其在特定领域（如风机叶片缺陷）下常规RGB或固定颜色空间难以充分分离目标与背景。

Method: 将LDA扩展到深度学习场景，学习一个任务定制的非线性多维颜色空间。核心是最大化带符号的多维类间可分性、最小化类内方差，构建广义判别损失；并提出三种替代损失以保证训练稳定，使颜色空间学习与分割网络端到端联合优化。

Result: 在风电叶片数据集上，相比基线分割方法显著提升精度（文中称“显著增益”）；表明学习到的颜色表示能更好地拉开类别、减少误分。

Conclusion: 针对特定领域定制颜色空间是提升分割性能的有效途径；CSDA提供了稳定、可端到端训练的判别颜色表示学习框架，可作为分割模型前端的关键预处理模块。

Abstract: Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.

</details>


### [219] [FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation](https://arxiv.org/abs/2601.13837)
*Xinya Ji,Sebastian Weiss,Manuel Kansy,Jacek Naruniec,Xun Cao,Barbara Solenthaler,Derek Bradley*

Main category: cs.CV

TL;DR: 提出一种端到端前馈方法，从少量图像生成可实时驱动的高质量3D高斯头像，兼顾效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯头像方法要么依赖多视角采集，要么需对单人视频做推理阶段的个体优化，导致对新身份的扩展性差、效率低、使用门槛高。需要一种从少量图像即可快速生成高保真头像并支持实时动画的方案。

Method: 提出\OURS：1) 直接从输入图像学习逐像素高斯表示；2) 用Transformer编码器跨视角聚合特征，融合DINOv3与Stable Diffusion VAE的图像特征；3) 为实时动画，在显式高斯上附加每高斯特征，并用轻量MLP动态网络根据表情编码预测3D高斯形变；4) 利用大型重建模型的点映射作为几何监督以提升几何光滑度。

Result: 在渲染质量与推理效率上显著优于现有方法，同时支持实时的动态头像动画。

Conclusion: 少量图像即可生成高保真、可实时驱动的3D高斯头像，兼顾质量、效率与可泛化性，优于当前方法。

Abstract: Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.

</details>


### [220] [DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes](https://arxiv.org/abs/2601.13839)
*Aisha Al-Mohannadi,Ayisha Firoz,Yin Yang,Muhammad Imran,Ferda Ofli*

Main category: cs.CV

TL;DR: 提出灾害领域VQA基准DisasterVQA：1395张图、4405个专家标注问答，涵盖洪水、野火、地震等；支持二元、选择与开放式问题；多模型基线显示在细粒度推理与情境理解上仍有明显短板。


<details>
  <summary>Details</summary>
Motivation: 通用VQA在灾害响应这种复杂且安全关键的场景是否可靠尚不明；缺少与人道救援框架对齐、能评估感知与推理能力的权威基准，阻碍了面向实战的模型改进与部署。

Method: 构建DisasterVQA数据集：收集多类真实灾害图像；依据FEMA ESF和OCHA MIRA等人道框架设计任务与问题，覆盖态势感知与行动决策；问题形式包含二元、选择题和开放式；用七个SOTA视觉语言模型做基准评测，分析按题型、灾种、地区与人道任务的性能差异。

Result: 模型在二元题上准确率较高，但在细粒度定量推理、目标计数与情境敏感解读方面表现不佳，尤其在样本不足的灾种上波动更大；不同题型、灾类、区域和任务维度存在显著性能差异。

Conclusion: DisasterVQA是一个挑战性且贴近实务的基准，可用于推动更稳健、具操作意义的灾害响应VLM研发；当前模型离满足实地应用需求仍有差距，需针对定量与情境推理能力进行改进。

Abstract: Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.

</details>


### [221] [Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation](https://arxiv.org/abs/2601.13852)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出深度判别分析（DDA）及其概率化变体（PDDA），以稳定方式直接优化Fisher准则，显著减少类内方差与类间重叠，并在风电叶片分割上取得显著且更一致的性能。


<details>
  <summary>Details</summary>
Motivation: 经典LDA能提升类可分性，但对非线性可分数据无能为力，且直接在深网中优化Fisher准则易出现数值不稳定；需要一种既能处理非线性、又训练稳定、并能在分割任务上有效减少类别重叠的方法。

Method: 将Fisher判别准则嵌入深度网络，提出稳定训练的技巧：使用带符号的类间方差、用sigmoid限制输出范围、将乘法关系转为加法；据此构建两种稳定的DDA损失，并加入概率损失形成PDDA，从而最小化输出分布的类间重叠、降低类内方差，提升置信度与一致性。

Result: PDDA在风电叶片图像分割任务中取得显著性能提升与更高一致性，显示更小类内方差与更少类别重叠，预测更为高置信。

Conclusion: DDA/PDDA能在深度学习框架下稳定直接优化Fisher准则，改善非线性场景下的判别能力，并在风电叶片分割中验证有效性；据称为DDA首次用于图像分割。

Abstract: Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.

</details>


### [222] [OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting](https://arxiv.org/abs/2601.13871)
*Michail Spanakis,Iason Oikonomidis,Antonis Argyros*

Main category: cs.CV

TL;DR: 提出OCCAM：首个无需训练与额外信息的通用类无关计数方法，结合SAM2与阈值版FINCH，实现多类别逐类计数，在FSC-147与CARPK具竞争力；并提出合成多类数据集与F1评价指标。


<details>
  <summary>Details</summary>
Motivation: 现有类无关计数多假设单类场景，且依赖大模型长时间训练与额外提示（示例或文本），难以泛化与部署；缺少对多类别同时计数的无监督/零训练方案与合适的评价基准。

Method: 利用基础模型SAM2获得全图候选分割/掩码；基于定制阈值版FINCH进行实例分簇与去重，按类别聚合得到每类实例数；无需微调、无需示例或文本输入。提出一个合成多类数据集，以及以F1作为更合适的计数评估指标。

Result: 在FSC-147与CARPK基准上达到有竞争力的计数表现（训练零开销），并能在单图内对任意多类别分别计数。

Conclusion: OCCAM证明了借助通用分割基础模型与轻量聚类即可实现训练免、多类、无提示的类无关计数；新合成数据集与F1指标为后续研究提供了更贴近多类场景的评测框架。

Abstract: Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.

</details>


### [223] [Revisiting Multi-Task Visual Representation Learning](https://arxiv.org/abs/2601.13886)
*Shangzhe Di,Zhonghua Zhai,Weidi Xie*

Main category: cs.CV

TL;DR: MTV提出将视觉-语言对比、纯视觉自监督与稠密空间监督三类目标统一到一个多任务预训练框架中，利用专家模型生成伪标注，实现全局语义与局部空间精度的兼得，并系统分析多任务增益、协同/干扰与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习两极化：CLIP等擅长全局语义但空间定位弱；MAE/DINO等擅长局部结构却缺乏高层语义。需要一个能同时获取语义与空间信息、且可扩展且减少人工标注依赖的统一训练范式。

Method: 提出MTV多任务预训练：共享骨干网络，同时优化三类目标——(1)视觉-语言对比学习以捕获全局语义；(2)自监督（如遮挡重建/对比蒸馏）以学习细粒度结构；(3)稠密空间目标（深度、检测/分割等）。为避免人工标注，使用高容量专家模型（如Depth Anything V2、OWLv2）在大规模数据上合成稠密、结构化伪标签，并开展系统消融与缩放实验。

Result: MTV在不牺牲全局语义能力的前提下显著提升细粒度空间推理；消融显示各任务的边际贡献与协同效应，揭示何时存在干扰；在数据与模型规模扩大时保持良好伸缩性并取得“二者兼得”的SOTA级表现。

Conclusion: 高质量伪监督驱动的多任务学习是通向更通用视觉编码器的可扩展路径；将语义对齐与稠密空间学习联合优化可同时提升全局与局部表征能力。

Abstract: Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.

</details>


### [224] [OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3](https://arxiv.org/abs/2601.13895)
*Xu Zhang,Danyang Li,Yingjie Xia,Xiaohang Dong,Hualong Yu,Jianye Wang,Qicheng Li*

Main category: cs.CV

TL;DR: 提出OmniOVCD：基于SAM 3的开集遥感变化检测方法，通过融合SAM 3的语义/实例/存在输出并实例解耦，实现稳定高精度的类别识别与实例一致性，在四个数据集上取得SOTA IoU。


<details>
  <summary>Details</summary>
Motivation: 现有开集变化检测大多依赖CLIP识别并借助DINO等额外特征提取器，模型耦合复杂、特征对齐困难、系统不稳定。SAM 3将分割与识别能力集成于同一可提示模型，为消除多模型耦合与提升稳健性提供契机。

Method: 构建单体框架OmniOVCD，利用SAM 3解耦输出头（语义、实例、presence）。提出SFID（Synergistic Fusion to Instance Decoupling）：先协同融合三类输出生成地物掩膜，再将其分解为单实例掩膜以进行跨时相匹配与变化对比，从而同时保证类别识别准确与实例级时相一致性，最终生成变化掩膜。训练上为training-free/零样本思路（依托SAM 3能力）。

Result: 在LEVIR-CD、WHU-CD、S2Looking、SECOND四个基准上达到SOTA，类别平均IoU分别为67.2、66.5、24.5、27.1，全面超过既往方法。

Conclusion: 将SAM 3的多头输出通过SFID有机结合，可在无需额外模型的前提下实现稳定、精确的开集变化检测，并在多基准上验证了有效性与泛化性。

Abstract: Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.

</details>


### [225] [Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging](https://arxiv.org/abs/2601.13899)
*Masoumeh Javanbakhat,Piotr Komorowski,Dilyara Bareeva,Wei-Chang Lai,Wojciech Samek,Christoph Lippert*

Main category: cs.CV

TL;DR: 提出一种可解释的深度两样本检验框架，在无标签统计检验场景中同时给出样本级与特征（空间）级解释，用于定位哪些样本和输入特征驱动组间分布差异，尤其适用于生物医疗影像。


<details>
  <summary>Details</summary>
Motivation: 现有深度两样本检验虽有很强的检出能力，但因“黑箱”缺乏可解释性，难以在生物医学中被信任和采用；而主流事后解释方法依赖类别标签，不适用于无标签的统计检验（两样本检验）场景，需要一种在无监督/无标签条件下提供可解释性的统计测试。

Method: 在深度两样本检验框架上，加入面向无标签测试的解释模块：1）样本级解释，量化各样本对检出显著组差的贡献度，识别“影响力”样本；2）特征/空间级解释，定位输入特征（如影像的区域）对统计差异的贡献，给出可视化热图。整体实现为端到端可训练或可与现有深度检验（如基于特征映射与MMD/分类器判别的测试）组合的可解释管线。

Result: 在生物医学影像数据上，方法能稳定检测显著的组间差异，并准确指出最具影响力的样本与解剖学上有意义的区域，这些区域与疾病相关变异相一致。

Conclusion: 该框架将统计推断与可解释AI相结合，使深度两样本检验在无标签的人群分析中具备可解释性，为医学影像中的可解释、可审计的群体差异检测提供了实用工具。

Abstract: Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.

</details>


### [226] [On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.13913)
*Pavlo Melnyk,Cuong Le,Urs Waldmann,Per-Erik Forssén,Bastian Wandt*

Main category: cs.CV

TL;DR: 提出在单目3D人体姿态估计中，通过学习2D旋转等变性来改进从2D到3D的抬升；简单的旋转增强即可显著提升在平面内旋转情况下的鲁棒性，超过一些“等变性内置”的方法。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（2D关键点检测→2D到3D抬升）在输入图像被平面内旋转时性能显著下降，显示出对旋转的脆弱性。相比直接学习点到点的抬升映射，作者认为学习单一人体姿态及其平面内旋转族更容易、几何上更合理。

Method: 不改变网络结构、不过度约束参数空间，而是通过数据增广赋予模型2D旋转等变性：对2D关键点或图像进行系统的平面内旋转增强来训练抬升模型，使其输出与输入旋转一致地变换；与显式等变架构进行对比。

Result: 在常见HPE基准上，采用2D旋转增强的模型在与图像平面内旋转相关的姿态上取得更低误差，并在整体性能上优于若干“等变性设计”SOTA方法。

Conclusion: 2D旋转等变性本身就是关键提升因素；无需复杂的等变结构，通过简单高效的数据增强即可学到稳健的旋转等变性，从而改进单目3D人体姿态估计。

Abstract: Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.

</details>


### [227] [TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation](https://arxiv.org/abs/2601.13935)
*Anoushkrit Goel,Simroop Singh,Ankita Joshi,Ranjeet Ranjan Jha,Chirag Ahuja,Aditya Nigam,Arnav Bhavsar*

Main category: cs.CV

TL;DR: 提出TrackletGPT：将纤维束追踪分割表示为“类语言”序列建模，用tracklets（子流线片段）作为token，使GPT能利用顺序信息，跨数据集自动泛化，并在TractoInferno与HCP上优于SOTA（DICE/Overlap/Overreach），含跨数据集实验。


<details>
  <summary>Details</summary>
Motivation: 白质纤维束分割对连接组学、疾病研究和手术重要，但纤维束在个体/条件间差异大，传统方法难以稳健泛化；同时纤维在两侧半球与受试者间又具有相似的3D结构，提示可利用共享结构先验。现有方法对序列/拓扑信息利用不足，限制了精细和鲁棒分割。

Method: 提出TrackletGPT：将流线分割转化为“语言建模”问题。把流线切分为细粒度tracklets作为token，显式引入顺序信息；用GPT样式的自回归/注意力模型对tracklet序列进行编码与分类，实现可扩展、全自动的束分割。方法强调跨数据集无缝泛化与细粒度子流线表示。

Result: 在TractoInferno与HCP数据集上，TrackletGPT在平均DICE、Overlap与Overreach指标上优于现有最先进方法；跨数据集实验（训练/测试于不同数据集）同样保持领先。

Conclusion: 将纤维束追踪分割表述为序列建模并使用tracklets，可更好利用顺序与结构先验，提升精细度与泛化性能；方法自动、可扩展，并在多指标与跨域测试中达到SOTA。

Abstract: White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.

</details>


### [228] [Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning](https://arxiv.org/abs/2601.13942)
*Hongbo Bai,Yujin Zhou,Yile Wu,Chi-Min Chan,Pengcheng Wen,Kunhao Pan,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: 提出GoG框架，通过“选择性凝视”与复杂度自适应强化学习，减少无关视觉噪声、强化迭代反思，实现搜索增强LMM在复杂视觉问答上的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在长尾或时变知识相关问题上受限于静态参数知识；搜索增强方法多做整图检索，带来冗余与噪声，且缺乏深度迭代反思，导致复杂视觉查询效果不佳。

Method: 提出Glance-or-Gaze（GoG）主动视觉规划框架：1) 选择性凝视机制，在“全局一瞥”(glance)与“局部凝视”(gaze)间动态切换，仅对高价值区域进行检索以过滤无关信息；2) 双阶段训练：先用监督微调进行“反思式GoG行为对齐”，学习基本范式；再用复杂度自适应强化学习，在更复杂样例上迭代推理、策略优化。

Result: 在六个基准上达成SOTA；消融显示选择性凝视与复杂度自适应RL均为必要组件。

Conclusion: 主动视觉规划结合选择性检索与复杂度自适应学习，可显著提升LMM在知识密集和复杂视觉查询中的表现；数据与模型将开源以促进后续研究。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

</details>


### [229] [VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content](https://arxiv.org/abs/2601.13951)
*Shengyi Wu,Yan Hong,Shengyao Chen,Zheng Wang,Xianbing Sun,Jiahui Zhan,Jun Lan,Jianfu Zhang*

Main category: cs.CV

TL;DR: VTONGuard提出一个包含77.5万+真实与合成试穿图的大规模基准，系统评测多类检测方法，并提出结合分割的多任务框架，在该基准上取得最佳总体性能，旨在推动虚拟试穿生成内容的可检测性与负责任应用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI让虚拟试穿图像愈发逼真，带来真实性鉴别与滥用风险；现有检测方法与数据缺乏统一大规模基准与跨范式可泛化性评估。

Method: 构建覆盖姿态、背景、服饰风格等多样条件的VTONGuard数据集，含真实与操纵样本；在统一训练与测试协议下，对多种检测范式进行系统评测；提出融入辅助分割任务的多任务检测框架，强化边界感知特征学习。

Result: 揭示不同检测范式的优劣与跨范式泛化的持续难题；所提多任务框架在VTONGuard上取得最佳整体检测性能。

Conclusion: VTONGuard为公平比较与方法发展提供基础，促进更稳健的VTON内容检测，并推动虚拟试穿技术的安全与负责任落地。

Abstract: With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.

</details>


### [230] [DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging](https://arxiv.org/abs/2601.13954)
*Adrien Meyer,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: 提出DExTeR：一种用于医学影像的点到框（Point-to-Box）教师模型，结合Transformer与专家混合机制，在仅点标注和少量框标注条件下生成高质量伪框并训练检测器，在三大数据集上达SOTA，同时降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 医学影像目标检测需要昂贵的边界框标注；仅用点标注的WSSOD能降本，但医学场景存在重叠结构、尺度多样和目标模糊，导致从点推断框困难，现有Point-to-Box方法效果受限。

Method: 基于Point-DETR构建Transformer回归器DExTeR：1）将单点作为object query；2）提出类引导可变形注意力（使用点坐标与类别标签引导采样，提取类别特异特征）；3）提出CLICK-MoE（类别、实例与共识知识的专家混合）解耦类与实例表征，缓解相邻/重叠实例混淆；4）多点训练策略，强制不同点放置下预测一致，增强对标注变异的鲁棒性；用少量框标注训练教师，将点转伪框，进而训练学生检测器。

Result: 在内镜、胸片及内镜超声三个跨域数据集上取得SOTA点到框回归与目标检测性能，相比现有WSSOD方法在精度上显著提升，同时减少对完整框标注的依赖。

Conclusion: DExTeR通过类引导注意力、CLICK-MoE与多点一致性训练，有效解决医学影像中重叠、尺度变化与细微结构带来的点到框推断难题，在低成本标注条件下实现高精度检测，具备在多种医学影像场景推广的潜力。

Abstract: Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.

</details>


### [231] [STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames](https://arxiv.org/abs/2601.13974)
*Shih-Yao Lin*

Main category: cs.CV

TL;DR: 提出STEEC：一种无参考、轻量的帧采样质量指标，通过空间信息强度、时间分散度与非冗余性联合度量，能够区分不同采样策略并揭示跨视频的稳健性模式，但不用于预测下游任务准确率。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注感知质量或重建保真度，无法判断采样帧是否充分涵盖视频中信息性与代表性内容；缺乏在预算受限下对采样行为进行任务无关诊断的工具。

Method: 基于每帧的时空帧熵(STFE)量化空间结构复杂度；在此基础上设计STEC综合三要素：1) 空间信息强度（高熵更重要）；2) 时间覆盖/分散（鼓励覆盖更长时间跨度）；3) 非冗余（惩罚相似或密集重复帧）。无需参考视频或下游标签，直接对采样集合打分。

Result: 在MSR-VTT test-1k上，STEC能清晰区分随机、均匀、内容感知等常见采样策略；还能揭示仅看平均性能时被掩盖的跨视频稳健性差异与模式。

Conclusion: STEC是一个通用、任务无关的诊断指标，用于在预算受限时分析帧采样质量，强调覆盖与去冗余而非预测具体下游任务精度。

Abstract: Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.
  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.
  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.
  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.

</details>


### [232] [Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains](https://arxiv.org/abs/2601.13975)
*Marco Piccolo,Qiwei Han,Astrid van Toor,Joachim Vanneste*

Main category: cs.CV

TL;DR: 构建统一信息流水线评估固定检测器，揭示跨域性能主要受场景结构而非画质影响，并在低成本边缘设备上验证可用性，指向面向结构的可靠性与可部署性。


<details>
  <summary>Details</summary>
Motivation: 海洋生物多样性监测需要在复杂水下环境中具有可扩展、可靠的检测，但现有方法在新地点迁移时性能急剧下降，存在明显“部署鸿沟”，影响入侵物种长期监测与治理。

Method: 提出统一信息流水线（Unified Information Pipeline），将异构数据标准化为可比信息流；在受控的跨域协议下，用固定且具部署意义的检测器进行评测；分析结构性因素（场景构成、目标密度、上下文冗余）与视觉退化（浑浊度等）对性能的影响；并在低成本边缘硬件上进行推理基准测试与运行时优化。

Result: 跨多个域发现：场景结构因素比视觉退化更能解释跨域性能损失，稀疏场景易出现“上下文崩塌（Context Collapse）”失效模式；运行时优化可在廉价边缘设备上达到远程监测所需的采样率。

Conclusion: 应将研究重心从图像增强转向结构感知的可靠性；所提出的统一流水线与评测框架为一致、可民主化的海洋生态系统评估提供基础工具，并具备实际部署可行性。

Abstract: Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic "Context Collapse" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.

</details>


### [233] [FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2601.13976)
*Jing Zuo,Lingzhou Mu,Fan Jiang,Chengcheng Ma,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: FantasyVLN提出在不显式输出长CoT的情况下，利用视觉自回归编码器把“想象的视觉CoT”压缩到潜在空间，并在训练中统一学习文本、视觉与多模态CoT，推理时直接从指令到动作映射，实现兼具推理感知与实时性的VLN，显著提升成功率与效率并将延迟降一个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有VLN的CoT方法要么仅文本、缺乏空间落地且易过拟合稀疏标注；要么多模态生成“想象视觉”，导致token激增，推理速度慢，难以实时导航。需要一种既保留CoT推理优势、又避免显式token开销的统一方案。

Method: 提出FantasyVLN：用预训练Visual AutoRegressor将“想象的视觉token”编码为紧凑潜变量；在训练期采用统一multi-CoT策略，联合学习文本CoT、视觉CoT与多模态CoT，从而让模型获得推理感知表示；在推理期不生成显式CoT，直接做指令到动作映射，利用隐式的推理表示辅助决策。

Result: 在LH-VLN上，较显式CoT方法显著提升成功率与路径效率，并将推理时延降低约一个数量级，实现真实时间的导航能力。

Conclusion: 隐式、多模态对齐的CoT表示可在不牺牲实时性的前提下带来可解释与长程规划收益；通过VAR压缩“想象视觉”并统一多种CoT学习，是实现人类式导航推理与工程可用性的有效路径。

Abstract: Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.

</details>


### [234] [Equivariant Learning for Unsupervised Image Dehazing](https://arxiv.org/abs/2601.13986)
*Zhang Wen,Jiangwei Xie,Dongdong Chen*

Main category: cs.CV

TL;DR: 提出一种名为EID的无监督去雾框架，利用图像信号的等变性与雾一致性约束，并结合对抗式学习建模未知雾物理，在科学成像与自然图像上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去雾依赖精心设计的先验或大量无雾真值，获取成本高、在科学成像场景（如显微与内镜）不现实；需要一种无需真值、可泛化且物理合理的方法。

Method: 构建“等变图像去雾”框架：1) 强制雾一致性（在观测域保持物理一致）；2) 系统等变性（对几何/强度变换保持一致响应），从仅含雾的原始图像中恢复清晰结构；3) 采用对抗学习策略显式建模未知雾物理，作为生成器-判别器博弈促进去雾器学习；统一等变学习与雾物理建模。

Result: 在两个科学成像基准（细胞显微与医学内镜）以及自然图像去雾任务上，EID显著超过SOTA方法（文本未给出具体数值），显示更强的清晰度恢复与泛化能力。

Conclusion: 等变性约束与雾物理对抗建模的结合，可在无监督条件下有效去雾，特别适用于缺乏无雾真值的科学成像；方法具有通用性，代码与数据将开源，预计促进更广泛的去雾应用。

Abstract: Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.

</details>


### [235] [Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution](https://arxiv.org/abs/2601.14030)
*Samuel W. Remedios,Zhangxing Bian,Shuwen Wei,Aaron Carass,Jerry L. Prince,Blake E. Dewey*

Main category: cs.CV

TL;DR: 将扩散模型从单幅图像的逆问题推广到多图像超分辨MRI（MISR），利用DPS似然校正的可分离梯度，对多独立测量实现后验采样与重建，无需改动扩散模型或增加计算，取得在各向异性退化下的SOTA重建，接近各向同性体积。


<details>
  <summary>Details</summary>
Motivation: 现实MRI常获取多组互补的低分辨率切片（各向异性、不同轴退化），而现有扩散后验采样方法多针对单图像，无法充分利用多测量信息；希望在不重新训练扩散模型、不过度增加计算的前提下，将单图像方法扩展到多图像，以提升重建质量并满足临床流程。

Method: 理论上证明DPS（Diffusion Posterior Sampling）的似然梯度在独立测量下可精确分离为各测量项之和，因而可对每个测量分别计算梯度并累加，实现MISR而无需构建联合前向算子或修改扩散网络；据此推导出MISR版DPS、DMAP、DPPS以及扩散式PnP/ADMM流程，保持网络前向次数不增。

Result: 在4×/8×/16×各向异性退化的MRI上，相较单图像超分辨（SISR）显著提升；实现当前最好（SOTA）的各向异性MRI体数据超分辨，并能从常规2D多切片采集重建近似各向同性的解剖结构，显著改善正交视图的退化。

Conclusion: 利用DPS的可分离似然梯度，可无缝将扩散式逆问题求解从单图像扩展到多图像MRI超分辨，不需模型改动或额外计算，实证达SOTA并具临床实用价值。

Abstract: Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\times/8\times/16\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.

</details>


### [236] [Human detectors are surprisingly powerful reward models](https://arxiv.org/abs/2601.14037)
*Kumar Ashutosh,XuDong Wang,Xi Yin,Kristen Grauman,Adam Polyak,Ishan Misra,Rohit Girdhar*

Main category: cs.CV

TL;DR: 提出HuDA奖励模型，通过人检测置信度与时间一致性对齐分数评估并提升视频中人类运动质量；结合GRPO对视频生成模型后训练，显著改善复杂人体动作生成，超过SOTA（如Wan 2.1），并泛化到动物与人-物交互。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成虽有高保真与时序连贯，但在复杂非刚体运动（体育、舞蹈等）上常出现缺肢、多肢、姿态畸变、物理不合理等问题，需要一个无需昂贵标注与专门微调、可广泛适用的度量与优化信号。

Method: 构建简单的无训练奖励HuDA：1）外观质量：利用现成人体检测器的检测置信度；2）运动真实感：利用基于文本提示的时间对齐分数衡量动作与提示的一致性。将HuDA作为奖励，采用Group Reward Policy Optimization（GRPO）对视频扩散/生成模型进行后训练，以偏好优化提升生成。

Result: HuDA作为奖励在无需额外训练与标注的前提下，优于使用人工标注微调的专用模型；在复杂人体动作生成上显著提升，与SOTA（如Wan 2.1）对比获得73%胜率；并在动物视频与人-物交互生成上也带来明显改进。

Conclusion: 一个基于现成模型的简洁奖励即可有效引导视频模型学习真实的人体运动与动作-文本对齐，通过GRPO后训练取得广泛性能提升，并具备跨类别泛化能力。

Abstract: Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.

</details>


### [237] [Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving](https://arxiv.org/abs/2601.14038)
*Alexandre Justo Miro,Ludvig af Klinteberg,Bogdan Timus,Aron Asefaw,Ajinkya Khoche,Thomas Gustafsson,Sina Sharif Mansouri,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 该论文发现并纠正自动驾驶数据集中由激光雷达按时间扫描导致的3D框标注时空不一致问题，提出离线估计方法与新指标，在多个数据集上将标注质量提升>17%，并证明标注误差可达2.5米且显著影响算法评测结论。


<details>
  <summary>Details</summary>
Motivation: 主动传感器（如LiDAR）逐线/逐束扫描导致动态目标在不同时间被观测到，若忽视这一效应，3D框标注会出现系统性偏差；现有公开数据集可能存在这类隐性错误，影响监督学习与基准评测的公平性与可信度。

Method: 提出一种离线轨迹一致性估计与标注校正方法：利用物理可行的运动先验与传感器采样时间模型，将原始框与点云在时空上对齐，优化使目标轨迹在空间与时间上与传感器数据一致；同时首次定义针对时空一致性的评估指标。

Result: 在Argoverse 2、MAN TruckScenes及自有数据集上，标注质量提升超过17%；量化显示原标注最大偏移可达2.5米，动态目标受影响最明显；开源实现提供复现。

Conclusion: 时序扫描引发的标注误差广泛存在且幅度可显著影响SOTA基准比较；校正后能显著提高标注可靠性，准确标注对于正确解读模型性能至关重要。

Abstract: Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.

</details>


### [238] [Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation](https://arxiv.org/abs/2601.14039)
*Wesam Moustafa,Hossam Elsafty,Helen Schneider,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CV

TL;DR: 提出一种通用可插拔的“弃权”(abstention)框架，用于提升医学图像分割在噪声标签下的鲁棒性；通过知情正则与幂律自调节惩罚两大组件，生成三种鲁棒损失变体（GAC、SAC、ADS），在CaDIS与DSAD上显著优于原基线，尤其高噪声场景。


<details>
  <summary>Details</summary>
Motivation: 医学分割标注困难、易含噪声，传统模型易过拟合错误标签、泛化下降；分类领域的弃权机制已证实有效，但分割领域未被系统验证，需要一个通用方法把弃权思想迁移到分割并兼容多种损失。

Method: 构建通用弃权框架：1) 知情正则(informed regularization)引导模型在不确定或疑似噪声区域更倾向弃权；2) 基于幂律的自适应调参算法自动调节弃权惩罚力度。将该框架无缝嵌入三类常用分割损失，形成GAC、SAC、ADS三种噪声鲁棒损失变体。

Result: 在CaDIS与DSAD两套医学分割数据集上，三种变体在多种噪声强度下均显著超越对应的非弃权基线，提升在高噪声条件下尤为明显，验证了方法的通用性与有效性。

Conclusion: 允许模型对可疑或被污染样本“选择性忽略”（弃权）是一种强大且可迁移的策略；配合知情正则与幂律自调节的通用框架，可系统提升医学分割在噪声标签下的鲁棒性。

Abstract: Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.

</details>


### [239] [Federated Balanced Learning](https://arxiv.org/abs/2601.14042)
*Jiaze Li,Haoran Xu,Wanyi Wu,Changwei Wang,Shuaiguang Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Youyang Qu,Longxiang Gao,Xudong Yang,Lumin Xing*

Main category: cs.CV

TL;DR: 提出联邦平衡学习（FBL），在客户端通过生成式“知识填充”和“知识采样”实现样本平衡，配合知识对齐与知识丢弃策略，从源头缓解非IID导致的客户端漂移，实验证明优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在非IID场景中易产生客户端漂移，通常事后用损失或梯度校正全局模型，忽视了客户端样本分布的根因。作者希望从数据层面预防漂移，在不共享数据的前提下提高全局模型性能与稳健性。

Method: 在客户端侧引入边缘生成模型，在固定样本容量约束下：1）知识填充（生成并补齐缺失类别/模式）；2）知识采样（从生成库中抽样以达到类别均衡）。提出：a）知识对齐策略（缩小合成与真实数据分布差距）；b）知识丢弃策略（正则化，丢弃或降权低置信/冗余合成样本）。方法可异构部署，不同客户端可采用不同策略并扩展框架。

Result: 在多个真实且复杂场景的实验中，FBL在全局精度与鲁棒性上超越现有SOTA基线；对非IID引起的漂移有显著缓解。

Conclusion: 从客户端数据分布入手，通过生成式平衡与对齐/正则策略，FBL在不共享原始数据的前提下有效抑制客户端漂移并提升联邦学习性能，具有可扩展性。

Abstract: Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.

</details>


### [240] [Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology](https://arxiv.org/abs/2601.14044)
*Kaiyu Wu,Pucheng Han,Hualong Zhang,Naigeng Wu,Keze Wang*

Main category: cs.CV

TL;DR: 提出WeatherQA基准与LoCo-RFT强化微调框架，解决VLM在气象推理中的自相矛盾问题，推出Weather-R1并显著超越基线与原模型。


<details>
  <summary>Details</summary>
Motivation: VLM在通用任务上推理进步明显，但在高风险的气象领域存在两大缺口：领域迁移差距与推理-答案不一致（Self-Contradictory Reasoning）导致的不忠实推理。需要一个专门基准与训练策略来提升领域适配与推理一致性。

Method: 1) 构建气象多模态推理基准WeatherQA；2) 设计LoCo-RFT，在强化微调中加入“逻辑一致性奖励”以惩罚推理与最终答案矛盾；3) 基于Qwen2.5-VL-32B训练得到具备逻辑忠实性的气象推理模型Weather-R1。

Result: Weather-R1在WeatherQA上较基线提升9.8个百分点，优于SFT与传统RFT，并超过原始Qwen2.5-VL-32B，表明LoCo-RFT与专用数据有效。

Conclusion: 逻辑一致性导向的强化微调能够缓解VLM在气象领域的自相矛盾推理问题；WeatherQA与Weather-R1为该领域提供了可复现基准与强性能模型，验证了逻辑忠实性奖励的有效性。

Abstract: While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.

</details>


### [241] [Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model](https://arxiv.org/abs/2601.14052)
*Haoran Xu,Yanlin Liu,Zizhao Tong,Jiaze Li,Kexue Fu,Yuyang Zhang,Longxiang Gao,Shuaiguang Li,Xingyu Li,Yanran Xu,Changwei Wang*

Main category: cs.CV

TL;DR: 提出MM-OOD：利用多模态大模型（MLLM）的推理与多轮对话能力，改进零样本OOD检测；近OOD直接用MLLM判别，远OOD采用“勾勒-生成-细化”的多模态流程；在Food-101等数据集显著提升并在ImageNet-1K具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本OOD检测多依赖LLM文本知识与CLIP，对图像空间的OOD难点关注不足，导致对近/远分布外样本识别不稳定。需要一种能同时利用图像与文本、多轮推理的方案。

Method: 构建MM-OOD流水线：1）近OOD：将ID图像与文本提示直接输入MLLM，由其多模态理解与推理识别潜在异常；2）远OOD：提出“sketch-generate-elaborate”框架——先用文本勾勒OOD暴露（sketch），再生成对应视觉OOD样本（generate），最后用多模态提示进行细化判别（elaborate）。全流程为训练免或弱依赖训练的零样本设定。

Result: 在Food-101等多模态数据集上获得显著提升，并在ImageNet-1K上验证可扩展性与稳健性，相比现有基线在近/远OOD任务上均有改进。

Conclusion: 多模态推理与多轮对话的结合能有效弥补仅依赖文本知识的不足，MM-OOD在近/远OOD场景均提升检测性能，显示出在大规模数据集上的可扩展潜力。

Abstract: Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.

</details>


### [242] [Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI](https://arxiv.org/abs/2601.14055)
*Andrea Protani,Marc Molina Van Den Bosch,Lorenzo Giusti,Heloisa Barbosa Da Silva,Paolo Cacace,Albert Sund Aillet,Miguel Angel Gonzalez Ballester,Friedhelm Hummel,Luigi Serio*

Main category: cs.CV

TL;DR: 提出SVGFormer：一种无解码器、基于超体素语义图的3D医学影像表示学习框架，以更少参数专注特征编码并具双尺度可解释性；在BraTS上分别用于节点分类与肿瘤占比回归，取得F1=0.875、MAE=0.028。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学影像backbone多采用重参数的编码器-解码器，对稠密体素重建消耗大量参数与计算，削弱了对判别性特征的学习且可解释性不足。作者希望以更轻量、可解释的方式专注特征编码，同时兼顾局部与全局依赖。

Method: 先进行内容感知分组，将体积分割为超体素，构建语义图；在编码阶段采用分层设计：补丁级Transformer学习细粒度区域内特征，超体素级Graph Attention Network建模区域间依赖；整体为无解码器的encoder-only管线，输出节点表征用于不同下游任务（分类或回归）。

Result: 在BraTS数据集上训练两个特化模型：节点级分类与肿瘤占比回归。分类模型F1=0.875，回归模型MAE=0.028，显示所学表征具有判别性与局部化能力。

Conclusion: 基于图的encoder-only范式可在3D医学影像中实现准确且内在可解释的表示学习；通过超体素图与双层编码（补丁Transformer+超体素GAT）集中容量于特征编码，提供从补丁到区域的双尺度可解释性，并在BraTS上验证了有效性。

Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.

</details>


### [243] [POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion](https://arxiv.org/abs/2601.14056)
*Andrea Rigo,Luca Stornaiuolo,Weijie Wang,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出POCI-Diff：在扩散模型中实现可交互、可编辑、与3D布局一致的文本到图像生成，兼顾几何约束与实例级语义绑定，消除基于扭曲/变形的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有提升空间一致性的T2I方法多依赖2D线索或拷贝-变形-粘贴策略，易造成物体几何失真、编辑前后一致性差，难以支持多物体、可交互的3D布局控制与持续编辑。

Method: 提出POCI-Diff：在统一的扩散流程中联合施加3D几何约束与实例级语义绑定。关键点包括：1) Blended Latent Diffusion，将每个文本描述与指定3D包围盒绑定，实现按对象的显式语义控制与一次性多物体场景合成；2) 无变形（warping-free）的生成式编辑流程，通过再生成实现插入/删除/变换对象，避免像素级形变；3) 结合IP-Adapter以参考图像条件化扩散，保持对象外观身份与多轮编辑一致，同时维持全局场景一致性。

Result: 在多项实验中，生成结果在视觉质量与3D布局遵从度方面优于SOTA，并有效消除基于变形的几何伪影；可稳定支持对象插入、删除与变换等交互编辑。

Conclusion: POCI-Diff能在T2I中提供一致、可交互的3D布局控制与编辑，通过联合3D几何约束与实例语义绑定及参考图像条件化，兼顾高保真与编辑一致性，优于现有方法。

Abstract: We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.

</details>


### [244] [Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration](https://arxiv.org/abs/2601.14060)
*Yongcong Ye,Kai Zhang,Yanghai Zhang,Enhong Chen,Longfei Li,Jun Zhou*

Main category: cs.CV

TL;DR: 提出CVSI方法，通过补充视觉-语义信息提升零样本组合图像检索的细粒度能力，三大组件协同显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-CIR方法难以精准表达细粒度修改，常把多模态查询压缩成单一文本或依赖LLM生成描述，导致丢失互补的视觉信息与完整语义上下文，检索效果受限。

Method: CVSI由三部分组成：1) 视觉信息抽取：提取全局图像特征，并通过预训练映射网络将参考图像转为伪token，与修改文本及“可能被新增的对象”合并；2) 语义信息抽取：用预训练图像描述模型为参考图像生成多条caption，再用LLM生成对应的修改后caption与可能新增对象；3) 互补信息检索：将来自查询侧与库内图像的多源视觉-语义表示进行整合匹配，实现在多场景下的鲁棒检索。

Result: 在CIRR、CIRCO、FashionIQ三大公开数据集上进行大量实验，CVSI在各项指标上显著超越现有SOTA。

Conclusion: 多源互补的视觉-语义整合能更好捕捉细粒度修改并提升ZS-CIR检索性能，CVSI验证了该思路的有效性与通用性。

Abstract: Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.

</details>


### [245] [VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences](https://arxiv.org/abs/2601.14066)
*Hendrik Möller,Hanna Schoen,Robert Graf,Matan Atad,Nathan Molinier,Anjany Sekuboyina,Bettina K. Budai,Fabian Bamberg,Steffen Ringhof,Christopher Schlett,Tobias Pischon,Thoralf Niendorf,Josua A. Decker,Marc-André Weber,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke*

Main category: cs.CV

TL;DR: 提出VERIDAH算法，结合多头分类与加权序列预测，实现自动椎体标注并处理胸腰段计数异常，在MRI与CT上显著优于已有方法，且适用于任意视野。


<details>
  <summary>Details</summary>
Motivation: 临床上胸腰交界常被忽视，椎体计数异常（胸椎11/13、腰椎4/6）会影响慢性背痛评估与术前规划；已有深度学习标注方法难以自动识别与标注此类计数异常，亟需能在不同成像与视野下稳健工作的算法。

Method: 提出VERIDAH：以多分类头对椎体级别特征进行并行判别，结合加权的椎体序列预测（序列一致性与解剖先验）整合多头输出，从而在存在计数异常和任意视野条件下实现全脊柱编号；在T2加权TSE矢状位MRI与CT数据上评估。

Result: 在T2w MRI：全椎体正确标注率98.30%（对比基线94.24%，p<0.001）；在CT：99.18%（对比77.26%，p<0.001）。异常识别方面：胸椎计数异常正确标注率T2w为87.80%、CT为96.30%；腰椎计数异常T2w为94.48%、CT为97.22%。可在任意视野图像上工作。

Conclusion: VERIDAH在MRI与CT上显著提升椎体标注准确性，并首次有效自动处理胸腰段计数异常，具有临床报告与手术规划价值；代码与模型已开源（spineps）。

Abstract: The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing "Vertebra Identification with Anomaly Handling" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.

</details>


### [246] [Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management](https://arxiv.org/abs/2601.14069)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 论文提出一种无需标签的无监督视频类增量学习方法，通过深特征提取与渐进式深聚类，实现跨任务学习而不遗忘，在UCF101、HMDB51、Something-Else V2上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖有监督增量学习，需要标签与任务边界，成本高且不现实；因此需要在无标签场景下持续学习新视频类别，同时保持已学知识不遗忘。

Method: 1) 使用深度特征提取网络在每个任务阶段提取代表性视频特征，无需类或任务信息；2) 基于这些特征逐步构建一系列深聚类，以组织数据与形成伪结构；3) 在任务序列中，用前一任务训练后的模型作为当前任务的初始化，以迁移和保留知识。

Result: 在UCF101、HMDB51、Something-to-Something V2三个动作识别数据集上，在忽略监督标签的设定下，方法在各项指标上显著优于其他无监督或弱监督基线。

Conclusion: 简单而有效的无监督视频类增量学习框架，通过特征提取+渐进式深聚类+模型初始化迁移，实现无标签的持续学习并显著提升性能。

Abstract: Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.

</details>


### [247] [VENI: Variational Encoder for Natural Illumination](https://arxiv.org/abs/2601.14079)
*Paul Walker,James A. D. Gardner,Andreea Ardelean,William A. P. Smith,Bernhard Egger*

Main category: cs.CV

TL;DR: 提出一个旋转等变的变分自编码器，直接在球面上建模自然环境光，避免2D投影；通过SO(2)-等变的VN-ViT编码器与条件神经场解码器，获得更平滑且良性的潜变量空间，并在等变全连接层上优于标准Vector Neurons。


<details>
  <summary>Details</summary>
Motivation: 逆向渲染本质上不适定，需要先验约束；现有照明先验方法要么忽略环境光的球面与旋转等变特性，要么缺乏良好潜空间，导致插值不稳与泛化受限。

Method: 设计旋转等变VAE：1) 编码器为Vector Neuron Vision Transformer（VN-ViT），在SO(3)到SO(2)的等变性上降级；2) 引入新型SO(2)-等变全连接层（扩展自Vector Neurons），以保持环境贴图在球面上对绕视轴旋转的等变性；3) 解码器为旋转等变的条件神经场，直接在球面上生成环境照明；整体不依赖2D投影。

Result: 新SO(2)-等变全连接层在SO(2)-等变模型中优于标准Vector Neurons；所提VAE相较以往方法具有更平滑的潜空间插值与更“良性”的潜空间结构（稳定、可解释、连续）。

Conclusion: 在球面上以SO(2)等变性构建的VAE能作为有效照明先验，改善逆向渲染中的照明建模；结构中的SO(2)-等变FC层与旋转等变神经场是关键，使得潜空间平滑且可控。

Abstract: Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.

</details>


### [248] [DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning](https://arxiv.org/abs/2601.14084)
*Abdurrahim Yilmaz,Ozan Erdem,Ece Gokyayla,Ayda Acar,Burc Bugra Dagtas,Dilara Ilhan Erdil,Gulsum Gencoglan,Burak Temelkuran*

Main category: cs.CV

TL;DR: DermaBench 是一个基于 DDI 多样化皮肤科图像的数据集所构建的临床专家标注皮肤科视觉问答（VQA）基准，用于全面评估医学 VLM 在皮肤科中的视觉理解、语言对齐与临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤科评测多集中于图像级分类（如病灶识别），难以衡量多模态模型对细粒度形态学、语言落地与临床推理的能力。因此需要专门的 VQA 基准来测试模型对皮肤图像的解读与临床语义生成。

Method: 基于 DDI 数据集，选取 656 张、覆盖 570 位患者与 Fitzpatrick I–VI 各肤色的临床图像；采用层级化标注方案，设定 22 个主问题（含单选、多选与开放式），由皮肤科专家对每张图像标注诊断、解剖部位、病灶形态、分布、表面特征、颜色、图像质量，并提供开放式叙述与摘要；总计约 14,474 条 VQA 风格标注。数据以仅元数据形式发布以遵循上游许可，并开放于 Harvard Dataverse。

Result: 构建并发布了覆盖多肤色、题型多样、内容细粒度的皮肤科 VQA 基准 DermaBench，提供高质量临床专家标注与丰富问题类型，可系统评估 VLM 在皮肤科场景的理解与推理。

Conclusion: DermaBench 弥补了皮肤科多模态评测的空白，为评估医学 VLM 的视觉—语言—临床推理能力提供可靠基准，并通过元数据发布方式兼顾数据可用性与合规性。

Abstract: Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.

</details>


### [249] [Two-Stream temporal transformer for video action classification](https://arxiv.org/abs/2601.14086)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 提出一种两流Transformer视频分类器，联合RGB内容与光流以建模时空与运动关系，在三个人体活动数据集上取得优异分类表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解需要有效的运动表示；Transformer在多任务中表现出色，但如何同时捕获内容与运动（光流）间的时空关系仍待提升。

Method: 构建两流Transformer：一支处理原始帧（内容/时空），一支处理光流（运动）；在Transformer编码器内通过自注意力跨光流与时间帧联合建模，学习两种模态间的关系特征，用于视频分类。

Result: 在三个知名人体活动视频数据集上，所提方法实现了“优秀”的分类结果（摘要未给出精确数值）。

Conclusion: 融合光流与内容并在Transformer中以自注意力联合建模可有效提升人类活动视频分类性能，验证了所提两流Transformer框架的有效性。

Abstract: Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.

</details>


### [250] [Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition](https://arxiv.org/abs/2601.14101)
*Emily Kim,Allen Wu,Jessica Hodgins*

Main category: cs.CV

TL;DR: 研究通过课程学习策略，在不使用真实航拍数据训练的情况下，提高跨视角（地面到航拍）人体动作识别的泛化与训练效率。结合合成航拍与真实地面数据，并比较两种从合成到真实的课程方案，在REMAG上用SlowFast与MViTv2验证，性能接近简单数据直拼但训练迭代显著减少。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别多基于地面视角，难以泛化到视角差异巨大的航拍域，且缺少真实航拍训练数据。需要一种仅用合成航拍与真实地面数据，也能在真实航拍测试上泛化且高效训练的方法。

Method: 使用两类域外数据：合成航拍与真实地面。设计两种课程学习策略，从合成到真实的迁移路径不同：1）两阶段课程：先在合成航拍上预训练/适配，再直接在真实地面上微调；2）多阶段渐进课程：逐步扩展训练集（分多步增加真实地面比例）后再最终微调。与简单的“数据直拼训练”作对比。评估架构包括SlowFast与MViTv2，在REMAG上测试真实航拍表现。

Result: - 任一单域训练不如合成+真实的组合；组合明显更优。
- 两种课程策略的top-1与简单直拼相当（差距≤3%），但显著减少训练迭代：
  • 两步微调：SlowFast迭代减少至多37%，MViTv2至多30%。
  • 渐进多步：相对两步法，SlowFast再降至多9%，MViTv2再降至多30%。

Conclusion: 课程式从合成航拍过渡到真实地面，再面向真实航拍测试，可在保持接近的准确率下显著提升训练效率；在跨视角动作识别中，课程学习是有效的无真实航拍训练数据的泛化策略。

Abstract: Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.
  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.
  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.

</details>


### [251] [Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing](https://arxiv.org/abs/2601.14103)
*Xiaolu Liu,Yicong Li,Qiyuan He,Jiayin Zhu,Wei Ji,Angela Yao,Jianke Zhu*

Main category: cs.CV

TL;DR: Interp3D 是一个无需训练的三维带纹理变形框架，通过逐步对齐策略在几何与纹理上实现平滑、可信的从源到目标资产过渡，并在新构建的数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有三维变形方法要么只处理几何、忽略纹理，要么将二维插值生搬硬套到三维，导致语义歧义、结构错位和纹理模糊。因此需要一种同时保持几何一致性、纹理对齐与稳健性的变形方案。

Method: 提出 Interp3D：利用生成先验并遵循“渐进对齐”原则。流程包括三步：1) 在条件空间进行语义对齐的插值；2) 通过 SLAT（Structured Latent）引导的结构插值以保证结构一致；3) 通过细粒度纹理融合转移外观细节。整个框架无需训练。

Result: 构建了含难度分级的 Interp3DData 数据集，从保真度、过渡平滑性、可信度等维度评估。定量指标与用户研究均显示 Interp3D 明显优于既有方法。

Conclusion: 渐进对齐与生成先验的结合可在三维形状与纹理两方面实现高质量变形，提供平滑、语义一致的过渡；方法通用、稳健并可直接应用于动画与内容创作。

Abstract: Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.

</details>


### [252] [PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning](https://arxiv.org/abs/2601.14111)
*Jiaying Wu,Can Gao,Jinglu Hu,Hui Li,Xiaofeng Cao,Jingcai Guo*

Main category: cs.CV

TL;DR: 提出PMCE：一种利用多粒度语义与字幕引导增强的概率式小样本学习框架，通过从基类检索语义先验并与支持集原型做MAP融合，同时用冻结的图像描述器+轻量增强器优化支持与查询表示，显著提升少样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 小样本学习中因样本稀缺，直接由支持集估计的原型偏差大、泛化差，现有语义方法多只在支持侧加入粗粒度类别语义，忽略了查询表示的改进；缺乏同时利用类级与实例级语义的信息融合与稳健估计机制。

Method: 1) 构建非参数知识库：为每个基类存储视觉统计量与CLIP编码的类名嵌入；2) 元测试时依据类名嵌入相似度检索与新类别最相关的基类，将其视觉统计聚合为类别特定先验；3) 与支持集原型通过简单的MAP更新融合，得到校准后的原型；4) 使用冻结的BLIP生成无标签实例级图像描述，并训练轻量增强器（在基类上训练，测试时归纳式应用）同时优化支持原型与查询特征；5) 引入一致性正则抑制噪声字幕带来的不稳定。

Result: 在四个基准上均优于强基线；在MiniImageNet的1-shot设定上，相比最强语义方法获得最高达7.71%的绝对提升。

Conclusion: 结合多粒度语义（类级先验+实例级字幕）与概率式MAP融合，可有效校准原型并改进查询表示，显著提升小样本分类泛化能力；方法简单、可与现有语义与原型范式兼容。

Abstract: Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D

</details>


### [253] [The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning](https://arxiv.org/abs/2601.14127)
*Renmiao Chen,Yida Lu,Shiyao Cui,Xuan Ouyang,Victor Shea-Jay Huang,Shumin Zhang,Chengwei Pan,Han Qiu,Minlie Huang*

Main category: cs.CV

TL;DR: 提出MIR-SafetyBench，多图推理安全基准；评测19个MLLM显示：多图推理越强，越易被攻破；“表面安全”回复普遍、注意力熵低与不安全相关；代码数据已开源。


<details>
  <summary>Details</summary>
Motivation: MLLM逐步具备处理多图复杂指令的推理能力，但这也可能带来新的安全风险；现有安全评测多聚焦单图或文本，缺乏系统化的多图推理安全基准。

Method: 构建MIR-SafetyBench，覆盖9类多图关系、共2,676个实例；在19个主流MLLM上系统评测攻击成功率与答复质量；分析被判定安全与不安全回复特征，并比较其注意力熵差异。

Result: 发现多图推理能力更强的模型在该基准上更脆弱；大量“安全”判定的回复实则流于表面（误解、回避、模棱两可）；不安全生成的注意力熵平均更低，显示模型在求解任务时可能过度聚焦而忽略安全约束。

Conclusion: 多图推理强化可能带来安全脆弱性提升；当前安全评估易被“表面安全”掩盖；注意力熵可作为内部风险信号。建议发展面向多图推理的更强安全对齐与评测方法。

Abstract: As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.

</details>


### [254] [GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression](https://arxiv.org/abs/2601.14130)
*Till Aczel,David F. Jenny,Simon Bührer,Andreas Plesner,Antonio Di Maio,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 本文提出GIC-DLC：用可微分逻辑电路训练查找表的灰度图像压缩框架，兼顾神经压缩的灵活性与布尔运算的硬件效率，在灰度基准上比传统编解码器更高效，并显著降低能耗与延迟。


<details>
  <summary>Details</summary>
Motivation: 神经图像编解码能达到更高压缩率，但计算/能耗开销大，不利于在手机、相机、无人机等能量受限设备部署。需要一种既保留学习型方法压缩效率，又具备硬件友好、低能耗低延迟的方案。

Method: 设计“可微分逻辑电路”范式：以查找表（LUT）为基本单元，用可微方法训练其参数，使其在推理时仅需布尔运算/查表。构建面向灰度图的端到端压缩编解码器（GIC-DLC），在训练时优化码率-失真目标与硬件代价（能耗/延迟）相关的约束，实现硬件感知学习。

Result: 在灰度图基准数据集上，GIC-DLC在压缩效率上优于PNG、JPEG-XL等传统编解码器，同时在推理侧显著降低能耗与延迟（相较神经编解码器和传统算法均具优势）。

Conclusion: 学习型压缩可通过LUT+布尔操作的可微逻辑电路实现硬件友好化，在边缘设备上实现高压缩率、低能耗、低时延的图像压缩，指向面向低功耗场景的可部署学习型编解码新方向。

Abstract: Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.

</details>


### [255] [LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery](https://arxiv.org/abs/2601.14154)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur,Venu Govindaraju,Kenneth Seastedt*

Main category: cs.CV

TL;DR: 提出MIRACLE深度学习架构，用临床+影像多模态融合预测肺癌术后并发症，基于超球嵌入与可干预模块，在3,094例真实队列上优于传统ML与LLM变体，并提供可解释、可操作的风险管理。


<details>
  <summary>Details</summary>
Motivation: 术后并发症严重影响预后并推高成本，现有模型对异构临床与影像融合、可解释性与临床可操作性不足，亟需既准确又可交互的预测工具。

Method: 构建MIRACLE：将结构化临床数据与高维影像通过“超球嵌入空间”进行特征对齐与融合；加入可干预深度学习模块，使模型在预测同时生成可解释因子与可操作建议，允许专家交互调整；在POC-L真实队列（3,094例肺癌手术患者）上进行验证，并与传统机器学习和LLM变体对比。

Result: 在POC-L数据集上，MIRACLE显著优于多种传统机器学习模型与单独的LLM变体，提供更个体化、可解释的并发症风险预测（文摘未给出具体数值）。

Conclusion: 多模态超球融合+可干预模块能提升肺癌术后并发症风险预测的准确性与可解释性，助力临床个体化风险管理，具有实际应用潜力。

Abstract: Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.

</details>


### [256] [One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion](https://arxiv.org/abs/2601.14161)
*Yitong Dong,Qi Zhang,Minchao Jiang,Zhiqiang Wu,Qingnan Fan,Ying Feng,Huaqi Zhang,Hujun Bao,Guofeng Zhang*

Main category: cs.CV

TL;DR: 提出一种结合ViT几何主干与扩散式细节增强的稀视角高保真新视角合成框架，通过双域细节感知和统一训练，在保持3D一致性的同时处理高分辨率并恢复高频细节，跨数据集效果更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于ViT的3DGS前馈管线虽然几何先验强，但受算力限制只能用低分辨率输入，细节受损；而生成式细化多为3D无关，跨视角不一致，尤其在未见区域结构崩塌。

Method: 1) 设计双域细节感知模块（Dual-Domain Detail Perception），绕开ViT对分辨率的限制，并为高斯体赋予额外特征以存储高频；2) 构建特征引导的扩散网络，在恢复/增强过程中保留高频细节；3) 统一训练策略，联合优化ViT几何主干与扩散式细化模块，实现端到端协同。

Result: 在多数据集上实现更高的生成质量与跨视角一致性，尤其在高分辨率与未见区域的结构与细节保持方面优于现有方法。

Conclusion: 通过双域细节建模与扩散细化的联合训练，突破ViT解析度瓶颈与3D无关增强的限制，实现稀视角条件下的高保真、3D一致的新视角合成。

Abstract: We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.

</details>


### [257] [ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction](https://arxiv.org/abs/2601.14165)
*Zhenghong Li,Wensheng Cheng,Congwu Du,Yingtian Pan,Zhaozheng Yin,Haibin Ling*

Main category: cs.CV

TL;DR: 提出ASBA网络，从高度稀疏采样的A-scan重建高保真ODT血流B-scan，通过A线ROI状态空间建模与B线相位注意力，并以流动感知加权损失强化血流重建，在动物实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统ODT为保证B-scan质量需密集采样，导致扫描耗时、存储压力大且难以捕捉快速血流。现有稀疏采样方法采样率保守且将血流与背景信号同质化建模，限制了效果。

Method: 设计血流感知的ASBA网络：1) A-line ROI状态空间模型，沿深度方向稀疏提取血流特征；2) B-line相位注意力，基于相位差捕获沿横向的长程血流关联；3) 流动感知加权损失，提升对血流区域的重建优先级。输入为高度稀疏A-scan，输出为重建的ODT B-scan。

Result: 在真实动物数据上进行大量实验，ASBA在图像质量和血流细节恢复上显著优于现有最先进稀疏重建方法。

Conclusion: 通过结合A线状态空间建模、B线相位注意力与流动加权损失，ASBA能从高稀疏采样中高保真恢复ODT血流B-scan，缓解采样与速度/存储矛盾并更好捕捉快速血流动态。

Abstract: Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.

</details>


### [258] [Progressive self-supervised blind-spot denoising method for LDCT denoising](https://arxiv.org/abs/2601.14180)
*Yichao Liu,Yueyang Teng,Junwen Guo*

Main category: cs.CV

TL;DR: 提出一种仅用低剂量CT(LDCT)进行自监督去噪的新策略：逐步盲点学习+高斯噪声正则，在Mayo数据集上优于现有自监督方法，并可媲美甚至超越部分监督方法。


<details>
  <summary>Details</summary>
Motivation: 获取成对的正常剂量(NDCT)/低剂量(LDCT)训练数据在临床上困难，限制了监督式去噪；需要仅依赖LDCT的自监督方案，同时提升细粒度去噪能力并避免过拟合。

Method: 1) 设计逐步(step-wise)盲点去噪机制：通过分阶段强制条件独立(隐藏目标像素信息)，逐层/逐步学习更细粒度的噪声-信号分离；2) 在LDCT上注入高斯噪声作为数据扰动与正则，缓解模型对原始噪声模式的过拟合；3) 仅用LDCT训练，无需NDCT配对。

Result: 在Mayo LDCT数据集上，所提方法在多项指标上持续优于现有自监督方法，且性能与多种代表性监督去噪方法相当或更优。

Conclusion: 逐步盲点与噪声注入的自监督训练可在无配对NDCT条件下实现高质量LDCT去噪，兼具鲁棒性与泛化能力，为临床低剂量重建提供可行替代。

Abstract: Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.

</details>


### [259] [IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models](https://arxiv.org/abs/2601.14188)
*Liang Shi,Wei Li,Kevin M Beussman,Lin Chen,Yun Fu*

Main category: cs.CV

TL;DR: 论文提出IIR-VLM，通过引入预训练的实例级识别(ILR)专家编码器，让通用VLM具备一/少样本的上下文实例识别与实例感知理解能力，并在多类实例(人、脸、宠物、通用物体)基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型(VLM)在实例级识别(如行人重识别)上明显不如专用ILR模型，难以在需要“认出具体人/物”的实际场景中发挥作用；而传统个性化/实例学习多依赖逐实例标注与训练，成本高且细粒度区分困难。

Method: 提出IIR-VLM：在VLM中集成预训练ILR专家作为辅助视觉编码器，输出专门的实例判别特征；利用这些特征进行上下文(提示示例)的一次学习(one-shot)以识别新实例，并将实例知识注入到下游视觉理解任务中，实现实例感知；在现有个性化基准和新构建的跨难度、跨品类ILR基准上进行评测。

Result: 与通用VLM及现有个性化方案相比，IIR-VLM在一/少样本的实例识别上取得显著提升；在人员、面部、宠物及通用物体等多类别上均表现更好；在新提出的更具挑战的基准上也达成领先性能。

Conclusion: 通过把ILR专家编码器融入VLM并进行上下文一/少样本学习，可显著增强VLM的实例级识别与实例感知能力，降低数据与训练成本，并在多类型实例与多难度设定下表现出色。

Abstract: Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.

</details>


### [260] [Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting](https://arxiv.org/abs/2601.14208)
*Nitin Kulkarni,Akhil Devarashetti,Charlie Cluss,Livio Forte,Dan Buckmaster,Philip Schneider,Chunming Qiao,Alina Vereshchaka*

Main category: cs.CV

TL;DR: 提出一个基于三摄像机的端到端系统，从车辆驶过装置时采集底盘视频，构建可交互的3D底盘模型，显著提升检查效率与线上信任。核心是一套“机架感知”的SfM管线，解决广角畸变与低视差问题，并以高质量稀疏点云驱动高斯splating实现实时渲染。


<details>
  <summary>Details</summary>
Motivation: 传统二手车底盘检查需人员弯腰/钻入车底，费时且有安全隐患；线上交易缺少底盘可视化证据，影响买家信心。需要一种安全、高效、可规模化且能远程共享的底盘三维重建方案。

Method: 搭建三摄同步视频采集的固定机架；精确标定与时间同步；引入机架几何先验并在SfM中显式利用；采用受约束的匹配策略结合学习特征DISK与注意力匹配器LightGlue，生成高质量稀疏点云；以此作为初始，进行Gaussian Splatting得到可实时渲染的逼真3D模型；进行消融验证各组件贡献。

Result: 相较标准SfM，在广角畸变与低视差场景下仍可稳定产生高质量稀疏点云；生成的底盘3D模型具备可旋转、缩放、切片等交互能力并可实时渲染；实验与消融显示所引入的标定、同步、几何先验与学习型匹配器是达到SOTA质量的关键。

Conclusion: 机架感知的SfM结合受约束的学习匹配与Gaussian Splatting，可在困难的车底场景中实现高质量、可实时的三维重建，提升检修安全与效率，并增强线上购车信任，具有实际部署价值。

Abstract: Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.

</details>


### [261] [Soft Tail-dropping for Adaptive Visual Tokenization](https://arxiv.org/abs/2601.14246)
*Zeyuan Chen,Kai Zhang,Zhuowen Tu,Yuanjun Xiong*

Main category: cs.CV

TL;DR: STAT是一种长度自适应的一维离散视觉分词器：根据图像复杂度动态决定输出token数量，并与因果自回归生成模型自然兼容，在ImageNet上实现与或优于其他概率模型的生成质量并展现良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 以往视觉自回归(AR)生成在标记效率与可扩展性上受限：固定长度token既浪费简单图像的码长，又不足以刻画复杂图像细节，且与1D因果AR的序列建模负担不匹配。需要一种既保留细节又能按图像复杂度自适应长度的表示方式，提升AR视觉生成的质量与可扩展性。

Method: 提出Soft Tail-dropping Adaptive Tokenizer (STAT)：将图像编码为离散码序列并为每个token输出保留概率；除标准自编码器损失外，额外正则化这些保留概率沿序列单调递减，并将其分布与图像级复杂度度量对齐；推理时据此产生长度自适应的1D视觉token，直接供因果1D AR模型建模。

Result: 在ImageNet-1k上，用STAT生成的自适应token驱动的朴素因果AR模型在视觉生成质量上达到有竞争力或更优于其他概率模型家族的水平，并呈现此前在朴素AR尝试中难以获得的良好缩放行为。

Conclusion: 通过在分词阶段引入基于复杂度的软尾部丢弃与单调约束，STAT实现了与1D因果AR高度兼容的长度自适应视觉表示，从而提升AR视觉生成质量与可扩展性，为AR路线在高分辨率图像生成上提供了可行路径。

Abstract: We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.

</details>


### [262] [OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer](https://arxiv.org/abs/2601.14250)
*Pengze Zhang,Yanze Wu,Mengtian Li,Xu Bai,Songtao Zhao,Fulong Ye,Chong Mou,Xinghui Li,Zhuowei Chen,Qian He,Mingyuan Gao*

Main category: cs.CV

TL;DR: OmniTransfer 提出一个统一的时空视频迁移框架，利用多视角跨帧信息和时间线索，在外观一致性与细粒度时间控制上显著提升，覆盖ID/风格/相机运动/视频特效/动作等多类视频转移任务，并在无需姿态的前提下达到与姿态引导方法相当的动作迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频定制多依赖参考图像或任务特定的时间先验，难以充分利用视频本身的丰富时空信息，导致灵活性与泛化不足。需要一个能统一多种视频转移任务、兼顾外观一致性与时间可控性的通用方法。

Method: 提出OmniTransfer，核心包括三点：1) Task-aware Positional Bias（任务感知位置偏置），自适应利用参考视频信息以提升时间对齐或外观一致性；2) Reference-decoupled Causal Learning（参考解耦的因果学习），将参考与目标分支解耦，实现精确而高效的参考迁移；3) Task-adaptive Multimodal Alignment（任务自适应多模态对齐），借助多模态语义引导动态区分并处理不同任务。同时利用跨帧多视角信息与时间线索实现细粒度时序控制。

Result: 在外观（ID与风格）与时间迁移（相机运动与视频特效）上优于已有方法；在动作迁移上无需姿态也能达到与姿态引导方法相当的效果，展现高保真与高灵活度。

Conclusion: OmniTransfer建立了统一的时空视频转移范式，充分挖掘跨帧与时间信息，实现对多类任务的灵活高保真视频生成，并提高了效率与泛化能力。

Abstract: Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.

</details>


### [263] [LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR](https://arxiv.org/abs/2601.14251)
*Said Taghadouini,Adrien Cavaillès,Baptiste Aubertin*

Main category: cs.CV

TL;DR: LightOnOCR-2-1B 是一个10亿参数的端到端多语种视觉-语言模型，将文档图像直接转为有序文本，并可输出嵌入图片的归一化框；在更小更快的前提下达成SOTA，并开源模型、数据与评测。


<details>
  <summary>Details</summary>
Motivation: 传统OCR流水线脆弱、对扫描件/多语种/复杂版面（如科学PDF）表现不稳；现有端到端模型体量大、推理慢且定位能力不足，因此需要一个更小更快、覆盖扫描与法语及科学文档、并具备图像区域定位能力的模型与训练方案。

Method: 构建1B参数的端到端VLM；使用覆盖扫描件、法语文档、科学PDF的大规模高质量蒸馏训练混合；输出扩展为文本+嵌入图片的归一化边界框；预训练中通过“resume”策略引入定位任务，再用基于IoU奖励的RLVR精炼；通过checkpoint averaging与task-arithmetic模型合并提升鲁棒性。

Result: 在OlmOCR-Bench上取得SOTA，同时比此前最佳模型小9倍且推理更快；成功输出图片框定位。

Conclusion: LightOnOCR-2-1B在准确性、效率和多功能（文本与定位）方面实现平衡，验证了蒸馏数据配方、定位训练策略与鲁棒性增强方法的有效性；模型与数据与新基准（bbox-bench）均已按各自许可开放。

Abstract: We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.

</details>


### [264] [Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis](https://arxiv.org/abs/2601.14253)
*Hongyuan Chen,Xingyu Chen,Youjia Zhang,Zexiang Xu,Anpei Chen*

Main category: cs.CV

TL;DR: 提出 Motion 3-to-4：从单目视频与可选参考网格生成高质量4D动态对象的前馈式框架；将任务分解为静态形状生成与运动重建，学习紧凑运动潜变量并预测逐帧顶点轨迹，借助可扩展逐帧Transformer适配不同序列长度；在基准与新高精度数据集上优于现有方法，具更高保真度与时空一致性。


<details>
  <summary>Details</summary>
Motivation: 4D（随时间变化的3D）生成仍受限于训练数据稀缺与从单目视频反演几何与运动的歧义；尽管2D/视频/3D生成进展迅速，但4D仍难以稳定重建完整几何与时序一致的运动。作者希望借助分解式建模与参考网格缓解歧义，提升可扩展性与泛化。

Method: 将4D合成为两步：1) 静态3D形状（使用规范参考网格）与2) 运动重建。模型学习紧凑的运动潜在表示，预测逐帧顶点轨迹以恢复完整且时序一致的几何。引入可扩展的逐帧Transformer以适配不同视频长度并增强鲁棒性。支持从单目视频输入，且可选使用3D参考网格作为规范形状。

Result: 在标准基准与新构建、带精确真值几何的数据集上评测，方法在保真度与空间一致性上优于以往工作。

Conclusion: 分解式前馈框架（静态形状+运动潜变量+逐帧Transformer）有效缓解单目4D重建歧义，能从单目视频稳定生成高质量、时序连贯的4D动态对象，并在多数据集上取得SOTA性能。

Abstract: We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.

</details>


### [265] [VideoMaMa: Mask-Guided Video Matting via Generative Prior](https://arxiv.org/abs/2601.14255)
*Sangbeom Lim,Seoung Wug Oh,Jiahui Huang,Heeji Yoon,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: 提出VideoMaMa：把粗分割掩码转成精确alpha matte，依托预训练视频扩散模型，零样本泛化强；并据此构建大规模伪标注数据集MA‑V（5万+真实视频），再微调SAM2得到SAM2‑Matte，在野外视频更稳健。


<details>
  <summary>Details</summary>
Motivation: 真实世界视频抠像标注稀缺，使视频抠像模型难以泛化。需要一种既能在无大量真标注下获得高质量matte、又能扩展到大规模数据的方案。

Method: 1）提出VideoMaMa：以粗分割mask为先验，借助预训练视频扩散模型的生成先验，将mask细化为像素级alpha matte，实现零样本泛化；2）基于VideoMaMa搭建可扩展的伪标注流水线，对海量真实视频生成高质量matte标注，构建MA‑V数据集（>50K、多场景多运动）；3）用MA‑V微调现有分割/抠像基础模型（如SAM2），得到SAM2‑Matte。

Result: VideoMaMa在仅用合成数据训练的前提下，对真实视频实现强零样本泛化；MA‑V提供大规模高质伪标注；在MA‑V上微调的SAM2‑Matte较在既有数据集上训练的同模型，在野外视频鲁棒性更好、性能更优。

Conclusion: 利用生成式先验（视频扩散模型）与易得的分割线索（粗mask）可实现可扩展的视频抠像：既提升零样本表现，又能通过大规模伪标注推动下游模型（如SAM2）在真实场景中的鲁棒性与SOTA表现。

Abstract: Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.

</details>


### [266] [Implicit Neural Representation Facilitates Unified Universal Vision Encoding](https://arxiv.org/abs/2601.14256)
*Matthew Gwilliam,Xiao Wang,Xuefeng Hu,Zhenheng Yang*

Main category: cs.CV

TL;DR: 论文提出一个统一模型，同时学习对识别与生成都有用的图像表征：将图像映射成用于隐式神经表示（INR）重建的超网络权重，并结合蒸馏，得到极小且通用的嵌入，兼具SOTA表征性能与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多二分：对比学习擅长识别任务的判别性嵌入，而重建/生成模型学习适合生成的潜空间，二者难以兼得。作者希望用一个模型同时获得对下游识别有效的表征与可高质量生成的潜空间，并实现高压缩率的“微型嵌入”。

Method: 将模型设计为INR超网络：输入图像→超网络输出一个小型INR的权重→该INR可快速高精度重建图像。训练时以重建损失（像素/感知/可能对抗）驱动，同时引入知识蒸馏（从强表征模型蒸馏）以提升泛化与判别性。由此学习到高质量、压缩的通用嵌入。

Result: 模型在多种视觉任务上以极小嵌入达到与SOTA相竞争的表征学习效果，同时具备生成（重建）能力，实现高质量、快速的图像重建与显著压缩（“tiny embeddings”）。

Conclusion: 通过INR超网络+蒸馏的联合训练，可在单一框架下统一识别与生成目标，得到紧凑而强大的图像表示；在表征学习上与SOTA竞争，并提供高质量的生成能力与高压缩潜码。

Abstract: Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.

</details>
