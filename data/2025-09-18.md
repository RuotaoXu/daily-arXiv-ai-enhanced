<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 提出一种基于证据检索的、按实例自适应的不确定性感知决策机制，以每例融合的“信念”替代固定全局阈值；在CIFAR-10/100上较熵阈值法更可靠、可解释且审阅负担可控。


<details>
  <summary>Details</summary>
Motivation: 固定的全局不确定性阈值（如预测熵阈）在不同样本、语境下不稳健，易产生“自信但错误”的决定且可解释性弱。需要一种可按实例调节、透明可审计的机制，以提升不确定性下的运营决策可靠性。

Method: 对每个测试样本，在嵌入空间检索邻近样本作为证据；将这些证据的预测分布通过Dempster-Shafer(DS)证据理论进行融合，得到该样本的融合信念；以该融合信念作为该样本专属的阈值/门限，用于决定接受、拒绝或送审。骨干采用BiT与ViT；比较与固定预测熵阈方法。

Result: 在CIFAR-10/100上，提出方法取得更高或相当的不确定性感知性能，显著减少“自信但错误”的结果，并保持可持续的人工审阅量。只需少量证据即可获得主要收益，增加证据数量带来边际提升有限。

Conclusion: 基于证据条件化的按例标注/阈值策略，相较固定熵阈，更可靠、可解释，适合实际不确定性感知决策；透明可审计且对证据数量不敏感（少量足够）。

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [2] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 对比三种数据集上混合量子-经典神经网络与纯经典CNN：混合模型在准确率、训练速度、参数规模与资源占用上总体占优，简单数据集上更稳健，复杂数据集对抗鲁棒性相近。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习声称在表达能力与效率上具潜在优势，但缺乏系统性实证对比。本文旨在用标准视觉基准与统一训练设置，量化混合量子-经典模型在性能、效率与鲁棒性方面相对经典CNN的真实收益与局限。

Method: 将参数化量子线路（PQC）嵌入深度学习架构形成混合模型，并以等价容量的经典CNN作对照。在MNIST、CIFAR100、STL10上训练50轮，评估验证/测试准确率、训练时间、参数数量、资源使用（内存与CPU）、以及在ε=0.1下的对抗鲁棒性。

Result: 混合模型验证准确率优于经典：MNIST 99.38% vs 98.21%，CIFAR100 41.69% vs 32.25%，STL10 74.05% vs 63.76%，优势在复杂数据集更明显（+9.44%、+10.29%）。训练更快（5–12×，如每轮21.23s vs 108.44s）、参数更少（少6–32%）、内存更低（4–5GB vs 5–6GB）、CPU占用更低（9.5% vs 23.2%）。对抗鲁棒性：在MNIST显著更稳健（45.27% vs 10.80%），在CIFAR100均脆弱（~1%）。对测试集泛化优于经典。

Conclusion: 混合量子-经典架构在准确率、训练效率与参数可扩展性方面具实际优势，尤其在更复杂视觉任务上更显著；然而在高复杂度数据上的对抗鲁棒性仍未改善，提示鲁棒性仍是关键待解问题。

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [3] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: 论文提出一体化“感知-预警”框架，针对高速公路拥堵的检测与预测：改进YOLO与DeepSort提升遮挡下的车辆感知准确性，并用GRU+注意力进行拥堵提前预警，实验在实际视频中获得显著提升与高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有高速拥堵“检测-预测”系统存在两大短板：1) 车辆在遮挡场景下感知精度低、跟踪易丢失与ID切换多；2) 拥堵预测易丢失长序列时序依赖，难以及时准确预警。需要一个将高精度感知与时序预测紧耦合的框架，为拥堵管控提供量化支撑。

Method: 感知层面：将YOLOv11的GIoU Loss替换为DIoU Loss，形成YOLOv11-DIoU以改善定位与遮挡下的收敛；在跟踪端改进DeepSort，将马氏距离（运动）与余弦距离（外观）融合以更稳健关联目标。交通流建模：在10–15 veh/km高密度工况采用Greenberg模型，验证速密关系。预警层面：构建GRU-Attention模型，输入流量、密度、速度，训练300轮以捕捉拥堵先行特征，进行10分钟提前对30分钟拥堵事件的预警。

Result: YOLOv11-DIoU在长深高速视频上mAP 95.7%，较基线+6.5个百分点，遮挡漏检率5.3%；改进DeepSort MOTA 93.8%，较SORT+11.3个百分点，ID切换仅4次。Greenberg模型下速-密强负相关r=-0.97符合理论。GRU-Attention测试集准确率99.7%，较传统GRU高7–9个百分点；对30分钟拥堵实现10分钟前预警，时间误差≤1分钟；独立视频验证预警准确率95%，拥堵空间重叠>90%，在高流量(>5车/秒)下仍稳定。

Conclusion: 集成的视觉感知与时序预警框架在真实高速视频中显著提升检测与跟踪鲁棒性，并实现高精度、低时延的拥堵提前预警，为高速拥堵治理提供可量化支持，具备智能交通应用潜力。

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [4] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: 研究利用车载超声波众包数据，借助卷积神经网络自动化地面真实测试分析流程，大幅减人力并提升实时路侧停车服务质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于云的路侧停车服务需依赖人工进行地面真实（ground truth）测试与数据标注，成本高、效率低，限制了实时可用车位检测与服务质量的提升，因此需要用机器学习实现测试与分析流程的自动化与数据集扩充。

Method: 在众包车队的超声波检测数据与相关影像/图像上应用机器学习，特别是卷积神经网络的图像模式识别方法，设计并实现自动化的数据处理、分类与验证管线，以替代人工在主要分析环节中的工作；并使用预定义评测指标对系统性能与效率进行量化评估。

Result: 自动化分析工具显著降低了人工投入，按预设指标评估实现了最高达99.58%的人工时间缩减，同时维持/提升了对可用停车位检测与分类的准确性（文中以指标呈现）。

Conclusion: 通过CNN驱动的分析自动化，可有效丰富数据库、替代人工测试流程中的关键环节，显著提升实时路侧停车服务的效率与可扩展性；文章总结了改进点并展望了后续开发与在更广泛场景中的应用潜力。

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [5] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: 论文系统性分析了基于视觉-语言模型（如CLIP）的零样本分布外（OOD）检测：解释其机制、相对单模态方法的优势，以及对噪声与提示词表述的鲁棒性差异。


<details>
  <summary>Details</summary>
Motivation: VLM 被发现具有强大的零样本 OOD 检测能力，但社区对其为何有效、相较单模态的具体优势、以及在不同扰动下的行为鲁棒性仍缺乏系统理解，影响可靠 AI 的落地。

Method: 以ID与OOD提示词驱动的VLM零样本检测框架为对象，构建并形式化VLM嵌入空间中支持OOD检测的关键性质；在标准基线下与单模态方法进行对比实证；系统评估对图像噪声与提示词改写的敏感性以揭示鲁棒性画像与不对称性。

Result: (1) 总结并形式化了VLM嵌入空间促成零样本OOD检测的操作性属性；(2) 量化显示VLM较单模态基线有显著优势，归因于其可利用更丰富的语义新颖性；(3) 发现鲁棒性不对称：对常见图像噪声较稳健，但对提示词措辞极为敏感。

Conclusion: VLM在OOD检测上确有结构性优势，但存在对提示工程高度敏感的关键脆弱点。论文为未来更稳健的VLM OOD检测设计提供了经验证据与实践指引。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [6] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 提出基于“截面曲率”抽象概念的曲率画像方法，用于离散度量空间的几何刻画，并据此定义量化指标评估数据表征与降维效果；实验表明可估计数据内在维度，分析大规模经验网络并比较降维方法。


<details>
  <summary>Details</summary>
Motivation: 传统降维或数据表示方法缺少与底层几何（尤其是曲率/局部-全局度量关系）一致的量化评估手段；需要一种能在离散度量空间上刻画几何特性的通用工具，用以衡量表示质量、理解数据的内在结构与维度。

Method: 基于近期发展的离散“截面曲率”概念（考察三点与其它点之间的度量关系），为离散度量空间构建曲率几何画像；在此基础上定义一个定量指标，用该曲率画像评价数据表示（特别是降维结果）的有效性；将该分析框架应用到数据集与经验网络上，并利用曲率画像估计内在维度。

Result: 曲率画像与所定义的指标能有效区分不同数据表示/降维结果的质量；曲率分析可用于较准确地估计数据集的内在维度；在经验网络的宏观几何分析中，能揭示其大尺度几何特征。

Conclusion: 曲率为核心的几何画像为离散度量空间提供了统一刻画框架，并给出评估数据表示和降维效果的可操作量化指标；该方法还能用于内在维度估计与网络大尺度几何分析，展现出广泛应用潜力。

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [7] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: 研究以斐济纳迪为案例，用遥感与机器学习对2013–2024年土地利用/覆被变化进行制图与变化检测，强调城市扩张监测。方法结合Landsat-8、GEE上的k-means无监督分割与CNN监督分类，并可视化城市区域变化，为规划与管理提供技术支持。


<details>
  <summary>Details</summary>
Motivation: 斐济快速城市化带来住房、道路与市政建设扩张，亟需客观、可扩展的技术来量化和监测土地利用/覆被（LULC）变化，以支持城市规划、资源管理与环境评估。

Method: - 数据：Landsat-8影像（2013–2024），目标区为纳迪。
- 标注：构建带标签的训练集用于监督学习。
- 平台与算法：在Google Earth Engine上进行预处理与无监督k-means聚类生成初始地表覆被图；随后采用卷积神经网络（CNN）对选定区域进行监督分类。
- 输出：生成多期LULC图并进行变化检测与可视化，重点突出城市区域扩张。

Result: 得到2013–2024年的多时相LULC分类图与变化检测可视化，显示城市区域在研究期内显著扩张（具体定量指标未在摘要中给出）。

Conclusion: 所提遥感+机器学习框架能有效用于斐济纳迪的LULC制图与变化监测，为未来的土地利用建模、变化检测以及城市规划决策提供技术支撑。

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [8] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: 提出一个三阶段实时外破（FOI）检测与跟踪框架：YOLOv7分割用于快速定位；ConvNeXt+triplet loss学到判别特征；特征辅助IoU跟踪增强遮挡与运动鲁棒性；支持混合精度在Jetson等边缘设备高效部署，新增目标仅需加入特征库无需重训；在监控与无人机数据上表现准确稳健，并通过Jetson基准验证可用与可扩展。


<details>
  <summary>Details</summary>
Motivation: 电力输电线路受外来物入侵（风筝、气球、鸟类、无人机等）会引发安全风险。现有方法在复杂场景下的实时性、遮挡鲁棒性、跨设备部署与可维护性（无需频繁重训）不足，亟需一个兼顾准确、鲁棒、低成本边缘部署且可增量更新的解决方案。

Method: 三阶段流水线：1）检测/分割：采用YOLOv7-seg实现快速稳健的目标定位与轮廓获取；2）表征学习：以ConvNeXt为骨干，使用triplet loss训练特征提取器，输出判别性嵌入；3）跟踪：基于IoU的多目标跟踪器融合外观特征（嵌入）进行数据关联，提升遮挡与剧烈运动下的鲁棒性。工程上，使用混合精度推理优化至低成本Jetson等边缘硬件；并通过特征库实现增量更新，无需重新训练模型。

Result: 在真实监控与无人机视频数据集上，框架在准确率与鲁棒性上表现优异，涵盖多样FOI场景；在NVIDIA Jetson设备上的硬件基准显示该系统满足实时性与可扩展性需求。

Conclusion: 该方法在复杂场景中实现了实时、鲁棒的FOI检测与多目标跟踪，并且便于在低成本边缘设备部署与维护；通过增量式特征库更新避免重训，具有良好的实用性与可扩展性。

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [9] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: 提出EdiVal-Agent与EdiVal-Bench，用面向对象的多回合图像编辑自动评价框架，结合VLM+开放词汇检测、语义特征与人偏好模型，较仅用VLM/CLIP更贴近人类判断，并可扩展集成新工具；据此构建覆盖9类指令、11种SOTA模型的基准，揭示失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有指令式图像编辑评价受限：要么依赖成对参考图像，覆盖有限且继承生成模型偏差；要么仅靠零样本VLM，基于提示的跟随度/一致性/质量评估不稳定不精确。需要可扩展、细粒度、可解释且更接近人类判断的自动评价方案。

Method: 提出EdiVal-Agent：1) 对输入图像进行对象级分解；2) 自动合成多样且具上下文的编辑指令；3) 评价环节模块化融合：a) 用VLM+开放词汇目标检测评估指令跟随；b) 用语义级特征抽取评估内容一致性；c) 用人类偏好模型评估视觉质量；并支持多回合编辑与未来工具无缝集成。基于该流程构建EdiVal-Bench，覆盖多范式编辑模型（AR、flow-matching、扩散）。

Result: 实验证明：将VLM与对象检测结合，在指令跟随度上与人类标注的一致性优于仅用VLM及CLIP类指标；框架能系统暴露现有编辑模型的失败模式。EdiVal-Bench评测了9类指令与11个前沿模型（含Nano Banana、GPT-Image-1等）。

Conclusion: EdiVal-Agent提供了自动化、可扩展、细粒度且更贴近人类判断的多回合指令式编辑评价；其模块化设计可持续吸纳新工具提升精度。EdiVal-Bench为社区提供标准化评测与诊断手段，促进下一代编辑模型研发。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [10] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything 是一个统一的 Transformer 前馈模型，输入多张图像与可选几何先验（内参、位姿、深度、局部重建等），一次前向就回归场景的度量级3D几何与相机。通过“因子化”多视几何表征（深度图、局部射线图、相机位姿、尺度因子）与跨数据集标准化训练，它在 SfM、MVS、单目深度、定位、深度补全等多任务上达到或超越专用模型，指向通用3D重建骨干。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉任务（SfM、MVS、深度估计、定位等）往往各自为政：需要不同流水线、监督形式与模型，难以共享数据与参数，训练低效且泛化欠佳。作者动机是用一个统一的前馈模型与统一监督，把不同任务的输入/输出和目标空间对齐，减少任务碎片化，并获得跨任务的正迁移与更高效的训练。

Method: 提出 MapAnything：基于 Transformer 的前馈网络，输入若干图像与可选几何信息（相机内外参、深度、部分重建）。核心是因子化多视几何表示：输出一组深度图、局部射线图、相机位姿以及一个全局度量尺度因子，用于把局部重建提升到全局一致的度量坐标系。通过在多数据集上标准化监督信号与训练协议，并采用灵活的输入增强，使模型能在单次前向中解决多种3D任务。进行了大量消融与对比实验。

Result: 在多项3D任务上，MapAnything 匹配或超越专用前馈模型的精度，同时因共享表示与参数带来更高的训练与推理效率；其联合训练表现优于分别训练的专用模型。

Conclusion: 一个统一的、前馈的、基于 Transformer 的通用3D重建骨干是可行的。通过因子化几何表示与标准化跨数据集训练，可以用单一模型覆盖从无标定 SfM 到 MVS、单目深度、相机定位、深度补全等广泛任务，并在效率与性能上具备竞争力。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [11] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: 提出SCM-PR：用RGB语义增强在无GPS环境下的LiDAR跨模态地点识别，结合VMamba特征、语义感知融合、语义几何LiDAR描述子与跨模态语义注意的NetVLAD，并引入多视角语义几何匹配与语义一致性损失，在KITTI与KITTI-360上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB的VPR对光照、天气、季节变化敏感；现有RGB-激光跨模态方法在复杂场景、细粒度/高分辨匹配和视角变化下表现欠佳。需要一种能在跨模态下鲁棒捕获语义与几何信息的定位方案。

Method: 提出SCM-PR框架：1) 用VMamba作为RGB特征主干；2) 语义感知特征融合(SAFF)联合地点描述子与分割掩码；3) 设计融合语义与几何的LiDAR描述子；4) 在NetVLAD中加入跨模态语义注意以提升匹配；5) 在对比学习框架下引入多视角语义-几何匹配与语义一致性损失。

Result: 在KITTI与KITTI-360数据集上，SCM-PR在跨模态地点识别任务上优于现有方法，达到SOTA（摘要未给出具体指标，但强调在多场景与视角变化下的提升）。

Conclusion: 融合高层语义与几何信息，并在跨模态匹配中显式建模语义注意与一致性，可显著提升无GPS环境下的跨模态地点识别鲁棒性与精度。

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [12] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 提出一种用于3D Gaussian Splatting(3DGS)数据压缩的场景自适应晶格矢量量化(SALVQ)，以极小改动替换现有方法中的均匀标量量化(USQ)，显著提升率失真性能，并可通过缩放晶格基向量实现单模型多码率。


<details>
  <summary>Details</summary>
Motivation: 3DGS渲染真实感强且实时，但数据量巨大，存储/传输成本高。现有锚点式神经压缩方法虽有效，却普遍采用USQ，难以充分利用相关性和分布特性。作者希望用更强的量化器在几乎不增加复杂度与系统改动的前提下提升压缩率失真（R-D）表现，并避免为不同码率训练多模型的资源开销。

Method: 以晶格矢量量化（LVQ）替换USQ，并为每个场景优化晶格基（scene-adaptive），以更好匹配场景统计分布，提高向量量化的适配性与R-D效率；设计为可无缝集成到现有3DGS压缩框架（锚点式）中；通过缩放晶格基向量调节晶格密度，实现单模型覆盖多码率目标，无需多模型训练；保持计算与实现复杂度接近USQ。

Result: 与现有采用USQ的锚点式3DGS压缩相比，SALVQ在几乎不增加计算与系统复杂度的情况下，取得更优的R-D性能；通过基向量缩放可灵活达成多种码率设定，减少训练时间与显存/内存需求。

Conclusion: 场景自适应LVQ为3DGS压缩提供了在复杂度、适配性与R-D效率间的有效折中方案，能以最小改动增强现有架构，并通过单模型多码率显著降低训练与部署成本。

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [13] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: 提出MINGLE，一个三阶段管线，用检测/深度+VLM关系推理+空间聚合，从单幅街景图像定位“社交群体”区域；并发布含个人与群体标注的10万张数据集。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测难以捕捉群体社交互动这类抽象关系（距离、相对朝向、共同行动）。为支持城市规划与包容性空间设计，需要从图像层面识别并落地这些群体交互。

Method: 三步：1) 使用现成的人体检测与深度估计获取个体与空间信息；2) 借助多模态大模型（VLM）对人-人对进行社交隶属/互动关系分类；3) 通过轻量级空间聚合算法将具有关联的个体合并为群体并输出群体区域。

Result: 构建了一个包含10万张城市街景图的大规模数据集，含个体与社交群体的框与标签；标注由人工与MINGLE输出结合，覆盖多样真实场景。方法能够从图像中定位社会连接的群体区域。

Conclusion: 定义了“社交群体区域检测”新任务并提供基线MINGLE与大规模数据集，为后续研究在公共空间群体互动理解与应用（如城市规划）奠定基础。

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [14] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: 提出BiasMap框架，利用稳定扩散的交叉注意力归因图，揭示并量化人口统计属性与语义概念在生成过程中的空间耦合偏置（用IoU/SoftIoU度量），并通过能量引导的扩散采样在潜在噪声空间直接最小化SoftIoU以缓解偏置；相比现有方法，能在不依赖输出分布统计的情况下发现并缓解概念层面的纠缠偏置。


<details>
  <summary>Details</summary>
Motivation: 当前对文本到图像模型的偏置检测多依赖输出层面的人口统计分布差异，但这无法保证模型内部对“性别/种族”等人口属性与“职业”等语义概念实现解耦；即便分布被校正，模型可能仍在表示层面把人口属性与语义绑定。需要一种能揭示并量化生成过程中概念级表示耦合的模型无关方法，并据此进行更有效的缓解。

Method: 1) 提出BiasMap：基于稳定扩散的跨模态交叉注意力，计算文本概念（人口属性与语义词）的归因/显著性地图；2) 用这些归因图之间的空间重叠度（IoU/SoftIoU）来量化人口统计-语义的概念耦合强度；3) 缓解：设计能量引导的扩散采样，在去噪过程中通过对潜在噪声施加梯度，直接最小化期望SoftIoU，从而减小概念耦合；4) 以模型无关方式适用现有稳定扩散；与现有公平性干预进行对比评估。

Result: 实验证明：现有公平性干预可以缩小输出分布差距，但常无法消除概念层面的耦合；BiasMap能发现这些隐藏的表示偏置；所提能量引导采样在不恶化分布层面指标的同时，有效降低人口属性与语义概念的IoU/SoftIoU（即解耦程度提升），并作为对分布偏置缓解的补充。

Conclusion: 概念层面的表示偏置在TTI生成过程中广泛存在且易被分布统计掩盖。BiasMap提供了一种通过交叉注意力归因与IoU度量来发现与量化这类耦合的通用框架，并通过能量引导扩散在潜在空间直接优化SoftIoU实现缓解。该方法与现有分布层面干预互补，能更全面地提升生成模型的公平性。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [15] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: 提出 LivePyxel（LivePixel），一款Python图形界面工具，直接接入相机/显微镜等成像设备，实现实时采集与标注，支持贝塞尔曲线、二值掩膜与非破坏图层，基于OpenCV与Numpy优化以加速目标检测相关工作流，面向实验室按需管线，简化数据获取与标注流程。


<details>
  <summary>Details</summary>
Motivation: 现有标注软件多依赖预先收集的数据集并需上传，难以满足实验室环境中与显微镜等设备的实时联动，增加数据获取与标注的摩擦，阻碍AI模型在科学场景的落地。

Method: 开发一个Python GUI（LivePyxel），与视频设备（摄像头、显微镜等）直接集成，实现实时图像流的采集与标注；提供类商业图像编辑器的交互工具（贝塞尔样条、二值掩膜、精确区域描绘、非破坏图层）；在后端使用OpenCV与高性能线性代数/矩阵库（Numpy）以优化对象检测相关操作与编辑性能；支持广泛视频设备兼容。

Result: 实现了跨设备的实时标注工作流，用户可无缝采集、分层编辑并生成高质量标注；在目标检测类任务中具备较高的交互与运行性能；降低了构建训练数据集的时间与步骤。

Conclusion: LivePyxel为实验室和其他实时成像场景提供了灵活高效的标注解决方案，缓解了传统离线标注流程的瓶颈，能加速AI模型的数据收集与开发；项目已开源，便于社区采用与扩展。

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [16] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 提出DEFT-VTON：在不改动大规模预训练扩散模型参数的前提下，通过Doob’s h-transform的高效微调与自适应一致性损失，实现低成本、高质量的虚拟试衣，并在仅15步去噪下达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实VTO应用受限于训练/推理/部署预算；现有方法需大规模端到端训练或较高参数开销与推理时延。需要一种在冻结预训练扩散模型的同时，以极少可训练参数获得图像条件化VTO能力，并进一步缩短推理步数而不牺牲质量。

Method: 1) DEFT微调：冻结无条件扩散模型主体，新增一个h-transform小网络学习条件h-transform，将无条件模型适配到图像条件VTO；仅训练约1.42%参数（对比传统PEFT的5.52%）。2) 自适应一致性损失：受约束优化启发，将一致性训练与去噪得分匹配联合，以数据自适应的方式在微调阶段权衡两者，无需完整蒸馏流程即可把慢而强的模型特性迁移到快模型上，降低推理步数。

Result: 在虚拟试衣任务上取得SOTA；在仅15个去噪步的快速推理下仍保持竞争性/领先性能，较基线在参数与推理时间上更优。

Conclusion: DEFT-VTON以极低训练开销实现高质量VTO，并通过自适应一致性损失将高性能扩散模型的优势迁移到低步数推理，实现效率与效果兼得，适合资源受限的实际部署。

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [17] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: 论文提出一种用于自动驾驶的合成数据增强管线：在Cityscapes中注入虚拟行人，并通过对抗式生成网络学习数据集光照条件，以缩小合成与真实域的差距，从而提升行人相关分割任务表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要覆盖长尾和稀有交通场景，尤其涉及弱势道路使用者（VRUs，如行人）。现有合成数据存在域间差距，限制了在真实环境中的识别效果。作者希望通过更逼真的数据增强（特别是光照一致性）来提升行人识别/分割性能。

Method: 1) 构建Cityscapes数据增强管线：将可控的虚拟行人插入真实背景中，生成带标注的合成样本；2) 提出一种新型生成对抗网络结构，专门学习并迁移Cityscapes的光照/成像条件，使新插入的行人与背景在光照上匹配，提升合成图像真实感；3) 在语义分割与实例分割任务上进行评估。

Result: 增强后的数据用于训练分割模型，相比未增强或传统增强方法，在行人相关类别的语义与实例分割指标上取得提升（文摘未给出具体数值）。

Conclusion: 通过将虚拟行人与对抗式光照适配相结合，可有效缩小合成-真实域差距，改进自动驾驶场景下的行人识别分割性能；方法为构建定制化交通场景数据提供了可行管线。

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [18] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: 提出FunKAN与U-FunKAN，用函数空间版KAN与傅里叶-厄米特展开，提升医学图像增强与分割的可解释性与性能；在多数据集上优于其他KAN骨干（PSNR、TV、IoU、F1）。


<details>
  <summary>Details</summary>
Motivation: 医学图像增强与分割受伪影与形变影响，现有深度网络复杂且可解释性差；传统KAN需展平特征破坏空间结构，限制图像任务表现。因此需要在保持空间结构与可解释性的前提下，构建更合适的函数逼近框架。

Method: 提出Functional Kolmogorov-Arnold Network（FunKAN），将Kolmogorov–Arnold表示定理推广到函数空间；内函数以傅里叶分解在厄米特基上学习，保持图像的空间结构。进一步构建U-FunKAN用于二分类医学分割。任务涵盖MRI吉布斯振铃抑制（IXI数据集）与三类分割（BUSI、GlaS、CVC-ClinicDB）。与其他KAN型骨干进行对比。

Result: 在增强任务上，PSNR与TV指标优于其他KAN基线；在分割任务上，IoU和F1更高。跨多模态、多器官与多成像手段（MRI、超声、病理、内镜）均显示稳定优势。

Conclusion: FunKAN将理论函数逼近与医学图像分析连接起来，提供可解释、保持空间结构的框架；U-FunKAN在多数据集上达SOTA二分类分割表现，显示其在临床应用中的潜力。

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [19] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出一种用于仇恨视频检测的多模态双流图神经网络，通过实例划分与重要性权重图突出“少量仇恨片段”以决定整段视频类别，并在公开数据集上达SOTA且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视频仇恨检测虽优于单模态，但(1)未突出稀少且决定性的仇恨片段，通常对内容一视同仁；(2)缺乏对视频内外多模态结构化关系的系统建模，导致融合效果受限。

Method: 将视频切分为多个实例，构建实例图提取实例级特征；并构建互补的权重图，对实例分配重要性权重以突出仇恨实例；随后融合权重与实例特征得到视频级预测。整体采用图神经网络框架，建模模态内与跨模态的结构关系。

Result: 在公开数据集上实现当前最优（SOTA）性能，并展示出较强可解释性（通过权重突出被判为仇恨的关键实例）。

Conclusion: 通过双流图神经网络强调少量关键仇恨片段并系统建模多模态结构关系，可显著提升仇恨视频分类效果与可解释性；代码已开源以促进复现与应用。

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [20] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 提出ColonCrafter：一种用于结肠镜单目视频的扩散式深度估计模型，侧重生成时间一致的深度图；通过合成数据学习几何先验并用保几何的风格迁移适配真实视频；在C3VD上零样本达SOTA，并展示点云重建与表面覆盖评估等临床应用。


<details>
  <summary>Details</summary>
Motivation: 内镜场景具有光照复杂、纹理贫乏和非刚体形变，现有深度估计在视频序列上时间一致性差，影响3D重建与临床量化，需要能在单目结肠镜视频中稳定输出时间一致的深度。

Method: 1) 扩散模型框架用于深度估计，强调跨帧的时间一致性生成；2) 使用合成结肠镜序列学习稳健几何先验；3) 设计保几何结构的风格迁移，将真实临床视频域对齐到合成训练域；4) 在零样本设置下直接推理真实数据。

Result: 在C3VD数据集上实现零样本SOTA，优于通用与内镜专用方法；可生成时间一致的深度图，并支持3D点云重建与表面覆盖度评估；但完整轨迹的3D重建仍具挑战。

Conclusion: 扩散式、合成先验与几何保真的风格迁移相结合，可显著提升结肠镜视频的时间一致深度估计与下游临床应用价值，尽管长范围3D重建仍未完全解决。

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [21] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: 提出在资源受限平台（如MAV）上改进3D Gaussian Splatting：通过体素空间合并几何相似的高斯以降GPU显存，同时用Patch-Grid点采样初始化高斯以提质；实验表明内存更省、渲染更优且实时性不降。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS多面向桌面高性能GPU，忽视嵌入式平台（计算与内存受限）应用；SLAM中高斯原语冗余导致显存开销大，且初始化不足影响全局渲染质量，需要兼顾内存与质量而不牺牲实时性。

Method: 1) 在SLAM过程中将3D高斯按体素划分，基于几何相似性（位置/尺寸/方向等）合并冗余高斯，减少原语数以降GPU显存；2) 采用Patch-Grid（PG）点采样对3D高斯进行初始化，使初始分布覆盖更均匀、结构感更强，从而提升场景建模与渲染质量。

Result: 在公开数据集上的定量与定性实验显示：GPU内存占用显著降低且系统运行时性能不受影响；渲染质量提升（指标未给出，但文字声称改进明显）。

Conclusion: 所提体素合并与PG初始化共同实现对嵌入式/资源受限平台友好的3DGS方案：在不牺牲实时性的前提下降低显存并提升渲染质量，适用于MAV等平台。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [22] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 论文提出一种针对自动驾驶轨迹预测的自适应OOD检测框架，基于快速变化检测思想并显式建模随时间演化的误差模式，在多真实数据集上显著降低检测延迟和误报，且计算高效，优于现有不确定性量化与视觉OOD方法。


<details>
  <summary>Details</summary>
Motivation: 现实部署中轨迹预测模型会遭遇分布漂移与罕见场景，导致轨迹级OOD难以及时可靠地发现；以往OOD研究多集中在视觉任务，轨迹层面方法匮乏，现有QCD框架虽有保证但缺少对复杂驾驶环境中时变与模式依赖误差的建模与适应。

Method: 以QCD为基础，加入自适应机制：对预测误差进行模式（mode）划分并显式建模其随时间与数据集动态演化的分布；基于这些模式化误差分布构造检测统计量，实现在复杂交通环境中的鲁棒快速检测。

Result: 在多真实世界数据集与标准轨迹预测基准上，方法在检测延迟和误报率上均有“显著改善”，相较UQ方法与视觉OOD基线更准确且计算更高效。

Conclusion: 通过对时变、模式依赖的误差分布进行显式建模并与QCD结合，可在轨迹级OOD检测中实现更快更稳的告警与更低误报，为面向驾驶语义的可靠自动驾驶提供实用路径。

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [23] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 提出一种利用卫星图像对与深度学习检测亚马逊雨林砍伐，并自动生成语义标注的方法，实验显示能有效发现森林覆盖变化并产出相关关键词。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林对气候调节与生物多样性至关重要，持续的砍伐显著影响全球碳排与生态系统，需要自动化、可扩展的方法来及时监测和解释森林变化。

Method: 使用地球观测卫星的同区域不同时相图像对，通过深度学习模型进行跨时相变化检测以识别森林覆盖变化；同时提出视觉语义模型，从与亚马逊相关的科学文献中提取候选关键词，对检测到的变化进行自动标注。

Result: 在亚马逊图像对数据集上验证，方法能有效检测砍伐区域并生成与变化相关的语义关键词，显示出较好的检测性能与注释相关性（摘要未给出具体指标）。

Conclusion: 该方法为监测与研究亚马逊砍伐影响提供了实用工具；尽管以环境应用为例，但方法通用，可迁移至其他领域的变化检测与语义标注任务。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [24] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 提出一个基于视觉-语言模型的多模态医学影像分析框架，集成Gemini 2.5 Flash，实现多模态肿瘤检测与报告生成，结合坐标校验与高斯概率建模，提供可视化与结构化输出，零样本能力强，但需临床多中心验证。


<details>
  <summary>Details</summary>
Motivation: AI在医疗影像快速发展，需要能跨CT/MRI/X-ray/超声等多模态、自动检测病灶并生成可解释的临床报告的系统，以提升诊断效率并减少对大规模标注数据的依赖。

Method: 构建VLM驱动的多模态框架：使用Gemini 2.5 Flash进行视觉特征与NLP融合，加入坐标验证机制与高斯概率建模表征异常分布；多层可视化（医学插图、叠加对比、统计图）并提供像素级位置测量（平均偏差80像素）；通过精确提示工程与文本分析将结果结构化，前端用Gradio集成到临床流程，支持零样本推理。

Result: 在多种影像模态的异常/肿瘤检测中取得较高性能；位置测量平均偏差80像素；能自动生成临床报告并抽取结构化信息；展示了零样本能力与良好可用性。

Conclusion: 该框架在自动化诊断支持与放射科流程效率上具有潜力与实用价值；然而仍需临床验证和多中心评估后方可广泛部署。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [25] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: 提出并推广CLAP（一种基于聚类的假设选择框架），从2D定位扩展到3D定位与图像拼接，展示其与RANSAC与霍夫变换的关系，强调在含噪与异常值场景下的稳健性与通用性。


<details>
  <summary>Details</summary>
Motivation: 传统基于重投影误差的一致性验证（如RANSAC）在强噪声/大量外点或匹配错误时可能效率低或不稳定；需要一种对外点更鲁棒、可广泛适用且不依赖全局一致性验证的替代思路。

Method: CLAP通过对从匹配/测量中生成的多重假设在参数空间内进行聚类，选取高密度簇对应的模型或位姿，从而抑制噪声与外点；本文将该策略从2D定位推广到3D定位与图像拼接，并形式化其与RANSAC（基于一致性评分）和霍夫变换（参数空间投票/累加）之间的联系。

Result: 在RoboCup 2024中已证明2D版CLAP的实战鲁棒性；本文进一步展示其在3D定位与图像拼接任务中的可行性与有效性，并说明与现有方法之间的对应关系。

Conclusion: 基于聚类的CLAP提供了一个通用的、对外点与噪声鲁棒的建模与定位框架，适用于多种视觉/机器人任务，可作为RANSAC与霍夫变换的互补或替代工具。

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [26] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: 提出SAMIR：用SAM的图像编码器提取结构感知特征，并配合轻量3D头与分层特征一致性损失，实现弱监督/无标注条件下更精准的医学图像配准；在心脏与腹部数据集上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 弱监督配准常依赖分割/标注等弱标签，但这些在临床不易获取，限制实用性。视觉基础模型（SAM）在大规模自然图像上预训练，具备强泛化的表征能力，可能在缺标签场景下提供结构化特征以提升配准。

Method: 设计SAMIR框架：1) 使用SAM的图像编码器，对医学图像进行任务特化的适配，提取结构感知的嵌入特征；2) 在嵌入空间上加入轻量级3D头，细化局部形变表征；3) 提出分层（粗到细）的特征一致性损失，指导多尺度匹配与解剖对齐；总体以特征驱动替代对弱标签的依赖。

Result: 在心脏（ACDC，主体内）与腹部CT（跨主体）配准基准上，较SOTA取得显著提升：ACDC提升约2.68%，腹部数据集提升约6.44%。

Conclusion: 借助SAM预训练表征并结合3D细化与分层一致性约束，可在缺少弱标签的情况下高效提升医学图像配准的准确性与鲁棒性；方法通用且高效，具有实际应用潜力，代码将公开。

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [27] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 提出一个基于联邦学习的分布式方法，在多个边缘卫星中心协同识别并定位卫星影像中的毁林；采用FLOWER+RAY进行训练编排与客户端仿真，模型包含YOLOS-small与两种Faster R-CNN（ResNet50、MobileNetV3）。


<details>
  <summary>Details</summary>
Motivation: 集中式训练需要汇聚跨区域敏感影像数据，带来隐私与安全风险；同时卫星数据分布在多客户端/边缘中心，带宽与合规限制不利于集中化。需要一种能在保证数据本地化与隐私的前提下仍能协同训练、提升毁林识别能力的方法。

Method: 以“客户端=边缘卫星中心”的联邦学习设置，使用FLOWER作为FL编排框架并结合RAY进行并行化与客户端生成/选取；在各客户端本地训练目标检测/定位模型并进行参数聚合。采用YOLOS-small（ViT变体）、Faster R-CNN(ResNet50)与Faster R-CNN(MobileNetV3)在公开数据集上进行训练与测试，用于毁林识别与定位（图像分割/检测视角）。

Result: 在分布式FL设置下成功完成跨客户端的协同训练与推理，证明了在不集中数据的情况下也能进行有效的毁林检测与定位；RAY能够高效地生成并管理一定数量的客户端以创建仿真环境。具体数值指标未在摘要中给出。

Conclusion: 联邦学习结合FLOWER+RAY可在保持数据隐私与安全的同时，支持卫星影像毁林检测的分布式训练与部署；所选模型在FL框架下可行，并为基于卫星影像的分割/检测任务提供新的视角。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [28] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: GARPS提出一种无训练的相对位姿估计框架：用单目“有度量”的深度和高斯场重建，每幅图像各自生成度量GMM，再通过可微GMM对齐在3D中直接配准两幅图像的场景，从而得到尺度确定的相对位姿；在RealEstate10K上优于经典与最新学习方法（含MASt3R）。


<details>
  <summary>Details</summary>
Motivation: 传统两视图位姿估计仅能到尺度，且在宽基线、弱纹理或高反射场景下鲁棒性差。需要一种能给出度量尺度、在不依赖稠密2D对应的前提下仍对遮挡与纹理缺失鲁棒的方法。

Method: 1) 使用度量单目深度估计器与高斯场重建器，从每张图像独立得到度量3D高斯混合模型(GMM)。2) 以某个前馈两视图估计器给出的初始位姿为起点，最小化可微的GMM对齐目标函数。3) 目标函数联合考虑几何结构、与视角无关的颜色、一致的各向异性协方差以及语义特征一致性，避免显式2D对应，并对遮挡、弱纹理区域鲁棒。

Result: 在RealEstate10K数据集上，GARPS在相对位姿精度上超过经典和最新学习型方法，包括MASt3R。

Conclusion: 通过将单视图度量感知（深度+高斯场）与多视图几何对齐相结合，可实现鲁棒且具度量尺度的相对位姿估计；训练自由、对复杂外观与几何条件更稳健。

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [29] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 论文提出用可微查找操作替代CNN中的乘法运算，通过构建可端到端训练的查找表（LUT）实现加速与节能，同时在图像分类、超分、点云分类上保持或接近原性能并达SOTA。


<details>
  <summary>Details</summary>
Motivation: CNN包含大量不同类型运算，其中乘法计算复杂度高、能耗大、推理慢，限制了在移动与边缘设备上的部署；边缘设备常借助查找表以降低复杂度，作者受此启发将查找表作为神经网络的基本算子以替代乘法。

Method: 将权重与激活值的乘法用高效查找操作替代；构建可微的查找表以支持端到端训练，并提出若干训练策略加速与稳定收敛；据此在多任务（图像分类、图像超分、点云分类）中用查找网络替代传统卷积网络中的耗时乘法。

Result: 采用查找操作的网络在能耗与推理速度方面显著优于基线卷积网络，同时在精度/性能上保持竞争力；在多种任务与数据类型上达到或优于现有方法（SOTA）。

Conclusion: 可微查找表能作为通用高效算子替代传统乘法，在不牺牲或微弱影响精度的前提下，显著降低推理能耗与延迟，并具有跨任务、跨数据类型的适用性。

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [30] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: 提出一种面向指代表达图像分割（RIS）的“语义可视化投影器”，用SAM生成的语义超像素替代传统patch作为视觉token，动态压缩视觉token序列，在几乎不损精度的前提下将视觉token减少约93%，显著加速与提升MLLM+SAM框架的训练与推理效率，并优于现有压缩型投影器。


<details>
  <summary>Details</summary>
Motivation: MLLM与SAM结合的RIS很强，但视觉token冗余导致适配与推理由于计算量大。传统patch投影器难以同时做到“少token”和“语义清晰”，为避免性能下降常保留很长序列，效率受限。需要一种能按场景复杂度自适应压缩、且尽量保持语义的视觉表征方式。

Method: 受文本分词启发，将图像中由SAM生成的语义超像素视作“视觉词”。1）语义可视化投影器：对每个语义超像素进行压缩与投影，作为视觉token，序列长度随场景复杂度自适应变化；2）语义超像素位置嵌入：编码几何形状与空间位置以增强MLLM对超像素的空间感知；3）语义超像素聚合器：在超像素内部保留细粒度信息、在外部汇聚全局上下文，缓解压缩带来的信息损失。

Result: 在RIS任务上，视觉token数量减少约93%，训练与推理显著加速；性能基本不受影响，且在多项指标上优于现有压缩型视觉投影器。

Conclusion: 基于SAM超像素的语义级视觉分词与投影可在强压缩下维持语义完整性与分割精度，提升MLLM适配RIS的效率与效果，提示“语义超像素=视觉词”的方向对多模态理解与生成具有推广潜力。

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [31] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: 提出FishBEV：专为全景鱼眼相机设计的BEV分割框架，包含抗畸变多尺度特征骨干、基于不确定性的跨视角对齐和距离感知的时序注意，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有BEV分割多基于针孔相机，直接迁移到鱼眼相机时因严重几何畸变、多视角对应模糊与时序不稳定而显著退化，需专门方法提升鲁棒性与一致性。

Method: FishBEV包含三项关键组件：1) DRME抗畸变多尺度骨干，兼顾畸变鲁棒性与尺度一致性；2) U-SCA不确定性感知的空间跨注意力，利用不确定性估计实现更可靠的跨视角对齐；3) D-TSA距离感知的时序自注意，根据远/近场自适应平衡细节与上下文以保证时序连贯。

Result: 在Synwoodscapes数据集上进行大量实验，FishBEV在环视鱼眼BEV分割任务上持续超越SOTA基线。

Conclusion: 针对鱼眼相机BEV分割的核心难题（畸变、跨视角对应与时序稳定），FishBEV通过DRME、U-SCA与D-TSA三者互补设计，实现更稳健的空间-时间建模与对齐，取得领先性能。

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [32] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: 提出三种样条驱动的KAN（SBTAYLOR-KAN、SBRBF-KAN、SBWAVELET-KAN），在无需预处理、少样本且跨数据集条件下实现高效、可解释的医学影像分类。核心亮点：极少参数（~2.9K）却可达接近或优于大型CNN的性能，并具备良好泛化与稳健性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的临床环境中，医学影像分类常面临数据不足、类别不平衡、计算资源有限与可解释性需求高的问题。传统CNN通常参数量巨大、需大量标注与预处理且可解释性不足，因此需要一种轻量、可解释且能在小样本与异质数据上稳定泛化的方法。

Method: 引入样条驱动的Kolmogorov‑Arnold Networks（KAN），以B样条为基础并结合不同基函数族形成三类变体：1）SBTAYLOR-KAN：B样条+泰勒级数以刻画局部与全局多尺度非线性；2）SBRBF-KAN：B样条+径向基函数提升局部近似与平滑性；3）SBWAVELET-KAN：B样条嵌入Morlet小波变换以捕捉时频/空间频率特征。模型直接从原始图像学习，无需预处理；采用跨数据集验证与数据削减实验评估泛化与数据效率；使用Grad-CAM进行可解释性可视化。

Result: 在脑MRI、胸片、结核X光与皮肤病变等多数据集上取得强劲表现：SBTAYLOR-KAN最高准确率达98.93%，F1均衡；在仅用30%训练数据时仍保持>86%准确率（跨三数据集）；在皮肤癌不平衡与重采样平衡两种设置下均优于对比模型，最高68.22%准确率。与ResNet50（约2418万参数）相比，SBTAYLOR-KAN仅2872个可训练参数即可达到可比性能。

Conclusion: 样条驱动的KAN为医学影像分类提供了一种轻量、可解释且具备强泛化与数据效率的框架，适合资源受限与小样本临床场景；Grad-CAM验证了关注的病灶区域，显示方法具备可临床解释潜力。

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [33] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: 提出StyleProtect：一种通过选择性更新扩散模型的部分交叉注意力层来抵御风格微调滥用的轻量级保护方法；在WikiArt与Anita卡通数据上验证，能有效抑制风格模仿且几乎不可感知。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（尤其经微调）易被恶意用于高保真风格拟合，侵害艺术家的创作权益。需要一种既有效又高效的防护方法，在不显著损害作品外观的前提下阻止或削弱风格被模型学走。

Method: 提出“风格敏感”假设：部分cross-attention层对艺术风格的响应更强。通过测量注意力层对风格/内容表示的激活强度，并与外部特征模型提取的风格与内容特征相关性进行评估，定位最敏感的层。仅对这些选中的cross-attention层进行参数更新（保护性微调/扰动），实现StyleProtect，达到在微调定制场景下的风格防御。方法轻量、参数更新少，兼顾不可感知性。

Result: 在基于WikiArt选取的30位具有鲜明风格的艺术家作品以及Anita动漫数据上，StyleProtect显著降低微调后扩散模型对原风格的忠实复现能力，同时保持视觉不可感知或低可感知的修改；与基线相比具有更高的防御有效性与效率。

Conclusion: 跨注意力层存在对风格高敏感的子集，针对性更新这些层即可带来有效的风格防护。StyleProtect在艺术与动漫风格上均表现良好，兼顾保护强度与不可感知性，适合应对微调驱动的风格滥用。

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [34] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: 提出UM-Depth：在自监督单目深度估计中引入“运动+不确定性感知”的教师-学生框架，利用教师端光流与不确定性估计在训练期强化监督，推理时零额外开销，在KITTI/Cityscapes上达SOTA深度与位姿表现。


<details>
  <summary>Details</summary>
Motivation: 自监督单目深度估计无需深度真值，但在低纹理、动态区域等不确定性高处容易出现深度误差；现有运动感知方法多需额外标签/网络并增加推理开销，亟需一种在这些困难区域提升精度且无推理负担的方法。

Method: 构建教师-学生训练：在教师网络中引入光流与不确定性估计，作为对学生的软监督与掩蔽/加权信号；网络结构和训练管线均显式建模不确定性，在光度信号弱（动态边界、无纹理）处加强监督并进行运动感知校正。光流仅在训练期教师端使用，避免部署时开销与额外标签。

Result: 在KITTI与Cityscapes广泛实验显示，不确定性感知的细化策略有效提升深度精度，尤其在动态物体边界和纹理缺失区；在KITTI上单目自监督深度与位姿估计均达到了SOTA。

Conclusion: 通过将运动信息与不确定性估计嵌入教师-学生自监督框架，UM-Depth在不增加推理成本的前提下显著提升困难区域的深度与位姿估计精度，验证了训练期不确定性引导与光流辅助的有效性。

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [35] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出Triple Query Former (TQF)用于指代视频目标分割，通过将查询分解为外观、帧内交互和跨帧运动三类，并结合语言与视觉动态构建查询，辅以帧内位置感知聚合与跨帧轨迹对齐聚合，显著缓解静态文本查询的选择偏差并在多基准上取得领先。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS多采用静态文本查询进行跨模态对齐，容易被外观或运动相似的干扰目标误导，产生查询选择偏差，导致定位与跟踪不稳。需要一种能同时建模外观、空间关系与时间关联，并能动态适配视频内容的查询与特征聚合机制。

Method: 1) 将指代查询三因子化：外观查询（静态属性）、帧内交互查询（空间关系）、跨帧运动查询（时间关联）；2) 查询并非仅由文本嵌入驱动，而是融合语言线索与视觉引导动态构建；3) 设计两种运动感知聚合模块：帧内交互聚合（位置感知的目标间交互）与跨帧运动聚合（基于轨迹引导的跨帧对齐），共同提升目标token表示并保证时序一致性。

Result: 在多个RVOS基准上进行大量实验，TQF整体优于现有方法，验证了三分结构化查询设计与两类运动感知聚合模块的有效性。

Conclusion: 通过结构化三元查询与运动感知聚合，TQF有效缓解静态查询的选择偏差，强化跨模态与时空一致性，从而在RVOS任务上取得显著性能提升。

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [36] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出InstanceVG，一种具备实例感知能力的多任务通用视觉指代框架，统一解决GREC（框级）与GRES（像素级），通过实例查询与参考点实现点-框-掩膜一致预测，在十个数据集与四个任务上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有通用视觉指代方法通常将GREC和GRES分开处理，缺乏联合训练导致多粒度预测不一致；同时把GRES当作语义分割忽视实例感知，难以保证实例级框与掩膜一致性与匹配。

Method: 提出InstanceVG：以实例查询为核心，赋予每个实例查询一个先验参考点，并将其作为目标匹配的附加依据；统一预测同一实例的参考点、边界框与分割掩膜，实现联合与一致性学习；框架一次性同时处理GREC与GRES，引入实例感知设计以提升实例级定位与分割。

Result: 在十个数据集、四个任务上大幅超越现有方法，多个评测指标取得SOTA，展现更强的多目标和非目标情形下的稳健性与精度。

Conclusion: InstanceVG是首个同时解决GREC与GRES且具实例感知的通用视觉指代框架；通过实例查询与参考点实现跨粒度一致预测，显著提升性能，并为统一的视觉指代学习提供有效范式。

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [37] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出FMFA框架，通过显式细粒度对齐与现有隐式关系推理相结合，实现文本到图像人物检索的全模式对齐，并在三数据集上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: TIPR的难点是跨模态在共同潜空间的有效对齐。现有方法仅用注意力做隐式局部对齐，无法验证局部对齐是否正确；训练时偏重于处理hard negatives，忽视了被错误匹配的正样本对，导致全局对齐不精确。

Method: 提出FMFA框架，包含两大模块：1) 自适应相似度分布匹配（A-SDM），针对未对齐的正样本对，依据相似度分布自适应地拉近其在联合嵌入空间的距离，矫正全局对齐；2) 显式细粒度对齐（EFA），对跨模态相似度矩阵稀疏化，并采用硬编码的局部对齐策略，对局部匹配进行显式验证与强化，弥补隐式关系推理的不足。整体不需额外监督，结合全局匹配与细粒度互动实现“全模式”对齐。

Result: 在三个公开数据集上，相比所有仅做全局匹配的方法取得了最新最优（SOTA）的检索性能；证明A-SDM能有效纠正错配正样本、EFA能提升细粒度跨模态对齐质量。

Conclusion: FMFA通过自适应拉近错配正样本与显式细粒度对齐，联合隐式关系推理，实现更精确的跨模态全局与局部对齐，显著提升TIPR性能且无需额外监督。

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [38] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 提出一种显式“文本嵌入↔RGB颜色”映射的文本驱动图像编辑方法，实现精确、可控、连续的颜色编辑，优于仅用文本插值的方式。


<details>
  <summary>Details</summary>
Motivation: 纯自然语言描述存在歧义和离散性，导致基于文本的颜色编辑精度不足、难以连续控制；简单做文本嵌入线性插值虽能产生不同颜色，但无法精确限定颜色范围，且插值系数与图像颜色之间关系不明确、不可控。

Method: 设计一个颜色映射模块，显式建模文本嵌入空间与图像RGB值的对应关系：给定目标RGB值（或范围），模块预测相应的文本嵌入向量，引导生成模型在保持语义一致性的同时产生对应颜色；用户可指定RGB区间以生成在该范围内连续变化的图像。

Result: 在实验中，该方法在颜色连续性与可控性方面表现良好，相较仅依赖文本插值能更精确地控制颜色与变化范围。

Conclusion: 通过引入“RGB到文本嵌入”的映射，实现了细粒度、连续、可控的颜色编辑，缓解了语言歧义带来的不确定性，并保持语义一致性，具有实际应用前景。

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [39] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 提出一种结合视觉反馈的迭代式提示词优化算法，用VLM同时审视文本提示与生成图像，提升T2I输出的安全性且尽量保持用户意图；并构建含文本与视觉安全信号的新数据集用于SFT，实验表明在不牺牲意图对齐的情况下提升安全。


<details>
  <summary>Details</summary>
Motivation: 现有T2I安全方法多依赖LLM对提示词进行文本层面的改写，忽略生成图像本身，导致两类问题：一是改写后仍可能产生不安全图像；二是对原本安全的提示词做了过度干预，损害意图与质量。因此需要一种利用视觉反馈的安全控制机制，兼顾安全性与意图保真。

Method: 提出迭代式提示词优化（IPR）：在每轮迭代中先用当前提示词生成图像，再用VLM对提示词与生成图像进行多模态安全与意图分析；依据分析结果自动调整提示词（强化约束或恢复被过度抑制的意图），直至满足安全与意图标准或达到迭代上限。同时构建一个由多模态LLM打标的训练数据集，包含文本与视觉层面的安全标签，用于监督微调。

Result: 与仅基于LLM的提示改写方法相比，该方法在多个数据集与指标上实现更高的安全评分，同时保持与用户意图的对齐度与可靠性相当（未显著下降）。实验显示视觉反馈在减少不必要改写与避免不安全输出方面有效。

Conclusion: 多模态（文本+图像）闭环反馈的迭代提示优化是提升T2I安全性的有效途径；所构建的数据集和方法为实用的安全生成提供了手段，兼顾安全与意图对齐，代码已开源。

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [40] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出TA-ISP：一种紧凑的RAW到RGB任务感知ISP，通过轻量多尺度调制算子在全局/区域/像素层面重塑图像统计，提升检测/分割性能，同时显著降低参数量与推理延迟，适配资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 现有利用RAW数据的方法要么依赖大型端到端ISP网络，计算开销大；要么依赖手工或传统ISP管线调参，表达能力有限，难以为下游检测/分割等任务提供最优表征。需要一种既高效又具备强表达能力的RAW到RGB处理方案，直接为预训练视觉模型产生任务导向的输入。

Method: 设计Task-Aware ISP（TA-ISP）：不采用重型致密卷积，而是预测少量轻量化的多尺度调制算子，在全局、区域与像素尺度上对图像统计进行因子化控制与重塑，使空间可变变换的表达范围扩大，同时严控内存、计算与时延；输出与现有预训练模型兼容的任务导向RGB表示。

Result: 在多个RAW域的目标检测与语义分割基准（含日间与夜间场景）上，TA-ISP在保持或降低计算/参数/延迟的同时，一致性提升下游准确率；与大规模ISP网络相比显著更快、更小；与传统ISP调参方法相比效果更优。

Conclusion: TA-ISP实现了高效、可部署的任务感知RAW到RGB处理：通过多尺度调制扩展表达力、降低资源占用、提升检测/分割性能，适合资源受限设备与多场景应用。

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [41] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: 提出NDLPNet用于夜间低照条件下去雨，结合位置感知模块PPM以捕获雨条位置与密度分布，在新构建的NSR数据集及现有数据集上优于SOTA，并公开代码与数据。


<details>
  <summary>Details</summary>
Motivation: 现有去雨方法多为白天设计，夜间低照下因雨分布空间异质性与受光照影响的条纹可见性导致效果差，亟需能利用空间位置信息并适应夜间特性的去雨模型与数据基准。

Method: 提出Nighttime Deraining Location-enhanced Perceptual Network (NDLPNet)。核心为位置感知模块PPM，捕获并利用输入的空间上下文与位置信息，重标定不同特征通道的重要性，从而更好识别雨条位置与密度；整体网络在去除雨条同时保留背景细节。

Result: 在自建的夜景雨图像对数据集NSR（900对，真实夜间场景）与现有数据集上进行大量定性与定量实验，NDLPNet在夜间去雨指标上持续优于多种SOTA方法。

Conclusion: 利用位置增强感知的NDLPNet在夜间低照去雨任务中实现了更强去雨与背景保真；NSR数据集为该方向提供了新的基准。源码与数据已开源。

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [42] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: 提出VocSegMRI，一个结合视频、音频与语音学信号的多模态rtMRI声道结构分割框架，通过跨注意力对齐与对比学习，在USC-75子集上达成SOTA（Dice 0.95、HD_95 4.20 mm），即使推理无音频也具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有rtMRI声道分割多依赖视觉，难以在动态与低对比场景下精确分割；而同步声学与语音学（音位/发音部位等）信息可提供互补时序与语义约束，提高分割精度与稳定性。

Method: 构建多模态架构VocSegMRI：视频、音频、语音学输入经各自编码器提取特征；通过跨注意力融合实现动态跨模态对齐；引入对比学习目标以强化跨模态表示并在无音频推理时保持判别性；在USC-75 rtMRI子集上训练与评测，并进行消融验证跨注意力与对比学习的贡献。

Result: 在USC-75子集上取得SOTA：Dice 0.95，HD_95 4.20 mm，优于单模态与既有多模态基线。消融显示移除跨注意力或对比学习均会降低分割精度与鲁棒性。

Conclusion: 多模态（视频+音频+语音学）融合结合跨注意力与对比学习能显著提升rtMRI声道结构分割准确性与鲁棒性，且在推理缺失音频时仍有效，表明综合语音相关信号对声道分析具有重要价值。

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [43] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: 提出一种利用扩散先验的生成式图像压缩框架，在低码率下显著提升主观质量，并在多项实验中优于传统与学习型方法。


<details>
  <summary>Details</summary>
Motivation: 生成技术让视觉内容同时包含自然与AI图像，现有编解码器在高压缩比下难以保持主观质量；生成式方法又存在保真与泛化不足，迫切需要兼顾主观质量、低码率与跨域适配的高效编码方案。

Method: 采用预优化编码器生成通用的压缩域表征；通过轻量适配器与注意力融合模块将其与预训练扩散模型内部特征融合；并提出分布再归一化策略提升重建保真度。框架可在不同预训练扩散模型间高效迁移，最小化再训练成本。

Result: 在低码率下视觉保真度优于现有方法；相对H.266/VVC的压缩性能最高提升79%；对AI生成内容高效，同时能适配更广泛内容类型。

Conclusion: 基于扩散先验的生成式编码在低码率场景下实现更好的主观质量与压缩效率，具有良好的可迁移性与适配性，可作为处理多源（自然与AI生成）内容的实用方案。

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [44] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: 提出AdaThinkDrive：在VLA自动驾驶中自适应选择是否使用CoT推理，兼顾精度与效率；在Navsim上提升PDMS至90.3并减少14%推理时延。


<details>
  <summary>Details</summary>
Motivation: 现有在自动驾驶VLA中引入CoT常在简单场景无收益却带来计算开销，缺乏能按需启用推理的机制，难以在效率与决策质量间取得平衡。

Method: 1) 预训练：在大规模AD场景上联合QA与轨迹数据，学习世界知识与驾驶常识；2) SFT：构造双模式数据集，包含快速回答（无CoT）与慢思考（有CoT），让模型学会区分何时需要推理；3) 强化学习：提出Adaptive Think Reward并结合GRPO，通过对比不同推理模式下的轨迹质量来奖励“选择性使用CoT”的策略，从而学习自适应推理。

Result: 在Navsim基准上，PDMS达90.3，较最佳纯视觉基线+1.7；相对“从不思考”与“总是思考”基线分别提升PDMS 2.0与1.4；相较“总是思考”在推理时间上减少14%。

Conclusion: 自适应双模式推理能在自动驾驶VLA中有效平衡性能与效率；通过数据与奖励设计，模型学会在需要时启用CoT，在简单场景跳过，从而获得更高PDMS与更低延迟。

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [45] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: 提出一种结合局部与全局视角并通过形态学操作融合的深伪篡改区域定位方法，抑制噪声、提升空间一致性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 分类式深伪检测虽准，但对伪造区域的精确定位仍弱；现有方法虽用标注掩码训练，却忽视局部细节与全局语义的互补性，且简单相加融合会放大噪声与误差。

Method: 构建双分支：局部分支关注细节纹理，全局分支关注整体语义，分别独立预测篡改区域；随后利用形态学操作（如膨胀、腐蚀、开闭运算等）对两分支预测进行结构化融合，抑制孤立噪点并增强连贯性。

Result: 大量实验表明：双视角预测与形态学融合各自均能带来显著提升；融合后定位更准确、噪声更少、鲁棒性更强。

Conclusion: 同时利用局部与全局信息并以形态学策略进行预测融合，可有效提升深伪篡改区域定位的精度与稳定性。

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [46] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: 提出“Variable-Rate Spatial Event Mamba”，直接处理原始事件流，以线性复杂度时空建模，并通过自适应控制器按事件率变速推理，兼顾延迟与效率。


<details>
  <summary>Details</summary>
Motivation: 事件相机具备微秒级时间分辨率，适合高速视觉；但现有方法需把事件流聚合成帧/体素/点云，必须设定时间窗引入窗口延迟；逐点检测虽免窗口但计算代价高，难以实时。需要一种既避免窗口延迟又可实时的端到端方法。

Method: 1) 直接输入原始事件流，无中间表示。2) 轻量因果的空间邻域编码器，捕获局部几何关系。3) 基于Mamba的状态空间模型进行可扩展时间建模，整体复杂度线性。4) 推理期引入控制器，根据事件率自适应调整处理速度，在窗口延迟与推理延迟间动态折中。

Result: 在抽象中未给出具体数值，但声称实现了可扩展线性复杂度的时序建模，并通过可变速推理在延迟与效率间达到最优平衡，具备实时性优势。

Conclusion: 该架构避免中间表示带来的窗口延迟与逐点方法的高代价，实现对事件流的高效端到端处理；Mamba时空建模与自适应速率控制共同提升高速场景下的实时性能与延迟权衡。

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [47] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: 提出训练无关的Block-Wise Caching (BWCache)以在扩散时间步之间复用DiT块特征，利用中间时间步特征高相似的U型动态，达到最高2.24×加速且画质可比。


<details>
  <summary>Details</summary>
Motivation: DiT视频生成性能强但推理需按时间步顺序去噪，延迟高；现有加速要么改结构损画质，要么复用粒度不当，未充分利用跨时间步的特征冗余。

Method: 分析DiT块在扩散时间步上的特征变化呈U型，中间时间步相似度高。提出BWCache：对DiT各块进行跨时间步的动态缓存与重用；设计相似度指示器，仅当相邻时间步块特征差异低于阈值时触发复用，从而减少冗余计算且无需额外训练或改动主干。

Result: 在多个视频扩散模型上验证，BWCache在保持可比视觉质量的同时，推理速度最高提高至2.24×。

Conclusion: 利用DiT在扩散过程中的中间步特征冗余，通过块级动态缓存可显著加速推理且不牺牲画质；该方法通用、训练无关，适合实际部署以降低延迟。

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [48] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一套针对航天器姿态估计中关键点回归的监督域适应框架，结合合成与少量真实标注数据，学习域不变表示并最小化任务风险，在SPEED+基准上以极少目标域标注达到或超过更高标注量的“oracle”表现。


<details>
  <summary>Details</summary>
Motivation: 混合式SPE管线在合成数据上表现好，但在真实/实验室图像上因合成到真实域差导致性能骤降。现有无监督域适应在存在少量目标域标注时利用不足，缺乏面向关键点回归任务的监督域适应方法。

Method: 基于LIRR（Learning Invariant Representation and Risk）范式，联合优化：1）域不变表征学习，用于缩小合成与真实域差；2）任务风险最小化，用于关键点回归精度。训练同时使用源域（合成）标注与目标域少量标注，框架轻量、与骨干网络无关，计算高效。

Result: 在SPEED+数据集上，方法稳定超越仅源域训练、微调以及“oracle”基线；只用5%目标域标注即可匹配或超过使用更大量标注训练的oracle性能。

Conclusion: 监督域适应可有效缓解SPE中的域偏移，利用少量真实标注即可获得强泛化，方法实用、可部署，适用于真实空间环境中的航天器姿态估计。

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [49] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: 提出MAFS多高度飞行数据集与SWA-PF语义加权自适应粒子滤波，实现在低分辨率卫星图上秒级4-DoF定位，误差<10m，计算效率较特征法提升10倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的视觉UAV定位在GNSS拒止环境中受限于数据集不足、实时性差、对环境变化敏感、泛化能力弱，尤其在动态/时间变化场景中表现不稳，需要新的数据与方法提升鲁棒性与效率。

Method: 1) 构建大规模多高度飞行分段数据集MAFS，覆盖可变高度场景。2) 提出SWA-PF：在UAV图像与卫星图之间提取稳健语义特征，并引入语义加权机制以增强匹配可信度；3) 设计优化的粒子滤波架构，自适应更新与重采样，实现4-DoF（x,y,航向、尺度/高度相关）位姿估计；4) 使用低分辨率卫星图实现快速全局定位。

Result: 在MAFS上评估，SWA-PF较基于特征提取的方法计算效率提升10倍；全局定位误差保持在10米以内；可在数秒内完成4-DoF位姿估计，即使仅使用低分辨率卫星地图。

Conclusion: MAFS数据集与SWA-PF方法在GNSS拒止下显著提升UAV视觉定位的效率、精度与鲁棒性，适用于多高度与时变环境；代码与数据集将开源以促进研究。

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [50] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: 提出一种用于语义分割UDA的辅助任务——特征空间的掩码重建（MFM），通过在特征层面遮挡并重建，再用分割解码器进行分类，从而与主任务强耦合、推理零开销，跨多架构与基准显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 对比学习等自监督任务虽提升了可分性，但“掩码建模”在UDA分割中少见，主要因：1）重建低层输入/感知特征与分割主目标不一致；2）与主流架构（如DeepLab、DAFormer）不兼容；3）训练目标与推理流程脱节，可能干扰主任务。需要一种既与分割目标对齐、又无需改动推理的掩码建模方案。

Method: 提出Masked Feature Modeling：在编码器特征图上随机掩码，借助轻量辅助模块Rebuilder重建被遮挡的特征；重建后的特征直接送入原有分割解码器进行像素级分类。Rebuilder仅训练期参与、推理时丢弃，避免额外开销。该设计使辅助目标（重建可用于分割的判别性特征）与主任务强耦合，防止目标不一致带来的负迁移；无需改动标准分割架构与推理流程。

Result: 在多种架构（如DeepLab、DAFormer）和多套UDA基准上，MFM稳定带来性能提升，相较未使用MFM的基线显著提高分割精度，证明其简单有效与泛化性。

Conclusion: 在UDA语义分割中，直接在特征层进行掩码与重建、并用解码器进行分类的MFM，提供了与主任务对齐、零推理开销且易集成的辅助学习范式，能在各类设置中一致增强分割表现。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [51] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: 本文研究高光谱图像的光谱分类，在小样本场景下，用无可训练特征提取的MiniROCKET/HDC-MiniROCKET替代参数更少但易过拟合的1D-Justo-LiuNet，结果在有限数据时显著更优，常规情况下基本持平。


<details>
  <summary>Details</summary>
Motivation: 尽管空间-光谱方法整体最强，但纯光谱分类具有模型更小、训练数据需求更低等优势，且对空间-光谱方法具有互补性。现有SOTA 1D-Justo-LiuNet在训练数据有限时性能下滑，亟需对小样本更稳健的方法。

Method: 引入MiniROCKET与HDC-MiniROCKET用于一维光谱序列分类：通过固定、精心设计的卷积核生成特征（特征提取部分无可训练参数），随后用线性（或超维计算融合）的分类器完成判别，以降低对训练数据量的敏感性。

Result: 在有限训练数据条件下，MiniROCKET优于1D-Justo-LiuNet；在一般数据充足的情形下，其性能大多与1D-Justo-LiuNet相当。尽管MiniROCKET总体参数更多，但泛化更佳。

Conclusion: 固定特征提取的MiniROCKET/HDC-MiniROCKET在光谱分类中对小样本更鲁棒，可作为1D-Justo-LiuNet的有效替代，并为未来提升空间-光谱方法提供互补改进方向。

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [52] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出Semi-MOE：面向病理图像分割的半监督多任务专家混合框架，通过多门控伪标注与自适应多目标损失，在低标注场景优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 半监督病理分割依赖伪标签，但腺体边界模糊、形态相似易致伪标签噪声高，现有方法难以稳健利用未标注数据。需要同时刻画形态边界、距离信息与语义分割并动态抑噪。

Method: 设计三专家Mixture-of-Experts：主分割专家、带符号距离场(SDF)回归专家、边界预测专家，分别学习互补形态特征；提出多门控伪标注模块(Multi-Gating Pseudo-labeling)动态汇聚专家特征，先融合再细化伪标签；引入自适应多目标损失(Adaptive Multi-Objective Loss)，在训练中无须手动调参地平衡分割、SDF与边界目标。

Result: 在GlaS与CRAG基准的低标注比例设置下，Semi-MOE在分割性能上超越SOTA（具体指标未给出，但报告“广泛实验”验证优越性）。

Conclusion: 多任务MoE结合动态门控与自适应多目标优化可有效缓解伪标签噪声，提升半监督病理分割性能，显示MoE在该领域的潜力。

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [53] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 提出VHBench-10细粒度幻觉评测与VisionWeaver路由网络，通过多专家动态聚合视觉特征，显著降低LVLM物体幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: 不同视觉编码器因训练范式差异带来不同的归纳偏置，导致对物体幻觉的表现各异；现有基准多为粗粒度，无法细分与验证这些差异，需要一个细粒度基准来系统分析并推进减幻觉方法。

Method: 1) 构建VHBench-10：约一万样本，覆盖十类细粒度幻觉，用于评估LVLM。2) 实证分析多种视觉编码器在各类幻觉上的特征与差异。3) 提出VisionWeaver：上下文感知的路由网络，利用全局视觉特征生成路由信号，在多位专长（specialized）视觉专家之间动态聚合/选择特征；相较于简单特征融合具有更优适配性。

Result: 评测显示各视觉编码器在幻觉类型上具有独特特征与差异；VisionWeaver在多个实验中显著降低幻觉发生率，同时提升总体任务性能。

Conclusion: 细粒度评测揭示了视觉编码器归纳偏置与幻觉表现的关联；基于上下文感知路由的多专家特征聚合是缓解LVLM物体幻觉的有效路径。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [54] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 提出“Consistent View Alignment (CVA)”自监督方法，显式对齐多视角表示以捕获互补信息，避免虚假正样本，显著提升下游任务表现，并在 MICCAI 2025 SSL3D 挑战中获第1/第2名（分别基于Primus ViT与ResEnc CNN）。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习常假设“无关视角”足以产生有用表示，但作者发现潜在空间结构不会自然出现；若不显式诱导，对齐会引入噪声或丢失互补信息，影响下游性能。因此需设计结构化的视角对齐策略来稳定学习有效表示。

Method: 提出Consistent View Alignment：从同一数据点生成不同视角，学习其表示并进行显式对齐，侧重对齐“互补信息”而非盲目拉近；通过一致性约束与机制减少错误正样本（false positives），在不破坏判别性的前提下强化跨视角的一致结构。适配不同主干（Primus ViT与ResEnc CNN）。

Result: 在多项下游任务上优于对比方法；在MICCAI 2025 SSL3D挑战中，基于Primus ViT获得第一，基于ResEnc CNN获得第二；开源代码与预训练权重可复现。

Conclusion: 有效表示的关键在于显式、结构化的多视角对齐。CVA能对齐互补信息、抑制伪正样本，从而稳定提升自监督表示的下游性能，并具备跨架构适配性。

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [55] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 提出SpecDiff：一种结合历史与“自推测”未来信息的多级特征缓存策略，在SD3/3.5与FLUX上以近乎无质量损失实现约2.7–3.2×加速，突破速度-精度权衡。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型特征缓存方法仅依赖历史信息，导致信息利用不充分，速度与精度受限。作者从信息利用角度剖析这一瓶颈，寻求在不训练的前提下引入更多有效信息以提升推理效率与质量。

Method: 提出自推测范式：利用同一时间步跨迭代的相似性引入“未来信息”。据此设计SpecDiff（免训练），含两部分：1）基于自推测与历史信息的动态重要性评分，进行缓存特征选择；2）依据重要性分数进行多级特征分类，并采用分级计算策略以降低计算量。

Result: 在NVIDIA A800-80GB上，相比RFlow，Stable Diffusion 3、3.5与FLUX分别平均加速2.80×、2.74×、3.17×，图像质量几乎无损。

Conclusion: 通过融合推测（未来）与历史信息，SpecDiff突破传统仅历史缓存的限制，改善速度-精度帕累托前沿，为高效扩散推理提供通用、免训练的加速方案。

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [56] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: 提出EDITS：利用隐含文本语义进行数据集蒸馏，通过VLM/LLM引入全局与本地语义原型，并以扩散模型在双原型指导下合成高质量小样本数据，显著提升高层语义与结构保持能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏多聚焦低层视觉特征（纹理、边缘），忽视图像中高层语义与结构，导致在语义复杂任务中性能受限。作者希望将图像隐含的语义信息显式化并用于蒸馏，以在更小数据规模下保留更强的任务相关语义。

Method: EDITS框架三步：1) 全局语义查询（Global Semantic Query）：利用VLM为原始图像生成外部文本描述，与图像特征融合，形成先验聚类缓冲区；2) 本地语义感知（Local Semantic Awareness）：从缓冲区选取代表性样本，构建图像原型与文本原型；文本原型由精心设计的提示词驱动LLM生成；3) 双原型引导（Dual Prototype Guidance）：在扩散模型中同时以图像原型与文本原型为条件，合成最终的蒸馏数据集。

Result: 在多项实验中，EDITS相较传统仅依赖视觉特征的蒸馏方法取得更优性能，表明引入语义原型和扩散生成能更好地保留高层语义与结构；论文提供开源代码以复现。

Conclusion: 通过联合利用VLM/LLM的语义与扩散生成，EDITS在数据集蒸馏中有效弥补了高层语义缺失的问题，实现更紧凑但性能竞争力更强的合成数据集。

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [57] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: 提出LamiGauss：将高斯Splatting辐射栅格化与包含层析倾角的检测器到世界坐标变换结合，用极少视角（3%）即可从层析投影重建高质量体数据，优于全视角迭代法。


<details>
  <summary>Details</summary>
Motivation: 层析（CL）适合板状结构（如芯片、复合电池）但几何限制使传统CT表现不佳；CL在稀疏视角下重建易出现伪影与质量下降，现有方法在效率与准确性上受限，需一种能直接从稀疏投影稳定高效重建的算法。

Method: 提出LamiGauss：1）构建含层析倾角的检测器-世界坐标变换模型；2）采用Gaussian Splatting辐射栅格化进行体表示与可微渲染；3）引入初始化策略，从初步重建中过滤典型层析伪影，避免为虚假结构分配冗余高斯，集中模型容量；4）直接以稀疏投影为监督进行优化。

Result: 在合成与真实数据上广泛实验，LamiGauss在极少视角（仅3%全视角）情况下即可达到或超过现有方法，包括在全数据集上优化的迭代重建基线；重建精度与效率均更优，伪影显著减少。

Conclusion: 将高斯Splatting与针对CL几何的变换模型及伪影抑制初始化相结合，可在高度稀疏视角下实现准确高效的层析重建，优于现有迭代法，适用于芯片与电池等板状结构的无损检测。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [58] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: 提出DAM4SAM：为SAM2引入“干扰项感知”的记忆模块与自省式管理，显著降低对相似物体的漂移、提升遮挡后再检测；并构建干扰项数据集DiDi。在13个基准上优于SAM2.1，10个创SOTA；在实时/边缘跟踪器中也带来明显增益。


<details>
  <summary>Details</summary>
Motivation: 记忆式视频分割（如SAM2）在分割基准上强，但在视觉目标跟踪情景下，易被相似物体（干扰项）诱导产生漂移、且遮挡后再检测能力不足。缺乏针对“干扰项”的机制与评测数据。

Method: 1) 设计干扰项感知的可插拔记忆模块与基于“自省”的记忆管理策略，使模型在写入/读取时区分目标与干扰项并抑制错误记忆；2) 将该模块集成到SAM2形成DAM4SAM；3) 构建Distractor-Distilled数据集DiDi用于含干扰项情景的分析与评测；4) 进一步将该记忆模块集成到实时跟踪器EfficientTAM与边缘跟踪器EdgeTAM以验证通用性。

Result: DAM4SAM在13个基准上整体优于SAM2.1，并在10个基准上达成SOTA；集成到EfficientTAM带来约11%提升，达到非实时SAM2.1-L的跟踪质量；集成到EdgeTAM带来约4%提升，显示良好可迁移性。

Conclusion: 面向跟踪的干扰项感知记忆与自省管理有效缓解相似物体导致的漂移并增强再检测能力，可作为通用可插拔组件，为多种架构与实时/边缘场景带来显著增益；DiDi数据集为含干扰项情景提供评测支撑。

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [59] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: 提出PelFANet：将原始骨盆X光与骨分割图双流融合，通过注意力块提升骨折分类；在AMERI数据集上对可见/不可见骨折均取得较高准确率与AUC，显示对隐匿骨折的泛化潜力。


<details>
  <summary>Details</summary>
Motivation: 标准X线对细微或放射学隐匿的骨盆骨折识别困难，常导致漏诊与延误。需要一种能结合全局影像上下文与解剖局部细节、提升对微弱征象敏感性的自动化方法。

Method: 提出双流注意力网络PelFANet：输入为原始骨盆X光与对应的骨分割图；通过Fused Attention Blocks在两路间迭代交换与精炼特征，兼顾全局上下文与局部解剖；采用两阶段训练，先分割再在分割引导下进行分类训练，实现解剖感知的特征学习。

Result: 在AMERI数据集上，对可见骨折：准确率88.68%、AUC 0.9334；对不可见骨折（未参与训练）泛化：准确率82.29%、AUC 0.8688，优于传统方法。

Conclusion: 解剖感知的双输入注意力架构能稳健提升骨盆骨折检测，尤其在征象细微的场景中具有临床应用潜力。

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [60] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: EvHand-FPV提出一种基于事件相机的第一人称视角单摄3D手部跟踪轻量化框架，结合合成3D标注与真实2D标注数据集，利用腕部几何ROI与端到端ROI偏移映射及多任务辅助几何分支，在保持精度的同时显著降低参数与算力，适用于XR端侧部署。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机方法在XR等资源受限设备上难以同时满足高精度、低延迟与低能耗。事件相机具备微秒级时域分辨率和毫瓦级功耗，天然适合快速动态与低延迟场景，但缺乏针对自我视角手部跟踪的基准与高效算法。

Method: 1) 构建事件型FPV数据集：合成训练数据带3D标注+真实事件数据带2D标注用于评测。2) 提出基于手腕的几何ROI定位手部区域。3) 设计端到端映射，将ROI偏移嵌入网络以避免显式重建并降算。4) 多任务学习：加入辅助几何特征分支，训练期提升表征，测试期无额外开销。5) 轻量化网络架构以适配端侧推理。

Result: 在真实FPV测试集上，2D-AUCp由0.77提升至0.85；参数量从11.2M降至1.2M（-89%）；单次推理FLOPs从1.648G降至0.185G（-89%）。在合成数据上3D-AUCp达0.84，显示在3D指标上也具竞争力。

Conclusion: EvHand-FPV在保持或提升精度的同时大幅降低模型规模与计算量，证明事件相机驱动的自我视角手部跟踪可在XR设备上高效部署；所发布的数据集与代码有望推动该方向研究与应用。

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [61] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出WARM模块，通过在跨注意力前后加入白化与着色变换，稳健生成少样本3D点云分割的原型，显著提升多基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有FS-PCS多用FPS等启发式方法构建原型，初始随机性大、鲁棒性差且原型生成过程欠研究；直接用注意力的可学习原型与支撑特征存在分布鸿沟，影响效果。

Method: 基于注意力的原型生成：设计WARM(White Aggregation and Restoration Module)，在跨注意力前进行白化，将支撑特征对齐到可学习原型的分布；注意力聚合后再进行着色，将得到的原型恢复到原始分布。该“白化-注意力-着色”夹心结构缓解分布不匹配，捕捉支撑样本间语义关系，生成判别性原型。

Result: 在多个少样本3D点云分割基准上取得SOTA，并以显著优势超越现有方法；实验广泛，验证鲁棒性与有效性（具体数值未在摘要中给出）。

Conclusion: 通过WARM解决注意力原型与支撑特征的分布错配问题，可稳健生成代表性原型，显著提升FS-PCS性能，证明关注原型生成环节的重要性。

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [62] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 提出自我推理校准（SRC）框架，通过“先理后答”的格式化微调、生成多样候选、用R-Scorer成对评分理据质量与事实一致性，并基于置信加权偏好微调，将理据与答案对齐，显著提升LVLM在感知、推理与泛化上的表现。


<details>
  <summary>Details</summary>
Motivation: LVLM尽管VQA能力强，但常出现理据与答案不一致，导致错误或不可靠的推理路径与结论；需要一种能系统性对齐“推理过程-最终答案”的训练与对齐机制，以提高可解释性与可靠性。

Method: 1) 理据微调：调整输出格式，使模型在无显式提示下先给出理据再给答案；2) 候选生成：从微调后的LVLM为每样本搜索多样化候选响应；3) R-Scorer评分：设计成对打分模型，对候选的理据质量与事实一致性进行评价；4) 置信加权偏好整理：以置信度加权的偏好数据进行偏好微调；5) 迭代校准：将对齐过程分解并迭代更新，以更好对齐理据与答案。

Result: 在多项基准上取得显著提升，覆盖感知、推理与泛化能力；表现表明理据导向的对齐策略有效改善LVLM的一致性与正确性。

Conclusion: 以理据为中心的对齐（先校准理据，再偏好微调答案）能缓解理据-答案不一致问题，提升LVLM的可靠性与泛化；SRC提供了可扩展的迭代框架与评分机制（R-Scorer）来系统优化这一对齐过程。

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [63] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: 提出AntiPure，一种对抗“净化-定制”流程的保护性微扰，能在净化后仍保留不可感知扰动并在定制阶段造成显著失真，作为净化方法的压力测试基线。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的强定制能力带来滥用风险（深伪、版权侵权）。现有“保护性微扰”可嵌入不可感知噪声防滥用，但易被净化去除，导致再度暴露。缺乏对“抗净化”能力的系统化问题定义与有效方法。

Method: 形式化“抗净化”任务，提出AntiPure：在“净化-定制”工作流中引入两种引导机制以嵌入可持久的不可感知扰动——(1)分块频域引导：在频域按patch抑制模型对高频成分的掌控，降低净化后模型可恢复的细节；(2)错误时间步引导：跨扩散去噪时间步施加误导，扰乱模型去噪策略。通过额外引导在净化设置下仍能存活并在定制阶段触发失真。

Result: 在代表性净化设置下，AntiPure实现最小感知差异与最大定制后失真，相比其他保护性微扰在“净化-定制”流程中更鲁棒，作为净化的压力测试工具表现最佳。

Conclusion: AntiPure揭示了净化环节的脆弱性，能够在保持感知质量的同时抵抗净化并在后续定制中有效干扰；为评估与强化净化策略提供基线和诊断工具。

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [64] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: 提出Noise Level Guidance (NLG)，一种无需额外数据/网络/反传、直接在初始噪声层面进行优化的通用方法，以提升扩散模型生成质量与提示一致性，且可与现有指导无缝结合并保持高效。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量与提示服从性受初始高斯噪声随机性的显著影响；现有噪声层优化需要额外数据、辅助网络或代价高的反传，实用性差，亟需一种简单高效、可泛化且无需额外训练的噪声优化策略。

Method: 在采样前对初始噪声进行指导：通过设计与“通用指导”（可为条件或无条件的多种指导信号）一致性的似然增大规则，直接调整初始噪声，使其更可能生成与指导一致的样本。该框架不引入新数据或网络，也不需要反向传播，能与现有扩散级别指导方法联合使用。

Result: 在五个标准基准上，NLG稳定提升了生成图像质量与输入条件的遵从度；同时保持计算效率，并与既有指导方法兼容叠加获得额外收益。

Conclusion: NLG为扩散模型提供了一个简单、通用、可扩展的噪声层优化范式，能在不增加训练和基础设施成本的前提下提升生成质量与一致性，具有较强的实用性与推广价值。

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [65] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: 提出PairTally基准，用于评估细粒度、意图驱动的视觉计数；包含681张高分辨率、两类别并存的图像，涵盖跨类与同类细分场景；系统性评测表明现有方法与VLM在细粒度与歧义情况下易偏离用户意图；该基准为诊断与改进选择性计数提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有计数模型（含类无关计数与大规模视觉语言模型）在粗粒度任务上有进展，但其能否依据用户意图在复杂场景中做细粒度、可选择的计数尚不明确，缺乏专门数据集与评测标准来揭示这类能力缺口。

Method: 构建PairTally数据集：681张高清图像，每张含两种对象类别，要求模型基于形状、尺寸、颜色或语义等微小差异选择并计数。设置两种难度：跨类别（差异明显）与类内细分（子类别相近）。基准上评测多类方法：示例驱动（exemplar-based）、语言提示模型、以及大型VLM；比较其在意图对齐与细粒度区分下的计数表现。

Result: 多种最先进模型在PairTally上表现不稳，尤其在细粒度和视觉歧义情形下难以准确执行用户意图的选择性计数；总体显示当前方法的可靠性与泛化不足。

Conclusion: PairTally为细粒度视觉计数提供了系统化评测平台，揭示了现有模型在意图对齐与细粒度辨析方面的显著短板，可作为诊断瓶颈与推动方法改进的基础。

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [66] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA是一种将大型视觉-语言模型的区域级多模态语义蒸馏到轻量级纯视觉检测器中的方法，通过对象级对齐实现少样本个性化检测的大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏多聚焦于密集或全局特征对齐，难以高效传递对象级语义，且往往依赖文本输入或修改教师结构，不利于部署。为了在不改动教师、推理无文本的前提下，把大模型的多模态语义注入到小型对象检测器中，以提升少样本个性化检测性能。

Method: 提出MOCHA框架：引入特征翻译模块将学生检测器特征映射到与教师共享的联合空间；设计双目标损失，同时约束（1）局部对象级对齐（学生与教师在区域层面的语义一致性），（2）全局关系一致性（对象间关系结构保持）；整个过程无需在推理时使用文本，也无需改动教师模型。

Result: 在四个少样本个性化检测基准上验证，较各类基线平均提升+10.1分；在保持紧凑结构的同时，性能可与更大的多模态模型相当。

Conclusion: 对象级跨架构对齐能有效把多模态语义迁移到轻量纯视觉检测器，MOCHA在少样本场景中实现显著增益且具备部署友好性。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [67] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: 提出改进版 YOLO-FEDER FusionNet，将通用目标检测与伪装目标检测特征融合，在复杂背景下大幅降低漏检率并提升mAP；最佳方案（YOLOv8l+来自DWD的中间FEDER特征）在IoU=0.5时FNR降至最多-39.1个百分点，mAP提升最多+62.8个百分点。


<details>
  <summary>Details</summary>
Motivation: 无人机在复杂背景（杂波、小目标、伪装）中难以与背景分离，通用检测器（如YOLO）在低纹理场景表现好，但在杂乱环境性能下降，需要融合伪装检测思想与更强特征表征来提升鲁棒性。

Method: 在原YOLO-FEDER FusionNet基础上系统升级：1) 训练数据：大规模、照片级真实感的合成数据为主，辅以少量真实样本；2) 特征融合：系统评估中间多尺度FEDER特征的贡献，提出改进的特征融合策略；3) 骨干网络：在多种YOLO家族骨干上进行对比与升级（含YOLOv8l），并结合来自FEDER的模块（如DWD）中间特征进行融合与检测。

Result: 实验证明：引入中间FEDER特征并升级骨干可显著提升检测。在最佳配置（YOLO-FEDER FusionNet + YOLOv8l骨干 + 来自DWD模块的FEDER特征）下，于IoU=0.5取得FNR最多下降39.1个百分点、mAP最多提升62.8个百分点，相较初始基线。

Conclusion: 融合伪装检测中间特征与更强YOLO骨干、并使用合成+少量真实数据训练，可显著增强复杂视觉条件下的无人机检测能力。该框架为在杂乱、低可分性场景中提升小目标与伪装目标检测提供了有效路径。

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [68] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-VL2 是一套面向全面多模态理解与推理的开源视觉-语言基础模型家族，在2B与8B规模上于图像与视频基准取得SOTA，覆盖细粒度感知到复杂推理；其通过高质量数据策展、渐进式训练范式（含思维-融合的SFT+RL）、以及稀疏MoE架构三大创新，实现对106个数据集的强竞争力，并在MMMU、MathVista等高难度推理任务与OpenCompass 4B档中夺得领先。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在大规模数据质量、训练流程与架构效率之间存在权衡，难以同时兼顾细粒度感知与复杂推理，并在多样化评测与中小参数量级的开源生态中仍有性能与效率缺口。SAIL-VL2旨在通过数据、训练、架构三方面系统性改进，提供高效、可扩展且强泛化的开源多模态基础模型。

Method: 1) 数据：构建大规模数据策展流水线，基于评分与过滤提升质量与分布覆盖（含描述、OCR、问答、视频）。2) 训练：以强视觉编码器SAIL-ViT为起点，先多模态预训练，再采用“思维-融合”的SFT-RL混合范式逐步增强能力。3) 架构：在致密LLM之外，引入高效稀疏Mixture-of-Experts设计，提高推理与训练效率。

Result: 在106个数据集上表现竞争力；在MMMU、MathVista等高难度推理基准上达SOTA；在OpenCompass排行榜中，SAIL-VL2-2B在4B参数规模档的已正式开源模型中排名第一；2B与8B规模均在图像与视频任务上取得领先。

Conclusion: 数据质量与分布优化、渐进式（含SFT+RL）训练框架以及稀疏MoE架构相结合，可在小中等参数规模下实现强多模态理解与推理能力；SAIL-VL2作为开源可扩展基座，为多模态社区提供高效且具竞争力的方案。

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [69] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: 提出一种名为 PROFUSEme 的多模态中间融合方法，结合 Cox 比例风险模型，从临床、影像与病理数据中学习跨模态交互，以预测前列腺癌根治术后的生化复发；在内部嵌套交叉验证中取得较高C-index 0.861（σ=0.112），在CHIMERA 2025验证榜单上为0.7103，优于晚期融合。


<details>
  <summary>Details</summary>
Motivation: 约30%接受根治性前列腺切除的患者会发生生化复发（PSA升高），与更高死亡率相关。术时若能更准确地早期预测复发，可支持及时个体化治疗决策，改善结局。现有方法往往单模态或采用晚期融合，未充分利用跨模态交互。

Method: 构建名为 PROFUSEme 的多模态生存分析框架：收集临床、影像（放射）、病理三类数据；采用中间融合策略学习跨模态嵌入并进行交互建模；以 Cox 比例风险回归作为生存预测头；与晚期融合配置进行对比评估；内部使用5折嵌套交叉验证，并在CHIMERA 2025挑战的验证榜单上进行外部验证。

Result: 与晚期融合相比，中间融合取得更优的生存预测性能：内部5折嵌套CV的平均C-index为0.861（σ=0.112）；在CHIMERA 2025验证榜单（保留集）上C-index为0.7103。

Conclusion: 跨模态交互的中间融合（PROFUSEme）能有效提升前列腺癌术后生化复发的早期预测性能；内部验证效果显著且在外部榜单上具有一定泛化能力，优于晚期融合配置。

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [70] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate 是一个基于 Wan 的统一角色动画与替换框架，能用参考视频的姿态与表情驱动单张角色图像生成高保真视频，并可将生成角色无缝替换入原视频，通过辅助 Relighting LoRA 实现环境光照与色调匹配，达到 SOTA 效果并将开源权重与代码。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时高保真地复现动作与表情、并在角色替换中实现光照与色调的无缝融合；任务通常割裂，缺乏统一表征与可控性。

Method: 在 Wan 模型上改造输入范式，区分参考条件与生成区域，实现多任务统一的符号化表征；采用空间对齐的骨架信号驱动身体动作，用从源图隐式提取的人脸特征重演表情；引入辅助的 Relighting LoRA，在替换时保持角色外观一致性的同时匹配场景光照与色调。

Result: 在角色动画与替换任务上达到了最新性能，生成视频在动作/表情可控性与表现力、以及与场景的光照色调一致性上优于现有方法。

Conclusion: Wan-Animate 实现了统一、可控且高保真的角色动画与替换流程，并通过 Relighting LoRA 达到更好的环境融合效果；计划开源模型权重与源码，促进社区使用与研究。

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [71] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: 提出VSE-MOT：用视觉-语言全局语义增强多目标跟踪，在低质量视频中显著提升（约8%–20%）。


<details>
  <summary>Details</summary>
Motivation: 现有MOT在真实世界低清视频（噪声、模糊、压缩损伤等）下性能大幅下降，说明仅凭局部视觉或高质量假设不够。引入全局视觉语义有望提升鲁棒性，弥补退化信息丢失。

Method: 1) 三分支架构：借助视觉-语言模型从图像提取全局视觉语义，并与查询向量融合；2) MOT-Adapter：将全局语义适配为MOT任务可用表征；3) VSFM（视觉语义融合模块）：提升语义与视觉特征的融合质量与效率。整体形成VSE-MOT框架，在跟踪管线中显式利用全局语义。

Result: 在真实低质量视频上，经大量实验验证，相比现有方法各项指标提升约8%–20%；同时在常规（非退化）场景中保持稳健表现，无明显性能损失。

Conclusion: 将视觉-语言模型的全局语义引入MOT并通过MOT-Adapter与VSFM有效融合，可显著缓解低质视频带来的退化，提升跟踪鲁棒性与精度，并具有对常规场景的兼容性。

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [72] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: 提出AD-DINOv3：首次将DINOv3适配到零样本异常检测，通过多模态对比学习、轻量适配器与异常感知校准模块，提升对细微异常的识别并在8个工业与医疗基准上达SOTA或更优。


<details>
  <summary>Details</summary>
Motivation: 现有ZSAD多依赖CLIP的图文相似度，但存在两大痛点：1) 预训练域与异常检测任务域不一致，导致特征错配；2) 预训练表征偏向全局语义，细粒度异常容易被当作正常前景而忽略。DINOv3在通用视觉表征上表现强，但尚未被系统性用于ZSAD，具有潜力和空白。

Method: 构建AD-DINOv3多模态框架：以DINOv3为视觉骨干提取patch tokens与CLS token；以CLIP文本编码器生成“正常/异常”提示的文本嵌入。为缩小域差，引入视觉与文本两侧的轻量适配器进行表征重校准，并通过多模态对比学习进行对齐。进一步提出异常感知校准模块（AACM），显式引导CLS token关注异常区域而非泛化的前景语义，提升可分辨性。

Result: 在8个工业与医疗数据集上广泛实验，AD-DINOv3稳定达到或超越当前SOTA，显示其作为通用零样本异常检测框架的有效性和优越性。

Conclusion: 将DINOv3成功适配到ZSAD，通过适配器对齐与AACM增强局部异常感知，显著提升零样本异常检测性能，并在多基准上验证了泛化与领先性。

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [73] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出用于弱监督音视解析的两项关键策略：EMA引导的伪监督生成稳定片段级掩码；类感知跨模态一致性损失对齐音频与视觉嵌入。在LLP与UnAV-100上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有AVVP方法多在视频级全局预测上做对比/协作学习，缺乏可靠的片段级监督与类感知的跨模态对齐，导致时间定位不稳与模态不一致。

Method: 1) EMA引导的伪监督：通过教师-学生范式，用EMA更新的教师模型产生高置信片段级掩码，并采用自适应阈值或top-k选择获取可靠伪标签，以提供稳定的时间监督。2) 类感知跨模态一致性（CMA）损失：在被判定为可靠的片段-类别对上，对齐音频与视觉嵌入，既保证模态一致，又保留时间结构。

Result: 在LLP与UnAV-100数据集上，多个指标上达成SOTA，表明片段级伪监督与类感知对齐显著提升AVVP性能。

Conclusion: 引入EMA伪监督的稳定时间指导与类感知跨模态一致性，有效缓解弱监督AVVP中时间不稳定与模态不匹配问题，带来SOTA表现，可作为后续AVVP研究的强基线。

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [74] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 提出将Soft MoE集成到跨传感器掩码自编码器（CSMAE）中，形成CSMoE，以在保持或提升表征能力的同时显著降低训练与推理的计算开销；并通过主题-气候描述符驱动的采样构建多样化训练集，实验在多任务上验证了更优的精度-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基础模型要么训练/推理计算量大、难以落地，要么表征能力有限，难以覆盖多传感器与多任务需求，需要一种既高效又具备跨模态表征能力的方案。

Method: 在基础模型中引入Soft Mixture-of-Experts：通过软路由让不同模态拥有专门专家，同时共享跨传感器的表示学习；以CSMAE为基座得到CSMoE。并提出基于“主题-气候”描述符的采样策略，构建具有代表性与多样性的训练集。

Result: 在场景分类、语义分割、基于内容的图像检索三类任务上，CSMoE在保持或提升精度的同时显著降低计算需求；与SOTA遥感基础模型相比，平均计算效率提升超过2倍，整体性能具有竞争力。

Conclusion: Soft MoE与跨传感器自监督相结合可在不牺牲准确性的前提下显著提升遥感基础模型的计算效率，实现更优的容量-精度-效率权衡；代码与数据构建流程将开源，具备可复现性与实用价值。

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [75] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: 提出一种具备级联配准机制的虚拟染色框架，专门解决训练中生成结果与“真值”染色切片错位导致的监督困难，显著提升多数据集上的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 临床病理诊断常需多种化学染色，但获取同一切片的成对、精确配准的多染色图像几乎不可行；化学染色会引入形变、信息丢失，使现有方法难以进行像素级监督，限制了虚拟染色在真实场景的应用。

Method: 构建一个鲁棒的虚拟染色框架，引入级联式（多阶段）配准模块，将生成结果与对应“真值”目标在训练过程中逐步校正空间错配，从而在存在未对齐或粗配准数据时仍能进行有效监督与优化。

Result: 在五个数据集上超越现有SOTA：内部数据平均提升3.2%，外部数据平均提升10.1%；在错配显著的数据集上，峰值信噪比（PSNR）相较基线提升23.8%。

Conclusion: 通过级联配准缓解了虚拟染色对严格配对数据的依赖，显著增强跨数据集的泛化与鲁棒性，简化数据获取流程并为虚拟染色的临床落地提供新思路。

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [76] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 研究评估美颜/平滑滤镜对深度伪造与人脸拼接（morphing）检测器的影响，发现滤镜会明显削弱多种SOTA检测器在基准数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中普及的美颜滤镜改变人脸外观，可能影响人脸图像/视频的可信度与自动化分析可靠性。尤其在需识别深度伪造与人脸拼接攻击的安全场景，这些滤镜可能成为新的规避手段，因此需系统量化其影响。

Method: 在公开基准数据集上，选取多种代表性SOTA深伪与morphing检测器；在原始数据与施加多种“平滑/美容”滤镜后的数据上分别测试；对比性能变化，分析不同滤镜类型与强度下的鲁棒性差异。

Result: 在多种滤镜与数据集配置中，检测器性能普遍下降；平滑类美颜（磨皮、细节去除）尤为显著地削弱判别特征，导致误检率上升与检出率下降，暴露出现有模型对外观增强的脆弱性。

Conclusion: 美颜滤镜可成为绕过深伪与morphing检测的有效手段，现有检测器对这类分布偏移不鲁棒。应开发对美颜/平滑变换不敏感的鲁棒检测模型，并在训练与评测中纳入此类增强，完善对真实应用中社交媒体场景的防御。

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [77] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: 综述MARS2 2025多模态推理挑战：发布两数据集（Lens与AdsQA），设三赛道（VG-RS、VQA-SA、VR-Ads），评测40+基线、接收40+有效投稿，面向真实与专业场景推进MLLM多模态推理。


<details>
  <summary>Details</summary>
Motivation: 多模态与LLM发展迅速但评测碎片化，研究者难以追踪SOTA；现有通用测试床增多却缺少面向真实应用与垂直领域的系统性评测，因此组织挑战赛与大型基准，推动在现实与广告创意等场景中的多模态推理能力。

Method: 构建并公开两套测试集：Lens（覆盖12个日常情境的一般推理）与AdsQA（广告视频的领域特定推理）；设立三条赛道：视觉定位（VG-RS）、具空间意识的VQA（VQA-SA）、广告视频视觉推理（VR-Ads）；汇总并评测40+通用与任务特定模型作为基线；开放竞赛并收集参赛方法与提交。

Result: 76支队伍报名，1200+提交中筛得40+有效条目纳入排行榜；公开40+基线与15+参赛方法实现；给出各赛道排行榜与评测结果（细节在官网与GitHub）。

Conclusion: MARS2 2025通过针对真实与专业应用的基准和竞赛，推动MLLM在视觉定位、空间感知VQA与广告视频推理的研究；资源与排行榜持续更新，将促进社区对多模态推理SOTA的跟踪与方法进步。

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [78] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出HAID层次抽象图像数据集，用原始栅格图像在多抽象级别生成由基本图形组成的抽象图，系统比较抽象与栅格在分类/分割/检测上的差距与潜力，分析性能差距成因与抽象表达的语义承载能力。


<details>
  <summary>Details</summary>
Motivation: 抽象图像（由原始几何元构成）可被深度模型理解，但其表征性能往往不如传统栅格图像；缺乏系统性数据与基准来量化不同抽象层级能保留多少高层语义，以及差距来源。

Method: 构建HAID：从普通栅格图像自动生成多层级抽象图（由原始几何形状表示），并在分类、语义分割、目标检测等任务上，用常规视觉模型分别训练与评测抽象与栅格版本；跨层级分析性能、误差类型与语义保留度，讨论抽象图像作为视觉语义载体的有效性。

Result: 抽象图可传递一定高层语义并在多任务上取得可观表现，但整体仍落后于栅格表示；性能随抽象程度提高而下降，显示细粒度纹理/细节的缺失是主要瓶颈，不同任务对抽象层级的敏感度不同。

Conclusion: 抽象图像是可行的语义表达形式，适合研究语义与形状偏置，但当前在主流任务上仍不及栅格；需要在抽象生成、形状/结构编码与模型设计上改进，或结合混合表示，以缩小性能差距。

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [79] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: 提出BEVUDA++，一个几何感知的教师-学生框架，缓解多几何空间中的域偏移问题，实现多视角BEV 3D目标检测的无监督域自适应，在多种跨域场景取得SOTA（如昼夜适配提升12.9% NDS、9.5% mAP）。


<details>
  <summary>Details</summary>
Motivation: 视觉BEV感知虽在效率/精度上进展显著，但跨域迁移时性能大幅下降。BEV方法包含图像→深度/体素/BEV等多组件与多几何空间，域偏移在各空间累积，现有研究缺乏对BEV 3D检测域自适应的系统性解决方案。

Method: 提出几何感知教师-学生框架BEVUDA++：1) 可靠深度教师RDT：将目标域LiDAR与深度预测融合，并基于不确定性估计生成深度感知信息，提升体素与BEV特征对目标域的表征；2) 几何一致学生GCS：将2D、3D体素、BEV等多空间特征映射到统一几何嵌入空间，缩小源/目标域分布差距；3) 不确定性引导的EMA（UEMA）：用先验不确定性指导教师参数的指数滑动更新，降低因域偏移导致的误差累积。

Result: 在四个跨域场景上进行全面实验，BEV 3D检测取得SOTA；例如昼夜适配中NDS提升12.9%、mAP提升9.5%。

Conclusion: 通过RDT提供可靠深度监督、GCS在统一几何嵌入中对齐多空间特征，并辅以UEMA稳定训练，显著缓解BEV域偏移累积问题，在多跨域场景验证了方法有效性与鲁棒性。

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [80] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: 提出STEP框架（SuperToken + Early-Pruning）以降低ViT在语义分割中的计算与显存成本：通过CNN策略网络动态合并补丁为超级补丁，并在编码器中早退出高置信超级token，实现2.5–4x计算降幅，速度提升，精度下降≤2%。


<details>
  <summary>Details</summary>
Motivation: ViT在高分辨率语义分割上表现卓越但代价高（计算/内存）。需要在尽量保持精度的前提下，大幅减少token数量与推理开销，适配如1024×1024等高分辨率场景。

Method: 提出STEP混合token压缩框架：1) dCTS轻量CNN策略网络进行动态patch合并，形成superpatch/supertoken；2) 在编码器块中加入早退出机制，对高置信supertoken提前停止后续计算与传播；3) 在ViT-L骨干上评估，并与标准16×16分块方案比较。

Result: 仅用dCTS合并即可将token数相对16×16分块减少2.5倍，带来2.6x算力降低与3.4x吞吐提升（ViT-L）。完整STEP进一步将复杂度降至4x、推理速度提升1.7x，精度最大下降不超过2%。约40%的token可在到达最终编码层前被高置信早停。

Conclusion: STEP在不显著牺牲精度的情况下显著提高ViT语义分割的效率：动态合并与早退出相结合，适合高分辨率输入和大模型部署，表明token级自适应计算在视觉Transformer中有效。

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [81] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 提出DVU框架与GRT方法，实现高帧率视频理解并配套密集时序基准DIVE；通过运动补偿的门控残差标记化与场景内语义合并，降低标记化计算与token数量，取得优于更大VLLM的性能，且随FPS提升而扩展良好。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM多用低帧率采样（均匀/关键帧），为避免对每帧标记化的线性token增长与冗余计算，但这丢失密集时序信息，难以处理如课堂讲解这类几乎每帧都有关键信息且需精确时间对齐的任务。缺乏针对密集时序推理的基准也限制了评估。

Method: 提出Dense Video Understanding (DVU)范式与两阶段Gated Residual Tokenization (GRT)：(1) 运动补偿的帧间门控标记化（Motion-Compensated Inter-Gated Tokenization），通过像素级运动估计跳过静态区域，实现token与计算的亚线性增长；(2) 语义-场景内标记合并（Semantic-Scene Intra-Tokenization Merging），在场景内将静态区域跨帧的token融合，压缩冗余同时保持动态语义。并提出密集信息视频评测集DIVE以衡量密集时序推理能力。

Result: 在DIVE上，GRT优于更大规模的VLLM基线，且性能随FPS上升而提升，显示对高帧率输入的可扩展性与效率。

Conclusion: 密集时序信息对视频理解至关重要。GRT在降低标记化开销与token冗余的同时，实现可扩展的高FPS视频理解，并通过DIVE验证其有效性与优势。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [82] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出Cinéaste长篇电影理解基准，含3,119道多选题，覆盖细粒度语境推理，揭示现有多模态大模型在长程叙事推理上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频-语言基准多聚焦短片识别或模板化问答，无法充分评估对长篇叙事的细粒度推理与深层理解，存在评价空白。

Method: 构建Cinéaste数据集：从200部电影的1,805个场景生成3,119道多选题；利用GPT-4o融合视觉描述、字幕、场景标题与摘要生成多样化、语境丰富的问题。为保证质量，设计两阶段过滤：1) 上下文依赖过滤，确保问题必须依赖视频情境；2) 语境真实性过滤，以影片内容校验事实一致性，缓解幻觉。覆盖五类细粒度语境推理。

Result: 对现有MLLM的实验表明其在该基准上表现不佳；顶级开源模型准确率仅63.15%，主要瓶颈在长程时间与叙事关系推理。

Conclusion: Cinéaste揭示并量化了长篇电影理解中的细粒度语境推理难点，表明当前模型在长程叙事理解方面仍有显著差距，亟需在长时序建模与上下文整合能力上取得突破。

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [83] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam 提出首个多学科文本生成图像“考试”基准，含1,000题、10学科、四层题型体系，配真值图与细粒度评分点，用于严评语义正确性与视觉可信度；SOTA 模型严格得分<15%，多数近0%，凸显任务难度与AGI差距。


<details>
  <summary>Details</summary>
Motivation: 现有考试式基准多评理解与推理，生成类基准偏重常识与视觉概念展示，缺乏对“严谨作图/绘制题”的系统评测，无法检验模型在知识整合、推理与可控生成上的综合能力与可靠性。

Method: 构建GenExam基准：覆盖10个学科、1,000个考试式文本到图像题目，按四级分类法组织；为每题提供标准答案图（ground-truth）与细粒度评分点，设计严格评分（semantic correctness与visual plausibility），并在多模型上进行实验评测。

Result: 在GenExam上，包含GPT-Image-1与Gemini-2.5-Flash-Image在内的SOTA模型严格评分均低于15%，大多数模型接近0%，显示当前生成模型在满足考试式约束与精确作图方面明显不足。

Conclusion: GenExam将图像生成问题转化为“考试”，提供可量化、可比较的严格评测框架，以检验模型的知识整合、推理与生成能力；现阶段模型表现低迷，说明距离通用AGI仍有显著差距，GenExam为未来方法改进与能力诊断提供方向。

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>
