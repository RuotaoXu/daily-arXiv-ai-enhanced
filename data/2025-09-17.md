<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 108]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction](https://arxiv.org/abs/2509.12242)
*Mustafa Khanbhai,Giulia Di Nardo,Jun Ma,Vivienne Freitas,Caterina Masino,Ali Dolatabadi,Zhaoxun "Lorenz" Liu,Wey Leong,Wagner H. Souza,Amin Madani*

Main category: cs.CV

TL;DR: 提出人机协同的U‑Mamba分割流程，在120例乳腺MRI上实现对全乳、腺体与肿瘤的高精度分割与3D重建，显著提升临床规划、术中导航与患者沟通，展示跨数据集泛化潜力。


<details>
  <summary>Details</summary>
Motivation: 传统分割模型在不同成像序列与患者群体间泛化不佳，限制了术前规划与3D重建在临床中的广泛应用；需要一种能在多场景下稳定表现、且可融入临床反馈循环的方法。

Method: 对2018-2023年120例乳腺MRI进行匿名化与人工标注；完成T1与DCE共配准，分割全乳、纤维腺体、肿瘤；采用人机闭环方式用U‑Mamba进行自动分割并迭代修正；以Dice系数评估与真实标注的重叠；用ITK‑SNAP进行3D可视化，并通过临床医生与患者访谈评估临床相关性。

Result: 在T1像上，U‑Mamba的DSC：全器官0.97(±0.013)、纤维腺体0.96(±0.024)、肿瘤0.82(±0.12)；可生成准确3D重建与复杂解剖可视化；访谈显示其提升术前规划、术中导航、决策支持与患者教育和沟通。

Conclusion: 人机协同的U‑Mamba分割框架在乳腺MRI上取得高精度分割与3D重建，并表现出跨数据集与成像情景的泛化潜力，可增强临床可视化、改进术前规划并促进患者共同决策，具有向更广泛医学应用推广的可行性。

Abstract: Effective preoperative planning requires accurate algorithms for segmenting
anatomical structures across diverse datasets, but traditional models struggle
with generalization. This study presents a novel machine learning methodology
to improve algorithm generalization for 3D anatomical reconstruction beyond
breast cancer applications. We processed 120 retrospective breast MRIs (January
2018-June 2023) through three phases: anonymization and manual segmentation of
T1-weighted and dynamic contrast-enhanced sequences; co-registration and
segmentation of whole breast, fibroglandular tissue, and tumors; and 3D
visualization using ITK-SNAP. A human-in-the-loop approach refined
segmentations using U-Mamba, designed to generalize across imaging scenarios.
Dice similarity coefficient assessed overlap between automated segmentation and
ground truth. Clinical relevance was evaluated through clinician and patient
interviews. U-Mamba showed strong performance with DSC values of 0.97
($\pm$0.013) for whole organs, 0.96 ($\pm$0.024) for fibroglandular tissue, and
0.82 ($\pm$0.12) for tumors on T1-weighted images. The model generated accurate
3D reconstructions enabling visualization of complex anatomical features.
Clinician interviews indicated improved planning, intraoperative navigation,
and decision support. Integration of 3D visualization enhanced patient
education, communication, and understanding. This human-in-the-loop machine
learning approach successfully generalizes algorithms for 3D reconstruction and
anatomical segmentation across patient datasets, offering enhanced
visualization for clinicians, improved preoperative planning, and more
effective patient education, facilitating shared decision-making and empowering
informed patient choices across medical applications.

</details>


### [2] [RU-Net for Automatic Characterization of TRISO Fuel Cross Sections](https://arxiv.org/abs/2509.12244)
*Lu Cai,Fei Xu,Min Xian,Yalei Tang,Shoukun Sun,John Stempien*

Main category: cs.CV

TL;DR: 用CNN对TRISO燃料颗粒截面显微图进行自动分割，提出RU‑Net并在>2000张标注数据上训练，IoU优于U‑Net/ResNet/Attention U‑Net，从而加速客观化后照显微分析。


<details>
  <summary>Details</summary>
Motivation: 手工从成千上万颗TRISO颗粒的后照显微图中统计核芯膨胀、缓冲层致密化等形貌变化既费时又主观，需要自动、客观且可扩展的分割方法以提升数据处理效率与一致性。

Method: 构建包含2000+张辐照后TRISO粒子截面及其标注的大型数据集；比较多种CNN分割架构：U‑Net、ResNet、Attention U‑Net与新提出的RU‑Net；以交并比（IoU）作为主要评估指标，对不同层（核芯、缓冲层等）进行自动分割。

Result: 在同一数据与评估下，RU‑Net取得最佳IoU，优于对比的U‑Net、ResNet与Attention U‑Net；表明CNN能稳定分割TRISO多层结构。

Conclusion: CNN（尤其RU‑Net）能显著加速TRISO截面分析，减少人工工作量并提高客观性，可用于大规模后照形貌定量统计与性能评估。

Abstract: During irradiation, phenomena such as kernel swelling and buffer
densification may impact the performance of tristructural isotropic (TRISO)
particle fuel. Post-irradiation microscopy is often used to identify these
irradiation-induced morphologic changes. However, each fuel compact generally
contains thousands of TRISO particles. Manually performing the work to get
statistical information on these phenomena is cumbersome and subjective. To
reduce the subjectivity inherent in that process and to accelerate data
analysis, we used convolutional neural networks (CNNs) to automatically segment
cross-sectional images of microscopic TRISO layers. CNNs are a class of
machine-learning algorithms specifically designed for processing structured
grid data. They have gained popularity in recent years due to their remarkable
performance in various computer vision tasks, including image classification,
object detection, and image segmentation. In this research, we generated a
large irradiated TRISO layer dataset with more than 2,000 microscopic images of
cross-sectional TRISO particles and the corresponding annotated images. Based
on these annotated images, we used different CNNs to automatically segment
different TRISO layers. These CNNs include RU-Net (developed in this study), as
well as three existing architectures: U-Net, Residual Network (ResNet), and
Attention U-Net. The preliminary results show that the model based on RU-Net
performs best in terms of Intersection over Union (IoU). Using CNN models, we
can expedite the analysis of TRISO particle cross sections, significantly
reducing the manual labor involved and improving the objectivity of the
segmentation results.

</details>


### [3] [Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture](https://arxiv.org/abs/2509.12247)
*Abigail R. Cohen,Yuming Sun,Zhihao Qin,Harsh S. Muriki,Zihao Xiao,Yeonju Lee,Matthew Housley,Andrew F. Sharkey,Rhuanito S. Ferrarezi,Jing Li,Lu Gan,Yongsheng Chen*

Main category: cs.CV

TL;DR: 提出一个分层、能效优先的影像管线：AE做早期异常预警，随后用两种状态估计模块（VI+RF vs. ViT）在效率—精度之间取舍，并量化能耗与浪费氮的“隐含能”对比，支持资源受限环境下的作物营养监测。


<details>
  <summary>Details</summary>
Motivation: 营养管理对作物产量与资源可持续至关重要，但传统化验慢、难以实时优化；影像表型虽快却算力开销大，不利于边缘部署。需要一种既能早预警又兼顾能效与精度的可落地方案。

Method: 在营养梯度（100%、50%、25%肥）与多光谱成像条件下，构建分层管线：1) 自编码器进行无监督异常检测（早期预警）；2) 两种状态估计模块对新鲜重、干重与组织营养进行回归：a) 植被指数特征+随机森林（轻量）；b) 端到端整图Vision Transformer（高精度）。并对各方法进行能耗—精度与“隐含能”（浪费氮）比较分析。

Result: AE在移栽后第9天即可高效检测到T3（25%肥）样本，净检测率73%，能耗远低于因未检测导致的氮浪费所对应的能量。状态估计中，ViT在P、Ca预测上优于RF（R²：0.61 vs 0.58；0.48 vs 0.35），但能耗更高，呈现明显效率—精度权衡。

Conclusion: 分层、模块化的影像诊断可在边缘设备实现早期异常预警与营养状态估计；根据场景可选择RF或ViT模块，在能耗与精度间权衡，具有促进农业可持续与资源高效利用的潜力。

Abstract: Efficient nutrient management is critical for crop growth and sustainable
resource consumption (e.g., nitrogen, energy). Current approaches require
lengthy analyses, preventing real-time optimization; similarly, imaging
facilitates rapid phenotyping but can be computationally intensive, preventing
deployment under resource constraints. This study proposes a flexible, tiered
pipeline for anomaly detection and status estimation (fresh weight, dry mass,
and tissue nutrients), including a comprehensive energy analysis of approaches
that span the efficiency-accuracy spectrum. Using a nutrient depletion
experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer
strength) and multispectral imaging (MSI), we developed a hierarchical pipeline
using an autoencoder (AE) for early warning. Further, we compared two status
estimation modules of different complexity for more detailed analysis:
vegetation index (VI) features with machine learning (Random Forest, RF) and
raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated
high-efficiency anomaly detection (73% net detection of T3 samples 9 days after
transplanting) at substantially lower energy than embodied energy in wasted
nitrogen. The state estimation modules show trade-offs, with ViT outperforming
RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at
higher energy cost. With our modular pipeline, this work opens opportunities
for edge diagnostics and practical opportunities for agricultural
sustainability.

</details>


### [4] [Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics](https://arxiv.org/abs/2509.12248)
*Yuriel Ryan,Rui Yang Tan,Kenny Tsu Wei Choo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: PixelHumor 提出一个含 2,800 个多格漫画、带注释的基准，用于评估大规模多模态模型在幽默理解与叙事序列推理上的能力；现有顶尖模型在面板排序仅约 61% 准确率，显著落后于人类，显示多模态线索整合与叙事理解仍存明显短板。


<details>
  <summary>Details</summary>
Motivation: 尽管幽默理解是社会智能的重要组成，但当前 LMM 在多模态幽默与叙事连贯性方面表现不足，缺乏系统性的评测框架来量化这些能力差距。

Method: 构建 PixelHumor 基准：收集并标注 2,800 个多格漫画，设计任务（如面板排序）以检验模型对视觉—文本线索的综合理解与叙事推理；用多款最先进 LMM 进行系统实验评测。

Result: 顶尖模型在面板排序任务上仅达约 61% 准确率，远低于人类水平，显示模型在将视觉与文本信息整合为连贯叙事并捕捉幽默点上存在显著性能缺口。

Conclusion: PixelHumor 提供了一个严谨的评测框架，揭示并量化了 LMM 在多模态幽默与叙事推理上的局限，可推动更善于社会化交互的多模态模型研发。

Abstract: Understanding humor is a core aspect of social intelligence, yet it remains a
significant challenge for Large Multimodal Models (LMMs). We introduce
PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed
to evaluate LMMs' ability to interpret multimodal humor and recognize narrative
sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for
instance, top models achieve only 61% accuracy in panel sequencing, far below
human performance. This underscores critical limitations in current models'
integration of visual and textual cues for coherent narrative and humor
understanding. By providing a rigorous framework for evaluating multimodal
contextual and narrative reasoning, PixelHumor aims to drive the development of
LMMs that better engage in natural, socially aware interactions.

</details>


### [5] [OnlineHOI: Towards Online Human-Object Interaction Generation and Perception](https://arxiv.org/abs/2509.12250)
*Yihong Ji,Yunze Liu,Yiyao Zhuo,Weijiang Yu,Fei Ma,Joshua Huang,Fei Yu*

Main category: cs.CV

TL;DR: 提出OnlineHOI框架，面向在线人-物交互的生成与感知，基于Mamba与记忆机制，在Core4D、OAKINK2与HOI4D的在线任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有HOI方法大多离线，允许利用未来时刻信息，然而真实场景只能访问当前与历史信息；离线方法在在线设置下表现不佳，亟需面向流式数据与历史整合的在线方案。

Method: 定义两项新任务：在线HOI生成与在线HOI感知；提出基于Mamba的OnlineHOI架构，利用其对流式序列的建模能力，并加入Memory机制高效融合历史信息，实现只依赖过去与当前帧的预测。

Result: 在Core4D与OAKINK2的在线生成任务，以及HOI4D的在线感知任务上取得最新SOTA性能。

Conclusion: 面向在线场景的Mamba+记忆机制能有效建模HOI流式数据，显著优于将离线方法直接用于在线的做法，验证了所提框架与任务设置的有效性。

Abstract: The perception and generation of Human-Object Interaction (HOI) are crucial
for fields such as robotics, AR/VR, and human behavior understanding. However,
current approaches model this task in an offline setting, where information at
each time step can be drawn from the entire interaction sequence. In contrast,
in real-world scenarios, the information available at each time step comes only
from the current moment and historical data, i.e., an online setting. We find
that offline methods perform poorly in an online context. Based on this
observation, we propose two new tasks: Online HOI Generation and Perception. To
address this task, we introduce the OnlineHOI framework, a network architecture
based on the Mamba framework that employs a memory mechanism. By leveraging
Mamba's powerful modeling capabilities for streaming data and the Memory
mechanism's efficient integration of historical information, we achieve
state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as
well as the online HOI4D perception task.

</details>


### [6] [EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces](https://arxiv.org/abs/2509.12258)
*Li Kun,Milena Radenkovic*

Main category: cs.CV

TL;DR: 论文讨论深度学习与深伪（deepfake）技术在社会层面的利弊，重点强调其对隐私、名誉、选举与国家安全的风险。


<details>
  <summary>Details</summary>
Motivation: 深度学习广泛应用并带来便利的同时，深伪技术迅速发展并渗透日常生活，出现隐私泄露、人物形象被冒用、社会信任受损、政治操纵等问题，亟需系统性审视与治理。

Method: 基于综述与论述的方式，从应用现状与社会影响两个维度，阐释深伪技术如何生成以假乱真的图像/视频并影响人脸识别与公众舆论；未见实验设计或量化评估。

Result: 指出深伪可生成难以分辨的伪造内容，可能削弱人脸识别系统，诱导公众误判，导致隐私被窃取、名誉受损，并被用于干扰选举、抹黑公众人物，危及国家政治与经济秩序。

Conclusion: 深伪技术的不当使用对社会具有显著负面效应，需要提高警觉、完善监管与技术对策，以减轻对隐私、名誉、社会信任与国家安全的冲击。

Abstract: Currently, deep learning has been utilised to tackle several difficulties in
our everyday lives. It not only exhibits progress in computer vision but also
constitutes the foundation for several revolutionary technologies. Nonetheless,
similar to all phenomena, the use of deep learning in diverse domains has
produced a multifaceted interaction of advantages and disadvantages for human
society. Deepfake technology has advanced, significantly impacting social life.
However, developments in this technology can affect privacy, the reputations of
prominent personalities, and national security via software development. It can
produce indistinguishable counterfeit photographs and films, potentially
impairing the functionality of facial recognition systems, so presenting a
significant risk.
  The improper application of deepfake technology produces several detrimental
effects on society. Face-swapping programs mislead users by altering persons'
appearances or expressions to fulfil particular aims or to appropriate personal
information. Deepfake technology permeates daily life through such techniques.
Certain individuals endeavour to sabotage election campaigns or subvert
prominent political figures by creating deceptive pictures to influence public
perception, causing significant harm to a nation's political and economic
structure.

</details>


### [7] [A Modern Look at Simplicity Bias in Image Classification Tasks](https://arxiv.org/abs/2509.12265)
*Xiaoguang Chang,Teng Wang,Changyin Sun*

Main category: cs.CV

TL;DR: 研究CLIP中的“简单性偏置”（SB）与图像分类性能的关系：提出一种频率感知的复杂度度量，验证其比既有度量更稳健；在零样本与微调情景下发现SB与任务表现的多样关联，如更强SB更利于OOD泛化但不利于对抗鲁棒性；提示应将模型归纳偏置与任务特性对齐。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明神经网络的简单性偏置有助泛化，但过强会损害复杂任务表现，且不同任务对SB需求不同。然而以往多基于小模型或合成任务，既有复杂度度量难以直接用于大模型；CLIP等大型视觉语言模型中的SB如何度量及其与多类图像任务表现的关系仍不清楚。

Method: 1）理论分析既有复杂度/SB度量在大模型场景的局限；2）提出频率感知的SB度量以捕捉更细粒度的简单性差异；3）在CLIP模型上应用两种最新的SB调控方法进行对比评估，用所提度量与既有度量比较一致性与信息量；4）系统考察不同SB水平的CLIP在多种图像分类任务（零样本与微调）中的表现，包括OOD泛化与对抗鲁棒性。

Result: 频率感知度量在区分与排序不同SB水平的CLIP模型上更稳定、信息量更高；跨任务实验显示存在多样化关系：更强SB与OOD泛化正相关，但与对抗鲁棒性相关性较弱或负相关；不同任务情境（零样本 vs 微调）对最佳SB强度的需求不同。

Conclusion: 提出并验证了适用于大模型（CLIP）的频率感知SB度量，优于以往指标；SB与任务性能并非单调一致，需根据任务属性选择与调节SB；将模型归纳偏置与目标任务特征对齐可获得更佳表现，尤其提升OOD泛化而不必然提升对抗鲁棒性。

Abstract: The simplicity Bias (SB) of neural networks, i.e.\ their tendency to
represent simple functions, is a key factor in their generalization
capabilities. Recent studies show that an excessive SB may harm performance on
complex tasks, and the need for this bias varies across tasks. Many of these
studies focus on simple models or synthetic tasks. It remains challenging to
measure the SB in large models and little is known about the relevance of the
SB to various image classification tasks.
  In this paper, we investigate the relationship between the SB in CLIP models
and their performance across image classification tasks. First, we
theoretically analyze the potential limitation of existing measures of
complexity that have been used to characterize small models. To address this,
we propose a frequency-aware measure capturing finer-grained SB differences. We
validate this measure on CLIP models subjected to two recent SB-modulation
methods, demonstrating that it is more informative and consistent than previous
measures. Second, we examine the relation between the SB of those models and
their performance across a range of image classification tasks, including
zero-shot and fine-tuning settings. These experiments reveal a range of
behaviors. For example, a stronger SB correlates with a better performance on
OOD generalization than on adversarial robustness. These results highlight the
benefits of aligning a model's inductive biases with the characteristics of the
target task.

</details>


### [8] [GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions](https://arxiv.org/abs/2509.12277)
*Mehdi Yousefzadeh,Parsa Esfahanian,Sara Rashidifar,Hossein Salahshoor Gavalan,Negar Sadat Rafiee Tabatabaee,Saeid Gorgin,Dara Rahmati,Maryam Daneshpazhooh*

Main category: cs.CV

TL;DR: GraphDerm 将图神经网络与图像特征、毫米级尺度校准与患者元数据融合，用于ISIC皮肤镜多分类，在AUC上显著优于仅图像模型，并可在稀疏图下保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤镜AI多忽略患者元数据与物理尺度信息，限制了几何解析与临床可用性。作者希望把图像、真实物理尺度和人群层面关系整合，提升诊断性能并探索高效部署。

Method: 1) 整理ISIC 2018/2019并合成含尺的图像及精确掩膜；2) 以SE-ResNet-18 U-Net分割病灶与尺，3) 通过尺掩膜的两点相关函数，用轻量1D-CNN回归像素-毫米比例；4) 由病灶掩膜计算真实尺度几何描述符（面积、周长、回转半径）；5) 节点特征用EfficientNet-B3图像嵌入；6) 边基于元数据/几何相似性（全权重或阈值稀疏）；7) 用谱GNN做半监督节点分类；8) 以图像-only ANN作基线。

Result: 分割Dice：尺0.904、病灶0.908；尺度回归MAE 1.5px（RMSE 6.6）。图模型AUC 0.9812；阈值后仅保留约25%边仍有AUC 0.9788；图像-only基线AUC 0.9440；各类别AUC多在0.97–0.99。

Conclusion: 将尺度校准、病灶几何与元数据统一到人群图中，可在ISIC-2019上显著超越仅图像方法；稀疏图几乎不损性能，有利于高效部署。未来将改进可学习的边语义并在更广基准上验证。

Abstract: Introduction. Dermoscopy aids melanoma triage, yet image-only AI often
ignores patient metadata (age, sex, site) and the physical scale needed for
geometric analysis. We present GraphDerm, a population-graph framework that
fuses imaging, millimeter-scale calibration, and metadata for multiclass
dermoscopic classification, to the best of our knowledge the first ISIC-scale
application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,
synthesize ruler-embedded images with exact masks, and train U-Nets
(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are
regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.
From lesion masks we compute real-scale descriptors (area, perimeter, radius of
gyration). Node features use EfficientNet-B3; edges encode metadata/geometry
similarity (fully weighted or thresholded). A spectral GNN performs
semi-supervised node classification; an image-only ANN is the baseline.
Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale
regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a
thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440
for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99
range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in
a population graph yields substantial gains over image-only pipelines on
ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient
deployment. Scale-aware, graph-based AI is a promising direction for
dermoscopic decision support; future work will refine learned edge semantics
and evaluate on broader curated benchmarks.

</details>


### [9] [PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models](https://arxiv.org/abs/2509.12278)
*Wanru Zhuang,Wenbo Li,Zhibin Lan,Xu Han,Peng Li,Jinsong Su*

Main category: cs.CV

TL;DR: 提出位置感知的文本图像机器翻译（PATIMT），在传统TIMT基础上同时输出译文与文本框，实现细粒度、保版式的翻译；构建包含10类场景的PATIMTBench与1200条高质测试集，并提出自适应OCR精炼流水线；微调后小型LVLM在两子任务上达SOTA，具备可扩展与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有TIMT多仅给出整图文本的译文，忽略文本定位（边界框）、版式保持与多样化真实场景，导致难以用于需要对齐和落地的应用（如文档本地化、UI翻译、地图/指示牌翻译等）。需要一个基准与方法同时解决“翻译+定位”，并能在多场景下可靠评测。

Method: 1) 定义PATIMT任务，包含区域级翻译与具定位的整图翻译两子任务；2) 构建PATIMTBench：覆盖10种真实场景的数据集；3) 提出自适应图像OCR精炼流水线：按场景选择合适OCR工具并对文本密集图像结果进行精炼；4) 构建1200例人工标注、复审的高质量测试集；5) 使用所构建数据微调紧凑型LVLM并进行评测。

Result: 微调后的小型LVLM在区域级翻译和整图带定位翻译两项上取得SOTA；实验显示训练数据具有良好的可扩展性与跨场景泛化能力。

Conclusion: PATIMT将TIMT扩展为可定位、可保持版式的翻译任务，提供了数据基准、评测集与自适应OCR流水线；经该数据微调的紧凑LVLM表现突出，验证了方法与数据的有效性与通用性。

Abstract: Text Image Machine Translation (TIMT) aims to translate texts embedded within
an image into another language. Current TIMT studies primarily focus on
providing translations for all the text within an image, while neglecting to
provide bounding boxes and covering limited scenarios. In this work, we extend
traditional TIMT into position-aware TIMT (PATIMT), aiming to support
fine-grained and layoutpreserving translation, which holds great practical
value but remains largely unexplored. This task comprises two key sub-tasks:
regionspecific translation and full-image translation with grounding. To
support existing models on PATIMT and conduct fair evaluation, we construct the
PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world
scenarios. Specifically, we introduce an Adaptive Image OCR Refinement
Pipeline, which adaptively selects appropriate OCR tools based on scenario and
refines the results of text-rich images. To ensure evaluation reliability, we
further construct a test set, which contains 1,200 high-quality instances
manually annotated and reviewed by human experts. After fine-tuning on our
data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art
performance on both sub-tasks. Experimental results also highlight the
scalability and generalizability of our training data

</details>


### [10] [Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance](https://arxiv.org/abs/2509.12279)
*He Gao,Baoxiang Huang,Milena Radenkovic,Borui Li,Ge Chen*

Main category: cs.CV

TL;DR: 提出SimMemDA框架，实现光学到SAR的无监督跨模态船舶尾迹检测：用WakeGAN风格迁移、实例级相似度筛选、特征-置信记忆库与KNN融合校准伪标签，并通过区域混合训练提升泛化，显著提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SAR具备全天候优势但尾迹特征抽象且噪声大，标注困难；光学图像可视特征清晰，但直接迁移到SAR存在严重域偏移，导致性能下降。需要一种跨模态域适配方法，既缓解风格差异，又可靠利用目标域无标签数据。

Method: 1) WakeGAN对光学图像进行风格迁移，生成接近SAR风格的伪图像；2) 实例级特征相似度筛选，挑选与目标域分布相近的源样本，降低负迁移；3) 构建特征-置信记忆库，结合K近邻置信加权融合，对目标域伪标签进行动态校准，提升稳定性与可靠性；4) 区域混合训练，将源域真实标注与校准后的目标伪标签进行策略化混合，增强模型泛化。

Result: 在跨模态船舶尾迹检测任务上，SimMemDA较基线显著提升准确性与鲁棒性，实验验证方法有效且可行。

Conclusion: 通过风格迁移、相似度筛选与记忆库引导的伪标签校准，以及区域混合训练，SimMemDA有效缓解光学到SAR的域偏移问题，提升无监督跨模态尾迹检测性能。

Abstract: Synthetic Aperture Radar (SAR), with its all-weather and wide-area
observation capabilities, serves as a crucial tool for wake detection. However,
due to its complex imaging mechanism, wake features in SAR images often appear
abstract and noisy, posing challenges for accurate annotation. In contrast,
optical images provide more distinct visual cues, but models trained on optical
data suffer from performance degradation when applied to SAR images due to
domain shift. To address this cross-modal domain adaptation challenge, we
propose a Similarity-Guided and Memory-Guided Domain Adaptation (termed
SimMemDA) framework for unsupervised domain adaptive ship wake detection via
instance-level feature similarity filtering and feature memory guidance.
Specifically, to alleviate the visual discrepancy between optical and SAR
images, we first utilize WakeGAN to perform style transfer on optical images,
generating pseudo-images close to the SAR style. Then, instance-level feature
similarity filtering mechanism is designed to identify and prioritize source
samples with target-like distributions, minimizing negative transfer.
Meanwhile, a Feature-Confidence Memory Bank combined with a K-nearest neighbor
confidence-weighted fusion strategy is introduced to dynamically calibrate
pseudo-labels in the target domain, improving the reliability and stability of
pseudo-labels. Finally, the framework further enhances generalization through
region-mixed training, strategically combining source annotations with
calibrated target pseudo-labels. Experimental results demonstrate that the
proposed SimMemDA method can improve the accuracy and robustness of cross-modal
ship wake detection tasks, validating the effectiveness and feasibility of the
proposed method.

</details>


### [11] [Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning](https://arxiv.org/abs/2509.12329)
*Shengjie Kris Liu,Siqin Wang,Lu Zhang*

Main category: cs.CV

TL;DR: 提出“Amplifier Air-Transformer”深度学习框架，在美国本土以2 km/1小时分辨率生成近地表气温：先重建被云遮挡的GOES-16地表温度，再将其转化为气温，并用深度集成估计不确定性；在2018–2024年全国海量数据上验证，站点精度MAE≈1.93°C。


<details>
  <summary>Details</summary>
Motivation: 单一数据源难以在时空上无缝提供近地表气温：站点稠密但稀疏覆盖，卫星覆盖广但云等造成时空缺口与量测差异；需要一种融合物理约束与数据驱动的方法，实现高时空分辨率、连续的气温场，并量化不确定性。

Method: 两阶段、物理引导的深度学习：1) 云下重建地表温度（LST）：网络显式编码年周期信号，并包含线性放大项将ERA5气温信息加强到更精细尺度，卷积层捕捉时空变化，重建GOES-16被云遮挡像素；2) LST→气温转换：第二个网络利用LST与关键地表属性的潜在关系映射到近地表气温；3) 用深度集成实现预测不确定性估计，提高可靠性。

Result: 在2018–2024年覆盖美国本土的数据上训练与测试（777亿个LST像素、1.55亿个站点记录），实现小时级2 km气温制图；站点验证误差约1.93°C，提供不确定性评估；框架同时简化了LST重建与气温预测流程。

Conclusion: 该框架能够在大范围内连续生成高时空分辨率的近地表气温，并给出不确定性；方法可扩展到其他卫星数据源，适用于无缝气温监测与下游应用。

Abstract: Near-surface air temperature is a key physical property of the Earth's
surface. Although weather stations offer continuous monitoring and satellites
provide broad spatial coverage, no single data source offers seamless data in a
spatiotemporal fashion. Here, we propose a data-driven, physics-guided deep
learning approach to generate hourly air temperature data at 2 km resolution
over the contiguous United States. The approach, called Amplifier
Air-Transformer, first reconstructs GOES-16 surface temperature data obscured
by clouds. It does so through a neural network encoded with the annual
temperature cycle, incorporating a linear term to amplify ERA5 temperature
values at finer scales and convolutional layers to capture spatiotemporal
variations. Then, another neural network transforms the reconstructed surface
temperature into air temperature by leveraging its latent relationship with key
Earth surface properties. The approach is further enhanced with predictive
uncertainty estimation through deep ensemble learning to improve reliability.
The proposed approach is built and tested on 77.7 billion surface temperature
pixels and 155 million air temperature records from weather stations across the
contiguous United States (2018-2024), achieving hourly air temperature mapping
accuracy of 1.93 C in station-based validation. The proposed approach
streamlines surface temperature reconstruction and air temperature prediction,
and it can be extended to other satellite sources for seamless air temperature
monitoring at high spatiotemporal resolution. The generated data of this study
can be downloaded at https://doi.org/10.5281/zenodo.15252812, and the project
webpage can be found at https://skrisliu.com/HourlyAirTemp2kmUSA/.

</details>


### [12] [DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification](https://arxiv.org/abs/2509.12353)
*Anthony Miyaguchi,Chandrasekaran Maruthaiyannan,Charles R. Clark*

Main category: cs.CV

TL;DR: 论文报告DS@GT在AnimalCLEF 2025个体再识别挑战中的方案：比较通用骨干（DINOv2）与动物领域特化骨干（MegaDescriptor），并在其上进行后验度量学习与KNN判别。结论是：后验度量学习的收益强烈依赖骨干特征的领域性与质量，通用特征难以通过小样本微调获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 个体再识别在跨物种、少样本场景中难度高；通用视觉骨干虽强，但不一定适合细粒度再识别。作者想弄清：在有限数据下，后验度量学习能否将通用表征“塑形”到特定领域，或是否必须依赖领域特化预训练。

Method: - 两种骨干对比：通用DINOv2 vs 领域特化MegaDescriptor。
- 在骨干嵌入之上使用triplet-learning投影头进行后验度量学习。
- 用带鲁棒阈值的KNN分类器来区分已知个体与新个体（开放集）。
- 通过验证损失、可视化（流形结构）与挑战指标（BAKS/BAUS）评估。

Result: - 对MegaDescriptor，triplet投影头带来约+0.13的平均BAKS/BAUS提升。
- 对DINOv2，仅+0.03的微弱提升；验证损失基本停滞，流形可视化显示通用特征难以重塑。
- KNN+阈值在已知/未知识别上可行。

Conclusion: 后验度量学习并非万灵药；当骨干为通用表征时，在细粒度、数据受限的再识别上提升甚微。领域特化预训练对性能更关键。建议优先选择/构建领域特化骨干，再叠加轻量的度量学习与开放集阈值策略。实现代码已开源（github.com/dsgt-arc/animalclef-2025）。

Abstract: This paper details the DS@GT team's entry for the AnimalCLEF 2025
re-identification challenge. Our key finding is that the effectiveness of
post-hoc metric learning is highly contingent on the initial quality and
domain-specificity of the backbone embeddings. We compare a general-purpose
model (DINOv2) with a domain-specific model (MegaDescriptor) as a backbone. A
K-Nearest Neighbor classifier with robust thresholding then identifies known
individuals or flags new ones. While a triplet-learning projection head
improved the performance of the specialized MegaDescriptor model by 0.13
points, it yielded minimal gains (0.03) for the general-purpose DINOv2 on
averaged BAKS and BAUS. We demonstrate that the general-purpose manifold is
more difficult to reshape for fine-grained tasks, as evidenced by stagnant
validation loss and qualitative visualizations. This work highlights the
critical limitations of refining general-purpose features for specialized,
limited-data re-ID tasks and underscores the importance of domain-specific
pre-training. The implementation for this work is publicly available at
github.com/dsgt-arc/animalclef-2025.

</details>


### [13] [GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images](https://arxiv.org/abs/2509.12380)
*Florian Zager,Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: 提出GhostNetV3-Small以适配低分辨率（如CIFAR-10），在资源受限设备上更高效；在比较多种蒸馏（传统、助教、集成）后发现均降准，最佳模型在CIFAR-10上达93.94%且优于原版GhostNetV3。结论：结构改造比蒸馏更有效。


<details>
  <summary>Details</summary>
Motivation: 移动/边缘设备算力受限，标准深度网络推理代价高；现有移动架构在低分辨率小数据集上未必最优，且知识蒸馏在此场景的有效性存疑。作者动机是同时探索结构适配与蒸馏策略，以提升低分辨率任务的效率与精度。

Method: 1) 以GhostNetV3为基础提出面向低分辨率输入的GhostNetV3-Small（缩小与重配网络结构/通道/宏观深度宽度以匹配CIFAR分辨率和计算预算）。2) 系统比较三类蒸馏：传统单教师KD、Teacher Assistant（中间教师）和教师集成蒸馏。3) 在CIFAR-10上实验，对比原始GhostNetV3与改进小模型及各种蒸馏训练方案。

Result: GhostNetV3-Small在CIFAR-10上显著优于原始GhostNetV3，达到93.94%准确率。所有考察的蒸馏策略相较于基线训练均出现性能下降。

Conclusion: 在小尺度低分辨率图像分类上，面向输入域的架构适配比常规蒸馏更有效。需进一步研究更适合低分辨率域的模型设计与先进蒸馏方法，传统KD范式可能并不适配此场景。

Abstract: Deep neural networks have achieved remarkable success across a range of
tasks, however their computational demands often make them unsuitable for
deployment on resource-constrained edge devices. This paper explores strategies
for compressing and adapting models to enable efficient inference in such
environments. We focus on GhostNetV3, a state-of-the-art architecture for
mobile applications, and propose GhostNetV3-Small, a modified variant designed
to perform better on low-resolution inputs such as those in the CIFAR-10
dataset. In addition to architectural adaptation, we provide a comparative
evaluation of knowledge distillation techniques, including traditional
knowledge distillation, teacher assistants, and teacher ensembles. Experimental
results show that GhostNetV3-Small significantly outperforms the original
GhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to
expectations, all examined distillation strategies led to reduced accuracy
compared to baseline training. These findings indicate that architectural
adaptation can be more impactful than distillation in small-scale image
classification tasks, highlighting the need for further research on effective
model design and advanced distillation techniques for low-resolution domains.

</details>


### [14] [From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization](https://arxiv.org/abs/2509.12400)
*Rongkun Zhu,Kangning Cui,Wei Tang,Rui-Feng Wang,Sarra Alqahtani,David Lutz,Fan Yang,Paul Fine,Jordan Karubian,Robert Plemmons,Jean-Michel Morel,Victor Pauca,Miles Silman*

Main category: cs.CV

TL;DR: 研究比较无人机正射影像与原始影像在热带森林中油棕等树木检测与树冠中心定位的效果，并评估加入树冠中心标注对定位精度的提升，为生物多样性与保护监测提供实践建议。


<details>
  <summary>Details</summary>
Motivation: 现有基于UAV的树木映射多依赖正射拼接影像，但易产生拼接伪影且需要繁重预处理，限制现场部署与实时应用。需要评估直接利用原始航片能否在检测与定位上取得更好且更实用的表现，并明确不同标注策略（树冠中心 vs. 边界框质心）对定位精度的影响。

Method: 以热带森林棕榈为对象，使用当前SOTA目标检测器与关键点（keypoint）模型，分别在正射影像与原始航片上训练/测试，设计域内与跨域迁移实验；同时比较以树冠中心标注与以边界框中心为监督的定位误差与下游可用性。

Result: 在与部署相关的场景中（原始航片直接推理），原始影像显著优于正射影像；而正射影像在跨域泛化上仍具优势。引入树冠中心标注能进一步降低定位误差，提供更精确的树位点。

Conclusion: 原始UAV影像更适合现场部署和实用检测/定位，正射影像可作为跨域鲁棒性的补充；在训练中加入树冠中心标注能显著提升定位精度，为生态监测和保护应用提供更可靠的树位数据。

Abstract: Accurate mapping of individual trees is essential for ecological monitoring
and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs)
is widely used, but stitching artifacts and heavy preprocessing limit its
suitability for field deployment. This study explores the use of raw UAV
imagery for palm detection and crown-center localization in tropical forests.
Two research questions are addressed: (1) how detection performance varies
across orthomosaic and raw imagery, including within-domain and cross-domain
transfer, and (2) to what extent crown-center annotations improve localization
accuracy beyond bounding-box centroids. Using state-of-the-art detectors and
keypoint models, we show that raw imagery yields superior performance in
deployment-relevant scenarios, while orthomosaics retain value for robust
cross-domain generalization. Incorporating crown-center annotations in training
further improves localization and provides precise tree positions for
downstream ecological analyses. These findings offer practical guidance for
UAV-based biodiversity and conservation monitoring.

</details>


### [15] [DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction](https://arxiv.org/abs/2509.12430)
*Mayank Patel,Rahul Jain,Asim Unmesh,Karthik Ramani*

Main category: cs.CV

TL;DR: 提出MechBench数据集与DYNAMO模型，从静态CAD点云预测齿轮装配的耦合运动，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有对日常关节物体的运动理解依赖简化的运动学或关节标注，无法处理机械装配中因几何耦合（啮合齿、同轴）而产生的关联运动；缺乏系统数据集与方法来从纯几何推断这类耦合运动。

Method: 1) 构建MechBench：包含693个多样化合成齿轮装配，提供按部件的真实SE(3)运动轨迹；2) 提出DYNAMO：依赖关系感知的神经网络，从分割的CAD点云直接预测各部件随时间的SE(3)轨迹，建模部件间的耦合与时序一致性；3) 与强基线对比评测。

Result: 在多种齿轮配置上，DYNAMO实现更高精度且时间一致的运动预测，优于强基线。

Conclusion: MechBench与DYNAMO共同提供了一个系统框架，使数据驱动地从CAD几何学习耦合机械运动成为可能。

Abstract: Understanding the motion of articulated mechanical assemblies from static
geometry remains a core challenge in 3D perception and design automation. Prior
work on everyday articulated objects such as doors and laptops typically
assumes simplified kinematic structures or relies on joint annotations.
However, in mechanical assemblies like gears, motion arises from geometric
coupling, through meshing teeth or aligned axes, making it difficult for
existing methods to reason about relational motion from geometry alone. To
address this gap, we introduce MechBench, a benchmark dataset of 693 diverse
synthetic gear assemblies with part-wise ground-truth motion trajectories.
MechBench provides a structured setting to study coupled motion, where part
dynamics are induced by contact and transmission rather than predefined joints.
Building on this, we propose DYNAMO, a dependency-aware neural model that
predicts per-part SE(3) motion trajectories directly from segmented CAD point
clouds. Experiments show that DYNAMO outperforms strong baselines, achieving
accurate and temporally consistent predictions across varied gear
configurations. Together, MechBench and DYNAMO establish a novel systematic
framework for data-driven learning of coupled mechanical motion in CAD
assemblies.

</details>


### [16] [Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions](https://arxiv.org/abs/2509.12442)
*Rui-Feng Wang,Mingrui Xu,Matthew C Bauer,Iago Beffart Schardong,Xiaowen Ma,Kangning Cui*

Main category: cs.CV

TL;DR: 提出Cott-ADNet，一种基于YOLOv11n的轻量级实时检测器，用于田间复杂环境下棉铃/花识别与成熟度判别；通过NeLU全局注意力与膨胀感受野SPPF提升弱特征捕获与多尺度上下文，7.5 GFLOPs下达成mAP50 93.3%、mAP 71.3%、Precision 91.5%、Recall 89.8%、F1 90.6%，多尺度与旋转鲁棒，支持自动采收与表型分析。


<details>
  <summary>Details</summary>
Motivation: 棉花采收依赖人工，效率低且易错过最佳采收窗口，导致减产；自动化采收、产量评估与育种研究需要准确识别棉铃及成熟度，但田间场景复杂、目标弱小低对比，现有检测器在精度/速度/鲁棒性间难以平衡。

Method: 以YOLOv11n为骨干，改进卷积以增强空间表征与鲁棒性；引入两大模块：（1）NeLU增强的全局注意力机制（GAM），专注弱小与低对比特征；（2）膨胀感受野SPPF（Dilated Receptive Field SPPF），以低计算代价扩大感受野，改进多尺度上下文建模；构建4,966张标注训练集，并发布1,216张外部田间验证集。

Result: 在仅7.5 GFLOPs下，取得Precision 91.5%、Recall 89.8%、F1 90.6%、mAP50 93.3%、mAP 71.3%；在多尺度与旋转扰动下保持稳定表现；证明在复杂田间环境中具备准确高效的检测能力。

Conclusion: Cott-ADNet在保持轻量实时性的同时显著提升棉铃/花检测精度与鲁棒性，适合田间部署，为自动化采收和高通量表型分析提供可靠基础；公开代码与数据集促进后续研究与基准化评测。

Abstract: Cotton is one of the most important natural fiber crops worldwide, yet
harvesting remains limited by labor-intensive manual picking, low efficiency,
and yield losses from missing the optimal harvest window. Accurate recognition
of cotton bolls and their maturity is therefore essential for automation, yield
estimation, and breeding research. We propose Cott-ADNet, a lightweight
real-time detector tailored to cotton boll and flower recognition under complex
field conditions. Building on YOLOv11n, Cott-ADNet enhances spatial
representation and robustness through improved convolutional designs, while
introducing two new modules: a NeLU-enhanced Global Attention Mechanism to
better capture weak and low-contrast features, and a Dilated Receptive Field
SPPF to expand receptive fields for more effective multi-scale context modeling
at low computational cost. We curate a labeled dataset of 4,966 images, and
release an external validation set of 1,216 field images to support future
research. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8%
Recall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs,
maintaining stable performance under multi-scale and rotational variations.
These results demonstrate Cott-ADNet as an accurate and efficient solution for
in-field deployment, and thus provide a reliable basis for automated cotton
harvesting and high-throughput phenotypic analysis. Code and dataset is
available at https://github.com/SweefongWong/Cott-ADNet.

</details>


### [17] [Deep learning for 3D point cloud processing -- from approaches, tasks to its implications on urban and environmental applications](https://arxiv.org/abs/2509.12452)
*Zhenxin Zhang,Zhihua Xu,Yuwei Cao,Ningli Xu,Shuye Wang,Shen'ao Cui,Zhen Li,Rongjun Qin*

Main category: cs.CV

TL;DR: 这是一篇关于点云处理的深度学习综述与“元评述”，聚焦从空到地的真实应用（测绘、环境、城市/树结构、自动驾驶、机器人、灾害响应），系统梳理关键任务（场景补全、配准、语义分割、建模）的方法与数据集，强调超大规模数据、场景多样性、点密度变化与多模态等实际挑战，并指出研究到落地之间的鸿沟与改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习主导点云处理研究，但多数方法尚未真正落地；现有综述偏重网络结构，对真实应用中的工程与数据层面问题着墨不足。作者希望通过面向任务与数据集的视角，评估方法在典型城市与环境应用中的实际价值，识别研究与应用之间的差距。

Method: 进行“元评述”：横向梳理深度学习点云处理在四个关键任务（场景补全、配准、语义分割、建模）的代表性方法与数据集；纵向对比其在真实应用（城市、环境等）中的适用性；从算法与实践两个层面提炼问题与趋势。

Result: 形成对关键任务与数据集的系统性对照与评价，归纳出应用落地面临的核心挑战：数据规模超大、场景与模态多样、点密度不均、跨域泛化欠佳、工程可用性（效率、鲁棒性、可解释性）不足。

Conclusion: 建议未来研究在算法上提升可扩展性与跨域鲁棒、利用多模态与自主监督/弱监督，兼顾效率与精度；在实践上完善大规模数据管理与基准、与真实工作流对接，促进方法从学术走向应用。

Abstract: Point cloud processing as a fundamental task in the field of geomatics and
computer vision, has been supporting tasks and applications at different scales
from air to ground, including mapping, environmental monitoring, urban/tree
structure modeling, automated driving, robotics, disaster responses etc. Due to
the rapid development of deep learning, point cloud processing algorithms have
nowadays been almost explicitly dominated by learning-based approaches, most of
which are yet transitioned into real-world practices. Existing surveys
primarily focus on the ever-updating network architecture to accommodate
unordered point clouds, largely ignoring their practical values in typical
point cloud processing applications, in which extra-large volume of data,
diverse scene contents, varying point density, data modality need to be
considered. In this paper, we provide a meta review on deep learning approaches
and datasets that cover a selection of critical tasks of point cloud processing
in use such as scene completion, registration, semantic segmentation, and
modeling. By reviewing a broad range of urban and environmental applications
these tasks can support, we identify gaps to be closed as these methods
transformed into applications and draw concluding remarks in both the
algorithmic and practical aspects of the surveyed methods.

</details>


### [18] [Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis](https://arxiv.org/abs/2509.12453)
*Yiran Song,Yikai Zhang,Silvia Orengo-Nania,Nian Wang,Fenglong Ma,Rui Zhang,Yifan Peng,Mingquan Lin*

Main category: cs.CV

TL;DR: 提出TSDF双阶段解耦框架，先用自监督跨数据集学习通用表征，再用注意力式时间聚合处理变长序列，在OHTS与GRAPE上验证有效且参数量紧凑。


<details>
  <summary>Details</summary>
Motivation: 现有青光眼预后方法依赖固定长度序列、端到端训练且数据集规模小，导致灵活性差、泛化弱、易过拟合；需要能利用多源异质数据并处理可变长度随访序列的方案。

Method: 两阶段解耦：1) 表征学习阶段：基于自监督学习，在不依赖一致监督标签的情况下聚合多青光眼数据集训练通用特征表示；2) 时序聚合阶段：引入基于注意力的时间聚合模块，接收可变长度的历史序列，进行加权整合与预后预测；整体保持较小参数规模。

Result: 在两个规模与临床设置差异显著的基准数据集OHTS与GRAPE上进行大量实验证明该方法有效且鲁棒，相比基线显著提升性能，同时模型更紧凑。

Conclusion: TSDF通过自监督跨数据集表征学习与注意力时序聚合，有效解决变长输入与小数据问题，提升青光眼预后预测的准确性与泛化，并以较少参数实现稳健表现。

Abstract: Glaucoma is one of the leading causes of irreversible blindness worldwide.
Glaucoma prognosis is essential for identifying at-risk patients and enabling
timely intervention to prevent blindness. Many existing approaches rely on
historical sequential data but are constrained by fixed-length inputs, limiting
their flexibility. Additionally, traditional glaucoma prognosis methods often
employ end-to-end models, which struggle with the limited size of glaucoma
datasets. To address these challenges, we propose a Two-Stage Decoupling
Framework (TSDF) for variable-length glaucoma prognosis. In the first stage, we
employ a feature representation module that leverages self-supervised learning
to aggregate multiple glaucoma datasets for training, disregarding differences
in their supervisory information. This approach enables datasets of varying
sizes to learn better feature representations. In the second stage, we
introduce a temporal aggregation module that incorporates an attention-based
mechanism to process sequential inputs of varying lengths, ensuring flexible
and efficient utilization of all available data. This design significantly
enhances model performance while maintaining a compact parameter size.
Extensive experiments on two benchmark glaucoma datasets:the Ocular
Hypertension Treatment Study (OHTS) and the Glaucoma Real-world Appraisal
Progression Ensemble (GRAPE),which differ significantly in scale and clinical
settings,demonstrate the effectiveness and robustness of our approach.

</details>


### [19] [Image Tokenizer Needs Post-Training](https://arxiv.org/abs/2509.12474)
*Kai Qiu,Xiang Li,Hao Chen,Jason Kuen,Xiaohao Xu,Jiuxiang Gu,Yinyi Luo,Bhiksha Raj,Zhe Lin,Marios Savvides*

Main category: cs.CV

TL;DR: 论文指出图像生成模型常用的冻结tokenizer仅为重建优化，导致重建分布与生成分布不匹配，从而影响生成质量。作者在离散潜空间中系统分析该差异成因，并提出包含主训练与后训练的两阶段tokenizer方案：主训练用潜表示扰动模拟采样噪声，提升tokenizer鲁棒性并提出pFID评价；后训练对齐已训练生成器的生成与重建分布。实验显示在∼400M生成器下，gFID由1.60进一步降至1.36，并在多种离散/连续tokenizer与自回归/扩散生成器上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有图像tokenizer为重建任务优化，与后续生成阶段的采样误差与分布偏移无关，导致生成质量下降、收敛慢以及评价指标与最终生成质量脱钩。需要一种既考虑重建又考虑生成阶段误差的训练与评估机制来缩小分布差异并提升生成表现。

Method: 1) 理论/经验分析：在离散潜空间中分析重建-生成分布差异与“意外token”（采样噪声）如何放大误差。
2) 主训练（plug-and-play）：在tokenizer训练中引入潜表示扰动，显式模拟生成阶段的错误token；设计易插拔的训练策略以增强鲁棒性；提出与生成质量更相关的新指标pFID。
3) 后训练：在已有生成器固定的前提下，对tokenizer的解码器进行再优化，使其更好解码生成器产生的token分布，从而减少生成与重建的分布差异。

Result: 使用约4亿参数的生成器，采用所提主训练的离散tokenizer达到1.60的gFID；再加上后训练将gFID进一步降到1.36。额外实验证明后训练策略对现成的离散与连续tokenizer均有效，并可与自回归和扩散式生成器配合。

Conclusion: 关注并缓解重建-生成分布差异是提升图像生成质量的关键。通过在tokenizer训练中模拟采样噪声并在后期对解码器与已训练生成器对齐，可显著提升鲁棒性、加速收敛并降低gFID；pFID可更好地评估tokenizer对最终生成质量的贡献。

Abstract: Recent image generative models typically capture the image distribution in a
pre-constructed latent space, relying on a frozen image tokenizer. However,
there exists a significant discrepancy between the reconstruction and
generation distribution, where current tokenizers only prioritize the
reconstruction task that happens before generative training without considering
the generation errors during sampling. In this paper, we comprehensively
analyze the reason for this discrepancy in a discrete latent space, and, from
which, we propose a novel tokenizer training scheme including both
main-training and post-training, focusing on improving latent space
construction and decoding respectively. During the main training, a latent
perturbation strategy is proposed to simulate sampling noises, \ie, the
unexpected tokens generated in generative inference. Specifically, we propose a
plug-and-play tokenizer training scheme, which significantly enhances the
robustness of tokenizer, thus boosting the generation quality and convergence
speed, and a novel tokenizer evaluation metric, \ie, pFID, which successfully
correlates the tokenizer performance to generation quality. During
post-training, we further optimize the tokenizer decoder regarding a
well-trained generative model to mitigate the distribution difference between
generated and reconstructed tokens. With a $\sim$400M generator, a discrete
tokenizer trained with our proposed main training achieves a notable 1.60 gFID
and further obtains 1.36 gFID with the additional post-training. Further
experiments are conducted to broadly validate the effectiveness of our
post-training strategy on off-the-shelf discrete and continuous tokenizers,
coupled with autoregressive and diffusion-based generators.

</details>


### [20] [Towards Foundational Models for Single-Chip Radar](https://arxiv.org/abs/2509.12482)
*Tianshu Huang,Akarsh Prabhakara,Chuhan Chen,Jay Karhade,Deva Ramanan,Matthew O'Toole,Anthony Rowe*

Main category: cs.CV

TL;DR: 提出GRT（Generalizable Radar Transformer）作为mmWave单芯片4D雷达的基础模型，基于迄今最大原始雷达数据集（100万样本、29小时），在3D占据与语义分割上接近高分辨率传感器效果，并具有良好泛化与可微调能力。发现使用原始雷达优于常见有损表示（相当于多10倍数据），数据扩展呈对数收益（每10×数据带来约20%提升），预计需≈1亿样本（3000小时）才能充分挖掘潜力。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达价格低、耐用、抗遮挡、全天候，但单芯片设备角分辨率差，缺乏统一基础模型与大规模数据集，导致实践中多为小数据从头训练、任务割裂。作者旨在构建大规模原始数据集与可泛化的基础模型，提升单芯片雷达在3D理解任务的能力并量化数据规模与表示选择的影响。

Method: 收集100万条（约29小时）的原始4D单芯片毫米波雷达数据；提出Generalizable Radar Transformer（GRT），以原始雷达点云/张量为输入，训练可同时进行3D占据预测与语义分割的基础模型；进行跨场景泛化与下游任务微调实验；开展消融：比较原始数据 vs 常用有损表示；做数据尺度实验，分析性能与数据量的对数关系。

Result: GRT在3D占据和语义分割上达到接近高分辨率传感器的质量；具备强跨场景泛化并可通过微调适配不同任务；数据显示每扩大10倍数据带来约20%的性能提升；使用原始雷达数据显著优于有损表示，相当于额外10倍训练数据；给出对完全发挥模型潜力所需数据量的粗估计。

Conclusion: 原始数据驱动的GRT可作为单芯片4D毫米波雷达的通用基础模型，能在低成本传感器上实现高质量3D理解与语义分割。数据规模和表示选择至关重要：原始表示优于有损表示，性能随数据量近似对数增长。要充分释放潜力，需约1亿样本（约3000小时）级别的数据。

Abstract: mmWave radars are compact, inexpensive, and durable sensors that are robust
to occlusions and work regardless of environmental conditions, such as weather
and darkness. However, this comes at the cost of poor angular resolution,
especially for inexpensive single-chip radars, which are typically used in
automotive and indoor sensing applications. Although many have proposed
learning-based methods to mitigate this weakness, no standardized foundational
models or large datasets for the mmWave radar have emerged, and practitioners
have largely trained task-specific models from scratch using relatively small
datasets.
  In this paper, we collect (to our knowledge) the largest available raw radar
dataset with 1M samples (29 hours) and train a foundational model for 4D
single-chip radar, which can predict 3D occupancy and semantic segmentation
with quality that is typically only possible with much higher resolution
sensors. We demonstrate that our Generalizable Radar Transformer (GRT)
generalizes across diverse settings, can be fine-tuned for different tasks, and
shows logarithmic data scaling of 20\% per $10\times$ data. We also run
extensive ablations on common design decisions, and find that using raw radar
data significantly outperforms widely-used lossy representations, equivalent to
a $10\times$ increase in training data. Finally, we roughly estimate that
$\approx$100M samples (3000 hours) of data are required to fully exploit the
potential of GRT.

</details>


### [21] [Evaluating Robustness of Vision-Language Models Under Noisy Conditions](https://arxiv.org/abs/2509.12492)
*Purushoth,Alireza*

Main category: cs.CV

TL;DR: 评估多款前沿视觉-语言模型在受控噪声（光照变化、运动模糊、压缩失真）下的鲁棒性，结合词汇与语义相似度指标，揭示模型规模、数据集描述性与噪声类型对性能的细致影响，并提供标准化基准。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在理想条件下表现突出，但其在真实世界常见噪声干扰下的稳健性缺乏系统性了解与统一评价，阻碍了可靠落地与模型改进。

Method: 构建统一评测框架：对图像施加可控扰动（光照变化、运动模糊、JPEG压缩等），在多数据集上测试多款SOTA VLM（含大型如LLaVA与较小模型）；采用词汇类指标（BLEU/METEOR/ROUGE/CIDEr）与基于句向量的神经语义相似度联合衡量；分析不同数据集标注描述性对模型表现的影响。

Result: (1) 参考描述越详尽，模型指标越高；(2) 大模型在语义理解上占优，但并非在所有场景全面领先小模型；(3) JPEG压缩与运动模糊等噪声显著降低各模型表现；呈现模型规模、数据集特性与噪声类型之间的权衡。

Conclusion: 鲁棒性评测需同时考虑噪声种类、数据集描述性与模型规模。所提出框架与标准化基准可促进更稳健的多模态学习研究与模型设计。

Abstract: Vision-Language Models (VLMs) have attained exceptional success across
multimodal tasks such as image captioning and visual question answering.
However, their robustness under noisy conditions remains unfamiliar. In this
study, we present a comprehensive evaluation framework to evaluate the
performance of several state-of-the-art VLMs under controlled perturbations,
including lighting variation, motion blur, and compression artifacts. We used
both lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based
similarity measures using sentence embeddings to quantify semantic alignment.
Our experiments span diverse datasets, revealing key insights: (1)
descriptiveness of ground-truth captions significantly influences model
performance; (2) larger models like LLaVA excel in semantic understanding but
do not universally outperform smaller models; and (3) certain noise types, such
as JPEG compression and motion blur, dramatically degrade performance across
models. Our findings highlight the nuanced trade-offs between model size,
dataset characteristics, and noise resilience, offering a standardized
benchmark for future robust multimodal learning.

</details>


### [22] [Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2509.12496)
*Ali Torabi,Sanjog Gaihre,MD Mahbubur Rahman,Yaqoob Majeed*

Main category: cs.CV

TL;DR: 提出IG-CAM用于弱监督语义分割，通过实例引导、影响函数与多尺度边界增强，生成覆盖完整且边界清晰的定位图，在VOC2012上达SOTA（mIoU 82.3%/86.6%经CRF）。


<details>
  <summary>Details</summary>
Motivation: 现有WSSS多依赖图像级标签，CAM往往只覆盖判别性区域且边界模糊，难以获得完整目标与精确边界，同时缺乏对训练样本与预测关系的建模。

Method: IG-CAM包含三部分：1) 实例引导精炼：利用实例级线索（称为“ground truth segmentation masks”引导CAM）使定位覆盖完整目标；2) 融合影响函数：衡量训练样本对预测的影响，增强特征鲁棒性与可解释性；3) 多尺度边界增强：渐进式多尺度策略细化边界，获得更锐利的轮廓；并在CRF后处理进一步提升边界一致性。

Result: 在PASCAL VOC 2012上，无后处理mIoU 82.3%，经CRF提升到86.6%，超过现有WSSS方法；定性对比600张多样图像显示更好的定位完整性与边界精度；消融验证各组件均有贡献。

Conclusion: IG-CAM以实例引导+影响函数+多尺度边界增强协同，显著提升弱监督分割的覆盖度与边界质量，兼顾精度与效率，成为新的WSSS基准，适用于缺乏像素级标注的实际场景。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of
training segmentation models using only image-level annotations, eliminating
the need for expensive pixel-level labeling. While existing methods struggle
with precise object boundary localization and often focus only on the most
discriminative regions, we propose IG-CAM (Instance-Guided Class Activation
Mapping), a novel approach that leverages instance-level cues and influence
functions to generate high-quality, boundary-aware localization maps. Our
method introduces three key innovations: (1) Instance-Guided Refinement that
uses ground truth segmentation masks to guide CAM generation, ensuring complete
object coverage rather than just discriminative parts; (2) Influence Function
Integration that captures the relationship between training samples and model
predictions, leading to more robust feature representations; and (3)
Multi-Scale Boundary Enhancement that employs progressive refinement strategies
to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art
performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before
post-processing, which further improves to 86.6% after applying Conditional
Random Field (CRF) refinement, significantly outperforming previous WSSS
methods. Our approach demonstrates superior localization accuracy, with
complete object coverage and precise boundary delineation, while maintaining
computational efficiency. Extensive ablation studies validate the contribution
of each component, and qualitative comparisons across 600 diverse images
showcase the method's robustness and generalization capability. The results
establish IG-CAM as a new benchmark for weakly supervised semantic
segmentation, offering a practical solution for scenarios where pixel-level
annotations are unavailable or prohibitively expensive.

</details>


### [23] [Artist-Created Mesh Generation from Raw Observation](https://arxiv.org/abs/2509.12501)
*Yao He,Youngjoong Kwon,Wenxiao Cai,Ehsan Adeli*

Main category: cs.CV

TL;DR: 提出一个端到端框架，将噪声/不完整点云直接生成可动画与贴图的“艺术家风格”网格；核心是把3D点云修复重构转化为2D修复（inpainting）问题，借助强大的生成模型，在ShapeNet上初步显示能得到干净完整的网格。


<details>
  <summary>Details</summary>
Motivation: 现实传感（LiDAR、手机RGB-D）获取的点云往往噪声大且不完整；现有方法要么假设输入干净完整，要么依赖复杂多阶段管线，不利于工业图形流程中对艺术家友好网格的需求（可动画、易贴图、渲染高效）。

Method: 端到端网络：先对输入点云进行“2D化”表述，将3D点云修复问题重构为2D图像修复任务，利用强生成模型完成补全与去噪；随后直接输出高质量、艺术家风格的三角网格。关键在于2D inpainting 的设计与与网格生成模块的整合。

Result: 在ShapeNet上进行初步实验，能从噪声/不完整点云生成干净、完整的网格，效果有希望达到艺术家风格的质量。

Conclusion: 该端到端框架在真实世界噪声点云到可用艺术家网格的转换上显示出潜力；将3D修复表述为2D inpainting 使得可利用成熟生成模型，简化管线并提升鲁棒性与质量。

Abstract: We present an end-to-end framework for generating artist-style meshes from
noisy or incomplete point clouds, such as those captured by real-world sensors
like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for
commercial graphics pipelines due to their compatibility with animation and
texturing tools and their efficiency in rendering. However, existing approaches
often assume clean, complete inputs or rely on complex multi-stage pipelines,
limiting their applicability in real-world scenarios. To address this, we
propose an end-to-end method that refines the input point cloud and directly
produces high-quality, artist-style meshes. At the core of our approach is a
novel reformulation of 3D point cloud refinement as a 2D inpainting task,
enabling the use of powerful generative models. Preliminary results on the
ShapeNet dataset demonstrate the promise of our framework in producing clean,
complete meshes.

</details>


### [24] [Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery](https://arxiv.org/abs/2509.12511)
*Benjamin Vail,Rahul Harsha Cheppally,Ajay Sharda,Sidharth Rai*

Main category: cs.CV

TL;DR: 提出一种结合实例分割、3D点云重建与PCA切片的几何感知视觉管线，从RGB-D图像稳健估计作物茎秆直径，用于高通量表型测定。


<details>
  <summary>Details</summary>
Motivation: 传统茎秆直径测量费时、易错、不可扩展，制约了改良机械稳定性、生物量与抗病性等育种目标的高通量表型获取。需要一种能在复杂田间条件下应对弯曲、遮挡和噪声的自动化方法。

Method: 基于RGB-D：1) 深度学习实例分割定位目标茎秆；2) 重建3D点云；3) 利用PCA进行主轴对齐并做轴向切片；4) 在切片截面上估计直径，从而缓解曲率、遮挡与噪声的影响，实现稳健测量。

Result: 方法能够从RGB-D数据稳健估计茎秆直径，并在存在弯曲、遮挡和成像噪声的情形下表现可靠，适用于高通量表型流程。（摘要未给出具体数值指标。）

Conclusion: 几何感知的视觉管线为作物茎秆直径的可扩展、可靠估计提供了工具，可支持育种与农学研究中的高通量表型表征。

Abstract: Accurate, high-throughput phenotyping is a critical component of modern crop
breeding programs, especially for improving traits such as mechanical
stability, biomass production, and disease resistance. Stalk diameter is a key
structural trait, but traditional measurement methods are labor-intensive,
error-prone, and unsuitable for scalable phenotyping. In this paper, we present
a geometry-aware computer vision pipeline for estimating stalk diameter from
RGB-D imagery. Our method integrates deep learning-based instance segmentation,
3D point cloud reconstruction, and axis-aligned slicing via Principal Component
Analysis (PCA) to perform robust diameter estimation. By mitigating the effects
of curvature, occlusion, and image noise, this approach offers a scalable and
reliable solution to support high-throughput phenotyping in breeding and
agronomic research.

</details>


### [25] [Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew](https://arxiv.org/abs/2509.12544)
*Can Peng,Yuyuan Liu,Yingyu Yang,Pramit Saha,Qianye Yang,J. Alison Noble*

Main category: cs.CV

TL;DR: 提出一种针对多标签联邦学习的特征对齐方法：利用神经坍塌（NC）理论+特征解耦与聚类正则，使跨客户端的类内特征更紧致、类间更可分，从而在异质、长尾标签分布下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在异构、去中心化数据下深度模型性能下降，尤以多标签任务更难：存在标签共现、标签依赖、以及本地-全局标签关系不一致。现有FL多聚焦单标签分类，无法有效处理多标签的复杂特性与长尾分布，需要一种能在保护隐私前提下对齐不同客户端的特征表示并缓解分布偏移的方法。

Method: 基于神经坍塌（NC）几何理论，设计一个跨客户端共享的预定义NC结构作为对齐目标：1) 引入特征解耦模块，将图像级表示分解为语义（类别）特定的子特征；2) 以共享NC原型/结构引导这些类别特定子特征在潜空间聚类，使类内方差收敛、类间间隔最大；3) 设计正则化损失（如类内紧致、类间分离、与共享NC对齐）以稳定训练并缓解客户端间冲突；4) 在联邦框架下进行模型聚合与对齐。

Result: 在4个基准数据集、8种多样设置下实验，所提方法整体优于现有方法（数值未给出），表明在多标签、标签分布偏斜和客户端异质性场景中取得了更好的泛化与收敛表现。

Conclusion: 通过引入NC指导的跨客户端特征对齐与解耦聚类正则，本方法在多标签联邦学习中有效提升表示质量和下游性能，缓解数据异质与标签偏斜带来的冲突，验证了NC结构在多标签FL中的可行性与优势。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. However, the performance of
deep learning often deteriorates in FL due to decentralized and heterogeneous
data. This challenge is further amplified in multi-label scenarios, where data
exhibit complex characteristics such as label co-occurrence, inter-label
dependency, and discrepancies between local and global label relationships.
While most existing FL research primarily focuses on single-label
classification, many real-world applications, particularly in domains such as
medical imaging, often involve multi-label settings. In this paper, we address
this important yet underexplored scenario in FL, where clients hold multi-label
data with skewed label distributions. Neural Collapse (NC) describes a
geometric structure in the latent feature space where features of each class
collapse to their class mean with vanishing intra-class variance, and the class
means form a maximally separated configuration. Motivated by this theory, we
propose a method to align feature distributions across clients and to learn
high-quality, well-clustered representations. To make the NC-structure
applicable to multi-label settings, where image-level features may contain
multiple semantic concepts, we introduce a feature disentanglement module that
extracts semantically specific features. The clustering of these disentangled
class-wise features is guided by a predefined shared NC structure, which
mitigates potential conflicts between client models due to diverse local data
distributions. In addition, we design regularisation losses to encourage
compact clustering in the latent feature space. Experiments conducted on four
benchmark datasets across eight diverse settings demonstrate that our approach
outperforms existing methods, validating its effectiveness in this challenging
FL scenario.

</details>


### [26] [Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery Detection](https://arxiv.org/abs/2509.12546)
*Yingxin Lai,Zitong Yu,Jun Wang,Linlin Shen,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出Agent4FaceForgery：用多智能体模拟社交媒体中的伪造生成与互动，生成细粒度文本-图像一致性标注数据，通过自适应拒绝采样保证质量，显著提升多种检测器的真实场景表现。


<details>
  <summary>Details</summary>
Motivation: 现有人脸伪造检测在离线基准上表现良好，但在真实世界中失效，主要因训练数据生态无效：缺乏真实社交语境、复杂动机、多轮迭代及文本-图像博弈特征。

Method: 设计多智能体框架：由具备角色画像与记忆模块的LLM代理在模拟社交环境中合作/对抗，复现伪造生成的多样意图与迭代过程；同时生成包含细粒度文本-图像一致性标签的数据（超越二分类）；引入自适应拒绝采样（ARS）控制数据质量与多样性。

Result: 通过大量实验，使用该模拟数据训练/增强的多种架构的伪造检测器在性能上显著提升，相较仅用传统数据更接近真实场景需求。

Conclusion: 通过以代理驱动的数据生成与社交交互建模，可有效弥补数据生态缺口，提升人脸伪造检测的泛化与实用性，框架具有通用价值。

Abstract: Face forgery detection faces a critical challenge: a persistent gap between
offline benchmarks and real-world efficacy,which we attribute to the ecological
invalidity of training data.This work introduces Agent4FaceForgery to address
two fundamental problems: (1) how to capture the diverse intents and iterative
processes of human forgery creation, and (2) how to model the complex, often
adversarial, text-image interactions that accompany forgeries in social media.
To solve this,we propose a multi-agent framework where LLM-poweredagents,
equipped with profile and memory modules, simulate the forgery creation
process. Crucially, these agents interact in a simulated social environment to
generate samples labeled for nuanced text-image consistency, moving beyond
simple binary classification. An Adaptive Rejection Sampling (ARS) mechanism
ensures data quality and diversity. Extensive experiments validate that the
data generated by our simulationdriven approach brings significant performance
gains to detectors of multiple architectures, fully demonstrating the
effectiveness and value of our framework.

</details>


### [27] [Explicit Multimodal Graph Modeling for Human-Object Interaction Detection](https://arxiv.org/abs/2509.12554)
*Wenxuan Ji,Haichao Shi,Xiao-Yu zhang*

Main category: cs.CV

TL;DR: 提出MGNM：用多模态图神经网络显式建模人-物交互关系，四阶段图结构+多层次视觉/语言特征交互，刷新HICO-DET与V-COCO表现，并在更强检测器下进一步增益且兼顾长尾。


<details>
  <summary>Details</summary>
Motivation: Transformer虽强但未显式编码HOI中的关系结构，限制交互识别；GNN天然擅长关系建模，适合HOI。因此动机是将GNN的关系优势与多模态信息结合，提升HOI检测。

Method: 构建多模态图网络MGNM：1) 将HOI任务表述为四阶段图结构，逐步在不同层级/阶段对人-物对及其关系进行显式建模与推理；2) 设计多层次特征交互机制，融合视觉与语言（多层级）特征，增强跨人-物对的信息传播与关系推断；3) 可与更先进的目标检测器对接。

Result: 在HICO-DET与V-COCO上达到SOTA；接入更强的目标检测器时性能显著提升，并在稀有与非稀有类别之间保持较好平衡。

Conclusion: 显式的GNN关系建模结合多模态特征交互能有效提升HOI检测，MGNM在标准基准上验证了其优越性与可扩展性。

Abstract: Transformer-based methods have recently become the prevailing approach for
Human-Object Interaction (HOI) detection. However, the Transformer architecture
does not explicitly model the relational structures inherent in HOI detection,
which impedes the recognition of interactions. In contrast, Graph Neural
Networks (GNNs) are inherently better suited for this task, as they explicitly
model the relationships between human-object pairs. Therefore, in this paper,
we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork
\textbf{M}odeling (MGNM) that leverages GNN-based relational structures to
enhance HOI detection. Specifically, we design a multimodal graph network
framework that explicitly models the HOI task in a four-stage graph structure.
Furthermore, we introduce a multi-level feature interaction mechanism within
our graph network. This mechanism leverages multi-level vision and language
features to enhance information propagation across human-object pairs.
Consequently, our proposed MGNM achieves state-of-the-art performance on two
widely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a
more advanced object detector, our method demonstrates a significant
performance gain and maintains an effective balance between rare and non-rare
classes.

</details>


### [28] [VQT-Light:Lightweight HDR Illumination Map Prediction with Richer Texture.pdf](https://arxiv.org/abs/2509.12556)
*Kunliang Xie*

Main category: cs.CV

TL;DR: 提出VQT-Light：基于VQVAE与ViT的光照估计框架，将光照贴图预测转化为多分类问题，实现40FPS、纹理更丰富与更高保真度，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有光照估计方法在两难之间：要么难以恢复照明贴图细节纹理，要么速度慢且纹理保真度不足；同时CNN在全局上下文与视野外光照推断上受限，且连续潜变量方法易出现posterior collapse。

Method: 构建两模块框架：1) 特征提取：用VQVAE离散化光照贴图特征，避免posterior collapse；2) 光照估计：用ViT捕获输入图像的全局上下文与长程依赖，提升视野外光照预测；将任务表述为多类别分类，通过代码本与分类器从图像到光照贴图索引的映射进行重建。

Result: 模型可在保持轻量与快速的同时生成纹理更丰富、更高保真的光照贴图；推理速度达40FPS，并在多项评测指标上优于现有方法；定性与定量实验均显示SOTA性能。

Conclusion: 离散表示(VQVAE)与全局建模(ViT)的结合，将光照估计转化为分类有效提升细节与保真度并兼顾速度；VQT-Light在准确性与效率上均超越现有SOTA。

Abstract: Accurate lighting estimation is a significant yet challenging task in
computer vision and graphics. However, existing methods either struggle to
restore detailed textures of illumination map, or face challenges in running
speed and texture fidelity. To tackle this problem, we propose a novel
framework (VQT-Light) based on VQVAE and ViT architecture. VQT-Light includes
two modules: feature extraction and lighting estimation. First, we take
advantages of VQVAE to extract discrete features of illumination map rather
than continuous features to avoid "posterior collapse". Second, we capture
global context and dependencies of input image through ViT rather than CNNs to
improve the prediction of illumination outside the field of view. Combining the
above two modules, we formulate the lighting estimation as a multiclass
classification task, which plays a key role in our pipeline. As a result, our
model predicts light map with richer texture and better fidelity while keeping
lightweight and fast. VQT-Light achieves an inference speed of 40FPS and
improves multiple evaluation metrics. Qualitative and quantitative experiments
demonstrate that the proposed method realizes superior results compared to
existing state-of-the-art methods.

</details>


### [29] [Adaptive Sampling Scheduler](https://arxiv.org/abs/2509.12569)
*Qi Wang,Shuliang Zhu,Jinjia Zhou*

Main category: cs.CV

TL;DR: 提出一种适用于多种一致性蒸馏框架的自适应采样调度器，通过动态步长选择、优化的交替采样和稳健的高引导生成技巧，显著提升扩散模型的生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有一致性蒸馏在选择目标时间步时依赖固定或随机策略，且常需为不同蒸馏过程单独设计调度器，导致灵活性不足、采样潜力受限，难以在实际复杂场景中发挥最佳性能。

Method: 设计一个通用自适应采样调度器，包含三点：1) 动态目标时间步选择：基于时间步“重要性”度量自适应选择；2) 优化的沿解轨迹交替采样：依据重要性在前向去噪与反向加噪间切换，引导更有效的解空间探索；3) 稳定高质生成技巧：在高 guidance scale 下引入平滑剪裁与色彩平衡，提升稳定性与视觉质量。

Result: 在多种一致性蒸馏方法上做了系统实验，结果显示生成性能稳定且显著提升，并验证了方法对不同框架的强适配性与泛化性。

Conclusion: 自适应采样调度器能在无需为各蒸馏方法专门定制的情况下提升质量与效率，扩展了一致性蒸馏模型在复杂生成场景中的适用性。

Abstract: Consistent distillation methods have evolved into effective techniques that
significantly accelerate the sampling process of diffusion models. Although
existing methods have achieved remarkable results, the selection of target
timesteps during distillation mainly relies on deterministic or stochastic
strategies, which often require sampling schedulers to be designed specifically
for different distillation processes. Moreover, this pattern severely limits
flexibility, thereby restricting the full sampling potential of diffusion
models in practical applications. To overcome these limitations, this paper
proposes an adaptive sampling scheduler that is applicable to various
consistency distillation frameworks. The scheduler introduces three innovative
strategies: (i) dynamic target timestep selection, which adapts to different
consistency distillation frameworks by selecting timesteps based on their
computed importance; (ii) Optimized alternating sampling along the solution
trajectory by guiding forward denoising and backward noise addition based on
the proposed time step importance, enabling more effective exploration of the
solution space to enhance generation performance; and (iii) Utilization of
smoothing clipping and color balancing techniques to achieve stable and
high-quality generation results at high guidance scales, thereby expanding the
applicability of consistency distillation models in complex generation
scenarios. We validated the effectiveness and flexibility of the adaptive
sampling scheduler across various consistency distillation methods through
comprehensive experimental evaluations. Experimental results consistently
demonstrated significant improvements in generative performance, highlighting
the strong adaptability achieved by our method.

</details>


### [30] [DisorientLiDAR: Physical Attacks on LiDAR-based Localization](https://arxiv.org/abs/2509.12595)
*Yizhen Lao,Yu Zhang,Ziting Wang,Chengbo Wang,Yifei Xue,Wanpeng Shao*

Main category: cs.CV

TL;DR: 提出DisorientLiDAR，对激光雷达定位发起对抗性攻击：通过识别并移除局部关键点区域，使点云配准与车辆定位显著失准；在KITTI、Autoware与物理场景（近红外吸收材料遮挡）中均验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本研究多集中于3D感知（检测/分割），对基于LiDAR的定位攻击研究稀缺；而定位失真对自动驾驶安全影响巨大，需探索可行且现实的攻击路径。

Method: 逆向分析定位/配准模型（如特征提取网络）以识别关键关键点（Top-K）；在点云中策略性移除包含这些关键点的局部区域；在仿真与真实系统中实施，包括在物理世界用近红外吸收材料隐藏关键区域。

Result: 在KITTI上针对HRegNet、D3Feat、GeoTransformer，移除Top-K关键点区域显著降低配准精度；在Autoware平台上，仅隐藏少量关键区域即可引入明显定位漂移；物理实验中成功复现KITTI上的攻击效果。

Conclusion: DisorientLiDAR能有效破坏LiDAR定位，具有可迁移性与物理可行性；识别并移除关键点是高效攻击面，提示需要在定位管线中提升对关键点缺失的鲁棒性与对物理遮挡的防御。

Abstract: Deep learning models have been shown to be susceptible to adversarial attacks
with visually imperceptible perturbations. Even this poses a serious security
challenge for the localization of self-driving cars, there has been very little
exploration of attack on it, as most of adversarial attacks have been applied
to 3D perception. In this work, we propose a novel adversarial attack framework
called DisorientLiDAR targeting LiDAR-based localization. By
reverse-engineering localization models (e.g., feature extraction networks),
adversaries can identify critical keypoints and strategically remove them,
thereby disrupting LiDAR-based localization. Our proposal is first evaluated on
three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and
GeoTransformer) using the KITTI dataset. Experimental results demonstrate that
removing regions containing Top-K keypoints significantly degrades their
registration accuracy. We further validate the attack's impact on the Autoware
autonomous driving platform, where hiding merely a few critical regions induces
noticeable localization drift. Finally, we extended our attacks to the physical
world by hiding critical regions with near-infrared absorptive materials,
thereby successfully replicate the attack effects observed in KITTI data. This
step has been closer toward the realistic physical-world attack that
demonstrate the veracity and generality of our proposal.

</details>


### [31] [Exploring Spectral Characteristics for Single Image Reflection Removal](https://arxiv.org/abs/2509.12627)
*Pengbo Guo,Chengxu Liu,Guoshuai Zhao,Xingsong Hou,Jialie Shen,Xueming Qian*

Main category: cs.CV

TL;DR: 提出以光谱学习视角进行反射去除：用“光谱码本”重建反射的光学谱，并配合光谱先验细化与谱感知Transformer，在谱域与像素域联合分离反射与透射；在三套基准上优于SOTA且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在图像域处理，忽视反射光在不同光源下的光谱差异，导致反射与透射重叠难以区分，恢复背景困难。作者希望利用光谱维度（波长差异）作为可分辨线索。

Method: 1) 光谱码本：重建反射图像的光学光谱，显式建模不同光源的波长分布；2) 两个光谱先验细化模块：在空间维重新分配像素、在波长维自适应增强光谱差异；3) 谱感知Transformer（Spectrum-Aware Transformer）：在谱域与像素域联合建模，协同恢复透射内容。

Result: 在三个反射去除基准上进行实验，整体性能优于当前SOTA，并显示较强的跨数据泛化能力。

Conclusion: 引入光谱学习与码本化重建能有效辨析反射，结合先验细化与谱感知Transformer可更好地分离反射与透射，取得领先效果并具有良好泛化。

Abstract: Eliminating reflections caused by incident light interacting with reflective
medium remains an ill-posed problem in the image restoration area. The primary
challenge arises from the overlapping of reflection and transmission components
in the captured images, which complicates the task of accurately distinguishing
and recovering the clean background. Existing approaches typically address
reflection removal solely in the image domain, ignoring the spectral property
variations of reflected light, which hinders their ability to effectively
discern reflections. In this paper, we start with a new perspective on spectral
learning, and propose the Spectral Codebook to reconstruct the optical spectrum
of the reflection image. The reflections can be effectively distinguished by
perceiving the wavelength differences between different light sources in the
spectrum. To leverage the reconstructed spectrum, we design two spectral prior
refinement modules to re-distribute pixels in the spatial dimension and
adaptively enhance the spectral differences along the wavelength dimension.
Furthermore, we present the Spectrum-Aware Transformer to jointly recover the
transmitted content in spectral and pixel domains. Experimental results on
three different reflection benchmarks demonstrate the superiority and
generalization ability of our method compared to state-of-the-art models.

</details>


### [32] [Maps for Autonomous Driving: Full-process Survey and Frontiers](https://arxiv.org/abs/2509.12632)
*Pengxin Chen,Zhipeng Luo,Xiaoqi Jiang,Zhangcai Yin,Jonathan Li*

Main category: cs.CV

TL;DR: 论文综述自动驾驶地图从HD到Lite再到Implicit三阶段的演进，系统梳理各阶段的制图流程、技术挑战与学术解法，并展望新型地图表示与其在端到端自动驾驶中的融合。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶对地图的依赖、形态与生产成本都在快速变化：HD地图精度高但昂贵且难维护；Lite地图寻求在精度、成本与更新速度间折中；隐式地图尝试将环境先验融入模型内部以弱化外部地图依赖。需要一篇框架化综述统一概念、比较方法、总结挑战并指引未来研究。

Method: 按时间与技术范式将地图分为HD、Lite、Implicit三个阶段；对每一阶段，综述生产工作流（采集、标注/建图、融合、压缩/更新、发布）、关键技术点与难题，并汇总学术界解决方案；最后讨论新型表示（如语义稀疏元素、向量化、神经隐式场等）及其与端到端驾驶框架的耦合方案。

Result: 形成分阶段的系统评述：HD地图在精度与鲁棒性上成熟但成本高、时效差；Lite地图通过结构化/向量化与在线更新降低依赖与带宽；隐式地图将环境先验内化到感知-规划模型中，减少显式地图，但带来可解释性与泛化挑战；同时总结各环节的技术难题与已有方法。

Conclusion: 地图技术正由重、显式、高精向轻量与隐式过渡。未来趋势是在保证安全与泛化的前提下，将新型地图表示与端到端驾驶紧密融合，实现更低成本、更快更新与更强适应性的自动驾驶系统。

Abstract: Maps have always been an essential component of autonomous driving. With the
advancement of autonomous driving technology, both the representation and
production process of maps have evolved substantially. The article categorizes
the evolution of maps into three stages: High-Definition (HD) maps, Lightweight
(Lite) maps, and Implicit maps. For each stage, we provide a comprehensive
review of the map production workflow, with highlighting technical challenges
involved and summarizing relevant solutions proposed by the academic community.
Furthermore, we discuss cutting-edge research advances in map representations
and explore how these innovations can be integrated into end-to-end autonomous
driving frameworks.

</details>


### [33] [CIARD: Cyclic Iterative Adversarial Robustness Distillation](https://arxiv.org/abs/2509.12633)
*Liming Lu,Shuchao Pang,Xu Zheng,Xiang Gu,Anan Du,Yunhuai Liu,Yongbin Zhou*

Main category: cs.CV

TL;DR: 提出CIARD用于对抗鲁棒蒸馏，兼顾干净样本精度与对抗鲁棒性：通过多教师+对比推挤损对齐目标，并对教师进行持续对抗再训练以避免鲁棒退化；在CIFAR-10/100与Tiny-ImageNet上相较基线平均提升对抗防御率3.53和干净准确率5.87。


<details>
  <summary>Details</summary>
Motivation: 现有对抗鲁棒蒸馏常因双教师（干净教师与鲁棒教师）目标冲突、以及训练中鲁棒教师在迭代生成对抗样本下性能退化，导致学生模型鲁棒提升但干净精度下降。需要一种方法在不牺牲泛化的情况下稳定地传递鲁棒知识。

Method: 提出循环迭代ARD（CIARD）：(1) 多教师框架引入对比式推挤损（push-loss），在特征/Logit层面对齐，使学生在干净与鲁棒目标间获得一致的知识信号、缓解冲突；(2) 持续的教师对抗再训练（cyclic/continuous adversarial retraining），随着训练过程中对抗样本分布变化动态更新教师，避免鲁棒教师退化；整体以蒸馏损、对比对齐损与标准/对抗训练损联合优化学生。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet上，CIARD较现有ARD基线实现平均对抗防御率提升约3.53，并在干净样本准确率上提升约5.87；在多种攻击场景下均取得更优的鲁棒-精度权衡，建立新SOTA基线。

Conclusion: CIARD通过多教师对比对齐与教师的持续对抗再训练，缓解了双教师目标冲突与教师鲁棒退化两大瓶颈，实现了鲁棒性与干净精度的同步提升，为资源受限场景下的鲁棒蒸馏提供了更平衡有效的解决方案。

Abstract: Adversarial robustness distillation (ARD) aims to transfer both performance
and robustness from teacher model to lightweight student model, enabling
resilient performance on resource-constrained scenarios. Though existing ARD
approaches enhance student model's robustness, the inevitable by-product leads
to the degraded performance on clean examples. We summarize the causes of this
problem inherent in existing methods with dual-teacher framework as: 1. The
divergent optimization objectives of dual-teacher models, i.e., the clean and
robust teachers, impede effective knowledge transfer to the student model, and
2. The iteratively generated adversarial examples during training lead to
performance deterioration of the robust teacher model. To address these
challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key
innovations: a. A multi-teacher framework with contrastive push-loss alignment
to resolve conflicts in dual-teacher optimization objectives, and b. Continuous
adversarial retraining to maintain dynamic teacher robustness against
performance degradation from the varying adversarial examples. Extensive
experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD
achieves remarkable performance with an average 3.53 improvement in adversarial
defense rates across various attack scenarios and a 5.87 increase in clean
sample accuracy, establishing a new benchmark for balancing model robustness
and generalization. Our code is available at https://github.com/eminentgu/CIARD

</details>


### [34] [Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations](https://arxiv.org/abs/2509.12653)
*Jinjie Shen,Yaxiong Wang,Lechao Cheng,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出SAMM数据集与RamDG框架，面向“语义一致”的多模态篡改检测与定位，通过外部知识检索、图像伪造定位与深度操控检测联合编码，显著优于现有方法（在SAMM上检测准确率提升2.06%）。


<details>
  <summary>Details</summary>
Motivation: 现有多模态伪造检测基准多依赖跨模态不一致（如图文错配）作为信号，易与真实世界攻击不符；真实攻击更可能在视觉与文本上保持语义一致以提升欺骗性。因此需要能处理“语义协同”篡改的评测数据与方法。

Method: 1) 构建SAMM数据集：两阶段生成——先进行先进图像篡改，再自动生成与篡改语义一致、上下文合理的文本描述。2) 提出RamDG：检索外部知识作为上下文证据，与输入图文共同编码；结合图像伪造定位模块与深度操控检测模块，对篡改进行检测与定位（grounding）。

Result: 在SAMM基准上，RamDG较现有最优方法在检测准确率上提升2.06%，并在多项实验中表现显著优越。

Conclusion: 语义对齐的多模态篡改是更贴近真实世界的挑战。SAMM数据集与RamDG框架有效应对此类篡改，通过检索增强与多模块联合编码提升检测与定位性能，推动媒体取证向更真实场景迈进。

Abstract: The detection and grounding of manipulated content in multimodal data has
emerged as a critical challenge in media forensics. While existing benchmarks
demonstrate technical progress, they suffer from misalignment artifacts that
poorly reflect real-world manipulation patterns: practical attacks typically
maintain semantic consistency across modalities, whereas current datasets
artificially disrupt cross-modal alignment, creating easily detectable
anomalies. To bridge this gap, we pioneer the detection of
semantically-coordinated manipulations where visual edits are systematically
paired with semantically consistent textual descriptions. Our approach begins
with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM)
dataset, generated through a two-stage pipeline: 1) applying state-of-the-art
image manipulations, followed by 2) generation of contextually-plausible
textual narratives that reinforce the visual deception. Building on this
foundation, we propose a Retrieval-Augmented Manipulation Detection and
Grounding (RamDG) framework. RamDG commences by harnessing external knowledge
repositories to retrieve contextual evidence, which serves as the auxiliary
texts and encoded together with the inputs through our image forgery grounding
and deep manipulation detection modules to trace all manipulations. Extensive
experiments demonstrate our framework significantly outperforms existing
methods, achieving 2.06\% higher detection accuracy on SAMM compared to
state-of-the-art approaches. The dataset and code are publicly available at
https://github.com/shen8424/SAMM-RamDG-CAP.

</details>


### [35] [MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization](https://arxiv.org/abs/2509.12673)
*YiTong Liu,TianZhu Liu,YanFeng GU*

Main category: cs.CV

TL;DR: 提出MFAF：以EVA02为骨干，融合多频多尺度与空间注意，用于跨视角地理定位；在University-1652、SUES-200、Dense-UAV上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 跨视角（如无人机-卫星/地面）图像在视角、尺度、外观差异大，现有方法常靠特征图分块，忽略空间与语义关系，且难以同时捕获低频结构与高频细节，导致判别性不足、易受背景与视角变化干扰。

Method: 基于EVA02骨干，提出MFAF框架，包含：1) 多频分支块MFB：在多尺度上显式分解并提取低频结构与高频边缘细节，提升跨视角一致性与鲁棒性；2) 频率感知空间注意FSA：对频域特征施加自适应空间注意，聚焦关键区域，抑制背景噪声与视角变化带来的干扰；并进行跨视角匹配用于地理定位与导航。

Result: 在University-1652、SUES-200、Dense-UAV等基准上进行大量实验，MFAF取得具竞争力的性能，覆盖无人机定位与无人机导航两类任务（具体数值未在摘要中给出）。

Conclusion: 结合多频多尺度特征建模与频率感知空间注意，可有效提升跨视角地理定位的判别性与稳健性，对减轻背景噪声与视角变化有效，实验验证方法的竞争力。

Abstract: Cross-view geo-localization aims to determine the geographical location of a
query image by matching it against a gallery of images. This task is
challenging due to the significant appearance variations of objects observed
from variable views, along with the difficulty in extracting discriminative
features. Existing approaches often rely on extracting features through feature
map segmentation while neglecting spatial and semantic information. To address
these issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion
(MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block
(MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block
effectively captures both low-frequency structural features and high-frequency
edge details across multiple scales, improving the consistency and robustness
of feature representations across various viewpoints. Meanwhile, the FSA module
adaptively focuses on the key regions of frequency features, significantly
mitigating the interference caused by background noise and viewpoint
variability. Extensive experiments on widely recognized benchmarks, including
University-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method
achieves competitive performance in both drone localization and drone
navigation tasks.

</details>


### [36] [A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks](https://arxiv.org/abs/2509.12682)
*Gordon Hung,Ivan Felipe Rodriguez*

Main category: cs.CV

TL;DR: 对比YOLOv8–v11在两套水下数据集上的检测性能与速度，发现准确率在YOLOv9后趋于饱和，但推理速度持续提升；YOLOv10-s在AUV嵌入式部署上表现出最佳速度-精度权衡，并提供可复现实验基线与代码。


<details>
  <summary>Details</summary>
Motivation: AUV任务（栖息地测绘、生态监测、设施巡检）依赖视觉，但水下图像受光衰减、浑浊与类别不均衡影响且算力受限。陆地基准无法说明最新YOLO在海洋域的真实表现，亟需在水下场景中系统评估速度与精度。

Method: 整理两个公开水下数据集：珊瑚疾病（4480图/18类）与鱼类物种（7500图/20类）；为每个数据集设置四种训练规模（25/50/75/100%），固定平衡的验证/测试集。以相同超参训练YOLOv8/9/10/11-s（100轮、640输入、batch16、T4 GPU），评估Precision、Recall、mAP50、mAP50-95、单图推理时间、FPS，并用Grad-CAM进行特征与定位可解释性分析。

Result: 两数据集一致表明：准确率指标在YOLOv9后基本饱和，后续版本改进主要体现在效率；推理速度显著提高。轻量YOLOv10在速度-精度权衡上最佳，适合AUV嵌入式部署。

Conclusion: 提供首个在水下图像上的近期YOLO系统对比，给出YOLOv10作为AUV的优选方案，并发布开放、可复现的基准与代码，以促进海洋视觉研究。

Abstract: Autonomous underwater vehicles (AUVs) increasingly rely on on-board
computer-vision systems for tasks such as habitat mapping, ecological
monitoring, and infrastructure inspection. However, underwater imagery is
hindered by light attenuation, turbidity, and severe class imbalance, while the
computational resources available on AUVs are limited. One-stage detectors from
the YOLO family are attractive because they fuse localization and
classification in a single, low-latency network; however, their terrestrial
benchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how
successive YOLO releases perform in the marine domain. We curate two openly
available datasets that span contrasting operating conditions: a Coral Disease
set (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20
classes). For each dataset, we create four training regimes (25 %, 50 %, 75 %,
100 % of the images) while keeping balanced validation and test partitions
fixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical
hyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate
precision, recall, mAP50, mAP50-95, per-image inference time, and
frames-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature
utilization and localization faithfulness. Across both datasets, accuracy
saturates after YOLOv9, suggesting architectural innovations primarily target
efficiency rather than accuracy. Inference speed, however, improves markedly.
Our results (i) provide the first controlled comparison of recent YOLO variants
on underwater imagery, (ii) show that lightweight YOLOv10 offers the best
speed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an
open, reproducible benchmark and codebase to accelerate future marine-vision
research.

</details>


### [37] [StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo](https://arxiv.org/abs/2509.12683)
*Xianda Guo,Chenming Zhang,Ruilin Wang,Youmin Zhang,Wenzhao Zheng,Matteo Poggi,Hao Zhao,Qin Zou,Long Chen*

Main category: cs.CV

TL;DR: 提出StereoCarla高保真合成立体数据集，覆盖多相机配置与环境条件，显著提升跨数据集泛化，优于11个现有数据集，并在多数据集训练中进一步增益。


<details>
  <summary>Details</summary>
Motivation: 现有学习式立体匹配依赖合成数据但多样性不足，导致模型跨域泛化弱，难以满足自动驾驶与机器人复杂场景的深度感知需求。

Method: 基于CARLA构建大规模合成立体数据集StereoCarla：系统性采样多基线、视角、传感器布置；不同光照、天气、道路几何等环境；提供高保真标注。以多基准（KITTI2012/2015、Middlebury、ETH3D）进行跨域训练/测试，比较与11个现有立体数据集训练的模型，并将StereoCarla纳入多数据集联合训练评估兼容性与可扩展性。

Result: 在四个标准评测数据集上的跨域测试中，使用StereoCarla训练的模型在泛化精度上优于以11个其他数据集训练的模型；在多数据集联合训练中加入StereoCarla进一步提升跨域泛化性能。

Conclusion: StereoCarla作为面向自动驾驶的高保真、可控、多样化立体数据集，显著提升学习式立体匹配模型的跨域泛化，并与其他数据集兼容可扩展，为构建更鲁棒的深度感知系统提供有效基准与资源。

Abstract: Stereo matching plays a crucial role in enabling depth perception for
autonomous driving and robotics. While recent years have witnessed remarkable
progress in stereo matching algorithms, largely driven by learning-based
methods and synthetic datasets, the generalization performance of these models
remains constrained by the limited diversity of existing training data. To
address these challenges, we present StereoCarla, a high-fidelity synthetic
stereo dataset specifically designed for autonomous driving scenarios. Built on
the CARLA simulator, StereoCarla incorporates a wide range of camera
configurations, including diverse baselines, viewpoints, and sensor placements
as well as varied environmental conditions such as lighting changes, weather
effects, and road geometries. We conduct comprehensive cross-domain experiments
across four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury,
ETH3D) and demonstrate that models trained on StereoCarla outperform those
trained on 11 existing stereo datasets in terms of generalization accuracy
across multiple benchmarks. Furthermore, when integrated into multi-dataset
training, StereoCarla contributes substantial improvements to generalization
accuracy, highlighting its compatibility and scalability. This dataset provides
a valuable benchmark for developing and evaluating stereo algorithms under
realistic, diverse, and controllable settings, facilitating more robust depth
perception systems for autonomous vehicles. Code can be available at
https://github.com/XiandaGuo/OpenStereo, and data can be available at
https://xiandaguo.net/StereoCarla.

</details>


### [38] [SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes](https://arxiv.org/abs/2509.12701)
*Wenzhuo Jin,Qianfeng Yang,Xianhao Wu,Hongming Chen,Pengpeng Li,Xiang Chen*

Main category: cs.CV

TL;DR: 提出并公开了首个面向真实火灾早期监控场景的成对（有烟/无烟）图像数据集SmokeBench，并在其上系统评测多种图像去烟方法，填补真实配对数据缺失的空白。


<details>
  <summary>Details</summary>
Motivation: 早期火灾（点燃后0–15分钟）烟雾严重降低监控可见度，影响态势感知与救援；现有去烟研究受限于缺乏大规模、真实、严格配对的有烟/无烟图像数据，难以开展有监督训练与客观评测。

Method: 构建并发布真实世界监控图像去烟基准数据集SmokeBench：在多样场景与不同烟雾浓度下采集严格对齐的有烟/无烟图像对；提供用于监督学习与基准评测的高质量标注与划分；在该数据集上对多种去烟算法进行全面基线实验与对比。

Result: 获得一个覆盖多场景与多烟雾强度、具备精确对齐的成对图像数据集；实验给出了不同去烟方法在真实火灾监控场景中的系统化性能基线，验证了数据集对算法评测与训练的有效性。

Conclusion: SmokeBench为真实火灾场景图像去烟提供了关键的基础设施，促进鲁棒、实用的去烟算法发展与落地；数据集已公开，可从项目主页获取。

Abstract: Early-stage fire scenes (0-15 minutes after ignition) represent a crucial
temporal window for emergency interventions. During this stage, the smoke
produced by combustion significantly reduces the visibility of surveillance
systems, severely impairing situational awareness and hindering effective
emergency response and rescue operations. Consequently, there is an urgent need
to remove smoke from images to obtain clear scene information. However, the
development of smoke removal algorithms remains limited due to the lack of
large-scale, real-world datasets comprising paired smoke-free and
smoke-degraded images. To address these limitations, we present a real-world
surveillance image desmoking benchmark dataset named SmokeBench, which contains
image pairs captured under diverse scenes setup and smoke concentration. The
curated dataset provides precisely aligned degraded and clean images, enabling
supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of desmoking methods on our dataset. Our
dataset provides a valuable foundation for advancing robust and practical image
desmoking in real-world fire scenes. This dataset has been released to the
public and can be downloaded from https://github.com/ncfjd/SmokeBench.

</details>


### [39] [RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation](https://arxiv.org/abs/2509.12710)
*Siju Ma,Changsiyu Gong,Xiaofeng Fan,Yong Ma,Chengjie Jiang*

Main category: cs.CV

TL;DR: 提出RIS-FUSION框架，将文本驱动的红外-可见图像融合与指代分割统一优化，通过语言门控融合模块实现语义对齐，并发布大规模多模态指代分割基准MM-RIS；在mIoU上比现有方法提升超11%。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动融合缺乏与“文本是否真正发挥作用”对齐的监督与评测任务，难以衡量文本引导的有效性。指代图像分割与文本驱动融合共享“突出文本所指目标”的目标，因此可用RIS作为与融合目标一致的训练与评估信号。

Method: 提出级联的RIS-FUSION：联合优化文本驱动融合与指代分割。核心LangGatedFusion模块将文本特征注入融合骨干，实现跨模态语义对齐与目标突出。并构建MM-RIS数据集（红外-可见图像对+指代表达+分割掩膜），用于多模态指代分割任务的训练与评测。

Result: 在MM-RIS等实验中，RIS-FUSION取得SOTA，相比现有方法mIoU提升超过11%。代码与数据集将开源。

Conclusion: 通过将融合任务与指代分割统一，利用语言门控机制增强语义对齐，能显著提升文本对融合结果的有效引导；新数据集为该方向提供标准化训练与评测基准。

Abstract: Text-driven infrared and visible image fusion has gained attention for
enabling natural language to guide the fusion process. However, existing
methods lack a goal-aligned task to supervise and evaluate how effectively the
input text contributes to the fusion outcome. We observe that referring image
segmentation (RIS) and text-driven fusion share a common objective:
highlighting the object referred to by the text. Motivated by this, we propose
RIS-FUSION, a cascaded framework that unifies fusion and RIS through joint
optimization. At its core is the LangGatedFusion module, which injects textual
features into the fusion backbone to enhance semantic alignment. To support
multimodal referring image segmentation task, we introduce MM-RIS, a
large-scale benchmark with 12.5k training and 3.5k testing triplets, each
consisting of an infrared-visible image pair, a segmentation mask, and a
referring expression. Extensive experiments show that RIS-FUSION achieves
state-of-the-art performance, outperforming existing methods by over 11% in
mIoU. Code and dataset will be released at
https://github.com/SijuMa2003/RIS-FUSION.

</details>


### [40] [Learning by Imagining: Debiased Feature Augmentation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2509.12711)
*Haozhe Zhang,Chenchen Jing,Mingyu Liu,Qingsheng Wang,Hao Chen*

Main category: cs.CV

TL;DR: 提出DeFA方法，通过去偏的特征增强解决组合零样本学习中属性-对象纠缠与长尾分布问题，合成高保真组合特征，并在闭/开世界设定上达SOTA。


<details>
  <summary>Details</summary>
Motivation: CZSL需要在未见过的属性-对象组合上泛化，但属性与对象表征相互纠缠且数据长尾，导致对罕见/未见组合的识别能力弱。神经科学表明“想象≈感知”的机制启发可用“合成想象”来补足数据与偏差。

Method: 提出Debiased Feature Augmentation（DeFA）：(1) 解耦-重构框架，将属性与对象从已见样本中显式解耦并重新组合，以合成高保真组合特征；(2) 去偏策略，利用先验的已见属性与对象知识，减轻长尾与共现偏置对合成与学习的影响；最终以增强后的合成特征训练用于组合泛化。

Result: 在三个常用数据集上，DeFA在闭世界与开世界CZSL评测中均达到了当前最优性能。

Conclusion: 通过解耦重构与去偏的特征增强，DeFA有效利用已见先验生成高质量组合表征，提升了对未见组合的泛化能力，并在多数据集多设定上验证了有效性。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen
attribute-object compositions by learning prior knowledge of seen primitives,
\textit{i.e.}, attributes and objects. Learning generalizable compositional
representations in CZSL remains challenging due to the entangled nature of
attributes and objects as well as the prevalence of long-tailed distributions
in real-world data. Inspired by neuroscientific findings that imagination and
perception share similar neural processes, we propose a novel approach called
Debiased Feature Augmentation (DeFA) to address these challenges. The proposed
DeFA integrates a disentangle-and-reconstruct framework for feature
augmentation with a debiasing strategy. DeFA explicitly leverages the prior
knowledge of seen attributes and objects by synthesizing high-fidelity
composition features to support compositional generalization. Extensive
experiments on three widely used datasets demonstrate that DeFA achieves
state-of-the-art performance in both \textit{closed-world} and
\textit{open-world} settings.

</details>


### [41] [AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models](https://arxiv.org/abs/2509.12715)
*Heng Zhang,Haichuan Hu,Yaomin Shen,Weihao Yu,Yilei Yuan,Haochen You,Guo Cheng,Zijian Zhang,Lubin Gan,Huihui Wei,Hao Zhang,Jin Huang*

Main category: cs.CV

TL;DR: 提出AsyMoE：针对视觉-语言不对称性设计三类专家（同模态、双曲跨模态、证据优先语言），缓解深层语言专家失去情境锚定的问题，在准确率上较vanilla MoE与模态特定MoE分别提升26.58%与15.45%，激活参数较致密模型减少25.45%。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM中的MoE难以同时兼顾视觉的空间完整性与语言的序列上下文，导致深层语言专家依赖参数化知识、忽视提供的视觉/语言证据，跨模态交互与模态专长难平衡。

Method: 提出AsyMoE，将专家分为三组：1) 站内（同模态）专家，专注各自模态的特征处理；2) 双曲空间的跨模态专家，用于层级化建模跨模态交互；3) 证据优先的语言专家，通过机制抑制参数偏置、维持上下文锚定。整体以异质专家与路由策略显式建模视觉-语言不对称。

Result: 在多项实验中，AsyMoE较vanilla MoE提升26.58%准确率，较模态特定MoE提升15.45%；同时相对致密模型激活参数减少25.45%。

Conclusion: 针对LVLM的模态不对称性，AsyMoE通过三类专家与相应路由有效保持语言上下文与跨模态互动，显著提升性能并降低激活参数，优于现有MoE方案。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on multimodal tasks through scaled architectures and extensive training.
However, existing Mixture of Experts (MoE) approaches face challenges due to
the asymmetry between visual and linguistic processing. Visual information is
spatially complete, while language requires maintaining sequential context. As
a result, MoE models struggle to balance modality-specific features and
cross-modal interactions. Through systematic analysis, we observe that language
experts in deeper layers progressively lose contextual grounding and rely more
on parametric knowledge rather than utilizing the provided visual and
linguistic information. To address this, we propose AsyMoE, a novel
architecture that models this asymmetry using three specialized expert groups.
We design intra-modality experts for modality-specific processing, hyperbolic
inter-modality experts for hierarchical cross-modal interactions, and
evidence-priority language experts to suppress parametric biases and maintain
contextual grounding. Extensive experiments demonstrate that AsyMoE achieves
26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific
MoE respectively, with 25.45% fewer activated parameters than dense models.

</details>


### [42] [EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer](https://arxiv.org/abs/2509.12718)
*Pukun Zhao,Longxiang Wang,Miaowei Wang,Chen Chen,Fanqing Zhou,Haojian Huang*

Main category: cs.CV

TL;DR: 提出两个动态空间推理基准（局部可观察迷宫导航、配对消除），评估在局部感知、环境反馈与全局目标耦合下的空间理解与自适应规划，并引入基于主观经验的记忆机制促进跨任务迁移。实验证明主流模型在动态空间推理和长期记忆方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为静态或可全局观测，无法检验在部分可观测与环境动态变化下的长程推理与记忆利用能力，需要更贴近真实交互与规划难点的评测。

Method: 构建两类动态任务：1) 局部可观察迷宫导航；2) match-2 配对消除。两者都使每一步动作引发环境结构变化，迫使模型持续更新认知与策略。并提出“基于主观经验”的记忆机制，用于跨任务经验迁移与验证。

Result: 在所提基准上评测主流模型，发现其在动态空间理解、自适应规划与长期记忆方面表现受限，存在显著性能缺口。

Conclusion: 新基准全面揭示了现有模型在动态空间推理与长程记忆能力上的不足，并为未来方法改进提供了统一平台与可复现实验资源。

Abstract: Most existing spatial reasoning benchmarks focus on static or globally
observable environments, failing to capture the challenges of long-horizon
reasoning and memory utilization under partial observability and dynamic
changes. We introduce two dynamic spatial benchmarks, locally observable maze
navigation and match-2 elimination that systematically evaluate models'
abilities in spatial understanding and adaptive planning when local perception,
environment feedback, and global objectives are tightly coupled. Each action
triggers structural changes in the environment, requiring continuous update of
cognition and strategy. We further propose a subjective experience-based memory
mechanism for cross-task experience transfer and validation. Experiments show
that our benchmarks reveal key limitations of mainstream models in dynamic
spatial reasoning and long-term memory, providing a comprehensive platform for
future methodological advances. Our code and data are available at
https://anonymous.4open.science/r/EvoEmpirBench-143C/.

</details>


### [43] [SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation](https://arxiv.org/abs/2509.12721)
*Jingdong Zhang,Weikai Chen,Yuan Liu,Jionghao Wang,Zhengming Yu,Zhuowen Shen,Bo Yang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: 提出SPGen：基于2D球面展开（Spherical Projection, SP）的单视图3D生成框架，用单视点注入几何，避免多视不一致，并通过多层SP表示复杂内部结构；在图像域利用2D扩散先验实现高效微调与更高几何质量。


<details>
  <summary>Details</summary>
Motivation: 现有单视图3D生成多依赖多视扩散先验，存在视角间不一致，且难以表达复杂内部结构与非平凡拓扑；需要一种既一致、又能表达内外多层结构、且计算高效的新表示与生成方法。

Method: 将3D对象几何投影到包围球并展开成紧凑的多层2D球面投影（SP）图；通过注入式（injective）SP映射以单视点编码表面几何，在纯图像域上进行扩散模型生成与微调；多层SP用于表示嵌套内部结构，可直接提升为密闭或开放的3D表面。

Result: 在广泛实验中，SPGen在几何质量与计算效率上显著优于现有基线；能稳定避免多视不一致并更好重建复杂拓扑/内部结构。

Conclusion: SPGen通过将3D几何转化为多层2D球面投影并仅在图像域操作，兼具一致性、灵活性与效率，提供了单视图3D生成的新范式，并在质量与成本上取得明显优势。

Abstract: Existing single-view 3D generative models typically adopt multiview diffusion
priors to reconstruct object surfaces, yet they remain prone to inter-view
inconsistencies and are unable to faithfully represent complex internal
structure or nontrivial topologies. In particular, we encode geometry
information by projecting it onto a bounding sphere and unwrapping it into a
compact and structural multi-layer 2D Spherical Projection (SP) representation.
Operating solely in the image domain, SPGen offers three key advantages
simultaneously: (1) Consistency. The injective SP mapping encodes surface
geometry with a single viewpoint which naturally eliminates view inconsistency
and ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal
structures and support direct lifting to watertight or open 3D surfaces; (3)
Efficiency. The image-domain formulation allows the direct inheritance of
powerful 2D diffusion priors and enables efficient finetuning with limited
computational resources. Extensive experiments demonstrate that SPGen
significantly outperforms existing baselines in geometric quality and
computational efficiency.

</details>


### [44] [Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models](https://arxiv.org/abs/2509.12724)
*Yunhan Zhao,Xiang Zheng,Xingjun Ma*

Main category: cs.CV

TL;DR: 论文提出Defense2Attack：把“弱防御”融入攻击流程，反而显著提升对VLM的越狱成功率与一次性效率。方法含视觉通用扰动（带正向/鼓励语义）、防御风格文本优化、以及红队后缀生成（经RFT）。在4个VLM与4个安全基准上，一次尝试即优于需多次试探的SOTA攻击。


<details>
  <summary>Details</summary>
Motivation: 现有VLM越狱攻击虽有效但往往需要多轮试探、稳定性和效率不足。作者观察到：在攻击中注入“弱防御/防御模式”会令模型更顺从安全化表达，从而降低安全触发、提高绕过概率与一次性成功率。因此探索将防御启发的提示与扰动系统化为攻击策略。

Method: Defense2Attack由三部分组成：1) 视觉优化器：学习通用对抗扰动，并在视觉信号中编码正面、肯定、鼓励的语义，引导模型以“合规语气”响应；2) 文本优化器：用“防御风格”提示对输入进行重写/润色，使请求看似安全、符合政策；3) 红队后缀生成器：通过强化微调（RFT）产出高效越狱后缀，进一步提升触发率与鲁棒性。整体形成图文联合的对抗提示流水线。

Result: 在四个VLM与四个安全评测上，Defense2Attack以单次尝试取得更高的越狱成功率，超过需多次尝试的SOTA基线；同时效率提升，稳定性更好。

Conclusion: 引入“防御式模式”作为攻击引导是有效且通用的思路：弱防御可被武器化为攻击组件。Defense2Attack展示了该视角的实用性，并为VLM安全对抗研究提供了新的攻击范式与评测基准。

Abstract: Despite their superb capabilities, Vision-Language Models (VLMs) have been
shown to be vulnerable to jailbreak attacks. While recent jailbreaks have
achieved notable progress, their effectiveness and efficiency can still be
improved. In this work, we reveal an interesting phenomenon: incorporating weak
defense into the attack pipeline can significantly enhance both the
effectiveness and the efficiency of jailbreaks on VLMs. Building on this
insight, we propose Defense2Attack, a novel jailbreak method that bypasses the
safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak
prompt design. Specifically, Defense2Attack consists of three key components:
(1) a visual optimizer that embeds universal adversarial perturbations with
affirmative and encouraging semantics; (2) a textual optimizer that refines the
input using a defense-styled prompt; and (3) a red-team suffix generator that
enhances the jailbreak through reinforcement fine-tuning. We empirically
evaluate our method on four VLMs and four safety benchmarks. The results
demonstrate that Defense2Attack achieves superior jailbreak performance in a
single attempt, outperforming state-of-the-art attack methods that often
require multiple tries. Our work offers a new perspective on jailbreaking VLMs.

</details>


### [45] [Effective Gaussian Management for High-fidelity Object Reconstruction](https://arxiv.org/abs/2509.12742)
*Jiateng Liu,Hao Gao,Jiu-Cheng Xie,Chi-Man Pun,Jian Xiong,Haolun Li,Feng Xu*

Main category: cs.CV

TL;DR: 提出一种针对高保真三维对象重建的“高斯管理”方法：动态启用SH或法线、基于梯度自适应调阶与任务解耦剪枝，实现更高质量重建与更小模型规模，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Gaussian Splatting方法对属性（如SH、法线）一刀切赋值，导致双重监督（外观/表面）产生梯度冲突，重建质量受限且参数冗余；需要一种在不牺牲质量下提升效率、兼顾可集成性的管理机制。

Method: 1) 新型致密化/激活策略：在表面重建模块监督下，动态为每个高斯选择启用SH或法线，缓解双监督梯度冲突。2) 轻量高斯表示：依据梯度幅值自适应调整每个高斯的SH阶数；3) 任务解耦剪枝：在不损伤某任务的前提下，去除对目标重建任务影响最小的高斯，平衡表示能力与参数量。方法可无缝集成到其他框架。

Result: 在大量实验中，相比SOTA方法，获得更高的重建质量与效率，用显著更少的参数达到或超过现有最佳性能。

Conclusion: 动态属性管理+自适应调阶+解耦剪枝构成的高斯管理策略有效缓解梯度冲突，提高重建精度与效率，且具模型无关性，易集成，参数更少、性能更优。

Abstract: This paper proposes an effective Gaussian management approach for
high-fidelity object reconstruction. Departing from recent Gaussian Splatting
(GS) methods that employ indiscriminate attribute assignment, our approach
introduces a novel densification strategy that dynamically activates spherical
harmonics (SHs) or normals under the supervision of a surface reconstruction
module, which effectively mitigates the gradient conflicts caused by dual
supervision and achieves superior reconstruction results. To further improve
representation efficiency, we develop a lightweight Gaussian representation
that adaptively adjusts the SH orders of each Gaussian based on gradient
magnitudes and performs task-decoupled pruning to remove Gaussian with minimal
impact on a reconstruction task without sacrificing others, which balances the
representational capacity with parameter quantity. Notably, our management
approach is model-agnostic and can be seamlessly integrated into other
frameworks, enhancing performance while reducing model size. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art approaches in both reconstruction quality and efficiency,
achieving superior performance with significantly fewer parameters.

</details>


### [46] [Modelling and analysis of the 8 filters from the "master key filters hypothesis" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory](https://arxiv.org/abs/2509.12746)
*Tony Lindeberg,Zahra Babaiee,Peyman M. Kiasari*

Main category: cs.CV

TL;DR: 论文对ConvNeXt架构中深度可分离卷积所学到的感受野进行聚类，得到8个“主钥滤波器”，并用离散高斯平滑+差分算子的可分离离散尺度空间模型进行拟合；在多种约束与目标（方差匹配、L1/L2最小化）下，理想化模型与学习滤波器高度吻合，且替换后预测性能良好。


<details>
  <summary>Details</summary>
Motivation: 理解并简化深度可分离网络中学得的空间滤波器结构，验证其是否可由更简单、具有解释性的离散尺度空间（离散高斯平滑与差分）模型刻画，从而提升可解释性与潜在的可替代性/可压缩性。

Method: 1) 对学得滤波器（感受野）计算基于绝对值加权的空间均值与方差，用以检验可分离性与偏移接近半像素的假设；2) 通过聚类得到8个“主钥滤波器”；3) 构建两类理想化模型：a) 各坐标方向可用不同尺度参数；b) 两方向使用相同尺度；4) 以两种拟合准则：a) 匹配绝对值加权方差；b) 最小化与学习滤波器间的离散L1或L2距离；5) 在深度可分离网络中用理想化滤波器替换原学得滤波器进行实验验证。

Result: 理想化的离散尺度空间模型（离散高斯平滑+差分）与聚类得到的主滤波器在形状与空间扩展上高度一致；满足不同拟合准则时均能取得良好匹配；在网络中替换后对性能的影响很小，显示出良好的预测与替代能力。

Conclusion: 深度可分离网络学得的空间滤波器可被可分离的离散尺度空间滤波器（高斯平滑与差分算子）良好近似；非居中的滤波器偏移接近半像素；这一建模提高了滤波器的可解释性，并为用理想化滤波器替换学习权重提供了可行途径。

Abstract: This paper presents the results of analysing and modelling a set of 8
``master key filters'', which have been extracted by applying a clustering
approach to the receptive fields learned in depthwise-separable deep networks
based on the ConvNeXt architecture.
  For this purpose, we first compute spatial spread measures in terms of
weighted mean values and weighted variances of the absolute values of the
learned filters, which support the working hypotheses that: (i) the learned
filters can be modelled by separable filtering operations over the spatial
domain, and that (ii) the spatial offsets of the those learned filters that are
non-centered are rather close to half a grid unit. Then, we model the clustered
``master key filters'' in terms of difference operators applied to a spatial
smoothing operation in terms of the discrete analogue of the Gaussian kernel,
and demonstrate that the resulting idealized models of the receptive fields
show good qualitative similarity to the learned filters.
  This modelling is performed in two different ways: (i) using possibly
different values of the scale parameters in the coordinate directions for each
filter, and (ii) using the same value of the scale parameter in both coordinate
directions. Then, we perform the actual model fitting by either (i) requiring
spatial spread measures in terms of spatial variances of the absolute values of
the receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or
$l_2$-norms between the idealized receptive field models and the learned
filters.
  Complementary experimental results then demonstrate the idealized models of
receptive fields have good predictive properties for replacing the learned
filters by idealized filters in depthwise-separable deep networks, thus showing
that the learned filters in depthwise-separable deep networks can be well
approximated by discrete scale-space filters.

</details>


### [47] [What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment](https://arxiv.org/abs/2509.12750)
*Rishab Parthasarathy,Jasmine Collins,Cory Stephenson*

Main category: cs.CV

TL;DR: 研究比较人类与多模态LLM在评估文生图质量时依赖的属性与关联性，发现LLM对属性关系把握更弱，且在某些属性（如解剖准确性）上判断困难。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估文生图质量困难，尽管有用多模态LLM打分的方法，但缺乏对其如何利用人类相关概念（风格、构图等）形成整体判断的可解释性与一致性理解。

Method: 1) 构建基于合成图像对的人类偏好数据集；2) 对多个图像质量属性（美学、无伪影、解剖准确、构图正确、目标遵循、风格）进行任务间相关性分析，比较人类与LLM的属性间相关结构；3) 为每个属性轴合成高可控的数据集，分别评估人类与LLM在该单一属性上的判别能力。

Result: 人类在多个属性上形成判断时的属性间相关性较强；LLM的属性间关系显著更弱。在人类对各单一属性（如高/低美学）判断容易，而LLM在部分属性上表现不佳，尤其是解剖准确性。

Conclusion: 人类与多模态LLM在图像质量评估的感知与属性利用上存在系统差异；当前LLM难以稳定捕捉属性间关系并准确评判某些细粒度属性（如解剖），提示需要改进模型训练与评测基准以提高可解释性与一致性。

Abstract: Automated evaluation of generative text-to-image models remains a challenging
problem. Recent works have proposed using multimodal LLMs to judge the quality
of images, but these works offer little insight into how multimodal LLMs make
use of concepts relevant to humans, such as image style or composition, to
generate their overall assessment. In this work, we study what attributes of an
image--specifically aesthetics, lack of artifacts, anatomical accuracy,
compositional correctness, object adherence, and style--are important for both
LLMs and humans to make judgments on image quality. We first curate a dataset
of human preferences using synthetically generated image pairs. We use
inter-task correlation between each pair of image quality attributes to
understand which attributes are related in making human judgments. Repeating
the same analysis with LLMs, we find that the relationships between image
quality attributes are much weaker. Finally, we study individual image quality
attributes by generating synthetic datasets with a high degree of control for
each axis. Humans are able to easily judge the quality of an image with respect
to all of the specific image quality attributes (e.g. high vs. low aesthetic
image), however we find that some attributes, such as anatomical accuracy, are
much more difficult for multimodal LLMs to learn to judge. Taken together,
these findings reveal interesting differences between how humans and multimodal
LLMs perceive images.

</details>


### [48] [Recurrent Cross-View Object Geo-Localization](https://arxiv.org/abs/2509.12757)
*Xiaohan Zhang,Si-Yuan Cao,Xiaokai Bai,Yiming Li,Zhangkai Shen,Zhe Wu,Xiaoxi Hu,Hui-liang Shen*

Main category: cs.CV

TL;DR: 提出ReCOT，将跨视角目标地理定位从“一步到位”的回归改为循环细化定位，通过可学习任务token反复关注参考特征，结合SAM蒸馏与参考特征增强模块，在基准上达SOTA且参数降60%。


<details>
  <summary>Details</summary>
Motivation: 现有CVOGL方法把问题当作一次性检测，易受跨视特征噪声影响且缺乏纠错机制，导致定位不稳和精度不足。需要一种能逐步修正预测、并利用更清晰语义与更突出目标区域表示的方法。

Method: 提出ReCOT（循环Transformer）：1）从查询图像与点提示中构造可学习任务token，循环地对参考卫星图特征做注意力，逐步细化预测位置；2）SAM知识蒸馏：将Segment Anything的分割先验蒸馏到模型以提供更清晰语义指导，推理时无额外开销；3）参考特征增强模块（RFEM）：引入分层注意力，强化与目标相关的参考区域表达。

Result: 在标准CVOGL基准上达到最新SOTA，同时相比此前SOTA参数量减少约60%。

Conclusion: 将CVOGL重构为可循环纠错的定位过程，借助SAM先验与层级特征增强，显著提升精度与效率，表明递归注意力+先验蒸馏是跨视角地理定位的有效范式。

Abstract: Cross-view object geo-localization (CVOGL) aims to determine the location of
a specific object in high-resolution satellite imagery given a query image with
a point prompt. Existing approaches treat CVOGL as a one-shot detection task,
directly regressing object locations from cross-view information aggregation,
but they are vulnerable to feature noise and lack mechanisms for error
correction. In this paper, we propose ReCOT, a Recurrent Cross-view Object
geo-localization Transformer, which reformulates CVOGL as a recurrent
localization task. ReCOT introduces a set of learnable tokens that encode
task-specific intent from the query image and prompt embeddings, and
iteratively attend to the reference features to refine the predicted location.
To enhance this recurrent process, we incorporate two complementary modules:
(1) a SAM-based knowledge distillation strategy that transfers segmentation
priors from the Segment Anything Model (SAM) to provide clearer semantic
guidance without additional inference cost, and (2) a Reference Feature
Enhancement Module (RFEM) that introduces a hierarchical attention to emphasize
object-relevant regions in the reference features. Extensive experiments on
standard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art
(SOTA) performance while reducing parameters by 60% compared to previous SOTA
approaches.

</details>


### [49] [A-TDOM: Active TDOM via On-the-Fly 3DGS](https://arxiv.org/abs/2509.12759)
*Yiwei Xu,Xiang Wang,Yifei Yu,Wentian Gan,Luca Morelli,Giulio Perda,Xiongwu Xiao,Zongqian Zhan,Xin Wang,Fabio Remondino*

Main category: cs.CV

TL;DR: 提出A-TDOM：基于在线3D高斯表面(3DGS)与即刻SfM的近实时真值数字正射影像(正射影像图)生成，实现每张新图像到达后秒级优化与渲染，兼顾质量与几何精度。


<details>
  <summary>Details</summary>
Motivation: 传统TDOM依赖离线、复杂的摄影测量流程（全量SfM/空三、密集匹配、DSM/模型构建、正射纠正与镶嵌），延迟大，不适合实时/近实时场景；同时容易受相机位姿、DSM误差与遮挡影响导致正射质量下降。需要一种能在数据流式到达时增量更新、快速渲染、并缓解遮挡与姿态/表面误差影响的方法。

Method: 提出A-TDOM：1) On-the-Fly SfM在图像到达时即时估计该帧位姿与稀疏点云；2) 将新高斯（3DGS）增量融入先前场景，对新区域或粗糙区域进行局部优化；3) 结合orthogonal splatting进行正射渲染，实现每次3DGS场更新后即可输出TDOM；整体实现流式、增量、秒级的3DGS优化与渲染。

Result: 在多组基准数据上，A-TDOM可在每张新图像到来后数秒内完成3DGS优化并输出可用的TDOM，保持可接受的渲染质量与几何精度，满足近实时需求。

Conclusion: A-TDOM证明了基于在线3DGS与即刻SfM的增量优化框架能实现近实时TDOM生成，在延迟与质量之间取得平衡，适用于对时效性要求高的城市管理、规划、测绘等应用。

Abstract: True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in
various fields such as urban management, city planning, land surveying, etc.
However, traditional TDOM generation methods generally rely on a complex
offline photogrammetric pipeline, resulting in delays that hinder real-time
applications. Moreover, the quality of TDOM may degrade due to various
challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and
scene occlusions. To address these challenges, this work introduces A-TDOM, a
near real-time TDOM generation method based on On-the-Fly 3DGS optimization. As
each image is acquired, its pose and sparse point cloud are computed via
On-the-Fly SfM. Then new Gaussians are integrated and optimized into previously
unseen or coarsely reconstructed regions. By integrating with orthogonal
splatting, A-TDOM can render just after each update of a new 3DGS field.
Initial experiments on multiple benchmarks show that the proposed A-TDOM is
capable of actively rendering TDOM in near real-time, with 3DGS optimization
for each new image in seconds while maintaining acceptable rendering quality
and TDOM geometric accuracy.

</details>


### [50] [DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation](https://arxiv.org/abs/2509.12763)
*Yican Zhao,Ce Wang,You Hao,Lei Li,Tianli Liao*

Main category: cs.CV

TL;DR: 提出DyGLNet，通过融合全局与局部特征并引入动态上采样，实现高效、精确的医学图像分割，尤其在边界与小目标上表现突出，同时计算开销更低。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割常遇到多尺度病灶差异、边界模糊以及高计算成本三大难题；现有方法在捕获全局上下文与局部细节的平衡、以及高质量重建与效率之间存在权衡，亟需一种兼顾精度与效率的架构。

Method: 提出DyGLNet，包括：1) SHDCBlock：将单头自注意力（捕获全局上下文、低成本）与多尺度空洞卷积（建模局部细节与尺度变化）混合的特征提取模块；2) DyFusionUp：基于可学习偏移的动态自适应上采样，实现高保真特征图重建与边界细化；3) 轻量化设计以降低参数量与FLOPs；4) 融合全局与局部特征的解码策略。

Result: 在7个公开数据集上优于现有方法，尤其在边界精度与小目标分割上更佳，同时具备更低的计算复杂度。

Conclusion: DyGLNet在保持或提升分割精度（尤其边界与小目标）同时显著降低计算成本，为临床医学图像分析提供高效可靠的解决方案；代码即将开源。

Abstract: Medical image segmentation grapples with challenges including multi-scale
lesion variability, ill-defined tissue boundaries, and computationally
intensive processing demands. This paper proposes the DyGLNet, which achieves
efficient and accurate segmentation by fusing global and local features with a
dynamic upsampling mechanism. The model innovatively designs a hybrid feature
extraction module (SHDCBlock), combining single-head self-attention and
multi-scale dilated convolutions to model local details and global context
collaboratively. We further introduce a dynamic adaptive upsampling module
(DyFusionUp) to realize high-fidelity reconstruction of feature maps based on
learnable offsets. Then, a lightweight design is adopted to reduce
computational overhead. Experiments on seven public datasets demonstrate that
DyGLNet outperforms existing methods, particularly excelling in boundary
accuracy and small-object segmentation. Meanwhile, it exhibits lower
computation complexity, enabling an efficient and reliable solution for
clinical medical image analysis. The code will be made available soon.

</details>


### [51] [BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers](https://arxiv.org/abs/2509.12768)
*Mohammed Al-Habib,Zuping Zhang,Abdulrahman Noman*

Main category: cs.CV

TL;DR: 提出BATR-FST：通过预训练MIM与元微调中的双层自适应token精炼，提高ViT在小样本分类中的表现，兼顾全局与局部信息并增强判别性，1/5-shot均达SOTA。


<details>
  <summary>Details</summary>
Motivation: ViT在小样本学习中受限于：数据少导致token交互难精炼、缺乏强归纳偏置、现有方法依赖僵化匹配或简单相似度，难以有效融合全局上下文与局部细化特征。

Method: 两阶段框架：1) 预训练：采用遮罩图像建模(MIM)学习可迁移的patch级表示。2) 元微调：引入双层自适应token精炼(BATR)模块，包括：a) Token Clustering捕获局部交互；b) 不确定性感知权重分配(UAW)强调可靠token；c) 双层注意力(簇内/簇间)平衡局部与全局；d) 图Token传播在支撑与查询间传播语义一致性；e) 类间分离惩罚维持类别边界，增强判别性。

Result: 在三个小样本基准上，1-shot与5-shot均优于现有方法，显著提升基于Transformer的小样本分类性能。

Conclusion: 通过MIM预训练与元微调阶段的双层自适应token精炼，BATR-FST有效弥补ViT在小样本场景的归纳偏置与交互建模不足，实现更强的判别与泛化能力，并取得SOTA实证。

Abstract: Vision Transformers (ViTs) have shown significant promise in computer vision
applications. However, their performance in few-shot learning is limited by
challenges in refining token-level interactions, struggling with limited
training data, and developing a strong inductive bias. Existing methods often
depend on inflexible token matching or basic similarity measures, which limit
the effective incorporation of global context and localized feature refinement.
To address these challenges, we propose Bi-Level Adaptive Token Refinement for
Few-Shot Transformers (BATR-FST), a two-stage approach that progressively
improves token representations and maintains a robust inductive bias for
few-shot classification. During the pre-training phase, Masked Image Modeling
(MIM) provides Vision Transformers (ViTs) with transferable patch-level
representations by recreating masked image regions, providing a robust basis
for subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates
a Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to
capture localized interactions, Uncertainty-Aware Token Weighting to prioritize
dependable features, and a Bi-Level Attention mechanism to balance
intra-cluster and inter-cluster relationships, thereby facilitating thorough
token refinement. Furthermore, Graph Token Propagation ensures semantic
consistency between support and query instances, while a Class Separation
Penalty preserves different class borders, enhancing discriminative capability.
Extensive experiments on three benchmark few-shot datasets demonstrate that
BATR-FST achieves superior results in both 1-shot and 5-shot scenarios and
improves the few-shot classification via transformers.

</details>


### [52] [CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT](https://arxiv.org/abs/2509.12777)
*Zhifang Gong,Shuo Gao,Ben Zhao,Yingjing Xu,Yijun Yang,Shenghong Ju,Guangquan Zhou*

Main category: cs.CV

TL;DR: 提出首个利用多期CECT自动联合判别胰腺肿瘤亚型的方法，基于Mamba的时空建模与新颖采样/融合模块，在270例数据上区分PDAC与PNET达97.4%准确率与98.6%AUC。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效利用放射科诊断流程中的多期CECT上下文，尤其是跨期（动脉、静脉、延迟等）对比增强差异与空间-时间关联，导致胰腺肿瘤亚型判别受限。

Method: 以Mamba为核心的双层级对比增强感知模块：设计空间与时间两种采样序列，分别捕捉期内与期间对比增强变化；在时间建模中加入相似度引导的细化模块，突出肿瘤局部、显著时变区域；并通过空间互补整合器与多粒度融合模块进行跨尺度语义编码与聚合，实现更有效的多期时空特征学习。

Result: 在自建270例临床数据集上，二分类（PDAC vs PNET）取得97.4%准确率、98.6% AUC，优于以往方法（文中暗示）。

Conclusion: 多期CECT的时空联合建模（基于Mamba与新型采样/融合与细化机制）显著提升胰腺肿瘤亚型区分能力，展示为更准确高效的临床辅助诊断工具的潜力。

Abstract: Contrast-enhanced computed tomography (CECT) is the primary imaging technique
that provides valuable spatial-temporal information about lesions, enabling the
accurate diagnosis and subclassification of pancreatic tumors. However, the
high heterogeneity and variability of pancreatic tumors still pose substantial
challenges for precise subtyping diagnosis. Previous methods fail to
effectively explore the contextual information across multiple CECT phases
commonly used in radiologists' diagnostic workflows, thereby limiting their
performance. In this paper, we introduce, for the first time, an automatic way
to combine the multi-phase CECT data to discriminate between pancreatic tumor
subtypes, among which the key is using Mamba with promising learnability and
simplicity to encourage both temporal and spatial modeling from multi-phase
CECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware
Mamba module incorporating two novel spatial and temporal sampling sequences to
explore intra and inter-phase contrast variations of lesions. A
similarity-guided refinement module is also imposed into the temporal scanning
modeling to emphasize the learning on local tumor regions with more obvious
temporal variations. Moreover, we design the space complementary integrator and
multi-granularity fusion module to encode and aggregate the semantics across
different scales, achieving more efficient learning for subtyping pancreatic
tumors. The experimental results on an in-house dataset of 270 clinical cases
achieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between
pancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors
(PNETs), demonstrating its potential as a more accurate and efficient tool.

</details>


### [53] [Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection](https://arxiv.org/abs/2509.12784)
*Zhehao Li,Yucheng Qian,Chong Wang,Yinghao Lu,Zhihao Yang,Jiafei Wu*

Main category: cs.CV

TL;DR: 提出一种结合可供性推理与上下文提示的HOI检测网络，通过显式建模三元关系<human, tool, object>与跨模态注意融合语言-视觉上下文，在HICO-Det与V-COCO上取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段HOI方法上下文建模不足，难以处理依赖工具等辅助实体的复杂交互；需要将功能角色（可供性）与多实体关系纳入表征，提升对语义与视觉上下文的对齐与推理能力。

Method: 1) 将HOI从二元(human, object)扩展为包含辅助实体的三元<human, tool, object>结构，显式建模工具的功能角色（可供性）以识别如“filling”等工具依赖交互；2) 设计可学习的语言提示，编码实例类别信息；3) 使用注意力机制将提示与全局/区域视觉特征对齐，进行跨模态上下文融合，形成“情境化表征”；4) 将上述表征集成到常规两阶段HOI检测框架中进行检测与分类。

Result: 在HICO-Det与V-COCO数据集的大多数评测场景中取得优于现有方法的性能（具体指标未给出）。

Conclusion: 通过可供性引导的三元关系建模与上下文提示-视觉对齐的情境化表征学习，可更可靠地推理复杂、依赖上下文的HOI交互，方法在主流数据集上验证有效。

Abstract: Human-Object Interaction (HOI) detection aims to simultaneously localize
human-object pairs and recognize their interactions. While recent two-stage
approaches have made significant progress, they still face challenges due to
incomplete context modeling. In this work, we introduce a Contextualized
Representation Learning Network that integrates both affordance-guided
reasoning and contextual prompts with visual cues to better capture complex
interactions. We enhance the conventional HOI detection framework by expanding
it beyond simple human-object pairs to include multivariate relationships
involving auxiliary entities like tools. Specifically, we explicitly model the
functional role (affordance) of these auxiliary objects through triplet
structures <human, tool, object>. This enables our model to identify
tool-dependent interactions such as 'filling'. Furthermore, the learnable
prompt is enriched with instance categories and subsequently integrated with
contextual visual features using an attention mechanism. This process aligns
language with image content at both global and regional levels. These
contextualized representations equip the model with enriched relational cues
for more reliable reasoning over complex, context-dependent interactions. Our
proposed method demonstrates superior performance on both the HICO-Det and
V-COCO datasets in most scenarios. Codes will be released upon acceptance.

</details>


### [54] [Double Helix Diffusion for Cross-Domain Anomaly Image Generation](https://arxiv.org/abs/2509.12787)
*Linchun Wu,Qin Zou,Xianbiao Qi,Bo Du,Zhongyuan Wang,Qingquan Li*

Main category: cs.CV

TL;DR: 提出DH-Diff双螺旋扩散框架，同时生成高保真异常图像与像素级掩码，解决结构不一致与特征纠缠问题，并显著提升下游异常检测。


<details>
  <summary>Details</summary>
Motivation: 制造业视觉异常检测缺乏真实异常样本，现有合成方法常出现：1) 异常与背景结构不一致；2) 合成图与掩码特征纠缠，损害真实感。因此需要能同时生成真实感强且标注准确的数据。

Method: 设计双螺旋式跨域生成框架DH-Diff，循环经过特征分离、连接与融合模块。核心包括：1) 域解耦注意力，分别增强图像域与标注域以缓解特征纠缠；2) 语义评分图对齐模块，使异常前景与背景结构一致；并支持文本提示与可选图形引导以可控生成。

Result: 在多项实验中，DH-Diff在多样性与真实性上优于SOTA，并将下游异常检测性能显著提升。

Conclusion: DH-Diff有效解决合成异常数据的结构不一致与特征纠缠难题，能生成高质量图像与掩码，且具备可控性，适合作为异常检测的强力数据增广工具。

Abstract: Visual anomaly inspection is critical in manufacturing, yet hampered by the
scarcity of real anomaly samples for training robust detectors. Synthetic data
generation presents a viable strategy for data augmentation; however, current
methods remain constrained by two principal limitations: 1) the generation of
anomalies that are structurally inconsistent with the normal background, and 2)
the presence of undesirable feature entanglement between synthesized images and
their corresponding annotation masks, which undermines the perceptual realism
of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel
cross-domain generative framework designed to simultaneously synthesize
high-fidelity anomaly images and their pixel-level annotation masks, explicitly
addressing these challenges. DH-Diff employs a unique architecture inspired by
a double helix, cycling through distinct modules for feature separation,
connection, and merging. Specifically, a domain-decoupled attention mechanism
mitigates feature entanglement by enhancing image and annotation features
independently, and meanwhile a semantic score map alignment module ensures
structural authenticity by coherently integrating anomaly foregrounds. DH-Diff
offers flexible control via text prompts and optional graphical guidance.
Extensive experiments demonstrate that DH-Diff significantly outperforms
state-of-the-art methods in diversity and authenticity, leading to significant
improvements in downstream anomaly detection performance.

</details>


### [55] [Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation](https://arxiv.org/abs/2509.12791)
*Julien Walther,Rémi Giraud,Michaël Clément*

Main category: cs.CV

TL;DR: 提出SPAM框架：结合训练得到的特征提取与大规模预训练语义无关分割，在保证与物体边界对齐的同时生成规则、可解释且精确的超像素，优于现有方法，并支持交互与不确定区域处理。


<details>
  <summary>Details</summary>
Motivation: 传统超像素方法依赖低层特征，规则性好但难捕捉复杂对象；深度方法利用高层特征虽更准确，却牺牲超像素规则性与可解释性。需要一种既能贴合物体边界又保持规则性的超像素方法，且可利用大模型先验并适配多种上游分割结果。

Method: 提出SPAM：1) 训练一个模型学习用于超像素生成的图像特征；2) 在推理阶段引入大规模预训练的语义无关分割（如通用掩码生成器）作为先验，使生成的超像素与对象掩码对齐；3) 能接收任意高层分割先验，解决不确定区域；4) 支持交互式聚焦于特定对象。

Result: 在多项分割任务上，SPAM在定性与定量评估均优于SOTA，生成的超像素既准确又规整，提升解释性与适用性。

Conclusion: SPAM是一个通用、稳健的超像素生成框架，可在保持规则性的同时对齐对象边界，支持多种先验与交互操作，适用于多种计算机视觉应用；代码与预训练模型已开源。

Abstract: Superpixels are widely used in computer vision to simplify image
representation and reduce computational complexity. While traditional methods
rely on low-level features, deep learning-based approaches leverage high-level
features but also tend to sacrifice regularity of superpixels to capture
complex objects, leading to accurate but less interpretable segmentations. In
this work, we introduce SPAM (SuperPixel Anything Model), a versatile framework
for segmenting images into accurate yet regular superpixels. We train a model
to extract image features for superpixel generation, and at inference, we
leverage a large-scale pretrained model for semantic-agnostic segmentation to
ensure that superpixels align with object masks. SPAM can handle any prior
high-level segmentation, resolving uncertainty regions, and is able to
interactively focus on specific objects. Comprehensive experiments demonstrate
that SPAM qualitatively and quantitatively outperforms state-of-the-art methods
on segmentation tasks, making it a valuable and robust tool for various
applications. Code and pre-trained models are available here:
https://github.com/waldo-j/spam.

</details>


### [56] [Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation](https://arxiv.org/abs/2509.12815)
*Biwen Lei,Yang Li,Xinhai Liu,Shuhui Yang,Lixin Xu,Jingwei Huang,Ruining Tang,Haohan Weng,Jian Liu,Jing Xu,Zhen Zhou,Yiling Zhu,Jiankai Xing,Jiachen Xu,Changfeng Ma,Xinhao Yan,Yunhan Yang,Chunshi Wang,Duoteng Xu,Xueqi Ma,Yuguang Chen,Jing Li,Mingxin Yang,Sheng Zhang,Yifei Feng,Xin Huang,Di Luo,Zebin He,Puhua Jiang,Changrong Hu,Zihan Qin,Shiwei Miao,Haolin Liu,Yunfei Zhao,Zeqiang Lai,Qingxiang Lin,Zibo Zhao,Kunhong Li,Xianghui Yang,Huiwen Shi,Xin Yang,Yuxuan Wang,Zebin Yao,Yihang Lian,Sicong Liu,Xintong Han,Wangchen Qin,Caisheng Ouyang,Jianyin Liu,Tianwen Yuan,Shuai Jiang,Hong Duan,Yanqi Niu,Wencong Lin,Yifu Sun,Shirui Huang,Lin Niu,Gu Gong,Guojian Xiao,Bojian Zheng,Xiang Yuan,Qi Chen,Jie Xiao,Dongyang Zheng,Xiaofeng Yang,Kai Liu,Jianchen Zhu,Lifu Wang,Qinglin Lu,Jie Liu,Liang Dong,Fan Jiang,Ruibin Chen,Lei Wang,Chao Zhang,Jiaxin Lin,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Yinhe Wu,Jiayao Du,Jupeng Chen,Xinyue Mao,Dongyuan Guo,Yixuan Tang,Yulin Tsai,Yonghao Tan,Jiaao Yu,Junlin Yu,Keren Zhang,Yifan Li,Peng Chen,Tian Liu,Di Wang,Yuhong Liu,Linus,Jie Jiang,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D Studio提出一个端到端AI平台，可从单张概念图或文本快速生成符合游戏引擎要求的可用3D资产（含优化几何与高质量PBR贴图），显著降低制作门槛与迭代成本。


<details>
  <summary>Details</summary>
Motivation: 现有游戏3D资产制作流程专业性强、耗时耗力，缺乏能从创意快速到可部署资产的一体化自动化工具。

Method: 构建统一平台，集成多种先进神经模块（如部件级3D生成、网格/多边形生成、语义UV展开等），将输入的图像或文本转化为生产级3D模型，并进行几何优化与PBR贴图生成。

Result: 生成的资产视觉效果好，且满足主流游戏引擎的技术规范；在实验/案例中表现出更快迭代速度，降低制作门槛。

Conclusion: Hunyuan3D Studio为AI辅助游戏开发带来显著进步，提供从创意到技术资产的无缝桥接，推动交互媒体生产流程自动化与规模化。

Abstract: The creation of high-quality 3D assets, a cornerstone of modern game
development, has long been characterized by labor-intensive and specialized
workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered
content creation platform designed to revolutionize the game production
pipeline by automating and streamlining the generation of game-ready 3D assets.
At its core, Hunyuan3D Studio integrates a suite of advanced neural modules
(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into
a cohesive and user-friendly system. This unified framework allows for the
rapid transformation of a single concept image or textual description into a
fully-realized, production-quality 3D model complete with optimized geometry
and high-fidelity PBR textures. We demonstrate that assets generated by
Hunyuan3D Studio are not only visually compelling but also adhere to the
stringent technical requirements of contemporary game engines, significantly
reducing iteration time and lowering the barrier to entry for 3D content
creation. By providing a seamless bridge from creative intent to technical
asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted
workflows in game development and interactive media.

</details>


### [57] [SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention](https://arxiv.org/abs/2509.12817)
*Yuan Cao,Dong Wang*

Main category: cs.CV

TL;DR: 提出SAGA（Selective Adaptive Gating）线性注意力：用输入自适应门控选择性聚合KV，缓解线性注意力的低秩瓶颈，在保持全局感受野与线性复杂度的同时提升表达力与精度，并通过哈达玛分解实现零额外显存开销；在高分辨率下实现更高吞吐、更低显存和更高ImageNet精度。


<details>
  <summary>Details</summary>
Motivation: 软最大注意力在高分辨率视觉任务中因O(N^2)复杂度成为瓶颈；现有线性注意力虽降为O(N)但普遍对历史KV做均匀压缩，导致特征冗余、与Q方向性对齐缺失、KV低秩，性能落后于softmax注意力。

Method: 将注意力从(QK)V改写为Q(KV)的线性注意力框架内，引入输入自适应可学习门控，对KV聚合过程进行选择性调制，提升语义多样性并打破低秩限制；同时设计高效的Hadamard乘积分解用于门控计算，不增加额外显存开销。

Result: 在1280×1280分辨率下，相比PVT-T吞吐提升1.76×、峰值显存降低2.69×；在ImageNet上Top-1最高提高4.4%。

Conclusion: SAGA在保持线性复杂度与全局建模的前提下，通过自适应门控有效增强线性注意力的表达能力与效率，缩小与softmax注意力的性能差距，并在实际大分辨率视觉任务中实现显著吞吐、显存与精度优势。

Abstract: While Transformer architecture excel at modeling long-range dependencies
contributing to its widespread adoption in vision tasks the quadratic
complexity of softmax-based attention mechanisms imposes a major bottleneck,
particularly when processing high-resolution images. Linear attention presents
a promising alternative by reformulating the attention computation from $(QK)V$
to $Q(KV)$, thereby reducing the complexity from $\mathcal{O}(N^2)$ to
$\mathcal{O}(N)$ while preserving the global receptive field. However, most
existing methods compress historical key-value (KV) information uniformly,
which can lead to feature redundancy and the loss of directional alignment with
the query (Q). This uniform compression results in low-rank $KV$ feature maps,
contributing to a performance gap compared to softmax attention. To mitigate
this limitation, we propose \textbf{S}elective \textbf{A}daptive
\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which
introduces input-adaptive learnable gates to selectively modulate information
aggregation into the $KV$ feature map. These gates enhance semantic diversity
and alleviate the low-rank constraint inherent in conventional linear
attention. Additionally, we propose an efficient Hadamard-product decomposition
method for gate computation, which introduces no additional memory overhead.
Experiments demonstrate that SAGA achieves a 1.76$\times$ improvement in
throughput and a 2.69$\times$ reduction in peak GPU memory compared to PVT-T at
a resolution of $1280 \times 1280$. Moreover, it improves top-1 accuracy by up
to 4.4\% on the ImageNet dataset, demonstrating both computational efficiency
and model effectiveness.

</details>


### [58] [Data Scaling Laws for Radiology Foundation Models](https://arxiv.org/abs/2509.12818)
*Maximilian Ilse,Harshita Sharma,Anton Schwaighofer,Sam Bond-Taylor,Fernando Pérez-García,Olesya Melnichenko,Anne-Marie G. Sykes,Kelly K. Horst,Ashish Khandelwal,Maxwell Reynolds,Maria T. Wetscherek,Noel C. F. Codella,Javier Alvarez-Valle,Korfiatis Panagiotis,Valentina Salvatelli*

Main category: cs.CV

TL;DR: 研究比较在单一机构3.5百万张胸片上，对两类视觉编码器（CLIP范式的MI2与DINOv2范式的RAD-DINO）进行持续预训练的扩展性与任务迁移表现，发现MI2在放射学异常（findings）相关任务随数据规模更受益，RAD-DINO在导管/管线（tubes）相关任务更强；结合报告与结构化标签（UniCL）可进一步提升；某些任务用3万张院内数据即可超越开源基础模型，证明中心特定持续预训练的价值。


<details>
  <summary>Details</summary>
Motivation: 通用视觉基础模型（如CLIP、DINOv2）在自然图像上表现优异，但医疗影像受限于数据规模与域差异，尚不清楚数据规模、预训练范式与持续预训练对不同医疗任务（分类、分割、报告生成）的影响。以往研究偏重“放射学异常”任务，忽略对细长结构（管线、导管）的表征能力评估。

Method: 在统一算力与评估协议下，对两种代表性编码器范式进行中心特定持续预训练：CLIP式的MedImageInsight（MI2）与DINOv2式的RAD-DINO；数据为单机构多达350万张胸片。评估任务涵盖：分类（放射学异常、管线与导管）、分割（管线与导管）以及报告生成。并研究将报告文本与结构化标签联合训练（UniCL）对性能的影响；同时分析所需院内数据规模阈值（如3万张）以超越开源基础模型。

Result: - MI2在放射学异常相关任务随数据规模提升更显著；- RAD-DINO在管线/导管相关任务（包括分割与相关分类）上表现更强；- 用UniCL将报告与结构化标签共同用于MI2的持续预训练可进一步提升性能，说明大规模结构化监督的价值；- 对部分任务，仅需约3万张院内样本即可超过开源可用的通用基础模型。

Conclusion: 持续预训练在单中心院内数据上能显著提升医疗影像模型性能；不同范式对任务有偏好：CLIP式更利于发现类任务，DINO式更利于细长结构相关任务；融合报告与结构化标签能进一步增益。对医疗机构而言，利用有限但同域数据进行中心特定持续预训练是务实而高效的路径。

Abstract: Foundation vision encoders such as CLIP and DINOv2, trained on web-scale
data, exhibit strong transfer performance across tasks and datasets. However,
medical imaging foundation models remain constrained by smaller datasets,
limiting our understanding of how data scale and pretraining paradigms affect
performance in this setting. In this work, we systematically study continual
pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO
representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M
chest x-rays from a single institution, holding compute and evaluation
protocols constant. We evaluate on classification (radiology findings, lines
and tubes), segmentation (lines and tubes), and radiology report generation.
While prior work has primarily focused on tasks related to radiology findings,
we include lines and tubes tasks to counterbalance this bias and evaluate a
model's ability to extract features that preserve continuity along elongated
structures. Our experiments show that MI2 scales more effectively for
finding-related tasks, while RAD-DINO is stronger on tube-related tasks.
Surprisingly, continually pretraining MI2 with both reports and structured
labels using UniCL improves performance, underscoring the value of structured
supervision at scale. We further show that for some tasks, as few as 30k
in-domain samples are sufficient to surpass open-weights foundation models.
These results highlight the utility of center-specific continual pretraining,
enabling medical institutions to derive significant performance gains by
utilizing in-domain data.

</details>


### [59] [Exploring Metric Fusion for Evaluation of NeRFs](https://arxiv.org/abs/2509.12836)
*Shreyas Shivakumara,Gabriel Eilertsen,Karljohan Lundin Palmerius*

Main category: cs.CV

TL;DR: 将两种感知指标DISTS与VMAF进行归一化与融合，以更好地评价NeRF生成结果，与主观质量更相关。


<details>
  <summary>Details</summary>
Motivation: 现有针对NeRF的客观质量评价困难：NeRF伪影独特、单一指标在不同数据集上泛化差；作者希望通过互补的感知方法提升与主观评分的相关性与稳健性。

Method: 选取两种成功但侧重不同的指标DISTS（图像结构与纹理相似度）与VMAF（视频多方法融合）。设计两种单指标归一化策略与两种融合策略（加权/学习或规则融合，文摘未细述），在两个数据集（Synthetic、Outdoor）和三种配置下评测。以与主观分的相关系数（如PLCC/SROCC/KROCC）比较单指标与融合指标。

Result: 融合指标普遍较单一指标与主观评分的相关性更高且更稳定；在两个数据集与三种配置中均显示出更好的鲁棒性与可泛化性。

Conclusion: 跨感知范式的指标融合（DISTS+VMAF）能弥补各自短板，提升NeRF质量评价与主观一致性；提出的归一化与融合管线在不同数据集/配置上表现稳健。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated significant potential in
synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however,
remains a challenge due to the unique artifacts they exhibit, and no individual
metric performs well across all datasets. We hypothesize that combining two
successful metrics, Deep Image Structure and Texture Similarity (DISTS) and
Video Multi-Method Assessment Fusion (VMAF), based on different perceptual
methods, can overcome the limitations of individual metrics and achieve
improved correlation with subjective quality scores. We experiment with two
normalization strategies for the individual metrics and two fusion strategies
to evaluate their impact on the resulting correlation with the subjective
scores. The proposed pipeline is tested on two distinct datasets, Synthetic and
Outdoor, and its performance is evaluated across three different
configurations. We present a detailed analysis comparing the correlation
coefficients of fusion methods and individual scores with subjective scores to
demonstrate the robustness and generalizability of the fusion metrics.

</details>


### [60] [Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses](https://arxiv.org/abs/2509.12866)
*Martin Thißen,Thi Ngoc Diep Tran,Barbara Esteve Ratsch,Ben Joel Schönbein,Ute Trapp,Beate Egner,Romana Piat,Elke Hergenröther*

Main category: cs.CV

TL;DR: 用LLM把狗的肌骨可视化记录（彩色笔画标注肌肉/关节）映射到文本，再生成合成训练样本，训练下游视觉分类器；仅用合成数据在真实70例上得F1=88%。


<details>
  <summary>Details</summary>
Motivation: 许多任务数据稀缺、采集昂贵，尤其是罕见病。该团队在狗的肌骨状况可视化记录上数据不足，想用LLM的强大生成与医学推理能力合成与诊断相关的标注图数据，缓解小样本问题。

Method: 将身体图谱分割为200+带标签的区域，将视觉笔画标注与文本标签互映射；利用引导式解码、链式思考、少样本提示，从文本空间生成对应的“笔画配置”，进而得到合成可视化文档。首先针对髌骨脱位生成1000例，分析位置、严重度敏感性与性别独立性；再为多种诊断各生成样本，构建二分类数据集；用仅合成数据训练模型并在真实数据上评估。

Result: 合成样本对病变部位与严重程度表达敏感、与性别无关；仅用合成数据训练的模型在70份真实文档上达到F1=88%。

Conclusion: LLM可有效生成与诊断相关的可视化标注数据，缓解数据稀缺，尤其适用于罕见病场景；方法虽面向医疗，但映射与生成技巧可推广到其他领域。

Abstract: It is well-established that more data generally improves AI model
performance. However, data collection can be challenging for certain tasks due
to the rarity of occurrences or high costs. These challenges are evident in our
use case, where we apply AI models to a novel approach for visually documenting
the musculoskeletal condition of dogs. Here, abnormalities are marked as
colored strokes on a body map of a dog. Since these strokes correspond to
distinct muscles or joints, they can be mapped to the textual domain in which
large language models (LLMs) operate. LLMs have demonstrated impressive
capabilities across a wide range of tasks, including medical applications,
offering promising potential for generating synthetic training data. In this
work, we investigate whether LLMs can effectively generate synthetic visual
training data for canine musculoskeletal diagnoses. For this, we developed a
mapping that segments visual documentations into over 200 labeled regions
representing muscles or joints. Using techniques like guided decoding,
chain-of-thought reasoning, and few-shot prompting, we generated 1,000
synthetic visual documentations for patellar luxation (kneecap dislocation)
diagnosis, the diagnosis for which we have the most real-world data. Our
analysis shows that the generated documentations are sensitive to location and
severity of the diagnosis while remaining independent of the dog's sex. We
further generated 1,000 visual documentations for various other diagnoses to
create a binary classification dataset. A model trained solely on this
synthetic data achieved an F1 score of 88% on 70 real-world documentations.
These results demonstrate the potential of LLM-generated synthetic data, which
is particularly valuable for addressing data scarcity in rare diseases. While
our methodology is tailored to the medical domain, the insights and techniques
can be adapted to other fields.

</details>


### [61] [Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of Object Detectors in Deployment](https://arxiv.org/abs/2509.12871)
*Avinaash Manoharan,Xiangyu Yin,Domenik Helm,Chih-Hong Cheng*

Main category: cs.CV

TL;DR: 提出无标注评估指标CCS，通过对同一图像做测试时数据增强、汇集各视图的检测框并用IoU计算空间一致性，作为可靠性代理，在真实部署中持续监控检测器表现；与F1、PDQ、OCC高度一致（>90%），模型无关、可定位欠佳案例，适用于DevOps监控。


<details>
  <summary>Details</summary>
Motivation: 部署阶段缺少人工标注，传统评估与监控难以持续进行；需要一种无需标注、可跨模型、能在真实场景中持续比较与报警的检测器性能度量。

Method: 对每张图像应用多种测试时数据增强；收集各增强视图的预测框；按视图两两计算IoU并取最大重叠；对最大重叠进行归一化并在所有增强配对上取平均，得到样本级的空间一致性分数CCS；可对模型或场景级汇总比较。

Result: 在Open Images与KITTI的受控实验中，CCS与F1、PDQ、OCC的符合度超过90%；适用于单阶段与两阶段检测器；能在案例层面识别表现欠佳的情形。

Conclusion: CCS在无标注条件下提供稳定、模型无关的空间一致性度量，可作为部署期对象检测的持续监控与比较基础，支持DevOps式运维与异常场景定位。

Abstract: Evaluating object detection models in deployment is challenging because
ground-truth annotations are rarely available. We introduce the Cumulative
Consensus Score (CCS), a label-free metric that enables continuous monitoring
and comparison of detectors in real-world settings. CCS applies test-time data
augmentation to each image, collects predicted bounding boxes across augmented
views, and computes overlaps using Intersection over Union. Maximum overlaps
are normalized and averaged across augmentation pairs, yielding a measure of
spatial consistency that serves as a proxy for reliability without annotations.
In controlled experiments on Open Images and KITTI, CCS achieved over 90%
congruence with F1-score, Probabilistic Detection Quality, and Optimal
Correction Cost. The method is model-agnostic, working across single-stage and
two-stage detectors, and operates at the case level to highlight
under-performing scenarios. Altogether, CCS provides a robust foundation for
DevOps-style monitoring of object detectors.

</details>


### [62] [Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation](https://arxiv.org/abs/2509.12878)
*Qianguang Zhao,Dongli Wang,Yan Zhou,Jianxun Li,Richard Irampa*

Main category: cs.CV

TL;DR: 提出PENet，用扩展原型容量与跨域对齐，结合扩散模型特征与监督特征，通过PAM与PCM提升小样本3D点云语义分割，在S3DIS与ScanNet上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 原型法在小样本3D分割中受限：同类内部多样性导致单一原型覆盖不足；支撑集与查询集特征分布不一致导致原型错配。需要更具泛化与容量的原型表示，并能与查询空间对齐。

Method: 利用预训练条件扩散模型的编码器作为通用特征源，提出PENet双流学习器：Intrinsic Learner(IL)提取受监督的内在判别特征；Diffusion Learner(DL)提供丰富的可泛化特征。两路形成的“双原型”输入到Prototype Assimilation Module(PAM)，通过“推-拉”式交互引导注意力迭代对齐原型与查询特征；最后由Prototype Calibration Mechanism(PCM)对大容量原型进行正则以避免语义漂移。

Result: 在S3DIS与ScanNet上，PENet在多种few-shot设置下均显著优于现有SOTA（摘要未给具体数值，但强调全面领先）。

Conclusion: 结合扩散模型特征与监督特征构建大容量、可对齐的原型，并通过PAM与PCM保证稳定与泛化，有效缓解类内多样性与跨集不一致问题，提升小样本3D点云分割性能。

Abstract: Few-shot 3D point cloud semantic segmentation aims to segment novel
categories using a minimal number of annotated support samples. While existing
prototype-based methods have shown promise, they are constrained by two
critical challenges: (1) Intra-class Diversity, where a prototype's limited
representational capacity fails to cover a class's full variations, and (2)
Inter-set Inconsistency, where prototypes derived from the support set are
misaligned with the query feature space. Motivated by the powerful generative
capability of diffusion model, we re-purpose its pre-trained conditional
encoder to provide a novel source of generalizable features for expanding the
prototype's representational range. Under this setup, we introduce the
Prototype Expansion Network (PENet), a framework that constructs big-capacity
prototypes from two complementary feature sources. PENet employs a dual-stream
learner architecture: it retains a conventional fully supervised Intrinsic
Learner (IL) to distill representative features, while introducing a novel
Diffusion Learner (DL) to provide rich generalizable features. The resulting
dual prototypes are then processed by a Prototype Assimilation Module (PAM),
which adopts a novel push-pull cross-guidance attention block to iteratively
align the prototypes with the query space. Furthermore, a Prototype Calibration
Mechanism (PCM) regularizes the final big capacity prototype to prevent
semantic drift. Extensive experiments on the S3DIS and ScanNet datasets
demonstrate that PENet significantly outperforms state-of-the-art methods
across various few-shot settings.

</details>


### [63] [Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder](https://arxiv.org/abs/2509.12883)
*Qifei Jia,Yu Liu,Yajie Chai,Xintong Yao,Qiming Lu,Yasen Zhang,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CV

TL;DR: Lego-Edit用多模态大模型作为“指挥官”，调用一套模型级编辑工具并通过渐进式强化学习在无标注开放指令上学会泛化，实现对多样化真实指令的稳健图像编辑，SOTA表现且可即插即用新工具。


<details>
  <summary>Details</summary>
Motivation: 现实用户的图像编辑指令极其多样，现有方法在训练域外指令上泛化差，难以实用；需要一种能理解开放域指令、组合多种细粒度编辑操作、并能持续吸收新工具的系统。

Method: 提出Lego-Edit：1）构建“模型级工具箱”，包含在少量数据上高效训练的多种编辑模型与基础图像操作函数，允许MLLM按需组合调用以形成细粒度编辑链；2）三阶段渐进式强化学习，在无标注开放指令上基于反馈优化MLLM，使其学到通用的推理与工具选择/调用能力；并支持新工具的无缝接入。

Result: 在GEdit-Bench与ImgBench上达成SOTA；对开放域指令表现出强推理与编辑稳健性；无需额外微调即可利用新引入的编辑工具。

Conclusion: 通过MLLM调度多样化模型级工具并用渐进式RL在开放指令上训练，可显著提升指令驱动图像编辑的泛化与实用性，系统具备可扩展、可组合与即插即用能力。

Abstract: Instruction-based image editing has garnered significant attention due to its
direct interaction with users. However, real-world user instructions are
immensely diverse, and existing methods often fail to generalize effectively to
instructions outside their training domain, limiting their practical
application. To address this, we propose Lego-Edit, which leverages the
generalization capability of Multi-modal Large Language Model (MLLM) to
organize a suite of model-level editing tools to tackle this challenge.
Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising
diverse models efficiently trained on limited data and several image
manipulation functions, enabling fine-grained composition of editing actions by
the MLLM; and (2) a three-stage progressive reinforcement learning approach
that uses feedback on unannotated, open-domain instructions to train the MLLM,
equipping it with generalized reasoning capabilities for handling real-world
instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art
performance on GEdit-Bench and ImgBench. It exhibits robust reasoning
capabilities for open-domain instructions and can utilize newly introduced
editing tools without additional fine-tuning.
  Code is available: https://github.com/xiaomi-research/lego-edit.

</details>


### [64] [Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing](https://arxiv.org/abs/2509.12888)
*Weiming Chen,Zhihan Zhu,Yijia Wang,Zhihai He*

Main category: cs.CV

TL;DR: 提出用于Rectified Flow模型的高阶Runge–Kutta反演与解耦扩散Transformer注意力（DDTA），提升重建一致性与精细语义控制，在重建与文本引导编辑上达SOTA。


<details>
  <summary>Details</summary>
Motivation: RF模型生成性能强于DDIM，但实际应用中存在两大痛点：1) 反演精度不足，影响与源图一致性；2) 多模态Transformer中的文本-图像注意力耦合，难以进行精确注意力控制，从而限制可编辑性与语义对齐。

Method: (A) 反演：将RF的反演过程视为常微分方程求解，引入高阶Runge–Kutta（RK）求解器进行高效高精度反演，减少误差积累，提升图像一致性。(B) 注意力：提出Decoupled Diffusion Transformer Attention（DDTA），在多模态扩散Transformer内部显式解耦文本与图像注意力通道，使语义对齐与空间细节建模彼此独立，便于精细控制。

Result: 在图像重建与文本引导编辑任务上，较现有方法取得更高保真度与更好可编辑性；实现SOTA水平（具体指标未给出），实验验证RK反演提高一致性，DDTA带来更精确的语义控制。

Conclusion: 高阶RK反演显著提升RF模型的逆向重建精度，DDTA有效解耦多模态注意力以实现精细编辑。两者结合，在多项任务上达到SOTA，并具备实用性与效率优势。

Abstract: Rectified flow (RF) models have recently demonstrated superior generative
performance compared to DDIM-based diffusion models. However, in real-world
applications, they suffer from two major challenges: (1) low inversion accuracy
that hinders the consistency with the source image, and (2) entangled
multimodal attention in diffusion transformers, which hinders precise attention
control. To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations. To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control. Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability. Code is
available at https://github.com/wmchen/RKSovler_DDTA.

</details>


### [65] [MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization](https://arxiv.org/abs/2509.12893)
*Yiyi Zhang,Yuchen Yuan,Ying Zheng,Jialun Pei,Jinpeng Li,Zheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出MEJO框架用于外科三元组识别（器械-动词-目标），通过S^2D表示解耦、MLLM概率提示增强与任务特定提示、以及CGL梯度协调，分别缓解跨任务与任务内（长尾）优化冲突，并在CholecT45/50上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 外科场景理解需同时识别器械、动作与目标及其组合，但数据长尾严重，现有多任务学习虽然有协同优势，却存在：1) 任务共享与特定表征纠缠导致跨任务优化冲突；2) 类别不平衡引发任务内正负梯度失衡与冲突，降低长尾类性能。

Method: 提出MLLM-Engaged Joint Optimization (MEJO)：(a) 共享-特定解耦学习S^2D，将特征分解为任务共享与任务特定两部分；(b) 用多模态大语言模型构建概率提示池，对视觉特征进行动态语义增强，提升共享表征，同时为各任务设计覆盖时空维度的特定提示以缓解歧义；(c) 协调梯度学习CGL，对头/尾类产生的正负梯度进行剖分与重加权，缓和不平衡带来的冲突，促进更协调的更新。

Result: 在CholecT45与CholecT50数据集上进行大量实验，MEJO在三元组识别上取得优于现有方法的表现，显示在跨任务与任务内优化冲突处理方面的有效性。

Conclusion: 通过S^2D表征解耦、MLLM语义增强与任务特定提示、以及CGL梯度协调，MEJO有效解决多任务三元组识别中的优化冲突与长尾问题，显著提升外科三元组识别性能。

Abstract: Surgical triplet recognition, which involves identifying instrument, verb,
target, and their combinations, is a complex surgical scene understanding
challenge plagued by long-tailed data distribution. The mainstream multi-task
learning paradigm benefiting from cross-task collaborative promotion has shown
promising performance in identifying triples, but two key challenges remain: 1)
inter-task optimization conflicts caused by entangling task-generic and
task-specific representations; 2) intra-task optimization conflicts due to
class-imbalanced training data. To overcome these difficulties, we propose the
MLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and
intra-task optimization for surgical triplet recognition. For inter-task
optimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning
scheme that decomposes representations into task-shared and task-specific
components. To enhance task-shared representations, we construct a Multimodal
Large Language Model (MLLM) powered probabilistic prompt pool to dynamically
augment visual features with expert-level semantic cues. Additionally,
comprehensive task-specific cues are modeled via distinct task prompts covering
the temporal-spatial dimensions, effectively mitigating inter-task ambiguities.
To tackle intra-task optimization conflicts, we develop a Coordinated Gradient
Learning (CGL) strategy, which dissects and rebalances the positive-negative
gradients originating from head and tail classes for more coordinated learning
behaviors. Extensive experiments on the CholecT45 and CholecT50 datasets
demonstrate the superiority of our proposed framework, validating its
effectiveness in handling optimization conflicts.

</details>


### [66] [DialNav: Multi-turn Dialog Navigation with a Remote Guide](https://arxiv.org/abs/2509.12894)
*Leekyeung Han,Hyunji Min,Gyeom Hwangbo,Jonghyun Choi,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出DialNav协作式具身对话导航任务，发布RAIN数据集与评测基准，强调向导需推断导航者位置，沟通对成功至关重要，并给出系统实验与开源资源。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航/对话研究往往单独评估或假设共享定位，缺少对“协作+对话+定位不确定”的整体评估。作者希望构建更贴近真实远程协助场景的任务设置，推动对话在导航中的关键作用研究。

Method: 1) 定义DialNav任务：导航者与远程向导多轮对话协作，向导需根据对话推断导航者位置并指导其到达目标；2) 收集RAIN数据集：真实人类对话与在写真环境中的导航轨迹配对；3) 设计综合基准：同时评估导航性能与对话质量；4) 实验：替换不同导航者/向导模型，系统分析其影响与挑战。

Result: 通过广泛实验，展示不同模型配置下的表现差异，证实沟通与位置推断对任务成功的关键性；基准能揭示当前方法在协作对话导航上的不足与瓶颈。

Conclusion: DialNav与RAIN为具身协作对话导航提供了标准化任务、数据与评测框架；当前模型仍存在显著挑战。作者开源数据、代码与评测以促进后续研究。

Abstract: We introduce DialNav, a novel collaborative embodied dialog task, where a
navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn
dialog to reach a goal location. Unlike prior work, DialNav aims for holistic
evaluation and requires the Guide to infer the Navigator's location, making
communication essential for task success. To support this task, we collect and
release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog
paired with navigation trajectories in photorealistic environments. We design a
comprehensive benchmark to evaluate both navigation and dialog, and conduct
extensive experiments analyzing the impact of different Navigator and Guide
models. We highlight key challenges and publicly release the dataset, code, and
evaluation framework to foster future research in embodied dialog.

</details>


### [67] [Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models](https://arxiv.org/abs/2509.12897)
*Jianfei Zhao,Feng Zhang,Xin Sun,Lingxing Kong,Zhixing Tan,Chong Feng*

Main category: cs.CV

TL;DR: 提出Cross-Layer Vision Smoothing (CLVS)，通过引入跨层视觉记忆平滑注意力，在早中层持续聚焦关键目标，并基于不确定性自适应停止平滑，在多基准与多LVLM上显著提升关系与属性理解，达SOTA。


<details>
  <summary>Details</summary>
Motivation: LVLM虽能定位关键物体，但对其关注短暂，导致视觉理解不稳。作者假设：若能在多层间持续、平滑地关注关键物体，可提升模型的视觉理解能力与鲁棒性。

Method: 引入“视觉记忆”以跨层平滑注意力：1) 在首层用无位置偏置的视觉注意初始化记忆；2) 后续层的视觉注意与上一层记忆联合计算，同时迭代更新记忆，使对关键目标的关注在层间保持平稳与持续；3) 观察到视觉理解主要发生在模型的早中层，用不确定性作为完成理解的指示器，触发停止平滑（早停）。

Result: 在3个不同LVLM、4个基准上验证，有效且可泛化；总体性能达SOTA，尤其在关系理解、属性理解任务上提升显著。

Conclusion: 跨层注意力平滑的记忆机制能稳定并延展对关键目标的关注，在无需改动大规模架构的情况下普适提升LVLM视觉理解；基于不确定性的自适应停止进一步避免过度平滑，带来稳健增益。

Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in
images, yet their attention to these objects tends to be very brief. Motivated
by the hypothesis that sustained focus on key objects can improve LVLMs' visual
capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of
CLVS is to incorporate a vision memory that smooths the attention distribution
across layers. Specifically, we initialize this vision memory with
position-unbiased visual attention in the first layer. In subsequent layers,
the model's visual attention jointly considers the vision memory from previous
layers, while the memory is updated iteratively, thereby maintaining smooth
attention on key objects. Given that visual understanding primarily occurs in
the early and middle layers of the model, we use uncertainty as an indicator of
completed visual understanding and terminate the smoothing process accordingly.
Experiments on four benchmarks across three LVLMs confirm the effectiveness and
generalizability of our method. CLVS achieves state-of-the-art performance on a
variety of visual understanding tasks, with particularly significant
improvements in relation and attribute understanding.

</details>


### [68] [MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion](https://arxiv.org/abs/2509.12901)
*Guihui Li,Bowei Dong,Kaizhi Dong,Jiayi Li,Haiyong Zheng*

Main category: cs.CV

TL;DR: 提出MSGFusion：以多模态场景图为语义引导的红外-可见光图像融合框架，显式建模实体/属性/关系并与视觉特征深度耦合，通过层级聚合与图驱动融合同步优化高层语义与低层细节，实验在多基准和下游任务上显著优于现有方法，尤其在细节、结构清晰度与语义一致性方面。


<details>
  <summary>Details</summary>
Motivation: 现有深度融合方法虽在特征提取/对齐/重建上进展显著，但主要依赖纹理、对比度等低层线索，难以捕获高层语义。尝试用文本提供语义引导多为非结构化描述，缺乏对实体、属性、关系的显式建模与空间定位，限制了细粒度融合效果。

Method: 构建MSGFusion：从文本与视觉中提取并耦合结构化场景图（实体、属性、空间关系）；设计场景图表示模块、层级聚合模块与图驱动融合模块，联合优化，将高层语义注入到融合过程中，同时保留低层细节。

Result: 在多项公开基准上取得显著领先，尤其在细节保留和结构清晰度方面更好；在低照度目标检测、语义分割、医学图像融合等下游任务上展现更强语义一致性与泛化能力。

Conclusion: 以多模态场景图为核心的语义显式建模与视觉深度耦合可有效提升红外-可见光融合质量和下游任务表现；结构化语义指导优于非结构化文本提示，能实现细粒度、结构清晰的融合。

Abstract: Infrared and visible image fusion has garnered considerable attention owing
to the strong complementarity of these two modalities in complex, harsh
environments. While deep learning-based fusion methods have made remarkable
advances in feature extraction, alignment, fusion, and reconstruction, they
still depend largely on low-level visual cues, such as texture and contrast,
and struggle to capture the high-level semantic information embedded in images.
Recent attempts to incorporate text as a source of semantic guidance have
relied on unstructured descriptions that neither explicitly model entities,
attributes, and relationships nor provide spatial localization, thereby
limiting fine-grained fusion performance. To overcome these challenges, we
introduce MSGFusion, a multimodal scene graph-guided fusion framework for
infrared and visible imagery. By deeply coupling structured scene graphs
derived from text and vision, MSGFusion explicitly represents entities,
attributes, and spatial relations, and then synchronously refines high-level
semantics and low-level details through successive modules for scene graph
representation, hierarchical aggregation, and graph-driven fusion. Extensive
experiments on multiple public benchmarks show that MSGFusion significantly
outperforms state-of-the-art approaches, particularly in detail preservation
and structural clarity, and delivers superior semantic consistency and
generalizability in downstream tasks such as low-light object detection,
semantic segmentation, and medical image fusion.

</details>


### [69] [AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring](https://arxiv.org/abs/2509.12905)
*Branko Mitic,Philipp Seeböck,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 提出一种用于医学影像异常检测与无监督分割的生成式方法：先做异常消除的图像到图像重建，再对原图与重建图进行补丁级相似度评分以定位异常；在胸部CT感染病灶与脑MRI缺血卒中任务上，实现比重建式SOTA更高的像素级分割DICE（+1.9%、+4.4%）。


<details>
  <summary>Details</summary>
Motivation: 医学AD与无监督分割在早期疾病发现、病灶严重程度评估、鉴别诊断与自动筛查中至关重要。但肺部等正常组织的细粒度解剖变异度很高，导致传统生成式AD方法容易把正常差异当异常，或重建不稳定。需要一种能鲁棒应对正常变异、又能精准定位异常的生成式框架。

Method: 两阶段：1）图像到图像的“无异常”重建（通过只学习正常分布，将输入映射为健康外观）；2）对原始图与重建图进行补丁级（patch-level）相似度评分，形成异常显著图，实现精细定位。方法在胸部CT感染病灶与脑MRI（T1）缺血灶上评估。

Result: 在两项任务上实现更优的像素级异常分割：相对DICE提升胸部CT约+1.9%，脑MRI约+4.4%，优于其它重建式SOTA基线。

Conclusion: 该生成式AD框架能在高正常变异场景下提升异常定位与分割精度，具备跨模态（CT与MRI）与跨任务的泛化潜力。

Abstract: Early detection of newly emerging diseases, lesion severity assessment,
differentiation of medical conditions and automated screening are examples for
the wide applicability and importance of anomaly detection (AD) and
unsupervised segmentation in medicine. Normal fine-grained tissue variability
such as present in pulmonary anatomy is a major challenge for existing
generative AD methods. Here, we propose a novel generative AD approach
addressing this issue. It consists of an image-to-image translation for
anomaly-free reconstruction and a subsequent patch similarity scoring between
observed and generated image-pairs for precise anomaly localization. We
validate the new method on chest computed tomography (CT) scans for the
detection and segmentation of infectious disease lesions. To assess
generalizability, we evaluate the method on an ischemic stroke lesion
segmentation task in T1-weighted brain MRI. Results show improved pixel-level
anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score
improvements of +1.9% and +4.4%, respectively, compared to other
state-of-the-art reconstruction-based methods.

</details>


### [70] [T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking](https://arxiv.org/abs/2509.12913)
*Hojat Ardi,Amir Jahanshahi,Ali Diba*

Main category: cs.CV

TL;DR: 提出T-SiamTPN：在SiamTPN基础上引入显式时间建模（时序特征融合与注意力交互），显著提升长时与遮挡下鲁棒性；在Jetson Nano上仍可实时（7.1 FPS），相较基线成功率+13.7%、精度+14.7%，达到SOTA竞争水平。


<details>
  <summary>Details</summary>
Motivation: 现有空中目标跟踪面临尺度变化、动态背景、遮挡与杂波干扰等问题。多数方法偏重空间线索，忽略时序依赖，导致长时跟踪和遮挡场景鲁棒性不足；基于相关运算的Siamese跟踪器受限于线性相关，难以应对非线性外观变化。需要在Siamese框架中显式引入时间建模以提升一致性与判别力，同时保持嵌入式实时性。

Method: 在SiamTPN架构上扩展时间维度：引入时序特征融合模块与基于注意力的时序交互，强化跨帧一致性与丰富特征表达；仍保持相关匹配范式但通过时序增强来弥补线性相关的不足；整体设计关注计算效率，适配嵌入式平台。

Result: 与基线相比，成功率提升13.7%，精度提升14.7%；总体性能与SOTA具有竞争力；在Jetson Nano上达到实时7.1 FPS，几乎无额外运行时开销。

Conclusion: 显式时序建模对Siamese跟踪框架至关重要。T-SiamTPN在保持效率的同时显著提升空中目标跟踪鲁棒性，适合资源受限的实际应用；代码将开源。

Abstract: Aerial object tracking remains a challenging task due to scale variations,
dynamic backgrounds, clutter, and frequent occlusions. While most existing
trackers emphasize spatial cues, they often overlook temporal dependencies,
resulting in limited robustness in long-term tracking and under occlusion.
Furthermore, correlation-based Siamese trackers are inherently constrained by
the linear nature of correlation operations, making them ineffective against
complex, non-linear appearance changes. To address these limitations, we
introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends
the SiamTPN architecture with explicit temporal modeling. Our approach
incorporates temporal feature fusion and attention-based interactions,
strengthening temporal consistency and enabling richer feature representations.
These enhancements yield significant improvements over the baseline and achieve
performance competitive with state-of-the-art trackers. Crucially, despite the
added temporal modules, T-SiamTPN preserves computational efficiency. Deployed
on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1
FPS, demonstrating its suitability for real-world embedded applications without
notable runtime overhead. Experimental results highlight substantial gains:
compared to the baseline, T-SiamTPN improves success rate by 13.7% and
precision by 14.7%. These findings underscore the importance of temporal
modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and
efficient solution for aerial object tracking. Code is available at:
https://github.com/to/be/released

</details>


### [71] [A Novel Compression Framework for YOLOv8: Achiev-ing Real-Time Aerial Object Detection on Edge Devices via Structured Pruning and Channel-Wise Distillation](https://arxiv.org/abs/2509.12918)
*Melika Sabaghian,Mohammad Ali Keyvanrad,Seyyedeh Mahila Moghadami*

Main category: cs.CV

TL;DR: 提出一个三阶段压缩管线（稀疏感知训练+结构化通道剪枝+通道级知识蒸馏）用于YOLOv8的空中目标检测，在大幅降低参数与计算量的同时，几乎不牺牲AP50，并显著提升边缘设备推理速度；结合TensorRT进一步加速。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署航拍目标检测需要大幅压缩模型但尽量保持精度；现有方法单独使用剪枝或蒸馏常导致精度下降或压缩不足，需一体化方案平衡效率与性能，特别关注小/中目标检测场景。

Method: 三阶段：1) 稀疏感知训练：在优化过程中引入动态稀疏，平衡参数压缩与精度；2) 结构化通道剪枝：利用BN缩放因子选择并裁剪冗余通道，降低模型尺寸与计算；3) 通道级知识蒸馏（CWD）：以原始模型为教师，设定温度与损失权重，针对小/中目标进行知识迁移，弥补剪枝带来的精度损失；并附加TensorRT轻量推理优化。

Result: 在VisDrone上多种YOLOv8变体验证。以YOLOv8m为例：参数25.85M→6.85M（-73.51%），FLOPs 49.6G→13.3G，MACs 101G→34.5G，AP50仅下降2.7%，得到47.9 AP50；推理速度从26 FPS提高到45 FPS；进一步用TensorRT，AP50从47.9降至47.6，但FPS从45提升到68。

Conclusion: 三阶段压缩管线在空中目标检测任务中实现高压缩比与近乎保精度，同时显著加速推理，适合资源受限、高吞吐的边缘部署；TensorRT可进一步加速，代价是微小精度损失。

Abstract: Efficient deployment of deep learning models for aerial object detection on
resource-constrained devices requires significant compression without
com-promising performance. In this study, we propose a novel three-stage
compression pipeline for the YOLOv8 object detection model, integrating
sparsity-aware training, structured channel pruning, and Channel-Wise Knowledge
Distillation (CWD). First, sparsity-aware training introduces dynamic sparsity
during model optimization, effectively balancing parameter reduction and
detection accuracy. Second, we apply structured channel pruning by leveraging
batch normalization scaling factors to eliminate redundant channels,
significantly reducing model size and computational complexity. Finally, to
mitigate the accuracy drop caused by pruning, we employ CWD to transfer
knowledge from the original model, using an adjustable temperature and loss
weighting scheme tailored for small and medium object detection. Extensive
experiments on the VisDrone dataset demonstrate the effectiveness of our
approach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model
parameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to
13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The
resulting compressed model achieves 47.9 AP50 and boosts inference speed from
26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge
devices. We further apply TensorRT as a lightweight optimization step. While
this introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly
improves inference speed from 45 to 68 FPS, demonstrating the practicality of
our approach for high-throughput, re-source-constrained scenarios.

</details>


### [72] [MATTER: Multiscale Attention for Registration Error Regression](https://arxiv.org/abs/2509.12924)
*Shipeng Liu,Ziliang Xiong,Khac-Hoang Ngo,Per-Erik Forssén*

Main category: cs.CV

TL;DR: 该文将点云配准质量验证从传统的分类转为回归，结合多尺度特征提取与注意力聚合，实现更细粒度且稳健的配准误差估计，并在异质密度点云与下游建图任务中优于SOTA分类方法。


<details>
  <summary>Details</summary>
Motivation: 现有PCR质量验证多以分类划分好/坏或少量等级，难以精确衡量误差大小，且对不同空间密度点云鲁棒性不足；需要能连续量化配准误差并在复杂场景下可靠工作的方案，以更好地服务SLAM、目标跟踪与建图等下游任务。

Method: 将质量验证建模为回归问题，直接估计配准误差；设计多尺度特征提取以捕获不同空间分辨率的信息，并通过注意力机制进行特征聚合，从而增强对异构点密度的适应性与对误差线索的聚焦。

Result: 在多样数据集上实现准确且鲁棒的配准误差估计，尤其在点云空间密度异质情况下表现显著；在用于指导映射的下游任务时，在相同的重配准帧预算下，相比SOTA分类法显著提升地图质量。

Conclusion: 将PCR质量验证转为回归并配合多尺度+注意力的特征设计，可更细粒度且稳健地评估配准误差，实际下游建图中带来明显收益，优于现有分类式方案。

Abstract: Point cloud registration (PCR) is crucial for many downstream tasks, such as
simultaneous localization and mapping (SLAM) and object tracking. This makes
detecting and quantifying registration misalignment, i.e.,~{\it PCR quality
validation}, an important task. All existing methods treat validation as a
classification task, aiming to assign the PCR quality to a few classes. In this
work, we instead use regression for PCR validation, allowing for a more
fine-grained quantification of the registration quality. We also extend
previously used misalignment-related features by using multiscale extraction
and attention-based aggregation. This leads to accurate and robust registration
error estimation on diverse datasets, especially for point clouds with
heterogeneous spatial densities. Furthermore, when used to guide a mapping
downstream task, our method significantly improves the mapping quality for a
given amount of re-registered frames, compared to the state-of-the-art
classification-based method.

</details>


### [73] [4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar](https://arxiv.org/abs/2509.12931)
*Xiao Tang,Guirong Zhuo,Cong Wang,Boyuan Zheng,Minqing Huang,Lianqing Zheng,Long Chen,Shouyi Lu*

Main category: cs.CV

TL;DR: 提出4DRadar-GS：利用4D雷达辅助的高斯表示初始化与速度引导跟踪（VGPT），在自监督框架下重建动态驾驶场景，实现新视角合成并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自监督3D重建/新视角方法在动态场景中依赖频域解耦或光流，因运动估计不准与时间一致性弱，导致动态物体重建不完整或畸变；需要能在无标注条件下，稳健利用额外传感器信息提升动态重建质量。

Method: 1) 4D雷达辅助高斯初始化：利用雷达的速度与空间信息进行动态目标分割与单目深度尺度恢复，生成更准确的高斯点表示；2) 提出Velocity-guided PointTrack (VGPT)：在场景流监督下与重建管线联合训练，跟踪细粒度动态轨迹，提升时间一致性；3) 整体为自监督框架，针对动态驾驶场景。

Result: 在OmniHD-Scenes数据集上取得动态驾驶场景3D重建的SOTA性能（相较现有方法在精度与一致性上显著提升）。

Conclusion: 4DRadar-GS通过4D雷达先验与速度引导的点级跟踪，显著缓解动态物体运动估计不准与时间一致性差的问题，实现更完整、稳定的动态场景3D重建与新视角合成。

Abstract: 3D reconstruction and novel view synthesis are critical for validating
autonomous driving systems and training advanced perception models. Recent
self-supervised methods have gained significant attention due to their
cost-effectiveness and enhanced generalization in scenarios where annotated
bounding boxes are unavailable. However, existing approaches, which often rely
on frequency-domain decoupling or optical flow, struggle to accurately
reconstruct dynamic objects due to imprecise motion estimation and weak
temporal consistency, resulting in incomplete or distorted representations of
dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a
4D Radar-augmented self-supervised 3D reconstruction framework tailored for
dynamic driving scenes. Specifically, we first present a 4D Radar-assisted
Gaussian initialization scheme that leverages 4D Radar's velocity and spatial
information to segment dynamic objects and recover monocular depth scale,
generating accurate Gaussian point representations. In addition, we propose a
Velocity-guided PointTrack (VGPT) model, which is jointly trained with the
reconstruction pipeline under scene flow supervision, to track fine-grained
dynamic trajectories and construct temporally consistent representations.
Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art
performance in dynamic driving scene 3D reconstruction.

</details>


### [74] [Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings](https://arxiv.org/abs/2509.12938)
*Abdalla Arafa,Didier Stricker*

Main category: cs.CV

TL;DR: 提出一种绕过可微渲染进行语义学习的方法：先在3DGS中获得物体级高斯分解，再用多视角CLIP聚合为“嵌入包”，实现开放词汇物体检索与2D/3D任务适配，避免alpha混合导致的语义模糊。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting虽能实时渲染，但其高斯模糊与alpha混合造成语义平均，难以实现精确的3D场景理解；现有通过2D大模型蒸馏的方法继承该缺陷，限制AR/VR与机器人应用。

Method: 不对语义采用可微渲染。先进行物体级高斯预分解；对每个物体从多视角提取CLIP特征并聚合，形成“嵌入包”作为物体表征。用该表征进行文本到物体的相似度检索，并将物体ID传播到像素（用于2D分割）或到高斯（用于3D提取）。

Result: 在3D开放词汇物体提取上有效克服语义混合问题；在2D开放词汇分割上与SOTA相当，性能几乎无损。

Conclusion: 物体级嵌入替代高斯级语义学习可实现更准确的开放词汇3D理解，同时保持2D任务性能，适合AR/VR与机器人等应用场景。

Abstract: Novel view synthesis has seen significant advancements with 3D Gaussian
Splatting (3DGS), enabling real-time photorealistic rendering. However, the
inherent fuzziness of Gaussian Splatting presents challenges for 3D scene
understanding, restricting its broader applications in AR/VR and robotics.
While recent works attempt to learn semantics via 2D foundation model
distillation, they inherit fundamental limitations: alpha blending averages
semantics across objects, making 3D-level understanding impossible. We propose
a paradigm-shifting alternative that bypasses differentiable rendering for
semantics entirely. Our key insight is to leverage predecomposed object-level
Gaussians and represent each object through multiview CLIP feature aggregation,
creating comprehensive "bags of embeddings" that holistically describe objects.
This allows: (1) accurate open-vocabulary object retrieval by comparing text
queries to object-level (not Gaussian-level) embeddings, and (2) seamless task
adaptation: propagating object IDs to pixels for 2D segmentation or to
Gaussians for 3D extraction. Experiments demonstrate that our method
effectively overcomes the challenges of 3D open-vocabulary object extraction
while remaining comparable to state-of-the-art performance in 2D
open-vocabulary segmentation, ensuring minimal compromise.

</details>


### [75] [Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain](https://arxiv.org/abs/2509.12959)
*Yuqi Xie,Shuhan Ye,Chong Wang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出TMKT（时间步Mixup知识迁移）方法，通过在SNN的不同时间步对RGB与DVS输入进行插值，并配合模态感知的辅助学习目标，实现更平滑的跨模态知识迁移，缓解模态偏移，提升脉冲图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机（DVS）与脉冲神经网络（SNN）在能效上有优势，但DVS数据稀缺且输出稀疏，导致训练困难。现有从RGB到DVS的知识迁移方法忽视两种模态间显著分布差异，导致迁移效果受限。需要一种能细粒度对齐时间与模态差异的迁移策略。

Method: 提出时间步Mixup知识迁移（TMKT）：利用SNN的异步时序特性，在多个时间步对RGB与DVS输入进行插值混合；为支持跨模态的标签混合，引入模态感知的辅助学习目标，增强模型在不同模态上的判别能力并稳定混合训练过程。

Result: 在多数据集上进行大量实验，TMKT在脉冲图像分类任务上优于现有方法，显示出更平滑的知识转移与减轻训练中的模态偏移。

Conclusion: 细粒度时间步混合结合模态感知辅助目标，可有效跨越RGB与DVS的分布鸿沟，促进能效视觉中的跨模态知识迁移与性能提升；代码将在双盲评审后开源。

Abstract: The integration of event cameras and spiking neural networks holds great
promise for energy-efficient visual processing. However, the limited
availability of event data and the sparse nature of DVS outputs pose challenges
for effective training. Although some prior work has attempted to transfer
semantic knowledge from RGB datasets to DVS, they often overlook the
significant distribution gap between the two modalities. In this paper, we
propose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing
strategy that exploits the asynchronous nature of SNNs by interpolating RGB and
DVS inputs at various time-steps. To enable label mixing in cross-modal
scenarios, we further introduce modality-aware auxiliary learning objectives.
These objectives support the time-step mixup process and enhance the model's
ability to discriminate effectively across different modalities. Our approach
enables smoother knowledge transfer, alleviates modality shift during training,
and achieves superior performance in spiking image classification tasks.
Extensive experiments demonstrate the effectiveness of our method across
multiple datasets. The code will be released after the double-blind review
process.

</details>


### [76] [MMMS: Multi-Modal Multi-Surface Interactive Segmentation](https://arxiv.org/abs/2509.12963)
*Robin Schön,Julian Lorenz,Katja Ludwig,Daniel Kienzle,Rainer Lienhart*

Main category: cs.CV

TL;DR: 提出一种用于交互式分割的多模态、多表面方法（MMMS），以用户点击为输入，结合RGB与非RGB模态、错误掩码和点击编码，通过在特征提取与多模态融合之后再注入交互信息的架构改进分割；并提出针对多表面相互缠绕情形的扩展评测指标。多模态融合在DeLiVER与MFNet上分别将NoC@90平均减少至多1.28与1.19次/表面；RGB-only基线在单掩码场景也具竞争力。


<details>
  <summary>Details</summary>
Motivation: 交互式分割常聚焦单对象/单掩码，但现实中同一图像内常存在多个相互毗邻、甚至缠绕的表面，传统指标与方法难以评估与处理；同时，多模态信息（如深度、法向、热成像等）可辅助分割，但如何在交互式场景高效融合，且在仅黑盒RGB骨干可用的约束下设计架构，尚缺系统性方案与指标。

Method: 提出MMMS交互式分割框架：输入包括RGB图像、若干非RGB模态、一个初始/错误掩码、以及编码后的用户点击。网络先进行图像特征提取与多模态融合（在不修改黑盒RGB骨干的前提下），随后在融合后阶段再注入交互相关信息（点击与错误掩码），以减少响应延迟并适配黑盒限制。并提出扩展评测指标，专门衡量多表面、相互邻接/缠绕情况下的交互分割表现。

Result: 多模态融合显著提高交互效率：在DeLiVER数据集上平均每个表面的NoC@90至多减少1.28次点击，在MFNet上至多减少1.19次。即便仅用RGB，所提基线在传统单掩码交互分割任务上仍具竞争力，部分情形优于现有方法。

Conclusion: 在黑盒RGB骨干约束与低延迟需求下，所提后期注入交互信息的多模态融合架构有效提升多表面交互式分割的效率与准确性；新评测指标更贴合多表面缠绕场景。多模态带来显著点击数收益，而RGB-only版本在经典单掩码场景同样强劲。

Abstract: In this paper, we present a method to interactively create segmentation masks
on the basis of user clicks. We pay particular attention to the segmentation of
multiple surfaces that are simultaneously present in the same image. Since
these surfaces may be heavily entangled and adjacent, we also present a novel
extended evaluation metric that accounts for the challenges of this scenario.
Additionally, the presented method is able to use multi-modal inputs to
facilitate the segmentation task. At the center of this method is a network
architecture which takes as input an RGB image, a number of non-RGB modalities,
an erroneous mask, and encoded clicks. Based on this input, the network
predicts an improved segmentation mask. We design our architecture such that it
adheres to two conditions: (1) The RGB backbone is only available as a
black-box. (2) To reduce the response time, we want our model to integrate the
interaction-specific information after the image feature extraction and the
multi-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface
interactive segmentation (MMMS). We are able to show the effectiveness of our
multi-modal fusion strategy. Using additional modalities, our system reduces
the NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to
1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline
achieves competitive, and in some cases even superior performance when tested
in a classical, single-mask interactive segmentation scenario.

</details>


### [77] [ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient handwritten documents (FEST)](https://arxiv.org/abs/2509.12965)
*Silvia Zottin,Axel De Nardin,Giuseppe Branca,Claudio Piciarelli,Gian Luca Foresti*

Main category: cs.CV

TL;DR: FEST竞赛提出在极少标注（每份手稿仅3张带标注图像）条件下进行历史手写文稿文本行分割，以推动少样本、鲁棒方法在复杂退化手稿上的应用。


<details>
  <summary>Details</summary>
Motivation: 历史手写文档存在书写不规则、墨迹褪色、布局复杂（重叠行、非线性流）等难点，且大规模标注数据稀缺，使得全监督方法不切实际，需要能以极少标注泛化的方案。

Method: 设立FEST少样本文本行分割竞赛：提供U-DIADS-TL数据集；训练阶段每份手稿仅允许使用3张标注图像；评价参赛系统在多样化古代手稿（不同退化程度与非标准格式）上的行分割能力。

Result: 本文摘要仅介绍任务与数据特性，未给出具体参赛结果或基线性能。

Conclusion: 通过强调少样本学习场景，竞赛旨在催生鲁棒、可迁移的文本行分割方法，降低人文研究中手工标注成本，促进自动文档分析工具在历史研究中的更广泛应用。

Abstract: Text line segmentation is a critical step in handwritten document image
analysis. Segmenting text lines in historical handwritten documents, however,
presents unique challenges due to irregular handwriting, faded ink, and complex
layouts with overlapping lines and non-linear text flow. Furthermore, the
scarcity of large annotated datasets renders fully supervised learning
approaches impractical for such materials. To address these challenges, we
introduce the Few-Shot Text Line Segmentation of Ancient Handwritten Documents
(FEST) Competition. Participants are tasked with developing systems capable of
segmenting text lines in U-DIADS-TL dataset, using only three annotated images
per manuscript for training. The competition dataset features a diverse
collection of ancient manuscripts exhibiting a wide range of layouts,
degradation levels, and non-standard formatting, closely reflecting real-world
conditions. By emphasizing few-shot learning, FEST competition aims to promote
the development of robust and adaptable methods that can be employed by
humanities scholars with minimal manual annotation effort, thus fostering
broader adoption of automated document analysis tools in historical research.

</details>


### [78] [SHREC 2025: Protein surface shape retrieval including electrostatic potential](https://arxiv.org/abs/2509.12976)
*Taher Yacoub,Camille Depenveiller,Atsushi Tatsuma,Tin Barisin,Eugen Rusakov,Udo Gobel,Yuxu Peng,Shiqiang Deng,Yuki Kagaya,Joon Hong Park,Daisuke Kihara,Marco Guerra,Giorgio Palmieri,Andrea Ranieri,Ulderico Fugacci,Silvia Biasotti,Ruiwen He,Halim Benhabiles,Adnane Cabani,Karim Hammoudi,Haotian Li,Hao Huang,Chunyan Li,Alireza Tehrani,Fanwang Meng,Farnaz Heidar-Zadeh,Tuan-Anh Yang,Matthieu Montes*

Main category: cs.CV

TL;DR: SHREC 2025蛋白质表面形状检索赛道：在含电势标注的11,555个蛋白表面数据上评测15种方法，9队参与。融合电静势与形状的方法取得最佳、且在小样本类别上仍占优。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能与相互作用与其表面形状及电静势密切相关，仅依赖几何形状的检索难以区分功能相近或相互补的分子。需要系统评估在大规模数据上，将电静势等分子表面描述符与形状联合用于检索的收益。

Method: 组织SHREC 2025赛道：构建含11,555个蛋白表面的基准库，并为其计算电静势；收集9个团队的15种检索方法；采用多种指标（Accuracy、Balanced Accuracy、F1、Precision、Recall）统一评测，比较仅形状 vs 形状+电静势等策略。

Result: 融合电静势与表面形状的方案整体检索性能最佳，多项指标全面领先；这一优势在样本稀缺类别中同样明显，表明附加表面描述符能提高鲁棒性和泛化。

Conclusion: 在蛋白表面检索中，单靠形状不足；应将电静势等补充描述符纳入表示与相似度度量中，可显著提升整体与小样本类别的检索效果。建议未来方法继续探索多模态表面表征与融合策略。

Abstract: This SHREC 2025 track dedicated to protein surface shape retrieval involved 9
participating teams. We evaluated the performance in retrieval of 15 proposed
methods on a large dataset of 11,555 protein surfaces with calculated
electrostatic potential (a key molecular surface descriptor). The performance
in retrieval of the proposed methods was evaluated through different metrics
(Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best
retrieval performance was achieved by the proposed methods that used the
electrostatic potential complementary to molecular surface shape. This
observation was also valid for classes with limited data which highlights the
importance of taking into account additional molecular surface descriptors.

</details>


### [79] [Improving Accuracy and Efficiency of Implicit Neural Representations: Making SIREN a WINNER](https://arxiv.org/abs/2509.12980)
*Hemanth Chandravamsi,Dhanush V. Shenoy,Steven H. Frankel*

Main category: cs.CV

TL;DR: 论文提出WINNER初始化，通过在SIREN权重上注入与目标信号频谱质心相关的高斯噪声，缓解频谱偏置与“谱瓶颈”，在音频、图像与3D形状拟合上优于基础SIREN。


<details>
  <summary>Details</summary>
Motivation: SIREN在不当初始化下，对超出其频率支持的信号拟合困难，甚至出现“谱瓶颈”：输出接近零，连可表示的频率成分也无法恢复。需要一种初始化策略，使网络频率支持与目标信号谱更好对齐，减轻频谱偏置而不增加额外参数。

Method: 提出WINNER（Weight Initialization with Noise for Neural Representations）：在均匀初始化的SIREN权重上注入高斯噪声；噪声尺度由目标信号的谱质心自适应确定。此举类似随机傅里叶特征的多频率注入，但不引入新可训练参数；通过目标感知的噪声注入拓展有效频率支持，避免训练早期陷入近零解。

Result: 在音频拟合任务达SOTA，并在图像与3D形状拟合上显著优于基础SIREN。方法稳定训练，缓解谱瓶颈与频谱偏置，能恢复更多频率成分。

Conclusion: 目标感知的噪声初始化可有效扩展SIREN的频率支持，克服谱瓶颈与偏置问题，提高多模态信号拟合性能；为深度网络提供了自适应、目标相关的初始化新思路。

Abstract: We identify and address a fundamental limitation of sinusoidal representation
networks (SIRENs), a class of implicit neural representations. SIRENs Sitzmann
et al. (2020), when not initialized appropriately, can struggle at fitting
signals that fall outside their frequency support. In extreme cases, when the
network's frequency support misaligns with the target spectrum, a 'spectral
bottleneck' phenomenon is observed, where the model yields to a near-zero
output and fails to recover even the frequency components that are within its
representational capacity. To overcome this, we propose WINNER - Weight
Initialization with Noise for Neural Representations. WINNER perturbs uniformly
initialized weights of base SIREN with Gaussian noise - whose noise scales are
adaptively determined by the spectral centroid of the target signal. Similar to
random Fourier embeddings, this mitigates 'spectral bias' but without
introducing additional trainable parameters. Our method achieves
state-of-the-art audio fitting and significant gains in image and 3D shape
fitting tasks over base SIREN. Beyond signal fitting, WINNER suggests new
avenues in adaptive, target-aware initialization strategies for optimizing deep
neural network training. For code and data visit
cfdlabtechnion.github.io/siren_square/.

</details>


### [80] [PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era](https://arxiv.org/abs/2509.12989)
*Xu Zheng,Chenfei Liao,Ziqiao Weng,Kaiyu Lei,Zihao Dongfang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Lu Qi,Li Chen,Danda Pani Paudel,Kailun Yang,Linfeng Zhang,Luc Van Gool,Xuming Hu*

Main category: cs.CV

TL;DR: 概览性综述：介绍全向（360°）视觉在具身智能时代的快速发展，综述生成、感知、理解与数据集的最新进展，并提出名为PANORAMA的理想全景系统架构及未来路线图与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统小孔成像在视野覆盖与环境完整感知上受限，而机器人、工业检测、环境监测等领域对全局场景感知和稳健决策的需求激增，学术基础研究相对滞后，亟需系统性梳理与指引。

Method: 以综述与观点文章的方式：汇总近年来学术与工业界在全向生成、感知、理解及数据集方面的突破；提出一个包含四个关键子系统的理想全景系统架构PANORAMA；讨论跨社群影响、趋势、路线图与开放问题。

Result: 凝练了该领域的最新进展与代表性成果脉络；给出了系统架构蓝图（PANORAMA）与关键模块划分；归纳了产业与学界共识与差异；明确了若干研究前沿与挑战。

Conclusion: 全向视觉正在成为具身AI的核心能力之一。通过标准化系统架构与跨领域协同，可推动通用、稳健的全向AI系统落地，但仍面临算法、数据、硬件与评测体系等多方面挑战，需沿所给路线图持续推进。

Abstract: Omnidirectional vision, using 360-degree vision to understand the
environment, has become increasingly critical across domains like robotics,
industrial inspection, and environmental monitoring. Compared to traditional
pinhole vision, omnidirectional vision provides holistic environmental
awareness, significantly enhancing the completeness of scene perception and the
reliability of decision-making. However, foundational research in this area has
historically lagged behind traditional pinhole vision. This talk presents an
emerging trend in the embodied AI era: the rapid development of omnidirectional
vision, driven by growing industrial demand and academic interest. We highlight
recent breakthroughs in omnidirectional generation, omnidirectional perception,
omnidirectional understanding, and related datasets. Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.
Moreover, we offer in-depth opinions related to emerging trends and
cross-community impacts at the intersection of panoramic vision and embodied
AI, along with the future roadmap and open challenges. This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.

</details>


### [81] [Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection](https://arxiv.org/abs/2509.12990)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Sicong Li,Qingming Huang*

Main category: cs.CV

TL;DR: 提出DR-MoE双阶段重加权专家混合框架，从自视角视频判断用户是否做错动作；第一阶段融合冻结与LoRA微调ViViT特征，第二阶段用三种差异化损失训练分类器并在分类层再融合，以应对稀有且细微错误，效果强。


<details>
  <summary>Details</summary>
Motivation: 自视角视频中的错误往往细微、稀少且分布偏斜，传统单一模型或损失函数容易被多数类主导、对罕见错误识别差、校准不佳，需要兼顾特征表达、类别不平衡、排序能力与不确定性校准的整体框架。

Method: 双阶段MoE：1) 特征阶段：用冻结的ViViT与LoRA微调的ViViT各自提取时空特征，通过特征级专家模块进行加权融合。2) 分类阶段：训练三个分类器，分别采用重加权交叉熵（应对类不平衡）、AUC损失（提升在偏斜分布下的排序性能）、标签感知损失结合SAM（提升校准与泛化）；再用分类级专家模块融合三者预测。

Result: 在识别稀有与模糊的错误实例方面取得强劲性能，优于常规方法（摘要未给出具体数值）。

Conclusion: DR-MoE通过特征与分类双层专家融合以及针对不平衡、排序与校准的多目标训练，有效提升了自视角视频中错误动作检测的鲁棒性与泛化能力，并尤其擅长发现罕见、细微错误。

Abstract: In this report, we address the problem of determining whether a user performs
an action incorrectly from egocentric video data. To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework. In the first stage, features are
extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are
combined through a feature-level expert module. In the second stage, three
classifiers are trained with different objectives: reweighted cross-entropy to
mitigate class imbalance, AUC loss to improve ranking under skewed
distributions, and label-aware loss with sharpness-aware minimization to
enhance calibration and generalization. Their predictions are fused using a
classification-level expert module. The proposed method achieves strong
performance, particularly in identifying rare and ambiguous mistake instances.
The code is available at https://github.com/boyuh/DR-MoE.

</details>


### [82] [Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection](https://arxiv.org/abs/2509.12995)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Jinhua Zeng,Bin Li*

Main category: cs.CV

TL;DR: 用现代视觉基础模型（VFM）上训练的简单线性分类器，在相同训练数据下显著优于专门设计的AI图像检测器，尤其在真实场景（in-the-wild）上准确率提升20%+；其优势来自VLM已将“AI生成”等伪造语义对齐到合成图像上，但这种能力对未见过的时间后移数据会下降，提示评测需独立于模型整个训练历史。


<details>
  <summary>Details</summary>
Motivation: 现有专用AI生成图像检测器在精心构建的基准上表现好，但在真实场景中假阴性高、鲁棒性差。作者希望寻找更具泛化性的路径，并检验是否可以借助更新的大规模预训练VFM/VLM获得更强的“通用”判别力。

Method: 提出一个极简基线：在现代VFM（如Perception Encoder、Meta CLIP2等）的冻结特征上训练线性分类器来判别AI合成 vs. 真实图像；对比与专用检测器在相同训练集下的表现；通过文本-图像相似度探针分析VLM是否已学会将合成图像与“AI生成/伪造”等概念对齐；构建时间后移的新数据（在VFM预训练截断日期之后抓取）以检验数据暴露效应。

Result: 线性分类器在in-the-wild基准上比定制检测器提升20%+准确率；VLM显示合成图像与伪造相关文本概念显著对齐；当使用严格时间独立的新数据时，这种对齐及整体检测精度明显下降，暗示此前收益部分来自预训练数据暴露。

Conclusion: 1) 在真实世界检测任务中，更新的VFM原始“火力”（加上线性头）比静态、手工设计的专用检测器更有效。2) 评测真正泛化能力必须确保测试数据独立于模型完整训练历史（含预训练），避免数据泄露式的乐观估计。

Abstract: While specialized detectors for AI-generated images excel on curated
benchmarks, they fail catastrophically in real-world scenarios, as evidenced by
their critically high false-negative rates on `in-the-wild' benchmarks. Instead
of crafting another specialized `knife' for this problem, we bring a `gun' to
the fight: a simple linear classifier on a modern Vision Foundation Model
(VFM). Trained on identical data, this baseline decisively `outguns' bespoke
detectors, boosting in-the-wild accuracy by a striking margin of over 20\%.
  Our analysis pinpoints the source of the VFM's `firepower': First, by probing
text-image similarities, we find that recent VLMs (e.g., Perception Encoder,
Meta CLIP2) have learned to align synthetic images with forgery-related
concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate
that this is due to data exposure, as both this alignment and overall accuracy
plummet on a novel dataset scraped after the VFM's pre-training cut-off date,
ensuring it was unseen during pre-training. Our findings yield two critical
conclusions: 1) For the real-world `gunfight' of AI-generated image detection,
the raw `firepower' of an updated VFM is far more effective than the
`craftsmanship' of a static detector. 2) True generalization evaluation
requires test data to be independent of the model's entire training history,
including pre-training.

</details>


### [83] [Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire](https://arxiv.org/abs/2509.12997)
*Anton Eldeborg Lundin,Rasmus Winzell,Hanna Hamrell,David Gustafsson,Hannes Ovrén*

Main category: cs.CV

TL;DR: 提出一种基于脉冲神经网络（SNN）与事件相机的全类脑无人机入侵检测系统，能以极低功耗在受限区域形成“虚拟绊线”实现早期自动化告警，能靠电池运行一年以上，能用合成数据训练并主要依赖目标形状特征。


<details>
  <summary>Details</summary>
Motivation: 小型无人机对军事与民用设施构成日益增长的威胁，需要早期、自动、低功耗、可长期部署的检测方案；传统帧相机+GPU方案能耗高、难以长期野外值守。

Method: 采用事件相机采集时空稀疏事件流；以脉冲神经网络进行目标检测，并将模型部署在类脑芯片上，实现端到端全类脑处理。多个检测节点可空间布设形成“虚拟绊线”；使用合成数据进行训练并开展消融分析以判断模型依赖的判别线索（形状 vs 螺旋桨时序）。

Result: 与边缘GPU参考实现相比，能效提升数个数量级；系统可依靠电池连续运行一年以上；实验表明模型主要依赖无人机形状而非螺旋桨的时间特征。

Conclusion: 全类脑事件感知+SNN的无人机检测在能耗与可部署性上显著优于传统方案，适合在电力受限或对抗环境中长期布设；合成数据可用于训练，但模型偏向形状线索，提示未来可进一步利用时序特征以提升鲁棒性。

Abstract: Small drones are an increasing threat to both military personnel and civilian
infrastructure, making early and automated detection crucial. In this work we
develop a system that uses spiking neural networks and neuromorphic cameras
(event cameras) to detect drones. The detection model is deployed on a
neuromorphic chip making this a fully neuromorphic system. Multiple detection
units can be deployed to create a virtual tripwire which detects when and where
drones enter a restricted zone. We show that our neuromorphic solution is
several orders of magnitude more energy efficient than a reference solution
deployed on an edge GPU, allowing the system to run for over a year on battery
power. We investigate how synthetically generated data can be used for
training, and show that our model most likely relies on the shape of the drone
rather than the temporal characteristics of its propellers. The small size and
low power consumption allows easy deployment in contested areas or locations
that lack power infrastructure.

</details>


### [84] [Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image](https://arxiv.org/abs/2509.13013)
*Gaofeng Liu,Hengsen Li,Ruoyu Gao,Xuetong Li,Zhiyuan Ma,Tao Fang*

Main category: cs.CV

TL;DR: 提出Dream3DAvatar：从单张图像生成可文本控制、动画就绪的高保真3D头像的两阶段方法。第一阶段用适配器增强的多视图生成（Pose-Adapter+ID-Adapter-G+BLIP2描述）确保姿态/几何一致与身份保持并提升遮挡区域的文本可控性；第二阶段用带多视图融合的Transformer将多视图图像重建为3D Gaussian Splat，并通过ID-Adapter-R门控融合人脸特征，恢复高频细节。实验显示在多指标上优于基线且无需后处理。


<details>
  <summary>Details</summary>
Motivation: 单目输入重建全身3D头像信息不足、遮挡区域几何与纹理难以受控，现有方法在跨视角姿态一致性、身份保持与高频细节方面存在不足，且生成效率与可编辑性（文本控制）有限。

Method: 两阶段框架：1）多视图生成阶段：在SDXL上加入Pose-Adapter（注入SMPL-X渲染与骨架信息，保证跨视角姿态/几何一致）与ID-Adapter-G（注入高分辨人脸特征以保身份），并用BLIP2为生成的多视图图像产生高质量文本描述，增强对被遮挡区域的文本可控性。2）重建阶段：设计前馈Transformer+多视图特征融合模块，从生成的多视图图像重建3D Gaussian Splat（3DGS）；引入ID-Adapter-R通过门控机制融合人脸特征，提升高频细节恢复。

Result: 无需后处理即可生成真实感强、可动画驱动的3D头像；在多项评估指标上稳定优于现有基线（如几何一致性、身份保持、细节质量与可控性），展示更高的效率与质量。

Conclusion: Dream3DAvatar通过适配器增强的多视图生成与Transformer式3DGS重建，有效缓解单目重建的病态性与遮挡控制难题，实现身份保持、姿态一致与高频细节的兼顾，并提供文本可控性；实验验证了其在质量与易用性上的领先与实用价值。

Abstract: With the rapid advancement of 3D representation techniques and generative
models, substantial progress has been made in reconstructing full-body 3D
avatars from a single image. However, this task remains fundamentally
ill-posedness due to the limited information available from monocular input,
making it difficult to control the geometry and texture of occluded regions
during generation. To address these challenges, we redesign the reconstruction
pipeline and propose Dream3DAvatar, an efficient and text-controllable
two-stage framework for 3D avatar generation. In the first stage, we develop a
lightweight, adapter-enhanced multi-view generation model. Specifically, we
introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information
into SDXL, enforcing geometric and pose consistency across views. To preserve
facial identity, we incorporate ID-Adapter-G, which injects high-resolution
facial features into the generation process. Additionally, we leverage BLIP2 to
generate high-quality textual descriptions of the multi-view images, enhancing
text-driven controllability in occluded regions. In the second stage, we design
a feedforward Transformer model equipped with a multi-view feature fusion
module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)
from the generated images. Furthermore, we introduce ID-Adapter-R, which
utilizes a gating mechanism to effectively fuse facial features into the
reconstruction process, improving high-frequency detail recovery. Extensive
experiments demonstrate that our method can generate realistic, animation-ready
3D avatars without any post-processing and consistently outperforms existing
baselines across multiple evaluation metrics.

</details>


### [85] [Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.13031)
*Yan Chen,Long Li,Teng Xi,Long Zeng,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出PeBR-R1：通过两阶段强化学习分别强化视觉感知与推理，结合数据集级采样缓解优势消失，在7个基准上取得更优视觉推理性能。


<details>
  <summary>Details</summary>
Motivation: 直接将针对LLM的RL方法迁移到VLM效果不佳，因为VLM任务更复杂：需要先准确感知并理解图像，再进行推理。因此需要一种同时提升感知与推理的训练方案，并缓解RL训练中优势信号消失的问题。

Method: 两阶段RL框架并配合数据集级采样。先在数据选择上针对不同能力（感知/推理）使用不同数据源以增强特定能力并缓解vanishing advantage。训练阶段一：通过粗粒度与细粒度视觉理解任务提升模型视觉感知；阶段二：在推理相关任务上进一步强化推理能力。最终得到强化后的VLM（PeBR-R1）。

Result: 在7个视觉推理基准数据集上，PeBR-R1展示出显著优于对比方法的性能，验证了两阶段RL与数据集级采样策略的有效性。

Conclusion: 两阶段强化学习能够协同提升VLM的感知与推理；数据集级采样有助于稳定并放大优势信号；整体方法通用且在多种视觉推理任务上实现了性能领先。

Abstract: Reinforcement learning (RL) has proven highly effective in eliciting the
reasoning capabilities of large language models (LLMs). Inspired by this
success, recent studies have explored applying similar techniques to
vision-language models (VLMs), aiming to enhance their reasoning performance.
However, directly transplanting RL methods from LLMs to VLMs is suboptimal, as
the tasks faced by VLMs are inherently more complex. Specifically, VLMs must
first accurately perceive and understand visual inputs before reasoning can be
effectively performed. To address this challenge, we propose a two-stage
reinforcement learning framework designed to jointly enhance both the
perceptual and reasoning capabilities of VLMs. To mitigate the vanishing
advantage issue commonly observed in RL training, we first perform
dataset-level sampling to selectively strengthen specific capabilities using
distinct data sources. During training, the first stage focuses on improving
the model's visual perception through coarse- and fine-grained visual
understanding, while the second stage targets the enhancement of reasoning
abilities. After the proposed two-stage reinforcement learning process, we
obtain PeBR-R1, a vision-language model with significantly enhanced perceptual
and reasoning capabilities. Experimental results on seven benchmark datasets
demonstrate the effectiveness of our approach and validate the superior
performance of PeBR-R1 across diverse visual reasoning tasks.

</details>


### [86] [HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models](https://arxiv.org/abs/2509.13067)
*Xu Li,Yuxuan Liang,Xiaolei Chen,Yi Zheng,Haotian Chen,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 提出HERO：在高分辨率LVLM中进行内容自适应的早期视觉token裁剪与选择，兼顾效率与精度，且无需再训练。


<details>
  <summary>Details</summary>
Motivation: HR-LVLM通过裁切高分辨率图像成局部tile并独立编码，带来细粒度理解，但视觉token数量暴涨，导致算力与内存开销高。作者想弄清哪些token真正有用、不同层关注点如何变化，从而在不牺牲性能的前提下降低成本。

Method: 经验分析三点：1) tile重要性由显著性与任务相关性共同决定；2) 基于CLIP的视觉编码器CLS token跨层呈现两阶段注意模式，不同阶段关注不同类型token；3) 不同阶段强调的token粒度不同且互补。基于此提出HERO：训练-free的高分辨率视觉token早丢弃框架，包含内容自适应的token预算分配（估计tile级重要性）与函数感知的token选择（保留具互补作用的token）。

Result: 在多种基准与不同规模模型上，实现更优的效率-准确率权衡：以更少视觉token、更低计算/内存开销，维持或提升性能。

Conclusion: 通过理解并利用视觉token的重要性与分工，HERO可在不训练的情况下显著提升HR-LVLM推理效率，提供了经验洞见与实用方案。

Abstract: By cropping high-resolution images into local tiles and encoding them
independently, High-Resolution Large Vision-Language Models (HR-LVLMs) have
demonstrated remarkable fine-grained visual understanding capabilities.
However, this divide-and-conquer paradigm significantly increases the number of
visual tokens, resulting in substantial computational and memory overhead. To
better understand and address this challenge, we empirically investigate visual
token utilization in HR-LVLMs and uncover three key findings: (1) the local
tiles have varying importance, jointly determined by visual saliency and task
relevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage
attention pattern across layers, with each stage attending to different types
of visual tokens; (3) the visual tokens emphasized at different stages encode
information at varying levels of granularity, playing complementary roles
within LVLMs. Building on these insights, we propose HERO, a High-resolution
visual token early dropping framework that integrates content-adaptive token
budget allocation with function-aware token selection. By accurately estimating
tile-level importance and selectively retaining visual tokens with
complementary roles, HERO achieves superior efficiency-accuracy trade-offs
across diverse benchmarks and model scales, all in a training-free manner. This
study provides both empirical insights and practical solutions toward efficient
inference in HR-LVLMs.

</details>


### [87] [TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation](https://arxiv.org/abs/2509.13070)
*Qianqi Lu,Yuxiang Xie,Jing Zhang,Shiwei Zou,Yan Chen,Xidao Luan*

Main category: cs.CV

TL;DR: 提出TFANet用于指代引导图像分割，通过三阶段图文对齐（KPS/KFS/KIS）与三个关键模块（MLAM/CFSM/WFDM）缓解跨模态错配与语义流失，在多相似目标复杂场景中提升定位与分割完整性。


<details>
  <summary>Details</summary>
Motivation: 现有RIS方法在复杂场景中易出现图文对齐不充分、跨模态错配以及语言语义在传递过程中的损失，导致目标误定位或分割不完整。需要一种体系化机制在不同尺度与层次上增强图像与文本的细粒度对齐，并弥补语义衰减。

Method: 构建三阶段层级式对齐框架TFANet：1) KPS：多尺度线性交叉注意力模块MLAM，实现多尺度双向语义交换，建立图像区域与不同粒度文本描述的高效对齐；2) KFS：跨模态特征扫描模块CFSM，进行选择性扫描，捕获长程依赖，构建统一的多模态表示，强化复杂场景下的对齐；3) KIS：词级语言特征引导的语义加深模块WFDM，补偿前序阶段带来的语义退化。

Result: 摘要未给出具体指标，但声称在复杂、多相似目标场景中提升跨模态对齐、定位准确性与分割完整性。

Conclusion: 分层三阶段对齐策略结合多尺度交叉注意、长程跨模态依赖建模与词级语义加深，可系统缓解RIS中的多模态错配与语义流失，提升复杂场景下的指引用分割表现。

Abstract: Referring Image Segmentation (RIS) is a task that segments image regions
based on language expressions, requiring fine-grained alignment between two
modalities. However, existing methods often struggle with multimodal
misalignment and language semantic loss, especially in complex scenes
containing multiple visually similar objects, where uniquely described targets
are frequently mislocalized or incompletely segmented. To tackle these
challenges, this paper proposes TFANet, a Three-stage Image-Text Feature
Alignment Network that systematically enhances multimodal alignment through a
hierarchical framework comprising three stages: Knowledge Plus Stage (KPS),
Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the
first stage, we design the Multiscale Linear Cross-Attention Module (MLAM),
which facilitates bidirectional semantic exchange between visual features and
textual representations across multiple scales. This establishes rich and
efficient alignment between image regions and different granularities of
linguistic descriptions. Subsequently, the KFS further strengthens feature
alignment through the Cross-modal Feature Scanning Module (CFSM), which applies
multimodal selective scanning to capture long-range dependencies and construct
a unified multimodal representation. This is essential for modeling long-range
cross-modal dependencies and enhancing alignment accuracy in complex scenes.
Finally, in the KIS, we propose the Word-level Linguistic Feature-guided
Semantic Deepening Module (WFDM) to compensate for semantic degradation
introduced in earlier stages.

</details>


### [88] [Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement](https://arxiv.org/abs/2509.13083)
*Yan Xingyang,Huang Xiaohong,Zhang Zhao,You Tian,Xu Ziheng*

Main category: cs.CV

TL;DR: 提出LLFDisc：结合频域感知的U形增强网络，用分布感知的KL散度损失直接拟合傅里叶域幅度/相位分布，并在VGG感知特征上嵌入KL以提升结构保真，较MSE更稳健，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 像素级损失（如MSE）在频域拟合时过度关注局部，易丢失全局结构信息；幅度承载亮度、相位承载结构，需面向分布的频域对齐以提升全局与结构保真度。

Method: 1) 设计U形增强网络LLFDisc，融合跨注意力与门控机制以实现频域感知特征融合；2) 提出分布感知损失：在傅里叶域对预测与真实的频谱分布进行直接拟合，使用封闭形式的KL散度最小化两者差异；3) 感知损失改进：在VGG提取的深层特征上引入KL散度，使结构一致性更好。

Result: 在多项基准数据集上，LLFDisc在主客观指标均优于现有方法，获得SOTA；定性上结构与全局一致性更佳。

Conclusion: 频域分布对齐（KL）结合改进的感知损失与频域感知网络架构，可比传统MSE像素损失更稳健地保持全局与结构信息，实现图像增强的SOTA表现。

Abstract: In the Fourier domain, luminance information is primarily encoded in the
amplitude spectrum, while spatial structures are captured in the phase
components. The traditional Fourier Frequency information fitting employs
pixel-wise loss functions, which tend to focus excessively on local information
and may lead to global information loss. In this paper, we present LLFDisc, a
U-shaped deep enhancement network that integrates cross-attention and gating
mechanisms tailored for frequency-aware enhancement. We propose a novel
distribution-aware loss that directly fits the Fourier-domain information and
minimizes their divergence using a closed-form KL-Divergence objective. This
enables the model to align Fourier-domain information more robustly than with
conventional MSE-based losses. Furthermore, we enhance the perceptual loss
based on VGG by embedding KL-Divergence on extracted deep features, enabling
better structural fidelity. Extensive experiments across multiple benchmarks
demonstrate that LLFDisc achieves state-of-the-art performance in both
qualitative and quantitative evaluations. Our code will be released at:
https://github.com/YanXY000/LLFDisc

</details>


### [89] [Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling](https://arxiv.org/abs/2509.13084)
*Yunyao Lu,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 提出一种双网络的半监督3D医学图像分割框架，通过跨一致性增强、基于不确定性的动态加权和对比学习对特征进行约束，显著降低伪标签噪声与预测不确定性，在三大数据集上优于SOTA，并有消融验证。


<details>
  <summary>Details</summary>
Motivation: 监督分割需大量标注，不现实；现有半监督方法伪标签噪声大、特征空间监督不足，导致性能受限。

Method: 采用双网络架构：1) 跨一致性增强模块，结合交叉伪标签与熵过滤监督，抑制噪声；2) 基于不确定性的动态权重，利用KL散度调节伪标签损失贡献；3) 自监督对比学习，将不确定体素的特征与可靠类别原型对齐，区分可信与不确定预测，降低不确定性。

Result: 在Left Atrial、NIH Pancreas、BraTS-2019上均取得SOTA或更优表现，例如在Left Atrial仅10%标注下Dice达89.95%。消融实验验证每个模块的有效性。

Conclusion: 结合跨一致性、不确定性加权与对比学习的半监督3D分割框架能有效降低伪标签噪声、增强特征监督并提升分割精度，具有通用性与实用价值。

Abstract: Despite the remarkable performance of supervised medical image segmentation
models, relying on a large amount of labeled data is impractical in real-world
situations. Semi-supervised learning approaches aim to alleviate this challenge
using unlabeled data through pseudo-label generation. Yet, existing
semi-supervised segmentation methods still suffer from noisy pseudo-labels and
insufficient supervision within the feature space. To solve these challenges,
this paper proposes a novel semi-supervised 3D medical image segmentation
framework based on a dual-network architecture. Specifically, we investigate a
Cross Consistency Enhancement module using both cross pseudo and
entropy-filtered supervision to reduce the noisy pseudo-labels, while we design
a dynamic weighting strategy to adjust the contributions of pseudo-labels using
an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In
addition, we use a self-supervised contrastive learning mechanism to align
uncertain voxel features with reliable class prototypes by effectively
differentiating between trustworthy and uncertain predictions, thus reducing
prediction uncertainty. Extensive experiments are conducted on three 3D
segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed
approach consistently exhibits superior performance across various settings
(e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to
the state-of-the-art methods. Furthermore, the usefulness of the proposed
modules is further validated via ablation experiments.

</details>


### [90] [A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control](https://arxiv.org/abs/2509.13089)
*Jonas Werheid,Shengjie He,Aymen Gannouni,Anas Abdelrazeq,Robert H. Schmitt*

Main category: cs.CV

TL;DR: 利用CAD驱动的仿真生成合成图像+目标检测，实现装配质控的低成本视觉方案；在合成数据上mAP@0.5:0.95达99.5%，迁移到真实相机数据达93%，适合SME快速落地。


<details>
  <summary>Details</summary>
Motivation: 装配质控需要可靠的视觉检测，但采集、标注与训练成本高，SME缺乏大规模数据与人工标注资源。合成数据可缓解成本与数据不足问题，但在装配场景中的实用方法与验证仍不足。

Method: 提出一个易集成、数据高效的视觉装配控制流程：以CAD数据为基础进行场景仿真与图像合成，自动生成带标签的数据；训练通用目标检测模型以识别行星齿轮系统等部件；构建从仿真到实机的迁移流程，并在真实相机拍摄数据上评估。

Result: 在模拟训练数据上达到mAP@0.5:0.95=99.5%；在真实测试数据上达到最高93%的mAP，表明合成数据训练的检测器具有较强的域迁移能力；整体流水线显著缩短数据准备时间。

Conclusion: 基于CAD的合成数据生成与目标检测相结合，可在有限资源下实现有效的装配视觉质控；该可适配流水线对SME落地友好，具有节省时间与成本的潜力，支持从仿真到真实的可靠部署。

Abstract: Quality control of assembly processes is essential in manufacturing to ensure
not only the quality of individual components but also their proper integration
into the final product. To assist in this matter, automated assembly control
using computer vision methods has been widely implemented. However, the costs
associated with image acquisition, annotation, and training of computer vision
algorithms pose challenges for integration, especially for small- and
medium-sized enterprises (SMEs), which often lack the resources for extensive
training, data collection, and manual image annotation. Synthetic data offers
the potential to reduce manual data collection and labeling. Nevertheless, its
practical application in the context of assembly quality remains limited. In
this work, we present a novel approach for easily integrable and data-efficient
visual assembly control. Our approach leverages simulated scene generation
based on computer-aided design (CAD) data and object detection algorithms. The
results demonstrate a time-saving pipeline for generating image data in
manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95)
up to 99,5% for correctly identifying instances of synthetic planetary gear
system components within our simulated training data, and up to 93% when
transferred to real-world camera-captured testing data. This research
highlights the effectiveness of synthetic data generation within an adaptable
pipeline and underscores its potential to support SMEs in implementing
resource-efficient visual assembly control solutions.

</details>


### [91] [Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2509.13107)
*Kohou Wang,Huan Hu,Xiang Liu,Zezhou Chen,Ping Chen,Zhaoxiang Liu,Shiguo Lian*

Main category: cs.CV

TL;DR: 提出HDFF层次深度融合框架，集成四个预训练视觉骨干（Swin-MLP、CoAtNet、EfficientNetV2、DaViT），经多阶段微调并级联特征后训练终分类器，在MultiFFDI竞赛私有榜得分0.96852，排名20/184，用于鲁棒人脸深伪检测。


<details>
  <summary>Details</summary>
Motivation: 深伪技术快速演进、手法多样，单一检测模型易过拟合特定伪造类型、跨分布泛化弱；需要鲁棒、可泛化、覆盖多种操纵方式的检测方案以提升数字安全与真实性保障。

Method: 构建层次化集成：选取四种结构互补的预训练模型（Swin-MLP、CoAtNet、EfficientNetV2、DaViT），在MultiFFDI数据集上进行多阶段微调；提取每个子模型的高层特征向量并进行级联（concatenate）；在拼接后的表示上训练一个终端分类器，实现特征级融合与决策。

Result: 在相关竞赛的私有榜单上取得0.96852的最终分数，排名第20/184，显示该融合策略在复杂图像（人脸伪造）分类任务上的有效性。

Conclusion: 层次化特征级深度融合能有效整合多架构的互补表征，提高深伪检测的鲁棒性与泛化性能；实证结果（竞赛成绩）支持该策略的可行性。

Abstract: The proliferation of sophisticated deepfake technology poses significant
challenges to digital security and authenticity. Detecting these forgeries,
especially across a wide spectrum of manipulation techniques, requires robust
and generalized models. This paper introduces the Hierarchical Deep Fusion
Framework (HDFF), an ensemble-based deep learning architecture designed for
high-performance facial forgery detection. Our framework integrates four
diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT,
which are meticulously fine-tuned through a multi-stage process on the
MultiFFDI dataset. By concatenating the feature representations from these
specialized models and training a final classifier layer, HDFF effectively
leverages their collective strengths. This approach achieved a final score of
0.96852 on the competition's private leaderboard, securing the 20th position
out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex
image classification tasks.

</details>


### [92] [Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving](https://arxiv.org/abs/2509.13116)
*Ruibo Li,Hanyu Shi,Zhe Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出一种利用LiDAR点云进行类别无关的运动预测的弱监督/自监督框架：用前景/背景或非地面/地面掩码替代运动标注，并设计鲁棒一致性感知的Chamfer距离损失，多帧信息+鲁棒惩罚抑制离群点；在更少标注(至0%～0.01%)下超越现有自监督方法，弱监督甚至可媲美部分监督方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要理解动态环境中物体的运动。传统监督方法依赖昂贵的逐点或逐实例运动标注；现有自监督方法性能受限。户外场景的先验（移动前景 vs 静态背景、运动多发生于非地面）可作为廉价监督信号，减少标注成本并提升性能。

Method: 1) 弱监督范式：用全/部分(1%、0.1%)前景/背景掩码取代运动标注，通过这些线索引导自监督的运动预测网络训练。2) 掩码替代：利用非地面/地面掩码作为近似的运动先验，进一步减少标注；据此提出：a) 仅需极少(0.01%)前景/背景标注的弱监督方法；b) 完全不需标注的自监督方法。3) 损失设计：提出鲁棒一致性感知Chamfer距离（RC-CD），融合多帧信息并采用鲁棒惩罚，缓解自监督中的离群点影响并提升跨帧一致性。

Result: 在公开数据上的实验表明：所提弱/自监督模型优于现有自监督基线；弱监督模型在极低标注比例下仍可达到或逼近部分监督方法的表现，展现出更优的标注效率-性能折中。

Conclusion: 利用场景先验（前景/背景与地面/非地面）可有效替代昂贵运动标注；配合RC-CD等鲁棒自监督目标，可在大幅降低标注成本的同时保持甚至提升运动预测性能，实现良好的性能-标注成本平衡。

Abstract: Understanding motion in dynamic environments is critical for autonomous
driving, thereby motivating research on class-agnostic motion prediction. In
this work, we investigate weakly and self-supervised class-agnostic motion
prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile
foregrounds and static backgrounds, allowing motion understanding to be
associated with scene parsing. Based on this observation, we propose a novel
weakly supervised paradigm that replaces motion annotations with fully or
partially annotated (1%, 0.1%) foreground/background masks for supervision. To
this end, we develop a weakly supervised approach utilizing
foreground/background cues to guide the self-supervised learning of motion
prediction models. Since foreground motion generally occurs in non-ground
regions, non-ground/ground masks can serve as an alternative to
foreground/background masks, further reducing annotation effort. Leveraging
non-ground/ground cues, we propose two additional approaches: a weakly
supervised method requiring fewer (0.01%) foreground/background annotations,
and a self-supervised method without annotations. Furthermore, we design a
Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame
information and robust penalty functions to suppress outliers in
self-supervised learning. Experiments show that our weakly and self-supervised
models outperform existing self-supervised counterparts, and our weakly
supervised models even rival some supervised ones. This demonstrates that our
approaches effectively balance annotation effort and performance.

</details>


### [93] [Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline](https://arxiv.org/abs/2509.13133)
*Zhihao Zhang,Chunyu Lin,Lang Nie,Jiyuan Wang,Yao Zhao*

Main category: cs.CV

TL;DR: 提出CRPS-D大规模环视泊车车位检测数据集与半监督基线SS-PSD（教师-学生框架），在含噪多样场景下显著优于现有方法，且未标注数据越多收益越大。


<details>
  <summary>Details</summary>
Motivation: 现有环视泊车位检测数据集规模小、场景噪声少（光照、遮挡等），人工标注易错成本高，限制了模型泛化与实用性；缺乏利用未标注数据的半监督方法。

Method: 1) 构建CRPS-D数据集：涵盖多种光照、天气、形态（密集且含大量斜置车位）。2) 提出半监督检测框架SS-PSD：基于教师-学生模型，引入置信度引导的掩码一致性学习与自适应特征扰动，利用未标注数据提升鲁棒性与精度。

Result: 在CRPS-D与现有数据集上，SS-PSD均超越SoTA；随着未标注数据规模增加，性能提升更显著。

Conclusion: 面向真实复杂场景，CRPS-D为研究提供更大更难的数据基座；SS-PSD首次将半监督范式引入泊车位检测并验证有效性，显示利用未标注数据是提升性能的关键路径。

Abstract: As automatic parking systems evolve, the accurate detection of parking slots
has become increasingly critical. This study focuses on parking slot detection
using surround-view cameras, which offer a comprehensive bird's-eye view of the
parking environment. However, the current datasets are limited in scale, and
the scenes they contain are seldom disrupted by real-world noise (e.g., light,
occlusion, etc.). Moreover, manual data annotation is prone to errors and
omissions due to the complexity of real-world conditions, significantly
increasing the cost of annotating large-scale datasets. To address these
issues, we first construct a large-scale parking slot detection dataset (named
CRPS-D), which includes various lighting distributions, diverse weather
conditions, and challenging parking slot variants. Compared with existing
datasets, the proposed dataset boasts the largest data scale and consists of a
higher density of parking slots, particularly featuring more slanted parking
slots. Additionally, we develop a semi-supervised baseline for parking slot
detection, termed SS-PSD, to further improve performance by exploiting
unlabeled data. To our knowledge, this is the first semi-supervised approach in
parking slot detection, which is built on the teacher-student model with
confidence-guided mask consistency and adaptive feature perturbation.
Experimental results demonstrate the superiority of SS-PSD over the existing
state-of-the-art (SoTA) solutions on both the proposed dataset and the existing
dataset. Particularly, the more unlabeled data there is, the more significant
the gains brought by our semi-supervised scheme. The relevant source codes and
the dataset have been made publicly available at
https://github.com/zzh362/CRPS-D.

</details>


### [94] [MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation](https://arxiv.org/abs/2509.13149)
*Minqing Huang,Shouyi Lu,Boyuan Zheng,Ziyao Li,Xiao Tang,Guirong Zhuo*

Main category: cs.CV

TL;DR: 提出MSDNet，一个多阶段蒸馏框架，将稠密LiDAR先验高效迁移到4D雷达特征，实现高质量、低延迟的点云超分辨并提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达超分辨方法要么训练代价高、要么依赖复杂扩散采样导致推理慢且泛化差，难以在精度与效率间取得平衡。需要一种能高效引入稠密先验、同时保持快速推理的方案。

Method: 多阶段蒸馏：1) 重建引导的特征蒸馏，通过特征重建对齐并加密学生特征；2) 扩散引导的特征蒸馏，将第一阶段结果视为教师表示的带噪版本，用轻量级扩散网络细化；并引入噪声适配器，将特征噪声水平与预设扩散时间步自适应对齐以实现更精确去噪。

Result: 在VoD和自建数据集上，MSDNet实现高保真重建与低延迟推理，并在下游任务上持续带来性能提升。

Conclusion: MSDNet有效在4D雷达超分辨中兼顾精度与效率，通过分阶段蒸馏与噪声适配器，将LiDAR先验高效迁移到雷达特征，优于现有方法并具备良好实用性。

Abstract: 4D radar super-resolution, which aims to reconstruct sparse and noisy point
clouds into dense and geometrically consistent representations, is a
foundational problem in autonomous perception. However, existing methods often
suffer from high training cost or rely on complex diffusion-based sampling,
resulting in high inference latency and poor generalization, making it
difficult to balance accuracy and efficiency. To address these limitations, we
propose MSDNet, a multi-stage distillation framework that efficiently transfers
dense LiDAR priors to 4D radar features to achieve both high reconstruction
quality and computational efficiency. The first stage performs
reconstruction-guided feature distillation, aligning and densifying the
student's features through feature reconstruction. In the second stage, we
propose diffusion-guided feature distillation, which treats the stage-one
distilled features as a noisy version of the teacher's representations and
refines them via a lightweight diffusion network. Furthermore, we introduce a
noise adapter that adaptively aligns the noise level of the feature with a
predefined diffusion timestep, enabling a more precise denoising. Extensive
experiments on the VoD and in-house datasets demonstrate that MSDNet achieves
both high-fidelity reconstruction and low-latency inference in the task of 4D
radar point cloud super-resolution, and consistently improves performance on
downstream tasks. The code will be publicly available upon publication.

</details>


### [95] [TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images](https://arxiv.org/abs/2509.13151)
*Rohan Kumar,Jyothi Swaroopa Jinka,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: 提出TexTAR：一种多任务、上下文感知的Transformer用于识别文本粗体、斜体、下划线、删除线等属性；并发布多语言多领域数据集MMTAD；通过2D RoPE式位置机制与数据选择管线提升上下文利用，取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 文本属性（粗体/斜体/下划线/删除线）承载语义突出与版面结构信息，对文档理解至关重要；现有方法在计算效率、噪声鲁棒性和多语言适应性上不足，尤其在真实复杂文档中表现欠佳。

Method: 1) 设计TexTAR：多任务、上下文感知Transformer，用于TAR；2) 引入2D RoPE风格机制，结合二维位置信息将上下文注入模型以更准确预测属性；3) 提出数据选择管线，增强上下文相关性；4) 构建MMTAD数据集：覆盖多语言、多领域、真实文档并标注文本属性；5) 在多基准上评测并与现有方法对比。

Result: TexTAR在广泛评测中优于现有方法，在多语言、多领域与噪声环境下表现出更好的准确性与稳健性；实验表明上下文感知显著提升TAR性能，达到SOTA。

Conclusion: 通过上下文感知建模（2D RoPE）与数据选择策略，TexTAR实现了高效、鲁棒且可泛化的文本属性识别；MMTAD为该任务提供了多样化基准。研究证明引入上下文可显著推动TAR的最新进展。

Abstract: Recognizing textual attributes such as bold, italic, underline and strikeout
is essential for understanding text semantics, structure, and visual
presentation. These attributes highlight key information, making them crucial
for document analysis. Existing methods struggle with computational efficiency
or adaptability in noisy, multilingual settings. To address this, we introduce
TexTAR, a multi-task, context-aware Transformer for Textual Attribute
Recognition (TAR). Our novel data selection pipeline enhances context
awareness, and our architecture employs a 2D RoPE (Rotary Positional
Embedding)-style mechanism to incorporate input context for more accurate
attribute predictions. We also introduce MMTAD, a diverse, multilingual,
multi-domain dataset annotated with text attributes across real-world documents
such as legal records, notices, and textbooks. Extensive evaluations show
TexTAR outperforms existing methods, demonstrating that contextual awareness
contributes to state-of-the-art TAR performance.

</details>


### [96] [Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)](https://arxiv.org/abs/2509.13161)
*Zhihao He,Tianyao He,Tieyuan Chen,Yun Xu,Huabin Liu,Chaofan Gan,Gui Zou,Weiyao Lin*

Main category: cs.CV

TL;DR: 提出一个多视频协作的视频语言模型框架：先把每个视频结构化为时空图，再通过图融合模块从相关视频中选择并融合有价值的信息，最后构建结构化多视频提示输入大模型，从而减少冗余与幻觉并提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 单个视频常存在时空不完整（信息缺失、遮挡、视角限制），导致VL（视频语言）模型推理出现幻觉和不准确。直接把多视频原始令牌喂给大模型会因冗余与成本过高而适得其反，需要一种既高效又可融合多源相关视频信息的方法。

Method: 1) 视频结构化模块：将视频知识表征为时空图（节点/边描述实体、动作与时序关系），获得紧凑且可操作的表示。2) 图融合模块：在结构化表示上，从相关视频中筛选并融合关键信息，生成增强的图节点token。3) 多视频结构化提示：把融合后的图token、必要的视觉与文本token共同组织为对LLM友好的输入提示，实现协作式多视频推理。

Result: 在多项实验上验证该框架有效，能降低冗余、缓解幻觉并提升跨视频综合推理表现（文中称“广泛实验”证明其优越性）。

Conclusion: 多视频协作+图结构表示与融合能够高效整合跨视频时空信息，增强VLM的稳健性与推理能力，是推进视频语言模型的重要方向。

Abstract: Despite the prosperity of the video language model, the current pursuit of
comprehensive video reasoning is thwarted by the inherent spatio-temporal
incompleteness within individual videos, resulting in hallucinations and
inaccuracies. A promising solution is to augment the reasoning performance with
multiple related videos. However, video tokens are numerous and contain
redundant information, so directly feeding the relevant video data into a large
language model to enhance responses could be counterproductive. To address this
challenge, we propose a multi-video collaborative framework for video language
models. For efficient and flexible video representation, we establish a Video
Structuring Module to represent the video's knowledge as a spatio-temporal
graph. Based on the structured video representation, we design the Graph Fusion
Module to fuse the structured knowledge and valuable information from related
videos into the augmented graph node tokens. Finally, we construct an elaborate
multi-video structured prompt to integrate the graph, visual, and textual
tokens as the input to the large language model. Extensive experiments
substantiate the effectiveness of our framework, showcasing its potential as a
promising avenue for advancing video language models.

</details>


### [97] [WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory](https://arxiv.org/abs/2509.13172)
*Ruifei Ding,Zhe Chen,Wen Fan,Chen Long,Huijuan Xiao,Yelu Zeng,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: 提出WHU-STree：跨城市、富标注、多模态（点云+高分图像）的街道树数据集，含21,007株、50物种、2形态参数，支持10+任务并给出分类与实例分割基线，实验证明多模态融合和跨域适应的重要性，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统地面调查成本高、效率低；现有MMS树数据集规模小、标注少或单一模态，难以支撑全面分析与真实部署需求。因此需要一个大规模、跨城市、细粒度标注、支持多任务与多模态融合的数据资源。

Method: 基于MMS同步采集两座城市的点云与高分辨率图像，构建WHU-STree数据集；对21,007棵街道树进行多层次标注（50物种、2个形态参数等），设计可支持10+任务的标注体系；选择代表性方法对两个关键任务（树种分类、个体树分割）进行基线评测，并开展多模态融合与跨域实验分析。

Result: 给出树种分类与个体树分割的基线性能；实验显示多模态数据融合显著提升效果；跨城市（域）泛化是算法实用化的关键难点；通过深入分析揭示影响性能的因素与挑战。

Conclusion: WHU-STree填补了跨城市、多模态、富标注街道树数据资源的空缺，能同时支持多任务研究。多模态融合与跨域泛化对实际部署至关重要。作者提出未来方向：多模态融合策略、跨任务协同、跨域适配、空间模式学习，以及利用多模态大模型赋能街道树资产管理；数据集与基线已开源。

Abstract: Street trees are vital to urban livability, providing ecological and social
benefits. Establishing a detailed, accurate, and dynamically updated street
tree inventory has become essential for optimizing these multifunctional assets
within space-constrained urban environments. Given that traditional field
surveys are time-consuming and labor-intensive, automated surveys utilizing
Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing
MMS-acquired tree datasets are limited by small-scale scene, limited
annotation, or single modality, restricting their utility for comprehensive
analysis. To address these limitations, we introduce WHU-STree, a cross-city,
richly annotated, and multi-modal urban street tree dataset. Collected across
two distinct cities, WHU-STree integrates synchronized point clouds and
high-resolution images, encompassing 21,007 annotated tree instances across 50
species and 2 morphological parameters. Leveraging the unique characteristics,
WHU-STree concurrently supports over 10 tasks related to street tree inventory.
We benchmark representative baselines for two key tasks--tree species
classification and individual tree segmentation. Extensive experiments and
in-depth analysis demonstrate the significant potential of multi-modal data
fusion and underscore cross-domain applicability as a critical prerequisite for
practical algorithm deployment. In particular, we identify key challenges and
outline potential future works for fully exploiting WHU-STree, encompassing
multi-modal fusion, multi-task collaboration, cross-domain generalization,
spatial pattern learning, and Multi-modal Large Language Model for street tree
asset management. The WHU-STree dataset is accessible at:
https://github.com/WHU-USI3DV/WHU-STree.

</details>


### [98] [More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era](https://arxiv.org/abs/2509.13175)
*Yingtai Li,Haoran Lai,Xiaoqian Zhou,Shuai Ming,Wenxin Ma,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 利用LLM从放射科报告自动提取诊断标签，低成本构建大规模“银标准”监督数据，并用于对比学习预训练；在CT任务上显著提升零样本诊断与跨模态检索表现。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的对比式视觉-语言预训练依赖高质量、大规模标注，但人工标注昂贵、BERT式专用抽取管线复杂且门槛高。作者希望用通用LLM低成本、高精度地从报告中抽取标签，从而扩大监督数据规模，并检验监督预训练是否能从根本上增强视觉-语言对齐与下游性能。

Method: 1) 直接用现代LLM对放射学报告进行标签抽取（几乎无需复杂提示），获得“银标准”标签；2) 用该标签大规模监督训练3D ResNet-18视觉编码器，并与基于专用BERT抽取的标签训练进行对比；3) 在此基础上配合标准CLIP式对比学习进行视觉-语言对齐；4) 在多个CT基准上评测零样本诊断与跨模态检索。

Result: - LLM标签抽取达到>96% AUC，成本约$3/5万对CT图像-报告；- 用银标准数据训练的视觉编码器性能与用BERT标签训练者相当；- 仅用3D ResNet-18+vanilla CLIP即达SOTA：CT-RATE零样本AUC 83.8%，RAD-ChestCT AUC 77.3%；检索显著提升：图-图MAP@50=53.7%，文-图Recall@100=52.2%。

Conclusion: LLM可低成本高精度地产生大规模监督信号，显著提升医疗对比式视觉-语言预训练的对齐与下游效果，实现更高性能且可扩展的医疗AI流程。

Abstract: The emergence of Large Language Models (LLMs) presents unprecedented
opportunities to revolutionize medical contrastive vision-language
pre-training. In this paper, we show how LLMs can facilitate large-scale
supervised pre-training, thereby advancing vision-language alignment. We begin
by demonstrate that modern LLMs can automatically extract diagnostic labels
from radiology reports with remarkable precision (>96\% AUC in our experiments)
without complex prompt engineering, enabling the creation of large-scale
"silver-standard" datasets at a minimal cost (~\$3 for 50k CT image-report
pairs). Further, we find that vision encoder trained on this "silver-standard"
dataset achieves performance comparable to those trained on labels extracted by
specialized BERT-based models, thereby democratizing the access to large-scale
supervised pre-training. Building on this foundation, we proceed to reveal that
supervised pre-training fundamentally improves contrastive vision-language
alignment. Our approach achieves state-of-the-art performance using only a 3D
ResNet-18 with vanilla CLIP training, including 83.8\% AUC for zero-shot
diagnosis on CT-RATE, 77.3\% AUC on RAD-ChestCT, and substantial improvements
in cross-modal retrieval (MAP@50=53.7\% for image-image, Recall@100=52.2\% for
report-image). These results demonstrate the potential of utilizing LLMs to
facilitate {\bf more performant and scalable} medical AI systems. Our code is
avaiable at https://github.com/SadVoxel/More-performant-and-scalable.

</details>


### [99] [Road Obstacle Video Segmentation](https://arxiv.org/abs/2509.13181)
*Shyam Nandan Rai,Shyamgopal Karthik,Mariana-Iuliana Georgescu,Barbara Caputo,Carlo Masone,Zeynep Akata*

Main category: cs.CV

TL;DR: 论文关注公路障碍物分割在视频序列中的一致性问题，提出其本质是时序任务，构建/改造四个评测基准，评测11种SOTA图像/视频方法，并基于视觉基础模型给出两个强基线，在长序列上取得新的SOTA并给出未来研究启示。


<details>
  <summary>Details</summary>
Motivation: 现有道路障碍物分割多在单帧上进行，忽视帧间时序关联，导致连续帧预测不一致，影响自动驾驶安全与稳定性。作者认为连续帧的分割结果高度相关，应以时序视角重新定义与评测该任务。

Method: 1) 论证任务的时序本质与帧间相关性；2) 整理并改造四个道路障碍物视频分割评测基准；3) 系统评测11种SOTA图像/视频分割方法；4) 基于视觉基础模型设计两种强力基线；5) 在长时程视频上进行统一评测与比较。

Result: 在四个视频基准上，对11种方法进行了全面评测；所提出的两种基线在长序列视频道路障碍物分割上达到新的SOTA性能，显示显著优于现有图像/视频方法的时序一致性与准确性。

Conclusion: 道路障碍物分割应作为视频时序问题对待；引入的基准与强基线为该领域提供了新的评测与方法参考，并在长序列上刷新SOTA，指明未来研究应重视时序一致性与利用视觉基础模型的潜力。

Abstract: With the growing deployment of autonomous driving agents, the detection and
segmentation of road obstacles have become critical to ensure safe autonomous
navigation. However, existing road-obstacle segmentation methods are applied on
individual frames, overlooking the temporal nature of the problem, leading to
inconsistent prediction maps between consecutive frames. In this work, we
demonstrate that the road-obstacle segmentation task is inherently temporal,
since the segmentation maps for consecutive frames are strongly correlated. To
address this, we curate and adapt four evaluation benchmarks for road-obstacle
video segmentation and evaluate 11 state-of-the-art image- and video-based
segmentation methods on these benchmarks. Moreover, we introduce two strong
baseline methods based on vision foundation models. Our approach establishes a
new state-of-the-art in road-obstacle video segmentation for long-range video
sequences, providing valuable insights and direction for future research.

</details>


### [100] [Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance](https://arxiv.org/abs/2509.13210)
*Ligang Chang,Shengkai Xu,Liangchang Shen,Binhan Xu,Junqiao Wang,Tianyu Shi,Yanhui Du*

Main category: cs.CV

TL;DR: 提出Vi-SAFE：结合轻量化YOLOv8与TSN的时空暴力行为检测框架，在RWF-2000上达0.88准确率，优于仅用TSN（0.77），兼顾准确与效率。


<details>
  <summary>Details</summary>
Motivation: 公共监控中的暴力行为通常目标小、场景复杂且需实时分析，现有方法在计算成本、时空融合与鲁棒性上不足，亟需一种兼顾精度与实时性的解决方案。

Method: 构建Vi-SAFE时空框架：1）空间端采用改进YOLOv8进行行人检测与区域提取；用GhostNetV3作为轻量主干、引入EMA注意力、并通过剪枝降低计算量同时保持精度。2）时间端采用TSN对提取的人体区域进行二分类（暴力/非暴力）。3）分别在行人数据集与暴力数据集上独立训练YOLOv8与TSN，实现级联推理。

Result: 在RWF-2000数据集上，Vi-SAFE准确率0.88，显著高于单独TSN的0.77；在精度与效率上均优于现有方法（文中宣称），满足实时监控需求。

Conclusion: 将轻量化的YOLOv8空间检测与TSN时间建模耦合，可有效提升暴力检测的准确性与效率，适合公共安全监控场景。提供开源代码便于复现与部署。

Abstract: Violence detection in public surveillance is critical for public safety. This
study addresses challenges such as small-scale targets, complex environments,
and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal
framework that integrates an enhanced YOLOv8 with a Temporal Segment Network
(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as
a lightweight backbone, an exponential moving average (EMA) attention
mechanism, and pruning to reduce computational cost while maintaining accuracy.
YOLOv8 and TSN are trained separately on pedestrian and violence datasets,
where YOLOv8 extracts human regions and TSN performs binary classification of
violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE
achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming
existing methods in both accuracy and efficiency, demonstrating its
effectiveness for public safety surveillance. Code is available at
https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.

</details>


### [101] [End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection](https://arxiv.org/abs/2509.13214)
*Fei Wang,Xuecheng Wu,Zheng Zhang,Danlei Huang,Yuheng Huang,BoWang*

Main category: cs.CV

TL;DR: 提出End4，一种端到端去噪扩散检测框架，结合重建对齐与尺度金字塔特征融合，用于可靠识别扩散模型的修补（inpainting）生成图像，并在多遮罩基准上展示对未见遮罩与扰动的强泛化。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成与修补上表现卓越，但带来被恶意滥用的风险。现有检测方法难以识别由扩散式修补产生的图像，即便训练时包含相似样本，说明对修补场景的泛化与判别特征提取不足。

Method: 提出End4：1) 端到端去噪式重建-检测框架，设计去噪重建模型以提升重建与检测潜在空间的对齐，从而获得利于检测的重建特征；2) 引入尺度感知的金字塔式融合模块（SPFM），利用多尺度注意力金字塔引导，细化局部图像特征并增强判别性；3) 构建涵盖五种不同遮罩区域的修补检测基准，用于系统评测。

Result: 在广泛实验中，End4对未见遮罩模式具有良好泛化能力，并在多种扰动下保持鲁棒；相较现有方法，检测性能显著提升。代码与数据集即将开源。

Conclusion: 通过端到端的去噪重建对齐与尺度金字塔特征融合，End4能够有效检测扩散修补生成图像，并在跨遮罩与扰动场景中表现稳健，为实际风险治理提供可用工具与评测基准。

Abstract: The powerful generative capabilities of diffusion models have significantly
advanced the field of image synthesis, enhancing both full image generation and
inpainting-based image editing. Despite their remarkable advancements,
diffusion models also raise concerns about potential misuse for malicious
purposes. However, existing approaches struggle to identify images generated by
diffusion-based inpainting models, even when similar inpainted images are
included in their training data. To address this challenge, we propose a novel
detection method based on End-to-end denoising diffusion (End4). Specifically,
End4 designs a denoising reconstruction model to improve the alignment degree
between the latent spaces of the reconstruction and detection processes, thus
reconstructing features that are more conducive to detection. Meanwhile, it
leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local
image features under the guidance of attention pyramid layers at different
scales, enhancing feature discriminability. Additionally, to evaluate detection
performance on inpainted images, we establish a comprehensive benchmark
comprising images generated from five distinct masked regions. Extensive
experiments demonstrate that our End4 effectively generalizes to unseen masking
patterns and remains robust under various perturbations. Our code and dataset
will be released soon.

</details>


### [102] [Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation](https://arxiv.org/abs/2509.13229)
*Hugo Carlesso,Josiane Mothe,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出CMTSSL：一种结合掩码图像建模与空间/光谱拼图的课程式多任务自监督框架，用于轻量HSI编码器预训练，在多数据集下显著提升下游分割表现，适合星载端侧部署。


<details>
  <summary>Details</summary>
Motivation: HSI高维度与卫星下行速率受限，需在星载端进行高效处理，过滤低价值数据（如云区），因此需要能在轻量模型上学习通用表征的自监督方法。

Method: 设计CMTSSL框架：1) 将掩码图像建模与解耦的空间拼图和光谱拼图联合训练；2) 采用课程学习，逐步提升自监督难度（如遮罩比例、拼图扰动强度与块数），引导编码器从局部连续性到全局语义逐级掌握；3) 统一高效的多任务设计，兼顾光谱连续性、空间结构与全局语义；4) 面向轻量化架构，计算与参数量低，适配星载部署。

Result: 在四个公开HSI基准上进行验证，使用极轻量架构（较部分SOTA轻16000倍参数量），在下游语义分割任务中获得一致增益。

Conclusion: CMTSSL在轻量模型上实现通用、可迁移的HSI表征学习，兼顾空间与光谱推理，适用于星载实时/在轨处理，减少冗余数据传输，具有实际应用潜力。

Abstract: Hyperspectral imaging (HSI) captures detailed spectral signatures across
hundreds of contiguous bands per pixel, being indispensable for remote sensing
applications such as land-cover classification, change detection, and
environmental monitoring. Due to the high dimensionality of HSI data and the
slow rate of data transfer in satellite-based systems, compact and efficient
models are required to support onboard processing and minimize the transmission
of redundant or low-value data, e.g. cloud-covered areas. To this end, we
introduce a novel curriculum multi-task self-supervised learning (CMTSSL)
framework designed for lightweight architectures for HSI analysis. CMTSSL
integrates masked image modeling with decoupled spatial and spectral jigsaw
puzzle solving, guided by a curriculum learning strategy that progressively
increases data complexity during self-supervision. This enables the encoder to
jointly capture fine-grained spectral continuity, spatial structure, and global
semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously
addresses spatial and spectral reasoning within a unified and computationally
efficient design, being particularly suitable for training lightweight models
for onboard satellite deployment. We validate our approach on four public
benchmark datasets, demonstrating consistent gains in downstream segmentation
tasks, using architectures that are over 16,000x lighter than some
state-of-the-art models. These results highlight the potential of CMTSSL in
generalizable representation learning with lightweight architectures for
real-world HSI applications. Our code is publicly available at
https://github.com/hugocarlesso/CMTSSL.

</details>


### [103] [Intelligent Vacuum Thermoforming Process](https://arxiv.org/abs/2509.13250)
*Andi Kuswoyo,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.CV

TL;DR: 提出一种基于视觉的质量控制系统，通过少量数据预测并优化真空热成型工艺参数，使用数据增强与kNN将低质件映射到高质件参数，显著改善加热功率/时间与真空时间设置，降低缺陷、提升效率。


<details>
  <summary>Details</summary>
Motivation: 真空热成型制品质量受材料属性与模具配置波动影响较大，传统依赖经验或大量数据/昂贵传感的控制策略难以稳定保证质量。需要一种能在数据有限条件下，快速、可泛化地指导工艺参数调整的方案。

Method: 构建以成型制件视觉图像为核心的数据集，并通过图像增强扩充样本；将不同工艺参数下的样件质量标注，利用k-近邻算法，在“低质量件到相似高质量件”的邻域映射中推断应调整的工艺参数（加热功率、加热时间、真空时间）。

Result: 所建模型能有效给出参数调整建议，在测试中成功降低缺陷发生率，并对三类关键参数（加热功率、加热时间、真空时间）给出合理调整，体现出良好的生产效率提升与质量改善效果。

Conclusion: 基于视觉+少样本的kNN参数推荐框架可在真空热成型中实现有效的质量控制与工艺优化，减少对大量数据与复杂模型的依赖，实用性强；未来可扩展到更多参数与更复杂的成型条件。

Abstract: Ensuring consistent quality in vacuum thermoforming presents challenges due
to variations in material properties and tooling configurations. This research
introduces a vision-based quality control system to predict and optimise
process parameters, thereby enhancing part quality with minimal data
requirements. A comprehensive dataset was developed using visual data from
vacuum-formed samples subjected to various process parameters, supplemented by
image augmentation techniques to improve model training. A k-Nearest Neighbour
algorithm was subsequently employed to identify adjustments needed in process
parameters by mapping low-quality parts to their high-quality counterparts. The
model exhibited strong performance in adjusting heating power, heating time,
and vacuum time to reduce defects and improve production efficiency.

</details>


### [104] [ResidualViT for Efficient Temporally Dense Video Encoding](https://arxiv.org/abs/2509.13255)
*Mattia Soldan,Fabian Caba Heilbron,Bernard Ghanem,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 提出ResidualViT与蒸馏方案，在高时间分辨率视频任务上以近似原始大模型精度的同时，将特征计算成本降至最多60%，推理加速至2.5倍。


<details>
  <summary>Details</summary>
Motivation: 高时间分辨率视频任务需要逐帧（或密集时间）特征，计算代价高。视频存在大量时间冗余，可被利用以降低计算而不显著损伤精度。

Method: 1) 设计ResidualViT：在ViT中加入可学习残差连接以维持相邻帧时间一致性；引入token reduction模块，选择性丢弃时间冗余token，并复用预训练基础模型权重。2) 轻量蒸馏：用原始基础模型的帧级特征作为教师，训练学生近似这些特征。3) 在零样本与全监督设定、多任务多数据集上评估。

Result: 在4个任务、5个数据集上，计算成本最高降低约60%，推理速度最高提升约2.5倍，同时准确率与原始基础模型接近。

Conclusion: 通过ResidualViT与轻量蒸馏，能有效利用视频时间冗余，实现密集时间特征的高效计算，在保持精度的同时显著降低成本与提升速度。

Abstract: Several video understanding tasks, such as natural language temporal video
grounding, temporal activity localization, and audio description generation,
require "temporally dense" reasoning over frames sampled at high temporal
resolution. However, computing frame-level features for these tasks is
computationally expensive given the temporal resolution requirements. In this
paper, we make three contributions to reduce the cost of computing features for
temporally dense tasks. First, we introduce a vision transformer (ViT)
architecture, dubbed ResidualViT, that leverages the large temporal redundancy
in videos to efficiently compute temporally dense frame-level features. Our
architecture incorporates (i) learnable residual connections that ensure
temporal consistency across consecutive frames and (ii) a token reduction
module that enhances processing speed by selectively discarding temporally
redundant information while reusing weights of a pretrained foundation model.
Second, we propose a lightweight distillation strategy to approximate the
frame-level features of the original foundation model. Finally, we evaluate our
approach across four tasks and five datasets, in both zero-shot and fully
supervised settings, demonstrating significant reductions in computational cost
(up to 60%) and improvements in inference speed (up to 2.5x faster), all while
closely approximating the accuracy of the original foundation model.

</details>


### [105] [RadGame: An AI-Powered Platform for Radiology Education](https://arxiv.org/abs/2509.13270)
*Mohammed Baharoon,Siavash Raissi,John S. Jun,Thibault Heintz,Mahmoud Alabbad,Ali Alburkani,Sung Eun Kim,Kent Kleinschmidt,Abdulrahman O. Alhumaydhi,Mohannad Mohammed G. Alghamdi,Jeremy Francis Palacio,Mohammed Bukhaytan,Noah Michael Prudlo,Rithvik Akula,Brady Chrisler,Benjamin Galligos,Mohammed O. Almutairi,Mazeen Mohammed Alanazi,Nasser M. Alrashdi,Joel Jihwan Hwang,Sri Sai Dinesh Jaliparthi,Luke David Nelson,Nathaniel Nguyen,Sathvik Suryadevara,Steven Kim,Mohammed F. Mohammed,Yevgeniy R. Semenov,Kun-Hsing Yu,Abdulrhman Aljouie,Hassan AlOmaish,Adam Rodman,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: RadGame 是一个将放射学教育游戏化的AI平台，面向两项核心技能：病灶定位与报告撰写；借助公共数据集标注与自动化反馈，显著提升学习者的定位与报告准确度。


<details>
  <summary>Details</summary>
Motivation: 传统放射科训练依赖被动看例或导师实时指导，难以提供即时、规模化、结构化反馈；需要一种可扩展且反馈丰富的训练方式来提升定位与报告能力。

Method: 构建两种游戏化模块：1) Localize：学员在影像上画框定位异常，与公共数据集中放射科医师标注自动比对，并由视觉-语言模型生成对遗漏病灶的可视化解释；2) Report：学员基于胸片、年龄与适应证撰写“所见/所见+结论”，利用基于报告生成指标的AI对比公共数据集的放射科真实报告，指出错误与遗漏，给出表现与风格评分。

Result: 前瞻性评估中，使用 RadGame 的参与者在定位准确度提升68%（对照：被动方法17%），报告撰写准确度提升31%（对照：4%），在相同病例暴露下表现显著更优。

Conclusion: AI驱动的游戏化平台可在大规模条件下提供结构化、即时反馈，显著提升放射学定位与报告技能，展示了将医学AI资源用于教育的新范式。

Abstract: We introduce RadGame, an AI-powered gamified platform for radiology education
that targets two core skills: localizing findings and generating reports.
Traditional radiology training is based on passive exposure to cases or active
practice with real-time input from supervising radiologists, limiting
opportunities for immediate and scalable feedback. RadGame addresses this gap
by combining gamification with large-scale public datasets and automated,
AI-driven feedback that provides clear, structured guidance to human learners.
In RadGame Localize, players draw bounding boxes around abnormalities, which
are automatically compared to radiologist-drawn annotations from public
datasets, and visual explanations are generated by vision-language models for
user missed findings. In RadGame Report, players compose findings given a chest
X-ray, patient age and indication, and receive structured AI feedback based on
radiology report generation metrics, highlighting errors and omissions compared
to a radiologist's written ground truth report from public datasets, producing
a final performance and style score. In a prospective evaluation, participants
using RadGame achieved a 68% improvement in localization accuracy compared to
17% with traditional passive methods and a 31% improvement in report-writing
accuracy compared to 4% with traditional methods after seeing the same cases.
RadGame highlights the potential of AI-driven gamification to deliver scalable,
feedback-rich radiology training and reimagines the application of medical AI
resources in education.

</details>


### [106] [Image Realness Assessment and Localization with Multimodal Features](https://arxiv.org/abs/2509.13289)
*Lovish Kaushik,Agnij Biswas,Somdyuti Paul*

Main category: cs.CV

TL;DR: 提出一个多模态框架，用大规模训练的视觉-语言模型生成的文本不一致描述，来同时做整体“真实感”客观评估与局部不一致区域定位，并输出稠密真实度图。结果显示相较以往方法，真实感预测更准、区域区分更清晰。


<details>
  <summary>Details</summary>
Motivation: AI生成图像在实用中需要可量化的“真实感”指标，并能找出图像中不真实的局部，以便筛查与用于训练时的反馈改进。目前人工标注成本高、主观性强，缺少可扩展、客观、细粒度的评估方法。

Method: 构建一个多模态评估框架：利用在大数据集上训练的视觉-语言模型（VLM）自动生成关于图像视觉不一致性的文本描述，作为替代人工标注的监督/提示；据此进行两项任务：1) 整体客观真实感评分预测；2) 通过将文本不一致描述与图像特征对齐，生成稠密的“真实度地图”，定位不真实区域。

Result: 该方法提升了客观真实感预测的性能，并能输出有效区分真实与不真实空间区域的稠密真实度图；相较基线在整体评分与局部检测上均有优势。

Conclusion: 以VLM生成的文本不一致描述可作为可靠的替代标注，支撑同时进行整体真实感评估与局部不一致识别。该多模态框架提高了预测准确性，并提供可解释的空间诊断，对生成模型训练的真实感反馈与实际应用具有价值。

Abstract: A reliable method of quantifying the perceptual realness of AI-generated
images and identifying visually inconsistent regions is crucial for practical
use of AI-generated images and for improving photorealism of generative AI via
realness feedback during training. This paper introduces a framework that
accomplishes both overall objective realness assessment and local inconsistency
identification of AI-generated images using textual descriptions of visual
inconsistencies generated by vision-language models trained on large datasets
that serve as reliable substitutes for human annotations. Our results
demonstrate that the proposed multimodal approach improves objective realness
prediction performance and produces dense realness maps that effectively
distinguish between realistic and unrealistic spatial regions.

</details>


### [107] [StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance](https://arxiv.org/abs/2509.13301)
*Zefan Qu,Zhenwei Wang,Haoyuan Wang,Ke Xu,Gerhard Hancke,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: StyleSculptor是一种零样本、无需训练的3D风格可控生成方法，从一张内容图和一张或多张风格图生成匹配纹理与几何风格的3D资产；其核心是风格解耦注意力（SD-Attn）与风格引导控制（SGC），实现纹理/几何/二者联合的细粒度可控与强度调节，并在实验中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 实际应用（游戏、VR）常需与现有资产风格一致的3D模型。现有文本/图像到3D生成虽进步显著，但缺乏对风格（特别是纹理与几何两方面）的精细、可控与零样本适配能力，且易出现语义泄露与风格-内容混淆。

Method: 提出StyleSculptor：一种训练免（training-free）、零样本的风格引导3D生成框架。核心模块SD-Attn通过跨3D注意力在内容与风格特征间进行动态交互与稳定融合，并结合基于3D特征patch方差的风格解耦特征选择，区分风格/内容显著通道，进行选择性注入；可分别计算纹理导向、几何导向或二者联合的特征。基于此设计SGC机制，支持仅几何或仅纹理风格化，以及连续的风格强度调节。

Result: 在广泛实验中，StyleSculptor生成的3D资产在保真度与风格一致性上优于现有基线，并实现稳定的纹理/几何解耦控制与强度可调的风格化。

Conclusion: StyleSculptor实现了无需训练的零样本3D风格可控生成，通过SD-Attn与SGC达到细粒度控制、减少语义泄露并提升生成质量，适用于实际3D内容生产场景。

Abstract: Creating 3D assets that follow the texture and geometry style of existing
ones is often desirable or even inevitable in practical applications like video
gaming and virtual reality. While impressive progress has been made in
generating 3D objects from text or images, creating style-controllable 3D
assets remains a complex and challenging problem. In this work, we propose
StyleSculptor, a novel training-free approach for generating style-guided 3D
assets from a content image and one or more style images. Unlike previous
works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,
enabling fine-grained 3D style control that captures the texture, geometry, or
both styles of user-provided style images. At the core of StyleSculptor is a
novel Style Disentangled Attention (SD-Attn) module, which establishes a
dynamic interaction between the input content image and style image for
style-guided 3D asset generation via a cross-3D attention mechanism, enabling
stable feature fusion and effective style-guided generation. To alleviate
semantic content leakage, we also introduce a style-disentangled feature
selection strategy within the SD-Attn module, which leverages the variance of
3D feature patches to disentangle style- and content-significant channels,
allowing selective feature injection within the attention framework. With
SD-Attn, the network can dynamically compute texture-, geometry-, or
both-guided features to steer the 3D generation process. Built upon this, we
further propose the Style Guided Control (SGC) mechanism, which enables
exclusive geometry- or texture-only stylization, as well as adjustable style
intensity control. Extensive experiments demonstrate that StyleSculptor
outperforms existing baseline methods in producing high-fidelity 3D assets.

</details>


### [108] [3D Aware Region Prompted Vision Language Model](https://arxiv.org/abs/2509.13317)
*An-Chieh Cheng,Yang Fu,Yukang Chen,Zhijian Liu,Xiaolong Li,Subhashree Radhakrishnan,Song Han,Yao Lu,Jan Kautz,Pavlo Molchanov,Hongxu Yin,Xiaolong Wang,Sifei Liu*

Main category: cs.CV

TL;DR: 提出SR-3D，一种共享视觉token空间的2D-3D感知视觉语言模型，支持跨帧/跨模态的灵活区域提示，通过为2D特征注入3D位置嵌入，实现不依赖穷尽标注的精确空间推理，并在2D与3D基准及无传感器3D的视频上达到SOTA与良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VL模型难以统一单视角2D图像与多视角/三维数据，导致空间推理弱、跨帧对象不共现时难以定位；同时区域级交互常需多帧繁琐标注，缺乏在真实视频中无3D标注场景的可用性。

Method: 构建共享视觉token空间，将2D视觉特征通过3D位置嵌入进行增强，使3D模型可利用强2D先验进行空间推理；设计可灵活的区域提示接口（任意帧上的框/分割或直接在3D中标注），避免多帧穷尽标注；在泛2D VL与3D空间任务上训练/评估。

Result: 在通用2D视觉语言和专门的3D空间基准上取得SOTA；在缺少传感器3D或真值3D标注的野外视频中，模型仍能准确推断空间关系与度量。

Conclusion: SR-3D有效统一了2D与3D表征空间，显著提升跨帧与跨模态的空间理解与区域级交互能力，具有对真实世界视频的实用性与泛化潜力。

Abstract: We present Spatial Region 3D (SR-3D) aware vision-language model that
connects single-view 2D images and multi-view 3D data through a shared visual
token space. SR-3D supports flexible region prompting, allowing users to
annotate regions with bounding boxes, segmentation masks on any frame, or
directly in 3D, without the need for exhaustive multi-frame labeling. We
achieve this by enriching 2D visual features with 3D positional embeddings,
which allows the 3D model to draw upon strong 2D priors for more accurate
spatial reasoning across frames, even when objects of interest do not co-occur
within the same view. Extensive experiments on both general 2D vision language
and specialized 3D spatial benchmarks demonstrate that SR-3D achieves
state-of-the-art performance, underscoring its effectiveness for unifying 2D
and 3D representation space on scene understanding. Moreover, we observe
applicability to in-the-wild videos without sensory 3D inputs or ground-truth
3D annotations, where SR-3D accurately infers spatial relationships and metric
measurements.

</details>
