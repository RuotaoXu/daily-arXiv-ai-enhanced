<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 171]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes](https://arxiv.org/abs/2512.17939)
*Yuncheng Lu,Yucen Shi,Aobo Li,Zehao Li,Junying Li,Bo Wang,Tony Tae-Hyoung Kim*

Main category: cs.CV

TL;DR: 提出一种将帧式与事件驱动跟踪融合的节能反无人机芯片系统，通过自适应模式切换与零跳过神经计算，实现小型高速无人机的高鲁棒检测与超高能效。


<details>
  <summary>Details</summary>
Motivation: 小型、快速运动的无人机在传统帧式视觉中易因运动模糊、低信噪而漏检；而纯事件相机方案在慢速、静态背景下效率高但易丢失外观信息。需要一种能在不同速度与尺度下稳健运行、且端侧能效高的反无人机感知与识别系统。

Method: 1) 事件数据用游程编码重构二值事件帧并生成候选区域；2) 根据目标尺寸与速度在帧模式与事件模式间自适应切换；3) 设计快速目标跟踪单元(FOTU)，通过自适应阈值与基于轨迹的分类提升高速目标鲁棒性；4) 神经处理单元支持灰度小块与轨迹双推理，采用定制指令与零跳过MAC以去除>97%冗余计算；5) 40nm实现，2mm^2芯片。

Result: 在0.8V下能耗为96 pJ/帧/像素与61 pJ/事件；在公共UAV数据集上50–400 m、5–80 px/s条件下识别准确率98.2%；神经计算冗余减少>97%，实现端到端能效达业界领先。

Conclusion: 融合帧/事件的自适应跟踪与高效NPU架构能在广速度与距离范围内稳健识别小型高速无人机，并以极高能效实现SOTA的反无人机端侧感知系统。

Abstract: We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.

</details>


### [2] [NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction](https://arxiv.org/abs/2512.17943)
*Karthik Prabhakar*

Main category: cs.CV

TL;DR: 提出NystagmusNet：预测眩光/亮度引发的眼震光敏风险，并实时给出视觉适配建议；双分支CNN用合成与增强数据训练，融合环境亮度与眼动方差，验证集（合成数据）准确率75%，并用SHAP与Grad-CAM提供可解释性，结合规则引擎推荐滤光策略，展望智能眼镜部署与强化学习个性化。


<details>
  <summary>Details</summary>
Motivation: 现有对光敏性眼震患者的辅助主要是事后缓解，缺乏对高风险环境的前瞻性预测与个体化适配，影响日常生活安全与舒适；需要一个可预测、可解释、可实时干预的系统。

Method: 构建NystagmusNet：双分支卷积网络分别处理环境亮度信息与眼动（方差）信号，基于合成与数据增强的训练集学习预测光敏风险评分；集成SHAP与Grad-CAM提供特征与区域级解释；外接基于规则的推荐引擎，按风险评分输出实时滤光/适配建议。

Result: 在合成数据验证集上达到75%准确率；可解释性模块能标注环境高风险区域，提升临床信任与可解释性；系统可生成实时滤光建议（未给出真实世界或临床统计）。

Conclusion: NystagmusNet展示了以AI预测光敏风险并给出适配建议的可行性与可解释性，但目前主要基于合成数据、准确率有限、缺少真实世界验证；未来计划在智能眼镜端部署，并通过强化学习实现个性化优化。

Abstract: Nystagmus patients with photosensitivity face significant daily challenges due to involuntary eye movements exacerbated by environmental brightness conditions. Current assistive solutions are limited to symptomatic treatments without predictive personalization. This paper proposes NystagmusNet, an AI-driven system that predicts high-risk visual environments and recommends real-time visual adaptations. Using a dual-branch convolutional neural network trained on synthetic and augmented datasets, the system estimates a photosensitivity risk score based on environmental brightness and eye movement variance. The model achieves 75% validation accuracy on synthetic data. Explainability techniques including SHAP and GradCAM are integrated to highlight environmental risk zones, improving clinical trust and model interpretability. The system includes a rule-based recommendation engine for adaptive filter suggestions. Future directions include deployment via smart glasses and reinforcement learning for personalized recommendations.

</details>


### [3] [SuperFlow: Training Flow Matching Models with RL on the Fly](https://arxiv.org/abs/2512.17951)
*Kaijie Chen,Zhiyang Xu,Ying Shen,Zihao Lin,Yuguang Yao,Lifu Huang*

Main category: cs.CV

TL;DR: SuperFlow 是一种用于流式生成模型的强化学习训练框架，通过方差感知采样自适应地分配每个提示的采样组大小，并以与连续时间流一致的方式计算逐步优势，从而提升训练效率与对齐质量。实验显示：在无需改动模型结构的情况下，SuperFlow 仅用 5.4%–56.3% 的训练步数即可达到或超过 baseline，并缩短 5.2%–16.7% 的训练时间；在多项 T2I 任务上，相比 SD3.5-M 提升 4.6%–47.2%，相比 Flow-GRPO 提升 1.7%–16.0%。


<details>
  <summary>Details</summary>
Motivation: 现有针对流式生成模型的 RL 训练面临两大问题：1) GRPO 固定的每提示组大小忽略了不同提示在采样重要性上的差异，造成采样低效与训练变慢；2) 将轨迹级优势重复用作逐步估计，违背了沿流的因果信号分配，导致信用分配偏差。为提高样本效率、加速训练并实现更准确的信用分配，需要新的 RL 训练方法。

Method: 提出 SuperFlow：- 方差感知采样：根据不同提示的回报方差和重要性自适应调整 per-prompt 的组大小，避免一刀切的固定采样数。- 连续时间一致的逐步优势估计：基于流模型的连续时间（或离散多步近似）动态，构造与时间演化一致的 step-level advantage，而非复用轨迹级优势。- 框架对现有流式架构无侵入，可直接替换训练策略。

Result: 在无需改动模型结构的前提下：- 训练效率：达到既定性能仅需 5.4%–56.3% 的原始训练步；总体训练时间降低 5.2%–16.7%。- 任务表现：在文本渲染、组合生成与人类偏好对齐等 T2I 基准上，相比 SD3.5-M 提升 4.6%–47.2%，相比 Flow-GRPO 提升 1.7%–16.0%。

Conclusion: 通过自适应采样与连续时间一致的逐步优势估计，SuperFlow 解决了流式模型 RL 中的采样低效与信用分配偏差问题，在不改动模型结构的情况下显著提升训练效率与多项 T2I 任务的对齐与质量表现。

Abstract: Recent progress in flow-based generative models and reinforcement learning (RL) has improved text-image alignment and visual quality. However, current RL training for flow models still has two main problems: (i) GRPO-style fixed per-prompt group sizes ignore variation in sampling importance across prompts, which leads to inefficient sampling and slower training; and (ii) trajectory-level advantages are reused as per-step estimates, which biases credit assignment along the flow. We propose SuperFlow, an RL training framework for flow-based models that adjusts group sizes with variance-aware sampling and computes step-level advantages in a way that is consistent with continuous-time flow dynamics. Empirically, SuperFlow reaches promising performance while using only 5.4% to 56.3% of the original training steps and reduces training time by 5.2% to 16.7% without any architectural changes. On standard text-to-image (T2I) tasks, including text rendering, compositional image generation, and human preference alignment, SuperFlow improves over SD3.5-M by 4.6% to 47.2%, and over Flow-GRPO by 1.7% to 16.0%.

</details>


### [4] [Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition](https://arxiv.org/abs/2512.17953)
*Ellie Zhou,Jihoon Chung,Olga Russakovsky*

Main category: cs.CV

TL;DR: 论文系统分析了人类动作识别中“背景偏置”的普遍性，并提出在分类模型与视频大语言模型中分别通过输入分割与提示词调优来缓解偏置，取得了量化提升。


<details>
  <summary>Details</summary>
Motivation: 动作识别模型常错误依赖场景/背景线索而非人的姿态与运动，导致可迁移性差与鲁棒性不足。作者希望量化这种偏置在不同模型范式（分类、对比预训练、VLLM）中的程度，并探索通用、低成本的缓解方法。

Method: 1) 基准与诊断：构造或采用包含背景与人类主体分离/控制的评测设置，跨分类模型、对比文本-图像预训练模型、以及视频大语言模型评估“背景 vs 人体”线索的使用倾向。2) 缓解（分类模型）：引入“分割后的人体输入”（如人体剪影/Mask裁剪）融入训练或推理管线。3) 缓解（VLLM）：进行手工与自动化提示词调优，引导模型在推理时聚焦人体动作与姿态。

Result: - 发现三类模型均显著倾向依据背景进行预测（强背景依赖）。- 在分类模型中，加入分割的人体输入可将背景偏置降低约3.78%。- 在VLLM中，通过提示词设计与调优，使模型更偏向以人体为中心的推理，偏置降低约9.85%。

Conclusion: 背景偏置在多种范式的动作理解模型中普遍存在。简单可行的干预（如人体分割输入、提示词调优）即可带来可量化的偏置缓解；提示工程对VLLM尤其有效。建议未来工作在训练与评测中显式去偏，构建更关注人体动态的模型与数据。

Abstract: Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.

</details>


### [5] [SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries](https://arxiv.org/abs/2512.17954)
*Bin Wang,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出SCS-SupCon：以Sigmoid对比损失+可学习温度/偏置与显式风格距离约束，缓解负样本稀释并自适应决策边界，在多数据集与多骨干上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 细粒度识别中类间差异微小、类内差异大，传统InfoNCE型监督对比学习受负样本稀释影响、缺乏自适应决策边界，难以充分利用监督信号与区分难样本。

Method: 1) 将对比损失从softmax/InfoNCE改为Sigmoid基的成对损失，引入可学习温度与偏置，提升对难负样本的权重并自适应边界；2) 加入显式风格-内容解耦的“风格距离”约束，促进风格与语义内容分离，增强表示鲁棒性；3) 在CNN与Transformer骨干上统一训练评估。

Result: 在6个基准上达SOTA：如CIFAR-100+ResNet-50下，较SupCon提升约+3.9个百分点、较CS-SupCon提升约+1.7（五折）；在CUB200、Stanford Dogs等细粒度数据集上较CS-SupCon提升0.4–3.0；消融与统计检验（Friedman与Nemenyi）验证改进稳定与泛化性。

Conclusion: Sigmoid成对监督对比损失与风格距离约束能缓解负样本稀释、实现自适应决策边界并解耦风格/内容，从而在多数据集与架构下稳定提升细粒度分类性能。

Abstract: Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.

</details>


### [6] [A Modular Framework for Single-View 3D Reconstruction of Indoor Environments](https://arxiv.org/abs/2512.17955)
*Yuxiao Li*

Main category: cs.CV

TL;DR: 提出一个模块化、基于扩散模型驱动的单视角室内场景三维重建框架：先做可见/不可见内容的图像级补全，再转换为3D，并通过深度融合与视域对齐实现精准放置，优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统单视角室内重建直接从不完整的2D图像回归3D，面对遮挡与复杂实例形状时易失真、细节缺失与布局不准。需要一种能先“补全可见信息与遮挡内容”，再稳健恢复三维的流程。

Method: 采用模块化两阶段：1) 扩散模型进行图像层面的补全与生成，包括（a）实例的amodal补全模块恢复被遮挡实例的完整外观，（b）专门训练的房间布局修复/修补（inpainting）模型，（c）混合深度估计策略在全局几何与细节表达间权衡。2) 视域空间对齐，联合2D与3D线索精确定位与摆放实例；随后将补全结果转换为3D重建。

Result: 在3D-Front数据集上进行大量实验，定性与定量均超过当前SOTA，在视觉质量与重建精度上更优，且能够同时重建前景实例与房间背景。

Conclusion: 将扩散驱动的图像级补全与模块化的几何重建管线相结合，缓解了单视角室内重建中的遮挡与形状复杂性问题，具备室内设计、房地产与AR等应用潜力。

Abstract: We propose a modular framework for single-view indoor scene 3D reconstruction, where several core modules are powered by diffusion techniques. Traditional approaches for this task often struggle with the complex instance shapes and occlusions inherent in indoor environments. They frequently overshoot by attempting to predict 3D shapes directly from incomplete 2D images, which results in limited reconstruction quality. We aim to overcome this limitation by splitting the process into two steps: first, we employ diffusion-based techniques to predict the complete views of the room background and occluded indoor instances, then transform them into 3D. Our modular framework makes contributions to this field through the following components: an amodal completion module for restoring the full view of occluded instances, an inpainting model specifically trained to predict room layouts, a hybrid depth estimation technique that balances overall geometric accuracy with fine detail expressiveness, and a view-space alignment method that exploits both 2D and 3D cues to ensure precise placement of instances within the scene. This approach effectively reconstructs both foreground instances and the room background from a single image. Extensive experiments on the 3D-Front dataset demonstrate that our method outperforms current state-of-the-art (SOTA) approaches in terms of both visual quality and reconstruction accuracy. The framework holds promising potential for applications in interior design, real estate, and augmented reality.

</details>


### [7] [Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization](https://arxiv.org/abs/2512.17987)
*Omar Faruq Shikdar,Fahad Ahammed,B. M. Shahria Alam,Golam Kibria,Tawhidur Rahman,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 构建茶叶病害自动分类系统：自建7类5278图像数据集，结合DenseNet/Incpetion与注意力，并在集成模型中加入EfficientNet与可解释性，最佳准确率85.68%。


<details>
  <summary>Details</summary>
Motivation: 茶叶病害若不及时识别会造成巨大经济损失，人工识别低效且不稳定，需开发自动化、准确且可解释的病害识别方法帮助农户早期干预、降低损失。

Method: 构建并预处理一个包含5278张、7个类别的茶叶病害数据集；采用预训练DenseNet与Inception作为基模型，引入两种注意力模块以增强特征表达；在集成模型中额外引入EfficientNet进行融合；使用可解释性AI方法对模型决策进行可视化与解释。

Result: 集成模型（含EfficientNet与注意力）取得最高分类准确率85.68%，优于单模型配置；可解释性结果展示模型关注的病斑区域，提高了可理解性。

Conclusion: 提出的基于注意力与集成学习的自动茶叶病害分类系统在自建数据集上取得较好效果（85.68%），具备实际应用潜力；可解释性增强了可信度，但仍有改进空间（如更大数据集与更高精度）。

Abstract: Tea is among the most widely consumed drinks globally. Tea production is a key industry for many countries. One of the main challenges in tea harvesting is tea leaf diseases. If the spread of tea leaf diseases is not stopped in time, it can lead to massive economic losses for farmers. Therefore, it is crucial to identify tea leaf diseases as soon as possible. Manually identifying tea leaf disease is an ineffective and time-consuming method, without any guarantee of success. Automating this process will improve both the efficiency and the success rate of identifying tea leaf diseases. The purpose of this study is to create an automated system that can classify different kinds of tea leaf diseases, allowing farmers to take action to minimize the damage. A novel dataset was developed specifically for this study. The dataset contains 5278 images across seven classes. The dataset was pre-processed prior to training the model. We deployed three pretrained models: DenseNet, Inception, and EfficientNet. EfficientNet was used only in the ensemble model. We utilized two different attention modules to improve model performance. The ensemble model achieved the highest accuracy of 85.68%. Explainable AI was introduced for better model interpretability.

</details>


### [8] [Name That Part: 3D Part Segmentation and Naming](https://arxiv.org/abs/2512.18003)
*Soumava Paul,Prakhar Kaushik,Ankit Vaidya,Anand Bhattad,Alan Yuille*

Main category: cs.CV

TL;DR: 提出ALIGN-Parts：将3D零件命名视为集合对齐问题，用“partlets”与文本描述做双向匹配，实现一站式、开放词表的3D零件分割与命名，并跨数据集统一本体。


<details>
  <summary>Details</summary>
Motivation: 现有3D部件数据集的部件定义不一致，难以统一训练；过去方法要么只做无标签分解，要么仅检索单一部件，缺乏完整、可命名的3D零件标注能力。

Method: 将形状分解为隐式3D部件表示“partlets”，通过二分图分配与文本部件描述进行集合级对齐；融合3D几何部件场、来自多视角的外观特征与由语言模型生成的可供性描述；用文本对齐损失把partlets与文本嵌入到同一空间，实现开放词表匹配；一站式（one-shot）推理并输出命名与置信度。

Result: 模型可零样本匹配任意描述，对已知类别产生置信度校准预测；作为可扩展的标注引擎，在人工校验下将PartNet、3DCoMPaT++、Find3D对齐成包含1,794个独特3D零件的统一本体，并展示了新建Tex-Parts数据集样例；提出了两个适用于命名3D部件分割的新指标。

Conclusion: ALIGN-Parts实现了开放词表、可命名的3D部件分割，能统一跨数据集的部件语义并支持下游任务与大规模标注；新指标与数据资源为社区提供了评测与扩展基础。

Abstract: We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.

</details>


### [9] [Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models](https://arxiv.org/abs/2512.18004)
*Shubham Kumar Nigam,Parjanya Aditya Shukla,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CV

TL;DR: 比较两类方法：传统“手写OCR→机器翻译”流水线 vs. 视觉大语言模型端到端直接从手写图像翻译，应用于低资源的马拉地语法律文书（FIR、起诉书、证词），在自建数据集上评测以便在边端部署，提升法律信息可及性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（马拉地语）手写文本缺少大规模数字化语料且书写体差异大，现有系统难以在法律场景（法院文书）中稳定工作。为实现可扩展、准确、可边端部署的翻译以服务非母语使用者与法律专业人士，需要评估传统两阶段方案与端到端视觉LLM方案。

Method: 构建和整理一套马拉地语手写法律文档数据集；实现并比较两种路线：1) OCR识别手写→MT翻译的两阶段流水线；2) 使用视觉大语言模型，将手写图像直接端到端翻译到目标语言。对两类方法进行实验评测与对比分析。

Result: 在该数据集上给出两类方法的性能对比，揭示端到端视觉LLM与传统OCR+MT在低资源、手写多样性与部署约束等条件下的优劣与权衡，提供可操作的经验结论。

Conclusion: 端到端视觉LLM有望简化流程并在某些条件下提升鲁棒性，但传统OCR+MT仍具可控性与组件可替换优势；论文提出的评测与经验为在资源受限场景下构建稳健、可边端部署的法律文书翻译系统提供指导。

Abstract: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.

</details>


### [10] [NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging](https://arxiv.org/abs/2512.18038)
*Fakrul Islam Tushar,Ehsan Samei,Cynthia Rudin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: NodMAISI 提出一个面向肺结节的解剖约束CT合成与增强框架，可生成与器官结构和结节一致的合成CT，并进行病灶感知的数据增强；在多数据集上较基线显著提升分布拟真度、结节可检出性，及在小样本下的恶性分类AUC。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像数据虽多，但对肺癌筛查关键的小结节等异常样本不足且标注不一致，导致生成模型难以忠实再现病灶、下游检测/分类在数据稀缺时表现受限。

Method: 构建统一多源队列并标准化标注（器官掩膜+结节级标注）；基于MAISI-v2模块并引入ControlNet条件与rectified-flow的生成器，确保解剖与病灶一致的CT合成；设计病灶感知增强，对结节掩膜进行可控收缩扰动，同时保持周围解剖不变，生成成对CT变体。

Result: 在六个公开测试集上，真实-合成FID优于MAISI-v2（1.18–2.99 vs 1.69–5.21）；用MONAI结节检测器评估，可检出性更高、更接近临床（如IMD-CT敏感度0.69 vs 0.39；DLCS24为0.63 vs 0.20），尤其对亚厘米结节复现更佳；在LUNA25训练、LUNA16/LNDbv4/DLCS24外测的结节恶性分类中，使用该增强在≤20%临床数据时AUC提升0.07–0.11，在10%数据时提升0.12–0.21。

Conclusion: NodMAISI通过解剖约束的条件生成与病灶感知增强，显著改善小结节的合成质量与可检出性，并在数据匮乏情境下有效提升下游恶性分类性能，缩小与充分数据训练之间的差距。

Abstract: Objective: Although medical imaging datasets are increasingly available, abnormal and annotation-intensive findings critical to lung cancer screening, particularly small pulmonary nodules, remain underrepresented and inconsistently curated. Methods: We introduce NodMAISI, an anatomically constrained, nodule-oriented CT synthesis and augmentation framework trained on a unified multi-source cohort (7,042 patients, 8,841 CTs, 14,444 nodules). The framework integrates: (i) a standardized curation and annotation pipeline linking each CT with organ masks and nodule-level annotations, (ii) a ControlNet-conditioned rectified-flow generator built on MAISI-v2's foundational blocks to enforce anatomy- and lesion-consistent synthesis, and (iii) lesion-aware augmentation that perturbs nodule masks (controlled shrinkage) while preserving surrounding anatomy to generate paired CT variants. Results: Across six public test datasets, NodMAISI improved distributional fidelity relative to MAISI-v2 (real-to-synthetic FID range 1.18 to 2.99 vs 1.69 to 5.21). In lesion detectability analysis using a MONAI nodule detector, NodMAISI substantially increased average sensitivity and more closely matched clinical scans (IMD-CT: 0.69 vs 0.39; DLCS24: 0.63 vs 0.20), with the largest gains for sub-centimeter nodules where MAISI-v2 frequently failed to reproduce the conditioned lesion. In downstream nodule-level malignancy classification trained on LUNA25 and externally evaluated on LUNA16, LNDbv4, and DLCS24, NodMAISI augmentation improved AUC by 0.07 to 0.11 at <=20% clinical data and by 0.12 to 0.21 at 10%, consistently narrowing the performance gap under data scarcity.

</details>


### [11] [YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs](https://arxiv.org/abs/2512.18046)
*Ami Pandat,Punna Rajasekhar,Gopika Vinod,Rohit Shukla*

Main category: cs.CV

TL;DR: 提出YoloVN-CBi（在YoloV系上融入CBAM与BiFPN），并结合知识蒸馏，显著提升小型无人机实时检测的精度-速度折中；蒸馏后模型在保持较高mAP的同时大幅提速，优于多款新版本YOLO在小目标场景的表现。


<details>
  <summary>Details</summary>
Motivation: 小型无人机在民用与国防中风险上升，现有检测因目标小、运动快、对比度低而困难；需要在资源受限的边缘设备上实现高效、实时且对小目标敏感的检测。

Method: 在YoloV基线中加入CBAM注意力与BiFPN特征融合形成“CBi”架构；构建28K多类飞行目标训练集与含2500张极小无人机的本地测试集；在四个基准数据集与本地集上评估。设计四个CBi变体（不同CBAM与BiFPN放置/用法），并进行知识蒸馏（Yolov5m-CBi为教师、Yolov5n-CBi为学生）以满足边缘部署。

Result: Yolov5与Yolov5-CBi在小目标检测的速度-精度折中上优于Yolov8与Yolov12；蒸馏学生模型mAP@0.5:0.9=0.6573，相比教师0.6171提升6.51%；相对基线推理速度提升82.9%，更适合实时无人机检测。

Conclusion: 将CBAM和BiFPN融合进YOLO并结合知识蒸馏，可在不牺牲实时性的情况下显著提升小无人机检测性能；轻量蒸馏模型在边缘场景更具实用价值，优于多款新YOLO版本在小目标任务中的表现。

Abstract: Unmanned Aerial Vehicles, commonly known as, drones pose increasing risks in civilian and defense settings, demanding accurate and real-time drone detection systems. However, detecting drones is challenging because of their small size, rapid movement, and low visual contrast. A modified architecture of YolovN called the YolovN-CBi is proposed that incorporates the Convolutional Block Attention Module (CBAM) and the Bidirectional Feature Pyramid Network (BiFPN) to improve sensitivity to small object detections. A curated training dataset consisting of 28K images is created with various flying objects and a local test dataset is collected with 2500 images consisting of very small drone objects. The proposed architecture is evaluated on four benchmark datasets, along with the local test dataset. The baseline Yolov5 and the proposed Yolov5-CBi architecture outperform newer Yolo versions, including Yolov8 and Yolov12, in the speed-accuracy trade-off for small object detection. Four other variants of the proposed CBi architecture are also proposed and evaluated, which vary in the placement and usage of CBAM and BiFPN. These variants are further distilled using knowledge distillation techniques for edge deployment, using a Yolov5m-CBi teacher and a Yolov5n-CBi student. The distilled model achieved a mA@P0.5:0.9 of 0.6573, representing a 6.51% improvement over the teacher's score of 0.6171, highlighting the effectiveness of the distillation process. The distilled model is 82.9% faster than the baseline model, making it more suitable for real-time drone detection. These findings highlight the effectiveness of the proposed CBi architecture, together with the distilled lightweight models in advancing efficient and accurate real-time detection of small UAVs.

</details>


### [12] [FOODER: Real-time Facial Authentication and Expression Recognition](https://arxiv.org/abs/2512.18057)
*Sabri Mustafa Kahya,Muhammet Sami Yavuz,Boran Hamdi Sivrikaya,Eckehard Steinbach*

Main category: cs.CV

TL;DR: FOODER：用低成本FMCW雷达做实时、隐私友好的面部OOD认证与表情识别，分层架构在认证通过后再做表情分类，达成高AUROC和高准确率并优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统视觉人脸认证/表情识别存在隐私风险、光照遮挡敏感与跨域泛化差；需在只用雷达数据的前提下既能区分注册人与他人（OOD检测），又能细粒度识别其表情，并保证实时性与低成本。

Method: 提出FOODER：基于60GHz FMCW雷达，提取range-Doppler与micro range-Doppler并级联；认证阶段采用多编码器-多解码器结构，包含身体部位（BP）与中间线性编码-解码（ILED）模块，将被注册者判为ID，其余为OOD；认证通过后，使用ResNet块先判定表情为动态或静态，再分别用两个专用MobileViT网络分类动态（微笑、惊讶）与静态（中性、愤怒）表情，实现层级式处理与隐私保护。

Result: 在自采60GHz短距数据集上，认证AUROC 94.13%，FPR95 18.12%；表情识别平均准确率94.70%；实时运行，优于多种SOTA OOD检测与多种Transformer基线。

Conclusion: 雷达-only的分层FOODER在保证隐私与实时性的同时，实现稳健的人脸OOD认证与精细表情识别，显示出在非视觉场景中替代或补充摄像头方案的潜力。

Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of neural networks, as it enables the identification of samples outside the training domain. We present FOODER, a real-time, privacy-preserving radar-based framework that integrates OOD-based facial authentication with facial expression recognition. FOODER operates using low-cost frequency-modulated continuous-wave (FMCW) radar and exploits both range-Doppler and micro range-Doppler representations. The authentication module employs a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify a single enrolled individual as in-distribution while detecting all other faces as OOD. Upon successful authentication, an expression recognition module is activated. Concatenated radar representations are processed by a ResNet block to distinguish between dynamic and static facial expressions. Based on this categorization, two specialized MobileViT networks are used to classify dynamic expressions (smile, shock) and static expressions (neutral, anger). This hierarchical design enables robust facial authentication and fine-grained expression recognition while preserving user privacy by relying exclusively on radar data. Experiments conducted on a dataset collected with a 60 GHz short-range FMCW radar demonstrate that FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%. FOODER outperforms state-of-the-art OOD detection methods and several transformer-based architectures while operating efficiently in real time.

</details>


### [13] [FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis](https://arxiv.org/abs/2512.18073)
*Ekta Balkrishna Gavas,Sudipta Banerjee,Chinmay Hegde,Nasir Memon*

Main category: cs.CV

TL;DR: 提出FPBench：首个用于指纹领域理解的多模态大模型（MLLM）基准，覆盖7个数据集、8项生物特征与取证任务，评测20个模型在零样本与CoT提示下的表现，并讨论性能与可解释性、挑战与局限。


<details>
  <summary>Details</summary>
Motivation: MLLMs已在视觉问答、生成与推理等任务取得进展，并开始用于虹膜与人脸生物识别分析，但其在指纹理解方面尚无系统评估与基准，阻碍了面向指纹的基础模型研究与应用落地。

Method: 构建FPBench：收集7个真实与合成指纹数据集；定义8项与生物识别和取证相关的下游任务；选取20个开源与商业MLLM；采用零样本与链式思维提示策略进行统一评测；从性能与可解释性角度进行对比与分析，并总结挑战与限制。

Result: 系统性评估显示不同MLLM在指纹任务上的能力存在明显差异；零样本与CoT提示对任务表现有影响；给出了跨模型、跨任务、跨数据集的量化结果与案例分析，揭示当前模型在指纹细节理解与法证推断上的不足。

Conclusion: FPBench奠定了指纹领域MLLM理解与评测的首个全面基准，为后续指纹基础模型与更强多模态推理方法提供参考与发展路径，同时指出当前在可解释性、鲁棒性与任务泛化方面的挑战需要进一步研究。

Abstract: Multimodal LLMs (MLLMs) have gained significant traction in complex data analysis, visual question answering, generation, and reasoning. Recently, they have been used for analyzing the biometric utility of iris and face images. However, their capabilities in fingerprint understanding are yet unexplored. In this work, we design a comprehensive benchmark, \textsc{FPBench} that evaluates the performance of 20 MLLMs (open-source and proprietary) across 7 real and synthetic datasets on 8 biometric and forensic tasks using zero-shot and chain-of-thought prompting strategies. We discuss our findings in terms of performance, explainability and share our insights into the challenges and limitations. We establish \textsc{FPBench} as the first comprehensive benchmark for fingerprint domain understanding with MLLMs paving the path for foundation models for fingerprints.

</details>


### [14] [Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation](https://arxiv.org/abs/2512.18082)
*Shreshth Rajan,Raymond Liu*

Main category: cs.CV

TL;DR: 提出一种区域级基于不确定性门控的检索机制，在跨域场景下显著提升街景语义分割精度与校准，并以低检索开销实时运行。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶、机器人与无障碍出行需要在多变环境（光照、天气、噪声）中实时且可靠地区分道路、行人、车辆等关键类别，传统模型在域移位下精度与校准性能下降，亟需兼顾鲁棒性与效率的方法。

Method: 在语义分割框架中引入“区域级不确定性门控检索”：将图像划分为区域，对每个区域估计不确定性，仅对高不确定性区域触发外部或记忆库的特征/结果检索以辅助判别；对低不确定性区域直接用主模型预测，从而在保证精度与校准的同时降低检索开销。

Result: 在域移位场景中，相较始终检索的基线，提出方法的mIoU提升11.3%，且仅对12.5%的区域执行检索，检索成本降低87.5%；同时改善了模型校准（置信度与准确率更一致）。

Conclusion: 不确定性驱动的按需区域检索能在跨域条件下显著提升街景语义分割的准确性与校准，并以更小的计算/检索代价实现接近实时的部署，适合自动驾驶等安全关键应用。

Abstract: Semantic segmentation of outdoor street scenes plays a key role in applications such as autonomous driving, mobile robotics, and assistive technology for visually-impaired pedestrians. For these applications, accurately distinguishing between key surfaces and objects such as roads, sidewalks, vehicles, and pedestrians is essential for maintaining safety and minimizing risks. Semantic segmentation must be robust to different environments, lighting and weather conditions, and sensor noise, while being performed in real-time. We propose a region-level, uncertainty-gated retrieval mechanism that improves segmentation accuracy and calibration under domain shift. Our best method achieves an 11.3% increase in mean intersection-over-union while reducing retrieval cost by 87.5%, retrieving for only 12.5% of regions compared to 100% for always-on baseline.

</details>


### [15] [SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping](https://arxiv.org/abs/2512.18128)
*Thomas Boudras,Martin Schwartz,Rasmus Fensholt,Martin Brandt,Ibrahim Fayad,Jean-Pierre Wigneron,Gabriel Belouze,Fajwel Fogel,Philippe Ciais*

Main category: cs.CV

TL;DR: SERA-H将超分辨率(EDSR)与时序注意编码(UTAE)结合，用免费Sentinel-1/2时序在ALS高密度激光雷达监督下，生成2.5 m树冠高度图，精度达MAE 2.6 m、R² 0.82，优于常规S1/S2基线并可媲美商业高分影像方法。


<details>
  <summary>Details</summary>
Motivation: 现有用卫星影像预测森林树高的方法在数据可获得性与空间分辨率间存在权衡：免费数据分辨率不够、商业高分虽精细但昂贵且覆盖/复访受限。需一种能用免费、时序丰富的数据实现高分辨率树高制图的方案。

Method: 提出端到端模型SERA-H：将超分辨率网络EDSR与时序注意编码UTAE耦合，输入为10 m分辨率的Sentinel-1/2多时相序列，利用高密度机载激光雷达(ALS)树高作为监督信号，学习从时空信息中重建2.5 m树高图。

Result: 在法国开源基准上，SERA-H达到MAE 2.6 m、R² 0.82，优于标准S1/S2基线；其表现与依赖SPOT-6/7、PlanetScope、Maxar等商业高分影像的方法相当或更好。

Conclusion: 高分辨率监督结合免费时序卫星数据的时空信息，可突破传感器原生分辨率限制，重建细节。SERA-H可高频、低成本地生成高精度森林树高图，具备实际应用潜力并已开源。

Abstract: High-resolution mapping of canopy height is essential for forest management and biodiversity monitoring. Although recent studies have led to the advent of deep learning methods using satellite imagery to predict height maps, these approaches often face a trade-off between data accessibility and spatial resolution. To overcome these limitations, we present SERA-H, an end-to-end model combining a super-resolution module (EDSR) and temporal attention encoding (UTAE). Trained under the supervision of high-density LiDAR data (ALS), our model generates 2.5 m resolution height maps from freely available Sentinel-1 and Sentinel-2 (10 m) time series data. Evaluated on an open-source benchmark dataset in France, SERA-H, with a MAE of 2.6 m and a coefficient of determination of 0.82, not only outperforms standard Sentinel-1/2 baselines but also achieves performance comparable to or better than methods relying on commercial very high-resolution imagery (SPOT-6/7, PlanetScope, Maxar). These results demonstrate that combining high-resolution supervision with the spatiotemporal information embedded in time series enables the reconstruction of details beyond the input sensors' native resolution. SERA-H opens the possibility of freely mapping forests with high revisit frequency, achieving accuracy comparable to that of costly commercial imagery. The source code is available at https://github.com/ThomasBoudras/SERA-H#

</details>


### [16] [EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams](https://arxiv.org/abs/2512.18159)
*Hao Li,Daiwei Lu,Jiacheng Wang,Robert J. Webster,Ipek Oguz*

Main category: cs.CV

TL;DR: 提出EndoStreamDepth：针对内镜视频的单目深度估计，逐帧处理并用时序模块传播信息，实现清晰边界、时序一致、实时性能，在两套结肠镜数据集上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度方法多依赖批量输入或离线处理，难以满足内镜场景对实时性、边界精细度和跨帧稳定性的要求；内镜特有成像特性（光照变化、反光、组织纹理弱）导致边界模糊和几何不一致，限制了机器人手术等下游任务。

Method: 构建EndoStreamDepth框架，包括：1）带内镜特定图像变换的单帧深度网络，提升单帧准确性；2）多层级Mamba时序模块，在多尺度上跨帧传播与聚合信息，增强精度并稳定输出；3）层次化多尺度监督，结合互补损失，兼顾局部边界锐利与全局几何一致；逐帧在线处理，非批处理。

Result: 在两个公开结肠镜深度数据集上，较现有单目深度SOTA显著提升定量指标，并生成解剖结构对齐、边界清晰的深度图，同时具备跨帧时序一致性与实时吞吐。

Conclusion: EndoStreamDepth通过逐帧+Mamba时序融合与层次化监督，实现面向内镜视频的高精度、边界锐利、时序稳定且实时的深度估计，可支持机器人手术等下游应用；代码已开源。

Abstract: This work presents EndoStreamDepth, a monocular depth estimation framework for endoscopic video streams. It provides accurate depth maps with sharp anatomical boundaries for each frame, temporally consistent predictions across frames, and real-time throughput. Unlike prior work that uses batched inputs, EndoStreamDepth processes individual frames with a temporal module to propagate inter-frame information. The framework contains three main components: (1) a single-frame depth network with endoscopy-specific transformation to produce accurate depth maps, (2) multi-level Mamba temporal modules that leverage inter-frame information to improve accuracy and stabilize predictions, and (3) a hierarchical design with comprehensive multi-scale supervision, where complementary loss terms jointly improve local boundary sharpness and global geometric consistency. We conduct comprehensive evaluations on two publicly available colonoscopy depth estimation datasets. Compared to state-of-the-art monocular depth estimation methods, EndoStreamDepth substantially improves performance, and it produces depth maps with sharp, anatomically aligned boundaries, which are essential to support downstream tasks such as automation for robotic surgery. The code is publicly available at https://github.com/MedICL-VU/EndoStreamDepth

</details>


### [17] [Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction](https://arxiv.org/abs/2512.18161)
*Taewon Yang,Jason Hu,Jeffrey A. Fessler,Liyue Shen*

Main category: cs.CV

TL;DR: 提出一种面向高分辨率3D医学影像（CT）重建的3D“补丁式”扩散模型，在有限数据与算力下学习完整3D先验，并显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 直接在3D体数据上训练扩散模型代价高（GPU/数据需求大）；现有将2D先验迁移到3D的方案无法充分发挥扩散模型在高维数据上的生成能力。需要一种既能学习真正3D先验、又具有可扩展效率的新方法来支持实际3D成像（如CT）。

Method: 设计位置感知的3D局部patch扩散模型，并以下采样的全局3D体作为上下文，联合建模局部—全局的联合分布。通过对3D补丁的先验学习实现可扩展训练/推理，同时用全局体信息耦合以保证生成一致性与高质量，实现高分辨率3D体的可扩展生成与逆问题求解。

Result: 在多套3D CT数据集上的重建实验中，方法在性能与效率上均优于现有SOTA；可在约20分钟内完成512×512×256分辨率的高质量3D重建。

Conclusion: 3D补丁+全局上下文的扩散建模能在有限数据与资源下学习强大的3D先验，既提升3D生成质量，又为高分辨率3D逆问题（如CT重建）提供高效准确的解决方案。

Abstract: Diffusion models learn strong image priors that can be leveraged to solve inverse problems like medical image reconstruction. However, for real-world applications such as 3D Computed Tomography (CT) imaging, directly training diffusion models on 3D data presents significant challenges due to the high computational demands of extensive GPU resources and large-scale datasets. Existing works mostly reuse 2D diffusion priors to address 3D inverse problems, but fail to fully realize and leverage the generative capacity of diffusion models for high-dimensional data. In this study, we propose a novel 3D patch-based diffusion model that can learn a fully 3D diffusion prior from limited data, enabling scalable generation of high-resolution 3D images. Our core idea is to learn the prior of 3D patches to achieve scalable efficiency, while coupling local and global information to guarantee high-quality 3D image generation, by modeling the joint distribution of position-aware 3D local patches and downsampled 3D volume as global context. Our approach not only enables high-quality 3D generation, but also offers an unprecedentedly efficient and accurate solution to high-resolution 3D inverse problems. Experiments on 3D CT reconstruction across multiple datasets show that our method outperforms state-of-the-art methods in both performance and efficiency, notably achieving high-resolution 3D reconstruction of $512 \times 512 \times 256$ ($\sim$20 mins).

</details>


### [18] [Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation](https://arxiv.org/abs/2512.18176)
*Ziyu Zhang,Yi Yu,Simeng Zhu,Ahmed Aly,Yunhe Gao,Ning Gu,Yuan Xue*

Main category: cs.CV

TL;DR: AtlasSegFM：用单个标注样本，通过“图谱注册+FM交互分割+测试时融合”在临床特定场景中定制基础模型，显著提升尤其是小而细结构的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有交互式基础模型虽具泛化能力，但仍依赖精确提示，在训练数据稀缺或分布偏移的临床场景下表现不稳；临床部署需要低样本、可落地的定制化方案。

Method: 提出AtlasSegFM：1）将“场景图谱”与查询图像进行配准，生成上下文感知的提示（如器官先验、位置/形状引导）；2）在测试时用适配器融合来自图谱配准与基础模型（如nnInteractive）的预测，利用一例标注进行一次性定制；无需重训，仅做推理阶段的策略与融合。

Result: 在多模态（多器官、公共与院内数据）上广泛实验，AtlasSegFM稳定提高分割精度，提升在小而精细结构上的性能最为明显，优于仅用基础模型或仅用配准的方案。

Conclusion: AtlasSegFM是一种轻量、可部署的一次样本临床定制框架，通过图谱引导提示与测试时融合，强化基础模型在特定临床情境中的鲁棒性与精度；代码将开源。

Abstract: Accurate medical image segmentation is essential for clinical diagnosis and treatment planning. While recent interactive foundation models (e.g., nnInteractive) enhance generalization through large-scale multimodal pretraining, they still depend on precise prompts and often perform below expectations in contexts that are underrepresented in their training data. We present AtlasSegFM, an atlas-guided framework that customizes available foundation models to clinical contexts with a single annotated example. The core innovations are: 1) a pipeline that provides context-aware prompts for foundation models via registration between a context atlas and query images, and 2) a test-time adapter to fuse predictions from both atlas registration and the foundation model. Extensive experiments across public and in-house datasets spanning multiple modalities and organs demonstrate that AtlasSegFM consistently improves segmentation, particularly for small, delicate structures. AtlasSegFM provides a lightweight, deployable solution one-shot customization of foundation models in real-world clinical workflows. The code will be made publicly available.

</details>


### [19] [MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation](https://arxiv.org/abs/2512.18181)
*Kaixing Yang,Jiashu Zhu,Xulong Tang,Ziqiao Peng,Xiangyue Zhang,Puwei Wang,Jiahong Wu,Xiangxiang Chu,Hongyan Liu,Jun He*

Main category: cs.CV

TL;DR: 提出MACE-Dance：以“运动专家+外观专家”的级联MoE，实现从音乐到3D舞蹈再到视频合成，兼顾运动真实与外观一致，三项评测均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有音乐驱动舞蹈视频生成无法兼顾高质量外观与逼真动作；相关领域方法（3D舞蹈生成、姿态动画、音频驱动说话人）不可直接迁移。缺乏统一数据与评价协议，使任务难以客观比较。

Method: 级联MoE框架：1) Motion Expert：音乐→3D骨架运动，采用扩散模型+BiMamba-Transformer混合架构，并提出无引导训练（GFT）策略以平衡可行动力学与艺术表现。2) Appearance Expert：以运动与参考图像为条件的视频合成，采用解耦“运动学-审美”微调以保持身份一致与时空连续。3) 构建大规模多样数据集并设计“运动-外观”双维评测协议。

Result: 在3D舞蹈生成与姿态驱动图像动画两项子任务上达SOTA；在新提出的运动-外观评测协议上整体优于现有方法；能在视觉身份保真与动作真实感上同时取得高分。

Conclusion: MACE-Dance通过级联专家化设计实现音乐→动作→视频的端到端高质生成，兼顾运动物理合理性与外观一致性，并以新数据与评价标准推动该方向基准化，达成SOTA。

Abstract: With the rise of online dance-video platforms and rapid advances in AI-generated content (AIGC), music-driven dance generation has emerged as a compelling research direction. Despite substantial progress in related domains such as music-driven 3D dance generation, pose-driven image animation, and audio-driven talking-head synthesis, existing methods cannot be directly adapted to this task. Moreover, the limited studies in this area still struggle to jointly achieve high-quality visual appearance and realistic human motion. Accordingly, we present MACE-Dance, a music-driven dance video generation framework with cascaded Mixture-of-Experts (MoE). The Motion Expert performs music-to-3D motion generation while enforcing kinematic plausibility and artistic expressiveness, whereas the Appearance Expert carries out motion- and reference-conditioned video synthesis, preserving visual identity with spatiotemporal coherence. Specifically, the Motion Expert adopts a diffusion model with a BiMamba-Transformer hybrid architecture and a Guidance-Free Training (GFT) strategy, achieving state-of-the-art (SOTA) performance in 3D dance generation. The Appearance Expert employs a decoupled kinematic-aesthetic fine-tuning strategy, achieving state-of-the-art (SOTA) performance in pose-driven image animation. To better benchmark this task, we curate a large-scale and diverse dataset and design a motion-appearance evaluation protocol. Based on this protocol, MACE-Dance also achieves state-of-the-art performance. Project page: https://macedance.github.io/

</details>


### [20] [Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching](https://arxiv.org/abs/2512.18184)
*Junho Lee,Kwanseok Kim,Joonseok Lee*

Main category: cs.CV

TL;DR: 提出一个用于分析flow matching源分布选择的2D可解释模拟，得出四点关键洞见，并据此提出“范数对齐训练+方向裁剪采样”的实用框架，在无需重训的前提下提升质量与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管flow matching允许灵活选择源分布，实际大多用高斯；在高维生成中是否存在更优源分布及其训练动力学机理仍缺乏系统研究与实证指导。作者希望通过可控且可解释的设定揭示源分布对学习稳定性与性能的影响，并给出现实可用的改进策略。

Method: 1) 设计一个能反映高维几何性质的2D模拟场景，跟踪训练中密度近似、方向对齐与范数对齐的动力学；2) 基于分析，归纳四个行为规律（密度近似的反效果、方向过度集中导致路径缠绕、高斯的全向覆盖优势、范数失配的学习代价）；3) 提出实践框架：训练阶段做范数对齐以保留稳健的全向监督，推理阶段进行方向裁剪以剔除数据稀疏区域的初始化；4) 给出无需重训即可在任何以高斯为源的flow matching模型上使用的裁剪策略。

Result: 在多项实证评测中，所提方法在生成质量与采样效率上均有一致提升。方向裁剪作为推理时插件对现有高斯源FM模型即插即用带来收益。

Conclusion: 源分布的设计对flow matching的稳定性与性能至关重要：高斯的全向覆盖提供稳健学习信号，但需要通过范数对齐与推理期方向裁剪缓解模式不匹配与路径缠绕问题。该工作给出实践指南与可直接落地的改进方案，并开源实现。

Abstract: Flow matching has emerged as a powerful generative modeling approach with flexible choices of source distribution. While Gaussian distributions are commonly used, the potential for better alternatives in high-dimensional data generation remains largely unexplored. In this paper, we propose a novel 2D simulation that captures high-dimensional geometric properties in an interpretable 2D setting, enabling us to analyze the learning dynamics of flow matching during training. Based on this analysis, we derive several key insights about flow matching behavior: (1) density approximation can paradoxically degrade performance due to mode discrepancy, (2) directional alignment suffers from path entanglement when overly concentrated, (3) Gaussian's omnidirectional coverage ensures robust learning, and (4) norm misalignment incurs substantial learning costs. Building on these insights, we propose a practical framework that combines norm-aligned training with directionally-pruned sampling. This approach maintains the robust omnidirectional supervision essential for stable flow learning, while eliminating initializations in data-sparse regions during inference. Importantly, our pruning strategy can be applied to any flow matching model trained with a Gaussian source, providing immediate performance gains without the need for retraining. Empirical evaluations demonstrate consistent improvements in both generation quality and sampling efficiency. Our findings provide practical insights and guidelines for source distribution design and introduce a readily applicable technique for improving existing flow matching models. Our code is available at https://github.com/kwanseokk/SourceFM.

</details>


### [21] [ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection](https://arxiv.org/abs/2512.18187)
*Janghyun Baek,Mincheol Chang,Seokha Moon,Seung Joon Lee,Jinkyu Kim*

Main category: cs.CV

TL;DR: 提出ALIGN方法，通过更优的查询初始化提升多模态(相机+激光雷达)3D检测，尤其在遮挡/拥挤场景中显著增益。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的3D检测在初始化上多用随机或BEV热力图采样，导致查询浪费、对被遮挡或密集目标不敏感，影响精度与鲁棒性。

Method: ALIGN包含三模块：1) 遮挡感知中心估计(OCE)：融合LiDAR几何与图像语义以精确估计目标中心；2) 自适应邻域采样(ANS)：以LiDAR聚类为锚生成候选，并在每个目标周围依据空间与语义对齐进行补充采样；3) 动态查询均衡(DQB)：在前景与背景区域间自适应分配/平衡查询，避免前景目标被淹没。

Result: 在nuScenes上对多种SOTA检测器均有稳定提升，最高带来+0.9 mAP与+1.2 NDS增益，尤其在遮挡或人群密集场景中更明显。

Conclusion: 通过更精细的、目标感知且遮挡鲁棒的查询初始化，ALIGN通用于多模型并显著提升3D检测性能；代码将开源。

Abstract: Recent query-based 3D object detection methods using camera and LiDAR inputs have shown strong performance, but existing query initialization strategies,such as random sampling or BEV heatmap-based sampling, often result in inefficient query usage and reduced accuracy, particularly for occluded or crowded objects. To address this limitation, we propose ALIGN (Advanced query initialization with LiDAR and Image GuidaNce), a novel approach for occlusion-robust, object-aware query initialization. Our model consists of three key components: (i) Occlusion-aware Center Estimation (OCE), which integrates LiDAR geometry and image semantics to estimate object centers accurately (ii) Adaptive Neighbor Sampling (ANS), which generates object candidates from LiDAR clustering and supplements each object by sampling spatially and semantically aligned points around it and (iii) Dynamic Query Balancing (DQB), which adaptively balances queries between foreground and background regions. Our extensive experiments on the nuScenes benchmark demonstrate that ALIGN consistently improves performance across multiple state-of-the-art detectors, achieving gains of up to +0.9 mAP and +1.2 NDS, particularly in challenging scenes with occlusions or dense crowds. Our code will be publicly available upon publication.

</details>


### [22] [Multi-Part Object Representations via Graph Structures and Co-Part Discovery](https://arxiv.org/abs/2512.18192)
*Alex Foo,Wynne Hsu,Mong Li Lee*

Main category: cs.CV

TL;DR: 提出显式部件图与“共部件”对象发现算法，在遮挡和分布外场景下比现有方法更稳健地识别多部件对象，并在下游属性预测上更准。


<details>
  <summary>Details</summary>
Motivation: 现有面向多部件物体的对象中心表示多用隐式表示，假设通过间接训练能学到部件-整体关系，但在遮挡或分布外时难以正确识别对象，鲁棒性不足。

Method: 引入显式部件级图表示，设计“co-part”对象发现算法，利用部件与整体的图结构进行学习与推断；并构建三套基准，专门评估在遮挡与分布外条件下对多部件对象的识别鲁棒性。

Result: 在模拟、逼真与真实图像上，相比SOTA显著提升对象发现质量；能在遮挡与分布外场景准确识别多部件对象；其学得的对象中心表示在下游关键属性预测任务上更准确。

Conclusion: 显式部件图与共部件发现能够弥补隐式方法在部件-整体建模上的不足，提升对象中心表示的鲁棒性与泛化，并对下游任务有实际收益。

Abstract: Discovering object-centric representations from images can significantly enhance the robustness, sample efficiency and generalizability of vision models. Works on images with multi-part objects typically follow an implicit object representation approach, which fail to recognize these learned objects in occluded or out-of-distribution contexts. This is due to the assumption that object part-whole relations are implicitly encoded into the representations through indirect training objectives. We address this limitation by proposing a novel method that leverages on explicit graph representations for parts and present a co-part object discovery algorithm. We then introduce three benchmarks to evaluate the robustness of object-centric methods in recognizing multi-part objects within occluded and out-of-distribution settings. Experimental results on simulated, realistic, and real-world images show marked improvements in the quality of discovered objects compared to state-of-the-art methods, as well as the accurate recognition of multi-part objects in occluded and out-of-distribution contexts. We also show that the discovered object-centric representations can more accurately predict key object properties in a downstream task, highlighting the potential of our method to advance the field of object-centric representations.

</details>


### [23] [Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching](https://arxiv.org/abs/2512.18219)
*Mohammad Zolfaghari,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出ET-STPM：通过增强Teacher网络的学生-教师特征金字塔实现无监督异常检测，在MVTech-AD上图像级0.971、像素级0.977。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测在缺乏标注场景下仍难以取得高精度，现有学生-教师方法受限于教师表征能力与特征层级利用不足，导致图像级与像素级检测性能不够理想。

Method: 采用学生-教师框架：先在ImageNet上预训练ResNet-18作为Teacher，再在MVTech-AD上对其微调以增强教师特征；构建学生网络并进行特征金字塔蒸馏（ET-STPM），使学生重构/对齐多尺度中间特征，从而以重建误差或特征差异衡量异常。

Result: 在MVTech-AD数据集上，图像级平均准确率0.971，像素级平均准确率0.977，优于以往方法。

Conclusion: 增强Teacher并结合特征金字塔蒸馏可显著提升无监督异常检测的图像级与像素级表现，在MVTech-AD上取得SOTA水平。

Abstract: Anomaly detection or outlier is one of the challenging subjects in unsupervised learning . This paper is introduced a student-teacher framework for anomaly detection that its teacher network is enhanced for achieving high-performance metrics . For this purpose , we first pre-train the ResNet-18 network on the ImageNet and then fine-tune it on the MVTech-AD dataset . Experiment results on the image-level and pixel-level demonstrate that this idea has achieved better metrics than the previous methods . Our model , Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM), achieved 0.971 mean accuracy on the image-level and 0.977 mean accuracy on the pixel-level for anomaly detection.

</details>


### [24] [Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo's 23 Wards](https://arxiv.org/abs/2512.18226)
*Takuya OKi,Yuan Liu*

Main category: cs.CV

TL;DR: 研究提出从二维与三维双视角量化住宅“空间开敞度”的框架，基于东京23区4004套租房数据，结合VGA平面可视性与Mask2Former室内语义分割，分析其时空演变、与租金及房屋属性关系。结果显示：客厅可视性上升、整体开敞度在1990年代达峰；开敞度与租金、建筑特征存在局部相关但2D与3D指标彼此不直接相关；更高开敞度往往对应更高租金；既有“印象分”与开敞度关系弱，可能受软装影响更大。


<details>
  <summary>Details</summary>
Motivation: 以往对住宅“开敞度”的研究多将影响因素割裂考察，缺乏同时兼顾平面布局与立体空间体验的统一量化框架，难以系统解释其与市场价格、城市更新的耦合关系。

Method: 构建双维度量化体系：1) 2D开敞度：基于户型图，用可视性图分析(VGA)计算平面可视性指标；2) 3D开敞度：基于室内照片，使用Mask2Former进行语义分割（墙、顶、地、窗等），推导三维开敞度。以东京23区4004套租赁住房为样本，开展时间序列与空间分布分析，并检验与租金、建筑属性及既有印象评分的相关性/偏相关。

Result: - 客厅可视性持续上升，整体开敞度在1990年代达到峰值；- 开敞度与租金及建筑特征存在部分（局部）相关，反映城市再开发影响；- 2D与3D开敞度指标彼此不直接相关，但更高开敞度总体对应更高租金；- 既有印象分与开敞度相关性弱，提示软装与家具对空间感知影响更大。

Conclusion: 提出可扩展的多维数据驱动框架，能从平面与立体两层面量化住宅开敞度并关联市场与城市动态；为住宅设计与改造、租赁定价及更新评估提供新工具，同时指出感知空间还强受室内设计与家具布置影响，需在未来模型中纳入软装因素。

Abstract: Understanding spatial openness is vital for improving residential quality and design; however, studies often treat its influencing factors separately. This study developed a quantitative framework to evaluate the spatial openness in housing from two- (2D) and three- (3D) dimensional perspectives. Using data from 4,004 rental units in Tokyo's 23 wards, we examined the temporal and spatial variations in openness and its relationship with rent and housing attributes. 2D openness was computed via planar visibility using visibility graph analysis (VGA) from floor plans, whereas 3D openness was derived from interior images analysed using Mask2Former, a semantic segmentation model that identifies walls, ceilings, floors, and windows. The results showed an increase in living room visibility and a 1990s peak in overall openness. Spatial analyses revealed partial correlations among openness, rent, and building characteristics, reflecting urban redevelopment trends. Although the 2D and 3D openness indicators were not directly correlated, higher openness tended to correspond to higher rent. The impression scores predicted by the existing models were only weakly related to openness, suggesting that the interior design and furniture more strongly shape perceived space. This study offers a new multidimensional data-driven framework for quantifying residential spatial openness and linking it with urban and market dynamics.

</details>


### [25] [Investigating Spatial Attention Bias in Vision-Language Models](https://arxiv.org/abs/2512.18231)
*Aryan Chaudhary,Sanchit Goyal,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: 论文发现并系统刻画：在水平拼接图像上，多数视觉-语言模型倾向于先描述左侧再描述右侧内容（约97%案例），揭示其空间处理存在稳定偏置。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在视觉理解上表现卓越，但其空间处理是否存在系统性偏差尚未被充分研究。作者旨在检验并量化模型在左右空间顺序上的注意与描述偏置，以评估VLM在空间推理与公平性方面的基础局限。

Method: 构造受控的水平拼接图像对，采用中性提示词，对开源与闭源多种架构VLM进行测试；统计描述顺序（左先/右先），并在阿拉伯语微调模型上复现实验以隔离语言书写方向因素；同时审视PixMo与Visual Genome等训练数据标注指南，检查是否存在显式“先左后右”的指令。

Result: 在中性提示下，不同架构的VLM约97%案例先描述左侧内容，阿拉伯语微调模型仍表现相同偏置；训练数据的公开指南未发现“左先”的显式规则。偏置由体系性与跨模型一致性体现。

Conclusion: 当前VLM在空间信息处理上存在固有的左先描述偏置，更可能源自模型/架构或训练流程的内在因素而非语言阅读方向或显式数据指令；这一发现提示需要在模型设计、训练和评测上引入更严格的空间偏置纠正与基准。

Abstract: Vision-Language Models have demonstrated remarkable capabilities in understanding visual content, yet systematic biases in their spatial processing remain largely unexplored. This work identifies and characterizes a systematic spatial attention bias where VLMs consistently prioritize describing left-positioned content before right-positioned content in horizontally concatenated images. Through controlled experiments on image pairs using both open-source and closed-source models, we demonstrate that this bias persists across different architectures, with models describing left-positioned content first in approximately 97% of cases under neutral prompting conditions. Testing on an Arabic-finetuned model reveals that the bias persists despite right-to-left language training, ruling out language reading direction as the primary cause. Investigation of training dataset annotation guidelines from PixMo and Visual Genome reveals no explicit left-first ordering instructions, suggesting the bias is consistent with architectural factors rather than explicit training data instructions. These findings reveal fundamental limitations in how current VLMs process spatial information.

</details>


### [26] [Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction](https://arxiv.org/abs/2512.18237)
*Shahram Najam Syed,Yitian Hu,Yuchao Yao*

Main category: cs.CV

TL;DR: 论文提出联合学习框架，把深度、位姿和辐射场耦合，借助ViT度量尺度深度、特征空间BA抑制漂移、增量式局部NeRF层级扩展到城市街区尺度。在Tanks and Temples上显著降低ATE并保持亚像素RPE，实现单目未标定相机的度量尺度、无漂移重建与高保真新视角合成。


<details>
  <summary>Details</summary>
Motivation: 传统从单目视频重建大规模场景时，若将深度、位姿、辐射场分别求解，会出现：尺度歧义导致幽灵几何；长时间轨迹位姿漂移；单个全局NeRF难以覆盖数百米内容。为解决这些互相耦合的失败模式，需要一个统一框架。

Method: 1) 以带度量尺度监督的ViT深度网络初始化，适应大视场变化并提供全局一致深度；2) 多尺度特征BA层：在学习到的金字塔特征空间进行束束调整，抛弃易碎的传统关键点，直接优化相机位姿以抑制漂移；3) 场景表示采用增量式本地辐射场层级：当视角重叠低于阈值时，在线分配并冻结新的哈希网格NeRF，使系统可在单GPU上覆盖城市街区尺度。

Result: 在Tanks and Temples基准的8个室内外序列上，绝对轨迹误差ATE达到0.001–0.021 m，相比BARF最高降低18倍，相比NoPe-NeRF降低2倍，同时保持亚像素级相对位姿误差RPE。

Conclusion: 将度量尺度深度、特征空间位姿优化与分层本地NeRF耦合，可在单个未标定RGB相机下实现尺度正确、几乎无漂移的3D重建与高保真新视角合成，并可扩展到城市街区规模。

Abstract: Photorealistic 3-D reconstruction from monocular video collapses in large-scale scenes when depth, pose, and radiance are solved in isolation: scale-ambiguous depth yields ghost geometry, long-horizon pose drift corrupts alignment, and a single global NeRF cannot model hundreds of metres of content. We introduce a joint learning framework that couples all three factors and demonstrably overcomes each failure case. Our system begins with a Vision-Transformer (ViT) depth network trained with metric-scale supervision, giving globally consistent depths despite wide field-of-view variations. A multi-scale feature bundle-adjustment (BA) layer refines camera poses directly in feature space--leveraging learned pyramidal descriptors instead of brittle keypoints--to suppress drift on unconstrained trajectories. For scene representation, we deploy an incremental local-radiance-field hierarchy: new hash-grid NeRFs are allocated and frozen on-the-fly when view overlap falls below a threshold, enabling city-block-scale coverage on a single GPU. Evaluated on the Tanks and Temples benchmark, our method reduces Absolute Trajectory Error to 0.001-0.021 m across eight indoor-outdoor sequences--up to 18x lower than BARF and 2x lower than NoPe-NeRF--while maintaining sub-pixel Relative Pose Error. These results demonstrate that metric-scale, drift-free 3-D reconstruction and high-fidelity novel-view synthesis are achievable from a single uncalibrated RGB camera.

</details>


### [27] [SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality](https://arxiv.org/abs/2512.18241)
*Pan Ben Wong,Chengli Wu,Hanyue Lu*

Main category: cs.CV

TL;DR: 提出SG-RIFE：在RIFE上通过注入冻结DINOv3语义先验与轻量微调，实现接近扩散方法的感知质量且接近实时速度。


<details>
  <summary>Details</summary>
Motivation: 流法（如RIFE）速度快但在大运动/遮挡下感知质量欠佳；扩散法（如Consec. BB）质量高但延迟大，不适合实时。需要在保证实时性的同时提升复杂场景下的感知质量。

Method: 对预训练RIFE进行参数高效微调，引入冻结的DINOv3 ViT语义特征；设计Split-Fidelity Aware Projection Module（Split-FAPM）压缩与细化高维语义特征；设计Deformable Semantic Fusion（DSF）将语义先验与像素级光流/运动场对齐融合。整体形成语义引导的流式插帧框架SG-RIFE。

Result: 在SNU-FILM上，语义注入明显提升感知保真；SG-RIFE在FID/LPIPS上优于扩散式LDMVFI，并在复杂基准上达到与Consec. BB相当的质量，同时显著更快，接近实时。

Conclusion: 语义一致性对流式插帧至关重要；通过冻结语义先验与轻量融合模块，可在无需训练新模型的情况下，让流法达到接近扩散法的感知质量并保持高吞吐，适用于近实时VFI。

Abstract: Real-time Video Frame Interpolation (VFI) has long been dominated by flow-based methods like RIFE, which offer high throughput but often fail in complicated scenarios involving large motion and occlusion. Conversely, recent diffusion-based approaches (e.g., Consec. BB) achieve state-of-the-art perceptual quality but suffer from prohibitive latency, rendering them impractical for real-time applications. To bridge this gap, we propose Semantic-Guided RIFE (SG-RIFE). Instead of training from scratch, we introduce a parameter-efficient fine-tuning strategy that augments a pre-trained RIFE backbone with semantic priors from a frozen DINOv3 Vision Transformer. We propose a Split-Fidelity Aware Projection Module (Split-FAPM) to compress and refine high-dimensional features, and a Deformable Semantic Fusion (DSF) module to align these semantic priors with pixel-level motion fields. Experiments on SNU-FILM demonstrate that semantic injection provides a decisive boost in perceptual fidelity. SG-RIFE outperforms diffusion-based LDMVFI in FID/LPIPS and achieves quality comparable to Consec. BB on complex benchmarks while running significantly faster, proving that semantic consistency enables flow-based methods to achieve diffusion-competitive perceptual quality in near real-time.

</details>


### [28] [Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image](https://arxiv.org/abs/2512.18245)
*Xiao He,Chang Tang,Xinwang Liu,Wei Zhang,Zhimin Gao,Chuankun Li,Shaohua Qiu,Jiangfeng Xu*

Main category: cs.CV

TL;DR: 提出SDCM网络通过语义一致性学习、谱门控与谱差异感知提升高光谱目标检测，缓解谱间不一致与冗余，取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像具有高光谱分辨率，能区分相似物质，但由于传感噪声、光照、跨带空间差异等导致谱间不一致、类内/类间相似度高，传统检测难以稳健提取判别特征并受冗余影响，需要一种方法在众多波段中提取一致且有判别力的语义，同时抑制冗余与干扰。

Method: 提出SDCM框架，核心包括：1) 语义一致性学习（SCL）模块，利用谱间上下文约束，降低各波段信息异质性，得到一致的谱维表示；2) 光谱门控生成器（SGG），根据波段重要性过滤冗余信息，实现自适应波段选择/加权；3) 光谱差异感知（SDA）模块，从像素级光谱特征丰富高层语义表示，强调与类判别相关的谱差异。整体结合跨模态（谱-空间/高层语义）一致性与谱选择机制以提升检测。

Result: 在两个高光谱数据集上进行大量实验，SDCM优于现有方法，达到SOTA性能（文中未给出具体数值）。

Conclusion: 通过跨波段语义一致性、重要性驱动的谱门控与差异感知，SDCM有效缓解谱间不一致与冗余，提升高光谱目标检测的鲁棒性与准确性，具有在复杂干扰场景中的应用潜力。

Abstract: Hyperspectral images with high spectral resolution provide new insights into recognizing subtle differences in similar substances. However, object detection in hyperspectral images faces significant challenges in intra- and inter-class similarity due to the spatial differences in hyperspectral inter-bands and unavoidable interferences, e.g., sensor noises and illumination. To alleviate the hyperspectral inter-bands inconsistencies and redundancy, we propose a novel network termed \textbf{S}pectral \textbf{D}iscrepancy and \textbf{C}ross-\textbf{M}odal semantic consistency learning (SDCM), which facilitates the extraction of consistent information across a wide range of hyperspectral bands while utilizing the spectral dimension to pinpoint regions of interest. Specifically, we leverage a semantic consistency learning (SCL) module that utilizes inter-band contextual cues to diminish the heterogeneity of information among bands, yielding highly coherent spectral dimension representations. On the other hand, we incorporate a spectral gated generator (SGG) into the framework that filters out the redundant data inherent in hyperspectral information based on the importance of the bands. Then, we design the spectral discrepancy aware (SDA) module to enrich the semantic representation of high-level information by extracting pixel-level spectral features. Extensive experiments on two hyperspectral datasets demonstrate that our proposed method achieves state-of-the-art performance when compared with other ones.

</details>


### [29] [Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model](https://arxiv.org/abs/2512.18247)
*Rui Xing,Runmin Cong,Yingying Wu,Can Wang,Zhongming Tang,Fen Wang,Hao Wu,Sam Kwong*

Main category: cs.CV

TL;DR: 提出APS古植物种子图像分类数据集与专用模型APSNet，在8,340张、17类古种子上达90.5%准确率，优于SOTA。核心贡献：引入尺寸感知（SPE）与异步解耦解码（ADD）以强化细粒度判别。


<details>
  <summary>Details</summary>
Motivation: 古代饮食与区域时段演变关系是理解人地互动的关键；种子是考古植物学核心材料，但传统依赖专家、耗时低效。智能方法在考古其他方向有进展，但在古植物种子分类上缺乏数据与方法体系，需建立基准数据集与针对性模型。

Method: 1) 构建APS数据集：来自中国18处遗址、17个属/种类别、8,340张种子图像。2) 设计APSNet：以细粒度识别为目标，显式引入“尺度（尺寸）”线索。- SPE（Size Perception and Embedding）模块在编码阶段提取并嵌入尺寸信息以补充细粒度纹理/形态特征。- ADD（Asynchronous Decoupled Decoding）基于渐进式学习，从通道与空间两个视角异步/解耦地解码特征，提升判别性与效率。

Result: 在定量与定性评估上，APSNet优于现有SOTA图像分类方法，整体Top-1准确率达90.5%。可解释分析显示模型能聚焦关键“证据”区域。

Conclusion: 首次提供古植物种子图像分类基准与专用网络，显著提升自动化识别性能，为大规模、系统性的考古研究提供有效工具，并有望减轻专家工作量、促进人地关系研究。

Abstract: Understanding the dietary preferences of ancient societies and their evolution across periods and regions is crucial for revealing human-environment interactions. Seeds, as important archaeological artifacts, represent a fundamental subject of archaeobotanical research. However, traditional studies rely heavily on expert knowledge, which is often time-consuming and inefficient. Intelligent analysis methods have made progress in various fields of archaeology, but there remains a research gap in data and methods in archaeobotany, especially in the classification task of ancient plant seeds. To address this, we construct the first Ancient Plant Seed Image Classification (APS) dataset. It contains 8,340 images from 17 genus- or species-level seed categories excavated from 18 archaeological sites across China. In addition, we design a framework specifically for the ancient plant seed classification task (APSNet), which introduces the scale feature (size) of seeds based on learning fine-grained information to guide the network in discovering key "evidence" for sufficient classification. Specifically, we design a Size Perception and Embedding (SPE) module in the encoder part to explicitly extract size information for the purpose of complementing fine-grained information. We propose an Asynchronous Decoupled Decoding (ADD) architecture based on traditional progressive learning to decode features from both channel and spatial perspectives, enabling efficient learning of discriminative features. In both quantitative and qualitative analyses, our approach surpasses existing state-of-the-art image classification methods, achieving an accuracy of 90.5%. This demonstrates that our work provides an effective tool for large-scale, systematic archaeological research.

</details>


### [30] [Loom: Diffusion-Transformer for Interleaved Generation](https://arxiv.org/abs/2512.18254)
*Mingcheng Ye,Jiaming Liu,Yiren Song*

Main category: cs.CV

TL;DR: Loom 提出一个统一的扩散-Transformer 框架，用交错的文本-图像嵌入与语言规划，实现长跨度、可控且时序一致的交错文本-图像生成，并在多项任务上显著优于开源基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在单一序列内同时生成多帧图像与匹配文本，且在长时序一致性、组合性与高效条件化方面受限（如需要拼接全部历史、缺乏明确的规划）。作者希望构建一个能进行多条件推理、序列规划、并兼顾效率与可控性的统一模型。

Method: 在 Bagel 统一模型上做全参数微调，提出交错架构：在序列中交替放置文本与视觉嵌入以进行多条件推理与顺序规划；引入语言规划策略，将用户指令分解为步骤化提示与帧嵌入以引导时序一致合成；对每一帧，仅条件化于少量采样的先前帧与全局文本上下文，而非拼接全部历史，从而实现可控且高效的长程生成。并构建5万规模的交错教程数据集用于训练与评测。

Result: 在风格迁移、组合生成与教程式过程等任务上，Loom 在组合性、时序一致性与文图对齐方面优于基线；在文本到交错生成基准上，相比开源基线 Anole 平均提升2.6分（满分5分）的时序与语义指标；在新构建的50K交错教程数据上，对比统一模型与扩散编辑基线表现更强。

Conclusion: 交错文本-图像生成可通过统一的扩散-Transformer 与语言规划有效实现。通过交错嵌入与稀疏历史条件化，Loom 在保持可控性的同时显著提升长时序质量与效率，并建立了新的数据与基线对比，为后续多模态序列生成提供了通用范式。

Abstract: Interleaved text-image generation aims to jointly produce coherent visual frames and aligned textual descriptions within a single sequence, enabling tasks such as style transfer, compositional synthesis, and procedural tutorials. We present Loom, a unified diffusion-transformer framework for interleaved text-image generation. Loom extends the Bagel unified model via full-parameter fine-tuning and an interleaved architecture that alternates textual and visual embeddings for multi-condition reasoning and sequential planning. A language planning strategy first decomposes a user instruction into stepwise prompts and frame embeddings, which guide temporally consistent synthesis. For each frame, Loom conditions on a small set of sampled prior frames together with the global textual context, rather than concatenating all history, yielding controllable and efficient long-horizon generation. Across style transfer, compositional generation, and tutorial-like procedures, Loom delivers superior compositionality, temporal coherence, and text-image alignment. Experiments demonstrate that Loom substantially outperforms the open-source baseline Anole, achieving an average gain of 2.6 points (on a 5-point scale) across temporal and semantic metrics in text-to-interleaved tasks. We also curate a 50K interleaved tutorial dataset and demonstrate strong improvements over unified and diffusion editing baselines.

</details>


### [31] [Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks](https://arxiv.org/abs/2512.18264)
*Yucheng Fan,Jiawei Chen,Yu Tian,Zhaoxia Yin*

Main category: cs.CV

TL;DR: 提出一种在视觉一致性约束下联合优化“隐私抑制+可用性保留”的图像保护方法，并发布VPI-COCO基准，以公平评估对VLM属性推断攻击的防护。实验显示在多种VLM上可将隐私回答率降至<25%，非隐私回答率保持>88%，且对未见/改写问题具备泛化。


<details>
  <summary>Details</summary>
Motivation: VLM在社交媒体场景普及，使攻击者可通过图像推断敏感属性（属性推断攻击），造成隐私风险。现有防护往往牺牲图像质量或破坏平台视觉功能，难以平衡隐私保护与用户体验，同时缺少公开评测数据集，比较不公平。

Method: 提出一种保护框架：在视觉一致性约束下，联合优化隐私抑制（降低对隐私问题的可回答性）与实用性保留（维持非隐私任务性能）。并构建VPI-COCO数据集，包含522张图像、分层隐私问题及对应非隐私问题，用于细粒度、联合评测隐私与体验。

Result: 在多个VLM上评测，方法将隐私回答率（PAR）降至25%以下，非隐私回答率（NPAR）保持在88%以上，同时维持高视觉一致性；对未见和同义改写的隐私问题仍有效。

Conclusion: 该方法在不显著损害用户体验的前提下有效抑制VLM的隐私推断，且在公开基准VPI-COCO上验证了稳健与泛化，具有实际部署潜力。

Abstract: As vision-language models (VLMs) become widely adopted, VLM-based attribute inference attacks have emerged as a serious privacy concern, enabling adversaries to infer private attributes from images shared on social media. This escalating threat calls for dedicated protection methods to safeguard user privacy. However, existing methods often degrade the visual quality of images or interfere with vision-based functions on social media, thereby failing to achieve a desirable balance between privacy protection and user experience. To address this challenge, we propose a novel protection method that jointly optimizes privacy suppression and utility preservation under a visual consistency constraint. While our method is conceptually effective, fair comparisons between methods remain challenging due to the lack of publicly available evaluation datasets. To fill this gap, we introduce VPI-COCO, a publicly available benchmark comprising 522 images with hierarchically structured privacy questions and corresponding non-private counterparts, enabling fine-grained and joint evaluation of protection methods in terms of privacy preservation and user experience. Building upon this benchmark, experiments on multiple VLMs demonstrate that our method effectively reduces PAR below 25%, keeps NPAR above 88%, maintains high visual consistency, and generalizes well to unseen and paraphrased privacy questions, demonstrating its strong practical applicability for real-world VLM deployments.

</details>


### [32] [Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System](https://arxiv.org/abs/2512.18269)
*Se-Young Jang,Su-Yeon Yoon,Jae-Woong Jung,Dong-Hun Lee,Seong-Hun Choi,Soo-Kyung Jun,Yu-Bin Kim,Young-Seon Ju,Kyounggon Kim*

Main category: cs.CV

TL;DR: 提出一个基于YOLO的可视化暗黑模式实时检测框架，构建并开放标注数据集，在mAP@50=92.8%、40.5 FPS下实现高精度与实时性。


<details>
  <summary>Details</summary>
Motivation: 随着企业平台设计愈发复杂，暗黑模式损害用户知情与理性选择的社会技术问题突出；监管多为事后被动手段，亟需前瞻、可实时的自动检测技术。

Method: 手动从194个网站收集4,066张含暗黑模式的UI/UX截图，标注5类与暗黑模式相关的UI组件（按钮、复选框、输入框、弹窗、二维码），公开数据集；采用YOLOv12x目标检测模型，使用迁移学习进行微调以适配视觉暗黑模式识别，并评估准确率与速度。

Result: 在该数据集上，模型达到mAP@50为92.8%，推理速度40.5 FPS，兼顾精度与实时性；数据集已在GitHub公开。

Conclusion: 所提框架可在在线环境中实现高效实时的暗黑模式视觉检测，为监管与平台治理提供可用工具；公开数据集有助于推动后续研究与技术发展。

Abstract: With the accelerating pace of digital transformation and the widespread adoption of online platforms, both social and technical concerns regarding dark patterns-user interface designs that undermine users' ability to make informed and rational choices-have become increasingly prominent. As corporate online platforms grow more sophisticated in their design strategies, there is a pressing need for proactive and real-time detection technologies that go beyond the predominantly reactive approaches employed by regulatory authorities. In this paper, we propose a visual dark pattern detection framework that improves both detection accuracy and real-time performance. To this end, we constructed a proprietary visual object detection dataset by manually collecting 4,066 UI/UX screenshots containing dark patterns from 194 websites across six major industrial sectors in South Korea and abroad. The collected images were annotated with five representative UI components commonly associated with dark patterns: Button, Checkbox, Input Field, Pop-up, and QR Code. This dataset has been publicly released to support further research and development in the field. To enable real-time detection, this study adopted the YOLOv12x object detection model and applied transfer learning to optimize its performance for visual dark pattern recognition. Experimental results demonstrate that the proposed approach achieves a high detection accuracy of 92.8% in terms of mAP@50, while maintaining a real-time inference speed of 40.5 frames per second (FPS), confirming its effectiveness for practical deployment in online environments. Furthermore, to facilitate future research and contribute to technological advancements, the dataset constructed in this study has been made publicly available at https://github.com/B4E2/B4E2-DarkPattern-YOLO-DataSet.

</details>


### [33] [UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations](https://arxiv.org/abs/2512.18279)
*Zhangshuo Qi,Jingyi Xu,Luqi Cheng,Shichen Wen,Yiming Ma,Guangming Xiong*

Main category: cs.CV

TL;DR: UniMPR提出一个统一的多模态地点识别框架，用单一模型适配任意相机/激光雷达/雷达组合，在极坐标BEV空间对齐特征，并通过多分支网络与大规模跨数据预训练实现对缺失/退化模态和不同传感器配置的鲁棒泛化，在七个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态地点识别方法难以：1) 在统一框架内动态适配任意模态组合；2) 面对缺失或退化模态保持鲁棒；3) 跨多样传感器配置与环境条件泛化。需要一种单模型即可统一处理多模态并具备强泛化与鲁棒性的方案。

Method: 将不同传感器（相机、LiDAR、雷达）输入统一映射到极坐标BEV特征空间，缓解模态异质性；在此基础上设计多分支网络，提取并融合任意模态组合的区分性“模内/模间”特征；构建跨多个数据集的大规模训练集，并引入自适应标签分配策略进行广泛预训练，以提升泛化与鲁棒能力。

Result: 在七个数据集上进行评测，面对不同传感器配置、模态组合与环境变化，UniMPR整体取得SOTA性能，表现出对缺失/退化模态的稳健性与强泛化能力。

Conclusion: 统一到极坐标BEV的特征对齐与多分支融合架构，加上跨数据大规模预训练与自适应标签分配，使单一模型即可适配任意模态组合并在多场景下达SOTA；为多模态地点识别提供了鲁棒、通用的解决方案。

Abstract: Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments. Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities. Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to arbitrary modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups. In this paper, we propose UniMPR, a unified framework for multimodal place recognition. Using only one trained model, it can seamlessly adapt to any combination of common perceptual modalities (e.g., camera, LiDAR, radar). To tackle the data heterogeneity, we unify all inputs within a polar BEV feature space. Subsequently, the polar BEVs are fed into a multi-branch network to exploit discriminative intra-model and inter-modal features from any modality combinations. To fully exploit the network's generalization capability and robustness, we construct a large-scale training set from multiple datasets and introduce an adaptive label assignment strategy for extensive pre-training. Experiments on seven datasets demonstrate that UniMPR achieves state-of-the-art performance under varying sensor configurations, modality combinations, and environmental conditions. Our code will be released at https://github.com/QiZS-BIT/UniMPR.

</details>


### [34] [Pyramidal Adaptive Cross-Gating for Multimodal Detection](https://arxiv.org/abs/2512.18291)
*Zidong Gu,Shoufu Tian*

Main category: cs.CV

TL;DR: 提出PACGNet，通过对称交叉门控与金字塔特征感知门控，在主干内实现深度多模态融合，抑噪并保持特征层级，从而显著提升航拍目标检测（mAP50在DroneVehicle和VEDAI分别达81.7%与82.1%）。


<details>
  <summary>Details</summary>
Motivation: 现有多模态航拍目标检测多用简单特征融合，易引入跨模态噪声并破坏特征金字塔的层级结构，导致小目标细粒度检测受损。需要一种能在融合中抑噪、保持语义完整和层级细节的架构。

Method: 在主干中进行深度融合，提出两模块：1) 对称交叉门控（SCG），通过双向、对称的“水平”门控选择性吸收互补信息、抑制噪声，保持各模态语义完整；2) 金字塔特征感知多模态门控（PFMG），采用逐级的层级门控，用上一级高分辨率细节引导当前低分辨率层的融合，重建并维护特征金字塔层级，保留细节传播。

Result: 在DroneVehicle与VEDAI数据集上取得新的SOTA，mAP50分别为81.7%与82.1%。

Conclusion: PACGNet通过SCG与PFMG在主干内实现鲁棒的跨模态深度融合，有效抑制噪声并保持金字塔层级与细粒度信息，从而提升航拍小目标检测性能并刷新基准。

Abstract: Object detection in aerial imagery is a critical task in applications such as UAV reconnaissance. Although existing methods have extensively explored feature interaction between different modalities, they commonly rely on simple fusion strategies for feature aggregation. This introduces two critical flaws: it is prone to cross-modal noise and disrupts the hierarchical structure of the feature pyramid, thereby impairing the fine-grained detection of small objects. To address this challenge, we propose the Pyramidal Adaptive Cross-Gating Network (PACGNet), an architecture designed to perform deep fusion within the backbone. To this end, we design two core components: the Symmetrical Cross-Gating (SCG) module and the Pyramidal Feature-aware Multimodal Gating (PFMG) module. The SCG module employs a bidirectional, symmetrical "horizontal" gating mechanism to selectively absorb complementary information, suppress noise, and preserve the semantic integrity of each modality. The PFMG module reconstructs the feature hierarchy via a progressive hierarchical gating mechanism. This leverages the detailed features from a preceding, higher-resolution level to guide the fusion at the current, lower-resolution level, effectively preserving fine-grained details as features propagate. Through evaluations conducted on the DroneVehicle and VEDAI datasets, our PACGNet sets a new state-of-the-art benchmark, with mAP50 scores reaching 81.7% and 82.1% respectively.

</details>


### [35] [MatE: Material Extraction from Single-Image via Geometric Prior](https://arxiv.org/abs/2512.18312)
*Zeyu Zhang,Wei Zhai,Jian Yang,Yang Cao*

Main category: cs.CV

TL;DR: MatE 从单张非受控照片与掩膜出发，经几何粗整形+双分支扩散模型，生成可平铺的完整PBR材质（albedo/normal/roughness/height），对未知光照与视角具鲁棒不变性。


<details>
  <summary>Details</summary>
Motivation: 真实感渲染需要高质量PBR材质，但传统获取流程昂贵、繁琐，依赖专用设备与专家处理；希望让普通用户也能从随手拍得到可用且可平铺的PBR材质。

Method: 1) 输入：单张图像与用户掩膜。2) 粗整形：估计深度图作为几何先验，对输入做透视与几何畸变校正。3) 双分支扩散模型：基于旋转对齐与尺度对齐的数据训练，学习一致性；在粗结果上进一步消除残余畸变，并同时翻译生成材质贴图（albedo、法线、粗糙度、高度）。4) 设计使模型对未知光照与视角具不变性，实现从成像到固有属性的分解。

Result: 在合成与真实数据上进行广泛实验，生成的材质可平铺、细节完整，具备对实际拍摄条件（光照、视角）的鲁棒性；重建出的PBR属性能驱动真实感渲染。

Conclusion: MatE将随手拍转化为高保真、可平铺的PBR材质，降低材质制作门槛；通过几何先验+双分支扩散与对齐训练，达成对光照与透视的不变性并稳健恢复固有材质属性。

Abstract: The creation of high-fidelity, physically-based rendering (PBR) materials remains a bottleneck in many graphics pipelines, typically requiring specialized equipment and expert-driven post-processing. To democratize this process, we present MatE, a novel method for generating tileable PBR materials from a single image taken under unconstrained, real-world conditions. Given an image and a user-provided mask, MatE first performs coarse rectification using an estimated depth map as a geometric prior, and then employs a dual-branch diffusion model. Leveraging a learned consistency from rotation-aligned and scale-aligned training data, this model further rectify residual distortions from the coarse result and translate it into a complete set of material maps, including albedo, normal, roughness and height. Our framework achieves invariance to the unknown illumination and perspective of the input image, allowing for the recovery of intrinsic material properties from casual captures. Through comprehensive experiments on both synthetic and real-world data, we demonstrate the efficacy and robustness of our approach, enabling users to create realistic materials from real-world image.

</details>


### [36] [MatSpray: Fusing 2D Material World Knowledge on 3D Geometry](https://arxiv.org/abs/2512.18314)
*Philipp Langsteiner,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 提出一个将2D扩散模型预测的PBR材质图（反照率/粗糙度/金属度）融合到3D重建几何中的框架，结合高斯Splatting重建、基于投影与基于图像优化的融合，以及轻量神经细化（Neural Merger），实现更准确、可重光照的写实渲染，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建虽能还原几何与外观，但缺乏精准的空间可变材质参数，导致重光照表现差；而2D扩散模型能预测PBR属性，却难以稳定移植到3D几何上。行业中手工建模材质与几何耗时，迫切需要自动、准确、可重光照的资产生成流程。

Method: 1) 用Gaussian Splatting重建场景几何；2) 采用任意可从图像/视频输出PBR属性的扩散模型生成2D材质图（albedo/roughness/metallic）；3) 两种融合策略：a) 基于图像的损失优化，将预测材质与渲染图像对齐；b) 基于高斯光线追踪的直接投影，将材质参数投到高斯表示上；4) 提出Neural Merger轻量神经细化模块，以光线追踪得到的材质特征为输入，进行细节增强与多视角一致性校正。

Result: 在定量指标与主观视觉真实感上均优于现有方法；得到更准确的材质参数与细节，并在多视角下保持一致，支持高质量可重光照渲染。

Conclusion: 融合2D扩散材质预测与3D高斯表示的学习/投影一体化框架，加上轻量神经细化，可显著提升重光照与写实度，提升资产制作效率，适合内容生产流水线落地。

Abstract: Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.

</details>


### [37] [A two-stream network with global-local feature fusion for bone age assessment](https://arxiv.org/abs/2512.18331)
*Qiong Lou,Han Yang,Fang Lu*

Main category: cs.CV

TL;DR: 提出BoNet+双流模型，结合Transformer的全局通道与RFAConv的局部通道，并用Inception-V3融合，RSNA与RHPE上MAE分别为3.81与5.65个月，达SOTA水平，提升自动骨龄评估的准确性与客观性。


<details>
  <summary>Details</summary>
Motivation: 现有骨龄评估深度模型在兼顾全局骨架形态与局部骨骼细节方面存在权衡难题，影响预测精度与临床可用性。作者希望通过结构性改进同时强化全局与局部特征表征，提高自动化评估的准确性、稳定性与客观性，减轻临床工作负担。

Method: 构建两条特征提取分支的BoNet+：1) 全局分支引入Transformer，利用多头自注意力获取长程依赖的全局表征；2) 局部分支采用RFAConv，在多尺度感受野内生成自适应注意力图，增强局部细节捕获；3) 将全局与局部特征在通道维拼接后，交由Inception-V3进行融合优化与回归，输出骨龄。

Result: 在RSNA与RHPE测试集上分别达到3.81与5.65个月的MAE，性能与当前SOTA相当，显示出较高的精度与泛化能力。

Conclusion: 双流架构有效平衡全局与局部信息，借助Transformer与RFAConv的互补优势，提高骨龄评估准确度并具备临床应用潜力，可减轻医生负担、提升评估客观性。

Abstract: Bone Age Assessment (BAA) is a widely used clinical technique that can accurately reflect an individual's growth and development level, as well as maturity. In recent years, although deep learning has advanced the field of bone age assessment, existing methods face challenges in efficiently balancing global features and local skeletal details. This study aims to develop an automated bone age assessment system based on a two-stream deep learning architecture to achieve higher accuracy in bone age assessment. We propose the BoNet+ model incorporating global and local feature extraction channels. A Transformer module is introduced into the global feature extraction channel to enhance the ability in extracting global features through multi-head self-attention mechanism. A RFAConv module is incorporated into the local feature extraction channel to generate adaptive attention maps within multiscale receptive fields, enhancing local feature extraction capabilities. Global and local features are concatenated along the channel dimension and optimized by an Inception-V3 network. The proposed method has been validated on the Radiological Society of North America (RSNA) and Radiological Hand Pose Estimation (RHPE) test datasets, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively. These results are comparable to the state-of-the-art. The BoNet+ model reduces the clinical workload and achieves automatic, high-precision, and more objective bone age assessment.

</details>


### [38] [MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation](https://arxiv.org/abs/2512.18344)
*Zhiheng Zhang,Jiajun Yang,Hong Sun,Dong Wang,Honghua Jiang,Yaru Chen,Tangyuan Ning*

Main category: cs.CV

TL;DR: 提出MCVI-SANet，一种轻量级半监督模型，利用VI饱和感知模块与VICReg策略，在冬小麦LAI与SPAD估计上显著提升准确性与泛化，并保持极低参数量与高推理速度。


<details>
  <summary>Details</summary>
Motivation: VI在致密冠层阶段饱和、冬小麦标注样本稀缺，导致LAI与SPAD估计偏差大；传统基于VI或纹理的ML特征表达力不足；常规深度学习模型对域移和数据量敏感，泛化差。

Method: 提出MCVI-SANet：1) 设计VI饱和感知模块VI-SABlock，进行自适应通道-空间特征增强，缓解VI饱和带来的信息损失；2) 引入基于VICReg的不变性正则的半监督策略，利用未标注数据提升鲁棒性与泛化；3) 采用基于植株高度的分割策略划分数据集，确保各生育期代表性；4) 以轻量级网络实现仅0.10M参数的高效推理。

Result: 10次重复实验中，LAI估计R²=0.8123、RMSE=0.4796；SPAD估计R²=0.6846、RMSE=2.4222。相较最佳基线，平均R²提升：LAI+8.95%、SPAD+8.17%。同时保持快速推理与极小模型规模（0.10M参数）。

Conclusion: 将半监督学习与农学先验（VI饱和感知、基于高度的数据划分）融合，可在有限标注与域差条件下提升遥感精准农业指标估计的准确性与泛化；MCVI-SANet在LAI与SPAD估计上达到SOTA且高效，具备实际应用潜力。

Abstract: Vegetation index (VI) saturation during the dense canopy stage and limited ground-truth annotations of winter wheat constrain accurate estimation of LAI and SPAD. Existing VI-based and texture-driven machine learning methods exhibit limited feature expressiveness. In addition, deep learning baselines suffer from domain gaps and high data demands, which restrict their generalization. Therefore, this study proposes the Multi-Channel Vegetation Indices Saturation Aware Net (MCVI-SANet), a lightweight semi-supervised vision model. The model incorporates a newly designed Vegetation Index Saturation-Aware Block (VI-SABlock) for adaptive channel-spatial feature enhancement. It also integrates a VICReg-based semi-supervised strategy to further improve generalization. Datasets were partitioned using a vegetation height-informed strategy to maintain representativeness across growth stages. Experiments over 10 repeated runs demonstrate that MCVI-SANet achieves state-of-the-art accuracy. The model attains an average R2 of 0.8123 and RMSE of 0.4796 for LAI, and an average R2 of 0.6846 and RMSE of 2.4222 for SPAD. This performance surpasses the best-performing baselines, with improvements of 8.95% in average LAI R2 and 8.17% in average SPAD R2. Moreover, MCVI-SANet maintains high inference speed with only 0.10M parameters. Overall, the integration of semi-supervised learning with agronomic priors provides a promising approach for enhancing remote sensing-based precision agriculture.

</details>


### [39] [Enhancing 3D Semantic Scene Completion with a Refinement Module](https://arxiv.org/abs/2512.18363)
*Dunxing Zhang,Jiachen Lu,Han Yang,Lei Bao,Bo Song*

Main category: cs.CV

TL;DR: 提出ESSC-RM，一个可插拔的语义场景补全(SSC)增强与细化框架，通过在现有SSC模型后加入基于3D U-Net的预测噪声感知模块(PNAM)与体素级局部几何模块(VLGM)进行多尺度监督细化，在SemanticKITTI上为多种基线带来稳定mIoU提升（如CGFormer从16.87%到17.27%，MonoScene从11.08%到11.51%），显示其通用可用性。


<details>
  <summary>Details</summary>
Motivation: 现有SSC模型常产生粗糙、含噪的体素预测，难以兼顾全局语义与局部几何细节，需要一种普适、轻量、可无缝集成的后端细化机制来提升语义与几何一致性和总体mIoU。

Method: 搭建一个两阶段框架：1) 任意现有SSC基线先输出粗体素语义；2) 通过3D U-Net构建的预测噪声感知模块(PNAM)与体素级局部几何模块(VLGM)对粗预测进行细化；在多尺度监督下联合优化，以抑制噪声并增强局部几何表示。该框架为即插即用、与模型无关的后处理/细化模块。

Result: 在SemanticKITTI上作为后置细化器接入不同SSC模型，均得到稳定提升：CGFormer mIoU由16.87%→17.27%，MonoScene由11.08%→11.51%。

Conclusion: ESSC-RM是通用的SSC细化框架，可与多种基线无缝集成，通过PNAM与VLGM在多尺度监督下细化体素预测，带来稳定但幅度较小的mIoU增益，验证其模型无关性与实用性。

Abstract: We propose ESSC-RM, a plug-and-play Enhancing framework for Semantic Scene Completion with a Refinement Module, which can be seamlessly integrated into existing SSC models. ESSC-RM operates in two phases: a baseline SSC network first produces a coarse voxel prediction, which is subsequently refined by a 3D U-Net-based Prediction Noise-Aware Module (PNAM) and Voxel-level Local Geometry Module (VLGM) under multiscale supervision. Experiments on SemanticKITTI show that ESSC-RM consistently improves semantic prediction performance. When integrated into CGFormer and MonoScene, the mean IoU increases from 16.87% to 17.27% and from 11.08% to 11.51%, respectively. These results demonstrate that ESSC-RM serves as a general refinement framework applicable to a wide range of SSC models.

</details>


### [40] [Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance](https://arxiv.org/abs/2512.18365)
*Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: 提出一种新的零样本扩散编辑方法，用更高效的似然代理替代需反向传播的做法，避免向量-雅可比乘、显著降耗，同时保持与观测一致的高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有零样本图像编辑利用预训练扩散模型，但要通过一串代理似然并在每个反向步都做经由去噪器的VJP，导致显存与时间开销大。需要一种无需在推理时反传、仍能保证与观测区域一致性的策略。

Method: 设计新的似然代理，使反向扩散中的后验转移为易采样的高斯形式，从而在每一步只需前向计算，不用对去噪网络做向量-雅可比乘或反传；整体形成简单高效的采样过程以实现修复与局部编辑。

Result: 在大量实验中，相比需要微调的基线，方法具有更强的观测一致性与连贯性，生成质量高；同时显著降低推理时的显存与运行开销。

Conclusion: 通过高斯后验转移的似然代理，可在零样本扩散编辑中兼顾质量与效率，避免昂贵的反向传播，达到强一致性与低成本的重建。

Abstract: Diffusion models have emerged as powerful priors for image editing tasks such as inpainting and local modification, where the objective is to generate realistic content that remains consistent with observed regions. In particular, zero-shot approaches that leverage a pretrained diffusion model, without any retraining, have been shown to achieve highly effective reconstructions. However, state-of-the-art zero-shot methods typically rely on a sequence of surrogate likelihood functions, whose scores are used as proxies for the ideal score. This procedure however requires vector-Jacobian products through the denoiser at every reverse step, introducing significant memory and runtime overhead. To address this issue, we propose a new likelihood surrogate that yields simple and efficient to sample Gaussian posterior transitions, sidestepping the backpropagation through the denoiser network. Our extensive experiments show that our method achieves strong observation consistency compared with fine-tuned baselines and produces coherent, high-quality reconstructions, all while significantly reducing inference cost. Code is available at https://github.com/YazidJanati/ding.

</details>


### [41] [RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion](https://arxiv.org/abs/2512.18386)
*Wenhao Hu,Haonan Zhou,Zesheng Li,Liu Liu,Jiacheng Dong,Zhizhong Su,Gaoang Wang*

Main category: cs.CV

TL;DR: RecurGS 提出一种递归融合的高斯场景表示，能在多次离散变化中持续更新，同步保持历史结构，实现交互与新状态合成，并显著提升更新效率与重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高保真新视角合成方法难以应对场景的离散状态变化与交互；单场景更新或一次一态的扩散解耦无法跨观测融合信息，易遗忘历史、更新低效。

Method: 提出 RecurGS：1) 跨相邻状态检测物体级变化；2) 通过语义对应与基于李代数的 SE(3) 精配准对齐几何运动；3) 递归更新并用重放监督保留历史结构；4) 体素化、可见性感知的融合模块只吸收新可见区域并冻结稳定区域，缓解灾难性遗忘、支持长时程高效更新；并支持对象级操控与无需额外扫描的状态合成。

Result: 在合成与真实数据集上实现高质量重建与新状态合成，相比以往方法显著提升更新效率并维持写实逼真度与一致性。

Conclusion: RecurGS 将多次场景状态增量整合为单一可交互的高斯表示，可无额外扫描合成新状态并稳定跨时演化，是迈向持续可交互高斯世界的可扩展方案。

Abstract: Recent advances in 3D scene representations have enabled high-fidelity novel view synthesis, yet adapting to discrete scene changes and constructing interactive 3D environments remain open challenges in vision and robotics. Existing approaches focus solely on updating a single scene without supporting novel-state synthesis. Others rely on diffusion-based object-background decoupling that works on one state at a time and cannot fuse information across multiple observations. To address these limitations, we introduce RecurGS, a recurrent fusion framework that incrementally integrates discrete Gaussian scene states into a single evolving representation capable of interaction. RecurGS detects object-level changes across consecutive states, aligns their geometric motion using semantic correspondence and Lie-algebra based SE(3) refinement, and performs recurrent updates that preserve historical structures through replay supervision. A voxelized, visibility-aware fusion module selectively incorporates newly observed regions while keeping stable areas fixed, mitigating catastrophic forgetting and enabling efficient long-horizon updates. RecurGS supports object-level manipulation, synthesizes novel scene states without requiring additional scans, and maintains photorealistic fidelity across evolving environments. Extensive experiments across synthetic and real-world datasets demonstrate that our framework delivers high-quality reconstructions with substantially improved update efficiency, providing a scalable step toward continuously interactive Gaussian worlds.

</details>


### [42] [Automated Mosaic Tesserae Segmentation via Deep Learning Techniques](https://arxiv.org/abs/2512.18406)
*Charilaos Kapelonis,Marios Antonakakis,Konstantinos Politof,Aristomenis Antoniadis,Michalis Zervakis*

Main category: cs.CV

TL;DR: 论文提出利用经过微调的 SAM 2 对古代马赛克图像中的小块（tesserae）进行自动分割，并发布标注数据集；在自建测试集与既有基准上均显著优于基础 SAM 2 与先前方法。


<details>
  <summary>Details</summary>
Motivation: 马赛克作为珍贵文化遗产，因年代久远而易受损。为实现数字化保存，需要准确从背景中分割出马赛克小块。然而该领域公开数据有限、传统分割器泛化与效率受限，亟需强健、可泛化的自动分割方案与数据支撑。

Method: 采用 Meta 的基础模型 SAM 2 作为核心分割器：构建并标注马赛克图像数据集，对 SAM 2 进行针对性微调；在自建测试集与既有基准上进行定量评估（IoU、Recall、F-measure、绝对差误差等），与原始 SAM 2 及前人方法比较。

Result: 在自建测试集上，IoU 从 89.00% 提升至 91.02%，Recall 从 92.12% 提升至 95.89%；在前人基准上，F-measure 提升约 3%，预测与真实小块数量的绝对差误差由 0.20 降至 0.02。

Conclusion: 微调后的 SAM 2 在马赛克小块分割任务上显著优于基线与既有方法；结合新标注数据集，为马赛克图像实时分割与更广泛的文化遗产数字化提供基础与潜力。

Abstract: Art is widely recognized as a reflection of civilization and mosaics represent an important part of cultural heritage. Mosaics are an ancient art form created by arranging small pieces, called tesserae, on a surface using adhesive. Due to their age and fragility, they are prone to damage, highlighting the need for digital preservation. This paper addresses the problem of digitizing mosaics by segmenting the tesserae to separate them from the background within the broader field of Image Segmentation in Computer Vision. We propose a method leveraging Segment Anything Model 2 (SAM 2) by Meta AI, a foundation model that outperforms most conventional segmentation models, to automatically segment mosaics. Due to the limited open datasets in the field, we also create an annotated dataset of mosaic images to fine-tune and evaluate the model. Quantitative evaluation on our testing dataset shows notable improvements compared to the baseline SAM 2 model, with Intersection over Union increasing from 89.00% to 91.02% and Recall from 92.12% to 95.89%. Additionally, on a benchmark proposed by a prior approach, our model achieves an F-measure 3% higher than previous methods and reduces the error in the absolute difference between predicted and actual tesserae from 0.20 to just 0.02. The notable performance of the fine-tuned SAM 2 model together with the newly annotated dataset can pave the way for real-time segmentation of mosaic images.

</details>


### [43] [Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval](https://arxiv.org/abs/2512.18407)
*Dimitrios Georgoulopoulos,Nikolaos Chaidos,Angeliki Dimitriou,Giorgos Stamou*

Main category: cs.CV

TL;DR: 提出PRISm框架：用重要性预测+边感知图神经网络，保留关键对象/关系并编码语义结构，得到更符合人类感知的图像检索；在基准与真实数据上顶排表现更优且可解释。


<details>
  <summary>Details</summary>
Motivation: 传统图像检索难以捕获场景中对象间关系与语境，导致与人类语义相似度不一致。需要显式建模对象及交互的重要性与结构，以提升语义对齐与可解释性。

Method: 1) 重要性预测模块：在语义图上评估对象与三元组的重要性，剪枝无关元素，仅保留关键信息。2) 边感知GNN：显式编码关系结构，并融合全局视觉特征，生成语义感知的图像嵌入。整体为多模态框架，将关系推理与视觉表征结合用于图像到图像检索。

Result: 在多个基准与真实世界数据集上，取得持续优于现有方法的前几名（top-ranked）检索性能；定性结果显示能准确捕获关键对象与交互，具备良好可解释性。

Conclusion: 通过显式建模对象/关系的重要性与结构，PRISm实现更符合人类感知的语义检索，兼具性能与可解释性，证明关系推理与视觉表示的结合对图像检索有效。

Abstract: Accurately retrieving images that are semantically similar remains a fundamental challenge in computer vision, as traditional methods often fail to capture the relational and contextual nuances of a scene. We introduce PRISm (Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs), a multimodal framework that advances image-to-image retrieval through two novel components. First, the Importance Prediction Module identifies and retains the most critical objects and relational triplets within an image while pruning irrelevant elements. Second, the Edge-Aware Graph Neural Network explicitly encodes relational structure and integrates global visual features to produce semantically informed image embeddings. PRISm achieves image retrieval that closely aligns with human perception by explicitly modeling the semantic importance of objects and their interactions, capabilities largely absent in prior approaches. Its architecture effectively combines relational reasoning with visual representation, enabling semantically grounded retrieval. Extensive experiments on benchmark and real-world datasets demonstrate consistently superior top-ranked performance, while qualitative analyses show that PRISm accurately captures key objects and interactions, producing interpretable and semantically meaningful results.

</details>


### [44] [AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning](https://arxiv.org/abs/2512.18411)
*Fei Song,Yi Li,Jiangmeng Li,Rui Wang,Changwen Zheng,Fanjiang Xu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出AmPLe：一种自适应去偏的多提示集成方法，解决跨视觉-语言模型与样本的提示匹配偏差，通过信息论指导提取提示相关语义并自适应加权集成，多任务上显著优于现有方法，并给出因果理论支撑。


<details>
  <summary>Details</summary>
Motivation: 多提示学习在低资源下快速适配视觉-语言模型有效，但同一提示在不同基础模型中语义不一致（模型-提示匹配偏差），且输入样本含有与提示无关的语义会干扰集成权重（样本-提示匹配偏差）。现有方法忽视这些偏差，导致预测不一致与权重估计次优。

Method: 采用集成学习汇聚多个VLM与多提示的多样化预测；通过信息论分析从输入中提取与提示相关的语义特征，抑制提示无关信息；基于提取的提示相关语义自适应计算去偏的集成权重，从而在预测层面与权重层面同时缓解两类偏差。并提供因果视角的理论论证。

Result: 在三类代表性设置（新类别泛化、新数据集迁移、未见域偏移鲁棒性）上，AmPLe普遍优于现有多提示/适配方法，取得显著性能提升；实验广泛验证了方法的有效性。

Conclusion: 针对多提示学习中的模型-提示与样本-提示匹配偏差，AmPLe以信息论指导的自适应去偏集成框架同时缓解两者，实现更稳健且泛化性更强的VLM适配，并得到因果理论的支持。

Abstract: Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble MultiPrompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe.

</details>


### [45] [E-RGB-D: Real-Time Event-Based Perception with Structured Light](https://arxiv.org/abs/2512.18429)
*Seyed Ehsan Marjani Bajestani,Giovanni Beltrame*

Main category: cs.CV

TL;DR: 提出将事件相机与DLP投影组成主动结构光的RGB-D方案，实现彩色与深度的无帧高速感知：约1400 fps颜色与4 kHz像素级深度，生成高分辨率彩色点云。


<details>
  <summary>Details</summary>
Motivation: 传统单色事件相机虽具高动态范围、微秒级时间分辨率和低功耗，但难以感知静止或缓慢目标且缺乏颜色；许多任务（机器人、3D重建）需要颜色与深度的高时空分辨率融合。

Method: 用商业TI LightCrafter 4500 DLP投影仪主动投射结构光图样，与单目单色事件相机配合；通过动态调整投影以优化带宽，仅在需要处选择性获取颜色数据；将事件流解析为每像素的颜色与深度，构建无帧RGB-D感知并输出彩色点云。

Result: 实现等效约1400 fps的颜色检测速率与4 kHz像素级深度检测；在不牺牲空间分辨率的前提下获得彩色点云；系统以单个单色EC与商用DLP实现。代码开源。

Conclusion: EC与ASL的结合克服了单色EC对静止物体与颜色缺失的局限，实现超高速、低带宽的RGB-D感知，适用于机器人与3D重建等场景。

Abstract: Event-based cameras (ECs) have emerged as bio-inspired sensors that report pixel brightness changes asynchronously, offering unmatched speed and efficiency in vision sensing. Despite their high dynamic range, temporal resolution, low power consumption, and computational simplicity, traditional monochrome ECs face limitations in detecting static or slowly moving objects and lack color information essential for certain applications. To address these challenges, we present a novel approach that integrates a Digital Light Processing (DLP) projector, forming Active Structured Light (ASL) for RGB-D sensing. By combining the benefits of ECs and projection-based techniques, our method enables the detection of color and the depth of each pixel separately. Dynamic projection adjustments optimize bandwidth, ensuring selective color data acquisition and yielding colorful point clouds without sacrificing spatial resolution. This integration, facilitated by a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, not only enables frameless RGB-D sensing applications but also achieves remarkable performance milestones. With our approach, we achieved a color detection speed equivalent to 1400 fps and 4 kHz of pixel depth detection, significantly advancing the realm of computer vision across diverse fields from robotics to 3D reconstruction methods. Our code is publicly available: https://github.com/MISTLab/event_based_rgbd_ros

</details>


### [46] [MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading](https://arxiv.org/abs/2512.18437)
*Shurui Xu,Siqi Yang,Jiapin Ren,Zhong Cao,Hongwei Yang,Mengzhen Fan,Yuyu Sun,Shuyan Li*

Main category: cs.CV

TL;DR: 提出MeniMV多视角半月板前后角撕裂分级数据集（0-3级），含3,000次检查/750名患者、6,000配准的矢状-冠状图像，提供基线并揭示分级挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MRI自动分析多依赖粗粒度或二分类标签，缺乏解剖部位定位与严重度分级；临床诊断依赖矢状与冠状双视角，但公开数据与标注不足，制约研究与落地。

Method: 构建多中心（3家医院）多视角基准数据集MeniMV：为每次检查提供配准的矢状与冠状图像，对前后角分别由主任级骨科医师给出0-3级分级标注；规模较以往数据集翻倍以上。基于该数据集系统评测多种SOTA CNN与Transformer模型，建立分级基线。

Result: 形成覆盖750患者、3,000次检查、6,000张配准图像的高质量标注集；多模型基准实验给出强基线，同时暴露出严重度细粒度分级的难点。

Conclusion: MeniMV为半月板角部撕裂细粒度分级提供首个多视角大规模基准，体现临床双视角背景，能推动自动肌骨影像研究并为后续方法改进奠定基础。

Abstract: Precise grading of meniscal horn tears is critical in knee injury diagnosis but remains underexplored in automated MRI analysis. Existing methods often rely on coarse study-level labels or binary classification, lacking localization and severity information. In this paper, we introduce MeniMV, a multi-view benchmark dataset specifically designed for horn-specific meniscus injury grading. MeniMV comprises 3,000 annotated knee MRI exams from 750 patients across three medical centers, providing 6,000 co-registered sagittal and coronal images. Each exam is meticulously annotated with four-tier (grade 0-3) severity labels for both anterior and posterior meniscal horns, verified by chief orthopedic physicians. Notably, MeniMV offers more than double the pathology-labeled data volume of prior datasets while uniquely capturing the dual-view diagnostic context essential in clinical practice. To demonstrate the utility of MeniMV, we benchmark multiple state-of-the-art CNN and Transformer-based models. Our extensive experiments establish strong baselines and highlight challenges in severity grading, providing a valuable foundation for future research in automated musculoskeletal imaging.

</details>


### [47] [Object-Centric Framework for Video Moment Retrieval](https://arxiv.org/abs/2512.18448)
*Zongyao Li,Yongkang Wong,Satoshi Yamazaki,Jianquan Liu,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: 提出一个以对象为中心的视频片段检索框架：用场景图解析器提取与查询相关的对象与关系，构建对象级特征序列，并用关系tracklet Transformer建模时空关联与状态变化，从而更准确地定位与对象相关的时刻，在Charades-STA、QVHighlights、TACoS上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于帧/片段全局特征，难以捕获细粒度的对象语义、外观与其交互，尤其忽略对象层面的时序动态，导致对包含具体实体与交互的对象导向查询的定位效果不佳。

Method: 1) 用文本查询驱动的场景图解析器筛选相关对象与关系；2) 从视频帧生成场景图，得到对象与关系表征；3) 基于场景图构建对象级特征序列，编码视觉与语义信息；4) 通过“关系tracklet Transformer”建模对象跨时间的空间与语义关联，显式捕获对象状态变化；5) 将建模结果用于片段检索与时刻定位。

Result: 在Charades-STA、QVHighlights、TACoS三项基准上全面超越现有SOTA（摘要未给出具体数值）。

Conclusion: 对象中心、显式建模对象状态变化与关系的时空动态能显著提升面向对象查询的视频时刻定位 Accuracy，方法具有通用性并在多基准上验证有效。

Abstract: Most existing video moment retrieval methods rely on temporal sequences of frame- or clip-level features that primarily encode global visual and semantic information. However, such representations often fail to capture fine-grained object semantics and appearance, which are crucial for localizing moments described by object-oriented queries involving specific entities and their interactions. In particular, temporal dynamics at the object level have been largely overlooked, limiting the effectiveness of existing approaches in scenarios requiring detailed object-level reasoning. To address this limitation, we propose a novel object-centric framework for moment retrieval. Our method first extracts query-relevant objects using a scene graph parser and then generates scene graphs from video frames to represent these objects and their relationships. Based on the scene graphs, we construct object-level feature sequences that encode rich visual and semantic information. These sequences are processed by a relational tracklet transformer, which models spatio-temporal correlations among objects over time. By explicitly capturing object-level state changes, our framework enables more accurate localization of moments aligned with object-oriented queries. We evaluated our method on three benchmarks: Charades-STA, QVHighlights, and TACoS. Experimental results demonstrate that our method outperforms existing state-of-the-art methods across all benchmarks.

</details>


### [48] [Plasticine: A Traceable Diffusion Model for Medical Image Translation](https://arxiv.org/abs/2512.18455)
*Tianyang Zhanng,Xinxing Cheng,Jun Cheng,Shaoming Zheng,He Zhao,Huazhu Fu,Alejandro F Frangi,Jiang Liu,Jinming Duan*

Main category: cs.CV

TL;DR: 提出Plasticine：首个以“可溯源性（像素级对应）”为核心目标的端到端医学影像域间翻译框架，在扩散模型中联合强度映射与空间形变，实现可解释、空间一致的合成影像。


<details>
  <summary>Details</summary>
Motivation: 医学影像跨设备/人群导致域间差异，现有图像到图像翻译方法追求风格/外观匹配，常忽略空间对应，造成解剖尺度/形状随意变化，不利于临床可解释与追踪。临床需要对原图与译图的像素级对应关系（可溯源性）。

Method: 在去噪扩散框架中，将强度翻译（外观/对比度/纹理）与空间变形（形变场）联合建模：同时学习可解释的强度过渡与空间上连贯的形变，使译图与原图在像素层面保持可追踪映射。端到端训练，输出合成图及其像素对应关系。

Result: 可生成具有临床可解释的强度变化与空间一致形变的合成图像，并在整个翻译过程中提供像素级可追踪性（抽象中未给定具体指标，但强调实现了可溯源性与高质量合成）。

Conclusion: Plasticine首次把“可溯源性”作为医学影像域翻译的核心目标，在扩散模型内耦合强度与形变，兼顾合成质量与像素级对应，为跨域适配与临床解释性提供更可靠方案。

Abstract: Domain gaps arising from variations in imaging devices and population distributions pose significant challenges for machine learning in medical image analysis. Existing image-to-image translation methods primarily aim to learn mappings between domains, often generating diverse synthetic data with variations in anatomical scale and shape, but they usually overlook spatial correspondence during the translation process. For clinical applications, traceability, defined as the ability to provide pixel-level correspondences between original and translated images, is equally important. This property enhances clinical interpretability but has been largely overlooked in previous approaches. To address this gap, we propose Plasticine, which is, to the best of our knowledge, the first end-to-end image-to-image translation framework explicitly designed with traceability as a core objective. Our method combines intensity translation and spatial transformation within a denoising diffusion framework. This design enables the generation of synthetic images with interpretable intensity transitions and spatially coherent deformations, supporting pixel-wise traceability throughout the translation process.

</details>


### [49] [Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.18496)
*Xiaoyang Guo,Keze Wang*

Main category: cs.CV

TL;DR: 提出Adaptive-VoCo，为VLM按图像复杂度自适应压缩视觉patch token，降低计算与显存同时保持性能，优于固定压缩率基线。


<details>
  <summary>Details</summary>
Motivation: 现有VLM需处理高维视觉特征，计算/内存开销大。VoCo-LLaMA以视觉token压缩缓解但使用固定压缩率，难以适应不同图像复杂度，可能在简单场景浪费算力、复杂场景丢失关键信息。

Method: 在VoCo-LLaMA上加入轻量预测器，根据视觉编码器统计信号（如patch token熵、注意力图方差）估计图像视觉复杂度并动态选择压缩率；提出联合损失，将压缩率正则与复杂度对齐约束结合，以在效率与表示能力间权衡。

Result: 在多项多模态任务上，相比固定压缩率方法，Adaptive-VoCo取得一致性更优的性能，同时降低推理计算与显存占用，尤其在困难场景表现更稳健。

Conclusion: 自适应视觉压缩能在不显著牺牲跨模态对齐与推理能力的前提下降低VLM成本；基于复杂度的动态压缩较固定压缩更高效与鲁棒，具备推广潜力。

Abstract: In recent years, large-scale vision-language models (VLMs) have demonstrated remarkable performance on multimodal understanding and reasoning tasks. However, handling high-dimensional visual features often incurs substantial computational and memory costs. VoCo-LLaMA alleviates this issue by compressing visual patch tokens into a few VoCo tokens, reducing computational overhead while preserving strong cross-modal alignment. Nevertheless, such approaches typically adopt a fixed compression rate, limiting their ability to adapt to varying levels of visual complexity. To address this limitation, we propose Adaptive-VoCo, a framework that augments VoCo-LLaMA with a lightweight predictor for adaptive compression. This predictor dynamically selects an optimal compression rate by quantifying an image's visual complexity using statistical cues from the vision encoder, such as patch token entropy and attention map variance. Furthermore, we introduce a joint loss function that integrates rate regularization with complexity alignment. This enables the model to balance inference efficiency with representational capacity, particularly in challenging scenarios. Experimental results show that our method consistently outperforms fixed-rate baselines across multiple multimodal tasks, highlighting the potential of adaptive visual compression for creating more efficient and robust VLMs.

</details>


### [50] [PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs](https://arxiv.org/abs/2512.18500)
*Santwana Sagnika,Manav Malhotra,Ishtaj Kaur Deol,Soumyajit Roy,Swarnav Kumar*

Main category: cs.CV

TL;DR: 提出PlantDiseaseNet-RT50：基于ResNet50并经针对性微调的植物病害自动检测模型，在多作物多病害数据集上达到约98%准确率、精确率与召回率，具备高效、可部署的农业诊断价值。


<details>
  <summary>Details</summary>
Motivation: 植物病害导致巨额产量损失，而传统人工目检耗时费力、难以规模化；需要一种高准确且计算高效、可在实际农业场景部署的自动化检测方法。

Method: 以ResNet50为骨干进行迁移学习与针对性微调：选择性解冻末端层；自定义分类头并加入批归一化与Dropout等正则化；采用余弦退火（cosine decay）动态学习率调度；在涵盖多作物多病害类别的综合数据集上训练评估。

Result: 模型PlantDiseaseNet-RT50在数据集上取得约98%的准确率、精确率与召回率，展示出优异的检测性能与计算效率。

Conclusion: 通过对预训练模型进行有针对性的层解冻、正则化与优化策略，能将通用骨干转化为高性能、可落地的农业病害诊断工具；该方法有助于快速准确诊断、支持及时干预、降低作物损失。

Abstract: Plant diseases pose a significant threat to agricultural productivity and global food security, accounting for 70-80% of crop losses worldwide. Traditional detection methods rely heavily on expert visual inspection, which is time-consuming, labour-intensive, and often impractical for large-scale farming operations. In this paper, we present PlantDiseaseNet-RT50, a novel fine-tuned deep learning architecture based on ResNet50 for automated plant disease detection. Our model features strategically unfrozen layers, a custom classification head with regularization mechanisms, and dynamic learning rate scheduling through cosine decay. Using a comprehensive dataset of distinct plant disease categories across multiple crop species, PlantDiseaseNet-RT50 achieves exceptional performance with approximately 98% accuracy, precision, and recall. Our architectural modifications and optimization protocol demonstrate how targeted fine-tuning can transform a standard pretrained model into a specialized agricultural diagnostic tool. We provide a detailed account of our methodology, including the systematic unfreezing of terminal layers, implementation of batch normalization and dropout regularization and application of advanced training techniques. PlantDiseaseNet-RT50 represents a significant advancement in AI-driven agricultural tools, offering a computationally efficient solution for rapid and accurate plant disease diagnosis that can be readily implemented in practical farming contexts to support timely interventions and reduce crop losses.

</details>


### [51] [NASTaR: NovaSAR Automated Ship Target Recognition Dataset](https://arxiv.org/abs/2512.18503)
*Benyamin Hosseiny,Kamirul Kamirul,Odysseas Pappas,Alin Achim*

Main category: cs.CV

TL;DR: 提出NASTaR：一个面向NovaSAR S波段的船舶目标识别数据集，含3415个配准AIS标签的船舶小样本与辅助尾迹，覆盖23类与近/远海分割，并用基准深度模型验证在多种分类场景下达到60%–87%准确率。


<details>
  <summary>Details</summary>
Motivation: SAR在海事监测上具备全天时全天候优势，但船型多样、跨卫星频段与分辨率差异大，缺乏足量高质量标注数据制约了深度学习模型的鲁棒性与泛化。因此需要一个具代表性、公开可用、带精确标签的数据集来推动船型分类研究与基准评测。

Method: 构建NASTaR数据集：从NovaSAR S波段影像中裁剪船舶patch（共3415），通过与AIS数据匹配生成标签；设计23个船型类别，并提供近岸/离岸区分和可见尾迹的辅助子集。随后以主流深度学习模型在多种常见分类设定上进行基准实验评测。

Result: 在基准模型上，四大船型分类准确率>60%；三分类场景>70%；货轮与油轮二分类>75%；渔船识别>87%。同时公开数据集下载与基准代码仓库，便于复现与扩展。

Conclusion: NASTaR为S波段SAR船型识别提供了带AIS对齐标签、覆盖面广的公共基准，能支撑不同任务设定与研究比较；实验显示其对多类与特定二分类任务均具实用价值，可促进跨平台SAR船舶识别模型的发展与泛化。

Abstract: Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://10.5523/bris, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.

</details>


### [52] [GTMA: Dynamic Representation Optimization for OOD Vision-Language Models](https://arxiv.org/abs/2512.18504)
*Jensen Zhang,Ningyuan Liu,Keze Wang*

Main category: cs.CV

TL;DR: 提出GTMA，在推理时为OOD图像动态优化连续伪词嵌入，与视觉锚对齐，突破文本词表限制，显著提升零样本/小样本OOD性能（+15–20%），且不损伤ID表现。


<details>
  <summary>Details</summary>
Motivation: VLM在开放世界遇到OOD概念时，跨模态对齐崩塌，零样本性能大幅下降。根因在于模态不对称：视觉编码器可对未见图像提取判别特征，但文本编码器受限于固定离散词表，无法为新概念合成语义锚。现有CoOp/LoRA仍局限于预训练语义空间，缓解有限。

Method: 提出动态表示优化框架GTMA：在推理阶段为给定OOD图像构造可连续优化的“伪词”文本嵌入，使其与图像的视觉锚最大化对齐。采用自适应、基于梯度的表示策略优化算法，并加入语义正则，确保与模型先验兼容且语义可 plausibility。核心即目标匹配（target-matching）引导的文本表示连续优化，绕过词表限制。

Result: 在ImageNet-R与VISTA-Beyond上，零样本与小样本OOD准确率较基线VLM提升约15–20%，同时维持ID概念上的原有性能。消融实验证实伪词优化的必要性。

Conclusion: 通过在推理时对文本侧进行连续伪词优化，缓解模态不对称并避免跨模态对齐崩塌，可在不损伤ID表现的前提下显著提升OOD泛化。

Abstract: Vision-language models (VLMs) struggle in open-world applications, where out-of-distribution (OOD) concepts often trigger cross-modal alignment collapse and severely degrade zero-shot performance. We identify the root cause as modal asymmetry: while the visual encoder can extract discriminative features from unseen images, the text encoder is constrained by a fixed discrete vocabulary and cannot synthesize new semantic anchors. Existing approaches such as CoOp or LoRA provide only partial remedies, as they remain confined to the pre-trained semantic space.
  To overcome this bottleneck, we propose dynamic representation optimization, realized through the Guided Target-Matching Adaptation (GTMA) framework. At inference time, GTMA constructs a continuous pseudo-word embedding that best aligns with an OOD image's visual anchor, effectively bypassing vocabulary limitations. The optimization is driven by an adaptive gradient-based representation policy optimization algorithm, which incorporates semantic regularization to preserve plausibility and compatibility with the model's prior knowledge.
  Experiments on ImageNet-R and the VISTA-Beyond benchmark demonstrate that GTMA improves zero-shot and few-shot OOD accuracy by up to 15-20 percent over the base VLM while maintaining performance on in-distribution concepts. Ablation studies further confirm the necessity of pseudo-word optimization.

</details>


### [53] [Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism](https://arxiv.org/abs/2512.18527)
*Rahul Yumlembam,Biju Issac,Nauman Aslam,Eaby Kollonoor Babu,Josh Collyer,Fraser Kennedy*

Main category: cs.CV

TL;DR: 提出多源不确定性融合的拒识式检测框架，用于区分AI生成图像与自然图像；在分布外与对抗攻击下更稳健，能高比例拒绝潜在错误预测。


<details>
  <summary>Details</summary>
Motivation: 单一置信度/不确定性指标在分布移位与对抗攻击下易失效，难以可靠区分逼真AI图像与自然图像；需要一个能在不同生成器、攻击和域外样本上保持鲁棒的检测与拒识机制。

Method: 结合三类不确定性：1) Fisher Information（参数对输入扰动敏感度）；2) MC Dropout的熵不确定性（预测可变性）；3) 深核学习+高斯过程分类器的预测方差（贝叶斯式不确定性）。使用粒子群优化同时学习三者权重与自适应拒识阈值。在Stable Diffusion上训练，跨GLIDE、VQDM、Midjourney、BigGAN、StyleGAN3评测；并在FGSM/PGD攻击下测试。

Result: - 分布内：常规概率与Fisher指标表现良好；分布外：显著退化。
- 融合后的Combined Uncertainty在未见生成器上约70%错误拒绝率（拒掉多数误判的AI样本），同时对自然图像与域内AI的正确预测保持高接受率。
- 对抗条件下：Combined方法拒绝约61%成功攻击；单独GP不确定性可达约80%。

Conclusion: 多源不确定性融合与自适应拒识策略能在跨生成器分布移位与对抗场景下提供更稳健的AI图像检测；虽对新生成器有一定保守性（误拒正确样本），但可用于后续增量训练与系统迭代。

Abstract: As AI-generated images become increasingly photorealistic, distinguishing them from natural images poses a growing challenge. This paper presents a robust detection framework that leverages multiple uncertainty measures to decide whether to trust or reject a model's predictions. We focus on three complementary techniques: Fisher Information, which captures the sensitivity of model parameters to input variations; entropy-based uncertainty from Monte Carlo Dropout, which reflects predictive variability; and predictive variance from a Deep Kernel Learning framework using a Gaussian Process classifier. To integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings and determine an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated on GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3, each introducing significant distribution shifts. While standard metrics such as prediction probability and Fisher-based measures perform well in distribution, their effectiveness degrades under shift. In contrast, the Combined Uncertainty measure consistently achieves an incorrect rejection rate of approximately 70 percent on unseen generators, successfully filtering most misclassified AI samples. Although the system occasionally rejects correct predictions from newer generators, this conservative behaviour is acceptable, as rejected samples can support retraining. The framework maintains high acceptance of accurate predictions for natural images and in-domain AI data. Under adversarial attacks using FGSM and PGD, the Combined Uncertainty method rejects around 61 percent of successful attacks, while GP-based uncertainty alone achieves up to 80 percent. Overall, the results demonstrate that multi-source uncertainty fusion provides a resilient and adaptive solution for AI-generated image detection.

</details>


### [54] [WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring](https://arxiv.org/abs/2512.18528)
*Moses Kiprono*

Main category: cs.CV

TL;DR: 提出WoundNet-Ensemble，一个IoMT系统，集成ResNet-50、DINOv2 ViT与Swin Transformer，对六类临床创面进行自动分类，达99.90%集成准确率，较SOTA提升3.7%，并提供纵向愈合跟踪与临床预警，面向远程监测与临床部署。


<details>
  <summary>Details</summary>
Motivation: 慢性创面（如糖足溃疡）负担巨大且现有评估主观、易不一致，导致延误治疗。需要客观、可扩展、可远程的自动化评估工具以改进分型、监测愈合并支持远程医疗。

Method: 构建一个物联网医疗系统WoundNet-Ensemble，融合三种互补深度模型：ResNet-50、DINOv2自监督ViT、Swin Transformer；采用加权融合以进行六类创面分类；并实现纵向跟踪模块，计算愈合率、严重度评分并生成临床警报；在5175张多类型创面图像数据集上训练与评估；承诺开源实现与模型。

Result: 集成模型在六类创面分类上达到99.90%准确率；加权融合较既有SOTA提升3.7%；系统可输出愈合率、严重度与预警，实现可部署的临床工具。

Conclusion: WoundNet-Ensemble在创面自动分类与随访监测上表现强劲，可用于远程医疗与患者居家监测，具有临床可行性与再现性（因开源）。

Abstract: Chronic wounds, including diabetic foot ulcers which affect up to one-third of people with diabetes, impose a substantial clinical and economic burden, with U.S. healthcare costs exceeding 25 billion dollars annually. Current wound assessment remains predominantly subjective, leading to inconsistent classification and delayed interventions. We present WoundNet-Ensemble, an Internet of Medical Things system leveraging a novel ensemble of three complementary deep learning architectures: ResNet-50, the self-supervised Vision Transformer DINOv2, and Swin Transformer, for automated classification of six clinically distinct wound types. Our system achieves 99.90 percent ensemble accuracy on a comprehensive dataset of 5,175 wound images spanning diabetic foot ulcers, pressure ulcers, venous ulcers, thermal burns, pilonidal sinus wounds, and fungating malignant tumors. The weighted fusion strategy demonstrates a 3.7 percent improvement over previous state-of-the-art methods. Furthermore, we implement a longitudinal wound healing tracker that computes healing rates, severity scores, and generates clinical alerts. This work demonstrates a robust, accurate, and clinically deployable tool for modernizing wound care through artificial intelligence, addressing critical needs in telemedicine and remote patient monitoring. The implementation and trained models will be made publicly available to support reproducibility.

</details>


### [55] [Hierarchical Bayesian Framework for Multisource Domain Adaptation](https://arxiv.org/abs/2512.18553)
*Alexander M. Glandon,Khan M. Iftekharuddin*

Main category: cs.CV

TL;DR: 提出用于多源域自适应预训练的分层贝叶斯框架，利用源域间分布相似性，显著提升无标注目标域识别性能，在Daily-DA人体动作识别上比SOTA提升17.29%。


<details>
  <summary>Details</summary>
Motivation: 以往MDA预训练做法零散：要么强制共享权重，要么各源域独立训练，未系统利用源域间分布相似性，导致对目标域迁移不稳和性能受限。

Method: 构建分层贝叶斯模型，将各源域参数视为来自共享先验的样本，通过建模源域分布相似度来学习共享先验与各域后验；据此进行预训练，使源模型在共享结构与域特异性之间权衡，从而为目标域无监督适配提供更好的初始化。

Result: 在大型基准上整体识别精度提升；在多域Daily-DA RGB人体动作识别上，相比现有SOTA方法准确率提升17.29%。

Conclusion: 利用源域间相似性的分层贝叶斯预训练能显著增强MDA的泛化与稳定性，对无标注目标域任务尤其有效，优于共享权重或独立训练的传统方案。

Abstract: Multisource domain adaptation (MDA) aims to use multiple source datasets with available labels to infer labels on a target dataset without available labels for target supervision. Prior works on MDA in the literature is ad-hoc as the pretraining of source models is either based on weight sharing or uses independently trained models. This work proposes a Bayesian framework for pretraining in MDA by considering that the distributions of different source domains are typically similar. The Hierarchical Bayesian Framework uses similarity between the different source data distributions to optimize the pretraining for MDA. Experiments using the proposed Bayesian framework for MDA show that our framework improves accuracy on recognition tasks for a large benchmark dataset. Performance comparison with state-of-the-art MDA methods on the challenging problem of human action recognition in multi-domain benchmark Daily-DA RGB video shows the proposed Bayesian Framework offers a 17.29% improvement in accuracy when compared to the state-of-the-art methods in the literature.

</details>


### [56] [Enhancing Medical Large Vision-Language Models via Alignment Distillation](https://arxiv.org/abs/2512.18554)
*Aofei Chang,Ting Wang,Fenglong Ma*

Main category: cs.CV

TL;DR: 提出MEDALIGN：从医疗领域CLIP向Med-LVLM蒸馏视觉对齐知识，通过空间感知与注意力感知损失，缓解幻觉、提升医疗报告生成与VQA的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: Med-LVLM在临床任务中易出现视觉幻觉，根源在于视觉表征学习不足与视觉注意力对齐不佳，需要一种低开销方法将可靠的视觉对齐能力注入到现有模型。

Method: 以领域特定CLIP为教师、Med-LVLM为学生，进行对齐蒸馏。设计两类损失：1) 空间感知视觉对齐损失——基于视觉token级相似度结构，使学生的视觉表征分布对齐教师；2) 注意力感知蒸馏损失——引导学生注意力聚焦诊断相关区域。框架简单、轻量，可无缝应用于现有Med-LVLM。

Result: 在医疗报告生成与医疗VQA多基准上，性能与可解释性均显著提升，输出更具视觉扎根性，减少幻觉。

Conclusion: 通过从领域CLIP向Med-LVLM蒸馏视觉对齐知识，MEDALIGN有效缓解视觉幻觉问题，改进理解与推理的可靠性，并以低成本提升多种临床任务表现。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a simple, lightweight alignment distillation framework that transfers visual alignment knowledge from a domain-specific Contrastive Language-Image Pre-training (CLIP) model to Med-LVLMs. MEDALIGN introduces two distillation losses: a spatial-aware visual alignment loss based on visual token-level similarity structures, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Extensive experiments on medical report generation and medical visual question answering (VQA) benchmarks show that MEDALIGN consistently improves both performance and interpretability, yielding more visually grounded outputs.

</details>


### [57] [OpenView: Empowering MLLMs with Out-of-view VQA](https://arxiv.org/abs/2512.18563)
*Qixiang Chen,Cheng Zhang,Chi-Wing Fu,Jingwen Ye,Jianfei Cai*

Main category: cs.CV

TL;DR: 提出OpenView：面向“视野外”(OOV)理解的管线、数据集与基准，显著提升MLLM在OOV多选VQA上的表现（平均从48.6%到64.1%），但仍低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型主要擅长对图像帧内可见内容进行推理，缺乏对“画外”对象/活动/场景的外推与空间推断能力；缺少系统化的数据与评测来训练和衡量这类OOV理解。

Method: 1) 设计OpenView四阶段管线，利用全景图像合成多选VQA：自由取景生成帧、基于全景上下文与空间约束构造问题与选项，并产出可解释的理由；2) 构建OpenView-Dataset：来自多样真实全景的高质量合成数据，用于监督微调MLLM；3) 构建OpenView-Bench：同时评测答案选择与理由正确性，实现可诊断评估。

Result: 多种MLLM在使用OpenView数据微调后，OOV多选VQA平均准确率由48.6%提升到64.1%；与人类表现仍存在显著差距。

Conclusion: OOV理解是MLLM的关键短板。通过全景驱动的数据生成与联合评测，可系统性提升模型的OOV外推与空间推理能力，但仍需进一步研究以逼近人类水平。

Abstract: Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.

</details>


### [58] [Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model](https://arxiv.org/abs/2512.18573)
*Sumaiya Ali,Areej Alhothali,Ohoud Alzamzami,Sameera Albasri,Ahmed Abduljabbar,Muhammad Alwazzan*

Main category: cs.CV

TL;DR: 提出一种混合3D深度学习模型（3D DenseNet121 + 3D ViT）自动检测胎盘植入谱系（PAS），在独立测试集上五次运行平均准确率84.3%。


<details>
  <summary>Details</summary>
Motivation: PAS 诊断依赖MRI但受放射科医师解读差异影响，亟需客观、稳定的自动化工具以提高一致性和及时性。

Method: 构建混合架构：3D DenseNet121提取局部特征，3D Vision Transformer建模全局空间上下文；在1,133例MRI体数据的回顾性队列上训练与评估，并与多种3D深度学习基线模型对比；以独立测试集进行五次重复实验取平均表现。

Result: 混合DenseNet121-ViT在独立测试集上表现最佳，五次运行的平均准确率为84.3%，优于其他评估的3D架构。

Conclusion: CNN-Transformer混合模型在PAS MRI自动诊断中具有优势，能作为计算机辅助决策工具提升解读一致性与诊断效率，有潜力改善PAS诊断的准确性与及时性。

Abstract: Placenta Accreta Spectrum (PAS) is a serious obstetric condition that can be challenging to diagnose with Magnetic Resonance Imaging (MRI) due to variability in radiologists' interpretations. To overcome this challenge, a hybrid 3D deep learning model for automated PAS detection from volumetric MRI scans is proposed in this study. The model integrates a 3D DenseNet121 to capture local features and a 3D Vision Transformer (ViT) to model global spatial context. It was developed and evaluated on a retrospective dataset of 1,133 MRI volumes. Multiple 3D deep learning architectures were also evaluated for comparison. On an independent test set, the DenseNet121-ViT model achieved the highest performance with a five-run average accuracy of 84.3%. These results highlight the strength of hybrid CNN-Transformer models as a computer-aided diagnosis tool. The model's performance demonstrates a clear potential to assist radiologists by providing a robust decision support to improve diagnostic consistency across interpretations, and ultimately enhance the accuracy and timeliness of PAS diagnosis.

</details>


### [59] [Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach](https://arxiv.org/abs/2512.18597)
*Zhe Li,Kun Cheng,Hanyue Mo,Jintao Lu,Ziwen Kuang,Jianwen Ye,Lixu Xu,Xinya Meng,Jiahui Zhao,Shengda Ji,Shuyuan Liu,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出一种基于摄像视频的轨迹分析算法，替代/纠偏低速场景中不可靠的CAN车速用于AEB零速误刹问题。通过CLAHE增强的SIFT+KNN-RANSAC、多帧位移统计、双阈值判决和OBD-II驱动ROI，实现车辆静/动/抖动高精度判别，在大规模实测中显著降低误触发、保持紧急制动成功率，并满足嵌入式实时性。


<details>
  <summary>Details</summary>
Motivation: 商用车在低速行驶/起步/拥堵等工况，CAN总线车速信号精度不足或延迟，导致AEB误判“零速”，触发不必要的紧急制动，带来安全与运营风险。需要一种不依赖CAN精度、能在复杂环境下鲁棒判别自车运动状态的感知方案，且要在车规嵌入式平台上实时运行。

Method: 在Jetson AGX Xavier上，对盲区摄像头的视频序列进行：1) 自适应CLAHE增强后提取SIFT特征；2) 采用KNN匹配并用RANSAC去除外点，估计帧间特征位移；3) 5帧滑窗统计轨迹位移与稳定性；4) 通过双阈值状态判决矩阵将状态划分为静止/抖动/运动；5) 结合OBD-II信息动态配置ROI，抑制环境干扰与非相关动态目标影响。

Result: 在真实道路数据集（1852辆、32454段视频，704x576）上：静止检测F1=99.96%，运动识别F1=97.78%，平均时延14.2 ms。实际部署：误刹事件降低89%，紧急制动成功率100%，故障率<5%。

Conclusion: 基于视觉的多帧轨迹分析可在低速场景有效替代不可靠的CAN速度信息，显著降低AEB零速误刹并满足车载实时性；提出的多帧统计、双阈值判决与动态ROI是关键。可进一步研究跨天气/夜间泛化与与多传感器融合提升鲁棒性。

Abstract: A vision-based trajectory analysis solution is proposed to address the "zero-speed braking" issue caused by inaccurate Controller Area Network (CAN) signals in commercial vehicle Automatic Emergency Braking (AEB) systems during low-speed operation. The algorithm utilizes the NVIDIA Jetson AGX Xavier platform to process sequential video frames from a blind spot camera, employing self-adaptive Contrast Limited Adaptive Histogram Equalization (CLAHE)-enhanced Scale-Invariant Feature Transform (SIFT) feature extraction and K-Nearest Neighbors (KNN)-Random Sample Consensus (RANSAC) matching. This allows for precise classification of the vehicle's motion state (static, vibration, moving). Key innovations include 1) multiframe trajectory displacement statistics (5-frame sliding window), 2) a dual-threshold state decision matrix, and 3) OBD-II driven dynamic Region of Interest (ROI) configuration. The system effectively suppresses environmental interference and false detection of dynamic objects, directly addressing the challenge of low-speed false activation in commercial vehicle safety systems. Evaluation in a real-world dataset (32,454 video segments from 1,852 vehicles) demonstrates an F1-score of 99.96% for static detection, 97.78% for moving state recognition, and a processing delay of 14.2 milliseconds (resolution 704x576). The deployment on-site shows an 89% reduction in false braking events, a 100% success rate in emergency braking, and a fault rate below 5%.

</details>


### [60] [SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback](https://arxiv.org/abs/2512.18599)
*Jianglin Lu,Yuanwei Wu,Ziyi Zhao,Hongcheng Wang,Felix Jimenez,Abrar Majeedi,Yun Fu*

Main category: cs.CV

TL;DR: 提出一种基于策略优化的轻量级图像恢复智能体，通过顺序决策选择工具序列，在无需标注的环境中训练，并以多模态大模型作为奖励评估器，达到与SOTA相当或更优的效果且显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM/LLM的恢复智能体需反思回滚和迭代搜工具，效率低；且依赖重标注的降质识别模型，难以在无标注场景推广。需要一种既高效又可在无标签环境训练的恢复框架。

Method: 将复杂图像恢复建模为序列决策问题：轻量级代理在每一步从工具集合中选择最合适的恢复操作，目标最大化最终图像质量。训练上使用策略优化（如RL），并引入由多模态大模型驱动的奖励机制，作为人对齐的感知评估器提供反馈，从而在无监督/无标注条件下提升策略。训练后代理以确定性计划执行，无冗余调用。

Result: 在多种复合降质场景中，无监督训练的代理在全参考指标上可匹配SOTA，在无参考指标上超越现有方法，同时由于确定性、无回滚/少迭代，显著提升推理效率。

Conclusion: 策略优化+MLLM奖励的轻量代理可在无标注环境中学得高质量的工具调用序列，实现高效且高质的复杂图像恢复，兼顾准确性与速度，减少对降质识别和重标注的依赖。

Abstract: Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.

</details>


### [61] [Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments](https://arxiv.org/abs/2512.18613)
*Saeideh Yousefzadeh,Hamidreza Pourreza*

Main category: cs.CV

TL;DR: 提出Text2Graph VPR：将图像序列转为文本，再解析为场景图，用图注意力嵌入+最短路径核进行双相似度检索，实现可解释、鲁棒的语义定位，并支持零样本文本查询。


<details>
  <summary>Details</summary>
Motivation: 长期部署的视觉地点识别在光照、天气、季节变化下像素级特征不稳定；实际系统需要可解释、可诊断的决策过程，现有方法多依赖黑箱特征，难以在安全敏感与资源受限场景中建立信任。

Method: 1) 将连续图像转为自然语言场景描述；2) 解析为包含对象、属性、关系的场景图；3) 跨帧聚合为紧凑的地点表示；4) 采用双通道检索：a) 基于GAT的学习型图嵌入；b) 基于最短路径（SP）核的结构匹配；5) 融合两者相似度实现检索，并利用可读中间表示支持诊断。

Result: 在Oxford RobotCar与MSLS(安曼/旧金山)上，面对剧烈外观变化实现稳健检索；还能零样本从人类文本查询到地点。

Conclusion: 语义与图结构推理为VPR提供了一条可解释且鲁棒的替代路线，适合安全敏感与资源受限应用。

Abstract: Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.

</details>


### [62] [PTTA: A Pure Text-to-Animation Framework for High-Quality Creation](https://arxiv.org/abs/2512.18614)
*Ruiqi Chen,Kaitong Cai,Yijia Fan,Keze Wang*

Main category: cs.CV

TL;DR: 提出PTTA：纯文本到动画的生成框架，在小规模高质图文配对动画数据上微调HunyuanVideo，显著提升动画视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型擅长自然视频，但在动画风格上效果欠佳；已有工作多为图生视频（如AniSora），而端到端文本到动画的探索不足，动画制作仍成本高、流程复杂。

Method: 构建小规模高质量的动画视频-文本配对数据集；以预训练文本到视频模型HunyuanVideo为基础，进行针对动画风格的微调（fine-tuning），得到纯文本驱动的动画生成系统PTTA；通过多维度可视化评测与基线比较。

Result: 在多个评测维度上，PTTA在动画视频合成质量上持续优于可比基线，生成更符合动画风格且质量更高的视频。

Conclusion: 小而精的配对动画数据结合针对性微调，能有效将通用T2V模型适配为高质量文本到动画生成器；PTTA验证了纯文本到动画路径的可行性与优势。

Abstract: Traditional animation production involves complex pipelines and significant manual labor cost. While recent video generation models such as Sora, Kling, and CogVideoX achieve impressive results on natural video synthesis, they exhibit notable limitations when applied to animation generation. Recent efforts, such as AniSora, demonstrate promising performance by fine-tuning image-to-video models for animation styles, yet analogous exploration in the text-to-video setting remains limited.
  In this work, we present PTTA, a pure text-to-animation framework for high-quality animation creation. We first construct a small-scale but high-quality paired dataset of animation videos and textual descriptions. Building upon the pretrained text-to-video model HunyuanVideo, we perform fine-tuning to adapt it to animation-style generation. Extensive visual evaluations across multiple dimensions show that the proposed approach consistently outperforms comparable baselines in animation video synthesis.

</details>


### [63] [Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers](https://arxiv.org/abs/2512.18635)
*Xiyue Bai,Ronghao Yu,Jia Xiu,Pengfei Zhou,Jie Xia,Peng Ji*

Main category: cs.CV

TL;DR: Uni-Neur2Img提出统一框架将神经信号用于图像生成与编辑，采用LoRA注入与因果注意力，支持多模态条件；并发布EEG-Style数据集，在多基准上显著提升生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经信号驱动生成多以文本为条件或中间表示，缺乏以视觉模态（如EEG）直接作为条件的系统化方法与数据集，且需要一种高效、可扩展的统一框架连接神经信号与视觉内容生成。

Method: 提出Uni-Neur2Img：1) 参数高效的LoRA神经信号注入模块，将不同条件信号独立处理为可插拔组件，无需改动基座扩散/生成模型参数；2) 采用因果注意力满足长序列条件建模；3) 构建EEG-Style数据集；4) 在多任务设置下进行训练/微调，实现生成、编辑与风格迁移。

Result: 在CVPR40进行EEG驱动图像生成、在Loongx进行语义感知的局部编辑、在自建EEG-Style进行EEG驱动风格迁移，均获得显著提升：生成保真度更高、编辑一致性更好、风格迁移质量更优，同时计算开销低、可扩展到更多模态。

Conclusion: Uni-Neur2Img提供了统一、效率高且可扩展的神经信号到视觉生成方案，填补视觉模态作为直接条件的空白，并通过新数据集和广泛实验验证了其有效性。

Abstract: Generating or editing images directly from Neural signals has immense potential at the intersection of neuroscience, vision, and Brain-computer interaction. In this paper, We present Uni-Neur2Img, a unified framework for neural signal-driven image generation and editing. The framework introduces a parameter-efficient LoRA-based neural signal injection module that independently processes each conditioning signal as a pluggable component, facilitating flexible multi-modal conditioning without altering base model parameters. Additionally, we employ a causal attention mechanism accommodate the long-sequence modeling demands of conditional generation tasks. Existing neural-driven generation research predominantly focuses on textual modalities as conditions or intermediate representations, resulting in limited exploration of visual modalities as direct conditioning signals. To bridge this research gap, we introduce the EEG-Style dataset. We conduct comprehensive evaluations across public benchmarks and self-collected neural signal datasets: (1) EEG-driven image generation on the public CVPR40 dataset; (2) neural signal-guided image editing on the public Loongx dataset for semantic-aware local modifications; and (3) EEG-driven style transfer on our self-collected EEG-Style dataset. Extensive experimental results demonstrate significant improvements in generation fidelity, editing consistency, and style transfer quality while maintaining low computational overhead and strong scalability to additional modalities. Thus, Uni-Neur2Img offers a unified, efficient, and extensible solution for bridging neural signals and visual content generation.

</details>


### [64] [Geometric-Photometric Event-based 3D Gaussian Ray Tracing](https://arxiv.org/abs/2512.18640)
*Kai Kohyama,Yoshimitsu Aoki,Guillermo Gallego,Shintaro Shiba*

Main category: cs.CV

TL;DR: 提出一种面向事件相机的3D高斯泼洒(3DGS)新框架：将渲染拆分为逐事件的几何(深度)渲染与基于快照的辐射(强度)渲染两支路，以兼顾高时间分辨率与重建精度，达到SOTA/具竞争力表现、无需先验或COLMAP初始化、对事件数量选择更灵活、边缘锐利且训练更快。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级时间分辨率，但事件稀疏、异步，使得传统帧式或直接迁移的3DGS难以充分利用其细粒度时序信息；现有方法往往在时间分辨率与重建精度间权衡不佳，依赖外部先验或初始化，且对事件选择敏感。

Method: 核心思想：渲染解耦为两条分支——(1) 逐事件的几何/深度渲染：基于光线追踪利用单个事件的时序信息估计几何；(2) 基于事件扭曲聚合图像的快照式辐射/强度渲染：将一定时间窗内事件进行时空扭曲形成“快照”，用于稳定的外观/强度估计。通过这种几何-辐射分离，既利用事件级时序，又保证辐射估计的稳健性。

Result: 在真实数据集上取得SOTA，在合成数据上具竞争力；无需预训练图像重建模型或COLMAP初始化；对事件数量选择更灵活；在场景边缘重建更锐利；训练时间更快。

Conclusion: 解耦的事件3DGS渲染有效利用稀疏事件的时序特性，实现高时间分辨率与高精度的兼顾，减少对外部先验与初始化的依赖，推动事件相机3D重建的发展；代码将开源。

Abstract: Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released.

</details>


### [65] [Adversarial Robustness in Zero-Shot Learning:An Empirical Study on Class and Concept-Level Vulnerabilities](https://arxiv.org/abs/2512.18651)
*Zhiyuan Peng,Zihan Ye,Shreyank N Gowda,Yuping Yan,Haotian Xu,Ling Shao*

Main category: cs.CV

TL;DR: 论文系统评估零样本学习（ZSL）在类级与概念级对抗扰动下的鲁棒性，发现现有方法对非目标类攻击在GZSL情形下存在“伪成功”，并提出CBEA可在所有校准点摧毁GZSL性能；同时提出两种概念级攻击（保类与不保类）显示ZSL对概念操控亦脆弱。


<details>
  <summary>Details</summary>
Motivation: ZSL依赖从视觉特征到语义概念的映射来识别未见类，具有可解释性与泛化潜力；然而其在系统性对抗扰动下的稳健性尚不清楚，尤其在更现实的GZSL与概念级语义通道上缺乏系统检验。

Method: 1) 在类级：对多种主流ZSL模型实施非目标类攻击（clsA），并在GZSL中分析不同校准点的性能变化；2) 提出类偏置增强攻击（CBEA），通过扩大见类/未见类概率差，使任意校准点下的GZSL准确率近零；3) 在概念级：提出两种新攻击——保类概念攻击（CPconA）与不保类概念攻击（NCPconA），通过抹除/注入概念来操控语义表示；4) 在近三年多种架构上进行广泛实验与比较。

Result: - clsA在标准ZSL有效；在GZSL中只在“原最佳校准点”显著失效模型，但最佳校准点会迁移，导致所谓攻击成功其实是伪像。- CBEA在所有校准点上均可将GZSL准确率压至极低或为零，显著强于clsA。- 概念级攻击（CPconA与NCPconA）同样能有效操控预测，显示ZSL不仅对类级攻击脆弱，也对概念操控高度敏感。- 不同ZSL方法在鲁棒性上差异显著。

Conclusion: 现有ZSL/GZSL模型对类级与概念级对抗扰动均缺乏鲁棒性；传统clsA在GZSL中的成功常为校准伪效应。CBEA可在全校准范围内瓦解性能。应发展具备更强对抗鲁棒性的ZSL方法，并在评估中考虑校准敏感性与概念级攻击面。

Abstract: Zero-shot Learning (ZSL) aims to enable image classifiers to recognize images from unseen classes that were not included during training. Unlike traditional supervised classification, ZSL typically relies on learning a mapping from visual features to predefined, human-understandable class concepts. While ZSL models promise to improve generalization and interpretability, their robustness under systematic input perturbations remain unclear. In this study, we present an empirical analysis about the robustness of existing ZSL methods at both classlevel and concept-level. Specifically, we successfully disrupted their class prediction by the well-known non-target class attack (clsA). However, in the Generalized Zero-shot Learning (GZSL) setting, we observe that the success of clsA is only at the original best-calibrated point. After the attack, the optimal bestcalibration point shifts, and ZSL models maintain relatively strong performance at other calibration points, indicating that clsA results in a spurious attack success in the GZSL. To address this, we propose the Class-Bias Enhanced Attack (CBEA), which completely eliminates GZSL accuracy across all calibrated points by enhancing the gap between seen and unseen class probabilities.Next, at concept-level attack, we introduce two novel attack modes: Class-Preserving Concept Attack (CPconA) and NonClass-Preserving Concept Attack (NCPconA). Our extensive experiments evaluate three typical ZSL models across various architectures from the past three years and reveal that ZSL models are vulnerable not only to the traditional class attack but also to concept-based attacks. These attacks allow malicious actors to easily manipulate class predictions by erasing or introducing concepts. Our findings highlight a significant performance gap between existing approaches, emphasizing the need for improved adversarial robustness in current ZSL models.

</details>


### [66] [SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement](https://arxiv.org/abs/2512.18655)
*Yue Wen,Liang Song,Hesheng Wang*

Main category: cs.CV

TL;DR: SplatBright提出首个可泛化的3D高斯（3D Gaussian Splatting）框架，实现从稀疏低照度sRGB视角同时进行低光增强与3D重建，兼顾新视合成质量与跨视一致性。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角、低照度条件下的3D重建易受曝光不均与颜色失真影响，现有方法常需要逐场景训练且容易出现跨视不一致；缺乏成对、几何一致的大规模低光数据也限制了训练与泛化。

Method: 1) 物理先验：融合物理引导的成像/照明建模；2) 几何-外观解耦：采用双分支预测器，稳定初始化3D高斯的几何参数；3) 外观分支：利用频域先验约束照明一致性，实现可控且跨视一致的光照；4) 外观细化模块：进一步分离照明、材质与视角相关成分，恢复细节纹理；5) 数据合成：基于物理摄像机模型从正常曝光图合成暗光视图以训练。

Result: 在公开与自采数据上，SplatBright在新视合成质量、跨视一致性与对未知低光场景的泛化方面均优于现有2D与3D方法。

Conclusion: 通过物理引导的照明建模与几何-外观解耦，SplatBright实现了可泛化的低光稀疏视3D重建与增强，为低光场景下稳定一致的新视合成提供了有效方案。

Abstract: Low-light 3D reconstruction from sparse views remains challenging due to exposure imbalance and degraded color fidelity. While existing methods struggle with view inconsistency and require per-scene training, we propose SplatBright, which is, to our knowledge, the first generalizable 3D Gaussian framework for joint low-light enhancement and reconstruction from sparse sRGB inputs. Our key idea is to integrate physically guided illumination modeling with geometry-appearance decoupling for consistent low-light reconstruction. Specifically, we adopt a dual-branch predictor that provides stable geometric initialization of 3D Gaussian parameters. On the appearance side, illumination consistency leverages frequency priors to enable controllable and cross-view coherent lighting, while an appearance refinement module further separates illumination, material, and view-dependent cues to recover fine texture. To tackle the lack of large-scale geometrically consistent paired data, we synthesize dark views via a physics-based camera model for training. Extensive experiments on public and self-collected datasets demonstrate that SplatBright achieves superior novel view synthesis, cross-view consistency, and better generalization to unseen low-light scenes compared with both 2D and 3D methods.

</details>


### [67] [PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2512.18660)
*Pengxiang Ouyang,Qing Ma,Zheng Wang,Cong Bai*

Main category: cs.CV

TL;DR: 论文提出一种新型RS图文检索框架，通过跨模态门控注意力与正负意识注意机制，抑制伪匹配对(PMPs)带来的噪声，对三大数据集均获SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实RS数据集中存在大量语义不匹配或弱对齐的图文伪匹配对，导致跨模态对齐学习受干扰、性能下降，需要机制来识别并削弱这些噪声关联。

Method: 设计两类注意模块：1) 跨模态门控注意力，动态调节图像与文本间信息流，抑制不可靠信号、保留有用特征；2) 正负意识注意(Positive-Negative Awareness)，在对齐学习中显式区分信息性(正)线索与误导性(负)线索，强化正样本贡献、减弱负样本影响。整体形成稳健的检索学习框架。

Result: 在RSICD、RSITMD、RS5M三套基准数据集上，方法取得一致的SOTA表现，展示出对真实世界不匹配与PMPs的强鲁棒性。

Conclusion: 通过门控与正负意识注意的结合，可有效缓解RS图文检索中的伪匹配问题，提升跨模态对齐质量与检索性能，并具备良好泛化与鲁棒性。

Abstract: Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.

</details>


### [68] [SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse](https://arxiv.org/abs/2512.18671)
*Yiming Sun,Mi Zhang,Feifei Li,Geng Hong,Min Yang*

Main category: cs.CV

TL;DR: SmartSight是一种无训练的策略，通过自省式多样化解码与注意力诊断，显著降低Video-LLM的感知幻觉，同时不牺牲、甚至提升视频理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLM容易产生感知幻觉（对视频内容的错误描述），带来安全与可信问题；已有抑制方法往往需要训练或会削弱模型理解/推理。需要一种既能减幻觉、又不损伤能力、且可即插即用的方案。

Method: 1) 多候选解码：从同一视频/问题生成多条候选回答，避免贪心解码掩盖低幻觉答案。2) 幻觉评分TAC（Temporal Attention Collapse）：依据生成时的时间维注意力分布，检测模型是否过度集中在琐碎时间段，从而评估每个候选的幻觉程度。3) VAV（Visual Attention Vanishing）点：定位视觉注意力消失的时间位置，据此更精确估计幻觉并对明显幻觉的候选提前终止，降低解码成本。4) 选择与早停：根据TAC选择低幻觉候选作为最终输出，同时利用VAV实现高效筛除。

Result: 在Qwen2.5-VL-7B上：VRIPT-HAL幻觉率降低10.59%；在VideoMMMU上视频理解/推理性能最高提升8.86%；同时推理开销因早停而下降。

Conclusion: SmartSight利用模型内省信号（时序/视觉注意力）在无需训练的前提下有效抑制幻觉，并提升视频理解与推理，证明其可作为提升开源Video-LLM可靠性的实用方法。

Abstract: Despite Video Large Language Models having rapidly advanced in recent years, perceptual hallucinations pose a substantial safety risk, which severely restricts their real-world applicability. While several methods for hallucination mitigation have been proposed, they often compromise the model's capacity for video understanding and reasoning. In this work, we propose SmartSight, a pioneering step to address this issue in a training-free manner by leveraging the model's own introspective capabilities. Specifically, SmartSight generates multiple candidate responses to uncover low-hallucinated outputs that are often obscured by standard greedy decoding. It assesses the hallucination of each response using the Temporal Attention Collapse score, which measures whether the model over-focuses on trivial temporal regions of the input video when generating the response. To improve efficiency, SmartSight identifies the Visual Attention Vanishing point, enabling more accurate hallucination estimation and early termination of hallucinated responses, leading to a substantial reduction in decoding cost. Experiments show that SmartSight substantially lowers hallucinations for Qwen2.5-VL-7B by 10.59% on VRIPT-HAL, while simultaneously enhancing video understanding and reasoning, boosting performance on VideoMMMU by up to 8.86%. These results highlight SmartSight's effectiveness in improving the reliability of open-source Video-LLMs.

</details>


### [69] [AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference](https://arxiv.org/abs/2512.18675)
*Longhuan Xu,Feng Yin,Cunjian Chen*

Main category: cs.CV

TL;DR: 提出一种“异步”扩散推理：解耦数值积分步与去噪器条件步，利用轻量时间步预测模块（TPM）选择更合适的条件时间步，从而在不改动更新日程的情况下控制细节与纹理，带来一致的指标提升。


<details>
  <summary>Details</summary>
Motivation: 同步调度在推理时强制去噪器与积分器对齐，可能在某些状态下不是最优；若能按当前潜变量状态自适应选择条件的噪声级别，可更好平衡细节、纹理与文本对齐，提高少步数推理质量与效率。

Method: - 解耦：保持图像更新（积分）时间表不变，学习让去噪器在不同的（可能更早/更晚）时间步上条件化。
- 时间步预测模块（TPM）：轻量网络按当前状态预测更可行的条件时间步；训练采用GRPO（组相对策略优化）以复合奖励为目标。
- 部署时加入缩放超参，在原始与预测的条件时间步之间插值，实现保守到激进的调节。
- 实验设置：SD3.5最多15步、Flux最多10步，以控制计算成本。

Result: 在Stable Diffusion 3.5 Medium与Flux.1-dev上、MS-COCO 2014与T2I-CompBench数据集，以ImageReward、HPSv2、CLIP Score、Pick Score的平均作为复合奖励，方法取得一致改进。

Conclusion: 异步调度使去噪条件步自适应于当前状态，在保持更新日程不变的前提下提升少步扩散推理质量；可通过超参平衡风险与收益，具有通用性与实际部署价值。

Abstract: Text-to-image diffusion inference typically follows synchronized schedules, where the numerical integrator advances the latent state to the same timestep at which the denoiser is conditioned. We propose an asynchronous inference mechanism that decouples these two, allowing the denoiser to be conditioned at a different, learned timestep while keeping image update schedule unchanged. A lightweight timestep prediction module (TPM), trained with Group Relative Policy Optimization (GRPO), selects a more feasible conditioning timestep based on the current state, effectively choosing a desired noise level to control image detail and textural richness. At deployment, a scaling hyper-parameter can be used to interpolate between the original and de-synchronized timesteps, enabling conservative or aggressive adjustments. To keep the study computationally affordable, we cap the inference at 15 steps for SD3.5 and 10 steps for Flux. Evaluated on Stable Diffusion 3.5 Medium and Flux.1-dev across MS-COCO 2014 and T2I-CompBench datasets, our method optimizes a composite reward that averages Image Reward, HPSv2, CLIP Score and Pick Score, and shows consistent improvement.

</details>


### [70] [brat: Aligned Multi-View Embeddings for Brain MRI Analysis](https://arxiv.org/abs/2512.18679)
*Maxime Kayser,Maksim Gridnev,Wanting Wang,Max Bain,Aneesh Rangnekar,Avijit Chatterjee,Aleksandr Petrov,Harini Veeraraghavan,Nathaniel C. Swinburne*

Main category: cs.CV

TL;DR: 提出brat：一种将脑部MRI与临床报告对齐的多视图表示学习框架，基于约8万份3D MRI-报告配对数据，通过隐式查询-特征匹配与质量-多样性思想来学习与报告句子对齐的多视图嵌入，并在多项视觉-语言与纯视觉任务上显著提升表现，模型已开源。


<details>
  <summary>Details</summary>
Motivation: 脑部MRI存在病灶多样、细微且常局灶于极少切片的难题，现有数据集规模偏小，导致通用表征学习与跨模态对齐效果有限；需要一种能利用大规模MRI-报告配对数据、并能细粒度捕捉临床特征的预训练方法。

Method: 1) 构建约8万例3D脑MRI与放射科报告配对的大规模数据集（约为既有数据集10倍）。2) 受文档检索启发，提出多视图预训练：将报告句子视作查询，MRI体数据提取多视图嵌入。3) 设计隐式查询-特征匹配机制，使句子级临床特征与相应的MRI视图/切片级表征对齐。4) 引入质量-多样性（quality-diversity）思想，促使模型发现互补、覆盖不同异常模式的多样视图嵌入。

Result: 在多种视觉-语言（如检索/对齐）与纯视觉（如分类/检测/局灶识别）任务上均取得显著优于现有方法的性能。

Conclusion: 基于大规模MRI-报告配对数据与多视图对齐的预训练可有效捕捉脑部细粒度异常，显著提升下游任务表现；所提出的brat可作为开源基础模型促进社区研究。

Abstract: We present brat (brain report alignment transformer), a multi-view representation learning framework for brain magnetic resonance imaging (MRI) trained on MRIs paired with clinical reports. Brain MRIs present unique challenges due to the presence of numerous, highly varied, and often subtle abnormalities that are localized to a few slices within a 3D volume. To address these challenges, we introduce a brain MRI dataset $10\times$ larger than existing ones, containing approximately 80,000 3D scans with corresponding radiology reports, and propose a multi-view pre-training approach inspired by advances in document retrieval. We develop an implicit query-feature matching mechanism and adopt concepts from quality-diversity to obtain multi-view embeddings of MRIs that are aligned with the clinical features given by report sentences. We evaluate our approach across multiple vision-language and vision tasks, demonstrating substantial performance improvements. The brat foundation models are publicly released.

</details>


### [71] [A Study of Finetuning Video Transformers for Multi-view Geometry Tasks](https://arxiv.org/abs/2512.18684)
*Huimin Wu,Kwang-Ting Cheng,Stephen Lin,Zhirong Wu*

Main category: cs.CV

TL;DR: 微调视频基础模型的ViT即可在光流等多视几何任务上取得SOTA，线性解码器+迭代细化，跨数据集泛化强。


<details>
  <summary>Details</summary>
Motivation: 以往多视几何方法依赖专用架构与任务预训练，迁移差、工程复杂。作者想验证：通用视频预训练的Transformer是否已学到足够的时空表示，可用最小改动直接解决几何推理问题。

Method: 采用预训练视频Transformer作为骨干，不改主干结构，仅追加线性解码器输出几何预测（如光流）；在此基础上加入简单的迭代细化模块提升精度。训练时对目标任务进行微调，评估跨数据集泛化。

Result: 在光流上取得强泛化与SOTA：Sintel clean/final/KITTI EPE分别为0.69/1.78/3.15；在线测试EPE 0.79/1.88、F1 3.79。拓展到3D深度估计与双目匹配也表现优异。

Conclusion: 视频预训练的通用注意力已编码时空与几何信息；通过最小适配（线性头+迭代细化）即可在多视几何任务上达到或超越SOTA，方法简单且通用性强。

Abstract: This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.

</details>


### [72] [EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images](https://arxiv.org/abs/2512.18692)
*Jongmin Park,Minh-Quan Viet Bui,Juan Luis Gonzalez Bello,Jaeho Moon,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: EcoSplat 是首个可控效率的前馈式 3D 高斯泼洒（3DGS）框架，能在推理时按目标原语数量自适应生成三维表征，在稠密视角下以更少原语达成更优渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有前馈 3DGS 多为逐视角像素对齐地预测高斯原语：1）稠密视角会导致原语爆炸式增长；2）缺乏对预测高斯数量的显式控制，限制了在不同算力/时延预算下的应用。因此需要一种能在推理时按给定原语数灵活权衡质量与效率的方法。

Method: 两阶段训练：1）PGT（像素对齐高斯训练）阶段，学习初始的原语预测；2）IGF（基于重要性的微调）阶段，学习对原语进行排序与参数自适应调整，使模型在给定目标原语数量时可优先保留高重要性原语，并相应优化其参数以提升渲染质量。整体为前馈推理，无需逐场景优化，并在推理时输入目标原语数以控制效率。

Result: 在多种稠密视角设置下，EcoSplat 在严格原语数量上限条件下仍优于现有方法，表现出稳健性与更好的质量-效率权衡。

Conclusion: EcoSplat 实现了可控的前馈 3DGS：在推理阶段按目标原语数自适应生成与调整高斯原语，显著减少冗余并在受限预算下提升新视角合成表现，适合灵活的下游渲染任务。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) enables efficient one-pass scene reconstruction, providing 3D representations for novel view synthesis without per-scene optimization. However, existing methods typically predict pixel-aligned primitives per-view, producing an excessive number of primitives in dense-view settings and offering no explicit control over the number of predicted Gaussians. To address this, we propose EcoSplat, the first efficiency-controllable feed-forward 3DGS framework that adaptively predicts the 3D representation for any given target primitive count at inference time. EcoSplat adopts a two-stage optimization process. The first stage is Pixel-aligned Gaussian Training (PGT) where our model learns initial primitive prediction. The second stage is Importance-aware Gaussian Finetuning (IGF) stage where our model learns rank primitives and adaptively adjust their parameters based on the target primitive count. Extensive experiments across multiple dense-view settings show that EcoSplat is robust and outperforms state-of-the-art methods under strict primitive-count constraints, making it well-suited for flexible downstream rendering tasks.

</details>


### [73] [Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts](https://arxiv.org/abs/2512.18718)
*Linwei Qiu,Gongzhe Li,Xiaozhe Zhang,Qinlin Sun,Fengying Xie*

Main category: cs.CV

TL;DR: UniRect提出一个统一的图像畸变校正框架，将多种校正与矩形化任务视为通用“失真→复原”的逆问题，通过模拟不同镜头形成统一失真模型；采用RP-TPS几何形变+基于RMB的复原模块，并用稀疏MoE缓解多任务冲突，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像校正/矩形化方法多为任务专用架构，泛化与跨任务适用性差；实际摄影中存在多样镜头与畸变类型，需要一个能统一处理多种失真并保持高性能的通用框架。

Method: 1) 统一失真建模：把不同任务对应的失真通过模拟不同镜头纳入通用畸变模型。2) 双组件结构：Deformation Module采用残差渐进薄板样条（RP-TPS）逐步拟合复杂几何形变；Restoration Module采用Residual Mamba Blocks（RMBs）补偿形变引入的退化并提升图像保真。3) 稀疏专家混合（SMoEs）：在多任务学习中进行稀疏路由，降低不同畸变之间的竞争与负迁移。

Result: 在多项图像校正与矩形化基准上，相较于最新方法取得最先进性能（SOTA），表明统一框架在准确性与泛化上的优势。

Conclusion: 通过统一的畸变视角与RP-TPS+RMB双模块，并辅以稀疏MoE路由，UniRect能在多种失真与任务上实现高质量校正与增强，优于任务特定方法，具备更强泛化与实用性。

Abstract: Image correction and rectangling are valuable tasks in practical photography systems such as smartphones. Recent remarkable advancements in deep learning have undeniably brought about substantial performance improvements in these fields. Nevertheless, existing methods mainly rely on task-specific architectures. This significantly restricts their generalization ability and effective application across a wide range of different tasks. In this paper, we introduce the Unified Rectification Framework (UniRect), a comprehensive approach that addresses these practical tasks from a consistent distortion rectification perspective. Our approach incorporates various task-specific inverse problems into a general distortion model by simulating different types of lenses. To handle diverse distortions, UniRect adopts one task-agnostic rectification framework with a dual-component structure: a {Deformation Module}, which utilizes a novel Residual Progressive Thin-Plate Spline (RP-TPS) model to address complex geometric deformations, and a subsequent Restoration Module, which employs Residual Mamba Blocks (RMBs) to counteract the degradation caused by the deformation process and enhance the fidelity of the output image. Moreover, a Sparse Mixture-of-Experts (SMoEs) structure is designed to circumvent heavy task competition in multi-task learning due to varying distortions. Extensive experiments demonstrate that our models have achieved state-of-the-art performance compared with other up-to-date methods.

</details>


### [74] [Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning](https://arxiv.org/abs/2512.18734)
*Jinqiu Chen,Huyan Xu*

Main category: cs.CV

TL;DR: 该研究利用深度学习对常规H&E全片图像进行分析，采用多实例学习框架（改进CLAM-SB、ABMIL、ConvNeXt-MIL-XGBoost）预测乳腺癌5年复发风险（低/中/高），以21基因RS为标签。在210例、5折交叉验证中，改进的CLAM-SB表现最佳，平均AUC 0.836，准确率76.2%，显示以组织病理图像实现快速、低成本的基因相关风险分层的可行性。


<details>
  <summary>Details</summary>
Motivation: 临床上需要准确、可及、低成本的复发风险评估手段；传统基因检测（如21基因RS）费用高、耗时且对取材有要求。作者希望用常规病理切片和深度学习，在无需额外实验的情况下实现与基因检测相关的风险分层，以提升临床决策效率与覆盖面。

Method: 在210例乳腺癌患者的H&E全片图像上，构建并比较三种MIL框架：改进的CLAM-SB、ABMIL、以及以ConvNeXt特征+XGBoost分类的MIL方案。风险分为低/中/高三档，真值来源为21基因RS。特征采用UNI与CONCH预训练模型提取，进行5折交叉验证，评估AUC与准确率。

Result: 改进版CLAM-SB在三者中性能最佳，5折平均AUC=0.836，分类准确率=76.2%；ABMIL与ConvNeXt-MIL-XGBoost相对较低（未给出具体数值）。表明在中小样本下，CLAM式注意力汇聚与实例选择策略更稳健。

Conclusion: 常规H&E切片结合MIL深度学习可实现与基因检测相关的复发风险分层，具有可行性与潜在临床价值，可作为快速、经济的辅助决策工具。后续需多中心更大样本、前瞻验证与外部独立测试集，并探讨可解释性与与病理/临床特征的整合。

Abstract: Predicting breast cancer recurrence risk is a critical clinical challenge. This study investigates the potential of computational pathology to stratify patients using deep learning on routine Hematoxylin and Eosin (H&E) stained whole-slide images (WSIs). We developed and compared three Multiple Instance Learning (MIL) frameworks -- CLAM-SB, ABMIL, and ConvNeXt-MIL-XGBoost -- on an in-house dataset of 210 patient cases. The models were trained to predict 5-year recurrence risk, categorized into three tiers (low, medium, high), with ground truth labels established by the 21-gene Recurrence Score. Features were extracted using the UNI and CONCH pre-trained models. In a 5-fold cross-validation, the modified CLAM-SB model demonstrated the strongest performance, achieving a mean Area Under the Curve (AUC) of 0.836 and a classification accuracy of 76.2%. Our findings demonstrate the feasibility of using deep learning on standard histology slides for automated, genomics-correlated risk stratification, highlighting a promising pathway toward rapid and cost-effective clinical decision support.

</details>


### [75] [$M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models](https://arxiv.org/abs/2512.18735)
*Kewei Wei,Bocheng Hu,Jie Cao,Xiaohan Chen,Zhengxi Lu,Wubing Xia,Weili Xu,Jiaao Wu,Junchen He,Mingyu Jia,Ciyun Zhao,Ye Sun,Yizhi Li,Zhonghan Zhao,Jian Zhang,Gaoang Wang*

Main category: cs.CV

TL;DR: 提出M^3-Verse基准，评测多模态模型在同一空间中跨视频前后状态变化理解与推理的能力，并给出简单基线显著提升多状态感知。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在静态图像与单一时态理解上强，但对同一环境在两段视频中的动态变化追踪与因果/状态转移推理薄弱；该能力对空间智能至关重要，缺少系统评测基准。

Method: 构建M^3-Verse：室内场景在“状态变化前/后”的成对多视角视频，覆盖270个场景与2,932个问题；将任务划分为50+子任务，围绕4项核心能力设计问答评测；评测16个SOTA LMM，分析其在状态跟踪上的局限；提出一个简单有效的基线方法以提升多状态感知。

Result: 现有16个LMM在跨视频状态跟踪、变化定位与归因等方面表现不足；所提基线在多状态感知任务上取得显著性能提升。

Conclusion: M^3-Verse为评测与推动多状态、多维度动态理解提供了新挑战基准和有效基线，有望促进更具整体动态感知能力的下一代多模态模型。

Abstract: Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.

</details>


### [76] [AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection](https://arxiv.org/abs/2512.18738)
*James E. Gallagher,Edward J. Oughton*

Main category: cs.CV

TL;DR: 提出AMLID：首个用于无人机扫雷的开源RGB+LWIR多光谱数据集，含12,078张标注图像、21种地雷、跨高度/季节/昼夜/融合级别等多变量，为自适应检测算法提供基准并降低数据采集门槛。


<details>
  <summary>Details</summary>
Motivation: 地雷仍造成重大人道主义危害，现有探测方式危险、低效且昂贵；缺乏公开、标准化的多光谱无人机数据来训练与评测鲁棒的地雷检测算法。

Method: 构建并发布AMLID数据集：同时采集RGB与长波红外（LWIR）图像；覆盖21种金属/塑料、反步兵/反坦克地雷；设置11个多模态融合等级、4个飞行高度、2个季节、3种日照条件；提供12,078张带有标签的图像，用于UAS平台的地雷检测研究与基准评测。

Result: 形成大规模、标注完善、变量全面控制的多光谱数据集，可直接用于开发、训练与基准对比自适应地雷检测算法，无需接触实弹或昂贵外场采集。

Conclusion: AMLID通过开放多光谱、跨环境条件的数据资源，显著降低人道扫雷研究门槛，促进鲁棒且可泛化的无人机地雷检测方法的发展与标准化评测。

Abstract: Landmines remain a persistent humanitarian threat, with an estimated 110 million mines deployed across 60 countries, claiming approximately 26,000 casualties annually. Current detection methods are hazardous, inefficient, and prohibitively expensive. We present the Adaptive Multispectral Landmine Identification Dataset (AMLID), the first open-source dataset combining Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) imagery for Unmanned Aerial Systems (UAS)-based landmine detection. AMLID comprises of 12,078 labeled images featuring 21 globally deployed landmine types across anti-personnel and anti-tank categories in both metal and plastic compositions. The dataset spans 11 RGB-LWIR fusion levels, four sensor altitudes, two seasonal periods, and three daily illumination conditions. By providing comprehensive multispectral coverage across diverse environmental variables, AMLID enables researchers to develop and benchmark adaptive detection algorithms without requiring access to live ordnance or expensive data collection infrastructure, thereby democratizing humanitarian demining research.

</details>


### [77] [Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation](https://arxiv.org/abs/2512.18741)
*Tianrui Zhu,Shiyi Zhang,Zhirui Sun,Jingqi Tian,Yansong Tang*

Main category: cs.CV

TL;DR: 提出MAG：将记忆压缩与帧生成解耦，用紧凑KV缓存保留长程历史，使长视频生成在不爆内存的前提下保持场景一致；并发布MAG-Bench评估记忆保留，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长视频生成中，窗口注意力会丢弃窗口外历史导致灾难性遗忘与场景不一致；保留全历史又内存开销过大，需要在“历史保留”与“计算/显存成本”之间取得平衡。

Method: 提出Memorize-and-Generate框架：训练一个记忆模型将历史压缩为紧凑的KV cache；再训练一个生成模型读取压缩表示生成后续帧。并构建MAG-Bench严格评测历史记忆保留能力。

Result: 在保持与主流视频生成基准相当性能的同时，MAG在历史场景一致性与长期记忆保留方面显著优于基线窗口注意力方法；内存成本显著降低。

Conclusion: 通过记忆压缩与生成解耦，MAG在长视频生成中有效缓解遗忘与不一致难题，达成更好的历史保留-效率折中；MAG-Bench提供标准化评估。

Abstract: Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose \textbf{Memorize-and-Generate (MAG)}, a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce \textbf{MAG-Bench} to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.

</details>


### [78] [InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search](https://arxiv.org/abs/2512.18745)
*Kaican Li,Lewei Yao,Jiannan Wu,Tiezheng Yu,Jierun Chen,Haoli Bai,Lu Hou,Lanqing Hong,Wei Zhang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: 提出O3-Bench评测多模态“以图思考”的推理能力，并用多智能体框架InSight-o3（vReasoner+vSearcher）显著提升多模态模型在复杂视觉推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有开放式多模态代理在真实任务所需的推理上仍偏弱，尤其对包含密集图表/示意图的文档分析与地图导航等，需要跨区域、细粒度的视觉细节与多步推理能力，前沿系统也表现不足。

Method: 1) 构建O3-Bench：包含需要在图像不同区域间“交错关注”的多步视觉推理题。2) 提出InSight-o3多智能体：由负责推理的vReasoner与负责广义视觉搜索的vSearcher组成。3) 将“广义视觉搜索”定义为用自由文本定位关系型、模糊或概念性区域（超越传统物体/自然图像中的显式实体）。4) 针对vSearcher使用强化学习进行专项训练，并与各类前沿多模态LLM（作为vReasoner）即插即用组合。

Result: O3-Bench对最强模型也具挑战性，OpenAI o3仅得40.8%准确率。集成vSearcher后，前沿多模态模型在O3-Bench及多项视觉推理基准上的成绩显著提升。

Conclusion: O3-Bench为多模态推理与细粒度视觉注意的评测提供新标准；InSight-o3通过引入可泛化的视觉搜索代理，实证提升复杂视觉推理能力，推动面向“o3式”开放系统的进展。

Abstract: The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .

</details>


### [79] [IPCV: Information-Preserving Compression for MLLM Visual Encoders](https://arxiv.org/abs/2512.18747)
*Yuan Chen,Zichen Wen,Yuzhou Wu,Xuyang Liu,Shuang Chen,Junpeng Ma,Weijia Li,Conghui He,Linfeng Zhang*

Main category: cs.CV

TL;DR: IPCV是一种无需训练的视觉编码器压缩框架，在ViT内部进行信息保持的激进token裁剪，并通过邻域引导重建与注意力稳定化，既减少计算又维持多模态理解性能，且可增强现有LLM侧裁剪方法。


<details>
  <summary>Details</summary>
Motivation: MLLM推理成本高主要来自ViT产生并处理大量视觉token。现有策略要么只在LLM侧裁剪忽视ViT开销，要么在ViT侧无语言引导地裁剪导致丢失与文本相关的关键信息，并因双向注意力带来特征失真。需要一种在不训练的前提下既显著降算又尽量保留与语言任务相关视觉信息的方法。

Method: 提出IPCV：1) 在ViT内部进行token裁剪，同时用邻域引导重建（NGR）对被裁剪token进行临时重建，使其以近乎零额外成本参与注意力计算，并在输出给LLM前完全恢复，降低信息丢失与失真；2) 注意力稳定化（AS），通过近似被裁剪token的K/V来缓解裁剪对注意力分布的扰动；3) AS也可作为插件用于既有LLM侧裁剪方法以提升效果；全流程无需额外训练。

Result: 在多种图像与视频基准上，IPCV显著降低端到端计算量，且在无需训练的token压缩方法中达到或超过当前SOTA性能。

Conclusion: 在保持信息完整性的前提下，IPCV能在ViT内部进行高强度token压缩，兼顾效率与精度，并具有通用可插拔性（尤其AS模块可增强现有LLM侧裁剪）。

Abstract: Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.

</details>


### [80] [Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos](https://arxiv.org/abs/2512.18750)
*Xiaoyang Li,Wenzhu Yang,Kanglin Wang,Tiebiao Wang,Qingsong Fei*

Main category: cs.CV

TL;DR: 提出Context-Aware Network（CAN），通过多尺度时间线索模块（MTCM）与分组空间线索模块（GSCM）联合建模多粒度时空信息，在五个动作识别数据集上取得主流竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别方法往往忽视动作的多粒度特性，难以同时捕获快速局部运动与整体时空语义，导致对复杂动作的辨识不足。

Method: 设计CAN架构：1）MTCM以多时间尺度建模，兼顾快速变化的细节运动与长时间范围的动作流；2）GSCM对特征图分组，并对不同组采用差异化的空间线索提取策略，以覆盖不同空间尺度；两者融合以形成更全面的时空表示。

Result: 在Something-Something V1/V2、Diving48、Kinetics-400、UCF101上分别达到50.4%、63.9%、88.4%、74.9%、86.9%的准确率，整体优于多数主流方法。

Conclusion: 多尺度时空线索的联合建模能显著提升动作识别鲁棒性与性能，CAN验证了捕获多粒度信息的必要性与有效性。

Abstract: Action recognition is a critical task in video understanding, requiring the comprehensive capture of spatio-temporal cues across various scales. However, existing methods often overlook the multi-granularity nature of actions. To address this limitation, we introduce the Context-Aware Network (CAN). CAN consists of two core modules: the Multi-scale Temporal Cue Module (MTCM) and the Group Spatial Cue Module (GSCM). MTCM effectively extracts temporal cues at multiple scales, capturing both fast-changing motion details and overall action flow. GSCM, on the other hand, extracts spatial cues at different scales by grouping feature maps and applying specialized extraction methods to each group. Experiments conducted on five benchmark datasets (Something-Something V1 and V2, Diving48, Kinetics-400, and UCF101) demonstrate the effectiveness of CAN. Our approach achieves competitive performance, outperforming most mainstream methods, with accuracies of 50.4% on Something-Something V1, 63.9% on Something-Something V2, 88.4% on Diving48, 74.9% on Kinetics-400, and 86.9% on UCF101. These results highlight the importance of capturing multi-scale spatio-temporal cues for robust action recognition.

</details>


### [81] [MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation](https://arxiv.org/abs/2512.18766)
*Guohui Zhang,Hu Yu,Xiaoxiao Ma,Yaning Pan,Hang Xu,Feng Zhao*

Main category: cs.CV

TL;DR: MaskFocus 是一个面向遮罩式生成模型（如掩码自回归/扩散式逐步填充）的强化学习框架，通过“聚焦关键步骤”的策略实现高效的策略优化，并在多项文本生成图像基准上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: RL 已在后训练大语言模型与自回归视觉生成中成功，但在遮罩式生成模型上困难重重：策略优化需考虑多步迭代中每一步的似然与轨迹，导致高计算成本；直接对随机步骤进行原生优化常出现次优解。需要一种既能捕获多步过程里“关键贡献步骤”，又能降低计算的RL方法。

Method: 1) 关键信息增益估计：在每个采样步，将该步的中间图像与最终生成图像做相似度度量，以此估计该步对最终结果的信息增益；2) 关键步聚焦优化：挑选信息增益最高的若干步进行策略梯度更新，避免对整条轨迹做代价高昂的优化；3) 基于熵的动态路由：对低熵（确定性高）的样本引导模型探索更有价值的遮罩策略，通过熵驱动的采样路由提升探索-利用平衡。

Result: 在多项文本到图像基准上，MaskFocus 相比基线（随机步优化、全轨迹优化等）取得更好的生成质量与训练效率，显示出更稳定、更有效的策略学习能力。

Conclusion: 通过评估并聚焦优化多步生成过程中的关键步骤，同时引入熵驱动的动态路由探索，MaskFocus 将RL有效嫁接到遮罩式生成模型，实现了更高效的训练与更优的生成表现。

Abstract: Reinforcement learning (RL) has demonstrated significant potential for post-training language models and autoregressive visual generative models, but adapting RL to masked generative models remains challenging. The core factor is that policy optimization requires accounting for the probability likelihood of each step due to its multi-step and iterative refinement process. This reliance on entire sampling trajectories introduces high computational cost, whereas natively optimizing random steps often yields suboptimal results. In this paper, we present MaskFocus, a novel RL framework that achieves effective policy optimization for masked generative models by focusing on critical steps. Specifically, we determine the step-level information gain by measuring the similarity between the intermediate images at each sampling step and the final generated image. Crucially, we leverage this to identify the most critical and valuable steps and execute focused policy optimization on them. Furthermore, we design a dynamic routing sampling mechanism based on entropy to encourage the model to explore more valuable masking strategies for samples with low entropy. Extensive experiments on multiple Text-to-Image benchmarks validate the effectiveness of our method.

</details>


### [82] [In-Context Audio Control of Video Diffusion Transformers](https://arxiv.org/abs/2512.18772)
*Wenze Liu,Weicai Ye,Minghong Cai,Quande Liu,Xintao Wang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出ICAC框架，将严格时间同步的音频条件引入统一全注意力视频扩散Transformer中，并通过掩码3D注意力稳定训练，显著提升语音驱动的唇形同步与视频质量。


<details>
  <summary>Details</summary>
Motivation: 统一的多模态视频生成模型已能处理文本、图像、深度等，但对时间同步的音频信号利用不足；需要在单一全注意力架构下有效引入音频以实现更强的语音驱动视频生成与对齐能力。

Method: 在FullDiT式统一全注意力架构中系统比较三种音频注入机制：交叉注意力（cross-attn）、2D自注意力、统一3D自注意力。发现3D注意力最有潜力但难训练，于是提出“掩码3D注意力（Masked 3D Attention）”，通过约束注意力时序模式以强制时序对齐，从而稳定训练并提升效果；模型以音频流与参考图像作为条件进行扩散式视频生成。

Result: 掩码3D注意力实现稳定训练，较基线获得更强的语音-画面时序对齐，表现为显著的唇形同步提升与总体视频质量提升。

Conclusion: 在统一视频扩散Transformer中引入音频的最佳路径是受约束的3D注意力：既保留了三维注意力对时空相关性的建模能力，又通过时序掩码解决训练不稳，最终实现优质的语音驱动视频生成。

Abstract: Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.

</details>


### [83] [Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers](https://arxiv.org/abs/2512.18784)
*Fanis Mathioulakis,Gorjan Radevski,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: Eff-GRot是一种无需特定对象/类别训练、从单张RGB查询图与多张有姿态参考图中一次前向即可估计旋转的高效通用方法。核心是在潜空间用Transformer联合对比查询与多参考的旋转感知特征，实现端到端、准确且低延迟的旋转估计。


<details>
  <summary>Details</summary>
Motivation: 现有旋转估计方法常需对象/类别特定训练、匹配或优化，计算开销大、泛化差且不适用于低延迟场景。作者希望在保持精度的同时提升效率与泛化，面向实际部署的延迟敏感应用。

Method: 以多参考图像（姿态已知）与一张查询图为输入，先提取旋转感知表征，再通过一个Transformer在潜空间对查询与多参考进行联合比较与交互，直接回归目标旋转；不依赖对象/类别特定的训练流程，单次前向完成预测，端到端训练。

Result: 在实验中，该方法在准确率与计算效率间取得良好平衡；相较于传统或更重的方案，在延迟敏感情境下展现更优的效率与可用性，具有良好的泛化能力。

Conclusion: Eff-GRot为旋转估计提供了高效、通用且可扩展的端到端方案，特别适合对推理延迟要求严格的应用场景。

Abstract: We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object's rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.

</details>


### [84] [Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation](https://arxiv.org/abs/2512.18804)
*Guangtao Lyu,Chenghao Xu,Qi Liu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出TempoMoE：一种面向节拍/节奏的分层专家混合模块，增强扩散模型在音乐到3D舞蹈生成中的节奏感知，无需嘈杂的风格标签即可实现SOTA的舞蹈质量与节奏对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法常依赖流派/风格标签来提升生成，但这些标签在真实场景中往往噪声大、粗粒度、不可用或不足以覆盖多样性，导致节奏错位和风格漂移。相反，节拍速度（Tempo/BPM）是跨数据集与流派更稳定且核心的节奏属性。作者因此希望用Tempo而非Genre作为主要条件信号来提升节奏感知与对齐。

Method: 在扩散生成框架中引入TempoMoE：1) 按BPM区间将运动专家组织为“节奏分组”的层级结构；2) 在每个分组内设多尺度“拍点专家”，以同时建模细粒度与长时程的节奏动态；3) 设计分层的节奏自适应路由，从音乐特征中动态选择并融合合适的专家，实现对不同速度与节拍层级的自适应；整体实现无需手工流派标签的灵活、节奏对齐的运动生成。

Result: 在多项实验上取得SOTA：相较基线显著提升舞蹈质量与音乐-动作节奏对齐度；在不同BPM范围内均能稳定生成与节拍一致的动作，减少风格漂移与节奏失配。

Conclusion: 利用Tempo这一稳健的节奏属性，通过分层MoE与自适应路由增强扩散模型，可在无需不可靠的流派标签下实现更高质量与更强节奏一致性的3D舞蹈生成。

Abstract: Music to 3D dance generation aims to synthesize realistic and rhythmically synchronized human dance from music. While existing methods often rely on additional genre labels to further improve dance generation, such labels are typically noisy, coarse, unavailable, or insufficient to capture the diversity of real-world music, which can result in rhythm misalignment or stylistic drift. In contrast, we observe that tempo, a core property reflecting musical rhythm and pace, remains relatively consistent across datasets and genres, typically ranging from 60 to 200 BPM. Based on this finding, we propose TempoMoE, a hierarchical tempo-aware Mixture-of-Experts module that enhances the diffusion model and its rhythm perception. TempoMoE organizes motion experts into tempo-structured groups for different tempo ranges, with multi-scale beat experts capturing fine- and long-range rhythmic dynamics. A Hierarchical Rhythm-Adaptive Routing dynamically selects and fuses experts from music features, enabling flexible, rhythm-aligned generation without manual genre labels. Extensive experiments demonstrate that TempoMoE achieves state-of-the-art results in dance quality and rhythm alignment.

</details>


### [85] [FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation](https://arxiv.org/abs/2512.18809)
*Ziyuan Tao,Chuanzhi Xu,Sandaru Jayawardana,Wei Bao,Kanchana Thilakarathna,Teng Joon Lim*

Main category: cs.CV

TL;DR: 提出一种端侧联邦学习用于短视频暴力检测：用自监督VideoMAE表示+LoRA高效微调+多层隐私保护（DP-SGD与安全聚合），在40客户端RWF-2000上达77.25%（无隐私）与65-66%（强DP），通信成本较全模型FL降28.3倍，训练参数仅5.5M（3.5%）。


<details>
  <summary>Details</summary>
Motivation: 短视频平台激增，需要在不上传原视频到云端的前提下做内容审查，避免隐私泄露、带宽与延迟开销；现有云端或全模型联邦方法要么隐私风险高，要么通信与训练成本大。

Method: 构建端侧联邦学习框架：1) 以自监督预训练的VideoMAE作为视觉骨干提取通用时空表征；2) 采用LoRA进行参数高效适配，将可训练参数缩减到5.5M（约占156M骨干的3.5%）；3) 在联邦优化中使用DP-SGD（可配置隐私预算）并结合安全聚合，实现纵深隐私防护；4) 在40个客户端场景上进行视频暴力检测任务训练与评估。

Result: 在RWF-2000数据集、40客户端设置下：无隐私保护时准确率77.25%；启用强差分隐私后准确率65–66%；与全模型联邦学习相比，通信成本降低28.3倍。

Conclusion: 在端侧完成视频暴力检测是可行的；通过VideoMAE表示与LoRA降低训练与通信开销，并可在启用强DP时仍保持合理精度，为隐私敏感的短视频审核提供实用方案。

Abstract: The rapid growth of short-form video platforms increases the need for privacy-preserving moderation, as cloud-based pipelines expose raw videos to privacy risks, high bandwidth costs, and inference latency. To address these challenges, we propose an on-device federated learning framework for video violence detection that integrates self-supervised VideoMAE representations, LoRA-based parameter-efficient adaptation, and defense-in-depth privacy protection. Our approach reduces the trainable parameter count to 5.5M (~3.5% of a 156M backbone) and incorporates DP-SGD with configurable privacy budgets and secure aggregation. Experiments on RWF-2000 with 40 clients achieve 77.25% accuracy without privacy protection and 65-66% under strong differential privacy, while reducing communication cost by $28.3\times$ compared to full-model federated learning. The code is available at: {https://github.com/zyt-599/FedVideoMAE}

</details>


### [86] [Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction](https://arxiv.org/abs/2512.18813)
*Guangtao Lyu,Xinyi Cheng,Chenghao Xu,Qi Liu,Muli Yang,Fen Fang,Huilin Chen,Jiexi Yan,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 论文分析LVLM内部机制，发现感知呈GATE三阶段（全局-聚拢-探索），生成呈SAD模式（次主导积累导致幻觉），据此提出VDC方法检测并替换不受支持的幻觉词，显著降低幻觉率。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态任务上强大但易产生幻觉（与图像或知识不一致的输出），需要理解其内部动态并据此设计抑制幻觉的通用策略。

Method: 对LVLM的层级注意与FFN动态进行系统分析：1) 跟踪不同层的视觉注意分布，提出GATE三阶段感知模式；2) 分析生成时token概率演化，发现SAD模式中无注意/知识支撑的次主导token经多步积累成为主导并引发幻觉；3) 基于此提出VDC：在解码过程中检测缺乏视觉或FFN支持的token，将其以“验证过的主导”token替换或校正。

Result: 在多种LVLM与多基准上，VDC显著减少幻觉（相较基线大幅提升可靠性），跨模型泛化良好。

Conclusion: LVLM的感知与生成有可复现的内部演化规律（GATE与SAD），利用该规律的VDC可有效抑制幻觉，提升多模态输出可信度。

Abstract: Large Vision-Language Models (LVLMs) have shown remarkable capabilities, yet hallucinations remain a persistent challenge. This work presents a systematic analysis of the internal evolution of visual perception and token generation in LVLMs, revealing two key patterns. First, perception follows a three-stage GATE process: early layers perform a Global scan, intermediate layers Approach and Tighten on core content, and later layers Explore supplementary regions. Second, generation exhibits an SAD (Subdominant Accumulation to Dominant) pattern, where hallucinated tokens arise from the repeated accumulation of subdominant tokens lacking support from attention (visual perception) or feed-forward network (internal knowledge). Guided by these findings, we devise the VDC (Validated Dominance Correction) strategy, which detects unsupported tokens and replaces them with validated dominant ones to improve output reliability. Extensive experiments across multiple models and benchmarks confirm that VDC substantially mitigates hallucinations.

</details>


### [87] [EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer](https://arxiv.org/abs/2512.18814)
*Yuxiao Yang,Hualian Sheng,Sijia Cai,Jing Lin,Jiahao Wang,Bing Deng,Junzhe Lu,Haoqian Wang,Jieping Ye*

Main category: cs.CV

TL;DR: EchoMotion提出将人体运动与外观外观联合建模，以改善复杂人体动作视频生成。通过DiT双分支、统一3D位置编码MVS-RoPE及两阶段训练，并构建8万对视频-动作数据集HuMoVe，实现更高时空一致性与跨模态生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型以像素重建为主，偏向外观保真，难以学习人体运动学规律，导致复杂动作生成失真或不连贯。需要显式引入人体运动表征并在模型结构与训练目标中对齐运动与视频，以提升人类动作的可控性、连贯性与物理合理性。

Method: 在DiT基础上构建双分支架构，联合处理视频与运动（如关节序列）拼接的多模态token；提出MVS-RoPE，为视频与运动token提供统一、同步的3D位置编码，建立同一坐标系以促进时间与空间对齐；提出“运动-视频两阶段训练策略”，既能联合生成视频与对应运动序列，也支持跨模态条件生成；同时构建HuMoVe数据集（约8万对高质量人类视频-动作配对）以支撑训练。

Result: 显式的人体运动表征与外观信息互补，能显著提升以人为中心的视频生成的连贯性、动作可行性与时序一致性；模型在复杂动作生成与跨模态条件任务上取得更好表现。

Conclusion: 将运动与外观联合建模，并通过同步位置编码与两阶段训练建立跨模态对齐，可有效缓解像素目标的局限，提升复杂人体动作视频生成的质量与可控性；大规模配对数据对这一范式至关重要。

Abstract: Video generation models have advanced significantly, yet they still struggle to synthesize complex human movements due to the high degrees of freedom in human articulation. This limitation stems from the intrinsic constraints of pixel-only training objectives, which inherently bias models toward appearance fidelity at the expense of learning underlying kinematic principles. To address this, we introduce EchoMotion, a framework designed to model the joint distribution of appearance and human motion, thereby improving the quality of complex human action video generation. EchoMotion extends the DiT (Diffusion Transformer) framework with a dual-branch architecture that jointly processes tokens concatenated from different modalities. Furthermore, we propose MVS-RoPE (Motion-Video Syncronized RoPE), which offers unified 3D positional encoding for both video and motion tokens. By providing a synchronized coordinate system for the dual-modal latent sequence, MVS-RoPE establishes an inductive bias that fosters temporal alignment between the two modalities. We also propose a Motion-Video Two-Stage Training Strategy. This strategy enables the model to perform both the joint generation of complex human action videos and their corresponding motion sequences, as well as versatile cross-modal conditional generation tasks. To facilitate the training of a model with these capabilities, we construct HuMoVe, a large-scale dataset of approximately 80,000 high-quality, human-centric video-motion pairs. Our findings reveal that explicitly representing human motion is complementary to appearance, significantly boosting the coherence and plausibility of human-centric video generation.

</details>


### [88] [Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models](https://arxiv.org/abs/2512.18843)
*Hasib Aslam,Muhammad Talal Faiz,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 提出用Transformer从EEG中提取时空表征并注入扩散模型注意力，以从脑电重建视觉刺激，显著提升语义聚类与零样本泛化，同时生成质量与基线相当。


<details>
  <summary>Details</summary>
Motivation: 脑电解码虽有进展，但噪声大、空间扩散、时序变化强，导致难以解释神经表征与通用语义建模不足；需要能从EEG中稳健提取与视觉刺激相关的语义特征并用于可视重建。

Method: 构建基于Transformer的时空特征提取器，从EEG记录学习与观测视觉刺激相关的表征；将提取的特征作为条件信息融入潜变量扩散模型（LDM）的注意力机制，引导图像重建。使用公共数据集进行训练与评估。

Result: 相比现有方法，在潜空间聚类准确率提升最高6.5%，在未见类别的零样本泛化提升最高11.8%；同时Inception Score和FID与基线相当，表明生成质量未劣化。

Conclusion: 方法有效从EEG中提取可泛化的语义结构，并能在不牺牲生成质量的前提下改进语义解码与跨类泛化，是迈向可解释、可泛化EEG语义解读的重要一步。

Abstract: Advances in neuroscience and artificial intelligence have enabled preliminary decoding of brain activity. However, despite the progress, the interpretability of neural representations remains limited. A significant challenge arises from the intrinsic properties of electroencephalography (EEG) signals, including high noise levels, spatial diffusion, and pronounced temporal variability. To interpret the neural mechanism underlying thoughts, we propose a transformers-based framework to extract spatial-temporal representations associated with observed visual stimuli from EEG recordings. These features are subsequently incorporated into the attention mechanisms of Latent Diffusion Models (LDMs) to facilitate the reconstruction of visual stimuli from brain activity. The quantitative evaluations on publicly available benchmark datasets demonstrate that the proposed method excels at modeling the semantic structures from EEG signals; achieving up to 6.5% increase in latent space clustering accuracy and 11.8% increase in zero shot generalization across unseen classes while having comparable Inception Score and Fréchet Inception Distance with existing baselines. Our work marks a significant step towards generalizable semantic interpretation of the EEG signals.

</details>


### [89] [VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference](https://arxiv.org/abs/2512.18853)
*Sicheng Song,Yanjie Zhang,Zixin Chen,Huamin Qu,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VizDefender提出用半脆弱水印+MLLM意图分析，精准定位可视化篡改并解释其误导意图。


<details>
  <summary>Details</summary>
Motivation: 数据可视化图像易被细微编辑而产生欺骗性篡改，现有方法难以同时做到精确定位与理解篡改的误导目的，需一体化检测与解释框架。

Method: 提出VizDefender框架：1) 半脆弱水印模块，在可视化图像中嵌入位置映射，能在不显著降低视觉质量的前提下定位被篡改区域；2) 意图分析模块，借助多模态大模型（MLLM）对检测到的操纵进行语义解释，推断攻击者意图与可能的误导效果。通过形成性研究定义问题与篡改类型（数据操纵、视觉编码操纵）。

Result: 实验与用户研究显示：框架能有效检测并精确定位篡改区域，且MLLM意图分析能合理解释操纵与潜在误导。

Conclusion: VizDefender能在保证图像质量的同时实现篡改定位与意图解释，为可视化完整性提供实用的检测与分析手段。

Abstract: The integrity of data visualizations is increasingly threatened by image editing techniques that enable subtle yet deceptive tampering. Through a formative study, we define this challenge and categorize tampering techniques into two primary types: data manipulation and visual encoding manipulation. To address this, we present VizDefender, a framework for tampering detection and analysis. The framework integrates two core components: 1) a semi-fragile watermark module that protects the visualization by embedding a location map to images, which allows for the precise localization of tampered regions while preserving visual quality, and 2) an intent analysis module that leverages Multimodal Large Language Models (MLLMs) to interpret manipulation, inferring the attacker's intent and misleading effects. Extensive evaluations and user studies demonstrate the effectiveness of our methods.

</details>


### [90] [Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification](https://arxiv.org/abs/2512.18864)
*Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: DeX提出一种无需再训练的跨模态分解式方法，用图像特定语义概念生成自然语言反事实解释，量化场景元素对隐私分类决策的差异化贡献，并揭示偏差以助公平性改进。


<details>
  <summary>Details</summary>
Motivation: 现有概念驱动的反事实解释常缺乏与具体图像绑定的语义、难比较多种解释、且在主观情境（如图像隐私）中难以量化元素贡献与相互影响；同时希望在不改动模型的前提下提供可操作、公平性的解释工具。

Method: 提出Decompose and Explain (DeX)：1) 跨模态分解，将图像分解为图像特定概念/场景元素并映射到自然语言；2) 多准则概念选择，兼顾与原图相似性（最小扰动）与决策置信度变化（影响力）；3) 生成自然语言反事实，比较不同解释并评估属性间依赖与相互影响；4) 全流程无需对原分类器再训练。

Result: 在图像隐私决策任务上，DeX生成稀疏、与图像贴合的解释，显著优于现有方法；实现对关键因素的差异化量化；能评估解释多样性与属性交互。

Conclusion: DeX能以训练无关、灵活的框架揭示主观决策的主要驱动因素，提供高质量反事实解释，并发现数据集偏差，从而支持有针对性的缓解与公平性改进。

Abstract: Concept-driven counterfactuals explain decisions of classifiers by altering the model predictions through semantic changes. In this paper, we present a novel approach that leverages cross-modal decompositionality and image-specific concepts to create counterfactual scenarios expressed in natural language. We apply the proposed interpretability framework, termed Decompose and Explain (DeX), to the challenging domain of image privacy decisions, which are contextual and subjective. This application enables the quantification of the differential contributions of key scene elements to the model prediction. We identify relevant decision factors via a multi-criterion selection mechanism that considers both image similarity for minimal perturbations and decision confidence to prioritize impactful changes. This approach evaluates and compares diverse explanations, and assesses the interdependency and mutual influence among explanatory properties. By leveraging image-specific concepts, DeX generates image-grounded, sparse explanations, yielding significant improvements over the state of the art. Importantly, DeX operates as a training-free framework, offering high flexibility. Results show that DeX not only uncovers the principal contributing factors influencing subjective decisions, but also identifies underlying dataset biases allowing for targeted mitigation strategies to improve fairness.

</details>


### [91] [Application of deep learning approaches for medieval historical documents transcription](https://arxiv.org/abs/2512.18865)
*Maksym Voloshchuk,Bohdana Zarembovska,Mykola Kozlenko*

Main category: cs.CV

TL;DR: 提出一条面向9–11世纪拉丁手稿的深度学习识读流水线，从目标检测到词级识别，并给出多种评估指标与可视化，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现代HTR/OCR在当代字体和版式上表现优异，但在中世纪拉丁手稿上效果显著下降；需要针对手稿的噪声、书写体多样性、版面不规整等特性构建专门方法与数据集。

Method: 1) 针对中世纪文献特性进行原始数据初探与清洗；2) 构建并标注数据集；3) 进行EDA以理解数据分布；4) 设计端到端流水线：文档图像中的对象/文本块检测→词图像提取→基于分类与嵌入的方法进行词级识别；5) 以多指标评估（召回、精确率、F1、IoU、混淆矩阵、平均字符串距离），并提供指标曲线；6) 开源实现（GitHub）。

Result: 在所构建数据集与模型上获得了可量化的识别性能，报告了召回、精确率、F1、IoU、混淆矩阵与平均字符串距离等结果与曲线；具体数值未在摘要中给出，但表明方法可行且性能有据可依。

Conclusion: 面向中世纪拉丁手稿的定制深度学习流水线有效提升了词级文本提取能力；数据集构建与EDA对性能至关重要；公开代码与流程为后续研究与复现提供基础。

Abstract: Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.

</details>


### [92] [CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis](https://arxiv.org/abs/2512.18878)
*Kaidi Liang,Ke Li,Xianbiao Hu,Ruwen Qin*

Main category: cs.CV

TL;DR: 提出CrashChat：基于VideoLLaMA3的多任务交通事故视频分析MLLM，通过指令微调与新的多任务分组学习策略，实现统一的事故识别、时间定位与高层语义理解，显著优于现有MLLM与传统视觉方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以在统一框架下同时完成事故识别、时间定位、描述与推理等多样化任务，且缺乏有效的多任务训练策略，限制了对快速增长的行车视频数据在安全研究与责任认定中的利用。

Method: 在VideoLLaMA3上构建CrashChat，进行交通事故领域指令微调；提出“任务解耦与分组”的多任务学习策略，在组内共享、组间协调以增强正迁移并抑制负迁移；统一支持事故识别、事故/预事故时间定位、描述与推理等任务；在整合的公共数据集上训练与评测。

Result: CrashChat在各模型规模上均优于通用MLLM与传统视觉方法：事故识别接近完美准确率；事故定位提升176%；预事故定位提升40%；描述与推理文本质量显著提高，BLEU提升0.18–0.41、ROUGE提升0.18–0.42。

Conclusion: CrashChat作为端到端的多任务事故视频分析工具，在统一框架下实现高性能并具备落地潜力；其任务分组多任务学习策略有效缓解负迁移并提升跨任务泛化。数据与代码已开源。

Abstract: Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\% improvement in crash localization, and a 40\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.

</details>


### [93] [Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)](https://arxiv.org/abs/2512.18888)
*Akshit Achara,Peter Triantafillou,Esther Puyol-Antón,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: 提出OSCAR框架，用相关性度量和区域排序量化/定位捷径学习，并在多个数据集上验证稳定、敏感且可区分局部/弥散捷径；还可用于测试时缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 深度网络常利用与语义无关但与标签相关的“捷径”特征，尤其当这些特征与敏感属性相关时会造成偏见。现有方法多依赖人眼可见的图像级检查，难以在如医学影像等场景适用，缺乏量化评估与细粒度定位。

Method: 提出OSCAR：先从模型生成图像级任务归因图（例如基于解释方法），将其转换为数据集层面的图像区域排序谱；对三类模型——平衡基线(BA)、待测模型(TS)和敏感属性预测器(SA)——分别得到排序谱；计算排序谱之间的成对、偏相关和基于偏离的相关度，形成量化指标来评估TS对捷径的依赖程度，并给出最贡献的空间区域排名。该流程模型无关、在像素空间操作，输出统计决策规则与空间热力图。

Result: 在CelebA、CheXpert、ADNI上：相关性指标对随机种子与数据划分稳定；对训练数据中捷径与标签关联强度敏感；可区分局部化与弥散型捷径特征。基于识别出的捷径区域，设计简单的测试时衰减策略，能降低最差群体的性能差异。

Conclusion: OSCAR提供轻量级、可量化的捷径审计与定位方法，适用于不可见或微弱的捷径特征，能用于检测、定位并缓解模型对捷径的依赖，促进公平与稳健。

Abstract: Deep neural networks often exploit shortcuts. These are spurious cues which are associated with output labels in the training data but are unrelated to task semantics. When the shortcut features are associated with sensitive attributes, shortcut learning can lead to biased model performance. Existing methods for localising and understanding shortcut learning are mostly based upon qualitative, image-level inspection and assume cues are human-visible, limiting their use in domains such as medical imaging. We introduce OSCAR (Ordinal Scoring Correlations for Attribution Representations), a model-agnostic framework for quantifying shortcut learning and localising shortcut features. OSCAR converts image-level task attribution maps into dataset-level rank profiles of image regions and compares them across three models: a balanced baseline model (BA), a test model (TS), and a sensitive attribute predictor (SA). By computing pairwise, partial, and deviation-based correlations on these rank profiles, we produce a set of quantitative metrics that characterise the degree of shortcut reliance for TS, together with a ranking of image-level regions that contribute most to it. Experiments on CelebA, CheXpert, and ADNI show that our correlations are (i) stable across seeds and partitions, (ii) sensitive to the level of association between shortcut features and output labels in the training data, and (iii) able to distinguish localised from diffuse shortcut features. As an illustration of the utility of our method, we show how worst-group performance disparities can be reduced using a simple test-time attenuation approach based on the identified shortcut regions. OSCAR provides a lightweight, pixel-space audit that yields statistical decision rules and spatial maps, enabling users to test, localise, and mitigate shortcut reliance. The code is available at https://github.com/acharaakshit/oscar

</details>


### [94] [Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs](https://arxiv.org/abs/2512.18897)
*Dmitry Demidov,Zaigham Zaheer,Zongyan Han,Omkar Thawakar,Rao Anwer*

Main category: cs.CV

TL;DR: FiNDR提出一种基于推理增强的大型多模态模型的词表无关细粒度识别框架，通过自动生成候选名称、VLM筛选成类集、再用已验证名称训练轻量分类器，显著超越现有方法与零样本基线，表明人类词表并非上限，且精心提示可让开源LMM匹敌闭源。


<details>
  <summary>Details</summary>
Motivation: 现有词表无关细粒度识别方法要么依赖庞大僵化词表、要么采用脆弱的多阶段启发式流程，导致泛化与鲁棒性不足；而近期LMM具备推理、分解与自纠错能力，提供了更原则化与自动化的替代方案。

Method: 提出FiNDR：三步自动流程。(1) 使用具推理能力的LMM为每张图像生成描述性候选标签；(2) 用视觉-语言模型对候选进行过滤与排序，形成一致的类别集合；(3) 将验证过的名称用于实例化一个轻量级多模态分类器以供推理时使用。并通过精心设计的提示让开源LMM也能胜任。

Result: 在多个细粒度基准上，在词表无关设定下取得SOTA，最高相对提升达18.8%；同时超过利用预定义真值名称的零样本基线；开源LMM在合适提示下可匹配专有模型表现。

Conclusion: 推理增强的LMM可作为可扩展、全自动、开放世界细粒度识别的有效基础，人类策划的词表不是性能上限，所提出的FiNDR框架验证了这一点，并已开源。

Abstract: Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.

</details>


### [95] [Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models](https://arxiv.org/abs/2512.18910)
*Mohamad Zamini,Diksha Shukla*

Main category: cs.CV

TL;DR: 提出Delta-LLaVA：一个更高效的视觉投影器，用低秩DeltaProjection将多层视觉特征压到紧凑子空间，再用轻量Transformer做专门化，在仅144视觉token下保持/提升表现，同时显著加速训练和推理。


<details>
  <summary>Details</summary>
Motivation: MLLM需要处理大量密集视觉token，带来高计算与冗余；常见用MLP直接映射视觉到语言，随分辨率升高扩展性差、冗余大。因此需要一种在不牺牲性能的前提下减少token并提高吞吐的视觉-语言对齐与交互方式。

Method: 设计Delta-LLaVA视觉投影器：1) 低秩DeltaProjection先将多层次（multi-level）视觉特征对齐到紧凑低秩子空间，进行“基底对齐”；2) 在该基础上堆叠少量轻量级Transformer块作为“专门化层”，在受限token预算下捕获全局与局部结构；3) 将形成的仅144个视觉token与LLM交互；并通过广泛消融验证基于“先对齐成基底、再专门化”的设计。

Result: 在多项基准中，使用仅144视觉token即可获得一致增益；推理吞吐量最高提升约55%；端到端训练预训练阶段加速约4-5倍，微调阶段加速超过1.5倍。

Conclusion: 先压缩对齐后专门化的投影-交互范式能在极少token下保持/提升MLLM性能，并显著提升推理与训练效率，提示在扩展交互容量前进行高质量token形成是关键。

Abstract: Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.

</details>


### [96] [LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer](https://arxiv.org/abs/2512.18930)
*Raina Panda,Daniel Fein,Arpita Singhal,Mark Fiore,Maneesh Agrawala,Matyas Bohacek*

Main category: cs.CV

TL;DR: 提出一种无需微调/适配器/额外推理、可解释的艺术风格迁移方法：在生成模型潜空间上训练艺术领域Sparse Autoencoder（LouvreSAE），从少量参考图像构建可分解的“风格剖面”向量以直接操控风格；在ArtBench10上达到/超越现有方法，同时1.7–20倍更快。


<details>
  <summary>Details</summary>
Motivation: 现有风格迁移常依赖模型微调、LoRA或提示工程，计算开销大且风格与内容纠缠严重，缺乏可解释性与复用性。需要一种轻量、可解释且能从少量样例稳健转移艺术风格的方法。

Method: 在生成图像模型的潜在嵌入之上训练一个面向艺术数据的稀疏自编码器（SAE）。该SAE在艺术语料上学习到解缠的风格与构图概念（如笔触、纹理、配色、语义与结构）。基于SAE激活构建“风格剖面”：紧凑、可组合的引导向量（steering vectors），可直接注入生成过程进行风格操控，无需微调、LoRA或额外推理。由少量参考图像提取风格剖面以实现风格迁移。

Result: 在ArtBench10上，风格评估指标（VGG Style Loss、CLIP Style Score）达到或优于现有方法，并带来1.7–20倍的速度提升；方法具备可解释性（概念级因果操控）。

Conclusion: LouvreSAE通过稀疏自编码器在潜空间学习解缠的风格概念，构建可分解的风格剖面，实现无需训练或额外推理的直接风格操控；在标准基准上更快且指标更优，提供实用且可解释的艺术风格迁移方案。

Abstract: Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.

</details>


### [97] [Point What You Mean: Visually Grounded Instruction Policy](https://arxiv.org/abs/2512.18933)
*Hang Yu,Juntu Zhao,Yufeng Liu,Kaiyu Li,Cheng Ma,Di Zhang,Yingdong Hu,Guang Chen,Junyuan Xie,Junliang Guo,Junqiao Zhao,Yang Gao*

Main category: cs.CV

TL;DR: 提出Point-VLA：在语言指令之外加入显式视觉指引（如框选）以消除指代歧义并提升物体级落地；通过自动化标注流水线低人力扩展数据；在真实场景指代任务上优于仅文本VLA，尤其在杂乱和OOD物体下，具更强泛化。


<details>
  <summary>Details</summary>
Motivation: 仅靠文本提示的VLA在杂乱或OOD场景中常出现目标指代不清，导致控制失败；需要一种能在像素层面明确指代目标、同时可规模化数据的方案。

Method: 1) 设计Point-VLA策略：在语言指令基础上加入显式视觉线索（如边界框/点），实现像素级 grounding，减少歧义并直接对接控制策略；2) 构建自动化数据标注流水线，以最小人力成本生成带视觉指引与动作对齐的数据，用于训练/微调；3) 在多种真实世界的指代操控任务上评估。

Result: 与仅文本的VLA相比，Point-VLA在多样真实场景任务中表现更强，尤其在杂乱场景和未见物体情况下提升显著，展现稳健泛化能力。

Conclusion: 通过在语言上叠加像素级视觉指引并配合自动化数据扩展，Point-VLA有效解决指代歧义，带来更精确与可泛化的具身控制。

Abstract: Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.

</details>


### [98] [Symmetrization of 3D Generative Models](https://arxiv.org/abs/2512.18953)
*Nicolas Caytuiro,Ivan Sipiran*

Main category: cs.CV

TL;DR: 通过仅在“半物体”数据上训练并在生成阶段镜像还原，促进3D生成模型产生具有反射对称性的完整形状。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型常生成缺乏反射对称（尤其沿平面反射）的形状，直接改模型结构复杂且代价高。作者希望以数据驱动方式，利用真实物体普遍的反射对称性，简化实现并提升生成质量与一致性。

Method: 1) 分析真实3D形状与SOTA模型样本的反射对称性；2) 将ShapeNet中三个类别（飞机、汽车、椅子）的形状沿x=0平面裁半并镜像得到“半物体”数据集；3) 仅在半物体上训练两种生成模型，使其学习半形状的丰富分布；4) 生成时对输出半形状做镜像拼接形成完整物体；5) 与原模型和原数据生成结果比较。

Result: 基于半物体训练的模型生成的形状在几何上明显更对称、外观更一致；在与原始模型及原始数据训练的基线相比，生成结果的对称性与稳定性提升。

Conclusion: 无需改动模型结构，仅通过数据预处理（半物体训练+生成时镜像）即可显著提升3D生成模型的反射对称性与一致性，验证了数据中心方法的有效性。

Abstract: We propose a novel data-centric approach to promote symmetry in 3D generative models by modifying the training data rather than the model architecture. Our method begins with an analysis of reflectional symmetry in both real-world 3D shapes and samples generated by state-of-the-art models. We hypothesize that training a generative model exclusively on half-objects, obtained by reflecting one half of the shapes along the x=0 plane, enables the model to learn a rich distribution of partial geometries which, when reflected during generation, yield complete shapes that are both visually plausible and geometrically symmetric. To test this, we construct a new dataset of half-objects from three ShapeNet classes (Airplane, Car, and Chair) and train two generative models. Experiments demonstrate that the generated shapes are symmetrical and consistent, compared with the generated objects from the original model and the original dataset objects.

</details>


### [99] [VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion](https://arxiv.org/abs/2512.18954)
*Zaidao Han,Risa Higashita,Jiang Liu*

Main category: cs.CV

TL;DR: 提出VOIC：将相机单目3D语义场景补全显式拆分为可见区感知与遮挡区推理，通过离线可见区标签提取（VRLE）净化监督，双解码器交互提升几何与语义精度，在SemanticKITTI与SSCBench-KITTI360达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目SSC把2D特征直接提升到3D并统一补全，易因单帧可见区置信度高、遮挡区置信度低而相互干扰，导致特征稀释与误差传播；缺乏对可见与遮挡区域的显式区分与针对性监督/推理。

Method: 1) VRLE：从稠密3D真值中离线提取体素级“可见区”监督，分离可见与遮挡体素；2) VOIC框架：先融合图像特征与深度占据构建基础3D体素表征；采用双解码器—可见解码器生成高保真几何与语义先验，遮挡解码器在这些先验与跨模态交互的辅助下进行全局遮挡区补全；显式将SSC拆为“可见感知”和“遮挡推理”，减少相互干扰。

Result: 在SemanticKITTI与SSCBench-KITTI360上，几何补全与语义分割均优于现有单目SSC方法，达到SOTA（文中报告的主流指标全面提升）。

Conclusion: 显式区分并分别监督/推理可见与遮挡区域，结合双解码交互与深度占据融合，可有效缓解单目SSC的干扰与误差传播问题，显著提升全局3D补全与语义精度。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.

</details>


### [100] [DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation](https://arxiv.org/abs/2512.18964)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: 提出DVI零样本身份定制框架，解耦“语义身份”与“视觉氛围”，用VAE潜空间的均值/方差描述全局视觉并无参调制语义特征，结合时间粒度调度，在扩散早期注重氛围、后期精化语义，在不微调参数的前提下缓解“贴纸感”，兼顾身份保真与视觉一致性，IBench上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有免调优身份定制方法虽能保留人脸几何/身份，但忽略光照、肤质、环境色调等视觉上下文，导致面部像贴纸般与场景氛围不一致的“语义-视觉失配”。需要一种既保身份又保全局视觉气质的方案，且最好无需训练/调参。

Method: 1) 将身份表示正交解耦为细粒度语义流与粗粒度视觉流；2) 利用VAE潜空间统计量（均值、方差）作为轻量全局视觉描述符；3) 参数无关的特征调制：用上述统计自适应调制语义嵌入，将“视觉灵魂”注入，无需训练；4) 动态时间粒度调度：随扩散进程，前期强调视觉氛围，后期强化语义细节。

Result: 大量实验显示：在不进行参数微调的情况下，DVI显著提升画面视觉一致性与氛围保真，同时保持稳健的身份保留；在IBench基准上整体优于现有SOTA。

Conclusion: 通过统计驱动的解耦与无参调制，加上与扩散过程对齐的时间调度，DVI有效解决身份定制中的语义-视觉失配，实现零样本、高保真、氛围一致的生成，并在标准评测中取得领先。

Abstract: Recent tuning-free identity customization methods achieve high facial fidelity but often overlook visual context, such as lighting, skin texture, and environmental tone. This limitation leads to ``Semantic-Visual Dissonance,'' where accurate facial geometry clashes with the input's unique atmosphere, causing an unnatural ``sticker-like'' effect. We propose **DVI (Disentangled Visual-Identity)**, a zero-shot framework that orthogonally disentangles identity into fine-grained semantic and coarse-grained visual streams. Unlike methods relying solely on semantic vectors, DVI exploits the inherent statistical properties of the VAE latent space, utilizing mean and variance as lightweight descriptors for global visual atmosphere. We introduce a **Parameter-Free Feature Modulation** mechanism that adaptively modulates semantic embeddings with these visual statistics, effectively injecting the reference's ``visual soul'' without training. Furthermore, a **Dynamic Temporal Granularity Scheduler** aligns with the diffusion process, prioritizing visual atmosphere in early denoising stages while refining semantic details later. Extensive experiments demonstrate that DVI significantly enhances visual consistency and atmospheric fidelity without parameter fine-tuning, maintaining robust identity preservation and outperforming state-of-the-art methods in IBench evaluations.

</details>


### [101] [Total Curvature Regularization and its_Minimization for Surface and Image Smoothing](https://arxiv.org/abs/2512.18968)
*Tianle Lu,Ke Chen,Yuping Duan*

Main category: cs.CV

TL;DR: 提出一种基于多方向法向曲率惩罚的总法向曲率正则化，通过PDE稳态求解与算子分裂离散，获得既保边缘又各向同性的平滑效果，算法高效稳健、参数不敏感，并在曲面与图像平滑中验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有曲率正则化常在保边缘与各向同性之间权衡，或需复杂参数调优，且高阶非线性优化求解困难；作者旨在同时实现尖锐边缘保留、精确各向同性和平滑效率鲁棒性。

Method: 构造“总法向曲率”正则项：从多方向惩罚法向曲率；将原高阶非线性问题等价为一组随时间演化的PDE稳态解；时间离散采用算子分裂，将每个分步化为闭式或可用高效算法求解的子问题；整体无需繁琐参数调节。

Result: 在曲面与图像平滑任务上，方法实现了清晰边缘、各向同性平滑与噪声抑制，运算高效、对参数不敏感；与基线相比表现更优（文中声称经过严格验证）。

Conclusion: 多方向法向曲率惩罚结合PDE稳态与算子分裂提供了兼顾锋利边缘与各向同性的高效鲁棒正则化框架，适用于表面与图像平滑等任务，减少了参数调优负担。

Abstract: We introduce a novel formulation for curvature regularization by penalizing normal curvatures from multiple directions. This total normal curvature regularization is capable of producing solutions with sharp edges and precise isotropic properties. To tackle the resulting high-order nonlinear optimization problem, we reformulate it as the task of finding the steady-state solution of a time-dependent partial differential equation (PDE) system. Time discretization is achieved through operator splitting, where each subproblem at the fractional steps either has a closed-form solution or can be efficiently solved using advanced algorithms. Our method circumvents the need for complex parameter tuning and demonstrates robustness to parameter choices. The efficiency and effectiveness of our approach have been rigorously validated in the context of surface and image smoothing problems.

</details>


### [102] [Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning](https://arxiv.org/abs/2512.18969)
*Cheng-Hong Chang,Pei-Hsuan Tsai*

Main category: cs.CV

TL;DR: 该文提出SASOW方法，在组合零样本学习任务中同时提升状态与对象识别，并在组合时引入状态/对象权重，较OW-CZSL与KG-SP在三大数据集未见组合上取得最高+2.1%、+1.7%、+0.4%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有物体识别多停留在“只识别对象”，缺乏对“对象+状态”的联合识别；把“状态-对象”当作单一类别训练需要覆盖所有组合，数据采集与训练成本高。CZSL通过将状态与对象分开学习来应对未见组合，但SOTA方法KG-SP在状态/对象分类精度及组合评估上仍有改进空间，且忽略了状态与对象在组合时的权重差异。

Method: 在KG-SP框架上增强：(1) 为状态分类器与对象分类器引入自注意力模块，提升各自识别精度与特征建模；(2) 在组合阶段显式引入状态与对象的加权机制，融合语义模型评估组合可行性，从而生成更合理的组合评分；(3) 在MIT-States、UT Zappos、C-GQA三个基准上进行验证，与OW-CZSL、KG-SP对比。

Result: 在未见组合（unseen compositions）准确率上优于对比方法：MIT-States提升2.1%，UT Zappos提升1.7%，C-GQA提升0.4%。

Conclusion: 自注意力增强的状态/对象分类与显式权重化组合能提高CZSL中的组合识别效果；SASOW在多个数据集上带来稳定但幅度不一的提升，证明考虑状态/对象权重与更强特征建模的有效性。

Abstract: Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.

</details>


### [103] [ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation](https://arxiv.org/abs/2512.18991)
*Gyeongrok Oh,Youngdong Jang,Jonghyun Choi,Suk-Ju Kang,Guang Lin,Sangpil Kim*

Main category: cs.CV

TL;DR: 提出ICP-4D：无需训练的4D LiDAR全景分割框架，用ICP+Sinkhorn软匹配在实例级点集上进行时空对齐，实现稳健关联与高效推理，在SemanticKITTI与panoptic nuScenes上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有4D全景分割方法依赖大量叠加点云训练或复杂实例关联模块，带来冗余点处理与高算力开销，同时忽视原始点云的几何先验；需要一种更简单、训练无关且能充分利用几何关系的方案。

Method: 把实例级点集视为几何对象，直接用ICP在相邻帧之间估计刚体变换实现实例时序关联；为应对噪声与不确定性，引入基于Sinkhorn的软匹配获取稳定点对应关系以提升几何对齐鲁棒性；设计考虑静态/动态/缺失三类实例的管线，实现遮挡感知与高效匹配。

Result: 在SemanticKITTI和panoptic nuScenes上，无需额外训练或额外点云输入，即在多项指标上持续超过当下SOTA方法，且具备较高的计算效率。

Conclusion: 通过统一的几何对齐与软匹配机制，ICP-4D在无需训练的前提下实现稳健高效的4D LiDAR全景分割，验证了利用原始点云几何先验即可取得领先性能。

Abstract: Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.

</details>


### [104] [Towards AI-Guided Open-World Ecological Taxonomic Classification](https://arxiv.org/abs/2512.18994)
*Cheng Yaw Low,Heejoon Koo,Jaewoo Park,Kaleb Mesfin Asfaw,Meeyoung Cha*

Main category: cs.CV

TL;DR: 提出TaxoNet用于开放世界生态分类，在多数据集上优于基线，特别提升稀有类识别。


<details>
  <summary>Details</summary>
Motivation: 现实生态监测存在长尾类别分布、细粒度区分难、时空域移、以及封闭集假设不满足开放世界需求，这些共同阻碍生物多样性监测与决策。

Method: 提出Open-World Ecological Taxonomy Classification框架与TaxoNet：基于嵌入的编码器，采用双边距惩罚损失（dual-margin penalization），增强对稀有类的学习信号、抑制头部类主导，面向开放世界识别并处理域移与细粒度差异。

Result: 在Google Auto-Arborist、iNat-Plantae、NAFlora-Mini等多生态域数据集上，方法总体优于各类基线，稀有类性能提升尤为显著；通用多模态基础模型在植物领域表现受限。

Conclusion: TaxoNet为开放世界植物分类提供强有力基线，能更好覆盖长尾与未知类挑战，但通用基础模型仍需领域适配。

Abstract: AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.

</details>


### [105] [CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization](https://arxiv.org/abs/2512.19020)
*Zelin Zhao,Xinyu Gong,Bangya Liu,Ziyang Song,Jun Zhang,Suhui Wu,Yongxin Chen,Hao Zhang*

Main category: cs.CV

TL;DR: CETCAM在无相机标注的前提下，实现可控相机的视频生成，通过几何一致的token化方法把深度与相机参数融入扩散模型，达到SOTA的几何一致性、时序稳定性与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成的相机控制依赖人工或估计的相机姿态标注，难以扩展、与深度估计不一致，训练/测试分布不匹配，影响可控性与画质。需要一种无需标注、几何一致且可扩展的方案。

Method: 提出CETCAM：利用几何基础模型（如VGGT）从原始视频估计深度与相机参数，将其统一编码为“几何感知token”，通过轻量级上下文块注入到预训练视频扩散骨干；采用两阶段训练：阶段一在大规模原始视频上学习稳健相机可控性；阶段二在高保真数据上精修细节与画质。框架可扩展至修复、布局等额外控制。

Result: 在多个基准上达到SOTA的几何一致性、时间稳定性与视觉真实感；在额外控制任务（如inpainting与layout）上展现良好适配性与灵活性。

Conclusion: 通过一致可扩展的几何token化与两阶段训练，CETCAM在无相机标注下实现强相机可控的视频生成，并兼具高质量与多控制模态的扩展能力。

Abstract: Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.

</details>


### [106] [VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation](https://arxiv.org/abs/2512.19021)
*Sihao Lin,Zerui Li,Xunyi Zhao,Gengze Zhou,Liuyi Wang,Rong Wei,Rui Tang,Juncheng Li,Hanqing Wang,Jiangmiao Pang,Anton van den Hengel,Jiajun Liu,Qi Wu*

Main category: cs.CV

TL;DR: 提出VLNVerse：一个用于视觉-语言导航（VLN）的可扩展大规模基准，统一多任务、具备真实物理与全身运动学仿真，并对现有方法与新提出的统一多任务模型进行系统评测，以缩小仿真到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 现有VLN基准数据集小、物理仿真简单、任务割裂、难以评估sim-to-real泛化且无法满足基于LLM的预训练数据规模需求，导致研究进展受限。

Method: 构建VLNVerse：一个“多样化、具身化、现实仿真、可评测”的统一框架与工具包。特性包括：统一先前分裂的任务；引入具身体动力学（非“幽灵”瞬移代理），在稳健物理引擎中进行真实感仿真；提供可扩展的数据与任务定义。基于该基准对经典到多模态大模型（MLLM）代理进行全面评测；并提出一个能覆盖全部任务的统一多任务模型。

Result: 利用VLNVerse进行大规模、系统性评测，显示对现有方法（含MLLM代理）提供更全面客观的比较；新提出的统一多任务模型能够在基准中同时解决所有任务（摘要未给出具体数值）。

Conclusion: VLNVerse作为大规模、可扩展、物理真实的VLN基准，统一任务、提升评测可信度，并有助于缩小仿真与现实的差距，推动面向通用具身导航/运动体代理的研究。

Abstract: Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting "ghost" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.

</details>


### [107] [Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection](https://arxiv.org/abs/2512.19022)
*Haoze Li,Jie Zhang,Guoying Zhao,Stephen Lin,Shiguang Shan*

Main category: cs.CV

TL;DR: 提出SVLP-IL：基于视觉-语言预训练模型的无回放增量学习框架，用于人脸攻击检测；通过多维提示和选择性弹性权重巩固，在不保留旧数据的前提下显著减缓遗忘并提升跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 人脸呈现攻击不断演化、域分布变化明显，实际部署需要能持续学习新攻击与新域；但隐私法规禁止保存旧任务数据，常见依赖重放的IL方法不可用，需一种隐私合规的无回放增量学习方案。

Method: 利用可提示调优的视觉-语言预训练模型作为基础架构：1) 多维提示（MAP）同时引入通用与域特定提示，隔离域依赖、增强对分布移位的敏感性并缓解遗忘；2) 选择性弹性权重巩固（SEWC）对先前任务关键参数施加选择性正则，既保留重要知识又为新任务保留可塑性。

Result: 在多个人脸攻击检测基准和跨域设置上进行系统实验，SVLP-IL相较基线显著降低灾难性遗忘，并在未见域上取得更好性能。

Conclusion: SVLP-IL在不存储历史数据的前提下实现对新攻击与新域的稳健增量适应，是隐私合规且实用的终身人脸PAD解决方案。

Abstract: Face Presentation Attack Detection (PAD) demands incremental learning (IL) to combat evolving spoofing tactics and domains. Privacy regulations, however, forbid retaining past data, necessitating rehearsal-free IL (RF-IL). Vision-Language Pre-trained (VLP) models, with their prompt-tunable cross-modal representations, enable efficient adaptation to new spoofing styles and domains. Capitalizing on this strength, we propose \textbf{SVLP-IL}, a VLP-based RF-IL framework that balances stability and plasticity via \textit{Multi-Aspect Prompting} (MAP) and \textit{Selective Elastic Weight Consolidation} (SEWC). MAP isolates domain dependencies, enhances distribution-shift sensitivity, and mitigates forgetting by jointly exploiting universal and domain-specific cues. SEWC selectively preserves critical weights from previous tasks, retaining essential knowledge while allowing flexibility for new adaptations. Comprehensive experiments across multiple PAD benchmarks show that SVLP-IL significantly reduces catastrophic forgetting and enhances performance on unseen domains. SVLP-IL offers a privacy-compliant, practical solution for robust lifelong PAD deployment in RF-IL settings.

</details>


### [108] [Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation](https://arxiv.org/abs/2512.19026)
*Connor Kilrain,David Carlyn,Julia Chae,Sara Beery,Wei-Lun Chao,Jianyang Gu*

Main category: cs.CV

TL;DR: 提出Finer-Personalization Rank，以检索排序方式评估个性化生成中的“身份保真”，优于仅语义相似的传统指标，并在多个细粒度与ReID数据集上揭示主流方法存在身份漂移。


<details>
  <summary>Details</summary>
Motivation: 个性化生成（如宠物/特定对象）要求保留独特身份细节，但现有评测多关注整体语义相似，忽视区分身份的细粒度特征，导致对“是否还是同一个个体/型号”的评估失真。

Method: 将评测转化为检索排序：把生成图像作为查询，去一个由真实、外观相似且带身份标签的图库中检索对应身份。使用mAP等检索指标度量，当生成图越能把对应身份排在前列，说明保留了更具体的身份细节。评估粒度涵盖细粒度类别（鸟种、车款）到个体实例（ReID）。

Result: 在CUB、Stanford Cars及动物ReID基准上，该协议比仅语义的指标更能反映身份保留情况，并发现多种流行个性化生成方法存在显著身份漂移。

Conclusion: 基于图库的排序评测为个性化生成提供了更有原则且可操作的身份保真测度，应作为评估个性化生成质量的标准方案之一。

Abstract: The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.

</details>


### [109] [Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach](https://arxiv.org/abs/2512.19032)
*Ran Li,Pan Xiao,Kaushik Dutta,Youdong Guo*

Main category: cs.CV

TL;DR: 提出一种用于光片显微4D钙成像的贝叶斯深度学习框架，融合时空信息进行神经元活动分割，并给出不确定性；在合成真值与复测一致性上取得约0.8的Dice，支持快速、可泛化的活动检测。


<details>
  <summary>Details</summary>
Motivation: 传统依赖人工分割，费时费力且泛化差，难以高效、稳定地从大规模在体钙成像数据中提取与行为相关的神经元活动。

Method: 在4D时空数据中：以像素级相关图刻画时间维度活动模式，以平均汇总图提供空间信息；将两者输入贝叶斯深度学习分割框架，输出概率分割并显式建模检测不确定性；通过“可重复性测试”评估网络跨次运行与数据的泛化能力。

Result: 相对基于大津法得到的合成真值，平均Dice=0.81；两次独立运行的可重复性Dice=0.79，显示较好一致性与稳健性。

Conclusion: 该方法能够快速、自动地检测活跃神经元，并量化不确定性，具有良好泛化与可重复性，适用于行为学研究中的大规模脑活动映射。

Abstract: Fluorescence Microcopy Calcium Imaging is a fundamental tool to in-vivo record and analyze large scale neuronal activities simultaneously at a single cell resolution. Automatic and precise detection of behaviorally relevant neuron activity from the recordings is critical to study the mapping of brain activity in organisms. However a perpetual bottleneck to this problem is the manual segmentation which is time and labor intensive and lacks generalizability. To this end, we present a Bayesian Deep Learning Framework to detect neuronal activities in 4D spatio-temporal data obtained by light sheet microscopy. Our approach accounts for the use of temporal information by calculating pixel wise correlation maps and combines it with spatial information given by the mean summary image. The Bayesian framework not only produces probability segmentation maps but also models the uncertainty pertaining to active neuron detection. To evaluate the accuracy of our framework we implemented the test of reproducibility to assert the generalization of the network to detect neuron activity. The network achieved a mean Dice Score of 0.81 relative to the synthetic Ground Truth obtained by Otsu's method and a mean Dice Score of 0.79 between the first and second run for test of reproducibility. Our method successfully deployed can be used for rapid detection of active neuronal activities for behavioural studies.

</details>


### [110] [Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition](https://arxiv.org/abs/2512.19036)
*Xiaoyang Li,Mingming Lu,Ruiqi Wang,Hao Li,Zewei Le*

Main category: cs.CV

TL;DR: 提出CLIP-SPM框架用于小样本动作识别，通过运动特征精炼、语义原型调制与原型-锚点双重调制，缓解时间建模困难、相似类别区分难与模态差距问题，并在多基准与多shot设定上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 小样本动作识别在真实应用中标注稀缺，模型易受静态背景干扰、相似动作难分、视觉查询与视觉-文本支撑原型存在模态鸿沟，需一种能同时增强时间建模、区分细粒度类别并缩小模态差距的方案。

Method: 提出CLIP-SPM框架：1) HSMR层次协同运动精炼，对齐深浅层运动线索、抑制静态背景，强化时间建模；2) SPM语义原型调制，为每个查询生成相关文本提示并与视觉特征融合，缩小视觉-文本模态差距并提升相似动作可分性；3) PADM原型-锚点双重调制，精炼支撑原型并以全局语义锚对齐查询特征，提升支撑与查询的一致性。

Result: 在Kinetics、SSv2-Full/Small、UCF101、HMDB51等标准基准上，在1/3/5-shot设置下取得有竞争力的性能；消融与可视化分析表明各组件均有显著贡献。

Conclusion: 通过运动特征对齐、语义提示调制与原型-锚点对齐，CLIP-SPM有效缓解小样本动作识别中的时间建模、相似类别区分与模态差距三大难题，整体方法通用且实证有效，代码已开源。

Abstract: Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.

</details>


### [111] [WaTeRFlow: Watermark Temporal Robustness via Flow Consistency](https://arxiv.org/abs/2512.19048)
*Utae Jeong,Sumin In,Hyunju Ryu,Jaewan Choi,Feng Yang,Jongheon Jeong,Seungryong Kim,Sangpil Kim*

Main category: cs.CV

TL;DR: WaTeRFlow提出一种针对图像转视频（I2V）场景的鲁棒深度水印框架，通过训练时引入真实编辑/视频代理失真、光流引导与时间一致性约束、以及语义保持损失，实现跨模态视频生成后的逐帧高准确度水印恢复，并在多种失真前后均表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有水印在面对强生成式编辑与跨模态转换（尤其图像转视频）时易失效；尽管深度水印对扩散编辑更鲁棒，但在I2V中逐帧检测变弱。I2V已快速进步并用于内容创作与世界建模，因此需要能在视频中可靠恢复图像水印的方案。

Method: 提出WaTeRFlow，包括：1）FUSE：在训练中用指令驱动编辑与快速视频扩散代理，向编码器-解码器暴露更贴近真实的失真分布；2）基于光流的帧间扭曲与时间一致性损失（TCL），稳定逐帧预测；3）语义保持损失，约束生成结果保留条件信号语义。

Result: 在多个代表性I2V模型上，WaTeRFlow能从视频帧中准确恢复水印，相比基线提升首帧与逐帧比特准确率；在视频生成前后施加多种失真时仍保持更强鲁棒性。

Conclusion: 通过训练时模拟I2V失真、引入光流与时间一致性以及语义保持，WaTeRFlow有效弥补图像水印在I2V中的鲁棒性缺口，实现跨模态视频场景下稳定的水印恢复。

Abstract: Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.

</details>


### [112] [Decoupled Generative Modeling for Human-Object Interaction Synthesis](https://arxiv.org/abs/2512.19049)
*Hwanhee Jung,Seunggwan Lee,Jeongyoon Yoon,SeungHyeon Kim,Giljoo Nam,Qixing Huang,Sangpil Kim*

Main category: cs.CV

TL;DR: 提出DecHOI：将HOI合成解耦为轨迹规划与动作生成两阶段，并用关注远端关节动力学的判别器提升接触真实感；在FullBodyManipulation与3D-FUTURE上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有HOI合成需要手工设定中间路标、将多目标压在单一网络上，导致复杂且不灵活，易出现人–物不同步、穿模和接触不真实等问题。

Method: 采用解耦式生成框架：1) 轨迹生成器在无预设路标条件下同时生成人和物体的全局轨迹/路径；2) 动作生成器以轨迹为条件合成细节动作与接触；3) 引入仅关注远端关节（如手/足等）动力学的判别器进行对抗训练，强化接触与交互的物理/感知真实感；4) 支持动态场景中的移动对手（moving counterpart）与长时序规划，并保证计划一致性。

Result: 在FullBodyManipulation与3D-FUTURE两套基准上，大多数定量指标和主观质评均超越SOTA；感知实验也更偏好其结果。

Conclusion: 解耦路径与动作的生成，使HOI合成更稳健灵活，减少不同步与穿模，并提升接触逼真度；框架可处理动态、长序列场景并保持一致性，具有广泛应用潜力。

Abstract: Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.

</details>


### [113] [6DAttack: Backdoor Attacks in the 6DoF Pose Estimation](https://arxiv.org/abs/2512.19058)
*Jihui Guo,Zongmin Zhang,Zhen Sun,Yuhao Yang,Jinlin Wu,Fu Zhang,Xinlei He*

Main category: cs.CV

TL;DR: 提出6DAttack：在6DoF姿态估计上利用3D触发器实现可控错误姿态的后门攻击，干净性能不降、触发时ASR极高；现有防御失效，揭示重大安全威胁。


<details>
  <summary>Details</summary>
Motivation: 现有后门研究多聚焦2D分类/检测分割，难以处理6DoF中连续的平移与旋转控制；6DoF姿态估计广泛应用且安全风险被忽视，亟需验证其可被精确操控与攻击。

Method: 设计基于3D物体触发器的后门框架6DAttack：在训练中注入带触发器样本并为其指定目标连续姿态（平移/旋转），学习从触发模式到指定姿态的映射；不触发时维持正常预测。跨多模型（PVNet、DenseFusion、PoseDiffusion）与数据集（LINEMOD、YCB-Video、CO3D）评估。

Result: 在不影响干净样本（最高100% ADD）的情况下，触发样本实现极高攻击成功率（ASR达100%，ADD-P达97.70%）；代表性防御方法对该攻击无效。

Conclusion: 6DoF姿态估计易受可控后门攻击，攻击可精确操控连续姿态参数且难以被现有防御检测/阻断，提示该领域存在严重但被低估的安全隐患，需开发针对连续输出任务的新型防御。

Abstract: Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.

</details>


### [114] [Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding](https://arxiv.org/abs/2512.19070)
*Ruiqi Ma,Yu Yan,Chunhong Zhang,Minghao Yin,XinChao Liu,Zhihong Jin,Zheng Hu*

Main category: cs.CV

TL;DR: 提出一种无需训练的解码策略HDD，通过图像分割与空白图像对比，减少LVLM在目标识别中的幻觉。


<details>
  <summary>Details</summary>
Motivation: LVLM在目标识别场景中常出现“看错/编造”对象，语言流畅但与图像不符，现实应用风险高。现有方法多只抑制语言侧幻觉，忽视视觉侧与语言先验的耦合影响。

Method: Hallucination Disentangled Decoding (HDD)：1) 对输入图像进行分割，生成若干区域图；2) 选择能增强原图识别的子图作为补充；3) 同时输入一张空白图，与原图及分割图一起推理，用空白图估计语言先验产生的偏置；4) 在解码阶段利用这些对比信号，抑制由语言先验驱动的输出并强化与视觉证据一致的词。全流程无需额外训练。

Result: 在对象识别相关任务上显著减少语言与视觉双模态的幻觉，提升识别准确性与与图像一致性（摘要未给出具体数值，但强调稳定改进）。

Conclusion: 通过分割增强视觉证据、以空白图消除语言先验偏置，HDD在无需训练的前提下有效缓解LVLM幻觉，提升安全性与可靠性；代码已开源。

Abstract: Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)

</details>


### [115] [Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2512.19088)
*Khanh Nguyen,Dasith de Silva Edirimuni,Ghulam Mubashar Hassan,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出一种在场景级点云中高效检索新颖类别物体的方法：用2D开放词汇检测器引导从RGB生成3D实例掩码，兼具开放词汇泛化与实时性。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D实例分割多依赖SAM/CLIP从图像生成与分类3D掩码，计算开销大、推理慢；Open-YOLO 3D虽快，但对训练中罕见类别泛化差。需要一种既能识别新类别、又高效的3D实例检索方法。

Method: 利用2D开放词汇检测器在RGB上定位并识别目标，同时从点云侧采用预训练3D分割器或图像引导生成3D实例掩码；通过2D检测结果引导将RGB信息与点云对齐，生成/筛选对应的3D实例掩码，从而以2D开放词汇能力完成3D中的实例发现与分类，避免SAM/CLIP并保持实时分类。

Result: 在无需SAM与CLIP的前提下，显著降低推理时间；相较Open-YOLO 3D，对训练数据中稀有类别与新类别的检索与分割更稳健、准确，能从开放文本查询中快速找回目标实例。

Conclusion: 该方法将2D开放词汇识别能力迁移至3D实例掩码生成与分类，兼顾速度与开放集泛化，特别提升对罕见/新颖类别的3D检索表现；代码将开源（BoxOVIS）。

Abstract: Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.

</details>


### [116] [Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges](https://arxiv.org/abs/2512.19091)
*Ariel Lubonja,Pedro R. A. S. Bassi,Wenxuan Li,Hualin Qiao,Randal Burns,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: RankInsight 提供一个用于医学AI竞赛排行榜的开源分析工具包，补齐显著性检验、器官合适指标与交叉人群公平性审计三大缺口，并展示这些改动如何实质改变排名与结论。


<details>
  <summary>Details</summary>
Motivation: 当前医学AI挑战赛排行榜存在三大痛点：名次差异缺乏统计显著性验证、对所有器官使用单一平均指标掩盖临床关键边界错误、很少报告交叉人群（性别×种族等）表现导致公平性问题被遮蔽。需要一个系统化方法让排名更稳健、临床更相关并兼顾公平。

Method: 提出并开源RankInsight工具包，包含：（1）成对方法的统计显著性检验并可视化显著性图；（2）依据器官形态与临床需求重算合适指标（如对管状结构用NSD替代Dice）；（3）交叉人群公平性审计，报告性别×种族等子群差异。用其分析既有提交（nnU-Net、Vision-Language、MONAI等）与自有JHH数据。

Result: （1）显著性图表明nnU-Net家族相对Vision-Language与MONAI提交具高统计把握的优越性；（2）按器官合适指标重排后，使用NSD评估管状结构时，原前四名顺序被颠倒；（3）交叉公平性审计显示超过一半的MONAI系条目在JHH数据上存在最大性别-种族性能差距。

Conclusion: RankInsight可直接应用于既往、正在进行和未来的挑战赛，使排行榜具备统计稳健性、临床意义与人口学公平性；建议组织者与参赛者采用以发布更可靠和公平的排名。

Abstract: Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.

</details>


### [117] [Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction](https://arxiv.org/abs/2512.19095)
*Weiyi Lyu,Xinming Fang,Jun Wang,Jun Shi,Guixu Zhang,Juncheng Li*

Main category: cs.CV

TL;DR: 提出MambaMDN：一种利用参考K空间并进行模态解耦的双域多对比MRI重建框架，结合迭代精炼，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 加速MRI受限于采样不足导致的走样伪影，以及多对比融合时无关信息污染目标重建质量。现有方法未充分利用K空间先验或无法有效区分模态特异信息。

Method: 1) 利用完全采样的参考K空间来补全目标的欠采样K空间，得到结构对齐但模态混合的输入；2) 设计基于Mamba（状态空间/序列建模）的模态解耦网络，提取并去除参考特异特征；3) 采用迭代精炼机制，多轮特征纯化与重建以逐步提升精度；4) 双域（K空间与图像域）联合优化。

Result: 在广泛实验中，MambaMDN在重建质量上显著优于现有多对比MRI重建方法，减少伪影并提升细节与保真度。

Conclusion: 通过将参考K空间先验用于补全并结合Mamba驱动的模态解耦与迭代精炼，MambaMDN有效缓解伪影与信息污染问题，实现更优的多对比MRI重建。

Abstract: Magnetic resonance imaging (MRI) is a cornerstone of modern clinical diagnosis, offering unparalleled soft-tissue contrast without ionizing radiation. However, prolonged scan times remain a major barrier to patient throughput and comfort. Existing accelerated MRI techniques often struggle with two key challenges: (1) failure to effectively utilize inherent K-space prior information, leading to persistent aliasing artifacts from zero-filled inputs; and (2) contamination of target reconstruction quality by irrelevant information when employing multi-contrast fusion strategies. To overcome these challenges, we present MambaMDN, a dual-domain framework for multi-contrast MRI reconstruction. Our approach first employs fully-sampled reference K-space data to complete the undersampled target data, generating structurally aligned but modality-mixed inputs. Subsequently, we develop a Mamba-based modality disentanglement network to extract and remove reference-specific features from the mixed representation. Furthermore, we introduce an iterative refinement mechanism to progressively enhance reconstruction accuracy through repeated feature purification. Extensive experiments demonstrate that MambaMDN can significantly outperform existing multi-contrast reconstruction methods.

</details>


### [118] [GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting](https://arxiv.org/abs/2512.19108)
*Tiantian Li,Xinjie Zhang,Xingtong Ge,Tongda Xu,Dailan He,Jun Zhang,Yan Wang*

Main category: cs.CV

TL;DR: 提出GaussianImage++：用更少高斯基元实现高保真图像表示与压缩，兼具实时解码与低内存，优于GaussianImage与基于INR的COIN。


<details>
  <summary>Details</summary>
Motivation: INR图像表示/压缩效果佳但训练耗时耗存；2D Gaussian Splatting渲染高效但需大量高斯基元才能保真，影响压缩与效率。需要一种既保真又用少量基元、并可高效压缩与实时解码的方法。

Method: 1) 失真驱动的稠密化：根据信号强度/失真逐步分配高斯基元；2) 上下文感知的高斯滤波：为每个基元设计上下文自适应滤波，辅助稠密化以适配不同内容；3) 属性分离的可学习标量量化+量化感知训练：分别量化基元属性并在训练中感知量化误差，实现高效压缩。

Result: 实验显示在表示质量与压缩率上均优于GaussianImage与INR方法COIN，同时保持实时解码与低内存占用。

Conclusion: 通过失真自适应的基元分配、上下文感知滤波与可学习量化，GaussianImage++以较少基元实现高保真与高压缩效率，兼顾速度与资源占用，优于现有GS与INR基线。

Abstract: Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.

</details>


### [119] [Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction](https://arxiv.org/abs/2512.19110)
*Tao Li,Zhenbao Yu,Banglei Guan,Jianli Han,Weimin Lv,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 提出两种利用已知垂直方向的多视图相对位姿估计求解器：一个四点三视图的线性闭式解，和一个三点的最小Gröbner基解；在RANSAC中高效、在KITTI与合成数据上精度优于替代方法。


<details>
  <summary>Details</summary>
Motivation: IMU可轻易提供每个视图的垂直方向，如果利用这一先验，可大幅减少位姿未知数与对应点需求，提高鲁棒性与效率，适用于自动驾驶、手机、无人机等视觉里程计场景。

Method: 将已知垂直方向纳入相机姿态参数化，仅需求解两个旋转角和两段平移；提出(1) 线性闭式三视图四点求解器；(2) 使用最新Gröbner基技术的三点最小求解器；在RANSAC框架中使用以剔除外点并估计位姿。

Result: 在合成数据与KITTI实测上，所提方法在姿态估计精度上优于其他替代方法，同时因所需对应点更少，在RANSAC中表现出更高效率与鲁棒性。

Conclusion: 利用已知垂直方向可将多视图位姿估计简化并提升性能；所提线性与最小求解器在精度与效率上具优势，适用于视觉里程计与实际应用。

Abstract: This work presents two novel solvers for estimating the relative poses among views with known vertical directions. The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs). Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors. In this paper, a linear closed-form solution has been described, requiring only four point correspondences in three views. We also propose a minimal solution with three point correspondences using the latest Gröbner basis solver. Since the proposed methods require fewer point correspondences, they can be efficiently applied within the RANSAC framework for outliers removal and pose estimation in visual odometry. The proposed method has been tested on both synthetic data and real-world scenes from KITTI. The experimental results show that the accuracy of the estimated poses is superior to other alternative methods.

</details>


### [120] [Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?](https://arxiv.org/abs/2512.19115)
*Hengyi Feng,Zeang Sheng,Meiyi Qiang,Wentao Zhang*

Main category: cs.CV

TL;DR: 论文发现：当前多模态大模型在零样本多模态检索上表现欠佳，根源在于其表征空间被文本语义主导、视觉成分稀薄且相互同质化，导致相似度计算被干扰特征主导、判别性不足。作者用稀疏自动编码器解析表征并指明改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在生成任务上成功，但在无需微调的检索任务中表现反常偏弱。作者动机是解释“为何作为生成器很强的MLLM，却不擅长做检索器”，并给出可操作的改进思路。

Method: 利用稀疏自动编码器（SAE）对MLLM的输出表征进行可解释分解，将高维嵌入拆解为稀疏、可解释的语义概念成分；分析各成分在相似度计算中的贡献，区分文本语义与视觉语义比例，并检查跨模态对齐导致的嵌入同质化现象与“干扰成分”。

Result: 1) 表征空间被文本语义主导，视觉信息仅占小部分；2) 为促进图文生成而强化的对齐使嵌入同质化，削弱了检索所需的判别性；3) 在相似度中贡献最大的部分竟多为“干扰特征”，主动拉低检索表现。

Conclusion: MLLM的检索劣势源于表征失衡与同质化，以及被错误特征主导的相似度机制。工作首次从可解释角度系统剖析这一问题，并提出应提升视觉成分权重、抑制干扰特征、在训练中兼顾判别性以增强多模态检索能力。

Abstract: Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.

</details>


### [121] [AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction](https://arxiv.org/abs/2512.19150)
*Ruikai Li,Xinrun Li,Mengwei Xie,Hao Shan,Shoumeng Qiu,Xinyuan Chang,Yizhe Fan,Feng Xiong,Han Jiang,Yilong Ren,Haiyang Yu,Mu Xu,Yang Long,Varun Ojha,Zhiyong Cui*

Main category: cs.CV

TL;DR: 提出AMap：面向前方（ahead-aware）的在线HD制图，通过“从未来蒸馏”让仅用当前帧的学生模型获得前视能力，在关键前向区域优于时序方法且保持单帧效率。


<details>
  <summary>Details</summary>
Motivation: 现有在线HD地图方法主要通过历史时序融合提升效果，但增强集中在已驶过区域，对未见的前方区域帮助有限；而驾驶安全对前向感知极其敏感，后向误差可容忍、前向误差会诱发危险决策，存在安全不对称。

Method: 提出AMap框架：1) 教师模型可访问未来时序上下文，学生仅用当前帧；通过“从未来蒸馏”将未来信息压缩进学生。2) 多层级BEV蒸馏：在不同特征层进行蒸馏，并配合空间掩码，强调前向/关键区域监督。3) 非对称查询自适应（Asymmetric Query Adaptation）：将教师的未来感知表示有效迁移到学生的静态查询上，赋予前视能力。整体实现零额外推理开销的前视感知。

Result: 在nuScenes与Argoverse 2上，AMap显著提升当前帧感知质量；在关键前向区域超越SOTA时序模型，同时保持单帧推理效率。

Conclusion: 通过“从未来蒸馏”的ahead-aware设计，AMap弥补时序融合的空间后视偏置，实现对前向区域更可靠的在线HD制图，兼顾安全性与效率。

Abstract: Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking." These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future" paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead" capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student's static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.

</details>


### [122] [OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions](https://arxiv.org/abs/2512.19159)
*Wendong Bu,Kaihang Pan,Yuze Lin,Jiacheng Li,Kai Shen,Wenqiao Zhang,Juncheng Li,Jun Xiao,Siliang Tang*

Main category: cs.CV

TL;DR: 提出OmniMoGen：将人类动作生成统一到“文本-动作交错指令”框架中，配合RVQ-VAE+Transformer，实现端到端多任务（文本生成、编辑、任意上下文）并在新数据集X2Mo与基准AnyContext上达SOTA，展现组合编辑、自反生成、知识增强等新能力。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成方法各自为政，难以像LLM一样在统一范式中处理多样任务，限制了自由形式与多目标的生成与编辑需求。作者希望构建类似LLM的统一接口与能力边界。

Method: 1) 设计OmniMoGen：以简洁的RVQ-VAE对动作进行离散化表示，结合Transformer以“文本-动作交错指令”作输入，实现端到端指令驱动生成；2) 构建大规模X2Mo数据集（>137K条交错文本-动作指令）；3) 提出AnyContext基准，用于评测交错式动作生成与编辑的多场景能力。

Result: 在文本到动作、动作编辑与AnyContext等任务上取得SOTA；呈现新兴能力：可组合的多目标编辑、自反式生成（自我反馈/反思迭代）、以及融入外部知识的生成。

Conclusion: 将动作生成统一到指令范式显著提升了通用性与性能，显示出向“下一代智能动作生成”的迈进，数据与评测基准为社区提供了可复现与可比较的土壤。

Abstract: Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.

</details>


### [123] [PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements](https://arxiv.org/abs/2512.19190)
*Marios Thoma,Zenonas Theodosiou,Harris Partaourides,Vassilis Vassiliades,Loizos Michael,Andreas Lanitis*

Main category: cs.CV

TL;DR: 提出PEDESTRIAN自我视角人行道障碍数据集（29类，340段手机采集视频），并用其对多种SOTA深度模型进行训练与评测，作为行人障碍检测/识别基准以提升城市行走安全。


<details>
  <summary>Details</summary>
Motivation: 城市人行道常被各类障碍物占据，影响通行与安全；要构建实时自动识别系统，亟需全面且均衡的自我视角（egocentric）数据集以支撑算法开发与评测。

Method: 收集从行人视角拍摄的手机视频，覆盖29种常见人行道障碍，共340段；整理标注为用于检测/识别的基准数据集，并用多种当前主流深度学习算法进行训练与实验对比。

Result: 基于该数据集，多个SOTA模型完成了障碍检测/识别实验，给出基准结果，表明数据集可有效支持模型训练与评测。

Conclusion: PEDESTRIAN数据集为行人视角的人行道障碍提供了系统化基准，可用于训练与评测障碍检测器，促进提高城市行走安全。

Abstract: Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.

</details>


### [124] [InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training](https://arxiv.org/abs/2512.19213)
*Zihao Luo,Shaohao Rui,Zhenyu Tang,Guotai Wang,Xiaosong Wang*

Main category: cs.CV

TL;DR: 提出InvCoSS：在不访问旧数据的前提下，通过模型反演生成合成影像进行联合自监督训练，缓解灾难性遗忘，并在多下游任务上媲美/超越含重放方法。


<details>
  <summary>Details</summary>
Motivation: 医疗多模态影像的持续自监督预训练有助于隐私保护与跨站点训练，但现有方法常依赖旧数据重放，既违背隐私限制又带来存储与传输负担，亟需无重放且有效防遗忘的方案。

Method: 在每个阶段完成自监督预训练后，对已训模型进行反演，生成逼近原训练分布的合成影像；将这些合成影像与新任务数据一起进行联合自监督优化。为提升反演质量，提出多尺度融合的InvUNet以恢复高/低频细节；为提升多样性与防模式坍塌，引入无类别引导的“排斥式表征学习”机制，鼓励合成影像在特征空间分散。

Result: 在九个下游任务上，InvCoSS在不访问任何历史真实数据的前提下，取得与甚至优于数据重放方法的性能，同时显著降低存储成本并消除数据隐私传输限制。

Conclusion: 模型反演驱动的持续自监督框架可在严格隐私约束下有效缓解遗忘；配合InvUNet与排斥式表征，能生成高保真且多样的合成数据，实现对重放的可行替代。

Abstract: Continual self-supervised learning (CSSL) in medical imaging trains a foundation model sequentially, alleviating the need for collecting multi-modal images for joint training and offering promising improvements in downstream performance while preserving data privacy. However, most existing methods still rely on replaying data from previous stages to prevent catastrophic forgetting, which compromises privacy and limits their applicability in real-world scenarios where data transfer across sites is often restricted. In this work, we propose InvCoSS, an inversion-driven continual self-supervised learning framework for medical multi-modal image pre-training. Specifically, after training on a previous task, InvCoSS inverts the pre-trained self-supervised model to generate synthetic images that approximate the original training distribution. These synthetic images are then combined with data from the new task for joint optimization, which effectively mitigates catastrophic forgetting while strictly adhering to the constraint of no access to previous real data. Furthermore, to improve the fidelity of synthetic images, we introduce a novel InvUNet with a multi-scale fusion architecture to restore both high- and low-frequency components of the inverted images. To enhance diversity and prevent mode collapse, we design a repulsive representation-learning mechanism that encourages a diverse feature space for synthetic images without class guidance. Extensive experiments across nine downstream tasks validate the effectiveness of InvCoSS, achieving performance comparable to or even superior to prior data-replay methods while significantly reducing storage requirements and eliminating data privacy constraints.

</details>


### [125] [HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry](https://arxiv.org/abs/2512.19214)
*Na Gao,Chenfei Ye,Yanwu Yang,Anqi Li,Zhengbo He,Li Liang,Zhiyuan Liu,Xingyu Hao,Ting Ma,Tengfei Guo*

Main category: cs.CV

TL;DR: 提出HippMetric：基于s-rep的海马亚结构形态计量与点对应框架，利用与解剖/功能对齐的可变形骨架坐标系，实现跨个体与纵向稳定对应，在两国际队列中优于现有形状模型。


<details>
  <summary>Details</summary>
Motivation: 海马亚结构对早期神经退行性变化敏感，但由于个体差异大和复杂折叠导致跨主体/纵向对应不稳定；现有方法多为个体化建模，缺乏稳定内在坐标系，限制了可靠对应与精细形态学分析。

Method: 构建HippMetric框架：1) 基于ARMM的可变形骨架坐标系，与海马沿长轴的层片（lamellae）组织对齐，使功能-解剖一致定位；2) 个体化s-rep生成流程，包括表面重建、形变以及在几何约束下的spoke精炼（边界贴合、正交性与无自交），以生成数学有效的骨架几何；实现点到点对应与亚结构形态计量。

Result: 在两个国际队列上进行广泛实验，HippMetric在准确性、可靠性和对应稳定性方面均优于现有形状模型。

Conclusion: 与解剖/功能一致的骨架坐标系结合严格几何约束的s-rep可为海马亚结构提供稳定的内在参考，实现跨个体与纵向的鲁棒对应与形态计量，具有作为早期生物标志物检测工具的潜力。

Abstract: Accurate characterization of hippocampal substructure is crucial for detecting subtle structural changes and identifying early neurodegenerative biomarkers. However, high inter-subject variability and complex folding pattern of human hippocampus hinder consistent cross-subject and longitudinal analysis. Most existing approaches rely on subject-specific modelling and lack a stable intrinsic coordinate system to accommodate anatomical variability, which limits their ability to establish reliable inter- and intra-individual correspondence. To address this, we propose HippMetric, a skeletal representation (s-rep)-based framework for hippocampal substructural morphometry and point-wise correspondence across individuals and scans. HippMetric builds on the Axis-Referenced Morphometric Model (ARMM) and employs a deformable skeletal coordinate system aligned with hippocampal anatomy and function, providing a biologically grounded reference for correspondence. Our framework comprises two core modules: a skeletal-based coordinate system that respects the hippocampus' conserved longitudinal lamellar architecture, in which functional units (lamellae) are stacked perpendicular to the long-axis, enabling anatomically consistent localization across subjects and time; and individualized s-reps generated through surface reconstruction, deformation, and geometrically constrained spoke refinement, enforcing boundary adherence, orthogonality and non-intersection to produce mathematically valid skeletal geometry. Extensive experiments on two international cohorts demonstrate that HippMetric achieves higher accuracy, reliability, and correspondence stability compared to existing shape models.

</details>


### [126] [Towards Minimal Fine-Tuning of VLMs](https://arxiv.org/abs/2512.19219)
*Tiange Luo,Lajanugen Logeswaran,Jaekyeom Kim,Justin Johnson,Honglak Lee*

Main category: cs.CV

TL;DR: 提出Image-LoRA：在视觉语言模型中仅对视觉token范围内注意力层的value通路施加LoRA，并仅适配一部分高影响注意力头，从而以更少可训练参数和更低FLOPs接近或匹配标准LoRA的表现，同时不损伤纯文本推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM微调（如标准LoRA）在全层或全头上适配，训练FLOPs和参数开销与序列长度（含视觉+文本）成正比；此外，全局适配可能影响模型原有的文本推理能力。需要一种更高效、对纯文本能力“无伤”的视觉侧微调方法，尤其适用于屏幕理解、指代与定位等任务。

Method: 1) 仅在视觉token跨度内，对注意力层的value路径施加低秩适配（LoRA），使训练开销约按视觉token占比缩放；2) 只微调部分注意力头：用rank-1 Image-LoRA估计头影响分数，选出高影响头；3) 为避免选头规模差异造成梯度不稳，对每层更新做selection-size normalization；整体形成轻量PEFT配方。

Result: 在多种“屏幕中心”的grounding与referring基准（从文本重到图像重）上，Image-LoRA以更少的可训练参数与更低的适配FLOPs，匹配或接近标准LoRA精度；并在GSM8K上展示微调前后纯文本推理性能保持不变。

Conclusion: 针对VLM，聚焦视觉token与高影响注意力头的选择性LoRA能显著降低训练开销且维持或接近SOTA性能，同时保持纯文本能力不受损，是一种实用的高效微调方案。

Abstract: We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.

</details>


### [127] [From Pixels to Predicates Structuring urban perception with scene graphs](https://arxiv.org/abs/2512.19221)
*Yunlong Liu,Shuyang Li,Pengyuan Liu,Yu Zhang,Rudi Stouffs*

Main category: cs.CV

TL;DR: 论文提出将街景图像转换为结构化的场景图表示，并用异质图自编码器学习嵌入，再用神经网络预测六类感知指标，相比图像基线平均提升26%，且跨城市泛化良好。


<details>
  <summary>Details</summary>
Motivation: 现有以街景为载体的感知研究多依赖像素特征或对象共现统计，忽略显式的对象—关系结构，而这类关系（如“涂鸦-在-墙上”、“汽车-停在-人行道上”）更贴近人类感知的生成机制与可解释性。

Method: 三阶段流程：1）用开放集全景场景图模型OpenPSG从SVI中抽取“主体-谓词-客体”三元组；2）将图构造成异质图，使用GraphMAE学习紧凑的场景级嵌入；3）以神经网络从嵌入回归六个感知分数，并与仅图像的基线进行比较与跨城迁移评估。

Result: 相对图像基线，平均准确率提升26%；在跨城市预测任务中保持较强泛化性能；结构化表示能揭示导致低感知分数的关键关系模式（如“graffiti on wall”、“car parked on sidewalk”）。

Conclusion: 基于图的结构化表示为城市感知建模提供了更具表达力、可泛化且可解释的信号，推进了以人为中心、具情境感知的城市分析。

Abstract: Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.

</details>


### [128] [VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis](https://arxiv.org/abs/2512.19243)
*Meng Chu,Senqiao Yang,Haoxuan Che,Suiyun Zhang,Xichen Zhang,Shaozuo Yu,Haokun Gui,Zhefan Rao,Dandan Tu,Rui Liu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出LGBench长目标基准与VisionDirector监督器，揭示现有生成模型在多目标设计指令上脆弱，并通过训练free规划与微网格逐步编辑+GRPO微调显著提升T2I/I2I对齐与编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽能产出逼真图像，但在真实设计场景的长、多目标复杂指令（布局、局部摆放、字体、Logo等）上表现欠佳，缺乏能细粒度衡量与推动改进的基准与方法。

Method: 1) 构建LGBench：包含2000个任务（各1000个T2I/I2I），单条指令含18–22个紧耦合目标；2) VisionDirector（免训练）作为视觉-语言监督器：a) 从长指令中抽取结构化目标；b) 在一次生成与分阶段编辑间自适应决策；c) 采用微网格采样、语义校验与逐步回滚；d) 记录目标级奖励；3) 进一步用GRPO微调规划器，缩短编辑轨迹并提升对齐。

Result: 评测显示SOTA模型在LGBench上目标满足率<72%，常漏掉局部编辑；VisionDirector在GenEval上整体+7%，在ImgEdit上绝对提升+0.07，并在字体排印、多物体场景与姿态编辑上有一致的质变提升；编辑步数从4.2降至3.1。

Conclusion: 长、多目标指令暴露了当前生成流水线的脆弱性。通过结构化目标解析、动态规划与可回滚的逐步编辑，再配合GRPO微调，可显著提升复杂设计任务的对齐与可靠性，并树立LGBench作为更贴近实用场景的评测标准。

Abstract: Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models' performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.

</details>


### [129] [3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory](https://arxiv.org/abs/2512.19271)
*Xinyang Song,Libin Wang,Weining Wang,Zhiwei Li,Jianxin Sun,Dandan Zheng,Jingdong Chen,Qi Li,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出3SGen统一框架，单一模型同时支持主体、风格、结构三类条件生成，通过MLLM+VAE与自适应任务记忆ATM实现解耦与组合控制，并发布统一基准3SGen-Bench；在多数据集上达最优。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法通常分别处理主体/风格/结构条件，导致特征纠缠、任务间干扰与迁移性差，难以在组合输入下稳定控制与泛化。需要一个能统一三类条件、减少干扰并可扩展到组合控制的模型与评价基准。

Method: 提出3SGen：1) 采用带可学习语义查询的多模态大模型（MLLM）对齐文图语义；2) 并行的VAE分支保留细粒度视觉细节；3) 核心ATM（Adaptive Task-specific Memory）模块以轻量门控+可扩展记忆项，按任务动态解耦、存储与检索条件先验（主体身份、风格纹理、结构布局），缓解跨任务干扰并自然支持条件组合；4) 构建统一基准3SGen-Bench，提供标准化度量评估跨任务保真度与可控性。

Result: 在3SGen-Bench及多种公开基准上取得优于现有方法的性能，显示在多种图像驱动生成任务上更高的保真度与可控性，且对组合输入具有更强的鲁棒性与可扩展性。

Conclusion: 3SGen在单一模型内统一三类条件生成，通过ATM实现条件先验的动态解耦与检索，缓解任务干扰并提升可控性；配套的3SGen-Bench为跨任务评测提供统一标准，实验验证其在多任务与组合控制上的领先表现。

Abstract: Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.

</details>


### [130] [Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation](https://arxiv.org/abs/2512.19275)
*Ivan DeAndres-Tame,Chengwei Ye,Ruben Tolosana,Ruben Vera-Rodriguez,Shiqi Yu*

Main category: cs.CV

TL;DR: 研究评估生成式AI人类动画模型在步态生物识别场景中的真实性：视觉质量高，但难以保留用于身份识别的细微时空步态特征，导致生物特征保真度低。


<details>
  <summary>Details</summary>
Motivation: AI生成动画在视觉上已逼真，但小瑕疵即可显得不自然；在行为生物识别（如步态识别）中，极细微的时空运动线索至关重要。需要检验当前GenAI是否能在复杂条件下保留和迁移这些与身份相关的步态细节，并探查识别模型究竟依赖外观还是时序运动。

Method: 选取四种最先进的GenAI人类动画模型，设计两类任务：1) 在不同复杂度条件下从参考视频“恢复”步态模式；2) 将该步态模式“迁移”到不同视觉身份（外观）上。通过识别/验证指标比较视觉质量与生物识别保真度，并分析身份与运动的可分离性。

Result: 总体视觉质量较高；但在以身份识别为核心的任务上，步态生物特征保真度偏低，显示模型难以将个体身份与动作解耦。在身份迁移实验中，一旦将纹理/外观与运动分离，基于外观的步态识别性能崩溃，揭示现有方法更多依赖外观属性而非时间动态。

Conclusion: 当前GenAI人类动画难以保留用于步态识别的关键微小时空线索，尚不能可靠支持基于步态的身份识别与跨身份迁移。研究还揭示了外观驱动的步态识别脆弱性，提示未来需要更强的身份-运动解耦与时间动态建模，以及评估指标从视觉逼真转向生物特征保真。

Abstract: Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.

</details>


### [131] [Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context](https://arxiv.org/abs/2512.19283)
*Kyungwon Cho,Hanbyul Joo*

Main category: cs.CV

TL;DR: HaMoS提出一种手部感知的序列级扩散模型，从自我视角视频结合头部轨迹与间歇可见的手部线索来重建全身3D动作，达成SOTA精度与平滑度。


<details>
  <summary>Details</summary>
Motivation: 自我视角（第一人称）设备流行但从该视角估计佩戴者全身运动极难，因大多数身体部位不可见。现有方法依赖头轨迹存在歧义，或假设手部持续可见不现实；缺少匹配多视角与人体动作的数据集亦限制了进展。

Method: 提出HaMoS：手感知、序列级扩散框架，直接以头部轨迹与因视野限制/遮挡而间歇可见的手部线索为条件；引入数据增强策略以模拟真实设备中的视野变化与手部可见性模式；利用局部注意力在长序列中高效建模，同时在序列级别融入体型与视野等上下文，以提升重建精度。

Result: 在公开基准上取得最先进的精度与时间平滑性，优于既有方法，表明其在真实场景中具备更可靠的自我视角3D动作理解能力。

Conclusion: 结合头轨迹与间歇手部线索的序列级扩散建模有效解决自我视角下全身动作重建的歧义与数据稀疏问题；序列上下文（体型、视野）与局部注意力对长序列重建至关重要，方法在实用性与稳定性上取得显著进展。

Abstract: Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.

</details>


### [132] [RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning](https://arxiv.org/abs/2512.19300)
*Jun Li,Zikun Chen,Haibo Chen,Shuo Chen,Jian Yang*

Main category: cs.CV

TL;DR: 提出RMLer：用强化学习优化跨类别文本概念混合以生成连贯高保真新奇对象。


<details>
  <summary>Details</summary>
Motivation: 当前T2I在跨类别概念融合上存在：混合不足、评估不严、输出质量差（概念失衡、表层拼接、简单并置）。需要一种能系统学习“如何混合”并有可量化反馈的机制。

Method: 将概念融合建模为RL：状态=混合后的文本特征，动作=混合策略（跨类别文本嵌入的动态系数），策略网络为MLP；奖励由两部分组成：(1) 语义相似度（融合结果与目标语义的一致性），(2) 组合平衡（融合体与各组成概念的均衡度）。使用PPO优化策略；推理阶段基于奖励的选择策略筛选最高质的融合对象。

Result: 在多个实验中，RMLer生成的对象更连贯、更高保真，跨类别融合优于现有方法。

Conclusion: RMLer提供了一个稳健的跨类别概念融合框架，通过强化学习学习混合系数并以视觉奖励引导，显著提升新颖视觉概念合成，适用于影视、游戏与设计等应用。

Abstract: Novel object synthesis by integrating distinct textual concepts from diverse categories remains a significant challenge in Text-to-Image (T2I) generation. Existing methods often suffer from insufficient concept mixing, lack of rigorous evaluation, and suboptimal outputs-manifesting as conceptual imbalance, superficial combinations, or mere juxtapositions. To address these limitations, we propose Reinforcement Mixing Learning (RMLer), a framework that formulates cross-category concept fusion as a reinforcement learning problem: mixed features serve as states, mixing strategies as actions, and visual outcomes as rewards. Specifically, we design an MLP-policy network to predict dynamic coefficients for blending cross-category text embeddings. We further introduce visual rewards based on (1) semantic similarity and (2) compositional balance between the fused object and its constituent concepts, optimizing the policy via proximal policy optimization. At inference, a selection strategy leverages these rewards to curate the highest-quality fused objects. Extensive experiments demonstrate RMLer's superiority in synthesizing coherent, high-fidelity objects from diverse categories, outperforming existing methods. Our work provides a robust framework for generating novel visual concepts, with promising applications in film, gaming, and design.

</details>


### [133] [Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing](https://arxiv.org/abs/2512.19302)
*Xu Zhang,Junyao Ge,Yang Zheng,Kaitai Guo,Jimin Liang*

Main category: cs.CV

TL;DR: Think2Seg-RS 通过将语言推理与像素分割解耦：用可控提示让冻结的 SAM 执行几何动作，并用仅基于掩膜的强化学习训练 LVLM 提示策略，实现对遥感图像的语义级推理分割，达到 SOTA 且具零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割将语言推理与像素预测端到端耦合，导致几何对齐弱、跨任务泛化差；遥感场景异质且复杂，更需要语义到空间动作的稳健映射。

Method: 冻结 SAM，训练一个 LVLM 作为 prompter，输出结构化几何提示控制 SAM；以“仅掩膜”的强化学习目标优化提示策略，使语言语义推理转化为空间上可执行的分割动作；在 EarthReason 上训练与评估，并测试到多种指代分割基准的零样本迁移。

Result: 在 EarthReason 数据集上达成 SOTA；提示策略可零样本泛化到多种指代分割任务，揭示语义级与实例级落地（grounding）的分界；在语义级监督下，小型分割器优于大型分割器；在异质航空背景中，负面提示效果不佳。

Conclusion: 语义级推理分割是可行且高效的遥感理解新范式：通过 LVLM 的结构化几何提示控制冻结分割器，可获得更强几何落地与任务泛化，具可解释性与统一潜力；代码与模型已开源。

Abstract: Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.

</details>


### [134] [MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture](https://arxiv.org/abs/2512.19311)
*Hui Li,Jiayue Lyu,Fu-Yun Wang,Kaihui Cheng,Siyu Zhu,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出MixFlow解决扩散模型训练-测试不一致（曝光偏差）：用“减速时刻”的插值混合对预测网络进行后训练，显著提升生成质量（ImageNet上256和512分辨率FID≈1.1–1.55）。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练时以真实插值噪声为输入，测试时以模型生成噪声为输入，存在分布偏移导致性能下降。作者观察到“Slow Flow”：测试时生成的噪声更接近于更高噪声（更早/更慢）时刻的真实插值，提示应在训练中对齐这种偏移。

Method: 提出MixFlow：对每个采样时刻t，找与生成噪声最接近的更高噪声“减速时刻”τ>t，构造该时刻的真实插值（或其加权混合，称为“减速插值混合”），用于对预测网络进行后训练（post-training/finetune），以弥合训练-测试输入分布差异。适用于类条件与文生图扩散框架（如SiT、REPA、RAE）。

Result: 在ImageNet上基于RAE的模型，256x256分辨率无指导FID 1.43、带指导1.10；512x512无指导1.55、带指导1.10；同样在类条件与文生图任务上均有一致改进。

Conclusion: 通过利用“减速时刻”的插值混合进行后训练，MixFlow有效缓解了扩散模型的曝光偏差，提升了多任务、多分辨率下的生成质量，并与现有框架兼容易集成。

Abstract: This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.

</details>


### [135] [Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations](https://arxiv.org/abs/2512.19316)
*Marica Muffoletto,Uxio Hermida,Charlène Mauger,Avan Suinesiaputra,Yiyang Xu,Richard Burns,Lisa Pankewitz,Andrew D McCulloch,Steffen E Petersen,Daniel Rueckert,Alistair A Young*

Main category: cs.CV

TL;DR: 提出NIHCs：从稀疏2D心脏分割直接预测标准化隐式坐标，再解码为致密3D分割与高分辨率网格；在大规模数据上实现毫米级重建误差，速度较传统流程显著提升。


<details>
  <summary>Details</summary>
Motivation: 临床中常只有稀疏切片（少量2D分割/轮廓），用其精确重建个体化心脏3D解剖困难；现有神经隐式方法难以在不同个体间保持解剖一致性与标准化参考框架。

Method: 提出神经隐式心脏坐标（NIHCs），以通用心室坐标为基础构建标准化隐式坐标系：模型从少量2D分割直接预测NIHCs，再由解码器生成任意分辨率的致密3D分割与高分辨率网格；以5000个心脏网格训练，能在严重切片稀疏与噪声下保持解剖一致并重建复杂结构（如瓣环/瓣平面）。

Result: 在临床轮廓测试上，平均表面欧氏误差：病患队列（n=4549）2.51±0.33 mm，健康队列（n=5576）2.30±0.36 mm；推理时间由>60 s降至5–15 s；在稀疏与噪声条件下仍能解剖一致地重建，包括复杂瓣膜区域。

Conclusion: NIHCs提供了稳健、高效、标准化的心脏解剖表示，可用最少输入实现患者特异的高精度3D重建，并促进跨个体的解剖一致映射与快速临床应用。

Abstract: Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.

</details>


### [136] [Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome](https://arxiv.org/abs/2512.19327)
*Moamal Fadhil Abdul,Jonas Bruun Hubrechts,Thomas Martini Jørgensen,Emil Hovad*

Main category: cs.CV

TL;DR: 论文扩展OpenTTGames乒乓球视频数据集，新增帧级击球类型、姿态与回合结果标注，为细粒度战术理解与基准提供可复用资源。


<details>
  <summary>Details</summary>
Motivation: 自动检测与分类乒乓球击球可用于训练、转播与分析，但缺乏公开、细粒度且可复用的标注数据。现有资源要么未公开、要么许可受限，难以复用与基准化评测。

Method: 在原OpenTTGames侧拍视频与已有事件（弹地、触网、空事件）、球坐标与语义分割的基础上，增加：1）帧级击球类型（正手、反手及子类）；2）球员姿态标签（身体前倾、站姿）；3）每分结束时的回合结果；并设计紧凑编码方案与代码辅助标注流程，按球员维度建立击球分类体系。

Result: 形成带有细粒度时序与语义标签的扩展数据集，支持从事件发现向战术层面的模型学习（如击球是否提高得分概率）。提供可复现标注流程与基线使用指引。

Conclusion: 该数据集填补公开细粒度乒乓球视频标注空白，以CC BY-NC-SA 4.0发布，便于非商业复用、修改与再发布，促进基准与研究发展。

Abstract: Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either "bounce", "net", or "empty_event" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.

</details>


### [137] [DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis](https://arxiv.org/abs/2512.19331)
*Yueting Zhu,Yuehao Song,Shuai Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DeltaMIL是一种针对WSI的多实例学习框架，通过显式选择语义相关区域并整合判别信息，利用门控delta规则实现高效筛选与记忆更新，结合局部模式混合以保留细粒度病理特征，在生存预测与切片级分类上取得SOTA提升。


<details>
  <summary>Details</summary>
Motivation: WSI尺度大、异质性强，导致补丁级信息冗余且分散，现有MIL难以有效剔除无用信息或充分整合相关特征，限制了在大型异质WSI上的性能。

Method: 提出DeltaMIL：核心为“门控delta规则”记忆模块，结合遗忘与记忆机制，依据与当前补丁的相关性动态删除旧值、写入新值，并用门控机制快速遗忘不相关信号；同时引入局部模式混合机制以保留细粒度局部病理特征；整体实现显式区域选择与判别信息整合。

Result: 在多项WSI任务上达SOTA：生存预测上，使用ResNet-50特征提升3.69%，使用UNI特征提升2.36%；切片级分类上，ResNet-50特征提升3.09%，UNI特征提升3.75%，表现稳定且一致。

Conclusion: DeltaMIL通过门控delta记忆与局部模式混合，有效提取有意义线索、抑制冗余噪声，提升鲁棒性与判别力，且在不同WSI任务与特征背骨上均表现强劲稳定。

Abstract: Whole Slide Images (WSIs) are typically analyzed using multiple instance learning (MIL) methods. However, the scale and heterogeneity of WSIs generate highly redundant and dispersed information, making it difficult to identify and integrate discriminative signals. Existing MIL methods either fail to discard uninformative cues effectively or have limited ability to consolidate relevant features from multiple patches, which restricts their performance on large and heterogeneous WSIs. To address this issue, we propose DeltaMIL, a novel MIL framework that explicitly selects semantically relevant regions and integrates the discriminative information from WSIs. Our method leverages the gated delta rule to efficiently filter and integrate information through a block combining forgetting and memory mechanisms. The delta mechanism dynamically updates the memory by removing old values and inserting new ones according to their correlation with the current patch. The gating mechanism further enables rapid forgetting of irrelevant signals. Additionally, DeltaMIL integrates a complementary local pattern mixing mechanism to retain fine-grained pathological locality. Our design enhances the extraction of meaningful cues and suppresses redundant or noisy information, which improves the model's robustness and discriminative power. Experiments demonstrate that DeltaMIL achieves state-of-the-art performance. Specifically, for survival prediction, DeltaMIL improves performance by 3.69\% using ResNet-50 features and 2.36\% using UNI features. For slide-level classification, it increases accuracy by 3.09\% with ResNet-50 features and 3.75\% with UNI features. These results demonstrate the strong and consistent performance of DeltaMIL across diverse WSI tasks.

</details>


### [138] [GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis](https://arxiv.org/abs/2512.19336)
*Siyuan Mei,Yan Xia,Fuxin Fan*

Main category: cs.CV

TL;DR: 提出GANeXt：基于3D ConvNeXt与PatchGAN的统一sCT合成框架，面向MRI→CT与CBCT→CT，多损失联合训练，滑窗推理，无区域微调即可跨解剖部位生成高质量sCT。


<details>
  <summary>Details</summary>
Motivation: 临床自适应放疗需准确解剖信息，但直接获取CT可能受限；从MRI或CBCT合成CT（sCT）可改进剂量计算与计划。然而现有方法往往针对单一模态或特定部位，泛化与统一性不足，3D上下文利用与训练稳定性也有待提升。

Method: 提出3D patch-based全ConvNeXt生成对抗框架GANeXt：U形生成器由堆叠的3D ConvNeXt块构成（小卷积核），判别器为条件PatchGAN并带多头分割辅助。损失包括MAE、感知损失、分割掩膜MAE、对抗损失，以及分割分支的Dice+交叉熵。训练用两套AdamW（warmup+余弦退火），学习率5e-4（G）与1e-3（D），batch=8。预处理含可变形配准、前景裁剪、输入模态分位数归一化、CT线性归一到[-1024,1000]。增强：MRI→CT随机缩放(0.8,1.3)、定尺裁块（MRI:32×160×192；CBCT:32×128×128）、随机翻转。推理采用0.8重叠滑窗+folding平均重建全尺寸并反归一化。联合多解剖部位训练，无微调。

Result: 在MRI→CT（3000轮）与CBCT→CT（1000轮）全量数据联合训练后，选择最终模型。实验表明能在不同模态与部位生成一致且高质量的sCT（摘要未给出具体数值指标）。

Conclusion: GANeXt通过3D ConvNeXt生成器、条件PatchGAN及多重损失设计，实现跨模态、跨解剖区域的统一CT合成，训练与推理流程高效稳健，无需针对部位微调即可得到高质量sCT，适用于自适应放疗计划。

Abstract: The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\times10^{-4}$ and $1\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\times160\times192$ for MRI-to-CT and $32\times128\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.

</details>


### [139] [ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining](https://arxiv.org/abs/2512.19354)
*Zhenyang Huang,Xiao Yu,Yi Zhang,Decheng Wang,Hang Ruan*

Main category: cs.CV

TL;DR: 提出ReasonCD多模态推理变化检测模型，利用大语言模型挖掘用户隐含任务意图，从而在遥感变化检测中对用户的隐式CRoI文本指令仍保持高性能；在BCDD上F1达92.1%，并在SECOND子集上的推理标注验证了可解释推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有利用多模态/语义引导的遥感变化检测方法过度依赖对CRoI的显式文本描述，面对隐式或含糊指令时性能几乎失效，亟需一种能够理解并推理用户隐含意图的模型。

Method: 构建ReasonCD：将预训练大语言模型的推理能力融入变化检测流程。根据用户文本（显式或隐式）推理出任务意图与约束，进而生成针对性的变化检测结果；同时提供推理链条以解释决策。并在SECOND上构建带推理标注的子集用于验证。

Result: 在公开数据上取得优异表现：BCDD数据集F1=92.1%；在SECOND推理子集上完成基于推理的变化检测任务并展示可解释推理过程，优于依赖显式描述的基线。

Conclusion: ReasonCD能有效挖掘隐式任务意图，缓解对显式文本的依赖，在保持高精度变化检测的同时提供可解释的推理过程，为将大模型推理引入遥感变化检测提供了可行路径。

Abstract: Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.

</details>


### [140] [Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization](https://arxiv.org/abs/2512.19365)
*Zhongwei Chen,Hai-Jun Rong,Zhao-Xu Yang,Guoqi Li*

Main category: cs.CV

TL;DR: 提出SpikeViMFormer：用于无人机视角地理定位的首个脉冲神经网络框架，结合脉冲Transformer骨干、选择性注意与混合状态空间模块，并配合分层重排序对齐学习，在保持低功耗的同时达到与先进ANN相竞争的性能。


<details>
  <summary>Details</summary>
Motivation: ANN在DVGL上性能强但计算密集、功耗高；SNN低功耗但在表征学习中因脉冲稀疏性易丢失关键信息且难以建模长程依赖，且SNN用于DVGL几乎未被系统探索。

Method: 1) 设计SpikeViMFormer框架：轻量级脉冲Transformer作为骨干提取粗粒度特征；2) 提出脉冲驱动选择性注意(SSA)块，以门控机制选择性增强判别区域，缓解信息丢失；3) 提出脉冲驱动混合状态空间(SHS)块，建模长程依赖；4) 推理时仅用骨干以降算力；5) 提出分层重排序对齐学习(HRAL)，通过邻域重排序与跨batch一致性直接优化骨干表征。

Result: 在多个实验中，SpikeViMFormer优于现有SNN方法；与先进ANN相比取得具有竞争力的DVGL性能，同时具备低功耗潜力。

Conclusion: SNN在DVGL任务中可通过专用的脉冲Transformer骨干与SSA/SHS模块有效缓解信息损失与长程依赖建模难题；配合HRAL训练策略，在推理开销低的前提下达到SOTA级别的SNN表现并接近ANN性能。

Abstract: Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer

</details>


### [141] [DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition](https://arxiv.org/abs/2512.19387)
*Yueyao Chen,Kai-Ni Wang,Dario Tayupo,Arnaud Huaulm'e,Krystel Nyangoh Timoh,Pierre Jannin,Qi Dou*

Main category: cs.CV

TL;DR: 提出DSTED双通路框架，通过可靠记忆传播与不确定性感知原型检索，减少外科流程识别中的时序抖动并提升模糊阶段判别，取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有外科手术流程识别存在两大痛点：连续帧预测抖动导致不稳定、对阶段边界和语义相近阶段的区分能力弱。需要一种既能稳住时间一致性，又能显式处理不确定与模糊样本的框架。

Method: 构建DSTED双通路：1) RMP（可靠记忆传播）：对历史特征进行多准则置信度评估，仅筛选高可靠历史信息并融合，维持时序一致；2) UPR（不确定性感知原型检索）：从高不确定样本中学习类特定可学习原型，并进行自适应原型匹配以细化模糊帧表征；3) 置信驱动门控根据当前预测置信度动态平衡两条通路输出。

Result: 在AutoLaparo-hysterectomy数据集上达SOTA：准确率84.36%、F1 65.51%，分别超第二优方法3.51%与4.88%。消融显示RMP贡献+2.19%，UPR贡献+1.93%，组合有协同增益；进一步分析表明显著降低时序抖动并提升阶段转换处性能。

Conclusion: 将“时序一致性建模”与“阶段歧义消解”解耦的双通路设计可显著提升外科流程识别的稳定性与鲁棒性，具更强临床实用价值。

Abstract: Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.
  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.
  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.
  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.

</details>


### [142] [Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis](https://arxiv.org/abs/2512.19415)
*Xiaoming Zhang,Chunli Li,Jiacheng Hao,Yuan Gao,Danyang Tu,Jianyi Qiao,Xiaoli Yin,Le Lu,Ling Zhang,Ke Yan,Yang Hou,Yu Shi*

Main category: cs.CV

TL;DR: 提出MOON++多器官多模态框架，基于NCCT联合食管、肝、脾信息评估食管静脉曲张分级，较单器官方法显著提升分级判别（AUC最高至0.921），为内镜的非侵入替代提供可能。


<details>
  <summary>Details</summary>
Motivation: 内镜是EV诊断金标准但侵入性强；NCCT易获取却未充分利用。临床上肝病严重度与肝脾体积关系、门静脉高压相关的多器官改变有关，启发以多器官协同特征改进非侵入评估。

Method: 构建MOON++多器官协同网络：从NCCT提取食管、肝、脾的影像与体积等特征，通过多模态学习与临床先验融合，对1,631例患者进行训练/验证；按内镜分为四级，并在独立验证（n=239）与独立测试（n=289）上评估，与单器官方法比较；并开展放射科医师读片实验。

Result: 在严重EV（G3 vs <G3）分类上AUC=0.894（对照0.803）；在中重度（≥G2 vs <G2）分类上AUC=0.921（对照0.793）。独立验证与读者研究显示MOON++优于传统单器官策略。

Conclusion: MOON++为首个结合临床知识先验的多器官NCCT综合分析框架，可非侵入地评估EV严重程度，显示相较单器官方法的显著优势，有潜力作为内镜的有力补充或部分替代。

Abstract: Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus <G3) and 0.921 versus 0.793 for the differentiation of moderate to severe grades (>=G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.

</details>


### [143] [dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models](https://arxiv.org/abs/2512.19433)
*Yi Xin,Siqi Luo,Qi Qin,Haoxing Chen,Kaiwen Zhu,Zhiwei Zhang,Yangfan He,Rongchao Zhang,Jinbin Bai,Shuo Cao,Bin Fu,Junjun He,Yihao Liu,Yuewen Cao,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出 dMLLM-TTS，在扩展采样轨迹与迭代精炼两轴上做测试时扩展（TTS），通过分层搜索和自验证反馈，在不依赖外部验证器的情况下，以更低复杂度提升扩散多模态大模型的生成质量与效率（在 GenEval 上最高达 6 倍效率增益）。


<details>
  <summary>Details</summary>
Motivation: 扩散多模态大模型既要“懂图”又要“生图”，但现有测试时扩展方法在线性地扩大“候选数量 N”和“迭代步数 T”上，成本 O(NT) 高且需外部验证器挑选最优样本，限制实用性与可扩展性。

Method: 提出 dMLLM-TTS，从两条扩展轴出发：1）轨迹探索扩展，增加生成假设多样性；2）迭代精炼扩展，提升生成稳定性。为降低成本与去外部依赖：a）设计分层搜索算法，基于自适应扩展与剪枝，将复杂度由 O(NT) 降为 O(N+T)；b）引入自验证反馈，利用 dMLLM 内生的图文理解能力进行对齐评估与选择，无需外部验证器。

Result: 在 GenEval 基准上、针对三种代表性 dMLLM（如 Lumina-DiMOO、MMaDA、Muddit），该方法显著提升图像生成质量，并相较线性搜索获得最高 6× 的效率提升。

Conclusion: dMLLM-TTS 通过分层搜索与自验证反馈，有效释放扩散多模态大模型的生成潜力：在保持或提升质量的同时显著降低 TTS 成本，消除外部验证依赖，具备通用性与实用价值。

Abstract: Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.

</details>


### [144] [MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation](https://arxiv.org/abs/2512.19438)
*Fei Ge,Ying Huang,Jie Liu,Guixuan Zhang,Zhi Zeng,Shuwu Zhang,Hu Guan*

Main category: cs.CV

TL;DR: 提出一种显式协作的深度图像水印框架，通过嵌入器与提取器的双向交互与互教式训练，在不依赖大量失真模拟的前提下学习鲁棒性，并通过自适应特征调制模块提升稳健与感知质量，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法把嵌入与提取视为松耦合组件，仅通过最终损失弱关联，缺乏在训练中互相指导的结构化机制，导致嵌入无法解码感知、提取也无法影响嵌入策略，鲁棒性往往依赖穷举失真模拟。

Method: 1) 重新建模为显式协作架构，引入协作交互机制（CIM）在嵌入器与提取器间建立直接、双向通信，实施“互为教师”的联合优化；2) 设计自适应特征调制模块（AFMM），将“调制结构”和“调制强度”解耦，实现内容感知的特征调制：在嵌入时引导水印贴合稳定图像特征，在提取时抑制宿主干扰；3) 在CIM下，嵌入侧与提取侧AFMM构成闭环协同，使嵌入行为与提取目标一致；4) 以协同表征学习代替大量失真模拟进行鲁棒性学习。

Result: 在真实与AI生成数据集上，较SOTA方法获得更高的水印提取准确率，同时保持高感知质量；表现出更强的鲁棒性与泛化能力。

Conclusion: 通过在体系结构层面引入嵌入-提取的显式协作与AFMM闭环调制，可在更少依赖失真模拟的情况下学得鲁棒水印表征，兼顾准确性与感知质量，优于现有方法。

Abstract: Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.

</details>


### [145] [D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning](https://arxiv.org/abs/2512.19443)
*Evelyn Zhang,Fufu Yu,Aoqi Wu,Zichen Wen,Ke Yan,Shouhong Ding,Biqing Qi,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出D2Pruner：通过“去偏重要性 + 结构化稀疏”联合剪枝，在不牺牲细粒度定位能力的前提下，大幅减少视觉token与FLOPs。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM视觉token剪枝方法在细粒度定位任务上崩溃：重要性法有位置偏置，忽略语义；多样性法对结构与用户提示不敏感，无法处理空间冗余，导致精度损失。

Method: 两阶段剪枝框架D2Pruner：1) 去偏重要性打分，先选出核心枢纽token；2) 对剩余token在“空间邻近+语义相似”的混合图上执行最大独立集（MIS）选择，迭代保留最重要且可用的token并删除其邻居，从而同时最大化重要性与多样性。

Result: 在LLaVA-1.5-7B上用于通用理解任务，FLOPs下降74.2%，保留99.2%原始性能；在InternVL-2.5-8B的困难定位基准上，在90% token裁剪下仍保留85.7%性能，相比现有方法最高提升达63.53%。

Conclusion: D2Pruner有效解决位置偏置与结构盲点，兼顾语义重要性与空间/语义冗余控制，实现对长视觉序列的高效且高保真剪枝，尤其在细粒度定位上显著优于现有方法。

Abstract: Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\% while retaining 99.2\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\% performance at a 90\% token reduction rate, marking a significant advancement with up to 63. 53\% improvement over existing methods.

</details>


### [146] [Sign Language Recognition using Parallel Bidirectional Reservoir Computing](https://arxiv.org/abs/2512.19451)
*Nitin Kumar Singh,Arie Rachmad Syulistyo,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CV

TL;DR: 提出一种轻量级手语识别系统：用MediaPipe提取手部关键点，再用并行双向水库计算（PBRC，基于两路双向ESN）进行分类；在WLASL上达到Top-1 60.85%、Top-5 85.86%、Top-10 91.74%，训练仅18.67秒，适合边缘设备实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习SLR精度高但计算与训练成本大，不利于在资源受限的边缘设备上实时部署，需要一种既低算力又高效的替代方案。

Method: 采用MediaPipe实时跟踪手部并提取关节坐标作为时序特征；构建并行双向水库计算架构（PBRC）：包含两路基于ESN的双向RC模块并行，捕获前后向时序依赖并融合形成丰富表征，随后进行分类。利用WLASL数据集进行训练与评估。

Result: 在WLASL上取得Top-1 60.85%、Top-5 85.86%、Top-10 91.74%；相较Bi-GRU等深度模型训练时间从>55分钟降至18.67秒。

Conclusion: PBRC+MediaPipe实现了轻量、低成本且可实时的手语识别，适合边缘端部署；水库计算的快速训练与双向并行结构在保持较好精度的同时显著降低计算负担。

Abstract: Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.

</details>


### [147] [Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation](https://arxiv.org/abs/2512.19479)
*Guoli Jia,Junyao Hu,Xinwei Long,Kai Tian,Kaiyan Zhang,KaiKai Zhao,Ning Ding,Bowen Zhou*

Main category: cs.CV

TL;DR: 提出Emotion-Director框架，通过跨模态协同扩散模型与多代理重写系统，实现超越语义的情感导向图像生成，并在质性与量化实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感导向图像生成方法倾向于把情感简化为语义（affective shortcut），忽略了情感与语义的差异，这限制了在广告等应用中对细腻情绪表达的生成能力。

Method: 1) MC-Diffusion：在扩散模型中联合使用视觉提示与文本提示进行协同引导，并在DPO优化中引入负视觉提示，提升在相同语义下对不同情感的区分敏感度。2) MC-Agent：跨模态多代理系统，重写文本提示以更好表达目标情感；通过多主体模拟主观性并采用“概念链”工作流增强重写提示的视觉可表达性，避免模板化。

Result: 在大量质性对比与定量指标上，Emotion-Director在表达目标情绪、情感区分能力与图像质量方面均优于现有方法。

Conclusion: 跨模态协同的生成与提示重写相结合，可有效突破“情感≈语义”的捷径，显著提升情感导向图像生成的准确性与表现力。

Abstract: Image generation based on diffusion models has demonstrated impressive capability, motivating exploration into diverse and specialized applications. Owing to the importance of emotion in advertising, emotion-oriented image generation has attracted increasing attention. However, current emotion-oriented methods suffer from an affective shortcut, where emotions are approximated to semantics. As evidenced by two decades of research, emotion is not equivalent to semantics. To this end, we propose Emotion-Director, a cross-modal collaboration framework consisting of two modules. First, we propose a cross-Modal Collaborative diffusion model, abbreviated as MC-Diffusion. MC-Diffusion integrates visual prompts with textual prompts for guidance, enabling the generation of emotion-oriented images beyond semantics. Further, we improve the DPO optimization by a negative visual prompt, enhancing the model's sensitivity to different emotions under the same semantics. Second, we propose MC-Agent, a cross-Modal Collaborative Agent system that rewrites textual prompts to express the intended emotions. To avoid template-like rewrites, MC-Agent employs multi-agents to simulate human subjectivity toward emotions, and adopts a chain-of-concept workflow that improves the visual expressiveness of the rewritten prompts. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director in emotion-oriented image generation.

</details>


### [148] [Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration](https://arxiv.org/abs/2512.19486)
*Shaochen Bi,Yuting He,Weiming Wang,Hao Chen*

Main category: cs.CV

TL;DR: 提出DySNet，通过动态感受野与动态权重缓解DMIR双输入导致的组合爆炸与干扰特征配对，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: DMIR需同时处理两幅图像，特征组合关系指数增长，模型在特征建模时易被大量无关/干扰的跨图像特征组合影响，降低匹配与配准精度。需要机制抑制无效组合并强调潜在有用关系。

Method: 构建Dynamic Stream Network (DySNet)，引入两点动态性：1) Adaptive Stream Basin (AdSB) 动态调整感受野形状与范围，使网络聚焦与相关性更高的跨图像特征关系；2) Dynamic Stream Attention (DySA) 生成输入自适应的动态权重，在特征流中搜索并强调更有价值的组合关系。整体形成可动态调参的特征流以过滤干扰、突出重要配对。

Result: 在多组实验中DySNet在DMIR任务上持续优于当前最先进方法，表现出更强泛化能力；给出代码仓库链接以复现。

Conclusion: 通过动态感受野与动态注意力权重，DySNet有效抑制双输入引起的组合干扰并更好建模潜在特征关系，从而提升配准性能与泛化。

Abstract: Combinatorial explosion problem caused by dual inputs presents a critical challenge in Deformable Medical Image Registration (DMIR). Since DMIR processes two images simultaneously as input, the combination relationships between features has grown exponentially, ultimately the model considers more interfering features during the feature modeling process. Introducing dynamics in the receptive fields and weights of the network enable the model to eliminate the interfering features combination and model the potential feature combination relationships. In this paper, we propose the Dynamic Stream Network (DySNet), which enables the receptive fields and weights to be dynamically adjusted. This ultimately enables the model to ignore interfering feature combinations and model the potential feature relationships. With two key innovations: 1) Adaptive Stream Basin (AdSB) module dynamically adjusts the shape of the receptive field, thereby enabling the model to focus on the feature relationships with greater correlation. 2) Dynamic Stream Attention (DySA) mechanism generates dynamic weights to search for more valuable feature relationships. Extensive experiments have shown that DySNet consistently outperforms the most advanced DMIR methods, highlighting its outstanding generalization ability. Our code will be released on the website: https://github.com/ShaochenBi/DySNet.

</details>


### [149] [FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors](https://arxiv.org/abs/2512.19504)
*Georgios Voulgaris*

Main category: cs.CV

TL;DR: 提出物理感知的多光谱表示学习框架，通过融合SWIR比值与TIR信息并在骨干中注入可训练的差分信号处理先验，提升跨谱鲁棒性与真实场景泛化，达到优于SOTA的精度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视觉模型常依赖与实际成像物理不匹配的归纳偏置，导致在跨光谱与真实世界条件下脆弱；尤其直接依赖热辐射的做法难以捕捉由持续热源引发但更稳定的环境间接变化。

Method: 提出物理感知表示学习：选用对土壤性质变化敏感的SWIR比值作为物理先验，并与TIR通过中间融合架构FusionNet集成。骨干包含：在卷积层中嵌入可训练的差分信号处理先验、混合池化策略、更宽感受野，以提升跨光谱鲁棒性。进行系统消融验证每个组件贡献；并与多种光谱配置下的SOTA比较；进行迁移学习实验评估ImageNet预训练影响。

Result: DGCNN在SWIR比值上达88.7%准确率；FusionNet达90.6%，在五种光谱配置上均优于SOTA。迁移学习显示ImageNet预训练会降低TIR性能，提示需要模态感知训练。真实世界数据上也获得鲁棒与可泛化表现。

Conclusion: 将物理先验的特征选择（SWIR比值）与有原则的深度架构（FusionNet与差分先验、混合池化、宽感受野）结合，可稳定刻画长期物理过程特征，提升跨谱与真实环境下的多光谱学习性能；跨谱学习应避免无关域的预训练偏置。

Abstract: Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.
  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.
  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.
  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.

</details>


### [150] [Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation](https://arxiv.org/abs/2512.19512)
*Ziyang Song,Zelin Zang,Zuyao Chen,Xusheng Liang,Dong Yi,Jinlin Wu,Hongbin Liu,Jiebo Luo*

Main category: cs.CV

TL;DR: 提出两种针对解剖理解的强化策略，解决GRPO在医疗图像推理中的知识共享与多样性不足问题，在SGG-VQA与OmniMedVQA上显著提升MLLM表现。


<details>
  <summary>Details</summary>
Motivation: MLLM在自然图像推理上进步显著，但在临床解剖外科图像中受限于数据复杂、专家标注稀缺，传统SFT难以获得精确且临床一致的答案。现有基于GRPO的方法虽可低数据提升推理，但存在跨结构知识难共享与单一路径收敛过快两大问题。

Method: 1) 解剖相似度课程学习：依据候选答案的相似度控制题目难度，逐步推进学习以促进跨解剖结构的知识迁移；2) 组多样性问题增强：对困难问题进行多样化扩增，扩大搜索空间，避免策略单一化；整体框架在GRPO上叠加上述策略以改善推理与收敛特性。

Result: 在SGG-VQA与OmniMedVQA两大基准上取得显著提升（相较基线/原GRPO），表明对医疗解剖推理的有效增强。

Conclusion: 通过课程学习与问题多样性增强缓解GRPO的知识共享不足与过早收敛问题，可在标注有限条件下稳步提升MLLM在解剖理解与医疗VQA任务中的推理能力；代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1

</details>


### [151] [A Convolutional Neural Deferred Shader for Physics Based Rendering](https://arxiv.org/abs/2512.19522)
*Zhuo He,Yingdong Ru,Qianying Liu,Paul Henderson,Nicolas Pugeault*

Main category: cs.CV

TL;DR: 提出pbnds+：基于物理的神经延迟着色管线，用CNN取代MLP以降参提效，并加入能量正则化以提升暗光照下的稳定性，实验优于经典、SOTA神经着色与扩散方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染多用MLP拟合渲染方程，虽能实现真实感重光照，但存在参数量大、训练与推理耗时高；数据驱动对大规模与均衡数据依赖强，易在不常见光照（如暗场景）失真或过曝，缺乏物理一致性约束。

Method: 提出pbnds+神经延迟着色：以卷积神经网络替代密集连接的MLP，降低参数与计算；在训练中引入能量正则化，限制在低照度输入下的反射能量（输出亮度），提升物理合理性与稳健性；在真实数据集上训练用于着色与重光照任务。

Result: 在多组实验中，相较经典基线、SOTA神经着色模型以及基于扩散的方法，pbnds+在质量与效率上均有优势，尤其在暗光照条件下表现更稳健。

Conclusion: 利用CNN的局部归纳偏置与能量正则化，可在保持真实感的同时显著降参提效，缓解数据不均衡导致的暗场景失真，构建更稳健的基于物理的神经重光照与着色流程。

Abstract: Recent advances in neural rendering have achieved impressive results on photorealistic shading and relighting, by using a multilayer perceptron (MLP) as a regression model to learn the rendering equation from a real-world dataset. Such methods show promise for photorealistically relighting real-world objects, which is difficult to classical rendering, as there is no easy-obtained material ground truth. However, significant challenges still remain the dense connections in MLPs result in a large number of parameters, which requires high computation resources, complicating the training, and reducing performance during rendering. Data driven approaches require large amounts of training data for generalization; unbalanced data might bias the model to ignore the unusual illumination conditions, e.g. dark scenes. This paper introduces pbnds+: a novel physics-based neural deferred shading pipeline utilizing convolution neural networks to decrease the parameters and improve the performance in shading and relighting tasks; Energy regularization is also proposed to restrict the model reflection during dark illumination. Extensive experiments demonstrate that our approach outperforms classical baselines, a state-of-the-art neural shading model, and a diffusion-based method.

</details>


### [152] [Multi-Modal Soccer Scene Analysis with Masked Pre-Training](https://arxiv.org/abs/2512.19528)
*Marc Peral,Guillem Capellera,Luis Ferraz,Antonio Rubio,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一种多模态、社交时空Transformer架构，在无球轨迹显式输入下推断球轨迹，并鲁棒判定球状态与持球者；引入CropDrop预训练屏蔽策略，促使跨模态融合；在大规模数据集上三任务均显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法高度依赖精准球跟踪和手工启发式规则，在真实比赛中易受遮挡与噪声影响，难以可靠推断球轨迹、球状态和持球者。需要一种能融合结构化运动信息与视觉线索、在缺失与遮挡下仍稳健的统一模型。

Method: 构建统一多模态框架，输入三类模态：球员轨迹、球员类型、球员图像裁剪。利用级联的社交时空Transformer模块建模空间交互与时间动态，在无直接球位置历史/未来的条件下推断球轨迹，并联合进行球状态分类与持球者识别。提出CropDrop：面向图像模态的掩蔽式预训练，随机丢弃裁剪以避免过度依赖视觉特征，鼓励跨模态对齐与协同。

Result: 在大规模真实顶级联赛数据上，三个任务（球轨迹推断、球状态分类、持球者识别）均显著超过最新基线方法，表现出在噪声与遮挡条件下的鲁棒性与泛化性。

Conclusion: 将结构化（轨迹、类型）与视觉线索在Transformer中深度融合，并辅以现实感更强的模态特定掩蔽预训练，可在足球战术相机场景理解的多任务上取得全面提升；合理的掩蔽策略对多模态学习至关重要。

Abstract: In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.

</details>


### [153] [SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates](https://arxiv.org/abs/2512.19534)
*Chi Zhang,Braedon Gunn,Andrew M. Read-Fuller*

Main category: cs.CV

TL;DR: 提出SlicerOrbitSurgerySim：在3D Slicer上的开源扩展，用于在患者特异环境中交互式比对多种预成型眶壁板的贴合度，并输出可重复的量化指标，以改进术前决策与研究教学。


<details>
  <summary>Details</summary>
Motivation: 现有眶壁重建常用预成型板以节省成本与时间，但缺乏公开工具和标准化量化指标来比较不同厂商、尺寸与解剖差异下的贴合度，导致适配不良、并发症和翻修风险。

Method: 基于3D Slicer开发开源扩展SlicerOrbitSurgerySim：支持将多块预成型眶板在患者特异虚拟模型中进行交互式配准、评估与比较；自动生成板-眶腔表面距离等量化指标与可视化图；支持个体病例规划与群体层面的统计分析；提供示例数据、教程与试点研究验证流程。

Result: 软件能在虚拟规划环境中产出可重复的板-眶腔距离测度与可视化，实现不同板型与放置策略的客观对比；初步试点与示例数据表明其具备可用性与可重复性，便于研究共享。

Conclusion: 该开源工具为眶壁重建提供标准化、量化的贴合度评估与比较框架，可改善术前选择、减少术中改型，并促进跨机构研究与教学的透明与再现。

Abstract: Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.

</details>


### [154] [CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion](https://arxiv.org/abs/2512.19535)
*Moritz Böhle,Amélie Royer,Juliette Marrie,Edouard Grave,Patrick Pérez*

Main category: cs.CV

TL;DR: 提出CASA：在跨注意力层中引入局部文本自注意，以接近“图像token插入”的效果，同时保持跨注意力的可扩展性，提升细粒度视觉任务并在长上下文/视频流场景更高效。


<details>
  <summary>Details</summary>
Motivation: 现有VLM常用将视觉token插入语言模型，使图文可完全互相注意，但在高分辨率图像、长对话与视频流时计算与内存成本过高；纯跨注意力虽高效但在细粒度视觉理解上性能明显落后。需要一种兼顾效率与性能的方法。

Method: 发现关键在于在专用跨注意力层中也允许局部文本-文本交互。基于此提出CASA（Cross-Attention via Self-Attention）：在跨注意力模块内加入局部文本自注意机制，实现文本局部交互与图文跨注意结合，从而无需全量图像token插入即可捕捉细节，同时保持跨注意力的线性伸缩性。

Result: 在常见图像理解基准上，CASA显著缩小与全量token插入方法的性能差距；在长上下文多模态任务（如视频流字幕生成）中，计算与内存效率与跨注意力模型一致，表现更可扩展。

Conclusion: CASA以简单高效的结构在效率与性能间取得更优折中：接近插入式VLM的精度，同时保留跨注意力在长序列与高分辨率场景的可扩展性，适合大规模多模态与流式应用。

Abstract: Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .

</details>


### [155] [StoryMem: Multi-shot Long Video Storytelling with Memory](https://arxiv.org/abs/2512.19539)
*Kaiwen Zhang,Liming Jiang,Angtian Wang,Jacob Zhiyuan Fang,Tiancheng Zhi,Qing Yan,Hao Kang,Xin Lu,Xingang Pan*

Main category: cs.CV

TL;DR: StoryMem 将单镜头视频扩散模型升级为多镜头“讲故事者”，用紧凑可更新的显式视觉记忆在迭代生成中保持跨镜头一致性，并在新评测集 ST-Bench 上取得更好美学与一致性表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成多为单镜头或短时跨度，难以在分钟级故事中兼顾叙事连贯、角色场景一致与镜头间平滑过渡。需要一种能在长程生成中保留并利用历史视觉信息的机制。

Method: 提出 StoryMem：把长视频讲述分解为“带记忆的迭代镜头合成”。核心是 Memory-to-Video (M2V)：维护由既往镜头关键帧构成的紧凑动态记忆库；通过潜空间拼接与负向 RoPE 偏移将记忆注入预训练单镜头视频扩散模型，仅需 LoRA 微调。并结合语义关键帧选择与审美偏好过滤，确保记忆信息量与稳定性；自然支持平滑镜头转场与定制化剧情生成。另构建多镜头评测集 ST-Bench。

Result: 在 ST-Bench 上相较已有方法，StoryMem 显著提升跨镜头一致性，同时维持高美学质量与文本遵循度；能够生成接近分钟级的连贯多镜头视频，并展现更平滑转场与可控叙事。

Conclusion: 显式视觉记忆驱动的迭代镜头生成是实现长时程、连贯、多镜头视频讲述的有效范式；StoryMem 以低开销改造现有模型，取得更好一致性与质量，为可控长视频生成迈出关键一步。

Abstract: Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.

</details>


### [156] [ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars](https://arxiv.org/abs/2512.19546)
*Ziqiao Peng,Yi Chen,Yifeng Ma,Guozhen Zhang,Zhiyao Sun,Zixiang Zhou,Youliang Zhang,Zhengguang Zhou,Zhaoxin Fan,Hongyan Liu,Yuan Zhou,Qinglin Lu,Jun He*

Main category: cs.CV

TL;DR: ActAvatar提出通过文本实现相位级精准动作控制的会说话头像生成框架，兼顾动作语义与时间对齐，并在音频-视觉-文本三模态间实现逐层对齐，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有会说话头像方法文本跟随弱、动作与音频缺乏时间对齐，且依赖额外控制信号（如骨架）。需要一种能仅凭文本指令实现细粒度、时间精确的动作控制，同时保持音视频一致性与高视觉质量。

Method: 提出ActAvatar，包含三点关键设计：1) 相位感知交叉注意力（PACA）：将文本提示分解为全局基础块与带时间锚点的相位块，使模型在不同时间段关注相位相关词，实现时间-语义对齐；2) 渐进式音视对齐：随网络层次推进调整模态影响，浅层以文本建立动作结构，深层以音频强化口型，减少模态干扰；3) 两阶段训练：先在多样数据上学习稳健音视对应，再用结构化标注微调注入动作控制，兼顾对齐与文本跟随能力。

Result: 在多项实验中，ActAvatar在动作控制精度（相位级对齐）与可视质量上均显著优于SOTA基线，展示更强的文本跟随和音画同步效果。

Conclusion: 通过PACA、逐层模态对齐与两阶段训练，ActAvatar实现了基于文本的精细动作时序控制并维持高质量音视一致性，为会说话头像的动作可控性与实用性带来显著提升。

Abstract: Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.

</details>


### [157] [BabyFlow: 3D modeling of realistic and expressive infant faces](https://arxiv.org/abs/2512.19560)
*Antonia Alomar,Mireia Masias,Marius George Linguraru,Federico M. Sukno,Gemma Piella*

Main category: cs.CV

TL;DR: 提出BabyFlow：基于正态化流的婴儿面部身份-表情解耦生成模型，结合跨年龄表情迁移与扩散模型，实现更准确的3D重建与高保真2D合成，助力数据增广与早期面部分析。


<details>
  <summary>Details</summary>
Motivation: 婴儿颅颌面形态可用于早期发育障碍检测，但婴儿面部数据稀缺且自发表情多、非线性强，传统线性模型难以表征并难以独立控制身份与表情。

Method: 1) 构建正态化流（normalizing flows）框架，学习灵活的概率表示，并显式解耦身份与表情潜变量；2) 引入跨年龄表情迁移：将成人3D扫描的表情系统性地适配到婴儿面部以扩充婴儿表达样本；3) 与扩散模型集成，在保持一致3D几何的前提下生成高保真2D婴儿图像。

Result: 在高度表情区域（口、眼、鼻）显著提升3D重建精度；可在保持身份的同时合成与编辑婴儿表情；生成与3D一致的高质量2D图像用于数据增广。

Conclusion: BabyFlow克服婴儿面部数据稀缺与表情复杂的挑战，实现身份-表情可控的生成与重建，并提供面向早期面部分析和数据增广的实用工具。

Abstract: Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.

</details>


### [158] [No Data? No Problem: Robust Vision-Tabular Learning with Missing Values](https://arxiv.org/abs/2512.19602)
*Marta Hasny,Laura Daza,Keno Bressem,Maxime Di Folco,Julia Schnabel*

Main category: cs.CV

TL;DR: 提出RoVTL框架，在影像+表格多模态中，通过预训练引入“表格缺失”增强、微调阶段门控交叉注意力与新损失，实现0%–100%表格可用度下的稳健表现，并在UKB心脏MRI与外部数据及自然图像任务上验证优越鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实训练常可获得丰富表格属性（如生理、临床指标），但部署时往往缺失且缺失比例不定。现有方法要么假设固定缺失模式、要么在缺失时性能骤降，缺乏对任意缺失程度的统一鲁棒方案。

Method: 两阶段：1) 对比预训练，将“表格属性缺失”作为数据增强，迫使视觉-表格表征在缺失扰动下对齐稳健；2) 下游微调，采用门控交叉注意力进行多模态融合，并提出“Tabular More vs. Fewer”排序损失，根据可用表格量对预测进行排序一致性约束；同时用“解耦梯度学习”抑制模态间梯度干扰，获得各缺失水平的一致优化。

Result: 在UK Biobank心脏MRI上，相比现有多模态与缺失处理方法，RoVTL在从0%到100%表格可用度的全范围均更稳健，性能退化最小。能泛化到外部心脏MRI数据的多模态疾病分类，并在自然图像车广告数据集上复现鲁棒性。

Conclusion: 通过将缺失视作训练时的第一类公民并配合门控交叉注意力、排序式损失与解耦优化，RoVTL在任意表格可用度下提供一致、稳健的多模态性能，并具备跨数据集与跨领域的可迁移性。

Abstract: Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.

</details>


### [159] [MapTrace: Scalable Data Generation for Route Tracing on Maps](https://arxiv.org/abs/2512.19609)
*Artemis Panagopoulou,Aveek Purohit,Achin Kulshrestha,Soroosh Yazdani,Mohit Goyal*

Main category: cs.CV

TL;DR: 提出用可扩展的合成数据流水线，在合成地图上自动生成像素级路径标注，微调MLLM以提升地图路径追踪能力，在MapBench上成功率最高提升6.4点并降低NDTW误差。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在人类擅长的细粒度空间理解（如地图路径追踪）上表现不足，常违反基本路径约束；高质量像素级路径标注难以大规模获取，限制了训练。

Method: 构建可扩展合成数据生成流水线：利用合成地图与像素级解析，自动产生精确路径标注；据此生成包含4k张地图、23k条路径的微调数据集；用该数据集微调开源与商用MLLM，并在MapBench进行评测。

Result: 微调后模型在MapBench上表现显著提升：成功率最高提升6.4个百分点，并降低路径追踪误差（NDTW），表现更稳健。

Conclusion: 细粒度空间推理虽在预训练中缺失，但可以通过合成监督显式教会MLLM；合成数据是解决像素级标注稀缺与成本问题的有效途径。

Abstract: While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.

</details>


### [160] [Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment](https://arxiv.org/abs/2512.19632)
*Da Tan,Michael Beck,Christopher P. Bidinosti,Robert H. Gulden,Christopher J. Henry*

Main category: cs.CV

TL;DR: 论文探索用扩散生成模型解决农业AI数据稀缺：微调Stable Diffusion合成作物图像、室内到室外图像翻译、以及基于专家偏好的对齐微调，显著提升分类与检测性能并提高生成稳定性。


<details>
  <summary>Details</summary>
Motivation: 农业AI需要大量多样高质量的田间植物图像，但真实田间采集昂贵、费时且受季节限制。需要一种数据高效的方式来扩充和迁移数据，同时保证生成质量与专家认可度。

Method: 1) 在室内与室外植物图像与文本描述上微调Stable Diffusion，生成可控（文本条件）的油菜与大豆图像；用IS、FID及下游表型分类评估，并用合成图像增强训练。
2) 使用基于DreamBooth的文本反演与图像引导扩散，将高分辨率室内数据“翻译”为逼真的室外风格，用于提升YOLOv8的杂草检测与分类。
3) 构建偏好引导微调：收集专家打分训练奖励模型，进行奖励加权更新，使生成结果更稳定且符合专家偏好。

Result: - 合成图像数据增强提高下游表型分类准确率；IS/FID显示生成质量可观。
- 室内到室外翻译的数据用于训练，显著提升YOLOv8在杂草检测与分类上的表现。
- 偏好引导微调后生成更稳定、更符合专家偏好。

Conclusion: 扩散模型结合数据合成、域迁移与偏好对齐可构建面向农业AI的高效数据生成流水线，在有限真实田间数据条件下提升分类与检测性能，具有实际可行性与推广潜力。

Abstract: The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.

</details>


### [161] [4D Gaussian Splatting as a Learned Dynamical System](https://arxiv.org/abs/2512.19648)
*Arnold Caleb Asiimwe,Carl Vondrick*

Main category: cs.CV

TL;DR: EvoGS 将4D Gaussian Splatting重释为连续时间动力系统，用学习到的神经动力场驱动高斯的状态随时间演化，实现更高的时序一致性、外推与可控组合动态，同时保持实时渲染。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景方法多依赖逐帧形变（deformation fields），对时间监督要求高，难以外推到未见时间，且难以在局部可控地组合/注入运动；作者希望通过学习“运动规律”而非逐帧配准，提高样本效率、实现时间外推与可控合成，同时保持4D GS的实时优势。

Method: 将高斯表征的状态视为随时间由神经动力场定义的常微分方程演化：x'(t)=f_θ(x(t), t)。通过对该动力场进行学习，而非每帧独立形变；渲染仍采用高斯光栅化。训练时以稀疏时序观测监督动力场积分后的状态；可进行前向/后向积分做时间外推；通过组合/局部注入不同子动力场实现可控的局部动态。

Result: 在动态场景基准上，相比基于形变场的基线，EvoGS获得更好的运动连贯性与时间一致性，并保持实时渲染性能；在稀疏时间监督下更样本高效，并能进行时间外推。

Conclusion: 把4D GS建模为连续时间动力系统能从根源上捕获运动规律，带来样本效率、外推性与可控组合动态等优势，同时不牺牲实时渲染。

Abstract: We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering

</details>


### [162] [Over++: Generative Video Compositing for Layer Interaction Effects](https://arxiv.org/abs/2512.19661)
*Luchao Qi,Jiaye Wu,Jun Myeong Choi,Cary Phillips,Roni Sengupta,Dan B Goldman*

Main category: cs.CV

TL;DR: 提出“增强合成”任务与Over++框架，在不依赖相机/深度假设下，为输入视频与图层生成符合文本条件的半透明环境效果（影子、反射、尘土、水花等），同时尽量保持原场景；在有限数据上仍优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 专业视频合成中，艺术家需手工为前景与背景创建环境交互效果，成本高且耗时；而现有视频生成/修复方法要么破坏原视频一致性，要么需要逐帧掩码、或产生不真实结果，难以满足真实制作流程需求。

Method: 定义“增强合成”任务；提出Over++视频效果生成框架：不假设相机姿态、场景静止或深度监督。构建成对效果数据集；提出不配对增强策略以保持文本可编辑性；支持可选的掩码控制与关键帧引导，无需密集标注；以文本与输入图层为条件合成半透明环境效果，同时尽量保留原视频。

Result: 在有限训练数据下，Over++能生成多样、逼真的环境效果，并在效果生成质量与场景保真方面均优于现有基线方法。

Conclusion: Over++证明了在无强假设和少数据条件下，通过成对数据与不配对增强策略，可实现文本驱动、可控且对原场景友好的环境效果生成，为视频合成工作流提供实用替代。

Abstract: In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.

</details>


### [163] [Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2512.19663)
*Argha Kamal Samanta,Harshika Goyal,Vasudha Joshi,Tushar Mungle,Pabitra Mitra*

Main category: cs.CV

TL;DR: 提出一个知识增强的联合嵌入多模态框架，将视网膜眼底图像、临床文本与结构化患者数据通过多模态Transformer对齐，显著提升医学跨模态检索与DR分级性能，在BRSET上实现近乎完美的检索Recall@1=99.94%和SOTA分类准确率，并在DeepEyeNet零样本上保持强泛化。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型（如CLIP）在自然图像上表现优良，但在医学场景，尤其是眼底图像的跨模态检索与图文对齐方面效果不佳。DR作为可防盲的主要原因，亟需精确、可泛化的自动诊断与检索系统，以支撑临床决策与知识发现。

Method: 构建知识增强的联合嵌入框架：分别使用ViT-B/16编码眼底图像、Bio-ClinicalBERT编码临床文本、多层感知机编码结构化临床/人口学特征；通过带有模态特定嵌入的联合Transformer进行多模态融合；采用多目标训练——模态对比损失（图-文、图-结构、文-结构）、图像与文本重构损失，以及依据ICDR与SDRG的DR分级分类损失。

Result: 在BRSET数据集上，文本到图像检索Recall@1达99.94%，显著超越微调CLIP的1.29%；分类准确率：SDRG 97.05%，ICDR 97.97%。在未见过的DeepEyeNet上零样本检索Recall@1为93.95%，对比微调CLIP仅0.22%，显示出强泛化能力。

Conclusion: 多模态联合训练与知识增强有效弥合医学图文对齐鸿沟，实现对眼底图像与临床信息的高质量跨模态表征，兼具卓越的检索能力与稳健的DR分级诊断性能，并在跨数据集零样本场景下保持强泛化。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.

</details>


### [164] [Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning](https://arxiv.org/abs/2512.19676)
*Mojtaba Safari,Shansong Wang,Vanessa L Wildman,Mingzhe Hu,Zach Eidex,Chih-Wei Chang,Erik H Middlebrooks,Richard L. J Qiu,Pretesh Patel,Ashesh B. Jania,Hui Mao,Zhen Tian,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出一种结合多头选择性状态空间模型（MHSSM）与轻量通道MLP（MambaFormer块）的MRI超分辨框架，在7T脑和1.5T前列腺数据上以极低参数量与计算量取得SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 临床高分辨MRI受制于采集时间过长；深度学习SR虽可后处理提升分辨率，但普遍在“保真-效率”间权衡，难以兼顾细节保留与推理开销，影响临床落地。

Method: 构建由MambaFormer块堆叠的轻量SR网络：2D patch提取与混合扫描以覆盖长程依赖；每个块融合MHSSM（选择性状态空间建模长依赖）、深度可分离卷积（局部纹理）、以及门控通道混合（轻量通道交互）。与Bicubic、GAN（CycleGAN/Pix2pix/SPSR）、Transformer（SwinIR）、Mamba（MambaIR）和扩散模型（I2SB/Res-SRDiff）比较。在7T T1 MP2RAGE（n=142）与1.5T前列腺T2w（n=334）上评估。

Result: 7T脑：SSIM 0.951±0.021，PSNR 26.90±1.41 dB，LPIPS 0.076±0.022，GMSD 0.083±0.017，显著优于所有基线（p<0.001）。前列腺：SSIM 0.770±0.049，PSNR 27.15±2.19 dB，LPIPS 0.190±0.095，GMSD 0.087±0.013。仅0.9M参数与57 GFLOPs，相比Res-SRDiff参数降99.8%、算力降97.5%，且在精度与效率上均优于SwinIR与MambaIR。

Conclusion: 该轻量MHSSM+通道MLP框架在多数据集实现高保真且高效率的MRI超分辨，显著减少计算成本并提升解剖细节，具备较强的临床转化潜力。

Abstract: Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p<0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.

</details>


### [165] [WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion](https://arxiv.org/abs/2512.19678)
*Hanyang Kong,Xingyi Yang,Xiaoxu Zheng,Xinchao Wang*

Main category: cs.CV

TL;DR: WorldWarp提出结合3D几何锚定与2D扩散细化的框架，通过3DGS缓存进行显式视角变换，配合时空扩散的“填充-修订”策略与可变噪声调度，在长时程与复杂相机运动下实现几何一致与高保真视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成在相机条件的潜空间中最有效，但几何一致性要求在像素空间严格遵循3D逻辑，二者脱节导致遮挡区域与复杂机位下的不一致、破绽与漂移。需要一种能把3D结构约束与2D生成能力无缝结合的方法。

Method: 1) 在线3D几何缓存：基于高斯点渲染(3DGS)从历史帧构建并维护一个可随时重投影的几何缓存，将过往内容显式warp到新视角，提供结构脚手架。2) 时空扩散模型(ST-Diff)：以“填充-修订”为目标，对空洞区域施加强噪声以触发生成，对已warp区域施加弱噪声以进行细化；并在每步更新3D缓存以跨片段维持一致。3) 组合策略：3D逻辑定结构，扩散逻辑润纹理。

Result: 在长程视频与复杂相机轨迹上实现最先进的几何一致性与视觉保真，能有效处理遮挡造成的空洞与伪影，跨视频块保持一致性。附项目页提供定性与定量结果。

Conclusion: 将3D显式几何锚定与2D扩散式细化通过时空可变噪声调度耦合，可在复杂场景中稳定生成长视频，兼顾结构一致与纹理质量，代表了一条有效弥合潜空间生成与像素空间几何约束的路径。

Abstract: Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a "fill-and-revise" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.

</details>


### [166] [VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation](https://arxiv.org/abs/2512.19680)
*Xinyao Liao,Qiyuan He,Kai Xu,Xiaoye Qu,Yicong Li,Wei Wei,Angela Yao*

Main category: cs.CV

TL;DR: 提出VA-π：一种在像素空间直接对齐AR视觉生成器与离散tokenizer的轻量后训练方法；用变分优化导出ELBO，将像素重建与自回归建模统一，并以像素重建质量作为强化学习内在奖励，无需重采样或重训tokenizer；在极少数据与短时调优下显著提升FID/IS与文本生成指标。


<details>
  <summary>Details</summary>
Motivation: AR视觉生成依赖tokenizer把图像映射为离散序列，但tokenizer按干净图像重建训练，而AR生成器仅按token似然训练，二者目标失配，导致生成token解码回像素时图像质量差，且缺乏像素空间的直接监督。

Method: 把生成器-tokenizer对齐表述为变分优化，推导同时包含像素重建与自回归似然的ELBO；在离散token空间中，以强化学习策略优化，将AR生成器视作策略，像素重建质量作为内在奖励，在teacher forcing下评估预测序列的重建能力以提供像素级指导；ELBO中的正则项保持token分布一致性；无需重训tokenizer或引入外部奖励模型，作为对现有AR模型的轻量后训练。

Result: 在仅用1% ImageNet-1K与约25分钟调优条件下，LlamaGen-XXL的FID从14.36降至7.65，IS从86.55升至116.70；在GenEval文本到图像任务上提升：LlamaGen由0.306到0.339，Janus-Pro由0.725到0.744。

Conclusion: VA-π有效缓解AR生成器与tokenizer的目标失配，以像素空间奖励和变分正则实现高效对齐，几乎无需额外代价即可显著提升图像与文图生成质量，具备快速适配现有AR生成器的实用价值。

Abstract: Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.

</details>


### [167] [From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs](https://arxiv.org/abs/2512.19683)
*Mingrui Wu,Zhaozhi Wang,Fangjinhua Wang,Jiaolong Yang,Marc Pollefeys,Tong Zhang*

Main category: cs.CV

TL;DR: 提出一个基于户外行人视角多传感器视频的大规模基准，用以系统评测与诊断MLLM的空间智能，发现其在开放世界中显著退化并依赖语言先验。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在语义任务上强但空间智能不足；现有基准要么只做简单定性关系推理，要么局限室内且缺少可验证的户外度量真值，无法深入诊断模型在真实开放场景中的空间推理能力。

Method: 构建包含同步双目、LiDAR、IMU/GPS的行人视角视频数据集，具有精确的三维度量真值；基于该数据自动生成分层次的空间推理问题，覆盖从定性关系到定量度量与运动学理解；在该基准上系统评测现有MLLM，并设计合成异常场景与遮蔽（blinding）测试以剖析模型依赖。

Result: 在室内结构化基准上观察到的性能提升在开放世界户外设置中消失；通过异常场景与遮蔽实验表明，当前MLLM高度依赖语言先验而非扎根的视觉推理。

Conclusion: 该基准为诊断与推进具物理扎根的空间智能提供了系统平台，揭示现有MLLM在开放世界空间推理上的显著不足，需要面向度量精确与运动学理解的训练与评测。

Abstract: While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.

</details>


### [168] [Zero-shot Reconstruction of In-Scene Object Manipulation from Video](https://arxiv.org/abs/2512.19684)
*Dixuan Lin,Tianyou Wang,Zhuoyang Pan,Yufu Wang,Lingjie Liu,Kostas Daniilidis*

Main category: cs.CV

TL;DR: 首个从单目RGB视频重建场景内“手-物体”操控的系统：用基础模型初始化场景/手/物体，再通过两阶段优化，在全局场景坐标中复原从抓取到交互的完整物体与手的动态，保证物理与度量一致。


<details>
  <summary>Details</summary>
Motivation: 以往方法多在手的局部坐标系中重建，忽略场景，导致度量尺度不准、手物深度歧义与物理不合理互动；需要能在真实场景坐标下重建手—物体交互，并与视频中场景观测一致。

Method: 1) 基于数据驱动的基础模型初始化：物体网格与姿态、场景点云、手部姿态；2) 两阶段优化：先对抓取阶段进行稳健配准与物理一致约束，再对交互阶段联合优化手—物体—场景，使手物接触、无穿透、动力学与场景观测一致；全程在场景坐标系中进行以确保度量尺度。

Result: 在单目视频上重建出完整的手—物体运动轨迹与物体网格/位姿，场景对齐良好、度量精度与物理可行性优于现有手心坐标系方法。

Conclusion: 通过基础模型初始化+两阶段全局优化，可从单目视频在场景坐标中可靠地重建物理一致的手—物体操控过程，克服深度歧义与场景忽略带来的度量误差。

Abstract: We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.

</details>


### [169] [Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models](https://arxiv.org/abs/2512.19686)
*Zixuan Ye,Quande Liu,Cong Wei,Yuanxing Zhang,Xintao Wang,Pengfei Wan,Kun Gai,Wenhan Luo*

Main category: cs.CV

TL;DR: 提出将“视觉上下文一致性”引入多模态统一模型的思维链，结合视觉清单规划与迭代自我校正，并配合SFT与flow-GRPO奖励，显著提升多参考生成的身份/属性/风格保持。


<details>
  <summary>Details</summary>
Motivation: 现有CoT多聚焦文本一致性，忽略与参考图像的视觉一致性，导致多参考生成时关键视觉要素（人物ID、属性、风格）难以保持，需要一种在推理过程中显式约束视觉一致性的机制。

Method: 在推理中显式建模视觉一致性：1) 自适应视觉规划，生成结构化的“视觉检查清单”，列出需保持的一致性要素；2) 迭代视觉纠正，基于清单进行自我反思与结果精修。训练上：用SFT教会模型如何规划—反思—精修；再用flow-GRPO配合自定义视觉检查奖励，进一步强化一致性。

Result: 在多模态生成（特别是多参考生成）上，优于零样本统一模型与仅有文本CoT的模型，在视觉上下文一致性指标上取得更高分，并在身份/属性/风格保持方面表现更稳健。

Conclusion: 将视觉一致性显式融入思维与生成回路，通过规划-反思-纠正和奖励优化，可系统提升多模态统一模型在多参考场景的视觉一致性与生成质量。

Abstract: Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.

</details>


### [170] [Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models](https://arxiv.org/abs/2512.19692)
*Pablo Ruiz-Ponce,Sergio Escalera,José García-Rodríguez,Jiankang Deng,Rolandos Alexandros Potamias*

Main category: cs.CV

TL;DR: Interact2Ar 提出首个端到端、文本条件的自回归扩散模型，可生成含手部细节的全身多人体交互动作，并在评测与应用上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略手部动作且一次性生成整段序列，难以捕捉交互中的反应与适应；数据稀缺与学习复杂度高进一步限制了真实感与表达力。

Method: 提出 Interact2Ar：1) 文本条件的自回归扩散框架，按时间块逐步生成，支持大上下文记忆以适应互动变化；2) 并行的手部运动分支，显式建模手部运动学，提升全身细节；3) 新的记忆机制用于长时间依赖与多轮反应；4) 设计面向全身交互的稳健评测器与扩展指标。

Result: 在定量与定性实验中优于现有方法，生成更高保真度的全身（含手部）人-人交互；模型展示出在时间拼接、实时扰动自适应、从双人扩展到多人等任务上的强泛化与可用性。

Conclusion: 自回归+手部专门分支+记忆机制的设计，使模型能捕捉交互中的反应性与适应性，实现高质量、可扩展的全身交互生成，并建立了更合适的评测体系。

Abstract: Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.

</details>


### [171] [The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding](https://arxiv.org/abs/2512.19693)
*Weichen Fan,Haiwen Diao,Quan Wang,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出“棱镜假设”：不同模态编码器的特征频谱与其功能对应；据此设计统一自编码(UAE)，通过频带调制统一语义低频与像素高频，在ImageNet与MS-COCO上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有语义编码器与像素编码器各擅胜场，缺乏统一视角与模型将抽象语义与细节保真在同一潜空间中兼容；对编码器行为缺少频谱层面的系统解释。

Method: 1) 频谱分析：系统比较多种语义与像素编码器的特征频谱分布，提出“棱镜假设”——语义编码器偏低频，像素编码器兼具高频。2) 统一自编码(UAE)：在潜空间加入频带调制模块，显式分配/融合不同频段表征，使语义结构与像素细节在同一潜空间中共存。

Result: 在ImageNet与MS-COCO上进行大量实验，UAE在同时保持语义抽象与像素级保真的任务中取得SOTA表现，验证频带统一策略有效。

Conclusion: 编码器功能与特征频谱存在稳定对应关系（棱镜假设）；基于此的UAE通过频带调制统一多模态低/高频信息，在多个基准上达到最优，提供理解与建模跨模态表示的新框架。

Abstract: Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.

</details>
