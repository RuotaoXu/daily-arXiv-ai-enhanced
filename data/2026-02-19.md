<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 46]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Egocentric Bias in Vision-Language Models](https://arxiv.org/abs/2602.15892)
*Maijunxian Wang,Yijiang Li,Bingyang Wang,Tianwei Zhao,Ran Ji,Qingying Gao,Emmy Liu,Hokin Deng,Dezhi Luo*

Main category: cs.CV

TL;DR: FlipSet是一个用于评估多模态模型Level-2视觉观点采择的基准：让模型从他者视角对二维字符串进行180度旋转判断。对103个VLM测试发现显著“自我中心偏差”，多数低于随机；错误多为复述相机视角。尽管模型单独在心智理论与心智旋转任务上表现不错，但在需要整合时崩溃，显示当前VLM缺乏将社会认知与空间操作绑定的机制。FlipSet为诊断多模态系统观点采择能力提供了认知学扎根的测台。


<details>
  <summary>Details</summary>
Motivation: 视觉观点采择（特别是L2：判断他者眼中所见的外观）是社会认知的核心，但现有VLM是否具备从他者视角进行空间变换尚不清楚。以往评测往往混杂3D场景复杂度，难以纯粹度量从他者视角进行180度翻转的能力。作者希望用一个可控、可诊断的任务揭示模型在社会视角与空间变换整合上的潜在缺陷。

Method: 提出FlipSet：构造简化的2D字符串图像与代理视角设定，要求模型从代理视角对字符串进行180°旋转（L2 VPT），以隔离空间变换因素。评测103个VLM，并设置对照实验：分别测试理论心智（ToM）相关理解与单独的心智旋转能力，再测试两者整合的FlipSet任务，以分析成分性与整合能力。统计错误类型，特别是是否复述相机视角以检验自我中心偏差。

Result: 大多数模型在FlipSet上低于随机水平；约四分之三的错误直接复述相机视角，显示强烈自我中心偏差。对照实验中，模型在ToM与独立心智旋转任务上可达高分或显著高于随机，但在需要将他者视角与空间变换结合时表现灾难性失效，出现明显的成分性缺失。

Conclusion: 当前VLM普遍缺乏将社会意识（他者视角）与空间操作（旋转/心智变换）进行绑定的机制，暴露出模型化空间推理的根本局限。FlipSet提供了一个认知动机充分、可诊断且可复现实验的平台，可用于未来改进多模态系统的观点采择与成分性推理能力。

Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D character strings from another agent's perspective, isolating spatial transformation from 3D scene complexity. Evaluating 103 VLMs reveals systematic egocentric bias: the vast majority perform below chance, with roughly three-quarters of errors reproducing the camera viewpoint. Control experiments expose a compositional deficit--models achieve high theory-of-mind accuracy and above-chance mental rotation in isolation, yet fail catastrophically when integration is required. This dissociation indicates that current VLMs lack the mechanisms needed to bind social awareness to spatial operations, suggesting fundamental limitations in model-based spatial reasoning. FlipSet provides a cognitively grounded testbed for diagnosing perspective-taking capabilities in multimodal systems.

</details>


### [2] [Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment](https://arxiv.org/abs/2602.15903)
*Jingwei Li,Jiaxin Tong,Pengfei Wu*

Main category: cs.CV

TL;DR: 提出MSBA-CLIP框架，用CLIP对齐与多变量软融合增广结合，并配合伪造强度估计模块，显著提升深度伪造检测的准确性与跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测在不同伪造方法与数据集间分布差异大，导致准确率与泛化性不足。需要一种能捕获细微伪造痕迹并在多伪造分布下稳健学习的策略。

Method: 1) 使用CLIP的多模态对齐能力，引导模型关注语义相关且细粒度的伪造痕迹；2) 提出多变量软融合增广（MSBA），将来自多种伪造方法的图像以随机权重进行软融合，合成含多模式伪造特征的训练样本；3) 设计多变量伪造强度估计（MFIE）模块，显式回归或分类不同伪造模式与强度，作为辅助监督信号；4) 端到端训练，提高检测器对不同伪造分布与强度的鲁棒性。

Result: 在域内测试上，较最佳基线Accuracy与AUC分别提升3.32%与4.02%；在五个跨域数据集上平均AUC提升3.27%。消融实验验证MSBA与MFIE均有贡献。

Conclusion: MSBA-CLIP有效缓解分布偏移，显著提升深伪检测的准确性与跨域泛化；代价是依赖大型视觉-语言模型带来更高计算开销。

Abstract: The proliferation of highly realistic facial forgeries necessitates robust detection methods. However, existing approaches often suffer from limited accuracy and poor generalization due to significant distribution shifts among samples generated by diverse forgery techniques. To address these challenges, we propose a novel Multivariate and Soft Blending Augmentation with CLIP-guided Forgery Intensity Estimation (MSBA-CLIP) framework. Our method leverages the multimodal alignment capabilities of CLIP to capture subtle forgery traces. We introduce a Multivariate and Soft Blending Augmentation (MSBA) strategy that synthesizes images by blending forgeries from multiple methods with random weights, forcing the model to learn generalizable patterns. Furthermore, a dedicated Multivariate Forgery Intensity Estimation (MFIE) module is designed to explicitly guide the model in learning features related to varied forgery modes and intensities. Extensive experiments demonstrate state-of-the-art performance. On in-domain tests, our method improves Accuracy and AUC by 3.32\% and 4.02\%, respectively, over the best baseline. In cross-domain evaluations across five datasets, it achieves an average AUC gain of 3.27\%. Ablation studies confirm the efficacy of both proposed components. While the reliance on a large vision-language model entails higher computational cost, our work presents a significant step towards more generalizable and robust deepfake detection.

</details>


### [3] [A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving](https://arxiv.org/abs/2602.15904)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: 本文综述LiDAR超分辨率在自动驾驶中的方法与实践，系统归纳四类技术路线、数据与评测基石、趋势与挑战，旨在推动从低成本稀疏点云到高质量重建的跨传感器落地。


<details>
  <summary>Details</summary>
Motivation: 高分辨率LiDAR价格高、低分辨率点云稀疏且缺细节，影响感知与部署成本；业界缺乏系统综述以指导算法选择、数据表征、评测与工程落地，亟需梳理方法谱系与实践要点。

Method: 以综述形式：1) 按技术范式分类四大路线——CNN、模型驱动深度展开、隐式表示、Transformer/Mamba；2) 统一问题定义与数据表征（如range image等）；3) 汇总数据集与指标；4) 提炼工程趋势（实时、压缩、分辨率自适应、跨传感器泛化）与对比分析。

Result: 形成首个针对自动驾驶场景的LiDAR超分辨率系统化框架与分类图谱，明确主流表示、训练与评测规范，梳理当前主流与前沿方向及其优劣与应用情境。

Conclusion: LiDAR超分辨率正向高效表示、极致压缩、实时推理与跨传感器泛化演进；仍存在数据分布偏移、真实噪声建模不足、标注与评测缺口、部署资源受限等挑战，需在方法、数据与系统协同推进。

Abstract: LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.

</details>


### [4] [MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2602.15915)
*Xianwei Mao,Kai Ye,Sheng Zhou,Nan Zhang,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.CV

TL;DR: 提出MaS-VQA：以“掩码-选择”机制过滤图像与外部知识噪声，并在受限语义空间引导模型内部知识激活，从而提升KB-VQA准确率。


<details>
  <summary>Details</summary>
Motivation: KB-VQA需结合视觉与外部知识，但检索到的知识常含噪声或与图像不对齐，模型内部知识又难以控制与解释。简单聚合多源知识会稀释有效信号、削弱推理与答案准确性。

Method: 1) 先检索候选文本知识。2) 设计Mask-and-Select机制：联合裁剪/抑制无关图像区域与弱相关知识片段，得到紧凑高信号的多模态知识表示。3) 利用该“净化”的显式知识在受限语义空间中约束并引导内部（隐式）知识的激活，实现显式与隐式知识的互补联合建模，最终进行答案预测。

Result: 在Encyclopedic-VQA与InfoSeek上，基于多种MLLM骨干取得一致性能提升；消融实验证明选择机制能有效降噪并提升知识利用率。

Conclusion: 通过选择驱动的显式过滤与隐式知识受控激活，MaS-VQA显著增强了KB-VQA中的多源知识对齐与推理鲁棒性，提高了答案准确率。

Abstract: Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to control and interpret. Naive aggregation of these sources limits reasoning effectiveness and reduces answer accuracy. To address this, we propose MaS-VQA, a selection-driven framework that tightly couples explicit knowledge filtering with implicit knowledge reasoning. MaS-VQA first retrieves candidate passages and applies a Mask-and-Select mechanism to jointly prune irrelevant image regions and weakly relevant knowledge fragments, producing compact, high-signal multimodal knowledge . This filtered knowledge then guides the activation of internal knowledge in a constrained semantic space, enabling complementary co-modeling of explicit and implicit knowledge for robust answer prediction. Experiments on Encyclopedic-VQA and InfoSeek demonstrate consistent performance gains across multiple MLLM backbones, and ablations verify that the selection mechanism effectively reduces noise and enhances knowledge utilization.

</details>


### [5] [EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery](https://arxiv.org/abs/2602.15918)
*Zelin Xu,Yupu Zhang,Saugat Adhikari,Saiful Islam,Tingsong Xiao,Zibo Liu,Shigang Chen,Da Yan,Zhe Jiang*

Main category: cs.CV

TL;DR: 提出EarthSpatialBench基准，用于评估MLLM在地球影像上的空间推理（距离、方向、拓扑与复杂几何），含32.5万+多样化QA；实验揭示现有开源与商用模型在定量与拓扑推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有地球影像基准多局限于2D定位、描述与粗粒度空间关系，缺乏对定量距离/方向、系统化拓扑关系与复杂几何（多边形、折线）的评测；而具备精确空间推理对具身智能与需要与物理世界交互的智能体至关重要。

Method: 构建EarthSpatialBench：在地球遥感/地理影像上生成/整理超32.5万问答，覆盖（1）定性与定量的距离与方向推理；（2）系统化拓扑关系；（3）单体、成对与组合群体查询；（4）多种目标指代方式：文本、可视化覆盖层、以及显式几何（2D框、折线、多边形）。对多种开源与闭源MLLM进行基准测试。

Result: 基于EarthSpatialBench的系统评测显示，当前MLLM在地球影像上的空间推理能力存在显著短板，尤其在定量方向/距离推断、复杂拓扑关系理解与处理复杂几何对象方面表现不佳。

Conclusion: EarthSpatialBench填补了地球影像空间推理评测空白，提供大规模、细粒度、多模态与多几何类型的统一基准，可用于诊断与推动MLLM在真实地理场景中的空间推理能力提升。

Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.

</details>


### [6] [A Study on Real-time Object Detection using Deep Learning](https://arxiv.org/abs/2602.15926)
*Ankita Bose,Jayasravani Bhumireddy,Naveen N*

Main category: cs.CV

TL;DR: 综述型论文：回顾并比较深度学习目标检测在实时场景中的方法、数据集与应用，给出对比实验与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实时目标检测在安防、交通、工业、医疗、AR/VR 等场景需求强烈；视觉数据激增与即时决策需求推动更高精度/速度的检测算法与系统化评估。

Method: 系统性文献综述与对比：汇总主流检测器（Faster/Mask/Cascade R-CNN、YOLO、SSD、RetinaNet 等）、公开基准数据集与应用实践；开展受控实验（对比多种策略）以量化性能差异。

Result: 总结各类模型的特点与适用场景，基于基准与受控实验给出性能对比与启示性发现，揭示在精度、速度、鲁棒性上的权衡。

Conclusion: 指出当前挑战与未来方向（如更优实时性与精度平衡、更强鲁棒性与泛化、数据与标注问题、方法改进），为后续研究与应用提供参考。

Abstract: Object detection has compelling applications over a range of domains, including human-computer interfaces, security and video surveillance, navigation and road traffic monitoring, transportation systems, industrial automation healthcare, the world of Augmented Reality (AR) and Virtual Reality (VR), environment monitoring and activity identification. Applications of real time object detection in all these areas provide dynamic analysis of the visual information that helps in immediate decision making. Furthermore, advanced deep learning algorithms leverage the progress in the field of object detection providing more accurate and efficient solutions. There are some outstanding deep learning algorithms for object detection which includes, Faster R CNN(Region-based Convolutional Neural Network),Mask R-CNN, Cascade R-CNN, YOLO (You Only Look Once), SSD (Single Shot Multibox Detector), RetinaNet etc. This article goes into great detail on how deep learning algorithms are used to enhance real time object recognition. It provides information on the different object detection models available, open benchmark datasets, and studies on the use of object detection models in a range of applications. Additionally, controlled studies are provided to compare various strategies and produce some illuminating findings. Last but not least, a number of encouraging challenges and approaches are offered as suggestions for further investigation in both relevant deep learning approaches and object recognition.

</details>


### [7] [Visual Memory Injection Attacks for Multi-Turn Conversations](https://arxiv.org/abs/2602.15927)
*Christian Schlarmann,Matthias Hein*

Main category: cs.CV

TL;DR: 论文提出“视觉记忆注入”(VMI)攻击：在图像中嵌入隐蔽扰动，使LVLM在日常对话中正常，但一旦用户给出触发提示，模型在多轮长上下文后仍会输出攻击者指定的信息。


<details>
  <summary>Details</summary>
Motivation: 现有对LVLM安全研究多聚焦单轮对话与文本触发，忽视现实中用户会在多轮长上下文下与模型交互，且可能从网络下载被操纵图像。作者想验证：是否能用隐蔽图像扰动在不破坏正常表现的前提下，长时保留“恶意记忆”，待触发词出现时才释放，从而实现大规模操纵。

Method: 设计VMI：在图像中植入对抗式信号，使模型在多轮对话的上下文记忆中保留隐藏指令。优化目标兼顾三点：1) 正常性——常规提示下输出与原图一致；2) 记忆持久——多轮对话后触发依然有效；3) 目标性——在触发提示出现时输出预定消息（如广告/政治口号）。在多种开源权重LVLM上实现与评测。

Result: VMI在多个开源LVLM上成功：在正常查询下模型表现几乎不变，但在长多轮对话后给出触发提示时，模型以高成功率输出攻击者设定的消息，优于以往仅限单轮的攻击。

Conclusion: 被操纵图像可在多轮会话中稳定植入“隐性记忆”并实现定向操纵，表明LVLM存在新的长期上下文安全风险。作者呼吁提升鲁棒性并开源了代码以促进研究。

Abstract: Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection

</details>


### [8] [Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families](https://arxiv.org/abs/2602.15950)
*Yuval Levental*

Main category: cs.CV

TL;DR: 论文通过简单实验揭示VLM在非文本视觉元素的空间定位上存在根本缺陷：对无文本标识的二值网格中“填充格”的定位显著失准。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在多模态任务上表现强劲，但其在细粒度空间定位上的真实能力与对文本的“读取”能力是否一致仍不清楚。作者怀疑VLM依赖高保真文本识别路径进行空间推理，而对纯视觉形状的定位能力较弱，因而设计实验验证两种输入形式下的差距。

Method: 生成15个15x15的二值网格（填充密度10.7%-41.8%），并以两种图像渲染：1) 文本符号图像（. 与 #）；2) 无网格线的实心方块。让三款前沿VLM（Claude Opus、ChatGPT 5.2、Gemini 3 Thinking）从图像转写为网格文本。比较两种条件下的格子级准确率与F1，并分析错误模式。关键控制：两种条件均经相同视觉编码器输入（文本符号也是图像）。

Result: 文本符号条件：Claude与ChatGPT约91%准确率、84% F1；Gemini 84%准确率、63% F1。实心方块条件：三者均降至60–73%准确率、29–39% F1。文本vs方块的F1差距为34–54点，显示对“图中之字”的识别与定位显著优于对纯形状的定位。每个模型在方块条件下有不同失败模式：Claude系统性低估、ChatGPT大幅高估、Gemini模板幻觉。

Conclusion: 当前VLM似乎拥有高保真“文本识别-驱动”的空间推理通道，但其原生视觉通道对非文本元素的空间定位显著退化，导致对无语义标签的形状定位失败。改进方向包括：加强非文本局部特征与全局拓扑的对齐学习、引入密集监督（如像素/格级掩膜）、减少对OCR样式线索的依赖，以及在训练与评测中纳入无语义形状定位基准。

Abstract: We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.

</details>


### [9] [Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration](https://arxiv.org/abs/2602.15959)
*Yiwen Wang,Jiahao Qin*

Main category: cs.CV

TL;DR: 提出GPEReg-Net，在双向光学分辨率光声显微（OR-PAM）中同时处理域偏移与几何错配，通过外观-场景解耦与全局位置编码，进行直接图像到图像的时序一致配准，超越现有方法（SSIM↑3.8%，PSNR↑1.99 dB）。


<details>
  <summary>Details</summary>
Motivation: 双向栅格扫描能把OR-PAM成像速度翻倍，但正反向扫描之间同时存在域偏移（外观差异）与几何错配，传统基于亮度恒常的配准方法难以兼顾，生成式方法又复杂且缺乏跨帧时序意识，需要一种既能处理域差异又能利用时间上下文的高效配准框架。

Method: 提出GPEReg-Net：1) 以AdaIN实现场景-外观解耦，提取域不变的场景特征与域特定外观编码，从而在不显式估计形变场的情况下进行直接图像到图像配准；2) 设计Global Position Encoding（GPE）模块，将可学习位置嵌入与正弦位置编码结合，并引入跨帧注意力，利用相邻帧的上下文增强时序一致性与稳健性。

Result: 在OR-PAM-Reg-4K基准（432个测试样本）上取得NCC=0.953、SSIM=0.932、PSNR=34.49 dB，相比SOTA，SSIM提高3.8%，PSNR提升1.99 dB，NCC保持有竞争力。

Conclusion: 通过外观-场景解耦与全局位置编码的时序建模，GPEReg-Net能在存在域偏移与几何错配的双向扫描OR-PAM中实现高质量、时序一致的无形变场配准，性能超越现有方法且代码开源。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at https://github.com/JiahaoQin/GPEReg-Net.

</details>


### [10] [Automated Re-Identification of Holstein-Friesian Cattle in Dense Crowds](https://arxiv.org/abs/2602.15962)
*Phoenix Yu,Tilo Burghardt,Andrew W Dowsey,Neill W Campbell*

Main category: cs.CV

TL;DR: 提出一种面向拥挤场景的“检测-分割-识别”流水线，结合开放词汇定位(无权重)与SAM进行预处理，再接Re-ID网络，在密集牛群中显著提升检测与重识别表现，达98.93%检测准确率与94.82% Re-ID准确率，远超现有基线；提供代码与数据集。


<details>
  <summary>Details</summary>
Motivation: 现有针对荷斯坦—弗里斯牛(Holstein-Friesian)的检测/重识别在个体间距较大时有效，但在牛群密集、花斑打破外轮廓时，YOLO等基于边界框的方法易失效；需要一种对密集、外观碎片化目标更稳健、可迁移的方案。

Method: 构建新型detect-segment-identify流水线：1) 采用Open-Vocabulary Weight-free Localisation进行开放词汇、无权重的初始定位；2) 使用Segment Anything Model对候选进行高质量分割；3) 基于分割掩膜的Re-ID网络完成个体识别；并发布9天的奶牛场CCTV数据用于评测；还探索无监督对比学习进一步提升Re-ID。

Result: 在密集牛群下避免检测崩溃，整体检测准确率98.93%；相较定向边界框与SAM物种检测基线，准确率分别提升47.52%与27.13%；无监督对比学习的Re-ID在测试集达94.82%准确率。

Conclusion: 通过分割驱动的开放词汇定位+SAM预处理，再接Re-ID，可在真实农场拥挤场景中稳定、无需人工干预地实现牛只检测与重识别，并显著超越主流边界框与SAM检测基线；代码与数据发布，具复现性与迁移潜力。

Abstract: Holstein-Friesian detection and re-identification (Re-ID) methods capture individuals well when targets are spatially separate. However, existing approaches, including YOLO-based species detection, break down when cows group closely together. This is particularly prevalent for species which have outline-breaking coat patterns. To boost both effectiveness and transferability in this setting, we propose a new detect-segment-identify pipeline that leverages the Open-Vocabulary Weight-free Localisation and the Segment Anything models as pre-processing stages alongside Re-ID networks. To evaluate our approach, we publish a collection of nine days CCTV data filmed on a working dairy farm. Our methodology overcomes detection breakdown in dense animal groupings, resulting in a 98.93% accuracy. This significantly outperforms current oriented bounding box-driven, as well as SAM species detection baselines with accuracy improvements of 47.52% and 27.13%, respectively. We show that unsupervised contrastive learning can build on this to yield 94.82% Re-ID accuracy on our test data. Our work demonstrates that Re-ID in crowded scenarios is both practical as well as reliable in working farm settings with no manual intervention. Code and dataset are provided for reproducibility.

</details>


### [11] [Non-Contact Physiological Monitoring in Pediatric Intensive Care Units via Adaptive Masking and Self-Supervised Learning](https://arxiv.org/abs/2602.15967)
*Mohamed Khalil Ben Salah,Philippe Jouvet,Rita Noumeir*

Main category: cs.CV

TL;DR: 提出一种针对PICU场景的自监督rPPG预训练框架，通过课程式训练与自适应掩码（Mamba控制器）提升在临床复杂环境下的心率估计稳健性；结合教师-学生蒸馏与三阶段数据（公开、合成遮挡、500例无标注临床）实现3.2 bpm MAE，较MAE与PhysFormer分别提升42%与31%。


<details>
  <summary>Details</summary>
Motivation: PICU需要连续、无创生命体征监测，但接触式传感器会引起皮肤刺激、感染风险与不适；现有rPPG在临床中受运动、遮挡、光照变化与域偏移限制，缺乏标注临床数据导致泛化不足。

Method: 提出基于VisionMamba的自监督预训练框架：1) 渐进式课程学习，从干净公开视频→合成遮挡→无标注临床视频；2) 自适应掩码机制：轻量Mamba控制器对时空patch打分并概率采样，逐步增加重建难度且保留生理相关区域；3) 教师-学生蒸馏：教师为在公共数据上监督训练的专家模型，提供潜在生理信号指导学生；无需显式ROI提取。

Result: 在PICU心率估计上，相对标准MAE式自编码器降低42% MAE；较PhysFormer提升31%；最终MAE=3.2 bpm；注意力自发聚焦脉搏丰富区域，并在临床遮挡与噪声下保持稳健。

Conclusion: 自监督+课程式训练结合Mamba自适应掩码与跨域蒸馏，可有效缓解实验室到临床的域迁移与遮挡问题，在PICU实现高精度、低接触成本的rPPG心率监测，具备临床部署潜力。

Abstract: Continuous monitoring of vital signs in Pediatric Intensive Care Units (PICUs) is essential for early detection of clinical deterioration and effective clinical decision-making. However, contact-based sensors such as pulse oximeters may cause skin irritation, increase infection risk, and lead to patient discomfort. Remote photoplethysmography (rPPG) offers a contactless alternative to monitor heart rate using facial video, but remains underutilized in PICUs due to motion artifacts, occlusions, variable lighting, and domain shifts between laboratory and clinical data.
  We introduce a self-supervised pretraining framework for rPPG estimation in the PICU setting, based on a progressive curriculum strategy. The approach leverages the VisionMamba architecture and integrates an adaptive masking mechanism, where a lightweight Mamba-based controller assigns spatiotemporal importance scores to guide probabilistic patch sampling. This strategy dynamically increases reconstruction difficulty while preserving physiological relevance.
  To address the lack of labeled clinical data, we adopt a teacher-student distillation setup. A supervised expert model, trained on public datasets, provides latent physiological guidance to the student. The curriculum progresses through three stages: clean public videos, synthetic occlusion scenarios, and unlabeled videos from 500 pediatric patients.
  Our framework achieves a 42% reduction in mean absolute error relative to standard masked autoencoders and outperforms PhysFormer by 31%, reaching a final MAE of 3.2 bpm. Without explicit region-of-interest extraction, the model consistently attends to pulse-rich areas and demonstrates robustness under clinical occlusions and noise.

</details>


### [12] [LAND: A Longitudinal Analysis of Neuromorphic Datasets](https://arxiv.org/abs/2602.15973)
*Gregory Cohen,Alexandre Marcireau*

Main category: cs.CV

TL;DR: 该综述盘点423+个神经形态数据集，揭示数据规模、标准化与可获取性问题，以及合成数据崛起带来的利弊，并提出“元数据集”思路以缓解数据短缺与偏置。


<details>
  <summary>Details</summary>
Motivation: 尽管神经形态数据集数量迅速增长，研究仍反复呼吁“需要更多、更大数据”。驱动因素包括深度学习的数据需求，以及现有数据集分散、任务定义不清、获取与使用困难，导致研究复现与拓展受阻。

Method: 系统性梳理并汇编超过423个神经形态数据集；从任务类型与数据结构进行分类与对比；分析数据集规模演化、标准化缺失、访问与使用障碍；重点审视模拟与视频转事件生成的合成数据，并引入“元数据集”（由现有数据集重组）概念。

Result: 发现主要问题：1) 数据集体量不断攀升，但缺乏统一格式与文档；2) 下载与运行门槛高，影响可重现性；3) 合成数据占比上升，便于基准测试却可能误导新应用探索；4) 元数据集可在不新增采集成本下扩充任务多样性并缓解偏置。

Conclusion: 神经形态研究面临数据发现、标准化与可用性瓶颈；需推进数据格式与基准协议、改进数据托管与工具链；谨慎使用合成数据，明确其适用边界；推动基于现有数据构建“元数据集”，以提升任务多样性、降低偏置与数据采集负担。

Abstract: Neuromorphic engineering has a data problem. Despite the meteoric rise in the number of neuromorphic datasets published over the past ten years, the conclusion of a significant portion of neuromorphic research papers still states that there is a need for yet more data and even larger datasets. Whilst this need is driven in part by the sheer volume of data required by modern deep learning approaches, it is also fuelled by the current state of the available neuromorphic datasets and the difficulties in finding them, understanding their purpose, and determining the nature of their underlying task. This is further compounded by practical difficulties in downloading and using these datasets. This review starts by capturing a snapshot of the existing neuromorphic datasets, covering over 423 datasets, and then explores the nature of their tasks and the underlying structure of the presented data. Analysing these datasets shows the difficulties arising from their size, the lack of standardisation, and difficulties in accessing the actual data. This paper also highlights the growth in the size of individual datasets and the complexities involved in working with the data. However, a more important concern is the rise of synthetic datasets, created by either simulation or video-to-events methods. This review explores the benefits of simulated data for testing existing algorithms and applications, highlighting the potential pitfalls for exploring new applications of neuromorphic technologies. This review also introduces the concepts of meta-datasets, created from existing datasets, as a way of both reducing the need for more data, and to remove potential bias arising from defining both the dataset and the task.

</details>


### [13] [SAM 3D Body: Robust Full-Body Human Mesh Recovery](https://arxiv.org/abs/2602.15989)
*Xitong Yang,Devansh Kukreja,Don Pinkus,Anushka Sagar,Taosha Fan,Jinhyung Park,Soyong Shin,Jinkun Cao,Jiawei Liu,Nicolas Ugrinovic,Matt Feiszli,Jitendra Malik,Piotr Dollar,Kris Kitani*

Main category: cs.CV

TL;DR: 提出SAM 3D Body (3DB)，一种可提示的单图全身3D人体网格重建模型，基于新的人体参数化表示MHR，结合编码器-解码器与辅助提示（2D关键点、掩膜），在多样野外条件下取得SOTA泛化与精度，并开放3DB与MHR。


<details>
  <summary>Details</summary>
Motivation: 现有HMR方法在野外复杂条件下泛化差，难统一身体/手/脚细粒度姿态；常用SMPL类表示将骨架与表面强耦合，限制灵活性；缺少高质量、多样化标注与按姿态/外观细分的评测基准。

Method: 1) 模型：编码器-解码器架构的3DB，可接收图像及辅助提示（2D关键点、分割掩膜）进行用户引导推理；同时回归身体、手和脚姿态。2) 表示：提出Momentum Human Rig (MHR)，将骨架结构与表面形状解耦的参数化网格。3) 数据：多阶段标注流水线，融合人工关键点、可微优化、多视几何、致密关键点检测，构建高质量注释；数据引擎主动筛选覆盖罕见姿态与成像条件。4) 评测：新建按姿态与外观类别组织的数据集，便于细粒度行为分析。

Result: 在用户偏好主观评测与常规定量指标上，较现有方法取得显著提升；在多样野外场景中表现出更强的泛化与稳定性；支持提示驱动的人体全身（含手足）姿态与网格重建。

Conclusion: 3DB结合可提示推理与MHR表示，实现单图全身3D人体重建的SOTA精度与泛化；数据与评测设计促进对复杂场景的稳健性；3DB与MHR均已开源，便于社区复现与扩展。

Abstract: We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.

</details>


### [14] [BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features](https://arxiv.org/abs/2602.16006)
*Juampablo E. Heras Rivera,Dickson T. Chen,Tianyi Ren,Daniel K. Low,Asma Ben Abacha,Alberto Santamaria-Pang,Mehmet Kurt*

Main category: cs.CV

TL;DR: BTReport提出一种将脑肿瘤影像报告生成分解为“确定性特征提取+LLM报告撰写”的开源框架，并发布与BraTS配套的合成报告数据集；该方法提高可解释性、降低幻觉，并在与临床报告一致性及临床结局预测上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤领域缺乏公开的成对影像-报告数据，限制了基于大规模配对数据的RRG进展；现有方法常用通用/微调的视觉-语言模型同时做理解与写作，易产生幻觉、可解释性差。

Method: 将RRG流程拆分为两步：1）对影像进行确定性（规则/算法驱动）的特征提取，生成结构化影像学特征；2）使用大语言模型仅用于语法组织与叙事格式化，把结构化特征转写为自然语言报告。并验证这些特征对生存期与IDH突变等临床结局的预测能力；将该流程用于生成与BraTS配套的合成报告数据集（BTReport-BraTS）。

Result: 相较现有RRG基线，BTReport生成的报告与临床参考报告更一致；用于生成报告的确定性特征对生存与IDH突变具有预测力；公开代码与一个与BraTS匹配的合成报告数据集。

Conclusion: 通过将影像理解与文本生成解耦，BTReport实现了高可解释、低幻觉的脑肿瘤影像报告生成，并为社区提供可复现代码与合成数据，推动神经肿瘤RRG研究。

Abstract: Recent advances in radiology report generation (RRG) have been driven by large paired image-text datasets; however, progress in neuro-oncology has been limited due to a lack of open paired image-report datasets. Here, we introduce BTReport, an open-source framework for brain tumor RRG that constructs natural language radiology reports using deterministically extracted imaging features. Unlike existing approaches that rely on large general-purpose or fine-tuned vision-language models for both image interpretation and report composition, BTReport performs deterministic feature extraction for image analysis and uses large language models only for syntactic structuring and narrative formatting. By separating RRG into a deterministic feature extraction step and a report generation step, the generated reports are completely interpretable and less prone to hallucinations. We show that the features used for report generation are predictive of key clinical outcomes, including survival and IDH mutation status, and reports generated by BTReport are more closely aligned with reference clinical reports than existing baselines for RRG. Finally, we introduce BTReport-BraTS, a companion dataset that augments BraTS imaging with synthetically generated radiology reports produced with BTReport. Code for this project can be found at  https://github.com/KurtLabUW/BTReport.

</details>


### [15] [MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval](https://arxiv.org/abs/2602.16019)
*Ahmad Elallaf,Yu Zhang,Yuktha Priya Masupalli,Jeong Yang,Young Lee,Zechun Cao,Gongbo Liang*

Main category: cs.CV

TL;DR: MedProbCLIP提出将胸片与放射学报告的多模态对齐从确定性嵌入升级为高斯概率嵌入，通过概率对比学习与信息瓶颈，使检索与零样本分类更准、更稳、更可校准，并在MIMIC-CXR上全面优于CLIP/CXR-CLIP/PCME++等基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言基础模型多采用点估计嵌入，难以表达医用场景中的不确定性与一对多/多对多对应，导致在高风险医疗检索与决策中出现过度自信与可靠性不足的问题。

Method: 提出MedProbCLIP：将图像与文本表示为高斯分布（均值与协方差），以概率对比目标学习跨模态对齐；引入变分信息瓶颈抑制过度自信；训练期使用多视角胸片编码与报告多段落编码提供细粒度监督，以临床一致的对应关系学习，但推理期仅需单张影像与单份报告。

Result: 在MIMIC-CXR上，MedProbCLIP在双向检索与零样本分类上优于确定性与概率基线（如CLIP、CXR-CLIP、PCME++）；并在校准度、风险-覆盖曲线、选择性检索可靠性以及对临床相关扰动的鲁棒性方面表现更佳。

Conclusion: 将不确定性显式建模的概率式视觉-语言框架能提升放射学图文检索与分类的准确性与可信度，为医疗高风险应用中的安全与可解释性提供更可靠的基础。

Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.

</details>


### [16] [LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization](https://arxiv.org/abs/2602.16086)
*Idil Bilge Altun,Mert Onur Cakiroglu,Elham Buxton,Mehmet Dalkilic,Hasan Kurban*

Main category: cs.CV

TL;DR: 提出LGQ，一种端到端可学习几何的离散图像量化器，用软分配+温度控制实现可微训练，低温收敛到最近邻；引入峰度与全局使用正则，兼顾自信与均衡用码；在ImageNet上较FSQ/SimVQ在大词表下显著提升rFID并减少有效码使用。


<details>
  <summary>Details</summary>
Motivation: 现有离散图像分词存在两难：VQ灵活但有直通估计偏差、码本利用不足、词表大时崩塌；结构化/标量量化利用率高但几何固定、对异质潜变量统计分配低效。需要一种既能学习几何、又稳定高利用率的量化器。

Method: 提出Learnable Geometric Quantization：用温度控制的软指派替代硬最近邻，实现端到端可微；指派等价于各向同性GMM的后验责任，优化变分自由能；温度趋零时可证明收敛为最近邻量化。配合两类正则：token级峰度正则促使指派尖锐，自信；全局使用正则促均衡用码、避免刚性网格。采用VQGAN式骨干，在ImageNet多词表规模验证。

Result: 在受控VQGAN骨架与ImageNet上多词表规模实验：优化稳定、用码均衡。16K码本时，相比FSQ，rFID提升11.88%，同时活跃码减少49.96%；相比SimVQ，rFID提升6.06%，有效表示率降低49.45%，以显著更少活跃条目达到相当保真度。

Conclusion: LGQ通过可学习几何与温度软指派+正则，实现稳定训练与高效码本利用，兼具VQ灵活性与结构化量化器的利用率优势，在大词表下显著优于FSQ与SimVQ，具备可扩展视觉生成的潜力。

Abstract: Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics.
  We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids.
  Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: https://github.com/KurbanIntelligenceLab/LGQ

</details>


### [17] [OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis](https://arxiv.org/abs/2602.16110)
*Tianwei Lin,Zhongwei Qiu,Wenqiao Zhang,Jiang Liu,Yihan Xie,Mingjian Gao,Zhenxuan Fan,Zhaocheng Li,Sijing Li,Zhongle Xie,Peng LU,Yueting Zhuang,Yingda Xia,Ling Zhang,Beng Chin Ooi*

Main category: cs.CV

TL;DR: OmniCT是一种统一处理CT切片与体数据的多模态大模型，结合体素一致性建模、器官级语义对齐与全新评测集，在多类临床任务上同时提升微观细节感知与宏观三维推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床解读CT既依赖切片级细节（小结节、边界）也依赖体数据的空间关系（肿瘤浸润、器官关系）。现有LVLM要么擅长切片但缺乏跨切片一致性，要么擅长体积语义但粒度粗、与切片输入不兼容，缺少统一范式，阻碍临床落地。

Method: 提出OmniCT的三大模块：1) 空间一致性增强（SCE）：将体数据视作由切片组成并引入三轴位置编码，融合体一致性；通过MoE混合投影在切片与体输入间高效适配。2) 器官级语义增强（OSE）：利用分割与ROI定位对齐解剖区域，显式强调病灶与器官级语义。3) MedEval-CT数据与评测：构建最大规模的切片-体混合CT数据集与混合基准，提供统一评测指标。

Result: OmniCT在多种临床任务与评测上较现有方法取得显著优势，同时兼顾微观细节与宏观三维推理，展现更强的泛化与一致性。

Conclusion: 通过统一的切片-体积LVLM范式与空间/语义增强，以及新的大规模评测基准，OmniCT为跨模态医学影像理解提供了可临床转化的方向，并设立了新的研究范式。

Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.

</details>


### [18] [CHAI: CacHe Attention Inference for text2video](https://arxiv.org/abs/2602.16132)
*Joel Mathew Cherian,Ashutosh Muralidhara Bharadwaj,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.CV

TL;DR: CHAI 提出“缓存注意力”与跨推理缓存，显著加速文本生成视频扩散模型的推理，在保持画质的同时将去噪步数降至≈8步，整体比 OpenSora 1.2 快约1.65×–3.35×。


<details>
  <summary>Details</summary>
Motivation: 文本到视频扩散模型因3D潜变量的逐步去噪而推理缓慢。现有提速法要么需昂贵的再训练，要么用启发式跳步，步数减少时容易画质劣化。需要一种既降延迟又维持视频质量的方法。

Method: 提出 CHAI：利用跨推理缓存（cross-inference caching）。核心是“缓存注意力”（Cache Attention），在跨样本/跨提示的潜变量之间进行选择性注意，聚焦共享的对象/场景，对语义相关的提示重用已缓存的潜变量，提高缓存命中率，从而减少实际去噪计算步骤（可至约8步）。

Result: 在使用缓存注意力时，可用极少的去噪步数（约8）生成高质量视频。系统整体集成后，相较基线 OpenSora 1.2，达到约1.65×–3.35×的推理加速，同时保持视频质量。

Conclusion: 跨推理缓存与缓存注意力能在不重新训练模型的前提下，大幅降低文本到视频扩散推理延迟，并维持画质，为高效视频生成提供通用、可扩展的加速思路。

Abstract: Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.

</details>


### [19] [IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models](https://arxiv.org/abs/2602.16138)
*Parsa Madinei,Srijita Karmakar,Russell Cohen Hoffing,Felix Gervitz,Miguel P. Eckstein*

Main category: cs.CV

TL;DR: IRIS 利用实时眼动数据在开放式VQA中消解歧义，无需训练，显著提升含歧义问题的回答准确率（35.2%→77.2%），且不损害无歧义问题表现；并发布含眼动的基准、交互协议与评测套件。


<details>
  <summary>Details</summary>
Motivation: 开放式VQA常遇到问题语义含糊、图像多目标等导致的大模型回答不确定性；现有方法多依赖额外训练或启发式提示，难以在实时交互中稳健应用。人类在提问瞬间的注视点可能携带用户意图信号，若可在推理时利用，将有望无侵入地提升消歧与对齐。

Method: 提出IRIS：在推理阶段接入眼动追踪流，捕获与用户开始发声时间最接近的注视（fixations），将其作为意图线索融入到大规模VLM推理管线中（例如通过区域提示、坐标/掩码约束或注意力引导），实现训练无关的歧义消解。开展包含500对图像-问题的人类实验，比较不同时间窗注视的重要性。

Result: 在多种SOTA VLM上，融入注视数据对含歧义样本带来一致提升，准确率从35.2%提升至77.2%，同时对无歧义问题无显著负面影响。关键发现：最接近发问起始时刻的注视点信息量最大。

Conclusion: 眼动在发问瞬间蕴含可被VLM利用的用户意图，推理时引入该信号可无需重新训练即可稳健消歧。论文还提供带眼动的VQA基准、实时交互协议与评估工具，为人机协同与注视引导的多模态推理奠定基础。

Abstract: We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.

</details>


### [20] [Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing](https://arxiv.org/abs/2602.16149)
*Huichan Seo,Minki Hong,Sieun Choi,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: 论文研究指令驱动的图像到图像(I2I)编辑在不同人口属性下的失效差异，提出“软消除”和“刻板替换”两类失败模式，基于受控基准与VLM/人工评测量化，发现身份保持错误普遍且对少数群体更严重；并展示无需改模型、只在提示层加入身份约束即可显著缓解少数群体的身份漂移。


<details>
  <summary>Details</summary>
Motivation: 虽然T2I的群体偏见已被关注，但I2I编辑在相同编辑指令下是否对不同种族/性别/年龄产生系统性差异尚缺系统研究；实践中用户期望在编辑特征时不改变主体身份，但模型可能受隐性社会先验影响而破坏身份一致性。

Method: 1) 定义两种失败：软消除（编辑被弱化/忽略）与刻板替换（引入未请求且与刻板印象一致的属性）。2) 构建受控基准：按种族、性别、年龄生成与编辑人像，配合诊断型提示集合。3) 使用多种开源权重I2I编辑器进行实验；以VLM评分与人工评测衡量编辑执行度、身份保持与刻板特征注入。4) 进一步测试在提示中加入身份约束对不同群体的影响。

Result: 发现身份保持失败广泛存在且在群体间不均：少数群体更易发生身份改变；编辑行为受隐性社会先验（如由职业推断性别）影响。加入提示级身份约束显著减少少数群体的人口学改变，而对多数群体影响较小，揭示编辑器存在非对称身份先验。

Conclusion: I2I编辑的核心失败模式是身份保持不稳且具人口学不均衡性。应开发人口学鲁棒的编辑系统；在缺乏模型改动时，提示级身份约束是实用缓解策略，同时现有编辑器的隐式社会先验与不对称性需在评测与训练中被正视。

Abstract: Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias

</details>


### [21] [Uncertainty-Guided Inference-Time Depth Adaptation for Transformer-Based Visual Tracking](https://arxiv.org/abs/2602.16160)
*Patrick Poggi,Divake Kumar,Theja Tulabandhula,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 提出UncL‑STARK，在不改网络结构的前提下，为Transformer目标跟踪器引入基于不确定性的动态深度自适应，利用随机深度+蒸馏训练，多深度保持鲁棒；推理时用角点热力图估计不确定性，按置信度选择下一帧的编码器/解码器深度，在GOT‑10k与LaSOT上以几乎不降精度实现显著算力、时延与能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer单目标跟踪器每帧都跑固定的全深度编码器‑解码器，即便视频大多帧变化很小，也要付出高计算代价；需要一种既不改架构又能按帧复杂度自适应计算量、且保证精度安全的机制。

Method: 提出UncL‑STARK：1) 训练阶段：对原有跟踪器进行随机深度训练（随机截断编码器/解码器层），并用全深度作为教师进行知识蒸馏，使各中间深度都具备鲁棒预测能力；2) 推理阶段：从角点定位热力图中直接构造轻量不确定性估计，作为反馈信号，基于当前帧预测置信度和视频时间一致性，动态选择下一帧的编码器/解码器层数，实现安全截断；不添加额外头，不改主干结构。

Result: 在GOT‑10k与LaSOT上，相比全深度基线，GFLOPs降低最高12%，端到端时延降低8.9%，能耗降低10.8%，同时跟踪精度平均仅下降约0.2%，在短期与长期序列均保持SOTA级表现。

Conclusion: 基于不确定性的动态深度自适应可在不改架构的前提下为Transformer跟踪器显著节省计算与能耗且几乎无精度损失；随机深度+蒸馏训练使多深度预测变得稳健，利用时间一致性进行深度调度是有效而通用的策略。

Abstract: Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads. The model is fine-tuned to retain predictive robustness at multiple intermediate depths using random-depth training with knowledge distillation, thus enabling safe inference-time truncation. At runtime, we derive a lightweight uncertainty estimate directly from the model's corner localization heatmaps and use it in a feedback-driven policy that selects the encoder and decoder depth for the next frame based on the prediction confidence by exploiting temporal coherence in video. Extensive experiments on GOT-10k and LaSOT demonstrate up to 12\% GFLOPs reduction, 8.9\% latency reduction, and 10.8\% energy savings while maintaining tracking accuracy within 0.2\% of the full-depth baseline across both short-term and long-term sequences.

</details>


### [22] [DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling](https://arxiv.org/abs/2602.16231)
*Yiming Ju,Hanyu Zhao,Quanyue Ma,Donglin Hao,Chengwei Wu,Ming Li,Songjing Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: DataCube 是一个自动化视频处理与检索平台，可将海量原始视频转为结构化语义表示，支持多维画像与混合检索（含神经重排序与深度语义匹配），帮助用户快速构建任务定制的视频子集与可搜索系统。


<details>
  <summary>Details</summary>
Motivation: 海量视频资源虽丰富，但将原始视频转化为高质量、任务特定的数据集过程昂贵低效，缺乏高效的结构化表示与便捷检索手段，阻碍训练、分析与评测。

Method: 搭建智能平台 DataCube：对视频片段进行自动处理与结构化语义建模；提供多维画像（内容、属性等）与基于混合策略的检索框架，结合神经重排序与深度语义匹配；通过交互式网页界面，支持基于查询的子集构建与私有视频库的可搜索部署。

Result: 平台可从海量仓库中高效抽取并组织视频，支持用户按需快速构建训练/分析/评测数据子集，并实现对私有视频集的搜索。系统已公开可访问并提供演示视频。

Conclusion: DataCube 显著降低视频数据集构建与检索的成本，提升定制化数据管线效率，为视频理解与生成任务提供可扩展的基础设施。

Abstract: Large-scale video repositories are increasingly available for modern video understanding and generation tasks. However, transforming raw videos into high-quality, task-specific datasets remains costly and inefficient. We present DataCube, an intelligent platform for automatic video processing, multi-dimensional profiling, and query-driven retrieval. DataCube constructs structured semantic representations of video clips and supports hybrid retrieval with neural re-ranking and deep semantic matching. Through an interactive web interface, users can efficiently construct customized video subsets from massive repositories for training, analysis, and evaluation, and build searchable systems over their own private video collections. The system is publicly accessible at https://datacube.baai.ac.cn/. Demo Video: https://baai-data-cube.ks3-cn-beijing.ksyuncs.com/custom/Adobe%20Express%20-%202%E6%9C%8818%E6%97%A5%20%281%29%281%29%20%281%29.mp4

</details>


### [23] [EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection](https://arxiv.org/abs/2602.16238)
*Hiroki Nakamura,Hiroto Iino,Masashi Okada,Tadahiro Taniguchi*

Main category: cs.CV

TL;DR: 提出EasyControlEdge，将图像生成基础模型改造用于边缘检测，结合边缘定制的损失和无条件引导，在有限数据下生成更清晰、可控稀疏度的边缘图，并在多数据集上优于SOTA，尤其无需后处理的清晰度与小样本场景。


<details>
  <summary>Details</summary>
Motivation: 实际应用（平面图墙体、卫星道路/建筑、医学器官边界）需要“清晰且数据高效”的边缘，但小样本条件下得到清晰原始边缘图很难。现有生成式基础模型具备强先验与迭代细节保真潜力，但在边缘检测上的迁移与高频细节保留尚未充分利用。

Method: 对图像生成基础模型进行“边缘专用”适配：1) 训练时引入面向边缘的目标与高效的像素空间损失，以更好匹配边缘检测任务；2) 推理时基于无条件动态的引导机制，通过可调引导尺度控制边缘密度，实现单模型多密度控制与细节保留的数据高效检测。

Result: 在BSDS500、NYUDv2、BIPED、CubiCasa等数据集上，相比SOTA获得一致提升，尤以无需后处理的边缘清晰度指标和小样本训练设定下优势显著。

Conclusion: 利用生成式基础模型的强先验与可迭代特性，通过边缘定制训练目标与无条件引导推理，实现清晰、可控且数据高效的边缘检测，并在多基准上验证其优越性。

Abstract: We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.

</details>


### [24] [HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis](https://arxiv.org/abs/2602.16245)
*J. Dhar,M. K. Pandey,D. Chakladar,M. Haghighat,A. Alavi,S. Mistry,N. Zaidi*

Main category: cs.CV

TL;DR: 提出HyPCA-Net，用高效残差自适应注意与双视图级联注意并行融合多模态医学影像，在10个数据集上比SOTA最高提升5.2%，计算成本降至73.1%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法计算开销大且常用级联注意堆叠，易在模块切换中信息流失，难以学到稳健的跨模态共享表示，导致在多疾病任务中的泛化受限，且不适合低资源环境。

Method: 设计Hybrid Parallel-Fusion Cascaded Attention Network（HyPCA-Net）：1）残差自适应学习注意块（高效、提炼模态特异表征）；2）双视图级联注意块（从两个互补视角学习跨模态共享表示）；总体采用并行融合与级联注意的混合架构，兼顾效率与鲁棒共享特征建模。

Result: 在10个公开数据集上广泛实验，较现有领先方法性能最高提升5.2%，计算成本最高降低73.1%；代码公开。

Conclusion: HyPCA-Net在保证或提升准确性的同时显著降低计算成本，通过高效模态专属建模与双视图级联共享表征，提升多疾病场景的泛化与实用性。

Abstract: Multimodal fusion frameworks, which integrate diverse medical imaging modalities (e.g., MRI, CT), have shown great potential in applications such as skin cancer detection, dementia diagnosis, and brain tumor prediction. However, existing multimodal fusion methods face significant challenges. First, they often rely on computationally expensive models, limiting their applicability in low-resource environments. Second, they often employ cascaded attention modules, which potentially increase risk of information loss during inter-module transitions and hinder their capacity to effectively capture robust shared representations across modalities. This restricts their generalization in multi-disease analysis tasks. To address these limitations, we propose a Hybrid Parallel-Fusion Cascaded Attention Network (HyPCA-Net), composed of two core novel blocks: (a) a computationally efficient residual adaptive learning attention block for capturing refined modality-specific representations, and (b) a dual-view cascaded attention block aimed at learning robust shared representations across diverse modalities. Extensive experiments on ten publicly available datasets exhibit that HyPCA-Net significantly outperforms existing leading methods, with improvements of up to 5.2% in performance and reductions of up to 73.1% in computational cost. Code: https://github.com/misti1203/HyPCA-Net.

</details>


### [25] [AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards](https://arxiv.org/abs/2602.16249)
*David Smerkous,Zian Wang,Behzad Najafian*

Main category: cs.CV

TL;DR: 提出AFFMAE：一种适配分层结构的、掩码友好的自监督预训练框架，通过仅对可见token进行自适应非网格合并，兼顾MAE的高效性与分层可扩展性；在高分辨率电镜分割上，以相同参数量达成与ViT-MAE相当的精度，同时FLOPs最高降至1/7、显存减半，并在单卡RTX 5090上更快训练。


<details>
  <summary>Details</summary>
Motivation: 现有自监督预训练虽数据高效，但高分辨率训练需大型算力；MAE只编码可见token很高效，却难与典型分层下采样架构直接结合，因其依赖致密网格先验与对掩码感知的结构妥协，限制了很多实验室在特定领域（如高分辨率电镜）的基础模型训练。

Method: 提出AFFMAE：抛弃被掩码token，仅在可见token上进行自适应、离网格（off-grid）的动态合并，去除致密网格假设并保持分层可扩展性；实现数值稳定的混合精度、类Flash的聚类注意力核；通过深度监督缓解稀疏阶段的表示坍塌。

Result: 在高分辨率电镜分割任务中，与相同参数规模的ViT-MAE达到相当性能；计算量最高减少约7倍，显存使用降低约50%，在单张RTX 5090上训练更快。

Conclusion: AFFMAE实现了与MAE兼容的、对分层架构友好的掩码式预训练，在保持精度的同时显著降低计算与内存成本，使中小型实验室也能在高分辨率场景中高效预训练与微调基础模型。

Abstract: Self-supervised pretraining has transformed computer vision by enabling data-efficient fine-tuning, yet high-resolution training typically requires server-scale infrastructure, limiting in-domain foundation model development for many research laboratories. Masked Autoencoders (MAE) reduce computation by encoding only visible tokens, but combining MAE with hierarchical downsampling architectures remains structurally challenging due to dense grid priors and mask-aware design compromises. We introduce AFFMAE, a masking-friendly hierarchical pretraining framework built on adaptive, off-grid token merging. By discarding masked tokens and performing dynamic merging exclusively over visible tokens, AFFMAE removes dense-grid assumptions while preserving hierarchical scalability. We developed numerically stable mixed-precision Flash-style cluster attention kernels, and mitigate sparse-stage representation collapse via deep supervision. On high-resolution electron microscopy segmentation, AFFMAE matches ViT-MAE performance at equal parameter count while reducing FLOPs by up to 7x, halving memory usage, and achieving faster training on a single RTX 5090. Code available at https://github.com/najafian-lab/affmae.

</details>


### [26] [Breaking the Sub-Millimeter Barrier: Eyeframe Acquisition from Color Images](https://arxiv.org/abs/2602.16281)
*Manel Guzmán,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一种基于多视角人工视觉的眼镜框轮廓追踪方法，用普通InVision系统拍摄的彩色图像与深度数据，通过分割、深度估计与多视角融合精确测量镜框轮廓，达到亚毫米级，减少对机械追踪设备与繁琐校准的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统机械式镜框追踪需精密定位与校准，操作耗时、需专用设备，影响验光与配镜流程效率；希望用计算机视觉替代机械装置，在不牺牲精度的前提下简化流程、降低成本。

Method: 构建从图像到3D轮廓的完整流水线：1) 使用InVision系统多视角采集RGB图像；2) 图像分割提取镜框区域；3) 深度估计恢复3D空间信息；4) 将分割后的RGB与深度进行多视角融合，估计精确的镜框边界与轮廓尺寸；并比较多种配置与变体。

Result: 在真实数据上，多种配置得到与现有方案相当的测量精度，能从静态彩色图像（加深度）实现亚毫米级的镜框轮廓测量，无需专用追踪硬件。

Conclusion: 基于多视角视觉与深度融合的镜框追踪可替代传统机械方案，维持竞争性精度的同时简化设备与流程，降低校准与操作复杂度，提升光学门店工作效率。

Abstract: Eyeframe lens tracing is an important process in the optical industry that requires sub-millimeter precision to ensure proper lens fitting and optimal vision correction. Traditional frame tracers rely on mechanical tools that need precise positioning and calibration, which are time-consuming and require additional equipment, creating an inefficient workflow for opticians. This work presents a novel approach based on artificial vision that utilizes multi-view information. The proposed algorithm operates on images captured from an InVision system. The full pipeline includes image acquisition, frame segmentation to isolate the eyeframe from background, depth estimation to obtain 3D spatial information, and multi-view processing that integrates segmented RGB images with depth data for precise frame contour measurement. To this end, different configurations and variants are proposed and analyzed on real data, providing competitive measurements from still color images with respect to other solutions, while eliminating the need for specialized tracing equipment and reducing workflow complexity for optical technicians.

</details>


### [27] [A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks](https://arxiv.org/abs/2602.16322)
*Santiago C. Vilabella,Pablo Pérez-Núñez,Beatriz Remeseiro*

Main category: cs.CV

TL;DR: 论文提出一种自监督特征提取器，在无需标注的情况下预训练，用于目标检测，在少量标注数据下依然超过现有以 ImageNet 预训练和面向检测优化的特征提取器；还能让模型更聚焦于目标的关键部分，因而更稳健可靠。


<details>
  <summary>Details</summary>
Motivation: 深度模型规模增长导致高质量标注数据稀缺且昂贵，尤其目标检测需要大量人工标注。希望通过更强的特征学习能力，减少对标注的依赖，同时提升检测鲁棒性与泛化。

Method: 采用自监督学习策略在大量未标注数据上预训练特征提取器，然后将其作为检测框架的 backbone 进行微调，与现有 SOTA（如基于 ImageNet 监督预训练或专为检测设计的骨干）进行对比评测；并通过注意力/显著性分析检验模型是否更关注目标的关键区域。

Result: 在目标检测任务上，该自监督预训练的特征提取器在样本有限（少标注）设置下取得优于 SOTA 的性能；可视化显示其注意更集中于对象的判别性区域，表明特征更具判别力与鲁棒性。

Conclusion: 通过自监督增强特征提取器可显著缓解标注数据需求，在少标注条件下超越传统 ImageNet 预训练与专用检测骨干，并使模型关注更相关的对象特征，从而提升可靠性与鲁棒性。

Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.

</details>


### [28] [Subtractive Modulative Network with Learnable Periodic Activations](https://arxiv.org/abs/2602.16337)
*Tiou Wang,Zhuoqian Yang,Markus Flierl,Mathieu Salzmann,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出一种受减法合成启发、参数高效的隐式神经表示架构SMN，利用可学习周期激活(振荡器)与调制掩码(滤波器)生成与增强谐波，在图像重建与NeRF任务上以更少参数达到或超过SOTA的PSNR表现。


<details>
  <summary>Details</summary>
Motivation: 现有INR往往依赖大量参数或固定傅里叶特征，难以同时兼顾参数效率、谐波表达能力与可解释的信号处理流程。作者受经典音频减法合成启发，期望以结构化的信号管线在小模型内高效产生多频/高阶谐波，从而提升重建质量与泛化。

Method: 设计Subtractive Modulative Network：1) 可学习的周期激活层(“Oscillator”)产生多频基；2) 一系列调制掩码模块(“Filters”)对频率分量进行选择与放大，主动生成高阶谐波；3) 提供理论分析说明其能以较少参数覆盖丰富频谱，并给出实现与训练细节；4) 在图像INR与NeRF上评测。

Result: 在两个图像数据集上达到40+dB的PSNR，参数更少且重建精度优于或可比当前SOTA；在3D NeRF新视角合成任务上也显示出一致优势。

Conclusion: 将可解释的减法合成思想引入INR，通过可学习振荡与调制滤波有效扩大谐波表达，兼顾参数效率与重建精度；方法通用，可扩展到图像与3D场景表示。

Abstract: We propose the Subtractive Modulative Network (SMN), a novel, parameter-efficient Implicit Neural Representation (INR) architecture inspired by classical subtractive synthesis. The SMN is designed as a principled signal processing pipeline, featuring a learnable periodic activation layer (Oscillator) that generates a multi-frequency basis, and a series of modulative mask modules (Filters) that actively generate high-order harmonics. We provide both theoretical analysis and empirical validation for our design. Our SMN achieves a PSNR of $40+$ dB on two image datasets, comparing favorably against state-of-the-art methods in terms of both reconstruction accuracy and parameter efficiency. Furthermore, consistent advantage is observed on the challenging 3D NeRF novel view synthesis task. Supplementary materials are available at https://inrainbws.github.io/smn/.

</details>


### [29] [SCAR: Satellite Imagery-Based Calibration for Aerial Recordings](https://arxiv.org/abs/2602.16349)
*Henry Hölzemann,Michael Schleiss*

Main category: cs.CV

TL;DR: SCAR利用公开卫星正射影像与高程数据，长期自动校准航拍视觉-惯性系统的内外参，通过2D–3D对齐在实地部署中检测并修正漂移，较Kalibr/COLMAP/VINS-Mono显著降低重投影与定位误差，实现无需人工的稳健可复现实地长期标定。


<details>
  <summary>Details</summary>
Motivation: 现有航拍视觉-惯性系统在长期实地运行中会因环境与时间造成标定退化；传统方法依赖专门机动或人工布设控制点，成本高、不可持续。作者希望借助持久可得的地理参考数据，在真实任务中持续、自动地检测并纠正标定误差。

Method: 提出SCAR：将机载相机采集的航拍图像与公开的正射影像与DEM/DSM建立2D–3D对应关系，通过对齐优化同时估计内参与外参；以地理参考作为全局锚点，在不同季节和环境下执行长周期自校准；与标准工具（Kalibr、COLMAP、VINS-Mono）对比评估。

Result: 在两年、六次大规模航测序列上，SCAR在所有数据上稳定优于基线，显著降低中位数重投影误差，并将标定改进转化为更低的视觉定位旋转误差与更高位姿精度。

Conclusion: SCAR能在无需人工干预的前提下，依托外部地理数据实现长期、准确、稳健、可复现实地航拍视觉-惯性系统标定，适用于长周期空中作业部署。

Abstract: We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.

</details>


### [30] [Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired](https://arxiv.org/abs/2602.16385)
*Qi He,XiangXiang Wang,Jingtao Zhang,Yongbin Yu,Hongxiang Chu,Manping Fan,JingYe Cai,Zhenglin Yang*

Main category: cs.CV

TL;DR: 提出AMAA框架，在单目3D语义场景补全中通过自适应多尺度注意与特征门控提升结构稳定与语义一致性，较MonoScene在NYUv2上取得小幅但稳定提升，并可在Jetson上稳定运行。


<details>
  <summary>Details</summary>
Motivation: 单目SSC对室内助盲安全感知重要，但现有方法在2D→3D提升与多尺度融合时缺少对体素特征可靠性的显式建模与受控的信息传播，易出现投影扩散与特征纠缠，导致结构不稳与语义不一致。

Method: 在MonoScene管线基础上引入AMAA：1) 并行通道-空间注意聚合，对提升后的体素特征在语义与空间两维联合校准，增强可靠/抑制不可靠特征；2) 分层自适应特征门控策略，调节跨尺度信息注入，稳定编码器-解码器间的多尺度融合；整体不显著增加主干复杂度，强调可靠性导向的特征调控。

Result: 在NYUv2上，SSC mIoU=27.25%（+0.31），SC IoU=43.10%（+0.59），相较MonoScene取得一致小幅提升；在NVIDIA Jetson嵌入式平台实现端到端稳定部署。

Conclusion: AMAA在不显著增大模型复杂度的前提下，提升单目SSC的结构稳定性与语义一致性，兼顾精度与可部署性，适用于室内助盲系统等安全关键场景。

Abstract: In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users.

</details>


### [31] [ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding](https://arxiv.org/abs/2602.16412)
*Daichi Yashima,Shuhei Kurita,Yusuke Oda,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出ReMoRa：一种直接在视频压缩域操作的多模态大模型框架，用稀疏关键帧+精炼运动表示替代全序列RGB，复杂度线性扩展，在多项长视频理解基准上优于基线。


<details>
  <summary>Details</summary>
Motivation: 长视频理解对MLLM极具挑战：全RGB序列输入冗余大且自注意力随序列长度呈平方增长，导致计算不可承受；同时需要同时捕获外观与时序动态。

Method: 在压缩域处理视频：仅保留少量RGB关键帧表征外观；用从压缩比特流中提取的块级运动矢量作为光流的紧凑代理，并设计去噪与细化模块生成更精细的运动表示；将外观与运动特征融合，并采用线性随序列长度扩展的特征压缩策略，使MLLM可扩展至长视频。

Result: 在LongVideoBench、NExT-QA、MLVU等长视频基准上广泛实验，ReMoRa在多项指标上优于现有基线方法。

Conclusion: 利用压缩域稀疏外观+精炼运动表示，可在保持时序信息的同时显著降低计算与冗余，使MLLM对长视频的理解更高效、更准确。

Abstract: While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.

</details>


### [32] [Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems](https://arxiv.org/abs/2602.16430)
*Ali Faraz,Raja Kolla,Ashish Kulkarni,Shubham Agarwal*

Main category: cs.CV

TL;DR: 论文比较两种用于印度多语种OCR的训练策略：端到端多模态VLM与对现有OCR进行跨语种微调；结果显示微调策略在准确率与延迟上更优，并推出Chitrapathak-2与面向政务文档的Parichay系列，分别在通用OCR与结构化字段抽取上达成SOTA/高效表现。


<details>
  <summary>Details</summary>
Motivation: 印度场景下OCR需兼顾多语种（含多种Indic语言）、文档形态多样以及实际部署的速度/成本限制，现有多模态大模型方案与传统OCR在工程化取舍上缺少系统比较与实践指引。

Method: 对比两条路径：1）使用通用视觉编码器+强多语语言模型，端到端训练做OCR；2）对现有OCR模型进行跨语言微调（即便其未在目标语言上训练）。在多语Indic基准与面向部署的指标上进行广泛评测。提出并实现两套系统：Chitrapathak-2（通用OCR，优化速度/精度）与Parichay（面向9类印度政府文档的结构化字段抽取OCR）。

Result: 跨语种微调策略在准确率-时延权衡上稳定优于端到端VLM方案。Chitrapathak-2相较前代推理加速3-6倍，在泰卢固语达到SOTA（字符级ANLS=6.69；按文意应为越低越好或度量特定定义），其余语言为次优。Parichay在9类政务证件关键字段抽取上Exact Match=89.8%，且推理更快。

Conclusion: 在印度多语OCR中，微调既有OCR模型较端到端VLM更具工程可行性与效率；结合Chitrapathak-2与Parichay，可在通用识别与结构化抽取两端达到SOTA并为生产级OCR流水线提供实践指南。

Abstract: Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.

</details>


### [33] [Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing](https://arxiv.org/abs/2602.16455)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CV

TL;DR: 提出一种视觉自校正范式VSR，并在图表解析任务中实现为ChartVSR，通过生成并回馈像素级定位可视化来迭代纠错，最终以这些“视觉锚点”解码结构化数据；同时构建更具挑战性的ChartP-Bench。


<details>
  <summary>Details</summary>
Motivation: LVLM在文本层面的推理/自纠错很强，但对以视觉感知为核心的复杂任务（如图表解析）帮助有限，常见问题包括数据遗漏、对齐错误和幻觉。人类读图时会用手指定位作为“视觉锚点”以避免出错，启发作者引入可视化的像素级反馈来提升准确性。

Method: 提出Visual Self-Refine (VSR)：模型先产生像素级定位结果并可视化，再将该可视化回馈给自身进行直观自检与纠错。具体实现ChartVSR包含两阶段：1）Refine阶段：迭代利用视觉反馈校准所有数据点的像素级定位；2）Decode阶段：以经验证的定位作为精确视觉锚点解析最终结构化数据。同时构建一个困难的新基准ChartP-Bench。

Result: ChartVSR在新提出的ChartP-Bench以及现有图表解析任务上显著降低遗漏、错位与幻觉等错误，整体解析精度提升（摘要未给出具体数值）。

Conclusion: VSR作为通用的视觉反馈机制，可在以视觉为中心的多类任务中提升准确性；ChartVSR验证了该范式在图表解析上的有效性，并配套发布高挑战性的基准ChartP-Bench。

Abstract: While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.

</details>


### [34] [MMA: Multimodal Memory Agent](https://arxiv.org/abs/2602.16493)
*Yihao Lu,Wanru Cheng,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 提出多模态记忆代理MMA：为每条检索记忆动态打分（可信度、时间衰减、冲突共识），据此重加权与选择性拒答；并发布含受控可靠性与图文矛盾的基准MMA-Bench，揭示RAG的“视觉安慰剂效应”。在FEVER与LoCoMo等上稳健性与选择性效用提升；在MMA-Bench视觉模式显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 长程多模态代理依赖外部记忆与RAG，但纯相似度检索常引入过期、低可信或相互冲突的信息，导致过度自信与错误决策。需要一种机制评估与调节检索证据的可靠性，并在证据不足时避免冒险作答，同时缺少能系统刻画信念动态与多模态矛盾的基准。

Method: 设计MMA：对每条检索记忆计算动态可靠性分数，融合三要素——(1)来源可信度建模；(2)时间衰减以惩罚陈旧信息；(3)冲突感知的网络共识以处理相互矛盾证据。利用该分数进行证据重加权与选择性拒答策略。构建MMA-Bench：程序化生成、可控说话者可靠性、结构化图文矛盾，用于评测信念更新与鲁棒性，并分析RAG中潜在视觉偏置（“视觉安慰剂效应”）。

Result: 在FEVER上，与基线准确率相当但方差下降35.2%，选择性效用提升；在LoCoMo上，安全取向配置提高可执行准确率并减少错误回答；在MMA-Bench上，视觉模式Type-B准确率41.18%，而基线同协议下为0.0%。

Conclusion: 动态可靠性建模与拒答策略能显著提升多模态RAG代理在不确定与冲突信息下的稳健性与安全性；MMA-Bench揭示并量化了基础模型的视觉偏置问题，为未来改进提供评测工具与研究方向。

Abstract: Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the "Visual Placebo Effect", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.

</details>


### [35] [Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection](https://arxiv.org/abs/2602.16494)
*Alexis Winter,Jean-Vincent Martini,Romaric Audigier,Angelique Loesch,Bertrand Luvison*

Main category: cs.CV

TL;DR: 论文提出统一基准评测框架，系统比较目标检测对抗攻击/防御；实证发现：CNN到Transformer的攻击迁移性很差；最强防御是混合高强度、不同目标的对抗训练数据。


<details>
  <summary>Details</summary>
Motivation: 目标检测对抗鲁棒研究缺乏统一标准：数据集、效率指标、扰动度量各不相同，难以公平比较攻击/防御与复现实验；同时不清楚攻击在CNN与ViT间的可迁移性，以及何种对抗训练策略最有效。

Method: 1) 提出面向数字（非贴片）攻击的统一基准：将定位与分类错误分离评估；使用多种感知扰动度量衡量代价；纳入效率指标。2) 基于该基准，对多种SOTA攻击与多类检测器（含CNN与Transformer）进行大规模实验。3) 评估多种对抗训练组合策略（空间/语义目标、不同强度混合）。

Result: - 现代目标检测攻击对Transformer式检测器的迁移性显著不足，相比对CNN迁移更弱。- 使用由高扰动、且目标多样（空间与语义）的攻击混合构建训练集的对抗训练，鲁棒性优于任何单一攻击训练。

Conclusion: 统一基准促进公平比较与复现；Transformer检测器对来自CNN的黑箱/迁移攻击更难攻破；最有效的鲁棒训练应采用多目标、高强度攻击的混合数据，而非单一攻击。

Abstract: Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cost. This paper addresses this gap by investigating three key questions: (1) How can we create a fair benchmark to impartially compare attacks? (2) How well do modern attacks transfer across different architectures, especially from Convolutional Neural Networks to Vision Transformers? (3) What is the most effective adversarial training strategy for robust defense? To answer these, we first propose a unified benchmark framework focused on digital, non-patch-based attacks. This framework introduces specific metrics to disentangle localization and classification errors and evaluates attack cost using multiple perceptual metrics. Using this benchmark, we conduct extensive experiments on state-of-the-art attacks and a wide range of detectors. Our findings reveal two major conclusions: first, modern adversarial attacks against object detection models show a significant lack of transferability to transformer-based architectures. Second, we demonstrate that the most robust adversarial training strategy leverages a dataset composed of a mix of high-perturbation attacks with different objectives (e.g., spatial and semantic), which outperforms training on any single attack.

</details>


### [36] [DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images](https://arxiv.org/abs/2602.16502)
*Zeng Tao,Ying Jiang,Yunuo Chen,Tianyi Xie,Huamin Wang,Yingnian Wu,Yin Yang,Abishek Sampath Kumar,Kenji Tashiro,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 提出DressWild：从单张野外图像直接预测物理一致的2D缝制版型及对应3D服装的端到端方法，利用VLM进行姿态归一化与特征提取，Transformer融合后回归版型参数，可直接用于仿真、贴图与多层试穿，兼具高效与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有前馈方法在多姿态与视角下鲁棒性差，而基于优化的方法计算量大、难扩展。实际建模与制作需要可编辑、可分离、可仿真的2D版型与对应3D服装，亟需一种既高效又能处理“野外”单图输入的解决方案。

Method: 从单张输入图像出发：1) 利用视觉-语言模型在图像层面归一化姿态与视角；2) 提取与姿态相关、包含3D语义的服装特征；3) 通过Transformer编码器融合多源特征；4) 回归服装缝制版型参数，并重建对应3D服装。输出的2D版型与3D网格可直接用于物理仿真、纹理生成和多层虚拟试穿。

Result: 在大量“野外”图像上进行实验，方法无需多视角或迭代优化即可稳健恢复多样化的缝制版型与对应3D服装，表现出较强的鲁棒性与效率。

Conclusion: DressWild提供了一个可扩展的前馈管线，实现从单图到物理一致的2D版型与3D服装的重建，满足仿真与动画需求，相比优化方法更高效，相比传统前馈方法更能适应复杂姿态与视角。

Abstract: Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.

</details>


### [37] [Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding](https://arxiv.org/abs/2602.16545)
*Kaiting Liu,Hazel Doughty*

Main category: cs.CV

TL;DR: 提出“类别拆分”任务：在不收集新数据的情况下，编辑现有视频分类器，将粗粒度类别细化为子类别，并保持其他类别准确率；提供零样本编辑与少样本微调（以零样本为初始化），在新基准上显著优于视觉-语言基线。


<details>
  <summary>Details</summary>
Motivation: 现实视频分类标签往往过于粗糙，把对象/方式/结果等差异合并到单一标签；当任务语义演化时，重新标注与再训练代价高昂，需要一种能在不破坏原有性能的前提下，对既有模型进行细化的能力。

Method: 1）定义“类别拆分”任务：将已训练分类器中的某个粗类别细化为若干子类别，同时保持对未拆分类别的精度。2）提出零样本编辑：利用视频分类器的潜在可组合结构，显式暴露细粒度区分，无需额外数据。3）提出少样本微调：在极少标注下进一步优化，并以零样本编辑作为良好初始化。

Result: 在作者构建的视频类别拆分基准上，所提方法在新细分类别的准确率显著提升，且对其余类别几乎无退化；总体上明显优于视觉-语言基线方法。

Conclusion: 通过利用模型的潜在组合性实现零样本编辑，并辅以少样本微调，可以高效将粗粒度类别细化为细粒度子类，同时保持整体性能，为动态任务与标签演化提供低成本适配路径。

Abstract: Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.

</details>


### [38] [Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face](https://arxiv.org/abs/2602.16569)
*Nicolò Di Domenico,Annalisa Franco,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 论文提出一种基于Arc2Face的人脸变形（morphing）方法，在两套大规模保密检测数据集和两个新构建数据集（FEI与ONOT派生）上表现出与传统基于关键点的变形方法相当的攻击潜力。


<details>
  <summary>Details</summary>
Motivation: 电子证件的人脸登记流程常缺乏受控活体采集，给人脸识别系统留下可被人脸变形攻击利用的漏洞。现有最具挑战性的变形多为关键点驱动，但深度生成模型能否同样有效、并更好地保留/操控身份信息，有待验证。

Method: 提出一种利用Arc2Face（基于身份表征条件的基础模型）生成高保真、可控身份的人脸图像的变形技术。将两个或多个人的紧凑身份表征进行融合/操控，通过Arc2Face合成逼真的morph图像；在两套保密检测数据集与两个新建的FEI、ONOT派生morph数据集上评估。

Result: 在衡量“morphing attack potential”的指标上，所提深度学习方法与最具挑战性的传统关键点（landmark）方法相当，并优于多种现有深度/传统基线。

Conclusion: Arc2Face驱动的morph生成能够有效保留与管理身份信息，在攻击潜力上达到传统关键点法的水平，表明基于身份条件的生成模型可成为强有力的morphing工具，对证件系统安全提出新挑战。

Abstract: Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.

</details>


### [39] [A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification](https://arxiv.org/abs/2602.16590)
*Qi You,Yitai Cheng,Zichao Zeng,James Haworth*

Main category: cs.CV

TL;DR: 提出CLIP-MHAdapter：在CLIP上以轻量可训练模块增强局部细粒度属性识别，在Global StreetScapes八个街景属性任务上以约140万参数达SOTA/竞争性能且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 街景图像属性分类需要捕捉复杂、拥挤场景中的局部细粒度特征；现有基于CLIP的适配/微调多用全局图像嵌入，难以建模局部与跨patch依赖，且训练成本高。

Method: 在CLIP适配范式中加入瓶颈式MLP并配备多头自注意力，直接作用于patch token以建模patch间依赖；整体仅约1.4M可训练参数，保持轻量；作为附加适配器而非整体微调。

Result: 在Global StreetScapes数据集的8个属性分类任务上取得优于或可比的准确率，刷新SOTA，同时维持低计算成本。代码开源。

Conclusion: 通过在CLIP上加入基于patch token的MH自注意力瓶颈适配器，可在极小参数量下显著提升街景细粒度属性分类效果，说明利用局部关系的轻量适配是高效而有效的方向。

Abstract: Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.

</details>


### [40] [Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge](https://arxiv.org/abs/2602.16664)
*Jiaming Liu,Felix Petersen,Yunhe Gao,Yabin Zhang,Hyojin Kim,Akshay S. Chaudhari,Yu Sun,Stefano Ermon,Sergios Gatidis*

Main category: cs.CV

TL;DR: 提出自监督语义桥（SSB），将外部自监督语义先验融入扩散桥模型，在无需跨域监督下实现空间保持的图像到图像翻译，并在医学图像合成与文本编辑上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对比学习的无监督翻译两类主线各有痛点：对抗式方法需目标域对抗损失，泛化到未见数据差；扩散反演方法因噪声潜空间反演不精确，常导致低保真翻译。需要一种同时具备高保真、强泛化、无需跨域监督的框架。

Method: 引入自监督视觉编码器（如通过对比/自蒸馏训练）获得对外观变化不敏感、但能捕获几何结构的表示，构造跨域共享的潜空间；以此潜空间作为条件，训练扩散桥模型进行无配对的域间映射，实现空间一致的翻译；框架可无缝扩展到文本条件编辑。

Result: 在多项严苛的医学图像合成实验中（含域内与域外评测），SSB优于强基线（对抗式与扩散反演类）并生成高保真、结构保持的结果；同时展现高质量文本引导编辑能力。

Conclusion: 通过自监督语义先验构造共享潜空间以条件化扩散桥，可在无跨域监督下实现空间忠实的图像翻译，兼具更强的泛化与保真度，并具备文本编辑扩展性。

Abstract: Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.

</details>


### [41] [PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction](https://arxiv.org/abs/2602.16669)
*Bo Lang,Nirav Savaliya,Zhihao Zheng,Jinglun Feng,Zheng-Hang Yeh,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 提出一个端到端、一致性的在线HD矢量地图构建框架，通过语义感知查询、历史栅格化记忆与短期预测联合实现更稳健的时序一致性与跟踪，在nuScenes与Argoverse2上优于SOTA且高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的方法多采用随机初始化并依赖隐式时序建模，导致在全局地图构建中出现时序不一致与不稳定。需要一种能显式利用历史信息并增强时序连续性的机制，同时在在线场景中维持高效率。

Method: 1) 语义感知查询生成器：用空间对齐的语义掩码初始化查询，获取全局场景级上下文；2) 历史栅格化地图记忆：为每个被跟踪实例存储细粒度实例级地图作为显式历史先验；3) 历史地图引导模块：将栅格化历史信息融合到跟踪查询中，提升时序连续性；4) 短期未来引导模块：基于历史轨迹预测实例短期运动，将预测的未来位置作为先验提示，避免不合理预测并保持时序一致；整体以端到端方式联合完成地图实例跟踪与短期预测。

Result: 在nuScenes与Argoverse2数据集上进行大量实验，方法在精度与稳定性上均超越当前SOTA，同时保持较好效率。

Conclusion: 通过显式历史记忆与短期未来引导，结合语义感知查询初始化，可显著提升在线HD矢量地图构建的时序一致性与稳健性，实现对现有SOTA的全面超越且具备计算效率。

Abstract: High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.

</details>


### [42] [VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection](https://arxiv.org/abs/2602.16681)
*Yingyuan Yang,Tian Lan,Yifei Gao,Yimeng Lu,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.CV

TL;DR: VETime提出统一时间与视觉模态的TSAD框架，通过细粒度视觉-时间对齐与自适应融合，在零样本下实现更精准定位与更低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有TSAD基础模型两难：1D时间模型定位细但缺乏全局语境；2D视觉模型有全局模式但时间对齐差、点级检测粗且信息瓶颈。需要一个既保留细粒度时间敏感度又拥有全局上下文视角的统一方法。

Method: 1) 可逆图像转换：将时间序列映射为图像表征且可逆，避免丢失判别细节；2) Patch级时间对齐：在视觉patch与时间步之间建立共享的视觉-时间时间轴，实现细粒度对齐；3) 异常窗口对比学习：以异常/正常窗口为单位进行对比，增强对异常上下文与边界的辨识；4) 任务自适应多模态融合：动态融合时间与视觉模态的互补感知能力，实现点级与上下文的协同检测。

Result: 在零样本设定下，相比现有SOTA显著提升定位精度，并且较当前视觉方法具有更低计算开销；实验覆盖多数据集且一致优于对照。

Conclusion: 通过精细对齐与自适应融合打破1D与2D模型的权衡，VETime实现高精度、低成本的时间序列异常检测，适合零样本与实际部署场景。

Abstract: Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.

</details>


### [43] [Learning Situated Awareness in the Real World](https://arxiv.org/abs/2602.16682)
*Chuhan Li,Ruilin Han,Joy Hsu,Yongyuan Liang,Rajiv Dhawan,Jiajun Wu,Ming-Hsuan Yang,Xin Eric Wang*

Main category: cs.CV

TL;DR: SAW-Bench 是一个用于评估多模态基础模型在第一人称情境感知（observer-centric）上的新基准，涵盖786段真实佩戴式眼镜视频与2,071个问答，对六类觉知任务进行测评，发现与人类存在约37.66%的性能差距，并揭示模型在相机几何与空间推理上的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有多模态评测多聚焦于环境中心的空间关系（物体与物体之间），而忽视与观察者相关的视点、姿态与运动等“观察者中心”关系。为弥补这一评测空白，需要一个能在真实世界第一人称视频中系统检验情境化空间智能的基准。

Method: 构建SAW-Bench：使用Ray-Ban Meta（第二代）智能眼镜自录786段多场景室内外视频；人工标注2,071个QA样例，设计六类观察者中心觉知任务；以主流多模态基础模型进行全面评测，并进行误差剖析以识别系统性失败模式（如对相机几何一致性推断不足）。

Result: 最佳模型（如Gemini 3 Flash）与人类表现仍有约37.66%的差距。模型可利用部分几何线索，但难以形成一致的相机几何表征，从而在空间推理上出现系统性错误。

Conclusion: SAW-Bench提供了一个面向真实世界、第一人称的情境空间智能评测框架，推动评测从被动观察转向以观察者为中心、具物理约束的动态理解；当前MFMs在该维度仍显著落后于人类，需要更好地建模相机几何与连贯的自我中心推理。

Abstract: A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.

</details>


### [44] [Are Object-Centric Representations Better At Compositional Generalization?](https://arxiv.org/abs/2602.16689)
*Ferdinand Kapl,Amir Mohammad Karimi Mamaghan,Maximilian Seitzer,Karl Henrik Johansson,Carsten Marr,Stefan Bauer,Andrea Dittadi*

Main category: cs.CV

TL;DR: 提出一个在三类受控视觉世界上的VQA基准，系统比较带/不带对象中心偏置的视觉编码器在组合泛化上的表现，发现对象中心表示在困难设定、低样本和低算力下更优，密集表示仅在简单设定且高算力/高数据下占优。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类与机器学习的关键能力差距；尽管对象中心表示被认为有助于组合泛化，但在复杂视觉环境中的系统性证据不足，且以往比较往往未控制训练数据多样性、样本量、表示大小和下游容量等混杂因素。

Method: 构建跨CLEVRTex、Super-CLEVR、MOVi-C的VQA基准，设计训练-测试分布错配以考察对未见属性组合的泛化；以DINOv2与SigLIP2作为密集基线，并训练其对象中心(OC)对应模型；严格控制数据规模/多样性、表示维度、下游模型容量与计算预算，评估不同设定下的泛化表现与样本效率。

Result: 1) OC方法在更困难的组合泛化设定中显著优于密集表示；2) 密集表示仅在较容易的设定中更好，且往往需要显著更高的下游计算；3) OC模型更具样本效率，在少样本下更强，密集编码器需要足够数据与多样性才能追平或超越。

Conclusion: 当数据规模、多样性或下游算力任一受限时，对象中心表示提供更强的组合泛化能力；密集表示要想匹敌通常需更大数据与计算。因此在资源受限或需稳健组合泛化的情境，应优先采用对象中心表示。

Abstract: Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.

</details>


### [45] [Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning](https://arxiv.org/abs/2602.16702)
*Mingjia Shi,Yinhan He,Yaochen Zhu,Jundong Li*

Main category: cs.CV

TL;DR: 提出SAP（显著性感知原则）用于VLM推理时的稳定可控与并行探索，在不训练的前提下提高性能、降低幻觉与时延。


<details>
  <summary>Details</summary>
Motivation: VLM在推理扩展上困难：视觉只在生成初始输入一次，后续自回归文本主导，早期视觉定位错误会累积；常规推理期视觉引导粗糙嘈杂，难以长距离稳定引导。需要一种能在推理过程中反复、稳健地回看视觉证据并避免早期误导的机制。

Method: 提出Saliency-Aware Principle (SAP) 选择：以“高层原则/策略”而非逐token轨迹为控制单元；利用显著性引导在需要时重新查证视觉证据；在噪声反馈下保持离散生成的稳定控制；支持多路径（multi-route）并行推理探索。方法与模型无关、数据无关、无需额外训练；在相同token预算下进行推理。

Result: 在实证评测中，SAP在与CoT等长序列推理相比，实现更稳定的推理、更低的响应延迟，并在减少物体幻觉方面表现突出；整体性能具有竞争力，且在同等生成token预算下取得优势。

Conclusion: SAP为VLM提供了一种训练外、模型无关的推理时控制与并行探索框架，通过高层原则选择与显著性回看机制，缓解文本主导与早期视觉误差累积问题，降低幻觉并提升效率与稳定性。

Abstract: Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.

</details>


### [46] [TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos](https://arxiv.org/abs/2602.16711)
*Namitha Padmanabhan,Matthew Gwilliam,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: TeCoNeRV 通过时空分解的超网络权重预测、残差式段间存储与时间一致性正则，降低内存与码率、提升速度与质量，在UVG/HEVC/MCL-JCV上首个支持至1080p的超网络视频压缩方法。


<details>
  <summary>Details</summary>
Motivation: INR用于视频压缩效果好但需对每段视频单独过拟合，导致高分辨率时编码效率差、训练慢、内存与码率高。现有超网络虽能快速预测权重但质量低、压缩体积大、分辨率升高时内存不可承受。

Method: 提出TeCoNeRV：（1）空间与时间上分解权重预测，将短视频段切成patch tubelets，预训练内存开销降20×；（2）采用段间权重残差存储，仅记录相邻段表示的差分以缩短比特流；（3）时间一致性正则，使权重空间的变化与视频内容变化相关，提高时域稳定性与重建质量。

Result: 在UVG上480p与720p分别较基线提升PSNR 2.47dB与5.35dB，码率降低36%，编码速度提升1.5–3×。低内存使其成为首个在UVG、HEVC、MCL-JCV上覆盖480p/720p/1080p的超网络方案。

Conclusion: TeCoNeRV解决了超网络INR视频压缩在高分辨率下的质量、码率与内存瓶颈，通过时空分解、残差存储和时间一致性正则实现高效高质、可扩展的压缩，并验证于多数据集与分辨率。

Abstract: Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .

</details>
