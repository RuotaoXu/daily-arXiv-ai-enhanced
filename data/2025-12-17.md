<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 113]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline](https://arxiv.org/abs/2512.13731)
*Weikang Bai,Yongkun Du,Yuchen Su,Yazhen Xie,Zhineng Chen*

Main category: cs.CV

TL;DR: 论文提出CMER-Bench基准与两大复杂表达式数据集（MER-17M、CMER-3M），并引入结构化表示SML与新tokenizer，基于此训练轻量模型CMERNet，在复杂数学公式识别上显著超越现有MER与通用MLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有MER和MLLM在简单/中等表达式上表现尚可，但对长式、多行、空间结构复杂的表达式性能急剧下降，根因是公开训练数据以简单样本为主且LaTeX难显式表达层级与空间关系。

Method: 1）构建CMER-Bench，将样例按难度分级；2）发布大规模偏重复杂样例的数据集MER-17M与CMER-3M；3）设计表达式tokenizer与Structured Mathematical Language（SML）以显式建模层级与空间布局；4）提出基于编码器-解码器的CMERNet（约1.25亿参数），在CMER-3M上训练。

Result: 系统评测显示：现有方法在复杂表达式上显著退化；CMERNet在CMER-Bench上显著优于现有MER模型与通用MLLM，同等或更小规模下取得更高准确率。

Conclusion: 针对复杂数学表达式，数据与表示同等关键：通过提供复杂样本数据集并引入显式结构表示与专用模型，可显著提升MER的鲁棒性与精度；CMERNet验证了该路线的有效性。

Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.

</details>


### [2] [Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage](https://arxiv.org/abs/2512.13739)
*Yajie Yang,Yuqing Zhao,Xiaochao Xi,Yinan Zhu*

Main category: cs.CV

TL;DR: 论文探讨在新闻专题报道中如何以可控方式使用AIGC生成图片，识别跨平台语义偏差与文化表达差异，并提出“人机协同+可追溯”生产管线与三项评估指标（CIS、CEA、U-PA）以保障真实性与可用性。


<details>
  <summary>Details</summary>
Motivation: 媒体在采用AIGC生成图像时面临误导、真实性、语义对齐和可解释性等风险；现有平台多为黑箱，难以同时满足内容准确与语义一致，造成伦理与信任困境。作者希望找到可控、可审计、可追溯的生产路径。

Method: 两项实验：1）跨平台可迁移性测试：用标准化提示词在三类场景下对比多平台输出，从语义对齐、文化特异性、视觉真实性等维度分析差异与成因（训练语料偏差、平台过滤等）。2）构建“人-机在环”的模块化管线：高精度分割（SAM、GroundingDINO）、语义对齐（BrushNet）、风格控制（Style-LoRA、Prompt-to-Prompt），并以CLIP语义评分、NSFW/OCR/YOLO过滤与可验证内容凭据进行把关与追踪部署。

Result: 实验1发现不同平台在语义、文化表达与真实感上存在明显差异，源于语料与策略差异。实验2显示模块化管线可显著提升编辑可控性与语义一致，且通过多重过滤与凭据机制提高可追溯性与合规性。

Conclusion: 提出面向新闻专题报道的AIGC人机协同机制与可追溯部署方案，并建议以角色身份稳定性（CIS）、文化表达准确性（CEA）和用户-公众适宜性（U-PA）作为关键评估维度，以平衡创作效率、编辑控制与伦理合规。

Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).

</details>


### [3] [DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](https://arxiv.org/abs/2512.13742)
*Md. Najib Hasan,Imran Ahmad,Sourav Basak Shuvo,Md. Mahadi Hasan Ankon,Sunanda Das,Nazmul Siddique,Hui Wang*

Main category: cs.CV

TL;DR: 提出DL+LLM框架：用专为内镜设计的MobileCoAtNet实现高准确度分类，再用LLM基于分类结果生成结构化临床推理；在两套专家标注基准上评估32个LLM，发现解释质量受益于强分类，但LLM在稳定性上仍明显不及人类。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类器缺乏可解释性，LLM会写临床文本但视觉推理弱、解释不稳定，临床期望的因果化推理与模型输出存在鸿沟；需要一种把视觉识别与临床推理联接起来、并能量化评估解释质量与稳定性的框架。

Method: 1) 设计面向胃部内镜的混合卷积-注意力模型MobileCoAtNet，对八类病灶进行高精度分类；2) 以分类输出作为条件，驱动多种LLM生成涵盖病因、症状、治疗、生活方式与随访的结构化解释；3) 构建并由专家核验的两套基准作为“金标准”评估解释的正确性与稳定性；4) 对32个LLM进行对比评测，并分析提示变化对推理一致性的影响。

Result: MobileCoAtNet在八类胃部任务上取得高准确率；强分类信号显著提升LLM的解释质量；但所有LLM在稳定性上均落后于人类基线，对不同提示敏感，推理易变。

Conclusion: DL分类器与LLM结合能产出有用的临床叙述与结构化推理视图，但当前LLM不可靠，难以胜任高风险医疗决策；所提框架既揭示LLM边界，也为构建更安全、可评估的临床推理系统提供路径与可复现实验资源。

Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.

</details>


### [4] [Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making](https://arxiv.org/abs/2512.13747)
*Siyuan Dai,Lunxiao Li,Kun Zhao,Eardi Lila,Paul K. Crane,Heng Huang,Dongkuan Xu,Haoteng Tang,Liang Zhan*

Main category: cs.CV

TL;DR: 研究发现，在医疗多模态决策任务中，现有多模态大模型（MLLMs）在视觉理解上薄弱，文本推理常优于多模态或纯视觉；提出三种缓解策略以提升表现。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在通用视觉-语言任务中具备零样本能力，但在需要细粒度、临床可靠性的医疗决策任务上表现不佳。作者希望系统性评估其短板，并寻找能在医疗场景中提高多模态决策质量的可行路径。

Method: 在两个具有挑战性的数据集上进行实证评估：1) 阿尔茨海默病三分类（正常/MCI/痴呆），细微视觉差异；2) MIMIC-CXR胸片14项非互斥病灶分类。比较纯文本、纯视觉与多模态输入下的推理/分类性能。进一步探索三种改进策略：a) 带理由标注的示例进行ICL；b) 先做视觉图像描述，再进行纯文本推理；c) 对视觉塔进行小样本、分类监督的微调。

Result: 在两个任务上，纯文本推理稳定优于纯视觉与多模态输入；多模态常低于纯文本。采用三种策略后可一定程度缓解，但仍揭示出模型在视觉落地理解上的明显不足。

Conclusion: 当前MLLM缺乏扎实的视觉语义对齐与临床可用的视觉理解能力。通过理由增强的ICL、先字幕后文本推理、以及对视觉塔进行少样本监督微调，是提升医疗多模态决策的有效方向。

Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.

</details>


### [5] [STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning](https://arxiv.org/abs/2512.13752)
*Jie Qin,Jiancheng Huang,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: 提出STAR：分阶段、可堆叠自回归的统一多模态学习框架，通过冻结基础AR模型并逐层叠加同构AR模块，在理解、生成、编辑三阶段逐步扩展能力，结合高容量VQ与隐式推理，在多项基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在“理解与生成统一优化”上存在冲突：提升生成质量常导致理解退化，且不同任务间互相干扰、权衡难。需要一种既能提升生成与编辑能力，又不破坏既有理解性能的训练与模型扩展机制。

Method: 采用“STacked AutoRegressive（STAR）”方案：1) 将多模态学习分解为理解、生成、编辑多阶段；2) 冻结基础自回归模型参数，逐步堆叠同构AR模块以扩展任务能力，减小跨任务干扰；3) 引入高容量矢量量化（VQ）以提升图像表征粒度；4) 设计隐式推理机制，在复杂条件下提升生成质量。

Result: 在多个基准上达到SOTA：GenEval 0.91、DPG-Bench 87.44、ImgEdit 4.34，显示其在生成与编辑上的显著提升，同时保持理解能力。

Conclusion: STAR通过“冻结底座+同构堆叠”的任务递进式设计，实现理解、生成、编辑的一体化又互不干扰的多模态学习，兼顾性能扩展与稳定性，具有通用可扩展前景。

Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.

</details>


### [6] [Time-aware UNet and super-resolution deep residual networks for spatial downscaling](https://arxiv.org/abs/2512.13753)
*Mika Sipilä,Sabrina Maggio,Sandra De Iaco,Klaus Nordhausen,Monica Palma,Sara Taskinen*

Main category: cs.CV

TL;DR: 利用时间编码的轻量模块增强SRDRN与UNet，对粗分辨率卫星臭氧数据进行下采样到高分辨率；在意大利案例中显著提升精度与收敛速度且计算开销很小。


<details>
  <summary>Details</summary>
Motivation: 卫星大气污染物数据空间分辨率粗，限制了本地环境分析与决策的有效性；需要将粗分辨率观测可靠地细化到高分辨率。

Method: 在两种常用深度学习下采样架构（超分辨率残差网络SRDRN与编码器-解码器UNet）中加入轻量级时间模块，将观测时间通过正弦位置编码或径向基函数（RBF）编码成时间特征，并与空间特征在网络中融合；与不含时间信息的基线模型对比评估。

Result: 在意大利地表对流层臭氧下采样任务中，加入时间模块后，在几乎不增加计算复杂度的前提下显著提升下采样性能（误差降低、指标改善）并加快训练收敛。

Conclusion: 时间感知的轻量编码可有效增强空间超分辨模型在大气污染下采样中的表现，是提高精度与效率的实用扩展。

Abstract: Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.

</details>


### [7] [Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries](https://arxiv.org/abs/2512.13796)
*Victor Rong,Jan Held,Victor Chu,Daniel Rebain,Marc Van Droogenbroeck,Kiriakos N. Kutulakos,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 提出一种将几何与外观解耦的紧凑表示：用surfel表达几何，外观由全局神经场+每primitive颜色共同表示；每像素只纹理化固定数量primitive，计算量低。在室外达成与3D Gaussian Splatting相当的感知质量，同时primitive数减少9.7×、内存降5.5×；室内primitive数降31×、内存降3.7×；相较纹理化primitive方法，渲染速度提升约2×且画质更好。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian splatting在新视角合成上效果好，但为表达高纹理细节，即使几何简单也需数百万个primitive，导致内存大、渲染慢。需要一种在不牺牲画质的前提下大幅减少primitive数量与内存、提升速度的表示。

Method: 超越点渲染的表示：将几何与外观解耦。几何采用surfels表示；外观由“全局神经场（global neural field）+每primitive的颜色”组合而成。渲染时对每个像素仅对固定数量的primitive进行神经场纹理化，限制额外计算。

Result: 在室外场景：与3D Gaussian splatting感知质量相当，primitive减少约9.7×，内存减少约5.5×。在室内场景：primitive减少约31×，内存减少约3.7×。相较现有纹理化primitive方案，渲染速度约提升2×且视觉质量更优。

Conclusion: 解耦几何（surfel）与外观（全局神经场+局部颜色）的紧凑表示，在保持甚至提升画质的同时显著降低primitive数量与内存，并提升渲染速度，是对高纹理场景高效新视角合成的一种有效替代。

Abstract: Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\times$ fewer primitives and $5.5\times$ less memory on outdoor scenes and using $31\times$ fewer primitives and $3.7\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.

</details>


### [8] [VajraV1 -- The most accurate Real Time Object Detector of the YOLO family](https://arxiv.org/abs/2512.13834)
*Naman Balbir Singh Makkar*

Main category: cs.CV

TL;DR: VajraV1 在保实时性的前提下，通过整合并改进YOLO家族设计，实现了在COCO上的SOTA实时目标检测精度：Nano至Xlarge全线超越YOLOv12/13同级，最高56.2% mAP。


<details>
  <summary>Details</summary>
Motivation: 在YOLOv10–v13快速迭代下，仍存在精度与延迟权衡未最优、不同规模模型的一致性设计不足等问题。作者希望在不显著牺牲推理速度的情况下，进一步提升实时检测器的精度上限，并给出从Nano到Xlarge的一致可扩展架构。

Method: 提出VajraV1架构，融合并优化既有YOLO中的有效组件与训练策略（例如更高效的主干与特征金字塔、解耦头、轻量注意力/跳连/瓶颈设计、可能的动态标签/边界框分配与数据增强等），在多尺度（N/S/M/L/XL）上进行统一设计与调参，以在相同或相近延迟下获得更高mAP。

Result: 在COCO val上：VajraV1-Nano 44.3% mAP，较YOLOv12-N +3.7%、较YOLOv13-N +2.7%，延迟与YOLOv12-N/YOLOv11-N相当；Small 50.4% mAP，较YOLOv12-S/YOLOv13-S +2.4%；Medium 52.7% mAP，较YOLOv12-M +0.2%；Large 53.7% mAP，较YOLOv13-L +0.3%；Xlarge 56.2% mAP，优于所有现有实时检测器。

Conclusion: VajraV1通过架构整合与针对多尺度的一致优化，在不显著增加延迟的情况下实现了SOTA实时检测精度，尤其在Nano/Small端优势明显，验证了其设计在不同模型规模上的可扩展性与有效性。

Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.

</details>


### [9] [MoLingo: Motion-Language Alignment for Text-to-Motion Generation](https://arxiv.org/abs/2512.13840)
*Yannan He,Garvita Tiwari,Xiaohan Zhang,Pankaj Bora,Tolga Birdal,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: MoLingo 是一种在连续潜在空间中去噪的文本到人体动作生成模型，通过语义对齐的潜在编码与跨注意力文本条件，实现更逼真的、与文本一致的人体动作。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在扩散的T2M方法要么在整体潜在上扩散，要么自回归地在多段潜在上扩散，但在语义对齐与文本条件注入方面仍存在优化空间，导致生成质量与文本一致性不足。作者希望回答：如何构建更有利于扩散的语义对齐潜在空间，以及如何最有效地注入文本条件。

Method: 1) 训练一个语义对齐的动作编码器：利用帧级文本标签，使语义相近的动作潜在彼此接近，从而让潜在空间更“扩散友好”。2) 文本条件注入对比：比较单token条件与多token跨注意力（cross-attention），发现跨注意力更优。3) 采用语义对齐潜在、自回归生成与跨注意力文本条件的组合来进行连续潜在扩散生成。

Result: 在标准指标和用户研究上均取得最新SOTA，生成的动作更逼真，且与文本描述的一致性更高。

Conclusion: 语义对齐的动作潜在空间结合跨注意力的多token文本条件与自回归生成，使连续潜在扩散在T2M任务上表现显著提升；代码与模型将开源以促进后续研究与应用。

Abstract: We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.

</details>


### [10] [Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging](https://arxiv.org/abs/2512.13855)
*Ujjwal Mishra,Vinita Shukla,Praful Hambarde,Amit Shukla*

Main category: cs.CV

TL;DR: 提出Telescopic Adapters，一种按层深度逐步增大适配器容量的PEFT方法，在CLIPSeg视觉与文本编码器中以极少参数实现对医疗影像分割的高效迁移，并在多数据集上优于传统微调。


<details>
  <summary>Details</summary>
Motivation: 传统微调对VLSM在医疗领域迁移的计算成本高；现有PEFT为所有Transformer层使用统一适配器维度，导致参数分配不合理、适应效率低。需要一种既节省参数又能针对层深度差异化分配容量的方法。

Method: 在CLIPSeg的视觉与文本编码器各层插入轻量瓶颈适配器，依据层深度与语义相关性进行维度动态缩放：浅层容量小、深层容量大（望远镜式/梯度式扩张）。整体可训练参数约613k，比端到端微调少244倍。进行了跨五个医疗数据集的训练与消融。

Result: 在息肉分割、皮损检测、乳腺超声等五个数据集上获得优于现有微调/PEFT的方法的分割性能；消融表明深层需要显著更大的适配容量，验证了深度感知缩放的有效性。

Conclusion: Telescopic Adapters通过深度感知容量分配实现对医疗VLSM的高效微调，在极少参数与算力下保持或提升分割精度，适合资源受限的临床部署，并为PEFT在多层Transformer上的参数分配提供新范式。

Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.

</details>


### [11] [Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models](https://arxiv.org/abs/2512.13869)
*Wenda Li,Meng Wu,Sungmin Eum,Heesung Kwon,Qing Qu*

Main category: cs.CV

TL;DR: 提出CFHA，一个三阶段扩散式合成到真实的对齐框架，用于UAV人检测，显著缩小合成与真实域差距并保留原标签，在基准上mAP50最高提升+14.1。


<details>
  <summary>Details</summary>
Motivation: UAV场景下目标分布常变且标注稀缺，合成数据虽易得但存在明显域差，直接训练检测器迁移性差，需要一种能将合成数据转化为接近真实分布且维持标注有效性的办法。

Method: 提出Coarse-to-Fine Hierarchical Alignment（CFHA），基于扩散模型的三阶段管线：1）全局风格迁移：用扩散对齐颜色/光照/纹理至少量真实参考的风格；2）局部细化：利用超分扩散模型增强小目标（行人）细节与边界，保持形状完整；3）幻觉移除：过滤与真实分布不符的人体实例，去除不可信外观。框架显式解耦全局风格与局部内容差异并逐级弥合。

Result: 在公开UAV Sim2Real检测基准上显著优于未变换的合成数据训练方案，Semantic-Drone基准mAP50最高提升+14.1；消融显示全局与局部阶段具有互补性，层次化对齐至关重要。

Conclusion: CFHA能在不丢失合成标注的前提下，有效缩小合成到真实的域差并提升UAV人检测性能；全局-局部分层对齐与幻觉移除共同驱动性能增益，方法通用且数据需求小（仅需少量真实参考）。

Abstract: Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \href{https://github.com/liwd190019/CFHA}{this url}.

</details>


### [12] [SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning](https://arxiv.org/abs/2512.13874)
*Jitesh Jain,Jialuo Li,Zixian Ma,Jieyu Zhang,Chris Dongjoo Kim,Sangho Lee,Rohun Tripathi,Tanmay Gupta,Christopher Clark,Humphrey Shi*

Main category: cs.CV

TL;DR: 提出SAGE任意时长视频推理系统：按需单/多轮推理，配合合成数据与RL后训练的多模态编排器SAGE-MM，并构建长视频基准SAGE-Bench；在开放式视频推理上最高提升6.1%，10分钟以上视频提升8.2%。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理模型通常一次性处理大量帧并单轮作答，类似“看完整个长视频”，资源消耗大且不具任意时长（any-horizon）灵活性。人类能根据任务在“快进略读”与“完整观看”间自如切换，启发构建能按需调整推理步数与观看时长的系统。

Method: 1) 设计SAGE代理：对简单任务单轮推理，对长/复杂视频采用多轮迭代“看-想-再看”的策略；2) 用Gemini-2.5-Flash构建简易合成数据管线，训练核心编排器SAGE-MM，决定何时扩展/缩短推理；3) 提出RL后训练配方，使SAGE-MM学会任意时长的策略选择；4) 构建SAGE-Bench（平均>700秒）评测真实娱乐场景的视频推理。

Result: 系统、数据与RL方案在开放式视频推理任务上带来显著收益：总体最高+6.1%；对>10分钟长视频取得+8.2%的提升。

Conclusion: 通过代理式多轮推理、合成数据训练与RL后训练，SAGE实现任意时长的视频推理能力，并在长视频与开放式任务上显著超越SOTA，SAGE-Bench为长视频场景提供了更贴近实用的评测基准。

Abstract: As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.

</details>


### [13] [Route-DETR: Pairwise Query Routing in Transformers for Object Detection](https://arxiv.org/abs/2512.13876)
*Ye Zhang,Qi Chen,Wenyou Huang,Rui Liu,Zhengjian Kang*

Main category: cs.CV

TL;DR: Route-DETR 通过在解码器自注意中引入“自适应成对路由”，区别并处理竞争/互补查询，训练期加入低秩可学习偏置以实现抑制与委派两类非对称交互，推理期不增开销，在 COCO/Cityscapes 多个 DETR 基线上稳定提升，R50 比 DINO +1.7 mAP，Swin-L 达 57.6 mAP。


<details>
  <summary>Details</summary>
Motivation: 原始 DETR 存在查询竞争：多个查询收敛到同一目标，导致冗余计算和检测重复；希望在保持端到端、不引入后处理的前提下，减少重复并鼓励查询探索更多区域。

Method: 在解码器自注意层中基于查询两两相似度、置信度与几何关系判断查询对的关系：竞争或互补。为此设计双路由机制：1) 抑制路由（suppressor）在竞争查询间调制注意，降低重复关注；2) 委派路由（delegator）鼓励将注意力分散到不同区域以覆盖更多目标。以上通过可学习的低秩注意力偏置实现非对称的查询交互。训练采用双分支：训练期引入路由偏置进行学习，推理期回退为标准注意力，不增加额外计算。

Result: 在 COCO 与 Cityscapes 上对多种 DETR 基线取得一致增益：相对 DINO-ResNet-50 提升约 +1.7 mAP；在 Swin-L 主干上达到 57.6% mAP，超过先前 SOTA。

Conclusion: 通过在自注意中进行自适应成对路由，区分并处理竞争与互补查询，能有效缓解 DETR 的查询竞争问题，提升精度且推理零额外开销，对多种基线与数据集均适用。

Abstract: Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.

</details>


### [14] [KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI](https://arxiv.org/abs/2512.13902)
*Anning Tian,Byunghyun Ko,Kaichen Qu,Mengyuan Liu,Jeongkyu Lee*

Main category: cs.CV

TL;DR: 提出KLO-Net：在U-Net中引入动态KNN注意力与CSP编码器，实现前列腺MRI分割的高效、低内存且高精度的实时部署。


<details>
  <summary>Details</summary>
Motivation: 临床工作站实时部署前列腺MRI分割受算力与内存限制；同时前列腺解剖变异大，使深度分割方法在保证精度与效率上两难。需要一种在资源受限条件下仍能保持可靠精度的高效模型。

Method: 设计KLO-Net：1) 动态K-NN注意力，针对切片中每个空间位置自适应确定注意连接数量（区别于固定K的KNN注意力），以在保证表达力的同时降低冗余计算；2) 采用Cross Stage Partial（CSP）编码器模块，分流特征以减少计算和显存占用；3) 在PROMISE12与PROSTATEx上进行全面实验与消融，评估效率与精度。

Result: 与对比方法相比，KLO-Net在两大公共数据集上展现更高的计算效率（更低的计算量与内存占用）并保持或提升分割质量；消融验证动态KNN注意力与CSP模块的有效贡献。

Conclusion: 动态KNN注意力结合CSP编码器可在资源受限场景中实现高效、准确的前列腺MRI分割；KLO-Net在临床实时部署方面具备优势。

Abstract: Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.

</details>


### [15] [An evaluation of SVBRDF Prediction from Generative Image Models for Appearance Modeling of 3D Scenes](https://arxiv.org/abs/2512.13950)
*Alban Gauthier,Valentin Deschaintre,Alexandre Lanvin,Fredo Durand,Adrien Bousseau,George Drettakis*

Main category: cs.CV

TL;DR: 评估在快速外观建模流水线中，从条件图像生成与SVBRDF预测融合得到多视角SVBRDF纹理图集的可行性与一致性；发现标准UNet在精度与多视角一致性上与更复杂架构相当。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型使得与几何一致的逼真RGB图像可被快速合成，同时SVBRDF网络可从RGB反推出材质参数。将两者结合可高效生成多视图SVBRDF并拼接成纹理图集，但存在单视图预测跨视角不一致的问题，也可能利用生成图像及其条件携带的额外信息提升估计质量。

Method: 构建评估基准，比较不同神经网络架构（如UNet与更复杂模型）及不同条件输入（由条件图像生成器提供的多模态信号）在SVBRDF估计中的表现，重点衡量精度与多视角一致性；在快速建模流水线下进行系统性实验。

Result: 多种架构与条件对比显示：尽管有更复杂的设计，标准UNet在准确性与多视角一致性方面依然具有竞争力；某些条件信号有助于提升估计，但单视图预测仍可能导致纹理图集中出现不一致。

Conclusion: 在将生成图像与SVBRDF预测结合的快速外观建模中，需权衡单视图不一致风险与条件信息带来的增益；简单架构如UNet足以作为强基线，应优先关注多视一致性的约束与融合策略以获得稳定的SVBRDF纹理图集。

Abstract: Digital content creation is experiencing a profound change with the advent of deep generative models. For texturing, conditional image generators now allow the synthesis of realistic RGB images of a 3D scene that align with the geometry of that scene. For appearance modeling, SVBRDF prediction networks recover material parameters from RGB images. Combining these technologies allows us to quickly generate SVBRDF maps for multiple views of a 3D scene, which can be merged to form a SVBRDF texture atlas of that scene. In this paper, we analyze the challenges and opportunities for SVBRDF prediction in the context of such a fast appearance modeling pipeline. On the one hand, single-view SVBRDF predictions might suffer from multiview incoherence and yield inconsistent texture atlases. On the other hand, generated RGB images, and the different modalities on which they are conditioned, can provide additional information for SVBRDF estimation compared to photographs. We compare neural architectures and conditions to identify designs that achieve high accuracy and coherence. We find that, surprisingly, a standard UNet is competitive with more complex designs. Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation

</details>


### [16] [From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation](https://arxiv.org/abs/2512.13953)
*Dawid Malarz,Artur Kasymov,Filip Manjak,Maciej Zięba,Przemysław Spurek*

Main category: cs.CV

TL;DR: 提出“去品牌化（unbranding）”任务：细粒度移除图像中的商标与隐性品牌结构特征，同时保持语义一致；并构建数据集与基于VLM问答的评测指标，显示新一代扩散模型更易生成品牌标识，凸显问题紧迫性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型迅速发展导致商标与品牌要素被未经授权复制的风险上升。既有研究多聚焦风格或名人等一般概念，或仅检测显式logo，忽视了品牌识别的多维性（包含抽象外观/结构“trade dress”）。因此需要能去除显式与隐性品牌特征且保持内容连贯的方法与评测。

Method: 1）定义并提出“去品牌化”任务；2）构建覆盖商标与结构性品牌要素的基准数据集；3）提出基于视觉语言模型（VLM）的问答式评测指标，可同时探测显式logo与隐性整体品牌特征；4）对比不同扩散模型（如Stable Diffusion、SDXL、FLUX）在品牌标识合成上的倾向与难度。

Result: VLM问答指标验证：现有logo检测器无法捕捉抽象trade dress；更高保真度模型（SDXL、FLUX）比旧模型更易生成品牌标识；去品牌化被证明为独立且具现实意义的问题，需要专门技术解决。

Conclusion: 去品牌化是必要而紧迫的新任务；VLM驱动的评测为显式与隐性品牌识别提供有效度量；随着模型能力提升，未授权品牌复现风险加剧，呼吁社区开发专门的去品牌化技术与基准。

Abstract: The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car's front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.

</details>


### [17] [Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation](https://arxiv.org/abs/2512.13970)
*Miaohua Zhang,Mohammad Ali Armin,Xuesong Li,Sisi Liang,Lars Petersson,Changming Sun,David Ahmedt-Aristizabal,Zeeshan Hayder*

Main category: cs.CV

TL;DR: 提出一种无需重新训练扩散模型、在推理时生成高质量且高多样性的海上障碍物分割训练样本的管线，通过类感知风格库和自适应退火采样+COD比例控制，提升罕见与纹理敏感类别的多样性并显著提高多种骨干的分割性能。


<details>
  <summary>Details</summary>
Motivation: 海上场景存在阳光闪斑、雾气、剧烈波形变化等恶劣条件，图像质量下降且数据稀缺、结构重复，导致分割模型鲁棒性不足。现有掩码条件扩散虽能对齐布局，但在低熵掩码/提示下生成结果多样性不足，难以作为有效数据扩增来源。

Method: 提出“质量驱动、面向多样性”的推理时样本扩增框架，无需微调扩散模型：1) 类感知风格库（class-aware style bank）自动构造高熵、语义扎实的文本提示；2) 自适应退火采样（adaptive annealing sampler）在早期扩散阶段扰动条件；3) 由COD（可能为目标/边界一致性度量）引导的比例控制器调节扰动强度，在不破坏布局对齐的前提下提升多样性；整体在掩码条件下生成布局一致且风格多样的合成图像用于训练。

Result: 在多个海上障碍物数据集基准上，用该合成样本扩增训练集，可在多种分割骨干网络上稳定提升性能；同时明显增加罕见类别与纹理敏感类别的视觉变化度。

Conclusion: 通过高熵提示与受控条件扰动相结合的推理时数据合成，可兼顾布局保真与风格多样性，从而增强海上障碍物分割的鲁棒性，且方法通用、无需重新训练扩散模型。

Abstract: Marine obstacle detection demands robust segmentation under challenging conditions, such as sun glitter, fog, and rapidly changing wave patterns. These factors degrade image quality, while the scarcity and structural repetition of marine datasets limit the diversity of available training data. Although mask-conditioned diffusion models can synthesize layout-aligned samples, they often produce low-diversity outputs when conditioned on low-entropy masks and prompts, limiting their utility for improving robustness. In this paper, we propose a quality-driven and diversity-aware sample expansion pipeline that generates training data entirely at inference time, without retraining the diffusion model. The framework combines two key components:(i) a class-aware style bank that constructs high-entropy, semantically grounded prompts, and (ii) an adaptive annealing sampler that perturbs early conditioning, while a COD-guided proportional controller regulates this perturbation to boost diversity without compromising layout fidelity. Across marine obstacle benchmarks, augmenting training data with these controlled synthetic samples consistently improves segmentation performance across multiple backbones and increases visual variation in rare and texture-sensitive classes.

</details>


### [18] [XAI-Driven Diagnosis of Generalization Failure in State-Space Cerebrovascular Segmentation Models: A Case Study on Domain Shift Between RSNA and TopCoW Datasets](https://arxiv.org/abs/2512.13977)
*Youssef Abuzeid,Shimaa El-Bana,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: 该研究针对脑血管分割中最先进的状态空间模型（UMamba）在跨数据集迁移时性能崩塌的问题，提出两阶段方法：先量化源/目标域差异，再用Seg-XRes-CAM解释模型失败原因。结果显示Dice从0.8604降至0.2902，注意力在目标域偏离真实血管，IoU对GT极低而对自身错误预测仍对齐，证实模型学到伪相关与数据集偏置。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像落地受领域迁移影响显著，单靠整体性能指标难以定位失败根因。需要可解释AI手段来诊断模型在新域中为何失效，特别是对新兴架构（如SSM/UMamba）在血管分割场景的泛化问题进行机制层面剖析。

Method: 两阶段流程：1）域差评估：以RSNA CTA Aneurysm为源域、TopCoW CT为目标域，量化Z向分辨率与背景噪声等统计差异，并在源/目标上评估Dice。2）失效诊断：采用Seg-XRes-CAM生成注意力图，计算其与GT分割及模型预测掩膜的重叠（IoU），以量化模型关注是否对齐解剖结构或仅对齐自身预测。

Result: 源域Dice=0.8604，目标域Dice=0.2902；在阈值0.3时，注意力与GT的IoU≈0.101，注意力与模型预测的IoU≈0.282。说明在目标域中模型注意力偏离真实血管却仍与自身错误预测一致。

Conclusion: UMamba在跨域泛化失败的根因是注意力机制转向伪相关而非真实解剖特征。Seg-XRes-CAM等XAI方法能有效诊断并量化这种偏移，从而揭示数据集偏置，强调在临床部署前应结合解释性诊断与域稳健性改进。

Abstract: The clinical deployment of deep learning models in medical imaging is severely hindered by domain shift. This challenge, where a high-performing model fails catastrophically on external datasets, is a critical barrier to trustworthy AI. Addressing this requires moving beyond simple performance metrics toward deeper understanding, making Explainable AI (XAI) an essential diagnostic tool in medical image analysis. We present a rigorous, two-phase approach to diagnose the generalization failure of state-of-the-art State-Space Models (SSMs), specifically UMamaba, applied to cerebrovascular segmentation. We first established a quantifiable domain gap between our Source (RSNA CTA Aneurysm) and Target (TopCoW Circle of Willis CT) datasets, noting significant differences in Z-resolution and background noise. The model's Dice score subsequently plummeted from 0.8604 (Source) to 0.2902 (Target). In the second phase, which is our core contribution, we utilized Seg-XRes-CAM to diagnose the cause of this failure. We quantified the model's focus by measuring the overlap between its attention maps and the Ground Truth segmentations, and between its attention maps and its own Prediction Mask. Our analysis proves the model failed to generalize because its attention mechanism abandoned true anatomical features in the Target domain. Quantitative metrics confirm the model's focus shifted away from the Ground Truth vessels (IoU~0.101 at 0.3 threshold) while still aligning with its own wrong predictions (IoU~0.282 at 0.3 threshold). This demonstrates the model learned spurious correlations, confirming XAI is a powerful diagnostic tool for identifying dataset bias in emerging architectures.

</details>


### [19] [FocalComm: Hard Instance-Aware Multi-Agent Perception](https://arxiv.org/abs/2512.13982)
*Dereje Shenkut,Vijayakumar Bhagavatula*

Main category: cs.CV

TL;DR: FocalComm是一种专注于“困难样本”特征交换的多智能体协同感知框架，通过仅传递对降低漏检有用的关键中间特征并进行查询式加权融合，显著提升行人等小目标的3D检测，且在V2X-Real与DAIR-V2X上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知多优化车辆检测等大目标指标，忽视行人等小而关键目标，导致安全隐患；同时常进行全特征通信，带宽低效且对减少漏检帮助有限，需要一种能聚焦困难实例并高效通信的方案。

Method: 提出FocalComm，包括：1) 可学习的渐进式困难实例挖掘（HIM）模块，在每个智能体端提取与困难目标相关的特征；2) 基于查询的中间层特征级融合，对被识别为关键的特征进行动态加权，实现跨车-路协作时的重点对齐与融合。系统仅交换“难例导向”的稀疏关键特征而非完整特征图。

Result: 在V2X-Real与DAIR-V2X两大真实数据集、车载-路侧多种协作设定下均超越SOTA；在V2X-Real上对行人检测取得显著提升（漏检率降低）。

Conclusion: 面向难例的特征选择与查询式动态融合能在保持通信高效的同时，显著改善小目标/安全关键目标的3D检测，适用于车—车/车—路协同场景。

Abstract: Multi-agent collaborative perception (CP) is a promising paradigm for improving autonomous driving safety, particularly for vulnerable road users like pedestrians, via robust 3D perception. However, existing CP approaches often optimize for vehicle detection performance metrics, underperforming on smaller, safety-critical objects such as pedestrians, where detection failures can be catastrophic. Furthermore, previous CP methods rely on full feature exchange rather than communicating only salient features that help reduce false negatives. To this end, we present FocalComm, a novel collaborative perception framework that focuses on exchanging hard-instance-oriented features among connected collaborative agents. FocalComm consists of two key novel designs: (1) a learnable progressive hard instance mining (HIM) module to extract hard instance-oriented features per agent, and (2) a query-based feature-level (intermediate) fusion technique that dynamically weights these identified features during collaboration. We show that FocalComm outperforms state-of-the-art collaborative perception methods on two challenging real-world datasets (V2X-Real and DAIR-V2X) across both vehicle-centric and infrastructure-centric collaborative setups. FocalComm also shows a strong performance gain in pedestrian detection in V2X-Real.

</details>


### [20] [Repurposing 2D Diffusion Models for 3D Shape Completion](https://arxiv.org/abs/2512.13991)
*Yao He,Youngjoong Kwon,Tiange Xiang,Wenxiao Cai,Ehsan Adeli*

Main category: cs.CV

TL;DR: 提出一种利用2D扩散模型进行3D点云补全的框架，通过“Shape Atlas”将3D几何紧凑映射为2D表示，从而在有限3D数据下实现高质量、细节保真的补全，并在PCN与ShapeNet-55上验证有效，且可用于艺术家网格生成。


<details>
  <summary>Details</summary>
Motivation: 2D扩散模型因充足的2D数据而成功，但3D扩散受高质量3D数据稀缺与3D输入和2D潜空间的模态鸿沟限制。需要一种方法既能利用2D生成能力，又能缩小模态差距以提升条件生成效果。

Method: 提出“Shape Atlas”作为3D形状的紧凑2D表示：将不完整点云条件与目标输出统一到2D空间，直接对接预训练2D扩散模型，实现更有效的条件控制与学习；在此统一2D表征下进行训练与推理完成3D形状补全，并可从补全点云进一步生成可供艺术家使用的网格。

Result: 在PCN与ShapeNet-55数据集上实现高质量、细节保真的形状补全，展示了在有限3D数据下的强泛化与视觉质量；补全结果能成功用于下游的艺术家网格创建。

Conclusion: 通过Shape Atlas将3D到2D统一，充分利用2D扩散模型能力并缓解模态鸿沟，可在数据受限场景下实现优质3D形状补全，并具有实际创作应用价值。

Abstract: We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.

</details>


### [21] [Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models](https://arxiv.org/abs/2512.14008)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Sparse-LaViDa通过在推理过程中动态裁剪冗余被掩码的token，并用少量寄存器token替代其信息，同时用与裁剪一致的注意力掩码进行训练，使得掩码离散扩散模型在多模态任务中推理速度最高提升约2倍且质量基本不降。


<details>
  <summary>Details</summary>
Motivation: MDM在多模态理解、生成、编辑等任务表现强，但推理慢，因为每个采样步都要重复处理大量无效（被掩码）token，造成计算冗余。需要一种方法减少这部分计算而不损害生成质量，并保证训练/推理一致性。

Method: 1) 动态裁剪：在每个扩散采样步，识别并截断不必要的被掩码token，从计算图中移除；2) 寄存器token：为被截断区域引入少量专用“寄存器”token，作为其紧凑表示以保留必要信息与上下文交互；3) 一致性注意力：设计与采样时裁剪过程严格匹配的注意力mask，在训练中模拟推理时的稀疏访问模式，减少训练-推理偏移；4) 框架落地：在统一MDM骨干LaViDa-O上实现，适配多任务。

Result: 在文本生成图像、图像编辑、数学推理等多种任务上，相比原LaViDa-O，实现最高约2倍的推理加速，同时保持或接近原有生成质量指标。

Conclusion: 稀疏化处理被掩码token并辅以寄存器token与一致性注意力，可显著降低MDM采样计算且不牺牲质量；Sparse-LaViDa为多模态MDM提供了通用、高效的推理加速方案。

Abstract: Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.

</details>


### [22] [KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding](https://arxiv.org/abs/2512.14017)
*Zongyao Li,Kengo Ishida,Satoshi Yamazaki,Xiaotong Ji,Jianquan Liu*

Main category: cs.CV

TL;DR: KFS-Bench 提出首个用于长视频问答关键帧采样的评测基准，提供多场景标注以直接评估采样策略，并给出新的采样质量指标与自适应平衡采样方法，显著提升采样与QA表现。


<details>
  <summary>Details</summary>
Motivation: 现有长视频QA通常以最终问答准确率间接评价关键帧选取，无法直接衡量采样是否覆盖必需场景与信息；长视频理解成本高，需高效且信息充分的帧选择机制。

Method: 1) 构建KFS-Bench：为每个问题提供需要的多个互不相交的场景标注，支持直接评估采样对关键内容的覆盖与平衡；2) 系统性比较多种关键帧采样方法；3) 设计综合考虑采样精度、场景覆盖度与采样平衡性的质量指标，并验证其与QA准确率的相关性；4) 提出基于问题-视频相关性的自适应平衡采样，在保持与问题相似度的同时提升多样性与场景覆盖。

Result: 实验显示：场景覆盖和采样平衡与精度同等关键；新提出的采样质量指标与QA准确率高度相关；所提出的自适应平衡采样在关键帧选择质量与QA性能上均优于现有方法。

Conclusion: 直接场景级标注使关键帧采样的评估更可靠；覆盖、平衡与精度共同决定长视频QA表现。提出的指标和方法在KFS-Bench上取得领先，推动高效长视频理解研究。

Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.

</details>


### [23] [Deep Learning Perspective of Scene Understanding in Autonomous Robots](https://arxiv.org/abs/2512.14020)
*Afia Maham,Dur E Nayab Tashfa*

Main category: cs.CV

TL;DR: 综述深度学习在机器人场景理解中的应用，涵盖检测、分割、深度估计、三维重建与视觉SLAM，强调对传统几何方法的补充与提升，并讨论集成于动态非结构环境时的效能与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统基于几何与手工特征的方法在遮挡、纹理缺失与动态环境中表现受限，难以提供实时、鲁棒且具语义的环境理解。深度学习的发展为机器人提供了更强的感知与语义推理能力，亟需系统性梳理现状、优势、挑战与趋势。

Method: 面向自动机器人场景理解的综述性研究：系统回顾并对比深度学习方法在目标检测、语义/实例分割、深度估计、三维重建与视觉SLAM中的创新；分析其如何弥补传统几何模型不足，并讨论在动态与非结构化场景中的集成与应用。

Result: 总结了深度学习在多感知模块中的性能改进：提升遮挡与纹理缺失条件下的深度与语义感知，强化实时性与环境语义理解；展示了这些模块集成后在决策、导航与交互方面的有效性；梳理了现存问题与瓶颈。

Conclusion: 深度学习显著提升了机器人场景理解能力，尤其在复杂动态环境中的鲁棒性与语义推理，但仍存在泛化、实时性、数据与标注成本、跨模态融合与可解释性等挑战。未来需在轻量化与高效训练、弱/自监督学习、通用3D表示、与SLAM深度融合、以及安全与不确定性估计等方向深入研究。

Abstract: This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.

</details>


### [24] [Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers](https://arxiv.org/abs/2512.14026)
*Yibing Fu,Yunpeng Zhao,Zhitao Zeng,Cheng Chen,Yueming Jin*

Main category: cs.CV

TL;DR: 提出CITab跨表自监督多模态框架，利用列名语义与P‑MoLin专化层，在多队列阿尔茨海默病任务上优于SOTA，提升可迁移与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图像+表格的SSL多局限于单一队列，因表格建模对异质特征不灵活，跨数据集难以共享与迁移医学知识；需要一种能跨不同表结构与来源进行预训练的机制。

Method: 1) 语义感知表格建模：将列标题作为语义线索，统一不同队列的异构表格空间，实现跨表对齐与可扩展预训练；2) 提出P‑MoLin（prototype‑guided mixture‑of‑linear）：以原型引导的线性专家混合，对表格特征进行专化与概念挖掘；3) 与医学影像分支进行多模态SSL预训练，实现跨数据源知识学习。

Result: 在3个公开AD队列、共4,461名受试者上评测，用于阿尔茨海默病诊断任务，CITab在多项指标上优于现有最先进方法，表现出更强的可迁移性与鲁棒性。

Conclusion: 通过引入列名语义和P‑MoLin实现跨表自监督多模态学习，CITab提升了医学知识的可迁移与预训练的可扩展性，为跨队列临床决策建模提供有效范式。

Abstract: Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.

</details>


### [25] [Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding](https://arxiv.org/abs/2512.14028)
*Jiaheng Li,Qiyu Dai,Lihan Li,Praneeth Chakravarthula,He Sun,Baoquan Chen,Wenzheng Chen*

Main category: cs.CV

TL;DR: 提出一种基于学习的单帧结构光深度解码框架，用特征空间匹配+几何先验成本体积与单目深度先验细化，在只用合成数据训练情况下，对多种投影图案和真实室内场景泛化良好，显著优于传统像素域解码、商业结构光与被动立体方法。


<details>
  <summary>Details</summary>
Motivation: 单帧结构光在消费级3D传感（如Face ID、RealSense）广泛使用，但传统像素域匹配在遮挡、细纹理和非朗伯表面下鲁棒性差。神经特征匹配在视觉任务中表现强，若能将其引入结构光解码，或可提升对应关系估计与深度质量。

Method: 1) 学习式解码：从投影图案与IR捕获图中提取神经特征；2) 融合几何先验，在特征空间构建cost volume进行对应匹配而非像素域匹配；3) 深度细化模块，结合大规模单目深度估计模型的强先验，提升细节与整体结构一致性；4) 物理结构光渲染管线，合成近百万对图案-图像数据，涵盖多物体与材料，用于室内训练；5) 设计支持多种结构光图案，测试可零微调泛化。

Result: 仅用合成数据训练，在真实室内场景中对多种投影图案均可直接应用，鲁棒处理遮挡、细结构与非朗伯表面；定量与定性均显著优于：传统像素域结构光解码、商业结构光设备，以及被动RGB立体深度方法。

Conclusion: 将结构光对应解码迁移到特征空间并注入几何与单目深度先验，可显著提升单帧结构光的鲁棒性与精度；所建物理渲染数据集使模型具备跨图案与跨域泛化能力，具备实用潜力。

Abstract: We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.

</details>


### [26] [ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM](https://arxiv.org/abs/2512.14032)
*Ignacio Alzugaray,Marwan Taher,Andrew J. Davison*

Main category: cs.CV

TL;DR: 提出一套基于场景坐标回归（SCR）的实时神经RGB-D SLAM，使用轻量网络将2D特征直接映射到3D全局坐标，实现低内存隐式地图、极快重定位与隐私友好，并在合成与真实数据上达到具竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式SLAM常受制于地图表示冗余、重定位速度慢、隐私风险与实时性不足；需要一种既紧凑高效又能实时运行、且在动态环境中稳健的隐式地图表示。

Method: 以SCR作为核心隐式地图：训练轻量网络从图像特征直接回归到3D全局坐标；设计专用的SCR网络结构与关键系统集成策略，使其在在线SLAM中支持稀疏/稠密特征，结合RGB-D信息实现位姿估计与映射；强调实时运行与对动态场景的鲁棒性。

Result: 系统成为首个严格实时的神经隐式RGB-D SLAM；在标准合成与真实基准上取得与SOTA相竞争的精度/鲁棒性；展现低内存占用、极快重定位以及无需特殊处理即可在动态环境中稳定运行。

Conclusion: SCR作为隐式地图在神经SLAM中切实可行且优势明显：紧凑高效、实时、隐私友好、快速重定位、对动态性稳健；所提框架简单灵活，可作为神经隐式SLAM的有效新范式。

Abstract: We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.
  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam

</details>


### [27] [ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization](https://arxiv.org/abs/2512.14039)
*Meng Wei,Cheng Zhang,Jianmin Zheng,Hamid Rezatofighi,Jianfei Cai*

Main category: cs.CV

TL;DR: 提出ASAP Textured Gaussians，通过自适应采样与误差驱动的各向异性参数化，显著减少纹理参数量同时保持高保真渲染。


<details>
  <summary>Details</summary>
Motivation: 现有给3D高斯加纹理的方案虽提升外观建模与下游任务，但带来内存效率问题：纹理在规范空间采样导致对低贡献区域浪费容量；且所有高斯统一纹理参数化，忽视视觉复杂度差异，出现过度参数化。

Method: 不更换纹理表述，而是优化使用方式：1）依据高斯密度分布进行自适应采样，将采样预算集中在贡献高的区域；2）基于渲染误差进行各向异性参数化，自适应分配纹理分辨率/通道等资源给复杂度高的高斯，实现按需增减参数。整体形成ASAP（Adaptive Sampling and Anisotropic Parameterization）。

Result: 在显著减少纹理参数数量与内存占用的同时，获得更高或相当的渲染保真度，质量-效率权衡显著优于现有纹理化高斯方法。

Conclusion: 通过密度引导的采样与误差驱动的各向异性参数分配，可避免纹理容量浪费与过度参数化，实现更高效的3D高斯纹理建模，为高保真渲染与下游任务提供轻量解决方案。

Abstract: Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.

</details>


### [28] [ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning](https://arxiv.org/abs/2512.14040)
*Boran Wang,Xinming Wang,Yi Chen,Xiang Li,Jian Xu,Jing Yuan,Chenglin Liu*

Main category: cs.CV

TL;DR: 提出ChartAgent：基于工具集成推理（TIR）的图表理解代理，能将复杂图表分析分解为可观察、可复现实例的步骤，动态调用多种视觉工具，并输出可追溯的证据包，从而在缺失文字标注时仍保持稳健表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在图表理解上对文本标注高度依赖，一旦关键数字或说明缺失，性能显著下降，缺乏透明可验证的推理过程。

Method: 设计ChartAgent框架：采用Tool-Integrated Reasoning把图表分析拆分为逐步的可观测流程；配套可扩展的模块化工具库（关键元素检测、实例分割、OCR等十余种核心工具）；通过代理动态编排这些工具以实现系统化的视觉解析；标准化并整合中间结果为结构化“证据包”，支持最终结论的可追溯与可复现。

Result: 在稀疏标注（缺少文字/数字）条件下显著提升鲁棒性与性能，相较黑箱式MLLM方案更可靠；实验验证其有效性与可扩展性。

Conclusion: ChartAgent以可解释、可验证的工具集成推理替代黑箱式图表理解路线，提供可追溯证据与更强的稳健性，为可信、可扩展的图表理解系统提供实用路径。

Abstract: With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.

</details>


### [29] [OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving](https://arxiv.org/abs/2512.14044)
*Zhenguo Zhang,Haohan Zhen,Yishen Wang,Le Xu,Tianchen Deng,Xuefeng Chen,Qu Chen,Bo Zhang,Wuxiong Huang*

Main category: cs.CV

TL;DR: 提出OmniDrive-R1：面向自动驾驶的端到端视觉语言模型，通过交错式多模态CoT与强化学习驱动的可视化对齐，显著缓解幻觉并大幅提升推理与答案准确率。


<details>
  <summary>Details</summary>
Motivation: VLM在自动驾驶等安全关键场景中易出现目标幻觉，主要因依赖未落地的文本CoT；现有多模态CoT存在感知与推理割裂、依赖昂贵稠密标注两大痛点。

Method: 构建端到端框架OmniDrive-R1：引入交错式多模态CoT（iMCoT）统一感知与推理；设计两阶段纯强化学习范式与Clip-GRPO算法，实现“放大/聚焦”式视觉引导；提出免标注、过程级的grounding奖励，约束视觉聚焦与文本推理的实时跨模态一致性，避免外部工具不稳定。

Result: 在DriveLMM-o1上，相比Qwen2.5VL-7B基线，综合推理分由51.77%升至80.35%，最终答案准确率由37.81%升至73.62%，显示显著性能增益与更少幻觉。

Conclusion: 端到端iMCoT结合强化学习的视觉grounding与免标注奖励，可有效减少自动驾驶VLM的幻觉并提升推理与答案质量，为安全关键场景提供更可靠的多模态推理方案。

Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.

</details>


### [30] [SELECT: Detecting Label Errors in Real-world Scene Text Data](https://arxiv.org/abs/2512.14050)
*Wenjun Liu,Qian Wu,Yifeng Hu,Yuke Li*

Main category: cs.CV

TL;DR: 提出SELECT与SSLC，用多模态编码与相似度驱动的标签扰动来检测并纠正场景文本数据集中的标签错误，显著提升错误检测与STR性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景文本识别数据集中普遍存在标签错误（长度变化、字符级错标、序列错位），现有方法难以准确发现并处理这类可变长度与字符相似性相关的错误，影响模型训练与下游识别性能。

Method: 1) SELECT：使用图文联合编码器与字符级分词器，将图像与文本序列对齐建模，显式处理可变长度序列与字符级误差；以多模态相似度与对齐得分来判别潜在错标。2) SSLC：在训练中基于视觉相似度对字符进行扰动，引入长度变化与相似字符替换，合成贴近真实的错误模式，提升检测器鲁棒性与泛化。

Result: 在真实世界场景文本数据集上，SELECT在标签错误检测的准确性上超过现有方法；同时，通过剔除/纠正错误标注，进一步提升了场景文本识别（STR）的准确率。

Conclusion: SELECT是首个能有效处理可变长度标签并检测真实场景文本标签错误的方法；结合SSLC，既能高效发现错误又能提升下游STR性能，具备实际应用价值。

Abstract: We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.

</details>


### [31] [HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices](https://arxiv.org/abs/2512.14052)
*HyperAI Team,Yuchen Liu,Kaiyang Han,Zhiqiang Xia,Yuhang Dong,Chen Song,Kangyu Tang,Jiaming Xu,Xiushi Feng,WenXuan Yu,Li Peng,Mingyang Wang,Kai Wang,Changpeng Yang,Yang Li,Haoyu Lu,Hao Wang,Bingna Xu,Guangyao Liu,Long Huang,Kaibin Guo,Jinyang Wu,Dan Wu,Hongzhen Wang,Peng Zhou,Shuai Nie,Shande Wang,Runyu Shi,Ying Huang*

Main category: cs.CV

TL;DR: HyperVL是一种面向端侧推理的高效多模态大模型，通过图像切片、可变分辨率编码与多尺度一致性训练，兼顾精度与速度，显著降低延迟与功耗并在同规模模型中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型感知与推理强但算力与内存消耗高，不适合端侧部署；小模型虽在通用能力上进步，但标准ViT在高分辨率下仍是延迟与内存瓶颈，需要一种既能高效处理高分辨率视觉输入、又保持性能的端侧方案。

Method: 提出HyperVL：1) 图像切片以限制峰值显存；2) 视觉分辨率压缩器（VRC）自适应预测最优编码分辨率，减少冗余计算；3) 双重一致性学习（DCL）在同一LLM下对齐多尺度ViT编码器，支持不同视觉分支动态切换；整体在统一框架中进行训练与推理。

Result: 在多项基准上，在相近参数规模下达到或优于现有方法的SOTA；在真实移动设备上大幅降低推理延迟与功耗，满足端侧推理需求。

Conclusion: HyperVL通过自适应分辨率与多尺度一致性，将高效视觉编码与端侧可部署性结合，在保证精度的同时显著降低资源开销，为移动端多模态推理提供了实用方案。

Abstract: Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.

</details>


### [32] [FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling](https://arxiv.org/abs/2512.14056)
*Kim Sung-Bin,Joohyun Chang,David Harwath,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出将说话人脸编辑与生成统一为“语音条件的人脸运动补全”任务，并用Diffusion Transformer（FacEDiT）+流匹配训练，通过掩码重建实现可局部编辑与生成；并发布编辑基准FacEDiTBench，实验显示方法在编辑与生成上均具备高保真、语音对齐与平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 现有工作将说话人脸编辑与生成割裂处理，缺乏统一视角与标准编辑数据集/指标，导致方法难以兼顾局部编辑、身份保持、口型同步与时间连续性。作者希望用一个自监督预训练任务同时覆盖编辑与生成，并建立评测基准。

Method: 将任务表述为“语音条件的人脸运动补全”：对人脸运动序列进行掩码，使用语音与上下文运动作为条件，预测被遮挡部分。模型为基于扩散的Transformer（FacEDiT），以flow matching训练；引入偏置注意力与时间平滑约束以增强边界连续性和唇形同步；支持替换、插入、删除等局部编辑；并构建FacEDiTBench数据集与新的评估指标。

Result: 在多项实验中，FacEDiT在说话人脸编辑任务上实现精确、与语音对齐的编辑，保持身份一致与视觉连续；同时具备良好的泛化，能胜任说话人脸生成。新基准涵盖多种编辑类型与长度，验证了统一任务设定的有效性。

Conclusion: 将说话人脸编辑与生成统一为语音条件运动补全是有效范式。FacEDiT通过掩码重建与流匹配扩散Transformer，在编辑与生成上兼顾口型同步、身份保持与边界平滑；FacEDiTBench为该方向提供了首个系统性评测平台。

Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.

</details>


### [33] [Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning](https://arxiv.org/abs/2512.14058)
*Zulin Zhuang,Yu Bian*

Main category: cs.CV

TL;DR: 提出一种从侧窗图像实时预测室内工作面照度分布的多模态深度学习框架；只看窗区像素以适应动态占用；在广州实测数据上R²达0.98/0.82、RMSE<0.14/0.17，具高精度与时序泛化性，可用于日光联动控制节能。


<details>
  <summary>Details</summary>
Motivation: 现有室内日光预测多基于静态场景且常需室内传感或全面成像，不利于动态占用空间的非侵入、实时控制。为实现DLC在充足日光条件下的高效节能，需准确、低干扰地实时预测工作面照度分布，并具备跨时间的泛化能力。

Method: 构建多模态深度学习框架，融合时空特征：仅从侧采光窗区域提取图像特征（避免采集室内人员隐私与动态干扰），结合时间序列处理以捕捉日光变化，输出工作面照度分布。开展广州试验房现场采集，共收集17,344个样本用于训练与验证；设置同分布测试集与“未见日”测试集检验时序泛化。

Result: 在同分布测试集上R²>0.98、RMSE<0.14；在未见日测试集上R²>0.82、RMSE<0.17，显示高精度与可接受的时间泛化能力。

Conclusion: 该非侵入、基于窗区图像的多模态深度学习方法可实时、准确预测室内工作面照度分布，适合与DLC集成以实现节能控制，并在跨日场景中保持可接受的泛化；有望在动态占用空间推广应用。

Abstract: Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.

</details>


### [34] [Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution](https://arxiv.org/abs/2512.14061)
*Hao Chen,Junyang Chen,Jinshan Pan,Jiangxin Dong*

Main category: cs.CV

TL;DR: 提出CODSR：一种可控的一步扩散超分模型，通过LQ特征调制、区域自适应先验激活与文本匹配引导，提升感知质量并保持竞争性的保真度，且推理高效。


<details>
  <summary>Details</summary>
Motivation: 现有一步扩散SR方法存在：1) LQ压缩导致信息缺失，保真度差；2) 生成先验缺乏对区域的区分激活，易牺牲结构；3) 文本提示与图像语义区域错配，条件利用不足。

Method: 设计CODSR，包含三点：a) LQ引导的特征调制模块，直接利用未压缩的LQ信息为扩散过程提供高保真条件；b) 区域自适应的生成先验激活，按区域动态增强感知细节，避免破坏局部结构；c) 文本匹配引导策略，使文本提示与对应语义区域对齐，充分发挥文本条件能力。

Result: 在大量实验中，CODSR在感知质量上显著优于SOTA，并在保真度上保持竞争力，同时实现一步推理的高效率。

Conclusion: CODSR通过三项关键设计克服了一步扩散SR的核心瓶颈，实现了高感知质量与良好保真度的兼顾，并具备高效推理优势。

Abstract: Recent diffusion-based one-step methods have shown remarkable progress in the field of image super-resolution, yet they remain constrained by three critical limitations: (1) inferior fidelity performance caused by the information loss from compression encoding of low-quality (LQ) inputs; (2) insufficient region-discriminative activation of generative priors; (3) misalignment between text prompts and their corresponding semantic regions. To address these limitations, we propose CODSR, a controllable one-step diffusion network for image super-resolution. First, we propose an LQ-guided feature modulation module that leverages original uncompressed information from LQ inputs to provide high-fidelity conditioning for the diffusion process. We then develop a region-adaptive generative prior activation method to effectively enhance perceptual richness without sacrificing local structural fidelity. Finally, we employ a text-matching guidance strategy to fully harness the conditioning potential of text prompts. Extensive experiments demonstrate that CODSR achieves superior perceptual quality and competitive fidelity compared with state-of-the-art methods with efficient one-step inference.

</details>


### [35] [SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding](https://arxiv.org/abs/2512.14068)
*Shuang Cheng,Yuhua Jiang,Zineng Zhou,Dawei Liu,Wang Tao,Linfeng Zhang,Biqing Qi,Bowen Zhou*

Main category: cs.CV

TL;DR: SDAR-VL提出高效稳定的块式离散扩散训练框架，在多模态理解上缩短训练、稳收敛并提升性能，达到或超越强AR与扩散基线。


<details>
  <summary>Details</summary>
Motivation: 块式离散扩散兼具并行生成与因果依赖建模潜力，但在VLU中实际落地受限于高训练成本、慢收敛与不稳定，落后于强AR模型。作者欲解决这些瓶颈，使块扩散成为实用主干。

Method: 提出SDAR-VL框架，整合三项策略：1) 异步块式噪声调度，在同一批次内多样化监督；2) 有效遮罩比例缩放，在随机遮罩下实现无偏的损失归一化；3) 渐进式β噪声课程学习，在保持扰动多样性的同时逐步增大有效遮罩覆盖。

Result: 在21个单图、多图与视频基准上，SDAR-VL较常规块扩散在训练效率、收敛稳定性与任务性能上均显著提升；在相同设置下达到扩散类SOTA，并匹配/超越LLaVA-OneVision和LLaDA-V。

Conclusion: 通过系统的训练改进，块式离散扩散成为实用的VLU骨干，兼具效率、稳定与高性能，为与或替代AR范式提供可行路径。

Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.

</details>


### [36] [GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants](https://arxiv.org/abs/2512.14087)
*Yang Yang,Risa Shinoda,Hiroaki Santo,Fumio Okura*

Main category: cs.CV

TL;DR: 提出GaussianPlant：在3D高斯溅射基础上，分离结构与外观，利用结构原语（枝为圆柱、叶为圆盘）和外观原语（3D高斯）联合优化，实现植物的高保真外观与可解释结构重建。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS擅长新视角合成的外观重建，但缺乏显式结构表达（如枝叶拓扑），限制了植物表型分析等需要结构信息的应用。需要一种既保留3DGS外观质量又能恢复内部结构的表示。

Method: 构建分层表示GaussianPlant：用结构原语StP显式建模枝（圆柱）与叶（圆盘），并通过自组织方式优化其类别属性；用外观原语ApP（3D高斯）绑定到各StP以表征对应外观。通过基于多视角重渲染损失联合优化StP与ApP，并利用绑定关系让ApP的梯度反向影响StP，以实现结构与外观的协同学习。

Result: 在合成与真实数据上定性评估显示：方法实现了高保真外观重建（由ApP提供）与准确结构重建（由StP提供），可以提取枝结构与叶实例。

Conclusion: 通过将结构与外观解耦并联合优化，GaussianPlant弥补了3DGS在结构表达上的缺陷，同时保持其外观重建优势，为植物表型等需要可解释几何的任务提供了有效表示。

Abstract: We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.

</details>


### [37] [ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes](https://arxiv.org/abs/2512.14092)
*Felix Holm,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 提出ProtoFlow：一种学习动态场景图原型的可解释外科流程识别框架，在CAT-SG上优于基线，少样本也稳健，能解释子技术和异常。


<details>
  <summary>Details</summary>
Motivation: 外科AI需要细粒度流程识别，但标注昂贵、数据稀缺、方法缺乏可解释性。场景图可结构化表示外科事件，但尚未充分利用。作者希望在数据受限情形下提升性能并提供临床可解释性。

Method: 采用GNN编码器-解码器框架：先进行自监督预训练获取丰富表示，再通过基于原型的微调阶段学习和精炼“动态场景图原型”，这些原型概括反复出现且具临床意义的交互模式，用于解释与推断外科工作流。

Result: 在细粒度CAT-SG数据集上，ProtoFlow整体准确率优于标准GNN基线；在少样本、甚至仅1个手术视频训练时仍保持强性能。定性分析显示，学到的原型能识别不同外科子技术，并清晰揭示流程偏差与罕见并发症。

Conclusion: ProtoFlow将稳健表征学习与内在可解释性结合，提升透明度、可靠性与数据效率，有望加速其在外科培训、实时决策支持与流程优化中的临床落地。

Abstract: Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.
  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.
  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.
  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.

</details>


### [38] [Quality-Aware Framework for Video-Derived Respiratory Signals](https://arxiv.org/abs/2512.14093)
*Nhi Nguyen,Constantino Álvarez Casado,Le Nguyen,Manuel Lage Cañellas,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 提出一个质量感知的视频呼吸频率估计框架，融合多源信号并动态评估可靠性，在三套公开数据集上优于单一方法。


<details>
  <summary>Details</summary>
Motivation: 视频RR估计常因不同提取方法的信号质量不一致而不稳定，需要一种能在多源异构信号中识别并利用高质量片段、提升鲁棒性与可泛化性的方案。

Method: 从人脸rPPG、上半身运动及深度学习三类管线共提取10路信号；用Welch、MUSIC、FFT和峰值检测四类谱/峰算法求RR候选；对片段提取质量指标并训练机器学习模型，预测每路信号的准确性或选择最可靠信号，实现自适应信号融合与基于质量的片段过滤。

Result: 在OMuSense-23、COHFACE、MAHNOB-HCI三数据集上，整体上该框架的RR误差低于各单独方法，改进幅度依数据集特性而异。

Conclusion: 质量驱动的预测建模与自适应融合能提升视频RR估计的可靠性与可扩展性，具备跨数据集的泛化潜力。

Abstract: Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.

</details>


### [39] [AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation](https://arxiv.org/abs/2512.14095)
*Sisi Dai,Kai Xu*

Main category: cs.CV

TL;DR: 提出AnchorHOI：利用图像与视频扩散模型的混合先验，并通过“锚点”蒸馏策略，在两步流程中生成更具交互感与多样性的4D人-物交互；用NeRF锚点负责交互构型、关键点锚点负责真实运动。


<details>
  <summary>Details</summary>
Motivation: 监督式4D HOI依赖稀缺的大规模4D数据集，难以扩展；零样本方法多依赖图像扩散模型，交互线索蒸馏不足，导致在复杂场景与动作组合上的泛化受限。

Method: 提出AnchorHOI框架：引入视频扩散模型与图像扩散模型的混合先验；设计“锚点式先验蒸馏”二步生成策略。1) 交互感构型：构建anchor NeRF以捕捉人与物的相对布局与几何/外观耦合；2) 运动合成：以anchor关键点（人体/物体关键点轨迹）引导生成真实且可控的姿态与组合运动。通过在高维4D空间外进行可控的中间锚点优化，降低直接优化难度。

Result: 在多项实验中，相比以往方法在多样性与泛化能力上显著提升，能在更多场景与交互中产生更逼真的4D HOI结果。

Conclusion: 混合先验（视频+图像扩散）结合锚点式蒸馏，可有效缓解4D HOI数据稀缺与高维优化难题；通过NeRF与关键点锚点的两阶段引导，实现更优的交互构型与真实运动，从而在零样本4D HOI生成中取得领先表现。

Abstract: Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.

</details>


### [40] [OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration](https://arxiv.org/abs/2512.14096)
*Ruitong Sun,Tianze Yang,Wei Niu,Jin Sun*

Main category: cs.CV

TL;DR: 提出 OUSAC：通过“可变CFG引导+自适应缓存”系统性加速DiT，同时保持/提升生成质量；在多个基准上显著降算与提质。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因迭代去噪而计算昂贵；CFG提升质量/可控性但将前向成本近乎翻倍。需要在不牺牲质量的前提下减少采样步数与CFG调用次数，并解决动态引导尺度导致的缓存失效问题。

Method: 两阶段框架：1）进化搜索联合优化“哪些步跳过CFG”和“各步使用的引导尺度”，利用可变尺度补偿跳步以稀疏化计算；2）针对变尺度引入的去噪偏移，提出自适应Rank分配的校准缓存，对不同Transformer Block按影响程度分配低秩校准容量，维持缓存有效性。

Result: 在DiT-XL/2 (ImageNet 512x512)上节省53%计算并提升15%质量；PixArt-alpha (MSCOCO)上节省60%并提升16.1%；在FLUX上实现5倍加速且超过50步基线的CLIP分数；最多可消除82%的无条件分支前向。

Conclusion: 可变CFG引导与自适应缓存相结合能在扩散Transformer中实现显著加速且提升质量；跨模型/数据集泛化良好，优于现有加速方法，证明按步与按层的细粒度优化是有效方向。

Abstract: Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.

</details>


### [41] [ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models](https://arxiv.org/abs/2512.14099)
*Ruishu Zhu,Zhihao Huang,Jiacheng Sun,Ping Luo,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出ViewMask-1-to-3：用离散扩散+掩码预测，在无需3D先验与多视角专模的情况下，从单图与文本生成多视角图像，并在GSO与3D-FUTURE上以更高PSNR/SSIM、更低LPIPS领先。


<details>
  <summary>Details</summary>
Motivation: 现有多视角生成难以保证跨视角几何一致性，常依赖3D-aware架构、复杂几何先验与大量多视角数据，训练与部署成本高、泛化受限。作者希望用更简单通用的框架，在不引入显式3D约束的前提下实现稳定的一致性与高质量生成。

Method: 把多视角合成表述为离散序列建模：先用MAGVIT-v2把每个视角图像离散为视觉token；统一文本与视觉，通过掩码token预测（masked token prediction）实现迭代解码；采用随机掩码与自注意力在不同视角token之间传播信息，实现跨视角一致；不使用连续潜空间扩散、3D几何约束或特化注意力结构。

Result: 在GSO与3D-FUTURE基准上，平均指标排名第一：PSNR、SSIM更高、LPIPS更低；同时保持架构简洁，无需3D-aware模块或复杂几何先验。

Conclusion: 离散扩散结合掩码预测为多视角生成提供了一条简单而有效的替代路径，可在缺少显式3D建模的情况下实现良好的跨视角一致性与视觉质量，具备更低的系统复杂度与更强的实用性。

Abstract: Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.

</details>


### [42] [Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries](https://arxiv.org/abs/2512.14102)
*Emanuele Mezzi,Gertjan Burghouts,Maarten Kruithof*

Main category: cs.CV

TL;DR: 提出RUNE：用LLM把文本转成一阶逻辑，再由神经符号推理在检测到的实体上进行显式推理以完成遥感图文检索；通过逻辑分解提速，并在改造的DOTA与新指标RRQC/RRIU上超越RS-LVLMs，兼顾性能、鲁棒性与可解释性，含洪灾检索用例。


<details>
  <summary>Details</summary>
Motivation: 现有RS-LVLMs依赖隐式联合嵌入，可解释性弱、复杂空间关系处理差，难以满足实际遥感检索需求。需要一种既能表达复杂关系又可解释、可扩展的检索方法。

Method: 1) 用LLM将自然语言查询解析为一阶逻辑(FOL)；2) 基于目标检测得到的实体与其属性/关系；3) 在实体图上进行神经符号推理，判断图像是否满足FOL；4) 逻辑分解策略：在条件化的实体子集上并行/分块推理以缩短运行时间；5) 仅将基础模型用于文本到逻辑的生成，检索推理由符号模块完成；6) 构建评测：将DOTA增强为含更复杂查询的数据集。

Result: RUNE在复杂查询与不确定图像条件下优于SOTA RS-LVLM联合嵌入模型；LLM在文本到逻辑转换上有效；提出RRQC与RRIU两项指标衡量对查询复杂度与图像不确定性的鲁棒性；实际洪灾后卫星图检索用例证明可用性。

Conclusion: 神经符号与LLM结合的显式推理框架能提升RS图文检索的性能、鲁棒性与可解释性，并具备可扩展性与实际应用潜力。

Abstract: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.

</details>


### [43] [Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach](https://arxiv.org/abs/2512.14113)
*Ashish Mishra,Gyanaranjan Nayak,Tarun Kumar,Arpit Shah,Suparna Bhattacharya,Martin Foltin*

Main category: cs.CV

TL;DR: 提出一种无需训练与数据的CLIP遗忘框架：通过多模态零空间与文本提示+合成视觉原型，实现全域、域特定和选择性域的类别遗忘，同时保持其余能力。


<details>
  <summary>Details</summary>
Motivation: 现实中常需从预训练多模态模型中移除特定类别（合规、隐私、版权等），但不希望重新训练或影响无关任务。现有方法多依赖再训练、需额外数据、成本高、易伤及无关知识，缺乏灵活的域级控制。

Method: 在CLIP联合嵌入空间中构建“多模态零空间”：将文本提示与从嵌入中合成的视觉原型协同使用，学习（或解析式构造）能消除目标类信息的子空间投影/屏蔽算子。该算子支持三种遗忘范式：1) 全域遗忘指定对象；2) 仅移除特定域（如素描）中的该类；3) 在选择的域中完全忘却并在其他域保留。过程无需额外数据与再训练。

Result: 在多种视觉域（自然图像、艺术、抽象）上的零样本分类中，目标类被显著抑制，同时非目标类与无关域的准确率基本保持；相较再训练式遗忘方法，代价更低、干扰更小，并具备更细粒度的域控制。

Conclusion: 利用CLIP多模态零空间可实现灵活、数据与训练均免的受控遗忘，兼顾目标类彻底移除与其他能力保留，优于传统再训练方案，适用于合规与定制化部署场景。

Abstract: Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or "unlearning") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.

</details>


### [44] [MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction](https://arxiv.org/abs/2512.14114)
*Rui-Yang Ju,KokSheik Wong,Yanlin Jin,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: 提出MFE-GAN，用多尺度特征提取（含Haar小波与归一化）在进入GAN前处理文档图像，单一模型同时做增强与二值化，在Benchmark/Nabuco/CMATERdb上以接近SOTA精度显著缩短训练与推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法常为彩色文档各通道分别训练多个GAN去阴影/降噪，带来训练与推理成本高、部署复杂的问题；直接在退化彩色文档上做OCR效果差，需高效的预处理框架。

Method: 设计MFE-GAN框架：在输入端引入Haar小波变换与归一化进行多尺度特征提取；提出新的生成器、判别器架构与损失函数，统一进行文档增强与二值化；通过消融实验验证各组件贡献。

Result: 在Benchmark、Nabuco与CMATERdb数据集上，相比多GAN通道方法，显著降低总训练与推理时间，同时在指标上与SOTA保持相当（可比）表现。

Conclusion: 通过引入HWT多尺度预处理与改进的GAN设计，可用单一高效模型替代多通道GAN方案，实现文档图像增强与二值化的高效处理，在不牺牲精度前提下大幅提升效率，并具有实际部署价值。

Abstract: Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.

</details>


### [45] [SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance](https://arxiv.org/abs/2512.14121)
*Wenbo Tian,Ruting Lin,Hongxian Zheng,Yaodong Yang,Geng Wu,Zihao Zhang,Zhang Zhang*

Main category: cs.CV

TL;DR: 提出SportsGPT：从骨架动作到训练指导的闭环系统，含对齐(MotionDTW)、可解释评估(KISMAM)与检索增强指导(SportsRAG)，在时间误差、IoU与诊断专业性上优于传统与通用LLM。


<details>
  <summary>Details</summary>
Motivation: 现有智能体能分析多停留在“计分+可视化”，缺少自动化、可解释的动作诊断与可操作训练建议。LLMs与动作分析的进展为构建端到端、可解释的训练指导系统提供契机。

Method: 1) MotionDTW：两阶段时序对齐，从骨架序列精确提取关键帧；2) KISMAM：以知识为基础的可解释动作评估，对比关键帧与高质量目标模型，输出诊断指标（如伸展不足）；3) SportsRAG：基于Qwen3并结合6B-token知识库的RAG，检索领域QA以生成专业训练指导；整体形成从时序输入到指导输出的闭环。

Result: MotionDTW在时间误差更低、IoU更高，优于传统方法；消融实验证明KISMAM和SportsRAG均有效；整体SportsGPT在诊断准确性与专业性上超越通用LLM。

Conclusion: 将时序对齐、可解释评估与RAG式指导整合为LLM驱动闭环，既提升对齐与评估精度，也提供专业、可操作的训练建议，填补了仅“计分与可视化”的不足。

Abstract: Existing intelligent sports analysis systems mainly focus on "scoring and visualization," often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.

</details>


### [46] [Consistent Instance Field for Dynamic Scene Understanding](https://arxiv.org/abs/2512.14126)
*Junyi Wu,Van Nguyen Nguyen,Benjamin Planche,Jiachen Tao,Changchang Sun,Zhongpai Gao,Zhenghao Zhao,Anwesa Choudhuri,Gengyu Zhang,Meng Zheng,Feiran Wang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 提出Consistent Instance Field：一种连续、概率化的时空表征，解耦可见性与持久实例身份，通过占据概率与条件实例分布建模每个时空点；基于可变形3D高斯并联合编码辐射与语义，辅以身份校准与语义重采样机制，大幅提升新视角全景分割与开放词汇4D查询效果。


<details>
  <summary>Details</summary>
Motivation: 动态场景理解需要在时间与视角变化下保持一致的实例级语义与几何，但现有方法多依赖离散跟踪或视角相关特征，难以获得跨时空一致的实例身份与可见性建模。

Method: 提出Consistent Instance Field：为每个时空点建模占据概率和条件实例分布；设计基于可变形3D高斯的实例嵌入表示，联合编码辐射与语义，并通过可微栅格化从RGB与实例掩码端到端学习；引入每个高斯的身份校准机制与面向语义活跃区域的重采样策略，保证跨时空实例一致性。

Result: 在HyperNeRF与Neu3D数据集上显著优于最新方法，尤其在新视角全景分割与开放词汇4D查询任务上取得明显提升。

Conclusion: 连续、概率化的实例场与可变形3D高斯表征、身份校准与语义重采样共同实现了跨时空一致的实例级理解，推进了动态场景的新视角分割与4D查询能力。

Abstract: We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.

</details>


### [47] [Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models](https://arxiv.org/abs/2512.14137)
*Ashish Mishra,Tarun Kumar,Gyanaranjan Nayak,Arpit Shah,Suparna Bhattacharya,Martin Foltin*

Main category: cs.CV

TL;DR: 提出一种对多模态模型（如CLIP）进行选择性遗忘的闭式解方法：对目标类别文本嵌入子空间做零空间投影，从最终投影层中抹除对应信息，无需再训练或遗忘集图像。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法多依赖迭代微调与数据整理，成本高且易牺牲整体性能。需要一种高效且“外科手术式”的方法，只对特定类知识去除，同时保留模型通用能力，以满足模型去污与隐私合规。

Method: 计算目标类别文本嵌入所张成子空间的正交规范基，构造对该子空间的（全或部分）投影/到其正交补的投影，将最终投影层中的这些方向成分“抹除”（nullspace projection），从而降低图像特征与目标类方向的对齐度；过程为闭式，不需要再训练或遗忘集。

Result: 对目标类别的零样本性能显著下降，表明遗忘生效；整体多模态知识基本保留。通过部分投影可在“完全遗忘”和“信息保留”之间取得权衡。计算开销低、精度高。

Conclusion: 基于零空间投影的选择性遗忘为多模态模型提供了高效、精确的去污与隐私保护途径，可在不再训练的前提下按需擦除特定类信息，并可通过投影强度控制遗忘-保留平衡。

Abstract: We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.

</details>


### [48] [SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing](https://arxiv.org/abs/2512.14140)
*Han Zou,Yan Zhang,Ruiqi Yu,Cong Xie,Jie Huang,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 提出SketchAssist：统一“指令驱动全局编辑”和“线稿引导局部重绘”的交互式素描编辑系统，保持未编辑区域与整体构图稳定；基于可控数据生成管线与轻量模型改造，实现SOTA的语义可控性、结构与风格保真。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑对稀疏、风格敏感的线稿不友好：全局语义修改与精确局部重绘难以兼顾，且易破坏结构与风格一致性。需要一个既能听懂自然语言指令、又能遵循线条约束、同时保持未改区域不变的编辑助手。

Method: 1) 可控数据生成：从无属性基线稿自动构造属性添加序列；跨序列采样形成多步编辑链；再用“风格保持的属性移除模型”在多样线稿上进行反向扩增，扩大风格覆盖。2) 统一编辑框架：在DiT类编辑器上最小改动，重用RGB通道编码多模态输入，实现指令编辑和线引重绘在同一接口间无缝切换。3) 专门化能力：在LoRA层引入任务引导的Mixture-of-Experts，根据文本与视觉线索路由，提升语义可控、结构一致与风格保留。

Result: 在指令遵循与线稿结构/风格保持上均达SOTA；较近期基线在语义可控性、结构保真、风格一致和对无关区域的保持上显著优于对比。提供大规模数据与可控编辑系统的实证。

Conclusion: SketchAssist与其数据管线共同构成实用的素描创作与修订助手：统一两类编辑形式、可控且稳定，适合规模化应用并能作为后续研究与创作工具的基础。

Abstract: Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.

</details>


### [49] [TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models](https://arxiv.org/abs/2512.14141)
*Hanning Chen,Keyu Man,Kevin Zhu,Chenguang Zhu,Haonan Li,Tongbo Luo,Xizhou Feng,Wei Sun,Sreen Tallam,Mohsen Imani,Partha Kanuparthy*

Main category: cs.CV

TL;DR: 提出首个用于检测ML性能反模式的PyTorch跟踪基准与两阶段检测框架：小模型定位可疑片段，LLM细粒度分类与建议，效果优于无监督与规则法。


<details>
  <summary>Details</summary>
Motivation: 学界普遍缺乏专职基础设施工程支持，长且复杂的执行trace中手动定位性能反模式耗时且难以自动化，现有LLM受上下文和推理效率限制；缺少针对“反模式检测”能力的标准数据集与评测。

Method: 1) 构建包含600+跨任务（分类/检测/分割/生成）与多硬件平台的PyTorch trace基准，标注反模式片段；2) 提出迭代式两阶段流水：轻量级ML模型先在长trace中粗定位反模式区域；随后LLM对候选片段做细粒度分类并给出针对性优化建议；3) 与无监督聚类、基于规则的统计阈值方法对比评测。

Result: 两阶段方法在反模式区域检测上显著优于无监督聚类和规则统计方案；在长上下文场景下，通过先筛再判缓解LLM上下文长度限制与推理低效问题。

Conclusion: 提供数据集+方法的完整方案：基准推动可复现评测；两阶段框架高效准确定位并诊断trace反模式，为缺乏基础设施资源的研究者降低优化门槛。

Abstract: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.

</details>


### [50] [CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World](https://arxiv.org/abs/2512.14158)
*Shuxin Zhao,Bo Lang,Nan Xiao,Yilang Zhang*

Main category: cs.CV

TL;DR: 提出CIS-BA：以“连续的跨对象交互模式”作为触发，突破单触发-单目标与脆弱像素线索限制，实现鲁棒多触发-多目标后门攻击；在MS-COCO与实拍视频上ASR>97%，多触发动态条件下>95%，还绕过三种SOTA防御。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测后门多依赖固定像素触发并与单类别一一映射，易被环境变化、预处理或防御破坏，且难以同时影响多目标/复杂场景。需要一种不依赖脆弱外观特征、能在交互密集场景中稳定生效并支持复杂攻击语义的新范式。

Method: 提出“连续交互空间”（CIS），将场景中对象共现与几何关系（如相对位置、尺度、角度、拓扑约束）建模为连续空间触发（space triggers），以类-几何约束形式进行投毒训练。实现框架CIS-Frame：1)交互分析抽取稳健几何关系；2)将其形式化为可微约束注入到样本标注/损失中；3)在训练探测器时嵌入后门。支持单目标（误分类、消失）与多目标同时攻击。

Result: 在MS-COCO与真实视频上，复杂环境下攻击成功率超过97%；在动态多触发条件下维持超过95%有效性；同时能规避三种最先进防御。

Conclusion: 以交互几何为触发实现鲁棒的多触发-多目标后门，扩展了交互密集场景中的攻击边界，并对检测系统安全提出新威胁与启示。

Abstract: Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.

</details>


### [51] [FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation](https://arxiv.org/abs/2512.14162)
*Qingyuan Cai,Linxin Zhang,Xuecai Hu,Saihui Hou,Yongzhen Huang*

Main category: cs.CV

TL;DR: 提出Fast3DHPE统一框架与FastDDHPose方法：前者标准化3D人体姿态估计的训练/评测，实现公平对比与高效训练；后者用解耦扩散模型分别建模骨长与骨向，并配合关节运动学分层的时空去噪器，达到SOTA并具野外泛化。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D人体姿态估计方法多以2D关键点序列回归3D姿态，但训练/评测各自为政，难以公平比较且复现实验成本高；同时需提升对骨骼结构先验与时空关系的显式建模，缓解层级误差累积并增强泛化。

Method: 1) 提出Fast3DHPE模块化统一框架：规范数据处理、训练和评测协议，便于快速复现与灵活扩展。2) 在该框架内提出FastDDHPose：基于扩散模型的解耦建模，将骨长与骨方向分布显式分开学习；设计运动学-层级的时空去噪器，引导关注关节的层级/运动学关系，避免对过于复杂拓扑的无效建模；整体自2D关键点序列直接回归3D姿态。

Result: 在Human3.6M与MPI-INF-3DHP上，框架支持对各方法的公平对比并显著提升训练效率；FastDDHPose在统一设置下取得SOTA性能，并在真实场景中表现出强泛化与鲁棒性。

Conclusion: 统一的Fast3DHPE框架解决了公平评测与复现难题；基于解耦扩散与运动学分层去噪的FastDDHPose兼顾精度与泛化，达成SOTA。框架与模型代码将开源。

Abstract: Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE

</details>


### [52] [Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes](https://arxiv.org/abs/2512.14177)
*Joseph Hoche,Andrei Bursuc,David Brellmann,Gilles Louppe,Pavel Izmailov,Angela Yao,Gianni Franchi*

Main category: cs.CV

TL;DR: 提出SGPU：通过答案嵌入的几何与谱特征，结合高斯过程分类器，实现鲁棒的语义不确定性估计，在多模型多数据集上达到SOTA且可迁移。


<details>
  <summary>Details</summary>
Motivation: 现有基于“聚类一致性”的语义不确定性估计对措辞极其敏感，易误分/漏分，导致校准与区分能力不稳。需要一种不依赖脆弱聚类、能稳健刻画答案语义结构的不确定性方法。

Method: 将模型生成的多样化答案映射到语义嵌入空间；计算嵌入的Gram矩阵并对其特征谱（eigenspectrum）进行摘要；以此谱表示为输入，训练高斯过程分类器（GPC）学习从“语义一致性模式”到“预测不确定性”的映射。方法可在黑盒（仅答案与嵌入）与白盒设定下应用。

Result: 在8个数据集（VQA、图像分类、文本QA）与6个LLM/LVLM上，校准指标ECE与判别指标AUROC/AUARC均取得SOTA。并展示了跨模型与跨模态的迁移能力，表明谱表示捕获了通用的语义不确定性模式。

Conclusion: 通过谱几何而非离散聚类来表征多回答的语义结构，结合贝叶斯GPC即可提供稳定、可迁移、可泛化的语义不确定性估计，优于现有聚类式方法。

Abstract: Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.

</details>


### [53] [Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere](https://arxiv.org/abs/2512.14180)
*Francesco Di Sario,Daniel Rebain,Dor Verbin,Marco Grangetto,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 提出用球面Voronoi(SV)替代SH/SG做外观建模，简化优化并显著提升高频与镜面反射表现，在3D Gaussian Splatting中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有辐射场/高斯点渲染多用球谐SH建模视角相关外观，但SH难表示高频、易Gibbs振铃、无法刻画镜面反射；球面高斯虽更好但优化复杂。需要一种既能稳定表达高频/镜面又优化友好的统一表示。

Method: 提出Spherical Voronoi(SV)：在方向域上用可学习的Voronoi分区，边界平滑。用于漫反射时作为简洁的方向分片参数化；用于反射时，遵循经典图形学，将反射方向作为输入，把SV视为可学习的“反射探针”。将SV嵌入3D Gaussian Splatting的外观模块以学习视角相关效应。

Result: 在合成与真实数据集上，对漫反射取得与最佳方法相当的效果且优化更简单；在镜面/反射场景中显著优于SH并达到SOTA表现。

Conclusion: SV为显式3D表示中的外观建模提供了统一、稳定且高效的方案：能处理高频与镜面反射，优化难度低，较SH与SG更实用、通用，并在多数据集上验证有效。

Abstract: Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.

</details>


### [54] [Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity](https://arxiv.org/abs/2512.14196)
*Cassandra Krause,Mattias P. Heinrich,Ron Keuth*

Main category: cs.CV

TL;DR: 提出一种将儿童骨折形态学自动标注为AO全球编码的方法，通过把“全局多标签分类”转化为“局部多类别分类”，在公共数据上平均F1提升约7.89%，但在使用不完美的骨折检测器时性能下降。


<details>
  <summary>Details</summary>
Motivation: 儿童期骨折发生率高，准确诊断依赖于包括形态学、部位与夹角等特征。现有多标签全局分类难以充分利用公开数据且学习难度大，需一种既能利用公共数据又能更稳健地识别形态学的方法。

Method: 先定位骨折（检测出边界框），然后为每个框自动分配AO全球编码，将原本的全图多标签判定转化为框级的多类别分类；在公开数据集上训练与评估，并比较不同检测器质量对下游分类的影响。

Result: 采用该局部多类别策略，平均F1较全局多标签基线提升7.89%；但当上游骨折检测器不够准确时，整体性能显著下降。

Conclusion: 局部化的AO编码分类有效提升了骨折形态学识别，但部署到真实环境需解决检测器误差传播问题；代码已开源，便于复现与改进。

Abstract: Between $15\,\%$ and $45\,\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\,\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.

</details>


### [55] [Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination](https://arxiv.org/abs/2512.14200)
*Zhuoxiao Li,Wenzong Ma,Taoyu Wu,Jinjing Zhu,Zhenchao Q,Shuai Zhang,Jing Ou,Yinrui Ren,Weiqing Qi,Guobin Shen,Hui Xiong,Wufan Zhao*

Main category: cs.CV

TL;DR: 提出SkyLume：一个多时段、城市级、UAV图像+LiDAR数据集，解决光照不一致导致的大规模重建/逆渲染问题，并引入跨时间反照率一致性的评估指标TCC。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF/3DGS在大规模UAV重建中受多时段采集的光照变化影响，出现颜色伪影与几何误差；缺少系统采集的多光照UAV数据集使该问题难以研究与评测。

Method: 构建SkyLume数据集：覆盖10个城市区域，>10万张高分辨率UAV图（四斜视+垂直），每区在一天中三个时段采集以隔离光照因素；为每场景配套LiDAR与高精3D真值（深度、法向、重建质量）；为逆渲染提出度量TCC，衡量跨时间反照率稳定性以评估光照-材质解耦鲁棒性。

Result: 得到一个规模化、带精确几何真值、专为多时段光照鲁棒性研究的数据资源；提供新的评价维度（TCC）可量化跨时间一致性；为几何、外观和新视角合成在变化光照下的性能评估提供基准。

Conclusion: SkyLume填补了多时段光照条件下的大规模UAV逆渲染与重建评测空白，期望成为推进光照鲁棒重建、逆渲染与新视角合成研究的基础平台。

Abstract: Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.

</details>


### [56] [DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos](https://arxiv.org/abs/2512.14217)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: DRAW2ACT提出一种面向机器人操作的“可控视频扩散生成”方法：从输入轨迹中提取深度、语义、形状、运动等正交表征，并联合生成空间对齐的RGB与深度视频，通过跨模态注意力与深度监督提升时空一致性；再以生成的视频（RGB+Depth）驱动策略网络回归关节角，实验在多数据集上提升视觉质量与任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽能作为具身智能的强大模拟器，但在机械臂操作场景中缺乏可控性；已有“轨迹条件视频生成”多以2D或单一模态为条件，难以确保三维几何与动作一致，导致生成演示与可执行性不足。

Method: 1) 轨迹表征：从输入轨迹中解耦出深度、语义、形状、运动等互补/正交特征，并注入扩散模型；2) 跨模态联合生成：同时生成空间对齐的RGB与深度视频，引入跨模态注意力与深度监督，提高时空一致性与几何约束；3) 策略学习：以生成的RGB+Depth序列作为条件，训练多模态策略网络回归机器人关节角，实现从“画轨迹到执行动作”。

Result: 在Bridge V2、Berkeley Autolab以及仿真基准上，较现有方法获得更高的视觉保真度与时空一致性，并带来更高的机器人操作成功率。

Conclusion: 多正交轨迹表征+RGB/深度联合扩散生成+多模态策略回归，可显著提高可控性与可执行性，为从视觉合成到实际操控搭建更可靠的桥梁。

Abstract: Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.

</details>


### [57] [History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation](https://arxiv.org/abs/2512.14222)
*Xichen Ding,Jianzhe Gao,Cong Pan,Wenguan Wang,Jie Qin*

Main category: cs.CV

TL;DR: 提出HETT：历史增强的两阶段Transformer，用粗到细管线在城市级AVLN中先粗定位再细动作，结合历史网格记忆，且对CityNav做人工精修，实验显著提升并经消融验证。


<details>
  <summary>Details</summary>
Motivation: 现有无人机视觉-语言导航多采用单粒度框架，难以兼顾大范围全局推理与局部场景理解，导致在城市级复杂环境下定位与行动不稳。需要一种同时利用历史上下文、空间地标与细粒度视觉线索的机制。

Method: 提出History-Enhanced Two-Stage Transformer (HETT)：1) 两阶段粗到细管线：阶段一融合语言、空间地标与历史上下文，预测粗粒度目标位置；阶段二基于细粒度视觉分析与Transformer策略精化动作。2) 历史网格图（historical grid map）：将时序视觉特征动态聚合为结构化空间记忆，提升全局场景感知。3) 对CityNav数据进行人工标注精修，提升训练/评测数据质量。

Result: 在精修后的CityNav上取得显著性能提升；大量消融实验表明两阶段设计、历史网格记忆等组件均有效并带来可观增益。

Conclusion: HETT通过历史增强的两阶段Transformer实现全局推理与局部理解的平衡，显著提升AVLN任务表现；结构化空间记忆与数据精修同样关键，未来可拓展到更复杂城市场景与多源传感。

Abstract: Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.

</details>


### [58] [OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving](https://arxiv.org/abs/2512.14225)
*Tao Tang,Enhui Ma,xia zhou,Letian Wang,Tianyi Yan,Xueyang Zhang,Kun Zhan,Peng Jia,XianPeng Lang,Jia-Wang Bian,Kaicheng Yu,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出OminiGen：在统一BEV空间上，利用UAE重建法（体渲染联合解码LiDAR与多视角图像）与带ControlNet的DiT，实现可控、对齐的一体化多模态传感器数据生成，提升一致性与可调性。


<details>
  <summary>Details</summary>
Motivation: 实车采集多样化与极端场景数据昂贵低效；现有生成方法多为单模态，导致多模态数据不对齐、效率低。需要一种能统一表征、对齐生成并可控调节传感器的多模态生成框架。

Method: 1) 用共享BEV空间统一多模态特征。2) 提出通用可泛化的多模态重建UAE：通过体渲染同时解码LiDAR与多视角相机数据，实现精确且灵活的重建。3) 采用Diffusion Transformer并结合ControlNet分支，实现条件/可控的多模态传感器数据生成与调整。

Result: 实验显示该方法在统一多模态数据生成中达到预期效果：多模态一致性更好，且支持灵活的传感器设置与调节。

Conclusion: OminiGen在统一框架内实现了对齐的多模态生成，通过BEV统一表示、UAE体渲染解码和带ControlNet的DiT，实现高一致性与可控性，适用于高效合成自动驾驶多模态数据。

Abstract: Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.

</details>


### [59] [Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients](https://arxiv.org/abs/2512.14232)
*Rawan Alyahya,Asrar Alruwayqi,Atheer Alqarni,Asma Alkhaldi,Metab Alkubeyyer,Xin Gao,Mona Alshahrani*

Main category: cs.CV

TL;DR: 提出一种多视角MRI深度学习方法，无需复杂3D网络即可预测GBM患者MGMT启动子甲基化，包含新肿瘤切片提取技术，并提供可复现流水线；实验优于现有方法，显示非侵入式放射基因组学在精准医疗中的潜力。


<details>
  <summary>Details</summary>
Motivation: MGMT启动子甲基化是GBM化疗疗效的重要生物标志，但目前依赖侵入性活检。希望以放射基因组学从常规MRI中无创推断基因状态，提升临床可及性与效率。

Method: 利用多视角（轴位/矢状/冠状）MRI输入的深度学习框架，显式建模三视图空间关系，从每个视图提取特征并融合；避免使用参数庞大的3D网络以降低计算开销与收敛难题。提出新的肿瘤切片提取策略，提高切片选择质量。构建并开源复现实验流水线，对比多种SOTA模型与评测指标。

Result: 所提多视角方法在MGMT甲基化状态判别上优于或可比现有SOTA；新切片提取技术在多项指标上优于基线；在计算效率、参数规模与训练稳定性方面优于3D方法。

Conclusion: 多视角2D融合替代3D模型可高效准确地从MRI预测MGMT甲基化，实现无创分型的可行性；复现管线促进透明与鲁棒工具开发，对GBM精准治疗具有临床转化潜力。

Abstract: The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.

</details>


### [60] [ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body](https://arxiv.org/abs/2512.14234)
*Juze Zhang,Changan Chen,Xin Chen,Heng Yu,Tiange Xiang,Ali Sartaz Khan,Shrinidhi K. Lakshmikanth,Ehsan Adeli*

Main category: cs.CV

TL;DR: ViBES提出一个同时规划语言与肢体动作的对话式3D代理，通过混合模态专家架构融合语音、表情与身体动作，实现可控、流式、多轮互动中的语言-动作协同生成，并在对话-动作一致性与行为质量上优于强基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多将“语言到动作”视为单向翻译，缺乏关于何时动、动什么以及如何在多轮对话中自适应的能动决策，造成时序脆弱、社会语境弱、以及语音/文本/动作割裂训练与推理。

Method: 构建SLB（Speech-Language-Behavior）模型ViBES，采用MoME骨干：为语音、面部表情、身体动作分别设置按模态分区的Transformer专家，使用硬路由分配参数、跨专家注意力共享信息；利用强预训练语音-语言模型，实现用户语音/文本/动作指令的混合主导互动，并提供可控行为钩子以支撑流式响应。

Result: 在多轮对话基准上，以自动化度量评估对话-动作对齐与行为质量，ViBES在多个指标上稳定优于强力的同声动作与文本到动作基线。

Conclusion: ViBES将语音、语言与动作的联合生成推进到能动虚拟体层面，实现可控、社交能力更强的3D互动；代码与数据将开源。

Abstract: Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond "speech-conditioned motion generation" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/

</details>


### [61] [4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation](https://arxiv.org/abs/2512.14235)
*Jimmie Kwok,Holger Caesar,Andras Palffy*

Main category: cs.CV

TL;DR: 提出4D-RaDiff：一种面向汽车毫米波雷达的潜空间扩散生成框架，可从无标注框或LiDAR数据合成高质量4D雷达点云，作为检测训练数据；用其增强/预训练能显著提升检测效果并将标注需求降至约10%。


<details>
  <summary>Details</summary>
Motivation: 雷达在恶劣天气下鲁棒且成本低，但带标注的雷达数据稀缺，限制了基于雷达的目标检测发展；需要一种能大规模生成逼真、可控的雷达点云数据的方法以缓解数据瓶颈。

Method: 基于扩散模型但针对稀疏、噪声强的雷达点云特点，在潜在点云表示上进行扩散生成；支持对象级或场景级条件控制；4D-RaDiff可将无标注的2D/3D框转化为高质量雷达标注，并把现有LiDAR点云转换为逼真的雷达场景，用于训练与评测雷达检测器。

Result: 将合成雷达数据作为数据增强加入训练，检测精度较仅用真实数据显著提升；用合成数据进行预训练后，仅需最多10%的人工标注雷达数据即可达到与全量标注相当的检测性能。

Conclusion: 4D-RaDiff有效缓解雷达标注稀缺问题：通过潜空间扩散生成可控的4D雷达点云，既能提升检测性能，又能显著降低对真实标注的依赖，适合作为雷达感知的训练与评测数据源。

Abstract: Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.

</details>


### [62] [Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding](https://arxiv.org/abs/2512.14236)
*Nando Metzger,Prune Truong,Goutam Bhat,Konrad Schindler,Federico Tombari*

Main category: cs.CV

TL;DR: Elastic3D 提出一种端到端的可控单目转双目视频方法，基于条件潜变量扩散并配合“引导式VAE解码器”，在免显式深度与扭曲的前提下生成清晰、满足极线一致性的立体视频，并可在推理时通过一个标量旋钮调节视差强度；在三套真实世界数据集上优于传统扭曲与近期免扭曲基线。


<details>
  <summary>Details</summary>
Motivation: 沉浸式3D内容需求增长，但现有单目转双目方案多依赖深度估计+重映射，易产生孔洞、拉伸、鬼影等伪影，且对视差强度控制不便；近期扩散式方法虽能避免显式扭曲，但难以保证极线一致与清晰度，缺少可控性。

Method: 提出 Elastic3D：以（条件）潜变量扩散模型为核心，直接生成右眼视图；引入“引导式VAE解码器”以在解码阶段强约束清晰度与极线一致性，输出高质量立体对；提供一个推理期标量控制旋钮来调节视差范围，从而控制立体感强弱；整体为端到端、免显式深度与扭曲的流程。

Result: 在三个真实世界双目视频数据集上，Elastic3D 在客观与主观评测上均超过传统基于深度扭曲方法和近期免扭曲基线，显示更少伪影、更佳清晰度与更稳定的极线一致，并实现可控的立体效果。

Conclusion: Elastic3D 通过扩散模型与引导式VAE解码器实现高质量、可控、端到端的单目转双目视频生成，设定了更可靠的转换基准，适用于实际3D内容制作场景。

Abstract: The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.

</details>


### [63] [Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs](https://arxiv.org/abs/2512.14257)
*Wentao Wan,Kaiyu Wu,Qingyang Ma,Nan Kang,Yunjie Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: 提出EVPG：将视觉编程(VP)执行过程映射为可微的概率图推断，以便用目标任务最终标签对VP中的子模型进行端到端梯度优化，在GQA、NLVRv2、Open Images上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的视觉编程主要提升LLM生成程序质量，忽视了被调用的预训练视觉子模型的优化；但只有最终任务标签、没有子任务标签，且VP执行不可微，难以用梯度端到端训练。

Method: 依据VP执行中的变量依赖关系，构建有向概率图，将原本不可微的VP执行过程重构为精确、可微的概率推断流程；从而利用最终标签对整个VP框架及子模块进行端到端、基于梯度的监督学习。

Result: 在三类复杂视觉推理任务（GQA、NLVRv2、Open Images）上进行广泛实验，EVPG显著提升VP性能，优于现有方法。

Conclusion: 通过将VP映射为可微的概率图推断，EVPG解决了缺乏子任务标签与不可微带来的训练瓶颈，使VP能够使用最终标签高效端到端优化，并带来显著性能收益。

Abstract: Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.

</details>


### [64] [DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance](https://arxiv.org/abs/2512.14266)
*Shreedhar Govil,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 提出DriverGaze360：首个面向自动驾驶的360°驾驶员注意力大数据集（约100万标注帧，19名司机），并配套提出DriverGaze360-Net，通过加入语义分割辅支路联合学习注意力与关注目标，在全景输入上实现SOTA预测。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员注意力研究多依赖窄前向视角与有限场景多样性，难以覆盖换道、转弯及周边行人/骑行者等侧后方情境，无法充分建模全空间环境语境与真实注视行为。

Method: 构建全景（360°）驾驶影像注意力数据集，提供约百万帧凝视标注；提出DriverGaze360-Net，在全景图像上进行注意力预测，并加入辅助语义分割分支，联合学习注意力热图与被关注对象，以提升空间感知与注意力定位。

Result: 在全景驾驶图像上，方法在多项评价指标上取得SOTA表现，相较现有方法显著提升对广角与外周目标的注意力预测精度。

Conclusion: 全景化数据与联合注意力-语义学习能更全面刻画驾驶员凝视规律，显著提升复杂交通场景（如变道、转弯、与周边交通参与者交互）下的注意力预测；数据集与代码已开源，促进可解释自动驾驶研究。

Abstract: Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\circ$ field of view driver attention dataset, containing $\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.

</details>


### [65] [Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in](https://arxiv.org/abs/2512.14273)
*Xiaoqian Shen,Min-Hung Chen,Yu-Chiang Frank Wang,Mohamed Elhoseiny,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出Zoom-Zero：一种先粗后细的GVQA框架，先定位相关时间段，再放大到关键帧做细粒度验证，并通过“放大准确度奖励”和“令牌选择性归因”改进GRPO的时序对齐与信用分配。实验在NExT-GQA、ReXTime与长视频基准上显著提升时序定位与答题准确率。


<details>
  <summary>Details</summary>
Motivation: LVLM在GVQA上时序感知弱，现有基于GRPO的方法虽能一定程度改进时序定位，但仍常出现时间错位与幻觉，答案未能忠实依赖视频证据。需要一种既能提升时序定位可信度，又能在长视频中保留关键信息的训练与推理机制。

Method: 提出Zoom-Zero的粗到细框架：1) 先粗定位与问题相关的时间片段；2) 再对最显著的帧进行放大式细粒度视觉验证。为此对GRPO做两点改进：i) Zoom-in准确度奖励：评估并强化所预测时间定位的保真度，同时在选中帧上进行细粒度验证；ii) Token选择性信用分配：将奖励分别归因给负责时序定位与答案生成的不同token，缓解多重奖励信号的混淆。推理阶段同样采用放大策略，在不损失全局上下文的情况下保留关键细节。

Result: 在NExT-GQA与ReXTime上时序定位分别提升5.2%与4.6%，平均答案准确率提升2.4%；在长视频基准上通过推理阶段的放大策略带来平均6.4%的提升。

Conclusion: Zoom-Zero通过粗到细的时序定位与帧级验证，并结合放大准确度奖励和令牌选择性信用分配，显著提升GVQA的时序对齐与答案准确性，尤其在长视频理解中效果突出。

Abstract: Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.

</details>


### [66] [TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning](https://arxiv.org/abs/2512.14274)
*Yu Chen,Hongwei Lin*

Main category: cs.CV

TL;DR: 提出TUN：结合增强PD描述符、自注意力与PointNet编码及融合的多模态网络，实现一维持久图中显著点的自动检测，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: PD能刻画点云拓扑，但难以自动判别哪些持久点代表真实信号，影响TDA在实际中的可解释与决策使用，故需稳健自动的显著性识别方法。

Method: 构建Topology Understanding Net (TUN)：(1) 稳定预处理与不平衡感知训练；(2) 多模态输入——增强的PD描述符与原始点云；(3) 模块化架构——PD分支引入自注意力，点云分支采用PointNet式编码；(4) 学习式特征融合；(5) 逐点分类以判断PD中每个点的显著性。

Result: 在多组实验中，TUN在检测一维PD显著点任务上显著优于经典方法，展现更高的准确率与实用性。

Conclusion: TUN为一维PD显著点识别提供自动、有效且稳定的解决方案，提升TDA在真实应用中的落地能力。

Abstract: Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.

</details>


### [67] [SS4D: Native 4D Generative Model via Structured Spacetime Latents](https://arxiv.org/abs/2512.14284)
*Zhibing Li,Mengchen Zhang,Tong Wu,Jing Tan,Jiaqi Wang,Dahua Lin*

Main category: cs.CV

TL;DR: SS4D是一个原生4D生成模型，直接从单目视频合成动态3D对象，通过结构化的时空潜变量与时间一致性模块，实现高保真、时序连贯与结构一致，同时用时域压缩与分层训练策略提升长序列效率与遮挡鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成多依赖在3D或视频生成器上做后优化，易导致时空不一致、效率低且训练数据稀缺；需要一个直接在4D域学习、兼顾空间与时间一致性并能处理长视频与遮挡的模型。

Method: 提出结构化时空潜变量的4D生成器SS4D：1) 基于预训练单图到3D模型初始化以保证强空间一致性；2) 设计跨帧的专用时间层确保时间一致；3) 通过因子化4D卷积与时间下采样块压缩潜变量序列，提高长序列训练/推理效率；并辅以遮挡鲁棒性的训练策略。

Result: 模型在合成动态3D对象时达成高保真、时序连贯和结构一致，并能高效处理长视频序列。

Conclusion: 直接在4D域训练的SS4D通过结构化时空潜变量与时域压缩实现高质量且高效的动态3D生成，相较基于后优化的方法具备更好的时空一致性与实用性。

Abstract: We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion

</details>


### [68] [PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition](https://arxiv.org/abs/2512.14309)
*Abdullah Al Mamun,Miaohua Zhang,David Ahmedt-Aristizabal,Zeeshan Hayder,Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: 提出PSMamba：结合Vision Mamba序列建模与双学生层级蒸馏的自监督框架，用多尺度监督对植物病害图像学习全局—中尺度—局部表征，一致性损失促跨尺度对齐，在三数据集上优于SOTA并在域迁移与细粒度任务更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有SSL多聚焦全局对齐，难以捕获植物病害图像中层级化、多尺度的病斑与纹理模式，导致细粒度与域迁移场景表现不足。需要一种既高效又能同时学习上下文与细节的多尺度自监督方法。

Method: 构建PSMamba框架：以共享全局教师为监督，设置两个专门学生分支——中尺度学生关注病斑分布与叶脉结构，局部学生关注纹理异常与早期病斑；骨干采用Vision Mamba以高效序列建模；通过多粒度视角生成与一致性损失进行跨尺度对齐与联合学习，实现渐进式层级蒸馏。

Result: 在三个基准数据集上，相比现有最先进SSL方法获得更高精度与鲁棒性，尤其在域偏移与细粒度识别场景中取得显著提升。

Conclusion: 多学生层级蒸馏结合VM的高效建模能有效学习植物病害图像的多尺度表征，带来稳定的跨域与细粒度性能增益，验证了PSMamba的有效性。

Abstract: Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.

</details>


### [69] [From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region](https://arxiv.org/abs/2512.14312)
*Akila Premarathna,Kanishka Hewageegana,Garcia Andarcia Mariangel*

Main category: cs.CV

TL;DR: 本文比较多种视觉-语言模型（VLM）在卫星影像中识别中东与北非（MENA）地区污水处理厂（WWTP）的能力，发现零样本条件下若干VLM在真阳性率上超越了训练良好的YOLOv8分割模型，Gemma-3表现最佳，显示VLM可在无需标注的情形下替代传统方法进行可扩展的遥感监测。


<details>
  <summary>Details</summary>
Motivation: MENA地区对污水处理设施的识别与监测需求迫切，但传统深度学习分割（如YOLOv8）依赖大规模、昂贵的人工作标。VLM具备视觉-语言推理与少/零样本泛化潜力，有望减少标注成本并提升跨区域可迁移性。

Method: 构建两条评测流：零样本与少样本。YOLOv8以政府提供的83,566张高分辨率卫星图像（约85%正样本WWTP，15%负样本）训练作为强基线。评测多款VLM（LLaMA 3.2 Vision、Qwen2.5-VL、DeepSeek-VL2、Gemma 3、Gemini、Pixtral 12B），通过专家设计的提示词让模型在600m×600m GeoTIFF（Zoom18, EPSG:4326）上识别WWTP典型构件（圆/矩形沉淀池、曝气池等）并以JSON输出类别、置信度与描述；数据覆盖1,207处经验证WWTP位置（UAE 198，KSA 354，Egypt 655）及等量非WWTP对照。

Result: 在WWTP图像的零样本测试中，多款VLM的真阳性率超过YOLOv8分割基线，其中Gemma-3最高。研究显示VLM可有效分辨易混淆目标并提供结构化解释；少样本设置亦维持较强性能。

Conclusion: VLM在零样本情形即可达到或超越传统需大量标注的YOLOv8，对MENA地区WWTP自动识别具现实可替代性；这为低成本、可扩展的遥感环境监测提供了可行路径，减少标注依赖并提升跨域泛化。

Abstract: In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.

</details>


### [70] [Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity](https://arxiv.org/abs/2512.14320)
*Shuai Dong,Jie Zhang,Guoying Zhao,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出SIFM方法与ISR指标，从语义失配与感知退化角度重新定义并度量“图像免疫”对抗扩散编辑的成功；通过操控扩散中间特征同时最大化轨迹偏离与最小化特征范数，达到最先进防护效果。


<details>
  <summary>Details</summary>
Motivation: 现有“免疫”评价以与未保护图像编辑结果的视觉差异为主，忽略了真正需求：让攻击性编辑无法与攻击者提示语保持语义对齐，即便外观差异小也算失败。因此需要方法与指标能直接针对语义对齐破坏与感知退化来评估与优化。

Method: 提出Synergistic Intermediate Feature Manipulation（SIFM）：在扩散模型的中间特征层注入不可感知扰动，联合优化两目标——(1) 最大化与原始编辑轨迹的中间特征分布/路径差异，打乱语义编辑对齐；(2) 最小化特征范数，使生成结果出现显著感知退化，从而阻断恶意用途。同时提出Immunization Success Rate（ISR）指标，利用多模态大模型判定生成结果是否相对提示语语义失败或是否存在显著感知退化，并据此统计成功率。

Result: 在多项实验与基准上，SIFM较现有防护方法取得更高的免疫成功率与更强的鲁棒性，能有效阻断扩散模型对受保护图像的恶意编辑。

Conclusion: 仅比较保护与未保护编辑结果的视觉差异不足以衡量免疫；通过在扩散中间特征层进行协同操控并以ISR进行评价，可更准确地刻画并提升对抗恶意编辑的真实防护效果，达到SOTA性能。

Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.

</details>


### [71] [Dual Attention Guided Defense Against Malicious Edits](https://arxiv.org/abs/2512.14333)
*Jie Zhang,Shuai Dong,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出DANP：在扩散模型多步去噪过程中，联合干预跨注意力与噪声预测，用不可感知扰动提升对恶意文本编辑的免疫，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 文本驱动图像编辑强大但易被滥用，现有基于微扰的防御易被篡改绕过、对语义与生成过程干预不足，缺乏稳健、可泛化的免疫方案。

Method: 设计Dual Attention-Guided Noise Perturbation：在扩散若干时间步上，利用动态阈值从注意力图生成掩码区分文本相关/无关区域；对相关区域降低跨注意力强度、对无关区域提高，从而误导编辑定位；同时最大化注入噪声与模型预测噪声的差异，干扰去噪轨迹；两路联合优化以生成不可感知的对抗扰动。

Result: 在多种文本到图像编辑基准与威胁模型下，显著压制恶意编辑的成功率，保持目标区域与原图一致性；与现有防御相比取得SOTA的免疫效果与稳健性。

Conclusion: 联合操控跨注意力与噪声预测的多步扰动可有效破坏扩散模型的语义对齐与生成路径，从而提供对恶意文本编辑的强免疫；方法通用、不可感知且实验验证优于现有方案。

Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.

</details>


### [72] [Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure](https://arxiv.org/abs/2512.14336)
*Jooyeol Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: 他们提出一种从噪声弱预测中恢复SVG语义结构、再驱动动画的框架，使VLM能更稳健地生成连贯动画。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在SVG动画上表现不佳，因为SVG常由碎片化的低层形状组成，缺乏“部件/物体”级语义，导致无法判断哪些元素应协同运动。需要一种方法在不依赖强监督的情况下恢复语义结构，提升动画一致性与可解释性。

Method: 对同一SVG进行多次或多源的“弱部件预测”（如由不同提示、视角或轻量模型得到的部件归属），然后用统计聚合（如投票/一致性最大化/图聚类）稳健整合这些嘈杂信号，重组SVG为语义组；将该语义层注入VLM的动画生成流程，使其在组级别进行运动规划与代码生成。

Result: 在基准与案例上，相比现有方法，生成的SVG动画在连贯性、一致性与可解释性上显著提升，错误分组与不协调运动明显减少。

Conclusion: 恢复SVG的语义结构是解锁稳健自动动画的关键环节。通过统计聚合形成稳定的语义分组，可显著增强VLM在矢量图形上的动画能力与人机交互可解释性。

Abstract: Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.

</details>


### [73] [Towards Transferable Defense Against Malicious Image Edits](https://arxiv.org/abs/2512.14341)
*Jie Zhang,Shuai Dong,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出TDAE框架，通过图像-文本双模态协同优化，提高扩散式图像编辑对恶意编辑的免疫性，并在跨模型上具有更强可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于对抗微扰的防御在同模型有效，但跨模型可迁移性差，难以抵御未知或不同架构的编辑模型。

Method: 双模块：1) 视觉侧FDM（FlatGrad Defense Mechanism），在对抗目标中引入梯度正则，引导扰动落在更平坦的极小值以增强对未见模型的鲁棒性；2) 文本侧DPD（Dynamic Prompt Defense），通过周期性优化与刷新文本嵌入，使免疫图像在多样化嵌入下与原图编辑结果对齐，再据此更新图像，迭代寻求更广泛的免疫特征。

Result: 在同模型（intra-model）与跨模型（cross-model）评测中，TDAE在削弱恶意编辑效果方面达到SOTA表现，显示出显著的迁移性与鲁棒性提升。

Conclusion: 图像-文本协同的对抗优化可显著提升对恶意编辑的跨模型免疫；平坦极小值引导与动态提示优化是实现可迁移防御的关键。

Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.

</details>


### [74] [HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis](https://arxiv.org/abs/2512.14352)
*Kaizhe Zhang,Yijie Zhou,Weizhan Zhang,Caixia Yan,Haipeng Du,yugui xie,Yu-Hui Wen,Yong-Jin Liu*

Main category: cs.CV

TL;DR: 提出Hybrid Gaussian Splatting (HGS)，用RBF显式分解静态/动态区域，显著压缩参数与提速，同时保持甚至提升动态细节与突变场景的质量。


<details>
  <summary>Details</summary>
Motivation: 现有动态NVS基于3DGS多用隐式形变或统一时变参数，模型臃肿、渲染慢，不适合资源受限与实时应用；需要在不牺牲质量下显著减少冗余与提升效率。

Method: 提出静动解耦的Hybrid Gaussian Splatting。核心为Static-Dynamic Decomposition (SDD)：对高斯原语采用RBF建模。动态区域使用随时间变化的RBF以表达时序变化与突变；静态区域共享与时间无关的参数以去冗余。配合显式模型的两阶段训练，强化静动态边界的时序一致性。

Result: 在RTX 3090上4K可达125 FPS，模型大小最多降98%；在RTX 3050上1352×1014达160 FPS并已接入VR系统。视觉质量与SOTA相当，对高频细节与突变更佳。

Conclusion: HGS通过RBF驱动的静动分解与两阶段训练，实现大幅压缩与实时渲染，同时保持/提升动态细节质量，是适配资源受限与交互式场景的高效动态NVS方案。

Abstract: Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.

</details>


### [75] [Enhancing Interpretability for Vision Models via Shapley Value Optimization](https://arxiv.org/abs/2512.14354)
*Kanglong Fan,Yunqiao Yang,Chen Ma*

Main category: cs.CV

TL;DR: 提出一种在训练中以Shapley值估计为辅助任务的自解释框架，几乎不改网络结构，既保性能又提升解释的忠实性与可比性，并在多基准上达成SOTA可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释方法两难：事后解释难以忠实反映模型行为；自解释网络需特殊结构，往往牺牲性能与通用性。需要一种既忠实又兼容、对性能影响小的解释方案。

Method: 将Shapley值（对预测贡献的公平分配）作为辅助任务融入训练：以图像patch为单位学习预测分数的公平分配，使解释与决策逻辑内在一致。通过轻量的结构改动与多任务训练，联合优化主任务（预测）与解释任务（Shapley估计）。

Result: 在多个基准数据集上，所提方法在解释性指标上达到SOTA，同时保持或仅轻微影响原有模型性能与兼容性。

Conclusion: 将Shapley估计内化为训练目标，可在不显著牺牲性能与通用性的前提下提升解释的忠实性与可用性，为自解释DNN提供了一条务实路径。

Abstract: Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.

</details>


### [76] [Mimicking Human Visual Development for Learning Robust Image Representations](https://arxiv.org/abs/2512.14360)
*Ankita Raj,Kaashika Prajaapat,Tapan Kumar Gandhi,Chetan Arora*

Main category: cs.CV

TL;DR: 提出一种“渐进模糊课程”，先用强模糊图像训练CNN再逐步减小模糊，鼓励模型先学全局结构，提升分布外与噪声鲁棒性；在CIFAR-10-C与ImageNet-100-C上分别降低mCE达8.30%与4.43%，且与CutMix/MixUp兼容，并增强对自然与对抗扰动的鲁棒性，几乎不损伤原分布精度。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿视觉先模糊后清晰，具极强适应能力；而CNN对输入分布变化与高频伪影敏感。现有观点认为训练初期模糊会造成刺激缺失、不可逆伤害性能；作者质疑并欲验证早期模糊是否可引导模型关注全局、提升泛化与鲁棒性。

Method: 设计“视觉敏锐度课程”：训练早期用高斯强模糊图像输入，随epoch按计划逐步减弱模糊直至清晰；与静态随机模糊增广不同，强调时间上的结构化进程。与标准训练、静态模糊、以及结合CutMix/MixUp等对比；评估CIFAR-10-C、ImageNet-100-C的mCE，以及自然与对抗鲁棒性。

Result: 相较无模糊的标准训练，mCE在CIFAR-10-C下降最多8.30%，在ImageNet-100-C下降4.43%；在不明显降低原始测试集精度下，跨多数据集稳定收益；与CutMix/MixUp叠加仍提升；在常见攻击下自然与对抗鲁棒性均增强。

Conclusion: 渐进式模糊符合人类视觉发育规律，可作为简单通用的训练课程，促使CNN优先学习全局结构，从而提升分布外泛化与鲁棒性；驳斥了“早期模糊会不可逆伤害性能”的担忧，且与主流增广和鲁棒训练互补，易于实践。

Abstract: The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.

</details>


### [77] [Unified Semantic Transformer for 3D Scene Understanding](https://arxiv.org/abs/2512.14364)
*Sebastian Koch,Johanna Wald,Hide Matsuki,Pedro Hermosilla,Timo Ropinski,Federico Tombari*

Main category: cs.CV

TL;DR: UNITE 是一个统一的3D语义理解模型，从多视角RGB图像端到端几秒内推理完整3D语义与几何，覆盖分割、实例嵌入、开放词汇特征、可供性与可动性；通过2D蒸馏与多视角一致性自监督训练，跨任务达SOTA，甚至超过基于真值几何的专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解方法多为特定任务、依赖密集标注或真实3D几何，难以在真实复杂环境下统一高效泛化。需要一个能从RGB直接统一预测多种语义属性、并可在未见场景上端到端推理的模型。

Method: 提出UNITE：基于Transformer的前馈统一框架，仅以RGB多视角作为输入；联合预测3D语义分割、实例嵌入、开放词汇特征、可供性与关节可动性。训练上采用2D教师模型蒸馏与大规模自监督，设计多视角3D一致性损失以对齐跨视角表征；推理时数秒完成整场景的3D语义与几何重建。

Result: 在多项3D语义任务上达到或刷新SOTA，跨任务统一模型性能优于多个专用方法，部分指标甚至超过使用真值3D几何的方法；具备在未见场景上快速端到端推理能力。

Conclusion: 统一的Transformer框架可在仅用RGB的条件下高效学习3D语义几何，依托2D蒸馏与多视角一致性自监督实现多任务SOTA，显示出用单一模型处理多样3D语义任务的可行性与优势。

Abstract: Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io

</details>


### [78] [Optimizing Rank for High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2512.14366)
*Julian McGinnis,Florian A. Hölzl,Suprosanna Shit,Florentin Bieder,Paul Friedrich,Mark Mühlau,Björn Menze,Daniel Rueckert,Benedikt Wiestler*

Main category: cs.CV

TL;DR: 论文指出INR普遍被认为难以表示高频，但问题源于训练中网络稳定秩(stable rank)退化，而非架构本身。通过在训练中调控秩、采用如Muon这类高秩近正交更新的优化器，可使简单MLP也能拟合高频，在多任务上显著提升，PSNR最高提升达9 dB。


<details>
  <summary>Details</summary>
Motivation: 现有INR研究多依赖位置编码或特殊激活来克服“低频偏置”，默认架构局限。作者动机是质疑这一默认假设：是否真是架构不足，还是训练动力学（如权重矩阵秩的退化）导致表达高频失败？若是后者，是否可通过优化策略修复？

Method: 提出从稳定秩角度分析INR训练，假设高频缺失源自稳定秩降低。实验性方法：在训练中正则/调控网络的稳定秩，并使用产生高秩、近正交更新的优化器（如Muon）来维持权重矩阵的高有效秩；对比无位置编码的简单ReLU MLP与带/不带这些优化策略的表现。

Result: 维持高稳定秩显著提升INR表达高频内容的能力，使简单MLP也能高保真拟合；使用Muon等优化器在多种INR架构上普适提升，优于以往SOTA；在自然图像、医学图像、以及新视角合成等任务上，PSNR最高提升可达9 dB。

Conclusion: 低频偏置并非MLP架构不可避免的限制，而主要由训练中稳定秩退化导致。通过在优化过程中维持高秩（如采用Muon的近正交更新），无需复杂位置编码或专用激活也可获得强表达力；该策略通用于多领域并带来显著性能收益。

Abstract: Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).

</details>


### [79] [EcoScapes: LLM-Powered Advice for Crafting Sustainable Cities](https://arxiv.org/abs/2512.14373)
*Martin Röhn,Nora Gourmelon,Vincent Christlein*

Main category: cs.CV

TL;DR: 论文提出一个多层系统，将专用大语言模型、卫星影像分析与知识库结合，帮助小城市在人员与数据整合受限的情况下制定气候适应策略，并提供开源代码。


<details>
  <summary>Details</summary>
Motivation: 小城市制定气候适应策略面临两大问题：人力资源不足与多源数据（如监测、规划、影像、文献）难以集成与综合分析。需要一种低门槛、可扩展、可复用的技术框架提升评估与决策效率与质量。

Method: 构建分层架构：1) 专用LLM用于任务分解、政策与文本解读、方案生成；2) 卫星影像与地理空间分析模块进行城市尺度的环境与风险指标提取；3) 知识库整合多源数据与最佳实践，作为检索增强支撑；模块通过流水线/编排协同；并开源实现（EcoScapes）。

Result: 系统实现了多源数据整合与自动化分析流程，能输出针对小城市的气候适应洞见与策略建议；初步结果显示在人力有限场景下提高了效率与覆盖面（具体量化指标摘要未给出）。

Conclusion: 多层系统可缓解小城市在气候适应中的资源与数据整合瓶颈，LLM+遥感+知识库的结合为制定适应策略提供可行路径；开源代码便于复用与扩展。

Abstract: Climate adaptation is vital for the sustainability and sometimes the mere survival of our urban areas. However, small cities often struggle with limited personnel resources and integrating vast amounts of data from multiple sources for a comprehensive analysis. To overcome these challenges, this paper proposes a multi-layered system combining specialized LLMs, satellite imagery analysis and a knowledge base to aid in developing effective climate adaptation strategies. The corresponding code can be found at https://github.com/Photon-GitHub/EcoScapes.

</details>


### [80] [Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos](https://arxiv.org/abs/2512.14406)
*Le Jiang,Shaotong Zhu,Yedi Luo,Shayda Moezzi,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 提出 ExpanDyNeRF：结合高斯splatting先验与伪真值生成，使单目动态NeRF在大视角偏转下仍能稳定、真实合成；并构建含侧视监督的合成多视角动态数据集 SynDM，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态NeRF在视点大幅偏离训练视角时易出现闪烁、形变与不真实渲染，缺乏侧视监督与稳健几何先验，限制了新视点合成的可靠性。

Method: 在单目动态NeRF中引入两点：1) 高斯splatting先验，作为几何与外观的正则/初始化以稳固密度与颜色特征；2) 伪真值生成策略，为大角度旋转视图提供监督信号。并优化密度与颜色特征以增强从困难视角的重建。另构建SynDM数据集：基于GTA V渲染管线生成含显式侧视监督的动态多视数据。

Result: 在SynDM与真实数据上，定量与定性均显示在极端视点偏移下，渲染保真度显著优于现有动态NeRF。

Conclusion: 结合高斯先验与伪真值监督的ExpanDyNeRF，有效提升动态场景在大视角变化下的稳定真实合成；SynDM为该方向提供了标准化评测与训练资源。

Abstract: In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.

</details>


### [81] [DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning](https://arxiv.org/abs/2512.14420)
*Nakamasa Inoue,Kanoko Goto,Masanari Oi,Martyna Gruszka,Mahiro Ukai,Takumi Hirose,Yusuke Sekikawa*

Main category: cs.CV

TL;DR: 提出DISCODE：一种无需微调的分布感知评分解码方法，用于在域移情景下更稳健地用LVLM评估图像描述；通过测试时自适应损失和高斯先验解析解，提升与人类一致性；并发布多域评测基准MCEval；在多项基准上达SOTA的无参考指标。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态任务上强，但在图像描述评价上遇到域移导致评分不稳、与人类一致性下降。缺乏能在不同领域稳健泛化、且无需昂贵再训练的评价方法与系统性基准。

Method: 提出Distribution-Aware Score Decoder（DISCODE）：在测试时进行自适应评估，引入Adaptive Test-Time（ATT）损失，用高斯先验建模评分分布，推导出可解析的最优解以在推理时高效最小化损失，无需微调。并构建多域图像描述评估基准MCEval，包含六个领域，用于检验鲁棒性。

Result: DISCODE作为无参考指标，在MCEval与四个代表性既有基准上取得SOTA，与人工一致性更高、跨域更稳健。

Conclusion: 通过在测试时引入分布先验与解析优化，DISCODE显著提升LVLM在跨域图像描述评价中的鲁棒性与人类一致性；MCEval为评估跨域鲁棒性提供了新标准。

Abstract: Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.

</details>


### [82] [LCMem: A Universal Model for Robust Image Memorization Detection](https://arxiv.org/abs/2512.14421)
*Mischa Dombrowski,Felix Nützel,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出LCMem，将“记忆检测”统一为重识别+拷贝检测的跨域问题，通过两阶段训练（先身份一致性，后抗增强拷贝检测），在6个数据集上显著提升（ReID最高+16pp，拷贝检测最高+30pp），用于更可靠的大规模隐私审计。


<details>
  <summary>Details</summary>
Motivation: 生成图像足以以假乱真，但其用于隐私安全数据共享的可行性不明；现有隐私审计方法缺乏可靠的记忆检测、量化不足、跨域泛化差。需要能在不同域中稳定发现模型是否记忆并“复制”训练样本。

Method: 把记忆检测表述为同时覆盖身份一致性（ReID）与抗增强拷贝检测（copy detection）的统一任务；提出LCMem：一个在潜空间进行对比学习的网络，采用两阶段训练——第一阶段学习身份一致性表征；第二阶段引入数据增强与拷贝检测目标以获得对增强鲁棒的复制检测能力。模型跨域训练与评测，联合在ReID和copy detection上优化与验证。

Result: 在6个基准上，LCMem在ReID上最高提升16个百分点，在拷贝检测上最高提升30个百分点；能够更可靠、可扩展地检测大规模模型记忆。实验还表明现有隐私过滤器效果与鲁棒性有限。

Conclusion: LCMem作为跨域隐私审计的新基线，提供可靠、可扩展的记忆检测；呼吁更强的隐私保护机制，现有过滤不足。代码与模型已开源。

Abstract: Recent advances in generative image modeling have achieved visual realism sufficient to deceive human experts, yet their potential for privacy preserving data sharing remains insufficiently understood. A central obstacle is the absence of reliable memorization detection mechanisms, limited quantitative evaluation, and poor generalization of existing privacy auditing methods across domains. To address this, we propose to view memorization detection as a unified problem at the intersection of re-identification and copy detection, whose complementary goals cover both identity consistency and augmentation-robust duplication, and introduce Latent Contrastive Memorization Network (LCMem), a cross-domain model evaluated jointly on both tasks. LCMem achieves this through a two-stage training strategy that first learns identity consistency before incorporating augmentation-robust copy detection. Across six benchmark datasets, LCMem achieves improvements of up to 16 percentage points on re-identification and 30 percentage points on copy detection, enabling substantially more reliable memorization detection at scale. Our results show that existing privacy filters provide limited performance and robustness, highlighting the need for stronger protection mechanisms. We show that LCMem sets a new standard for cross-domain privacy auditing, offering reliable and scalable memorization detection. Code and model is publicly available at https://github.com/MischaD/LCMem.

</details>


### [83] [The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy](https://arxiv.org/abs/2512.14423)
*Zhuo Chen,Fanyue Wei,Runze Xu,Jingjing Li,Lixin Duan,Angela Yao,Wen Li*

Main category: cs.CV

TL;DR: 提出SynPS：在扩散模型无训练图像编辑中，动态协同位置嵌入与语义特征，缓解注意力共享导致的注意力塌缩，提升非刚性编辑的忠实度与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力共享的无训练图像编辑在复杂非刚性变化（姿态、形变）时常出现过度或不足编辑，根因是注意力在位置或语义一端失衡（注意力塌缩），需要一种能自适应平衡两者影响的机制。

Method: 提出SynPS：1) 定义逐步去噪中的编辑量度，量化每一步所需的编辑强度；2) 基于该量度设计注意力协同流程，动态调制位置嵌入在注意力中的权重，与语义特征自适应融合，从而在编辑过程中平衡语义改动与原图保真。

Result: 在公开与新构建的数据集上进行大量实验，SynPS相较现有方法在非刚性编辑上表现更佳，能有效避免过/欠编辑，获得更高的编辑质量与忠实度。

Conclusion: 动态协同位置嵌入与语义信息并引入逐步编辑量度，可显著缓解注意力塌缩，使大扩散模型的无训练非刚性图像编辑更稳定、可控且忠实。

Abstract: Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.

</details>


### [84] [Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging](https://arxiv.org/abs/2512.14435)
*Chang Cai,Hao Jiang,Xiaojun Yuan,Ying-Jun Angela Zhang*

Main category: cs.CV

TL;DR: 提出将基于分数的生成模型作为MMSE去噪器嵌入消息传递，实现高效压缩成像重建，并扩展到量化测量；算法10步内收敛、性能优于基线且有SE可预测。


<details>
  <summary>Details</summary>
Motivation: 传统PnP消息传递依赖通用/手工先验的去噪器，难以刻画自然图像复杂统计结构，尤其在高度欠定时重建次优；直接用分数模型做后验采样又计算代价高，亟需兼顾表达力与效率的新框架。

Method: 利用分数模型与经验贝叶斯去噪的联系，构造带有基于分数的MMSE去噪器的涡轮式消息传递框架STMP；针对量化观测，加入逐元素MMSE反量化模块形成Q-STMP；并给出可精确预测渐近表现的状态演化(SE)方程。

Result: 在FFHQ数据集上，STMP在性能-复杂度权衡上显著优于多种基线；Q-STMP在1-bit量化下仍保持鲁棒；两者通常于10次迭代内收敛，计算开销显著低于直接后验采样。

Conclusion: 将分数生成先验与消息传递融合可实现快速、准确的压缩成像重建；SE提供理论可预测性；扩展到量化测量同样有效，显示出在实际系统中的应用潜力。

Abstract: Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.

</details>


### [85] [S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation](https://arxiv.org/abs/2512.14440)
*Leon Sick,Lukas Hoyer,Dominik Engel,Pedro Hermosilla,Timo Ropinski*

Main category: cs.CV

TL;DR: 提出一种仅用真实视频数据训练的无监督视频实例分割框架，通过从单帧无监督分割中筛选高质量“关键掩码”，并以稀疏到致密蒸馏与Temporal DropLoss进行隐式掩码传播学习，最终在多项基准上超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖从图像数据合成的视频，简单的平移缩放掩码无法刻画真实视频中的透视变化、部件运动与相机运动，导致时序一致性差与泛化不足。因此需要在真实视频上学习，并提升单帧无监督分割在时间维度上的可靠性。

Method: 1) 以无监督单帧实例分割作为起点；2) 利用深度运动先验从视频中自动挑选时序上可靠、质量较高的关键掩码（keymasks），作为稀疏伪标注；3) 提出Sparse-To-Dense Distillation，将稀疏关键掩码蒸馏为致密监督，学习一个能隐式传播掩码的分割模型；4) 设计Temporal DropLoss，在训练中有选择地忽略时序不可靠或低质区域，增强鲁棒性；5) 以致密标注再训练最终模型。

Result: 在多个无监督视频实例分割基准上取得领先性能，超越既有依赖合成视频的SOTA方法。

Conclusion: 仅用真实视频并通过关键掩码选择与稀疏到致密蒸馏，可显著提升无监督视频实例分割的时序一致性与准确性，为摆脱合成数据依赖提供了有效途径。

Abstract: In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.

</details>


### [86] [A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning](https://arxiv.org/abs/2512.14442)
*Zixin Zhang,Kanghao Chen,Hanqing Wang,Hongfei Zhang,Harold Haodong Chen,Chenfei Liao,Litao Guo,Ying-Cong Chen*

Main category: cs.CV

TL;DR: A4-Agent是一个无需训练的三阶段代理式框架，用预训练大模型在测试时协同完成从“想象交互”到“确定部件”再到“精确定位”的可供性预测，在多基准上零样本超越有监督SOTA并具备强泛化。


<details>
  <summary>Details</summary>
Motivation: 端到端可供性预测把高层推理与低层落地耦合在单一模型里，严重依赖标注数据，导致对新物体与未知环境泛化差。作者希望通过解耦流程、利用通用基础模型的互补能力，在不做任务微调的情形下实现更强的零样本泛化与更稳定的现实世界表现。

Method: 提出A4-Agent，将可供性预测拆分为三阶段、三代理：1) Dreamer：调用生成模型将语言指令下的交互“可视化”，即生成交互结果或期望姿态的图像/视频；2) Thinker：用大型视觉语言模型对“应与什么部件交互”做高层语义推理与部件选择；3) Spotter：整合视觉基础模型进行精确的空间定位，输出交互区域的像素级或盒级位置；整个流程零训练、测试时编排多种预训练模型协作。

Result: 在多个可供性预测基准上，零样本设定显著优于最新监督方法；同时在真实场景中展现出稳健的泛化与鲁棒性。

Conclusion: 通过将高层语义决策与低层视觉定位解耦，并在测试时编排预训练基础模型协作，A4-Agent无需任务特定微调即可实现强零样本可供性预测，优于监督SOTA并具有良好现实世界泛化。

Abstract: Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.

</details>


### [87] [TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels](https://arxiv.org/abs/2512.14477)
*Andreas Sjölander,Valeria Belloni,Robel Fekadu,Andrea Nascetti*

Main category: cs.CV

TL;DR: 提出一个公开隧道衬砌病害图像数据集（含裂缝、渗漏、白华/析出等），用于支持监督/半监督/无监督的缺陷检测与分割，旨在缓解隧道领域数据匮乏，推动自动化巡检与跨隧道类型泛化研究。


<details>
  <summary>Details</summary>
Motivation: 手工巡检耗时、主观且昂贵；虽然移动测绘与深度学习可自动化，但缺乏领域数据限制了模型效果与泛化能力。

Method: 构建并公开包含三类不同隧道衬砌的标注图像数据集，覆盖典型病害（裂缝、白华/渗漏、渗水），并按监督、半监督、无监督任务设计标签与分割标注；强调纹理与施工工艺多样性以便跨类型迁移与泛化研究。

Result: 得到一个多样化、可用于多范式学习的高质量数据集，可用于缺陷检测与分割实验，并能评估模型在不同隧道类型间的可迁移性与泛化。

Conclusion: 该数据集缓解了隧道领域专用数据稀缺问题，促进自动化隧道检测研究与更安全高效的运维决策；为跨域泛化与迁移学习提供实验基准。

Abstract: Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.

</details>


### [88] [SuperCLIP: CLIP with Simple Classification Supervision](https://arxiv.org/abs/2512.14480)
*Weiheng Zhao,Zilong Huang,Jiashi Feng,Xinggang Wang*

Main category: cs.CV

TL;DR: SuperCLIP在CLIP的全局对比学习基础上加入轻量线性分类头，以利用文本token级信号，显著提升细粒度视觉-文本对齐，在零样本分类、检索与纯视觉任务上均有提升，几乎不增加计算量并缓解小批量训练退化。


<details>
  <summary>Details</summary>
Motivation: CLIP仅优化全局图文相似度，未显式利用token级监督，导致对长、细粒度描述的对齐不足，表现受限且对大批量训练依赖强。

Method: 在视觉编码器上添加一个极轻量的线性分类层，将文本token级线索转化为分类式监督，与原有对比学习共同训练；无需额外标注数据，FLOPs仅增加约0.077%。

Result: 在零样本分类、图文检索及纯视觉任务上稳定优于CLIP；在原始网页数据与re-caption数据训练下均提升；并在小批量设置中显著缓解性能下降。

Conclusion: 以最小改动为CLIP引入分类式、token级监督可恢复并强化文本细粒度信息，提升泛化与鲁棒性，是对比学习的简洁有效增强。

Abstract: Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.

</details>


### [89] [SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition](https://arxiv.org/abs/2512.14489)
*Alessia Micieli,Giovanni Maria Farinella,Francesco Ragusa*

Main category: cs.CV

TL;DR: SignIT：一个用于意大利手语识别的新数据集与基准，含644段视频（3.33小时）、94类手势与2D关键点；评估多种SOTA模型，揭示时间信息、关键点与RGB对性能的影响及当前模型在该数据集上的局限。


<details>
  <summary>Details</summary>
Motivation: 手语识别在多模态、人机交互与无障碍沟通中重要，但LIS数据资源稀缺、标准化基准缺失，限制了方法比较和进展；需要含精细类别标注与关键点信息的数据集来系统评估当前模型能力与不足。

Method: 构建SignIT数据集：收集644段视频（总时长3.33小时），依据5个宏类（动物、食物、颜色、情感、家庭）标注94个手势类别；对手、脸、身体提取2D关键点。基于该数据集搭建基准，采用多种SOTA识别模型，系统比较使用/不使用时间建模、2D关键点和RGB帧对识别性能的影响。

Result: 在统一基准下，不同模型的性能对时序信息、关键点与RGB输入较为敏感；尽管引入这些信息可提升性能，但整体结果显示现有方法在SignIT上仍存在显著性能不足，说明任务具有挑战性。

Conclusion: SignIT为LIS识别提供了规模适中、标注细致且含姿态关键点的数据资源与评测基准；实验揭示当前SOTA方法的局限，提示未来需更强的时序建模、关键点与RGB融合策略和针对LIS特性的算法；数据与标注已公开以促进社区研究。

Abstract: In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.

</details>


### [90] [Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency](https://arxiv.org/abs/2512.14499)
*Jia Guo,Jiawei Du,Shengzhu Yang,Shuai Lu,Wenquan Cheng,Kaiwen Zhang,Yihua Sun,Chuhong Yang,Weihang Zhang,Fang Chen,Yilan Wu,Lie Ju,Guochen Ning,Longfei Ma,Huiping Yao,Jinyuan Wang,Peilun Shi,Yukun Zhou,Jie Xu,Pearse A. Keane,Hanruo Liu,Hongen Liao,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: 提出ReVision视网膜基础模型，直接从真实世界远程医疗图像-报告配对中学习，无需任务特定训练即可在多基准上实现强零样本性能，并具备高迁移性与低资源部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜基础模型依赖精心策划的研究数据集，缺乏真实临床语境，且每个应用需要大量任务特定优化，限制在低资源环境中的部署与泛化。作者希望从真实临床工作流中直接获取“临床原生智能”，减少标注与微调成本并提升跨场景可迁移性。

Method: 利用中国十年远程医疗项目中162家机构积累的485,980张彩色眼底照与对应诊断报告的“自然对齐”监督，训练名为ReVision的多任务/对齐式表征学习基础模型；评估其零样本与小样本/轻适配能力，并测试跨站点、域、模态与系统性健康任务的迁移；开展前瞻性读片者研究衡量对临床医生的辅助效果。

Result: 在27个眼科基准中，零样本疾病检测于12个公共数据集平均AUROC 0.946，在3个独立临床队列AUROC 0.952；在可进行最小适配时，以远少于对照方法的可训练参数与标注样本达到相当性能；表征在新临床站点、成像域/模态及系统性健康预测中有效迁移；对33位眼科医生的前瞻性读片研究中，零样本辅助将诊断准确率提升14.8%。

Conclusion: 通过挖掘远程医疗归档中影像-报告的天然监督，无需额外注释即可学习到可广泛迁移的“临床原生”表征，显著降低任务特定训练与资源需求，为低资源场景中的医学AI部署提供有效路径。

Abstract: Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.

</details>


### [91] [DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors](https://arxiv.org/abs/2512.14536)
*Yiheng Huang,Junhong Chen,Anqi Ning,Zhanhong Liang,Nick Michiels,Luc Claesen,Wenyin Liu*

Main category: cs.CV

TL;DR: 提出DASP：在夜晚低可见度下，通过对抗分支提取昼夜时空先验、配合自监督分支与3D一致性投影损失，实现SOTA的单目深度估计。


<details>
  <summary>Details</summary>
Motivation: 夜间单目自监督深度估计因光照不足、纹理缺失与运动模糊而性能显著下降；需要利用昼间数据中的时空结构先验来弥补夜间信息不足。

Method: 两分支框架DASP：1) 对抗分支：设计含四个SPLB的判别器以学习昼间时空先验。SPLB由STLM与ASLM构成：STLM用正交差分提取时间轴上的运动变化；ASLM以局部非对称卷积+全局轴向注意力获取多尺度结构特征。2) 自监督分支：提出3D一致性投影损失，将目标帧与源帧双向投影到共享3D空间，计算3D差异以优化3D结构一致性并注入昼间先验。

Result: 在Oxford RobotCar与nuScenes夜间场景上取得SOTA夜间深度估计效果；消融实验证明各模块（SPLB、STLM、ASLM、3D一致性损失）的有效性。

Conclusion: 利用对抗学习获得昼间时空先验并以3D一致性约束进行自监督训练，可显著提升夜间单目深度估计的准确性与鲁棒性，特别是在纹理缺失与运动模糊区域。

Abstract: Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.

</details>


### [92] [CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning](https://arxiv.org/abs/2512.14540)
*Andreas Lolos,Theofilos Christodoulou,Aris L. Moustakas,Stergios Christodoulidis,Maria Vakalopoulou*

Main category: cs.CV

TL;DR: 提出CAPRMIL：在弱监督病理MIL中，通过在聚合前学习全局上下文感知的patch表示，配合简单Mean聚合即可达SOTA，且参数、FLOPs、显存与训练时间更优。


<details>
  <summary>Details</summary>
Motivation: WSI为千兆像素、像素级标注稀缺，MIL成主流。但现有方法多依赖复杂注意力聚合学习实例相关性，带来高参数量与计算/内存开销。需要一种既能建模全局上下文、又计算高效、与聚合器无关的框架。

Method: 冻结patch编码器提取特征；将patch特征投影到少量“全局上下文/形态学感知token”；对这些token使用多头自注意力实现线性于bag大小的全局上下文注入；得到“上下文感知”的patch嵌入后，再用简单Mean MIL聚合进行slide级训练与推理。框架对聚合器无偏好（aggregator-agnostic），目标是将相关性学习从聚合器中剥离到前置表示学习阶段。

Result: 在多个公开病理基准上，配合Mean聚合可匹配SOTA的slide级性能；相较SOTA MIL方法，可减少可训练参数48%–92.8%，推理FLOPs降低52%–99%，在GPU显存效率与训练时间上也处于领先。

Conclusion: 在聚合前学习丰富的全局上下文感知实例表示，是替代复杂注意力池化的一种有效且可扩展的策略；CAPRMIL证明了以低复杂度实现高性能的可行性。

Abstract: In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL

</details>


### [93] [HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion](https://arxiv.org/abs/2512.14542)
*Yifang Xu,Benxiang Zhai,Yunzhuo Sun,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: 提出HiFi-Portrait：在零样本条件下实现高保真、可控的人像生成，融合多张同一ID参考图的细粒度特征与3D人脸关键点以提升身份保持与属性控制，超越现有方法并兼容SDXL生态。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式身份保持人像生成在使用同一ID的多张参考图时，常出现清晰度下降、身份一致性不足，且对目标面部属性（如姿态、表情、妆容等）的精确控制较弱，亟需一种既提高ID保真度又增强可控性的方案。

Method: 1) 引入“face refiner”与“landmark generator”：从多张参考脸提取细粒度多脸特征，并生成兼具参考ID与目标属性的3D感知人脸关键点；2) 设计HiFi-Net：将多脸特征与关键点进行融合与对齐，强化ID一致性与人脸可控性；3) 构建自动化ID数据集生成流程，为模型训练提供规模化、标注一致的数据。

Result: 在大量实验中，相比SOTA方法，HiFi-Portrait在身份相似度与可控性指标上取得更优表现；并在实践中表现出与基于SDXL的既有方法的良好兼容性。

Conclusion: 通过多参考图特征融合与3D关键点对齐，HiFi-Portrait在零样本人像生成中实现高保真身份保持与精细可控，提供可扩展的数据构建流水线，具备与主流SDXL方法的兼容性并优于现有SOTA。

Abstract: Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.

</details>


### [94] [TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration](https://arxiv.org/abs/2512.14550)
*Zhiwen Yang,Jiaju Zhang,Yang Yi,Jian Liang,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: 提出任务自适应Transformer（TAT），通过为不同任务生成专属权重并自适应平衡各任务损失，缓解任务干扰与不平衡，在PET合成、CT去噪、MRI超分三类任务及一体化设置上达SOTA。


<details>
  <summary>Details</summary>
Motivation: All-in-One医学图像复原需同时处理不同模态与退化类型，易出现：1）任务干扰——共享参数上的梯度冲突；2）任务不平衡——不同任务学习难度不同导致优化偏斜。因此需要一种既能避免跨任务冲突又能按难度动态分配学习资源的统一框架。

Method: 提出任务自适应Transformer（TAT）：1）任务自适应权重生成（task-adaptive weight generation），为每个任务生成任务专属的权重参数，避免在共享权重上产生冲突梯度；2）任务自适应损失平衡（task-adaptive loss balancing），依据各任务的学习难度动态调整损失权重，抑制任务主导或训练不足。整体框架在Transformer骨干上结合上述两种策略，实现面向多任务/多模态的动态适配。

Result: 在PET合成、CT去噪、MRI超分三项MedIR任务中，无论在单任务还是All-in-One设定下，TAT均达到或刷新SOTA表现。

Conclusion: 通过任务自适应的权重生成与损失平衡，TAT有效缓解多任务医学图像复原中的任务干扰与不平衡问题，提升统一模型的泛化与性能，并在多项基准上验证了其优越性。

Abstract: Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.

</details>


### [95] [CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer](https://arxiv.org/abs/2512.14560)
*Xianwei Cao,Dou Quan,Shuang Wang,Ning Huyan,Wei Wang,Yunan Li,Licheng Jiao*

Main category: cs.CV

TL;DR: 提出CLNet用于跨视角地理定位，通过显式建立空间对应关系，优于仅依赖全局表征的方法，并在多个数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往IRCVGL方法多靠全局特征或隐式对齐，难以显式建模跨视角（卫星-街景）间的语义与几何对应，导致定位精度与可解释性不足。

Method: 提出对应感知的特征细化框架CLNet，将视角对齐分解为三模块：1) NCM：学习潜在对应场，进行空间对齐；2) NEC：用MLP实现非线性嵌入转换，完成跨视角特征重映射；3) GFR：依据学到的空间线索重标定全局通道，强调信息性特征。三者协同捕获高层语义与细粒度对应。

Result: 在CVUSA、CVACT、VIGOR与University-1652四个公开基准上取得SOTA，同时表现出更好的可解释性与泛化能力。

Conclusion: 显式建模跨视角的空间对应并结合非线性嵌入转换与全局通道重校准，可有效提升IRCVGL性能与可解释性；CLNet为跨视角对齐提供了通用而强健的框架。

Abstract: Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.

</details>


### [96] [FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications](https://arxiv.org/abs/2512.14574)
*Mitsuki Watanabe,Sosuke Amano,Kiyoharu Aizawa,Yoko Yamakata*

Main category: cs.CV

TL;DR: 提出FoodLogAthl-218：来自真实用户饮食日志的食物图像数据集（6,925张、218类、14,349个框），并定义标准分类、时间增量微调、上下文感知分类三项任务，用LMM做基线评测。


<details>
  <summary>Details</summary>
Motivation: 现有食品图像数据多来自网页抓取，分布与真实用户餐食差异大，影响饮食管理应用中自动记录的实用性与泛化。需要一个来自真实使用场景、包含上下文与时间信息的数据集与评测协议。

Method: 从FoodLog Athl应用中收集真实用户上传的餐食照片与元数据（时间、匿名用户ID、餐次上下文等），先有照片后打标签，保留自然频率与类内多样性；构建218类数据集并提供目标框。设计三类任务：1）标准分类基准；2）沿用户时间流的增量微调协议；3）多菜同图的上下文感知分类（利用整餐上下文分类各菜）。用大型多模态模型进行评测。

Result: 得到FoodLogAthl-218数据集（公开在HF），显示出更贴近真实使用的类间分布与图像多样性；在三项任务上给出LMM基线结果（摘要未给具体数值）。

Conclusion: 真实世界来源的数据和与应用贴合的新任务设定更能检验与提升食物识别模型在实际饮食记录中的可用性；该数据集与协议为后续研究提供了基础与基线。

Abstract: Food image classification models are crucial for dietary management applications because they reduce the burden of manual meal logging. However, most publicly available datasets for training such models rely on web-crawled images, which often differ from users' real-world meal photos. In this work, we present FoodLogAthl-218, a food image dataset constructed from real-world meal records collected through the dietary management application FoodLog Athl. The dataset contains 6,925 images across 218 food categories, with a total of 14,349 bounding boxes. Rich metadata, including meal date and time, anonymized user IDs, and meal-level context, accompany each image. Unlike conventional datasets-where a predefined class set guides web-based image collection-our data begins with user-submitted photos, and labels are applied afterward. This yields greater intra-class diversity, a natural frequency distribution of meal types, and casual, unfiltered images intended for personal use rather than public sharing. In addition to (1) a standard classification benchmark, we introduce two FoodLog-specific tasks: (2) an incremental fine-tuning protocol that follows the temporal stream of users' logs, and (3) a context-aware classification task where each image contains multiple dishes, and the model must classify each dish by leveraging the overall meal context. We evaluate these tasks using large multimodal models (LMMs). The dataset is publicly available at https://huggingface.co/datasets/FoodLog/FoodLogAthl-218.

</details>


### [97] [LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction](https://arxiv.org/abs/2512.14594)
*Chenyu Zhao,Yingxue Xu,Fengtao Zhou,Yihui Wang,Hao Chen*

Main category: cs.CV

TL;DR: 提出KEMM：一个由大语言模型驱动、融合专家报告与预后背景知识的多模态癌症生存预测框架，通过知识增强的跨模态注意力从高维冗余的WSI和基因数据中提取判别特征，实验在五个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生存预测依赖WSI与基因组数据，维度高且冗余，难以提取判别特征并实现模态对齐；同时仅靠简单的随访生存标签监督不足以指导复杂模型学习。需要引入更具临床含义与可对齐性的知识信号来提升预测性能与可解释性。

Method: 构建KEMM框架：利用LLM精炼病理学家逐例专家报告，形成简洁、面向临床的诊断陈述；由LLM生成不同癌种的简明预后背景知识（PBK）。提出知识增强跨模态注意力（KECM），将这些文本知识与WSI、基因组模态进行交互，引导模型关注与生存相关、判别性强的特征，缓解高维冗余与跨模态对齐难题。

Result: 在五个数据集上进行大量实验，KEMM在生存预测性能上达到或超过当前最优方法（SOTA）。

Conclusion: 通过引入LLM生成与提炼的专家报告和预后背景知识，并以KECM实现知识引导的跨模态对齐与特征提取，KEMM显著提升多模态生存预测效果，展示了知识增强对高维冗余数据的有效性；代码将在接收后开源。

Abstract: Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.

</details>


### [98] [TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios](https://arxiv.org/abs/2512.14595)
*Mengyu Li,Xingcheng Zhou,Guang Chen,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 提出首个面向事件相机的ITS多目标检测与跟踪小型试验数据集，并基于此构建tracking-by-detection基线与专用特征提取器，表现优异。


<details>
  <summary>Details</summary>
Motivation: 帧式相机在弱光与高速运动场景下表现不佳，而事件相机具备低延迟、高动态范围与高时间分辨率，可缓解上述问题；但事件视觉研究与数据集明显匮乏，尤其在ITS场景。

Method: 采集并发布一个面向ITS（车辆、行人）的事件相机初始数据集；在此上建立tracking-by-detection基准流程，并设计适配事件数据的专用特征提取器，用于检测与多目标跟踪评测。

Result: 在所建数据集与基准上，所提出的专用特征提取器与tracking-by-detection框架取得了“优秀表现”（相较于基线/现有方法的具体指标未给出）。

Conclusion: 事件相机在ITS多目标跟踪中具潜力；提供的数据集与基准为后续研究奠定基础，证明事件驱动方案可在低光与高速场景中提升性能。

Abstract: In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.

</details>


### [99] [FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos](https://arxiv.org/abs/2512.14601)
*Zhaolun Li,Jichang Li,Yinqi Cai,Junye Chen,Xiaonan Luo,Guanbin Li,Rushi Lan*

Main category: cs.CV

TL;DR: FakeRadar 是一套面向跨域泛化的深度伪造视频检测框架，利用大型预训练模型探测特征空间差异，并通过伪造离群探测与三元训练策略，生成并利用靠近簇边界的离群样本，显著提升对未知伪造的鲁棒性与跨数据集表现。


<details>
  <summary>Details</summary>
Motivation: 现有检测器多依赖特定操控痕迹，对已知伪造有效但对新型伪造泛化差，原因是无法适应未见伪造的分布与特征模式差异；现实场景不断涌现新伪造手法，需提升跨域与未知操控的检测能力。

Method: 1) 利用大规模预训练模型（如 CLIP）对特征空间进行主动探测，刻画真实、已知伪造与潜在未知伪造的分布差异。2) 伪造离群探测（Forgery Outlier Probing）：动态子簇建模+簇条件离群生成，在估计子簇边界附近合成离群样本，模拟超出已知类型的新伪造伪影。3) 离群引导三元训练（Outlier-Guided Tri-Training）：以真实/伪造/离群为三类，通过离群驱动的对比学习与带离群条件的交叉熵损失共同优化检测器。

Result: 在多种深度伪造视频基准上优于现有方法，尤其在跨域评测中表现突出，能更好应对新出现的多样化操控技术。

Conclusion: 通过引入基于预训练模型的特征探测与离群样本生成，并以三元学习策略训练检测器，FakeRadar 有效提升了对未知伪造的识别与跨域泛化能力。

Abstract: In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.

</details>


### [100] [WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling](https://arxiv.org/abs/2512.14614)
*Wenqiang Sun,Haiyu Zhang,Haoyuan Wang,Junta Wu,Zehan Wang,Zhenwei Wang,Yunhong Wang,Jun Zhang,Tengfei Wang,Chunchao Guo*

Main category: cs.CV

TL;DR: WorldPlay 是一种实时流式视频扩散模型，兼顾速度与长程几何一致性，通过双重动作表示、重构式上下文记忆与“上下文强制”蒸馏，实现 720p/24FPS 的交互式世界建模并较少漂移。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散与世界建模方法在实时交互时面临“速度—记忆”权衡：要么低延迟但易丢失长程几何一致性、产生误差漂移，要么依赖庞大缓存导致速度慢与内存占用高。作者希望在保持实时性的同时维持长时几何一致与稳健的动作控制。

Method: (1) 双重动作表示：将键鼠输入编码为两种互补的控制信号以提升动作可控性与鲁棒性。(2) 重构式上下文记忆：从历史帧动态重建上下文，并用时间重构/重定帧机制把几何关键但久远的帧“拉回”可用范围，减轻记忆衰减。(3) 上下文强制蒸馏：在教师-学生框架中对齐二者的记忆上下文，使学生模型在较小内存/实时推理下仍能利用长程信息，抑制误差漂移。整体形成流式扩散推理管线。

Result: 在多场景上实现可交互的长时段流式生成，达到 720p、24 FPS 的实时性能；与现有方法相比具有更好的几何与外观一致性与更小漂移，并展现跨场景泛化与在线 Demo。

Conclusion: WorldPlay 通过动作控制、记忆重构与记忆感知蒸馏协同，打破实时性与长程一致性的权衡，实现高分辨率、长时稳定的交互式视频世界建模，优于现有技术并具备良好泛化。

Abstract: This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.

</details>


### [101] [Distill Video Datasets into Images](https://arxiv.org/abs/2512.14621)
*Zhenghao Zhao,Haoxuan Wang,Kai Wang,Yuzhang Shang,Yuan Hong,Yan Yan*

Main category: cs.CV

TL;DR: 提出SFVD：用单帧蒸馏视频，借可微插值生成序列并仅更新帧，结合少量真实视频与通道重排引入时间信息，显著提升视频数据集蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 视频数据集蒸馏难以像图像那样有效，核心瓶颈在于时间维度带来的可学习参数暴涨，优化困难、收敛差，需要一种更易优化且能保持判别力的蒸馏方式。

Method: 1) 发现单帧足以表达视频判别语义；2) 为每类学习少量“高信息帧”；3) 通过可微插值将这些帧扩展为伪视频并与原始数据进行匹配/对齐；4) 反向传播时仅更新帧参数以减小优化维度；5) 通过通道重排层将蒸馏帧与采样的真实视频在匹配阶段融合，引入必要的时间信息。

Result: 在多个基准上优于现有方法，在MiniUCF上最高提升5.3%，表现稳定且效率更高。

Conclusion: 将视频蒸馏降维为“单帧+可微生成序列+与真实视频融合”的方案，缓解时间维优化难题，在保持判别性的同时显著提升视频蒸馏性能。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets that allow models trained on them to achieve performance comparable to training on the full dataset. While this approach has shown promising results for image data, extending dataset distillation methods to video data has proven challenging and often leads to suboptimal performance. In this work, we first identify the core challenge in video set distillation as the substantial increase in learnable parameters introduced by the temporal dimension of video, which complicates optimization and hinders convergence. To address this issue, we observe that a single frame is often sufficient to capture the discriminative semantics of a video. Leveraging this insight, we propose Single-Frame Video set Distillation (SFVD), a framework that distills videos into highly informative frames for each class. Using differentiable interpolation, these frames are transformed into video sequences and matched with the original dataset, while updates are restricted to the frames themselves for improved optimization efficiency. To further incorporate temporal information, the distilled frames are combined with sampled real videos from real videos during the matching process through a channel reshaping layer. Extensive experiments on multiple benchmarks demonstrate that SFVD substantially outperforms prior methods, achieving improvements of up to 5.3% on MiniUCF, thereby offering a more effective solution.

</details>


### [102] [AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation](https://arxiv.org/abs/2512.14639)
*Fei Wu,Marcel Dreier,Nora Gourmelon,Sebastian Wind,Jianlin Zhang,Thorsten Seehaus,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 提出AMD-HookNet++：一种CNN-Transformer混合方法用于SAR冰川分割与崩解前缘描绘，在CaFFe数据集上创SOTA（IoU 78.2，HD95 1,318 m，MDE 367 m），边界更平滑。


<details>
  <summary>Details</summary>
Motivation: 纯CNN（如AMD-HookNet）虽擅长局部细节，但难以建模长程依赖；纯Transformer虽有全局视野，却易产生锯齿状边界。需要一种既保留局部纹理又融入全局上下文的混合框架，以更稳健地估计冰川崩解前缘位置，服务冰盖质量守恒与海平面评估。

Method: 构建两分支混合结构：Transformer上下文分支负责长程依赖与全局语义；CNN目标分支保留局部细节。提出增强的空间-通道注意力模块以在两分支间动态建模token关系、融合特征。引入像素到像素对比式深度监督，将像素级度量学习融入分割训练，强化边界与类别判别。

Result: 在CaFFe基准上达到新的SOTA：IoU 78.2；HD95 1,318 m；MDE 367 m；相较纯Transformer方法，前缘轮廓更平滑、锯齿显著减少。

Conclusion: 混合CNN-Transformer架构结合跨分支空间-通道注意力与像素对比深监督，可同时获取全局与局部信息，提升冰川分割与崩解前缘描绘精度与形态质量，为冰川动态监测与海岸海平面评估提供更可靠工具。

Abstract: The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.

</details>


### [103] [A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images](https://arxiv.org/abs/2512.14640)
*Rao Muhammad Umer,Daniel Sens,Jonathan Noll,Christian Matek,Lukas Wolfseher,Rainer Spang,Ralf Huss,Johannes Raffler,Sarah Reinke,Wolfram Klapper,Katja Steiger,Kristina Schwamborn,Carsten Marr*

Main category: cs.CV

TL;DR: 提出首个多中心淋巴瘤基准数据集，并系统评估多种病理基础模型与MIL聚合器；院内准确率>80%，院外仅~60%，泛化仍是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 临床淋巴瘤分型需IHC/流式/分子等多模态检测，流程昂贵且延迟治疗；希望用仅HE切片的深度学习辅助分型，但缺乏涵盖多中心的系统性基准与可比评测。

Method: 构建包含四种常见淋巴瘤亚型与健康组织的多中心数据集；在3个放大倍数（10x/20x/40x）上，将5个公开病理基础模型（H-optimus-1、H0-mini、Virchow2、UNI2、Titan）的特征与两类MIL聚合器（AB-MIL、TransMIL）组合评测；比较同分布与异分布测试表现，并做放大倍率/跨倍率聚合研究；提供自动化基准管线。

Result: 同分布测试中，多分类平衡准确率在各倍率均>80%，不同基础模型与两种聚合器表现相近；40x已足够，更高分辨或跨倍率聚合无收益。异分布测试显著下降至~60%平衡准确率，显示明显的泛化问题。

Conclusion: 单HE切片的深度学习在院内可达较高准确率，但对院外泛化不足；需要更大规模、多中心并覆盖更多稀有亚型的数据与研究。作者提供自动化基准管线以推动后续工作。

Abstract: Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.

</details>


### [104] [Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble](https://arxiv.org/abs/2512.14648)
*Daniel Capellán-Martín,Abhijeet Parida,Zhifan Jiang,Nishad Kulkarni,Krithika Iyer,Austin Tapp,Syed Muhammad Anwar,María J. Ledesma-Carbayo,Marius George Linguraru*

Main category: cs.CV

TL;DR: 提出一个灵活可适配的脑肿瘤MRI分割流水线，通过肿瘤/病灶感知的模型选择与前后处理、放射组学辅助的子型识别与均衡训练、及基于病灶级指标的集成优化，在BraTS 2025多子挑战测试集上取得与顶尖算法相当的性能，且不依赖特定网络架构。


<details>
  <summary>Details</summary>
Motivation: 多参数MRI脑肿瘤分割在成人/儿童、不同肿瘤类型与治疗阶段间差异大，模型鲁棒泛化难。BraTS 2025 Lighthouse 提供多样高质量数据集（PED、MEN、MEN-RT、MET）来基准化方法。需要能跨数据域与病灶类型稳定表现、且便于临床量化的解决方案。

Method: 搭建模块化可组合的分割流水线：1）挑选并组合SOTA模型；2）在训练前后进行肿瘤/病灶特异的处理；3）用MRI放射组学识别肿瘤亚型以平衡训练；4）定义病灶级性能度量以加权各模型在集成中的影响；5）基于这些指标优化后处理，按病例自适应微调预测。

Result: 在BraTS 2025多个测试集上，整体性能与排行榜顶尖算法相当；显示该策略在多数据集、多肿瘤场景中具有稳健的泛化能力。

Conclusion: 结合病灶感知的处理与模型选择、并通过放射组学与病灶级指标进行自适应集成，可在不绑定特定网络结构的情况下获得稳健分割；具备用于临床诊断与预后中的定量肿瘤测量潜力。

Abstract: Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.

</details>


### [105] [ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking](https://arxiv.org/abs/2512.14654)
*Lihong Wang,Liangqi Li,Weiwei Feng,Jiamin Wu,Changtao Miao,Tieru Wu,Rui Ma,Bo Zhang,Zhe Li*

Main category: cs.CV

TL;DR: 提出ViRC框架，通过“推理分块（Reason Chunking）”把多模态数学CoT拆成连续关键推理单元（CRUs），并配套CRUX数据集与分阶段训练（两次SFT+策略RL），在多数学基准上较基线平均提升18.8%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在数学任务中常仅基于单张静态图进行一次性文本推理，缺乏像人类那样反复查看视觉信息并逐步验证中间命题的动态过程；需要一种能在推理过程中结构化地整合视觉与文本、符合人类认知规律（Miller定律）的方法。

Method: 1) ViRC框架：将多模态CoT按关键逻辑节点切成连续的“关键推理单元”（CRUs）；每个CRU内保证文本连贯并验证中间命题，跨CRU动态调取视觉信息以生成后续命题。2) 数据：构建CRUX数据集，使用三种视觉工具与四类推理范式，为每题标注多条推理路径的显式CRUs。3) 训练：受人类认知学习启发的渐进式训练——Instructional SFT、Practice SFT、Strategic RL，以强化Reason Chunking能力。4) 模型：得到ViRC-7B。

Result: ViRC-7B在多个数学多模态基准上较基线平均提升18.8%，显示结构化分块推理与动态视觉整合的有效性。

Conclusion: 将多模态数学推理拆分为可验证的CRUs并在跨单元层面动态整合视觉信息，可显著提升MLLM的数学推理性能；CRUX数据与渐进训练策略对强化该能力关键，提供了符合人类专家解题模式的通用范式。

Abstract: CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.

</details>


### [106] [Enhancing Visual Sentiment Analysis via Semiotic Isotopy-Guided Dataset Construction](https://arxiv.org/abs/2512.14665)
*Marco Blanchini,Giovanna Maria Dimitri,Benedetta Tondi,Tarcisio Lancioni,Mauro Barni*

Main category: cs.CV

TL;DR: 他们提出一种基于“符号同构（semiotic isotopy）”概念的流程，用现有多源VSA数据构建更大且情感要素更一致的数据集，并用其训练模型，显著提升跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: VSA面临两大瓶颈：1）情感视觉的多样性极大且标注昂贵，难以获得覆盖充分的大规模数据；2）模型缺乏聚焦图像中“情感相关元素组合”的能力，导致跨数据集泛化差。需要一种方法同时扩展数据规模与提升对情感要素的对齐。

Method: 从多个现有VSA数据集出发，依据“符号同构”理论，在数据集构建阶段对图像进行情感元素与其组合的一致性约束与整合，形成更大、情感语义更清晰的数据集。随后在该数据上训练模型，使其学习到对情感相关元素组合的关注。

Result: 在多个主流VSA基准上，使用该方法生成的数据集训练的模型，较在原始数据上训练的模型表现更好，且跨数据集测试的泛化性能稳定提升。

Conclusion: 通过将符号学的同构概念引入数据集构建，可系统提升VSA数据质量与情感要素对齐度，从而显著增强模型的跨数据集泛化能力。

Abstract: Visual Sentiment Analysis (VSA) is a challenging task due to the vast diversity of emotionally salient images and the inherent difficulty of acquiring sufficient data to capture this variability comprehensively. Key obstacles include building large-scale VSA datasets and developing effective methodologies that enable algorithms to identify emotionally significant elements within an image. These challenges are reflected in the limited generalization performance of VSA algorithms and models when trained and tested across different datasets. Starting from a pool of existing data collections, our approach enables the creation of a new larger dataset that not only contains a wider variety of images than the original ones, but also permits training new models with improved capability to focus on emotionally relevant combinations of image elements. This is achieved through the integration of the semiotic isotopy concept within the dataset creation process, providing deeper insights into the emotional content of images. Empirical evaluations show that models trained on a dataset generated with our method consistently outperform those trained on the original data collections, achieving superior generalization across major VSA benchmarks

</details>


### [107] [ART: Articulated Reconstruction Transformer](https://arxiv.org/abs/2512.14671)
*Zizhang Li,Cheng Zhang,Zhengqin Li,Henry Howard-Jenkins,Zhaoyang Lv,Chen Geng,Jiajun Wu,Richard Newcombe,Jakob Engel,Zhao Dong*

Main category: cs.CV

TL;DR: ART 是一种与类别无关的前馈式模型，可从少量多状态RGB图像重建完整的可动（关节）三维物体，并输出可用于仿真的刚体部件+关节表示。相较依赖慢速优化或限于特定类别的先前方法，ART 以“部件”为中心，用Transformer从稀疏图像解码各部件的几何、纹理与关节参数，达成SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有可动物体重建要么需要脆弱的跨状态对应并进行耗时优化，要么只能在特定类别上前馈推理，难以泛化、效率低。需要一种既快又类别无关、且输出物理可解释的部件+关节表示的方法。

Method: 将可动物体视为刚性部件组合，把重建表述为基于部件的预测。设计一种Transformer，将稀疏多状态RGB输入映射到可学习的“部件槽”（part slots），并从这些槽联合解码每个部件的统一表示：3D几何、纹理以及显式关节/运动参数。使用大规模多样数据集的逐部件监督进行训练。

Result: 在多个基准上显著优于现有基线，建立了从图像进行可动物体重建的新SOTA；输出结果可导出用于物理仿真。

Conclusion: 部件中心的Transformer框架实现了高效、可解释、可导出的类别无关可动物体重建，为从稀疏图像到仿真的端到端流程提供了有效范式。

Abstract: We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.

</details>


### [108] [VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image](https://arxiv.org/abs/2512.14677)
*Sicheng Xu,Guojun Chen,Jiaolong Yang,Yizhong Zhang,Yu Deng,Steve Lin,Baining Guo*

Main category: cs.CV

TL;DR: VASA-3D是一种从单张人像出发、由音频驱动的3D头部头像生成方法。它把VASA-1的2D表情运动潜变量迁移到3D，并通过从单图合成的多帧视频进行优化，最终能实时生成高保真、可自由视角的说话人头像。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时做到：1）精细捕捉真实人脸微表情与口型细节；2）仅凭单张图重建高质量、可多视角的3D头部头像，并能在线实时驱动。

Method: - 采用VASA-1的运动潜变量表示表情与口型动态；- 设计以该运动潜变量为条件的3D头部生成模型，实现2D到3D的运动迁移；- 以单张输入图为个性化目标，先用该图合成同一身份的多帧视频，再对3D模型进行优化；- 使用对伪影与有限姿态覆盖鲁棒的多种损失函数来稳定训练与校准。

Result: 在定性与定量实验中，VASA-3D生成的3D说话人头像在真实感、细节还原与自由视角一致性上优于现有方法；实现最高512x512分辨率、最高约75 FPS的在线自由视角视频生成。

Conclusion: 通过将2D说话人运动潜变量迁移到3D并配合单图自监督优化，VASA-3D实现了高保真、实时的音频驱动3D头像生成，显著超越既有技术并提升交互沉浸感。

Abstract: We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.

</details>


### [109] [Native and Compact Structured Latents for 3D Generation](https://arxiv.org/abs/2512.14692)
*Jianfeng Xiang,Xiaoxue Chen,Sicheng Xu,Ruicheng Wang,Zelong Lv,Yu Deng,Hongyuan Zhu,Yue Dong,Hao Zhao,Nicholas Jing Yuan,Jiaolong Yang*

Main category: cs.CV

TL;DR: 提出O-Voxel稀疏体素与稀疏压缩VAE，并用大规模flow-matching模型生成高质量、复杂拓扑与丰富材质的3D资产，推理高效、质量显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成表示难以同时处理复杂拓扑（开/非流形/封闭）和丰富外观属性，导致生成资产的几何与材质质量受限，需要一种既能表达复杂拓扑又能高效压缩与生成的原生3D潜表示。

Method: 1) 提出O-Voxel（omni-voxel）稀疏体素，统一编码几何与外观（包括PBR参数），稳健覆盖任意拓扑。2) 基于O-Voxel设计Sparse Compression VAE，实现高空间压缩率与紧致潜空间。3) 以多样公共3D资产训练含约40亿参数的flow-matching生成模型，在O-Voxel潜空间中进行3D生成与解码。

Result: 模型在几何与材质质量上显著优于现有3D生成方法；能表示复杂拓扑与丰富表面属性；尽管模型规模巨大，但推理效率高。

Conclusion: O-Voxel与稀疏压缩VAE结合大规模flow-matching训练，为3D生成提供了结构化潜表示与高效推理路径，显著提升生成资产的几何与材质质量，推进3D生成前沿。

Abstract: Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.

</details>


### [110] [CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives](https://arxiv.org/abs/2512.14696)
*Zihan Wang,Jiashun Wang,Jeff Tan,Yiwen Zhao,Jessica Hodgins,Shubham Tulsiani,Deva Ramanan*

Main category: cs.CV

TL;DR: CRISP从单目视频同时重建可仿真的人体运动与场景几何，通过拟合平面元得到干净、凸性的“可模拟”场景，并用RL控制器验证物理可行性，显著降低跟踪失败率并提升仿真吞吐。


<details>
  <summary>Details</summary>
Motivation: 现有联合人-场景重建常依赖强数据先验、缺乏物理约束，或得到噪声/非凸几何，导致与场景交互的运动跟踪在仿真中频繁失败并效率低。需要一种既能恢复干净可模拟几何，又能保证人体与场景交互物理可行的方法。

Method: 1) 从单目视频重建点云与深度/法向/光流；2) 基于深度、法向、流的聚类，将场景分割并用平面原语拟合，得到凸且干净的几何；3) 利用人体-场景接触先验推断被遮挡几何（如坐姿推断椅面）；4) 用重建的人体与场景驱动类人控制器，采用强化学习验证并优化物理可行性与稳定性。

Result: 在EMDB与PROX等以人为中心的视频基准上，将运动跟踪失败率从55.2%降至6.9%，并使强化学习仿真吞吐提升43%；还能在野外视频（随手拍、互联网、Sora生成）上稳定工作。

Conclusion: 通过用平面原语构建凸、干净的可模拟场景，并以RL物理闭环验证，CRISP能大规模生成物理有效的人体运动与交互环境，推动现实到仿真的机器人与AR/VR应用。

Abstract: We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.

</details>


### [111] [Spherical Leech Quantization for Visual Tokenization and Generation](https://arxiv.org/abs/2512.14697)
*Yue Zhao,Hanwen Jiang,Zhenlin Xu,Chutong Yang,Ehsan Adeli,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 论文将多种非参数量化方法统一为“晶格编码”视角，并提出基于Leech晶格的球面量化(Λ24-SQ)，在图像标记化、压缩与自回归生成中，以更少比特得到优于BSQ的重建质量与更简化的训练。


<details>
  <summary>Details</summary>
Motivation: 非参数量化具有参数高效、可扩展到大码本等优势，但现有无查表变体（如BSQ）在自编码器训练中常需额外损失项且存在重建—压缩折衷的局限。缺乏统一理论解释与更优几何结构的量化方案激发了该研究。

Method: 以晶格编码为统一框架分析非参数量化的几何性质，解释为何某些变体需辅助损失；系统考察多种晶格候选（随机晶格、广义Fibonacci晶格、最密球堆积晶格），最终提出利用Leech晶格在超球面上均匀分布的Spherical Leech Quantization (Λ24-SQ)；将其用于图像tokenization与压缩，并与BSQ等方法比较。

Result: Λ24-SQ在相同或更低比特率下，在所有重建指标上均优于BSQ；训练流程更简化；在最先进的自回归图像生成框架中也带来性能增益。

Conclusion: 晶格视角统一了非参数量化方法并揭示辅助损失的必要性；基于高对称、均匀分布的Leech晶格的Λ24-SQ可在重建质量与压缩率间取得更好折衷，同时简化训练，并对生成任务同样有效。

Abstract: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($Λ_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.

</details>


### [112] [TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2512.14698)
*Jun Zhang,Teng Wang,Yuying Ge,Yixiao Ge,Xinhao Li,Ying Shan,Limin Wang*

Main category: cs.CV

TL;DR: 提出TimeLens：通过高质量数据与算法设计两条路径，重建VTG评测与训练范式，发布重标注基准TimeLens-Bench与高质训练集TimeLens-100K，并结合时间表达的交错文本编码与“无需思维链”的可验证奖励RL训练，获得在开源中SOTA、甚至超越部分专有模型的VTG表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视频理解强，但在视频时间定位（VTG）上优化配方缺失；同时，现有基准与训练数据存在严重噪声，导致排名不可靠、研究结论不稳，需要一个干净的评测与系统化方法学。

Method: 1) 数据面：系统暴露并修复基准标注问题，构建严格重标注的TimeLens-Bench；通过自动化重标注流水线清洗训练数据，得到大规模高质TimeLens-100K。2) 算法面：探索设计原则，包含用于时间表达的交错式文本编码；引入“无思维链”的可验证奖励强化学习（RLVR）作为训练范式，并提供稳定有效的训练配方。

Result: 在TimeLens-Bench上模型排名发生显著重排，证明旧基准不可靠；基于TimeLens数据与RLVR/编码策略的TimeLens系列模型在开源范围内达成VTG SOTA，并报告超越GPT-5与Gemini-2.5-Flash等专有模型的性能。

Conclusion: 高质量数据与针对性的训练范式是VTG性能的关键。通过干净评测、自动化数据校正与高效RLVR+时间编码，TimeLens为MLLM在VTG任务提供强健基线与可复制的优化路径，并将资源全面开源以推动后续研究。

Abstract: This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

</details>


### [113] [MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives](https://arxiv.org/abs/2512.14699)
*Sihui Ji,Xi Chen,Shuai Yang,Xin Tao,Pengfei Wan,Hengshuang Zhao*

Main category: cs.CV

TL;DR: MemFlow是一种用于流式视频生成的动态记忆机制，通过按片段文本提示检索最相关历史帧并仅激活最相关记忆token，提升长时一致性与效率，几乎不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频生成在长时一致性上困难，原因是需要有效记忆设计。主流方法用预设策略压缩历史帧，但不同待生成片段需要的历史线索不同，固定压缩难以兼顾事件变换与场景切换下的叙事连贯。

Method: 提出MemFlow：1）在生成每个新视频片段前，用该片段的文本提示检索历史中最相关的帧，动态更新记忆库；2）在注意力计算时，对每个查询仅激活记忆库中最相关的token（稀疏激活），减少无关干扰与计算量；3）可与带KV缓存的任意流式视频生成模型兼容。

Result: 在保证长上下文一致性的同时，计算负担极小，仅比无记忆基线慢7.9%，表现出显著的叙事连贯提升和生成效率。

Conclusion: 动态、按需检索与稀疏激活的记忆机制能在流式视频生成中保持长时一致性并兼顾效率，且作为通用模块易于接入现有模型。

Abstract: The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.

</details>
