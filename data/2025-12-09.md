<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 224]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

TL;DR: 论文声称：最新视频生成模型已具备一定“推理”能力。在象棋、迷宫、数独、心理旋转、瑞文矩阵等任务上，顶尖模型（如 Sora‑2）约达 60% 成功率。作者提出“任务成对（Task Pair）”范式与自动化评测框架（含 39 个模型），并证明自动评分与人工高度相关，可规模化扩展；据此可开展强化学习以进一步提升视频模型推理。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在理解与推理方面的真实能力与评测缺口尚大；缺乏统一、可扩展且可重复的实验范式与代码框架来系统比较不同模型的“推理”表现。作者希望提供标准化评测与证据，验证自动化评测可替代大量人工，从而推动用 RL 等方法强化推理。

Method: 1) 设计“Task Pair”实验范式，对同类任务构建系统化、成对的评测样例；2) 覆盖多类认知/推理任务（象棋、迷宫、数独、心理旋转、瑞文矩阵）；3) 搭建可扩展代码框架（VMEvalKit），已接入 39 个模型，便于新增任务与模型；4) 构建自动化评价指标，并与人工标注进行相关性验证。

Result: 在所测任务上，领先视频模型（如 Sora‑2）约达 60% 成功率；自动化评测与人类评判呈强相关；框架可顺畅扩展到更多模型与任务。

Conclusion: 视频生成模型已展现非平凡的推理迹象，但整体仍有限（约 60%）；“Task Pair”范式与VMEvalKit为大规模评测提供可行基础；自动评测的可靠性使基于此的强化学习训练成为可行方向。

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [2] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

TL;DR: 提出一种数据集量化方法，在保持训练性能的同时大幅压缩图像数据，通过样本内冗余压缩与自适应比特分配，在CIFAR/ImageNet等上优于传统剪枝/蒸馏/统一量化基线。


<details>
  <summary>Details</summary>
Motivation: 边缘设备存储与通信受限，现有方法多通过删样本或蒸馏处理样本间冗余，忽视了单个样本内部的冗余。需要一种能在不牺牲训练有效信息的前提下，用更少比特表示原始数据集的方法。

Method: 分两步：1）对每个样本进行线性对称量化，得到初始量化范围与scale；2）提出自适应量化分配算法，在总体压缩率固定的约束下，为不同样本分配不同量化比特/比例以满足其精度需求，实质是样本级可变比特率量化，突出关键信息、压缩冗余内容。

Result: 在CIFAR-10/100与ImageNet-1K上，方法在相同压缩率下保持训练性能（精度基本不降或降幅更小），并显著降低存储与通信开销，优于传统统一量化和数据集剪枝基线。

Conclusion: 数据集层面的自适应量化能够有效压缩样本内冗余，在固定压缩预算下更好地保留关键信息，从而在大幅减少数据集存储与传输成本的同时维持模型训练效果；该方向为资源受限场景提供了新范式。

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [3] [VG3T: Visual Geometry Grounded Gaussian Transformer](https://arxiv.org/abs/2512.05988)
*Junho Kim,Seongwon Lee*

Main category: cs.CV

TL;DR: 提出VG3T：直接从多视角联合预测带语义属性的3D高斯，占据重建更一致、更高效；在nuScenes上mIoU提升1.7个百分点，原语数量减少46%。


<details>
  <summary>Details</summary>
Motivation: 多视图融合易导致碎片化与不一致，单视图推断高斯在跨视几何与语义整合上不足，且像素对齐初始化存在随距离变稀的密度偏置。需要一种统一且高效的3D语义占据表示与推理方式。

Method: 提出VG3T：多视图前馈网络，直接预测一组带语义属性的3D高斯以表示语义占据与几何；引入两组件—基于网格的采样（Grid-Based Sampling）与位置精炼（Positional Refinement）—以缓解距离相关的密度偏置；联合多视图而非逐视图处理，输出更一致的高斯集合。

Result: 在nuScenes基准上，相比前SOTA，mIoU提高1.7个百分点，同时所需高斯原语减少46%，体现更高的精度与效率。

Conclusion: 联合多视图直接预测语义高斯可提供统一、紧凑且一致的3D场景表示；配套的采样与精炼策略缓解距离密度偏置，带来更好性能-效率权衡。

Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.

</details>


### [4] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: 提出EmoDiffTalk：一种基于情感感知高斯扩散的可编辑3D高斯说话人模型，实现细粒度AU驱动与文本到AU控制，显著提升情感细腻度、口型同步与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian Splatting的写实三维说话人对情感编辑支持不足，尤其难以进行细粒度、连续、跨模态（如文本）控制的丰富情绪操控。

Method: 提出Emotion-aware Gaussian Diffusion：1）AU-prompt高斯扩散生成器作为细粒度面部动画器；2）精确的Text-to-AU情感控制器，将文本情绪映射到连续AU表达空间，从而实现可控、连续、多模态的情感编辑。

Result: 在EmoTalk3D与RenderMe-360数据集上，EmoDiffTalk在情感细腻度、口型同步准确度和可控性方面优于现有方法。

Conclusion: EmoDiffTalk为基于扩散与3D高斯的可编辑三维说话人提供了系统化方案，率先实现于AU表达空间内的连续、多模态情感编辑。

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [5] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

TL;DR: 提出NeuroFM：面向脑组织的领域特化病理大模型，在多项神经病理任务上优于通用病理基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型主要训练于外科病理（非神经组织、肿瘤/炎症等），与神经病理的细胞类型、架构与特异性病理（神经原纤维缠结、淀粉样斑块、路易小体等）存在显著域差，限制其在神经退行性疾病判读中的表现。

Method: 构建并训练NeuroFM：以多种神经退行性病变的脑组织全切片影像为训练数据，学习能够捕捉神经病理形态学特征的表示；随后在多个面向神经病理的下游任务上进行评测。

Result: NeuroFM在混合性痴呆分类、海马区分割、以及小脑相关退行性共济失调（含本态性震颤与多种脊髓小脑性共济失调亚型）识别等任务上均优于通用基础模型。

Conclusion: 领域专用的脑组织基础模型能更好地捕捉神经病理特征，提升神经退行性疾病的AI分析准确性与可靠性，倡导在数字病理细分领域发展定制化基础模型。

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [6] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

TL;DR: FishDetector-R1 是一个面向水下鱼类图像的统一多模态大模型（MLLM）框架，在弱监督下同时完成检测、分割与计数。在 DeepFish 上显著超越基线：AP +20%、mIoU +10%，计数 MAE −30%、GAME −35%。核心贡献是“从检测到计数”的提示策略与基于可验证奖励的强化学习（RLVR），并结合稀疏点标注的可扩展监督。方法在跨数据集上也具备鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 水下图像受散射、色偏、低对比等退化影响，导致检测和分割困难；同时密集标注代价高昂，限制了大规模部署。现有方法在弱监督、多任务统一与跨域泛化上存在不足，需要一种能用低成本标注实现稳定准确检测/分割/计数的统一框架。

Method: 提出 FishDetector-R1：1) 统一 MLLM 框架，支持检测、语义/实例分割与计数。2) Detect-to-Count 提示：通过提示设计把空间一致性的检测结果与全局计数约束耦合，减少漏检/重检。3) RLVR：构建可验证奖励函数，以检测-计数一致性、分割质量等指标为信号进行强化学习；4) 可扩展弱监督：利用稀疏点标注与可扩展范式降低标注成本；5) 通过消融实验与跨数据集评测验证。

Result: 在 DeepFish 上，相比基线 AP 提升约20%，mIoU 提升约10%；计数指标 MAE 降低约30%，GAME 降低约35%。消融表明 detect-to-count 提示与 RLVR 奖励均显著贡献；在其他水下数据集上也保持性能提升，显示良好的跨域鲁棒性。

Conclusion: FishDetector-R1 在弱监督条件下实现对水下鱼类的高精度检测、分割与计数，成本更低且具备跨域泛化能力。核心机制是空间一致性提示与可验证奖励的强化学习，提供了可扩展的海洋视觉理解解决方案。项目主页见链接。

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [7] [PrunedCaps: A Case For Primary Capsules Discrimination](https://arxiv.org/abs/2512.06003)
*Ramin Sharifi,Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 作者研究在胶囊网络（CapsNet）中裁剪主胶囊（Primary Capsules, PCs），在MNIST、Fashion-MNIST、CIFAR-10、SVHN上可在准确率不降的情况下删除高达95%的PCs，使推理最高加速约9.9倍，并在动态路由阶段减少约95.36%的FLOPs；并分析为何不同数据集受益程度不同。


<details>
  <summary>Details</summary>
Motivation: 尽管CapsNet在抗仿射变换和重叠目标识别上优于CNN，但其主胶囊数量巨大，训练/推理缓慢且资源开销大，限制了实际应用。作者希望通过对PCs进行剪枝，提升效率同时保持精度，并理解数据集间差异。

Method: 对CapsNet的Primary Capsules进行系统性剪枝（移除大量PCs），评估在MNIST、Fashion-MNIST、CIFAR-10、SVHN上的精度、推理速度与动态路由FLOPs变化，并对不同数据集的受益差异进行分析与解释。

Result: 在不损失准确率的前提下，可剪掉多达95%的PCs；推理最高提速约9.90倍；动态路由阶段FLOPs降低超过95.36%；不同数据集受益程度存在显著差异。

Conclusion: 主胶囊剪枝能显著提升CapsNet的效率且保持精度，使CapsNet更具实用性；数据集结构与特征复杂度影响剪枝收益大小，需根据数据集特性调整剪枝策略。

Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.

</details>


### [8] [Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization](https://arxiv.org/abs/2512.06006)
*Xuefei,Wang,Kai A. Horstmann,Ethan Lin,Jonathan Chen,Alexander R. Farhang,Sophia Stiles,Atharva Sehgal,Jonathan Light,David Van Valen,Yisong Yue,Jennifer J. Sun*

Main category: cs.CV

TL;DR: 提出并实证评估一种面向“最后一公里”科学影像管线适配的代理式代码优化框架；简单代理在多个生物医学成像任务上生成的适配代码优于人类专家方案，并给出实用的代理设计路线图与开源框架。


<details>
  <summary>Details</summary>
Motivation: 科研领域常需将成熟的计算机视觉工具适配到小众、异质的科学数据集。现有途径要么需要大量标注来微调模型（科学家往往缺乏），要么需要大量手工代码改造（耗时数周至数月）。作者探索：能否用AI代理自动化“手工适配”，以及实现此目标的最佳代理设计。

Method: 提出一个系统化的代理式代码优化评测框架，选取三条生产级生物医学影像处理流水线作为基准，比较不同代理架构（从简单到复杂）的适配与优化能力，生成可直接部署的函数与代码模块，并与人类专家方案对照评测；同时在真实生产管线中在线部署代理生成的函数进行验证。

Result: 简单的代理框架在三条生物医学影像管线上，稳定地生成比人类专家更优的适配代码；复杂的多组件/层级代理并非普遍更好。框架与基准开源，且在生产环境成功部署，显示实际可用性与性能提升。

Conclusion: 针对科学影像管线“最后一公里”适配，AI代理可有效自动化代码改造；应优先采用简单、直接的代理设计，再按需增配复杂机制。开源评测框架与实际部署证明其现实影响与可推广性。

Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.

</details>


### [9] [Fast and Flexible Robustness Certificates for Semantic Segmentation](https://arxiv.org/abs/2512.06010)
*Thomas Massena,Corentin Friedrich,Franck Mamalet,Mathieu Serrurier*

Main category: cs.CV

TL;DR: 提出一种内置Lipschitz约束的可认证鲁棒语义分割网络及通用认证框架，实现实时、可计算最坏情况性能，并显著快于随机平滑。


<details>
  <summary>Details</summary>
Motivation: 现有对抗鲁棒研究多集中在分类任务，语义分割的高效认证方法稀缺；随机平滑等方法慢、难以实时；需要能够在分割任务上提供可证明的鲁棒性与可操作的计算开销。

Method: 构建具有内置Lipschitz约束的分割网络，实现端到端高效训练；提出通用的语义分割鲁棒性认证框架，能在ℓ2球半径ε下对多种性能指标（如像素精度等）给出最坏情况下界；利用Lipschitz边界与结构化分析以高效计算证书，并与随机平滑进行速度与证书强度对比。

Result: 在Cityscapes等数据集上取得具有竞争力的像素精度；认证过程在NVIDIA A100上推理时比随机平滑快约600倍，证书可比；能实时兼容地输出鲁棒证书；提供对最坏情况性能的计算。

Conclusion: 内置Lipschitz网络与通用认证框架使语义分割首次达到实时可认证鲁棒；在保证证书质量的同时大幅降低计算成本，并通过与SOTA攻击对比验证证书紧致性与方法有效性。

Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.

</details>


### [10] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

TL;DR: 提出一种将高通量成像与无监督聚类相结合的自动化框架，用于工业规模SLM金属粉末形貌快速量化；基于约12.6万颗粒数据，傅里叶形状描述符+k-means在精度与速度上最优，支持实时原料监测与复用周期跟踪。


<details>
  <summary>Details</summary>
Motivation: SLM零件性能高度依赖粉末形貌，但传统粉末表征通量低、偏定性、难以反映大批量异质性，亟需可扩展、自动化、可量化的形貌表征方法。

Method: 构建高通量显微成像→形状特征提取→无监督聚类的机器学习流水线；比较三种管线：自编码器（深度表示）、基于显式形状描述符（如傅里叶描述符）与k-means、以及函数型数据方法；用内部有效性指标（Davies–Bouldin、Calinski–Harabasz）与运行时评估。

Result: 在约12.6万颗粒（0.5–102 μm）数据上，傅里叶描述符+k-means获得最低DB指数与最高CH分数，并保持单颗粒亚毫秒级推理速度（台式机），优于自编码器与函数型方案。

Conclusion: 该无监督框架可快速、自动地聚类粉末形貌，支持跨复用周期的形状演化追踪，构建形貌—流动性、堆积密度与SLM质量关联研究的基础，并为SLM过程原料的准实时监测提供可行路径。

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [11] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

TL;DR: 提出Vision Action Transformer (VAT)，利用ViT全层特征与动作token逐层融合，显著提升机器人模仿学习性能，在LIBERO基准上平均成功率98.15%，刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于ViT的机器人视觉感知多仅用最后一层特征，丢弃了中间层蕴含的丰富表征，导致感知-动作耦合不足、表示不充分。作者希望解锁ViT完整层级“表示轨迹”，提高策略生成质量。

Method: 在标准ViT上引入专用的动作token，使其在每一层与视觉token共同参与自注意力与交互；通过跨层逐步融合，实现从浅层到深层的感知-动作联合建模；整体形成一个端到端的Vision Action Transformer，用于模仿学习策略。

Result: 在一系列仿真操作任务与四个LIBERO基准上，VAT达到98.15%的平均成功率，超过包括OpenVLA-OFT在内的现有方法，建立新的SOTA。

Conclusion: 利用ViT全层特征的“表示轨迹”对机器人策略学习至关重要；VAT作为简单有效的架构，在模仿学习中取得强性能，并为未来将多层视觉表征融入决策提供范式；代码开源于GitHub链接。

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [12] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

TL;DR: 对两种胸片基础模型在MIMIC-CXR与NIH ChestX-ray14上的统一基准对比：MedImageInsight略优，CXR-Foundation更稳；采用固定预处理与下游分类器，报告AUROC与F1并做聚类分析。


<details>
  <summary>Details</summary>
Motivation: 尽管医学影像基础模型表现强，但不同数据集间的可比性与稳健性研究不足，缺乏统一、可复现的评测规范与基线。

Method: 在统一预处理下，直接从两种预训练CXR编码器（ELIXR v2.0与MedImageInsight）提取嵌入；对多疾病标签训练轻量级LightGBM分类器；在MIMIC-CXR与NIH ChestX-ray14上，用固定设置评估并报告平均AUROC与F1（95%置信区间）；对MedImageInsight嵌入做无监督聚类以分析语义结构。

Result: MedImageInsight在多数任务上略高的AUROC/F1；CXR-Foundation在跨数据集迁移上更稳定；MedImageInsight的聚类呈现与疾病标签一致的清晰结构。

Conclusion: 需要标准化的医学基础模型评估流程；本文提供可复现的基线，支持未来多模态与临床集成研究。

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [13] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出一种利用多模态大语言模型抽取用户偏好表征，并通过对齐损失注入扩散模型，实现更贴合个人审美的图像生成的方法，显著提升图像质量与偏好一致性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像生成难以捕捉细粒度偏好，或缺乏有效的个性化视觉信号编码机制；需要一种既能表达用户细微审美、又能与扩散模型兼容的统一框架。

Method: 构建基于MLLM的多模态框架：1) 以“偏好导向的VQA”任务训练MLLM提取细粒度语义与审美线索；2) 设计两类探测任务隔离偏好相关特征：跨用户区分（识别不同用户）与用户内区分（区分喜欢/不喜欢）；3) 为与扩散模型文本编码器兼容，提出基于最大均值差异（MMD）的对齐损失，既弥合模态差距又保持多模态结构；4) 将得到的用户嵌入作为条件注入扩散生成器，与文本提示共同控制生成。

Result: 在多项实验中，相较强基线显著提升图像质量与与用户偏好的一致性，验证了偏好表征提取与对齐策略的有效性。

Conclusion: 通过MLLM驱动的偏好表征与MMD对齐，将用户个性嵌入扩散生成过程，可有效实现对提示与个人审美的双重遵循，优于现有方法。

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [14] [Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing](https://arxiv.org/abs/2512.06024)
*Jiabin Liu,Zihao Zhou,Jialei Yan,Anxin Guo,Alvise Benetazzo,Hui Li*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.

</details>


### [15] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

TL;DR: 论文分析了SAM2与SAM3之间的根本断裂：从以空间提示为核心的几何/时序分割，跃迁到多模态、概念驱动的开放词汇分割。指出在概念、架构、数据与标注、训练范式与超参、评测与失效模式上的全面差异，并将SAM3定位为新一代概念驱动分割基础模型。


<details>
  <summary>Details</summary>
Motivation: 社区常假设SAM系列能力可沿用迁移，但实践中发现SAM2在提示式分割的专业性难以转移到SAM3的概念驱动多模态范式。需要系统阐明两者在目标、机制与评测上的不连续性，以避免误用经验并指导未来研究。

Method: 从五个维度对比分析：1) 概念层面（提示式vs概念式、多模态融合、文本条件掩码生成）；2) 架构层面（SAM2的纯视觉/时序流程vs SAM3的视觉—语言编码器、几何与示例编码器、融合模块、DETR式解码器、对象查询、MoE处理歧义）；3) 数据与标注（SA-V视频掩码vs 多模态概念标注语料）；4) 训练与超参（优化目标、损失、对齐/对比学习差异）；5) 评测与失败模式（几何IoU到语义/开放词汇评测）。

Result: 论证了SAM2到SAM3存在范式级的跃迁：SAM2擅长基于点/框/掩码的几何与时序分割；SAM3具备开放词汇语义推理、语义定位、对比对齐与示例驱动概念理解等能力。由此导致经验不可直接迁移，且评测指标与失败模式显著不同。

Conclusion: SAM3应被视为新类别的分割基础模型，代表概念驱动分割时代的开端。未来方向包括：更强的多模态对齐与推理、标注与数据的语义覆盖扩展、消歧与不确定性建模、面向开放词汇的稳健评测，以及将几何与概念能力更好地统一。

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [16] [Representation Learning for Point Cloud Understanding](https://arxiv.org/abs/2512.06058)
*Siming Yan*

Main category: cs.CV

TL;DR: 论文围绕3D点云表征学习，提出以2D预训练模型辅助3D网络训练的框架，覆盖监督分割、自监督与2D→3D迁移，实验显示显著提升3D理解能力。


<details>
  <summary>Details</summary>
Motivation: 3D数据在视觉、机器人、遥感与医疗等领域快速普及，但3D标注昂贵、数据稀疏且多模态融合困难，限制了点云表征学习的性能与泛化。已有方法往往仅将2D信息投影或简单对齐，难以充分吸收2D大规模预训练知识。

Method: 1) 监督学习：针对点云基础元（primitive）分割，设计能显式利用几何结构的网络；2) 自监督：构建无需标签的预训练任务提升点云表征；3) 迁移学习：以预训练2D模型为教师/先验，通过蒸馏、对齐或跨模态约束，指导3D网络训练，而非仅把2D数据做形式上的转换。

Result: 在多项数据集与任务上进行大量实验，所提框架在分割与表示质量上优于对比方法，证明2D知识有效增强3D理解与泛化。

Conclusion: 有效整合2D预训练知识可显著推进3D点云表征学习；所提方法在监督、自监督与跨模态迁移三个方面均取得提升，显示在多领域应用中的潜力。

Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

</details>


### [17] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

TL;DR: 提出一套面向第一视角（egocentric）视频的指令驱动编辑生态：数据集EgoEditData、实时编辑器EgoEdit、评测基准EgoEditBench，实现低延迟、手与交互保真、时间稳定的编辑效果，显著优于现有方法并在通用任务上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法多针对第三人称视频，难以处理第一视角中快速机位运动与频繁手-物交互，且离线管线延迟高，无法满足交互式AR的实时需求。

Method: (1) 构建并人工策划EgoEditData，聚焦保留手部与手-物交互的第一视角编辑场景；(2) 设计支持指令跟随、单GPU实时流式推理的编辑模型EgoEdit；(3) 提出评测套件EgoEditBench，面向指令忠实度、手与交互保留、以及在自运动下的时间稳定性进行评价。

Result: EgoEdit在第一视角与通用编辑任务上均产生时间稳定、指令忠实且交互式低延迟的结果；在第一视角基准上取得显著提升，而在通用编辑任务上与最强基线相当。

Conclusion: 面向第一视角AR交互的端到端编辑生态可有效弥合第一视角域差与实时性瓶颈；公开的数据集与基准为社区研究提供基础设施。

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [18] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

TL;DR: 用单次测量的单光子激光雷达，在多点同时照明与多次反射条件下，通过学习式反演多跳光输运，从两跳光中分解各激光点贡献，实现含遮挡与镜面场景的3D重建。


<details>
  <summary>Details</summary>
Motivation: 单次测量重建3D在遮挡和镜面材料下很难；单光子激光雷达的多跳（尤其两跳）信号蕴含额外几何与材质信息，但既有方法多依赖逐点扫描，难以扩展到更实用的多点同时照明场景，且光输运解析反演复杂。

Method: 提出数据驱动的光输运反演：构建首个约10万条室内场景仿真激光雷达瞬态数据集，学习复杂光输运先验，将实测两跳瞬态分解为各激光斑的成分；在此基础上从单次测量中推断稠密深度、遮挡几何与材料属性。

Result: 在合成与实测中验证：方法能在存在遮挡与镜面的场景里，从一次测量分解两跳光并重建3D几何；相较需要逐点扫描的先前工作，在多点同时照明设置下取得可行且准确的结果。

Conclusion: 学习式分解多跳光使单光子雷达在多点同时照明下实现单次测量的鲁棒3D重建，扩展了该传感方式在复杂室内场景中的实用性；开源代码与数据集促进后续研究。

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [19] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

TL;DR: BeLLA 将360°鸟瞰图（BEV）统一空间表示与大语言模型连接，用于自动驾驶问答，在需空间推理的问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM/MLLM多依赖单视角编码或简单聚合多视角特征，缺乏统一空间表示，难以进行以自车为中心的方向、目标关系与上下文推理。

Method: 提出端到端架构BeLLA：先从多摄像头构建统一的360° BEV表示，再将该空间表达对接至LLM进行问答与推理，实现从感知到语言的闭环；在NuScenes-QA与DriveLM上评测。

Result: 在需要更强空间推理的题型（如相对位置、周边目标行为理解）上取得最多+9.3%的绝对提升；其他类别表现具竞争力。

Conclusion: 统一BEV空间表示与LLM的对接能显著提升自动驾驶问答中的空间与行为理解能力，并在多类问题上保持稳健表现。

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [20] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 提出SpectraIrisPAD：基于DINOv2 ViT并结合可学习光谱位置编码、token融合与对比学习的多光谱虹膜活体检测方法；并发布MSIrPAD数据集（5个NIR波段、8类攻击、18,848张图像）。在未见攻击协议下优于多项SOTA，展现更强鲁棒性与泛化。


<details>
  <summary>Details</summary>
Motivation: 传统虹膜识别主要在单一近红外波段工作，易受展示攻击（假体、打印、显示、纹理隐形眼镜等）影响，现有PAD在跨设备/跨攻击泛化差。多光谱NIR可提供互补反射特征，有望提高PAD泛化，但缺少针对性的模型设计与大规模多光谱数据集。

Method: 构建SpectraIrisPAD框架：以DINOv2 ViT为骨干，引入（1）可学习的光谱位置编码以建模不同NIR波段间差异；（2）token融合机制整合跨波段信息；（3）对比学习以强化类间分离与跨攻击泛化。配套采集并标注MSIrPAD数据集，使用定制多光谱传感器在800/830/850/870/980 nm五个波段成像，涵盖五类纹理美瞳、打印与显示攻击，共18,848张图像。采用未见攻击评估协议进行实验并与多种基线对比。

Result: 在未见攻击（unseen attack）设置下，SpectraIrisPAD在各项指标上均超过多种SOTA基线，表现出更好的鲁棒性与泛化能力，能有效区分真实样本与多种伪装伪造。

Conclusion: 多光谱信息与专门的光谱感知建模（光谱位置编码+token融合+对比学习）可显著提升虹膜PAD的泛化与鲁棒性。MSIrPAD为该方向提供了标准化基准。方法在五个NIR波段与多类PAI上表现优越，适用于实际部署场景中抵御多样化展示攻击。

Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [21] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

TL;DR: 提出CEFM跨模态可解释框架，将皮肤科ABC临床准则与ViT视觉特征对齐，通过对比学习与双投影头实现可解释分类，并生成结构化文本解释；在公开数据上达92.79%准确率、AUC 0.961和多项可解释性指标提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习在黑色素瘤分类上已达专家水平，但“黑箱”问题和缺乏可解释性阻碍临床落地。需要把临床可理解的诊断准则与模型内部表示建立可追溯关联，从而提升医生信任与可用性。

Method: 构建CEFM框架：以对比学习为核心，将临床ABC（不对称、边界、颜色）规则通过双投影头映射到ViT嵌入空间，实现临床语义与视觉特征对齐；随后利用自然语言生成把对齐后的表示转化为结构化文本解释，形成从图像到临床解释的透明链路。

Result: 在公开数据集上达到92.79%准确率与0.961 AUC；多项可解释性评估显著优于对比方法；定性分析显示嵌入空间的空间布局与临床ABC规则的应用一致。

Conclusion: CEFM在保持高分类性能的同时显著提升可解释性，通过跨模态对齐与文本解释有效增强临床可用性与信任，为黑色素瘤AI诊断的落地提供可行路径。

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [22] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

TL;DR: 提出Track4DGen：将多视角视频扩散、通用点跟踪器与混合4D高斯喷溅重建耦合，通过显式跟踪先验注入中间特征，实现外观与运动跨视角与时间一致的动态4D生成，并在基准上优于现有方法，同时发布Sketchfab28数据集。


<details>
  <summary>Details</summary>
Motivation: 稀疏输入下的动态4D生成易出现跨视角/跨时间的不一致、外观漂移与伪影。现有方法多依赖像素或潜空间的视频扩散损失，缺乏显式的时间感知、特征级跟踪约束，导致视差与时域漂移。作者欲通过引入跟踪先验，在特征层面提供时空一致性指导。

Method: 两阶段框架Track4DGen：1）在多视角视频扩散模型内部，使用基础点跟踪器产生密集点对应，并将其作为运动先验注入中间特征，施加特征级一致性约束，抑制外观漂移、提升跨视角一致性；2）用混合4D-GS重建：将与空间共位的扩散特征（携带第一阶段跟踪先验）与Hex-plane特征拼接作为运动编码，并结合4D球谐基函数以更高保真建模动态；最终得到时间稳定、可文本编辑的4D资产。

Result: 在多视角视频生成与4D生成基准上均超越对比方法，生成的对象在时间上更稳定、跨视角一致性更强；此外构建了高质量对象中心的Sketchfab28数据集用于4D生成评测。

Conclusion: 显式将点跟踪先验注入扩散与4D-GS特征，可有效改善稀疏输入下动态4D生成的时空一致性与保真度；Track4DGen在多项基准上领先并促进可编辑4D资产生成，数据集Sketchfab28将推动后续研究。

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [23] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

TL;DR: 提出一种利用深度学习从剪切散斑干涉（shearography）测量中自动生成缺陷标注的流程，可输出高分辨率分割与框选标签；与专家标注对比精度足以进行弱监督训练，从而减少人工、促进可规模化数据集构建。


<details>
  <summary>Details</summary>
Motivation: 工业界难以大规模采用剪切散斑检测的瓶颈在于缺乏高质量、标准化的带注释数据集；人工标注费时、主观且难统一，限制了鲁棒缺陷检测模型的训练与验证。

Method: 构建自动化工作流：以shearography测量为输入，借助深度学习模型自动产生缺陷的高分辨率语义分割与目标框注释；随后与专家标注进行对齐与评估，以验证可用性。

Result: 自动标注与专家标注吻合度较高，达到“足以支撑弱监督训练”的准确度水平，可在保证检测能力的同时显著降低人工标注工作量。

Conclusion: 该自动化标注流程能有效缓解数据标注瓶颈，支持规模化数据集构建，从而提升基于shearography的缺陷检测的实用性与工业落地潜力。

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [24] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 将物理建模（几何+光照）显式嵌入扩散模型的阴影生成框架：先估3D点云与主光方向，用物理投影得初始阴影，再由扩散模型细化，显著提升真实感与物理一致性，尤其在复杂几何/模糊光照场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习阴影生成多依赖数据驱动纹理与统计先验，缺少对“遮挡几何与光照方向”这一物理成因的显式约束，导致在复杂几何或不明确光照下位置、形状不稳或违背物理。作者希望用可解释的物理先验提高可控性与一致性。

Method: 1) 从单目RGB估计场景近似3D几何（稠密点图）与单一主导光方向；2) 根据几何与光照用物理投影机制计算初始阴影位置与轮廓；3) 将该物理先验作为条件输入扩散模型，进行外观细化与去噪，既保留物理一致性又提升纹理细节；4) 在DESOBAV2上训练/评估。

Result: 在DESOBAV2上，相比现有方法生成的阴影更逼真且与几何和光照更一致，特别在复杂几何或光照模糊场景中取得明显优势（定性与定量均优）。

Conclusion: 显式物理建模与深度生成式模型可互补：物理模块提供稳定、可解释的结构约束，扩散模型负责高保真外观重建，从而实现更真实且物理合理的阴影生成。

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [25] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 提出一种联合检测投射阴影与贴附阴影的闭环方法：用影子分割预测两类阴影、由预测反推光照方向、结合表面法线生成自遮挡先验，再反馈细化分割与光照；并构建1458张图像、分别标注两类阴影的数据集。实验显示贴附阴影检测显著提升（BER降低≥33%），同时保持整体与投射阴影性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注投射阴影，忽视对物体自身遮挡形成的贴附阴影，缺乏相应数据集与模型；而贴附阴影对三维结构理解至关重要。作者希望通过联合几何与光照推理，补齐这一空白并提升贴附阴影检测精度。

Method: 框架包含两模块：1) 阴影检测模块，分别预测投射与贴附阴影；2) 光照估计模块，从预测阴影推断光照方向。将估计的光方向与表面法线结合，生成几何一致的“部分自遮挡图”（可能自遮挡区域先验），再反馈到检测模块以细化预测，形成迭代的闭环几何-光照推理过程。与此同时，作者构建含1458张、区分两类阴影标注的数据集用于训练与评测。

Result: 在新数据集上的实验表明：迭代闭环推理显著改善贴附阴影检测，BER至少降低33%；同时保持整体阴影与投射阴影分割的强劲表现。

Conclusion: 通过显式建模光照与几何关系，并在检测-光照估计之间闭环迭代，可有效提升贴附阴影的检测而不牺牲其他阴影类型表现；新数据集为该任务提供训练与评测基准。

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [26] [SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling](https://arxiv.org/abs/2512.06185)
*Ankit Gupta,Christoph Adami,Emily Dolson*

Main category: cs.CV

TL;DR: 论文重探“愚弄图像”，发现当代CNN与ViT仍极易被高置信度欺骗；提出更简洁高效的黑箱攻击SPOOF，以极少像素改动生成不可识别却高置信的假象；对抗性再训练仅带来部分缓解。


<details>
  <summary>Details</summary>
Motivation: 尽管DNN在视觉任务上表现卓越，但对与自然图像毫不相干的输入仍过度自信。早期工作（Nguyen 2015）展示了可进化生成“愚弄图像”，但尚不清楚现代架构（尤其Transformer）是否更稳健，以及是否能以更低开销、更稳定的方法在黑箱设置下复现甚至强化这种愚弄。

Method: 1) 复现实验：在现代CNN与ViT-B/16上重实现CPPN与直接编码的进化式愚弄攻击，比较查询效率与置信度。2) 提出SPOOF：一种极简一致的黑箱攻击，通过少量像素改动与轻量计算生成高置信“不可识别”图像；评估其查询成本与成功率。3) 免疫性测试：将愚弄图像作为额外类别进行再训练，观察鲁棒性变化与SPOOF在更高查询预算下的持续有效性。

Result: - 现代网络依然可被高置信愚弄，且ViT-B/16最易受攻，需更少查询即可达近乎必然的错误高置信分类。- SPOOF在黑箱设定下以极低计算与最小像素更改生成高置信、不可识别的愚弄图像，效率优于复现的进化方法。- 将愚弄图像纳入再训练仅带来部分提升；在略增查询预算后，SPOOF仍能稳定愚弄。

Conclusion: 现代深度分类器在分布外与不可识别输入上仍存在系统性过度自信与脆弱性。Transformer（如ViT-B/16）并不更稳健，甚至更易受黑箱愚弄。SPOOF展示了以极低成本制造高置信错误的可行性，而简单的数据增广式再训练无法根治该问题。

Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.

</details>


### [27] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出一种多模态颜色轨迹预测方法，将高维时序颜色信息与干燥工艺参数融合，在未见工况下对饼干与苹果干燥的颜色演变实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究多用低维颜色特征，难以刻画食品干燥过程中复杂、动态的颜色轨迹；且模型对未见工况泛化差，限制了工业应用与过程优化。

Method: 构建多模态学习框架：融合高维时序颜色数据与干燥过程参数，进行数据高效的颜色轨迹建模与预测；在不同食品（饼干、苹果）与未见干燥条件下进行评估。

Result: 在未见干燥条件下，模型RMSE：饼干2.12、苹果1.29；相较基线模型误差降低超过90%，表现出更高的精度与稳健性。

Conclusion: 多模态颜色轨迹预测方法显著提升对食品干燥颜色演变的预测准确性与泛化能力，具备广泛适用性与工业价值。

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [28] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

TL;DR: FeTS 2024 探索在多机构脑胶质瘤 MRI 分割上的联邦学习聚合方法；基于PID控制的权重聚合在精度与通信效率上均获最佳，刷新历届挑战表现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗影像中受分布异质、通信成本和训练不稳定性影响，传统FedAvg等聚合在鲁棒性与收敛效率上存在不足；需要系统评测与改进聚合策略以提升多机构肿瘤分割的稳定与高效收敛。

Method: 以BraTS衍生的多机构数据（1251/219/570训练/验证/隐藏测试）和统一的FL基准框架，评测6支队伍的聚合方法；综合排名指标包括：分割性能（DSC、HD95）与通信效率（收敛分数）。重点方法为基于PID控制的权重聚合，用反馈控制稳定与加速全局模型更新。

Result: PID聚合方法获得总榜第一：ET/TC/WT的平均DSC分别为0.733/0.761/0.751，HD95为33.922/33.623/32.309 mm，并以0.764的收敛分数体现最高通信效率；整体结果超过以往挑战的最佳水平。

Conclusion: PID控制器可作为有效的联邦聚合机制，兼顾精度、稳健性与通信效率；本挑战提供标准化评测与代码基线，为医疗影像FL的稳健聚合研究与实际部署奠定基础。

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [29] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

TL;DR: 这是一项对“SVD+WDR”有损图像压缩方法的独立可复现性研究。复现与扩展实验显示：相较JPEG2000与单独WDR，该方法在PSNR上普遍不占优，SSIM仅部分更好；原论文存在实现细节缺失导致结果敏感与不稳定。


<details>
  <summary>Details</summary>
Motivation: 验证原论文声称的SVD与WDR结合可在视觉质量与压缩率上优于JPEG2000和WDR，并评估其在更广数据与更严指标下的稳健性与可复现性。

Method: 重实现SVD+WDR，补齐并审视原文中缺失或含糊的实现细节（如量化、阈值初始化），严格复现实验条件；并在新图像集上以PSNR与SSIM进行系统评测，与JPEG2000及WDR基线比较。

Result: 与原声称相反，SVD+WDR整体上PSNR不优于JPEG2000或WDR；相对JPEG2000仅在部分情况下SSIM略有提升。对实现细节的不同设定对性能影响显著。

Conclusion: 原方法的优势并未在独立复现中得到支持，其性能对未明确的实现细节高度敏感，提示需要更完整的算法描述与可复现实验流程；应谨慎解读原论文的性能优越性结论。

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [30] [GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking](https://arxiv.org/abs/2512.06230)
*Pranav Balakrishnan,Sidisha Barik,Sean M. O'Rourke,Benjamin M. Marlin*

Main category: cs.CV

TL;DR: 提出一种支持单目标多检测的GLMB变体，并展示其打破检测间依赖、便于并行与GPU加速，实验表明对目标数与假设数具有良好运行时可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统基于标记随机有限集（如GLMB）的多目标追踪虽有贝叶斯闭式优点，但在标准测量模型下需维护大量假设，计算代价高；在由机器学习虚拟传感器构成的分布式网络中，同一目标可能产生多次检测，现有模型难以高效处理与并行。

Method: 以GLMB为例，构造允许“同一目标-同一时刻-多检测”的滤波更新变体，从而打破标准GLMB更新中的检测间依赖；利用这一独立性实现并行化更新，并在GPU上加速；进行初步实验评估其在目标数量与保留假设上限方面的运行时扩展性。

Result: 理论上证明更新阶段的互依赖被消除，带来显著并行可扩展性；实现了GPU加速的原型，并给出初步结果表明随目标数与假设上限增长，运行时间扩展性明显优于标准GLMB（定量细节未在摘要中给出）。

Conclusion: 允许多检测的GLMB变体可在保持GLMB理论框架的同时，显著提升并行化与GPU部署效率；初步结果验证了其在大规模目标与多假设场景下的运行时可扩展性。

Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.

</details>


### [31] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

TL;DR: 研究将V-JEPA在儿童自摄SAYCam数据上预训练（仅为SOTA训练数据量的0.01%），评估直觉物理（IntPhys2）。结果：性能无显著提升，说明仅靠发展性真实分布或数据量/分布调整不足以让现有架构学到直觉物理。


<details>
  <summary>Details</summary>
Motivation: 人类通过对物理直觉形成的内部模型成功理解世界，但当前深度模型在直觉物理基准上仍落后。作者想探究：问题在于数据“分布”而非“规模”吗？儿童视角、发育真实的视觉经验是否更有助于学得直觉物理表示？

Method: 采用Video Joint Embedding Predictive Architecture (V-JEPA) 作为自监督视频表示学习框架，在SAYCam（3名儿童的日常第一人称视频）上进行预训练，再在IntPhys2基准上评估直觉物理相关能力，与使用海量互联网视频训练的SOTA进行比较。

Result: 尽管SAYCam数据与人类发育经验更接近，但仅占SOTA训练数据量的0.01%，在IntPhys2上的表现未显著改善；与大规模互联网数据预训练模型相比没有明显优势。

Conclusion: 仅使用发展性真实的儿童视角数据并不能使当前架构学到有效的直觉物理表示。单纯改变视觉数据的体量与分布不足以构建具有人工直觉物理的系统，可能需要新的模型归纳偏置、训练目标或与交互/行动结合的学习范式。

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [32] [NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks](https://arxiv.org/abs/2512.06251)
*Fangzhou Lin,Yuping Wang,Yuliang Guo,Zixun Huang,Xinyu Huang,Haichong Zhang,Kazunori Yamada,Zhengzhong Tu,Liu Ren,Ziming Zhang*

Main category: cs.CV

TL;DR: NexusFlow提出一种可插拔、轻量的部分监督多任务学习框架，通过可逆耦合层对齐不同任务的潜在特征分布，实现跨结构异质与同质任务的知识迁移，并在nuScenes与NYUv2上取得SOTA与一致增益。


<details>
  <summary>Details</summary>
Motivation: 现有PS-MTL方法多聚焦于同质、稠密预测任务，难以处理结构差异（如稠密/稀疏）与域划分带来的分布鸿沟，易出现表示塌陷与知识无法有效共享的问题。

Method: 引入一组代理网络（surrogate networks）包含可逆耦合层，将各任务的潜在特征双射映射到共享的规范空间，实现特征对齐与信息保持，避免表示塌陷，并支持跨任务结构差异的对齐；作为轻量、可插拔模块嵌入多任务体系。

Result: 在nuScenes的域划分自动驾驶场景（地图重建为稠密任务、多目标跟踪为稀疏任务且分地域监督）上超越强基线达到SOTA；在NYUv2上对分割、深度、法线三稠密任务均带来一致性能提升。

Conclusion: 可逆对齐的NexusFlow能够在部分监督条件下统一多样任务的表示空间，提升跨任务知识迁移与泛化，具有通用性与轻量易用的优势。

Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.

</details>


### [33] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: 提出LaFG：用LLM与VLM把类别名转为属性级监督，建立数据集范围的属性词表与类别原型以提升细粒度检索在未见类上的泛化。


<details>
  <summary>Details</summary>
Motivation: 传统FGIR以one-hot类别监督，忽略类别名的语义信息，难以建模跨类别可比的细节属性，导致对未见类别泛化不足。

Method: (1) 将类别名视为语义锚点，用LLM生成面向属性的详细描述；(2) 为缓解缺失/偏差，用冻结VLM把描述投到视觉对齐空间，聚类得到数据级属性词表，并从相关类别补充互补属性；(3) 用全局提示模板为每个类别选择相关属性并聚合为类别特定的语言原型；(4) 以这些语言原型监督检索模型学习可比较的细粒度表征。

Result: 在细粒度检索任务中，相比仅用one-hot标签的基线，LaFG在未见类别的检索效果显著提升，同时在已见类别保持或提升性能（具体数值需原文）。

Conclusion: 利用语言与视觉语言模型将类别名解构为属性级语义监督，可构建可迁移的类别原型，提升FGIR对跨类别细节的可比性与未见类泛化能力。

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [34] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: 论文指出LVLM常“答对但想错”，核心问题是推理路径选择偏差；提出两阶段PSO训练框架（GRPO+在线偏好优化+负样本回放）以修剪无效路径，平均提升7.4%并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM知识充足但推理不稳，Pass@K远高于Pass@1显示失败多源于路径选择与逻辑不一致，而非无知；需系统方法提升路径选择与稳定性。

Method: 两阶段后训练：1) 使用GRPO结合模板与答案奖励，促使结构化、逐步推理；2) 在线偏好优化：从GRPO数据采样多条推理路径，模型自评并对齐偏好路径；将错误/次优路径存入负回放记忆NRM，周期性重放以防重犯并持续改进。

Result: PSO能有效剪枝无效推理路径，显著提升推理准确性（平均+7.4%），并产出更稳定一致的思维链；多项实验验证泛化与稳定性提升。

Conclusion: LVLM推理不稳主要源于路径选择偏差；PSO通过强化结构化推理与偏好对齐、结合负样本回放，提升准确率与稳定性，是改进LVLM推理质量的有效后训练方案。

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [35] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

TL;DR: 提出一种在3D Gaussian Splatting中引入受约束多视角三角化的自监督几何一致性方法，显著减少“浮点”与几何散乱，提升重建质量并达SOTA（DTU上CD=0.50mm）。


<details>
  <summary>Details</summary>
Motivation: 仅依赖光度损失的3D高斯重建欠约束，易出现floater伪影与不结构化几何，难以得到高保真表面，需要引入跨视角的全局几何一致性约束。

Method: 在训练中对渲染的3D点施加几何一致性惩罚：从邻近多视角的估计进行稳健再三角化，得到共识3D点；惩罚渲染点偏离该共识点的距离。该过程为自监督，并通过受约束的多视角三角化实现全局几何一致性。

Result: 在多数据集上验证有效，达到SOTA。在DTU数据集上，平均Chamfer Distance为0.50 mm，优于其他显式方法。

Conclusion: 通过引入受约束的多视角三角化共识约束，3DGS重建获得更稳定一致的几何，显著减少floater并提升表面质量；代码将开源以利复现与验证。

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [36] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

TL;DR: 提出FacePhys，一种内存高效的rPPG算法，基于时空状态空间对偶与可迁移心脏状态表示，在保证实时低延迟与小内存的同时，实现跨数据集泛化和可扩展训练，达到新的SOTA并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 摄像头测生命体征可实现舒适且普适的健康监测，但前端设备算力受限、压缩传输降质会削弱rPPG精度；现有方法在模型规模、跨数据集泛化与实时性之间存在三难权衡，需要一种既高效又鲁棒的算法。

Method: 提出FacePhys：利用“时间-空间状态空间对偶”构建可迁移的心脏状态表示，捕捉跨帧细微周期性光强变化；算法在内存与计算上极简，支持长序列训练与低延迟推理，面向边端与受压缩视频场景。

Result: 在基准上取得新SOTA，相对误差降低49%；实时推理内存占用约3.6 MB、单帧延迟约9.46 ms，速度较现有方法快83%–99%；在压缩与实际部署条件下保持稳定性能，并提供在线演示。

Conclusion: FacePhys解决了rPPG在模型可扩展性、泛化能力与实时性上的三难题，适合前端设备与实际场景部署，显著提升精度与效率。

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [37] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

TL;DR: 提出RefBench-PRO基准，将指代表达理解分解为感知与推理两维六子任务，并配套自动数据生成；同时提出Ref-R1强化学习方案（动态IoU-GRPO）提升复杂推理下的定位精度，形成更强基线。实验显示该基准能更可解释地评估MLLM在REC中的能力并更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有REC基准多停留在感知层面，缺乏可解释的、多维细粒度评分，难以揭示MLLM在不同认知能力（感知与推理）的真实“落地对齐/指代落地”能力；同时缺少在复杂推理条件下鲁棒训练与强基线。

Method: 1) 基准：将REC拆为两大维度（感知/推理），细化为六类子任务（属性、位置、交互、常识、关系、拒识），并构建全自动数据生成流水线，生成多样化指代表达。2) 训练：提出Ref-R1，基于强化学习的学习方案，引入Dynamic IoU-based GRPO作为奖励/优化机制，使模型在更复杂推理场景下优化定位。

Result: RefBench-PRO实现对MLLM在REC任务上的可解释评测，覆盖从感知到推理的逐级难度；Ref-R1在复杂推理设定下带来更高定位准确率，作为更强基线。实验表明该基准更具挑战性并能区分不同能力维度表现。

Conclusion: RefBench-PRO弥补了REC评测在可解释性与认知维度覆盖上的缺口；Ref-R1提升复杂推理场景下的指代定位性能。二者共同推动对MLLM指代表达理解能力的细粒度评估与提升。

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [38] [Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://arxiv.org/abs/2512.06281)
*Hengzhuang Li,Xinsong Zhang,Qiming Peng,Bin Luo,Han Hu,Dengyang Jiang,Han-Jia Ye,Teng Zhang,Hai Jin*

Main category: cs.CV

TL;DR: LaVer 通过在LLM联合潜在语义空间做掩码图像建模，直接给予视觉监督，缓解MLLM深层“文本占优、视觉弱化”的模态失衡，显著提升需要密集视觉理解的任务表现。


<details>
  <summary>Details</summary>
Motivation: MLLM 在多模态任务中强，但训练以“下一词预测”为主，缺少直接视觉监督，致使随层加深视觉表征同质化、视觉注意力不足，带来视觉性能下降与幻觉。

Method: 提出 Latent Visual Reconstruction（LaVer）：在LLM的联合潜在语义空间进行掩码图像建模（MIM），对视觉特征进行重建学习；在训练中为模型提供直接的视觉激活/监督，促使更判别性的视觉表示与更高的视觉注意分配。

Result: 在多种基准与场景上广泛实验，尤其在需要密集视觉能力的任务中优于现有方法；观察到模型在推理中分配更多视觉注意。

Conclusion: 在LLM潜在空间进行视觉重建能有效缓解模态失衡，提升视觉利用与表现，减少幻觉；LaVer 为训练MLLM提供了一种通用且有效的视觉监督范式。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.

</details>


### [39] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

TL;DR: 提出一套基于事件的非接触睡眠干扰监测系统，融合深度、RGB与麦克风数据，分别建立深度与彩色背景模型以量化运动与光照变化，并结合噪声检测进行三类事件识别，实验验证了可行性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 家庭环境下客观量化评估睡眠干扰仍缺乏低侵入、可长期部署的手段；单一传感器难以同时覆盖运动、光照与噪声等多源干扰，因此需要一种多模态、事件驱动的方法。

Method: 在弱光环境中使用含红外深度、RGB摄像头与四麦克风阵列的设备；分别在深度数据上建立背景模型以度量身体运动幅度，在RGB图像上建立背景模型以度量光照变化；结合麦克风处理得到噪声特征；基于三路处理结果的阈值/事件检测算法识别运动、开关灯与噪声三类事件。

Result: 系统在睡眠场景中完成实验验证，成功检测并分类上述事件，结果显示具有一定可靠性与有效性（摘要未给出具体数值指标）。

Conclusion: 多模态、事件驱动的非接触睡眠监测可在家庭环境中可靠识别主要扰动类型；深度与RGB各自建模互补弥补单模态局限，为客观评估睡眠干扰提供可行方案。

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [40] [StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification](https://arxiv.org/abs/2512.06290)
*Yiheng Huang,Shuang She,Zewei Wei,Jianmin Lin,Ming Yang,Wenyin Liu*

Main category: cs.CV

TL;DR: 提出StrokeNet，通过引用点对（点+特征）表示笔画，结合序列注意与跨椭圆查询，多尺度建模局部交互，在多数据集达SOTA，CASIA-onDo准确率95.54%。


<details>
  <summary>Details</summary>
Motivation: 笔画分类难在笔画间语义关系建模：书写风格差异、内容歧义、位置动态导致现有深度模型难以捕获局部细粒度交互。点级视角虽细致但冗余。需要一种既细粒度又高效的表示。

Method: 将笔画编码为“参考点对”（参考点+特征向量）。为每个笔画动态选取并排序参考点；用Inline Sequence Attention(ISA)沿参考点序列构建上下文；提出Cross-Ellipse Query(CEQ)对参考点进行椭圆聚类并跨多空间尺度提取交互特征；联合优化：通过参考点回归预测类别，并用Aux-Branch建模相邻笔画语义转移。

Result: 在多份在线手写公开数据集上达到SOTA；在CASIA-onDo上准确率从93.81%提升到95.54%。

Conclusion: 以参考点对为核心的StrokeNet能有效捕获局部细粒度交互，减少点级冗余，并通过ISA与CEQ实现多尺度空间/序列建模，实验验证方法有效且鲁棒。

Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.

</details>


### [41] [Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation](https://arxiv.org/abs/2512.06306)
*Haoxian Zhou,Chuanzhi Xu,Langyi Chen,Haodong Chen,Yuk Ying Chung,Qiang Qu,Xaoming Chen,Weidong Cai*

Main category: cs.CV

TL;DR: 基于事件相机的人体姿态估计，改用点云式表征与时序卷积/排序模块，避免密集帧化，提升DHP19上多种骨干的精度。


<details>
  <summary>Details</summary>
Motivation: 事件相机具备高时间分辨率与低延迟，适合在光照、快速运动等复杂场景下鲁棒估计。但现有方法多把事件流转为稠密事件帧，带来额外计算并丢失时间分辨率，限制性能。

Method: 以点云为框架直接处理事件流：1) 事件时间切片卷积（Event Temporal Slicing Convolution，ETSC）捕获相邻切片间短期依赖；2) 事件切片序列化模块（Event Slice Sequencing，ESS）进行结构化时序建模；3) 在点云式事件表征中加入边缘增强，在稀疏事件条件下强化空间边缘信息。方法可无缝接入多种点云骨干（PointNet、DGCNN、Point Transformer）。

Result: 在DHP19数据集上，所提方法在三种代表性点云骨干上均带来一致性能提升（相对其原生基线）。

Conclusion: 直接以点云方式利用事件相机的时空稀疏性，并结合短期时序卷积、序列化建模与边缘增强，可在不牺牲时间分辨率的前提下提升人体姿态估计效果，且具有良好的骨干通用性。

Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.

</details>


### [42] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: ReCAD 用强化学习驱动预训练大模型，从文本/图像等多模态输入生成可编辑、参数化的 CAD 脚本与几何体，显著提升几何精度并具备复杂操作涌现能力。


<details>
  <summary>Details</summary>
Motivation: 现有 text/image-to-CAD 方法多依赖监督微调，难以充分利用大模型的生成先验，编辑性弱，且对复杂 CAD 操作（如阵列、镜像）支持不足，导致泛化与几何精度不佳。

Method: 1) 将 CAD 脚本重写为参数化代码，并据此自动生成精确文本描述，先对 VLM 进行基本 CAD 能力微调；2) 设计利用参数化代码作为引导信号的新型 RL 策略，提升复杂问题的推理；3) 分层的基本体（primitive）学习，在统一奖励下逐步习得结构化与组合式技能；奖励同时约束几何精度与语义一致性；4) 仅依赖简单功能接口（如点坐标）即可涌现复杂 CAD 操作。

Result: 在 text-to-CAD 与 image-to-CAD 上均达 SOTA。以图到 CAD 为例：Chamfer Distance 从 73.47 降至 29.61（ID），从 272.06 降至 80.23（OOD），显著优于现有基线；同时展现出阵列、镜像等复杂操作的涌现与可编辑性提升。

Conclusion: 将参数化代码与分层 RL 融入 VLM，可在最少接口假设下激发复杂 CAD 操作与推理能力，显著提升几何与语义匹配及泛化；为可编辑、可组合的多模态 CAD 生成提供了通用范式。

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [43] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

TL;DR: 提出S2WMamba：用2D/1D Haar小波解耦空间与光谱频率，并通过Mamba跨模态调制与动态门控融合，实现高效全色锐化，在WV3/GF2/QB上优于/匹配强基线。


<details>
  <summary>Details</summary>
Motivation: 全色锐化中同时处理PAN与MS易导致空间细节与光谱保真纠缠，现有方法要么细节增强引入光谱失真，要么保持光谱牺牲空间纹理，且复杂度较高。需要一种既解耦频率信息又能高效跨模态交互的方法。

Method: 1) 对PAN施加2D Haar DWT以定位边缘/纹理；对MS按通道对每个像素的光谱向量做1D Haar DWT，分离低/高频以抑制光谱失真。2) 构建并行双分支：Spectral分支将小波提取的空间细节注入MS特征；Spatial分支利用1D谱金字塔细化PAN特征。3) 通过Mamba式跨调制实现长程依赖、线性复杂度的跨模态交互。4) 采用“乘性+加性”的多尺度动态门自适应融合分支输出。

Result: 在WV3、GF2、QB数据集上，较FusionMamba、CANNet、U2Net、ARConv等达到或更优：PSNR最高提升至+0.23 dB；WV3全分辨率HQNR达0.956。消融验证2D/1D DWT位置、并行双分支与融合门的有效性。

Conclusion: 频率显式解耦结合线性复杂度的Mamba跨模态调制与动态门控融合，可在保持光谱保真的同时增强空间细节，实现参数与计算友好的全色锐化；方法稳健并具可复现性（代码开源）。

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [44] [CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks](https://arxiv.org/abs/2512.06332)
*Jeffrey Gu,Minkyu Jeon,Ambri Ma,Serena Yeung-Levy,Ellen D. Zhong*

Main category: cs.CV

TL;DR: 引入CryoHype：一种基于Transformer的超网络，动态调节隐式神经表示的权重，以从混合样品的冷冻电镜图像中并行重建多种分子结构；在含100种结构的基准上达SOTA，并在固定姿态设定下扩展到1000种结构。


<details>
  <summary>Details</summary>
Motivation: 现有cryo-EM方法多聚焦于单一或少数分子的构象异质性建模，难以处理由多种不同分子同时存在引起的组成异质性；为了实现真正的高通量、无标签的多目标同时重建，需要能区分并重建大量不同分子物种的模型。

Method: 提出CryoHype：利用Transformer驱动的超网络，根据输入数据动态生成/调整隐式神经表示（INR）的参数或权重；在重建过程中，通过超网络对不同分子实例产生相应的INR，适配多结构同时重建；实验设定包含固定姿态（fixed-pose）以聚焦组成异质性。

Result: 在包含100种结构的挑战性基准上取得SOTA性能；并在固定姿态设置下从未标注的cryo-EM图像中扩展至1000种不同结构的重建，显示出良好的可扩展性和准确性。

Conclusion: CryoHype能有效解决多物种组成异质性下的同时重建问题，显著提升多目标高通量cryo-EM结构解析能力，并具备良好的扩展性。

Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

</details>


### [45] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: 提出MTGC多模态引导、任务感知的生成式超低码率图像压缩，通过文本、极压缩图像和任务语义伪词三种引导，配合扩散解码器的双路径协同指导，显著降低语义偏差并提升感知与像素保真。


<details>
  <summary>Details</summary>
Motivation: 生成式压缩在极低码率下虽有高感知质量，但易出现生成幻觉导致语义偏差，难以在6G语义通信等可靠场景部署；需增强语义一致性并兼顾感知质量与保真度。

Method: 1) 框架MTGC：引入三种引导模态——简洁稳健的文本描述（全局语义）、高度压缩图像HCI（低层视觉细节）、语义伪词SPWs（与下游任务相关的细粒度语义）。2) TASCM模块：任务感知的语义压缩，驱动多头自注意力聚焦任务相关语义、过滤冗余以生成SPWs。3) MGDD扩散解码器：双路径协同指导，将三种引导通过跨注意力与ControlNet式加性残差注入扩散过程，借助扩散先验重建图像。

Result: 在DIV2K等数据集上显著提升语义一致性（如DISTS下降10.59%），同时在超低码率（bpp<0.05）下获得更优的感知质量与像素级保真。

Conclusion: 多模态与任务感知的联合指导能有效抑制超低码率生成式压缩的语义幻觉，实现更可靠的语义一致性与视觉质量，适用于带宽受限的6G语义通信等场景。

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [46] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

TL;DR: 提出CLUENet：结合软聚合+硬分配、温度缩放余弦注意力与门控残差、跨块硬且共享的特征派发、改进的聚类池化，以提升视觉语义理解的准确性、效率与可解释性，在CIFAR-100与Mini-ImageNet上优于现有聚类与主流视觉模型。


<details>
  <summary>Details</summary>
Motivation: 现有卷积与注意力模型感受野刚性、结构复杂，难以建模不规则空间模式并影响可解释性；而聚类范式虽具可解释与灵活语义建模优势，但存在精度低、效率差与训练梯度消失等问题。

Method: 提出CLUENet，核心包括：1) 全局软聚合与硬分配相结合，配合温度缩放余弦注意力与门控残差，强化局部建模与稳定训练；2) 互块（inter-block）硬且共享的特征派发机制，促进跨层/跨块信息的一致与高效路由；3) 改进的聚类池化策略，以更稳健地汇聚语义簇特征并提升判别力。

Result: 在CIFAR-100与Mini-ImageNet上相较现有聚类方法与主流视觉模型取得更高分类准确率，同时具备更高效率与更强可视化可解释性。

Conclusion: CLUENet通过融合可解释的聚类机制与注意力式相似度建模，解决聚类范式的训练与性能瓶颈，在保证透明性的同时实现对主流模型的精度与效率超越，适用于需要高可解释性的视觉语义理解任务。

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [47] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出TreeQ用于DiT量化：以TSS高效搜索、ENG统一PTQ/QAT、GMB缓解超低比特信息瓶颈；在DiT-XL/2上实现W3A3与W4A4下的SOTA，首次达成近无损4-bit PTQ。


<details>
  <summary>Details</summary>
Motivation: DiT在生成任务上优于U-Net但计算与显存开销巨大，现有混合精度量化在U-Net上已到亚4比特，但对DiT的探索不足，存在搜索低效、代理目标不匹配、超低比特信息丢失等问题。

Method: 提出TreeQ三组件：1) Tree Structured Search (TSS)：利用DiT层线性结构，O(n)遍历并以比较式剪枝提升目标对齐，解决混合精度分配搜索与代理失配；2) Environmental Noise Guidance (ENG)：引入单一超参的环境噪声指导，将PTQ与QAT优化目标对齐，统一两种流程配置；3) General Monarch Branch (GMB)：结构化稀疏支路，缓解超低比特信息瓶颈，保存细节。

Result: 在DiT-XL/2上，W3A3与W4A4的PTQ/PEFT均达SOTA；首次在DiT实现接近无损的4-bit PTQ性能，显著降低计算与内存开销。

Conclusion: TreeQ为DiT量化提供统一高效框架：以TSS高效且准确地分配比特，ENG对齐PTQ与QAT目标，GMB在超低比特下防止信息不可逆丢失；在标准设定上取得SOTA并证明4-bit PTQ可近无损，具备实用部署价值。

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [48] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 提出一种基于编辑型潜变量扩散模型的单张图像反射去除方法，通过在潜空间建模“线性叠加”的物理结构、任务专属文本引导和深度引导的早分支采样，实现更强的感知与分离能力，达到多基准SOTA并在真实场景泛化良好。


<details>
  <summary>Details</summary>
Motivation: 单幅反射去除高度病态，现有方法在含有多层合成区域时难以正确理解与分解，导致恢复失败与泛化差。核心原因被忽视：语义编码器的潜空间缺乏把复合图像解释为其分层线性叠加的内在结构。

Method: 将编辑型潜扩散模型重构为能处理层状合成输入的框架，包含三部分：1) 反射等变VAE，使潜空间与反射形成的线性物理过程对齐；2) 可学习的任务特定文本嵌入，避免含糊语言并提供精确引导；3) 结合深度信息的早分支采样策略，利用生成随机性提升结果。

Result: 在多项基准上取得新的SOTA表现，并在复杂真实场景中展现良好泛化能力。

Conclusion: 通过在潜空间嵌入反射的线性物理结构并配合任务化文本引导与深度引导采样，可显著提升单幅反射去除的恢复质量与泛化，验证了扩散模型在高度模糊、分层合成任务中的有效性。

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [49] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

TL;DR: 提出SPL-UAD，通过在提示空间解耦物理与数字攻击分支、引入自适应伪造上下文提示和线索感知增强，实现统一攻击检测并在大规模数据上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实人脸识别易受物理展示攻击与数字伪造攻击双重威胁。现有基于CLIP的统一检测利用同类提示正则化来提升泛化，但物理与数字的优化方向在共享提示空间中相互冲突，限制了鲁棒性与泛化。需要一种能同时兼顾两类攻击、缓解冲突并提升对未知攻击适应性的统一框架。

Method: 提出SPL-UAD：1) 在提示空间中解耦物理与数字攻击的优化，构建可学习的并行提示分支；2) 设计自适应“伪造上下文提示生成”，为每类攻击独立建模上下文以实现独立可控优化；3) 基于双提示机制的“线索感知增强”，通过困难样本挖掘式的数据增强，强化对未见攻击类型的鲁棒性；4) 统一在大规模UniAttackDataPlus上训练与评估。

Result: 在大规模UniAttackDataPlus数据集上进行广泛实验，SPL-UAD在统一攻击检测任务上取得显著优于现有方法的性能提升，表现出更好的跨物理与数字场景的泛化与鲁棒性。

Conclusion: 解耦的提示学习与自适应伪造上下文结合线索感知增强，有效缓解物理与数字检测目标冲突，提升统一攻击检测的泛化与对未知攻击的鲁棒性，适合现实生物特征防护的统一防御框架。

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [50] [Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos](https://arxiv.org/abs/2512.06368)
*Weitao Xiong,Zhiyuan Yuan,Jiahao Lu,Chengfeng Zhao,Peng Li,Yuan Liu*

Main category: cs.CV

TL;DR: 提出Human3R：在单目动态视频重建中，融合SMPL人体先验与单目深度先验，通过全分辨率场景重建+人像局部精修的层级管线，缓解几何不一致与分辨率退化，显著提升动态人体区域的几何一致性与边界细节；在TUM Dynamics与GTA-IM上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目动态场景重建缺乏3D人体结构理解，导致四肢比例失真、人-物体融合不自然；同时由于内存限制的下采样，人体边界向背景漂移、细节丢失。需要引入结构化人体先验并保持高分辨率细节，以兼顾整体几何一致性与局部精细度。

Method: 引入“混合几何先验”：将SMPL人体模型与单目深度估计结合。提出Human3R层级式流水线：先对全分辨率图像重建整体场景几何；再通过策略性裁剪定位人体区域，并以跨注意力融合提升人体细节。设计特征融合模块将SMPL先验注入特征空间，稳定人体几何、校正边界；同时保留单目深度带来的细节刻画。

Result: 在TUM Dynamics与GTA-IM数据集上进行广泛实验，动态人体重建的几何一致性、边界质量与整体视觉效果优于现有方法（摘要未给出具体数值）。

Conclusion: 混合几何先验（SMPL+单目深度）与层级式全局-局部精修管线能有效缓解单目动态重建中的几何不一致与分辨率退化问题，Human3R在多数据集上表现领先，证明了该策略的有效性。

Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.

</details>


### [51] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

TL;DR: 提出VG-Refiner框架，通过两阶段“思-复思”机制与精炼奖励，提升工具集成视觉推理在指代与落地任务中对不可靠工具输出的纠错与稳健性，显著提高准确率与纠错能力并保持通用性。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成视觉推理多依赖RL串联多种视觉工具，但缺乏对工具输出不可靠/错误时的有效响应策略，尤其在指代与定位类任务中，错误检测会诱发模型幻觉式推理，急需能够识别并纠正工具噪声的机制与评测。

Method: 提出VG-Refiner：1) 两阶段think-rethink流程，显式解析工具反馈、在发现不可靠时重新推理与改写；2) 设计“精炼奖励”，鼓励模型在工具结果较差时做出有效纠错；3) 提出两项新指标并给出公平评测协议；4) 以少量任务特定数据做精调以增强精炼能力。

Result: 在指代与推理落地基准上，VG-Refiner在准确率与纠错能力上显著优于现有方法，同时保持预训练模型的通用多模态能力。

Conclusion: 通过显式的反思与奖励设计，TiVR可在面对不可靠工具输出时实现鲁棒自我修正；VG-Refiner证明了精炼机制与评测框架的有效性，并为后续稳健的工具增强视觉推理提供了范式。

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [52] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出用于评估与筛选文本生成驾驶视频（AIGVs）对自动驾驶感知训练/评测有效性的诊断框架与基准，发现直接使用原始AIGVs会伤害性能，但用所提评估器ADGVE过滤后可显著提升视频质量指标并改善下游感知模型表现。


<details>
  <summary>Details</summary>
Motivation: 文本到视频模型能廉价生成高分辨率驾驶场景，有望缓解真实/仿真数据昂贵与稀缺的问题；然而其物理与交通语义可靠性存疑，亟需系统性方法判定AIGVs能否安全用于自动驾驶感知训练与评测。

Method: 1) 总结AIGV常见失效模态的系统分类（视觉伪影、物理不合理运动、交通语义违背等），并量化其对检测、跟踪、实例分割的影响；2) 构建驾驶场景基准ADGV-Bench，含多任务稠密标注与人类质量标注；3) 设计驾驶感知感知器ADGVE：融合静态语义、时序一致性、车道遵从信号与VLM引导推理，输出逐视频质量分数；4) 用ADGVE对AIGVs进行质量筛选并评估对通用视频质量指标与下游感知的影响。

Result: 未经筛选地加入AIGVs会降低感知性能；采用ADGVE过滤后，通用VQA指标与检测/跟踪/实例分割等下游任务均提升，AIGVs转而成为对真实数据的有益补充。

Conclusion: AIGVs既有风险也有潜力；通过提出的基准与驾驶感知评估器，可安全、有效地利用大规模生成视频以增强未来自动驾驶感知流水线。

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [53] [VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System](https://arxiv.org/abs/2512.06377)
*Yi Huo,Yun Ge*

Main category: cs.CV

TL;DR: 论文为FER2013构建了VAD（三维情感：效价-唤醒-支配）标注并提出正交卷积的回归网络（基于ResNet），提升VAD预测，尤其强调D维可测但最难获取与预测。


<details>
  <summary>Details</summary>
Motivation: 现有FER数据集多为七类离散情绪标签，表达力有限；AffectNet虽含VA但缺少D。未来情感计算需要连续、多维、更精确的度量，因此需要补齐D维并改进模型以更好学习多维情感表示。

Method: 1) 为FER2013新增人工VAD标注，首次引入D维；2) 提出在网络中使用正交化卷积（对卷积核进行正交约束/正则），以提高特征多样性与表达性；3) 基于ResNet的VAD回归框架；4) 消融实验验证正交卷积对VAD预测的作用。

Result: 实验表明：D维可被测量但较VA更难，无论是人工标注一致性还是回归预测误差均较差；引入正交卷积能提升VAD预测性能（优于无正交约束的基线）。

Conclusion: 本文提供了含VAD（含D维）的FER2013新标注，可作为VAD基准；提出的基于ResNet的正交化回归网络可作为VAD预测基线；代码与数据已开源（VAD-Net）。

Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .

</details>


### [54] [OCFER-Net: Recognizing Facial Expression in Online Learning System](https://arxiv.org/abs/2512.06379)
*Yi Huo,Lei Zhang*

Main category: cs.CV

TL;DR: 提出OCFER-Net：通过对卷积核施加正交性正则，提升FER特征多样性与表达力，在FER-2013上优于基线约1.087个百分点。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 在线学习场景下情感交互重要，教师需要了解学生情绪；现有FER方法较少利用卷积核正交性，可能限制特征多样性与泛化。

Method: 在CNN中对卷积核加入正交性正则项，鼓励卷积矩阵近似正交，从而减少特征冗余、增强判别性；构建OCFER-Net并在FER-2013数据集上评测。

Result: 在具有挑战性的FER-2013数据集上，相比多种基线方法准确率提升约1.087（应为百分点），表现更优。

Conclusion: 卷积核正交约束能提升FER模型的特征表达和识别性能；OCFER-Net验证了该思路的有效性并开源实现。

Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.

</details>


### [55] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种针对极端环境下IR与VIS融合的区域感知融合框架，利用空间可变曝光相机同时融合多曝光与多模态数据，兼顾几何保真与热辐射，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有红外-可见光融合方法在极端光照/温差条件下易牺牲可见图像的几何与细节，影响测量精度；单一曝光方案难以兼顾动态范围与多模态对齐与信息保留。

Method: 使用空间可变曝光（SVE）相机获取多曝光与多模态数据；先进行基于区域感知的特征融合以实现精确跨模态配准；随后进行带对比度增强的自适应融合；引入由区域显著图引导的结构相似度补偿机制以优化IR-VIS融合；并提供单一曝光情形下的自适应策略。

Result: 在合成与真实数据上，相较SOTA方法获得更清晰图像与更优定量与可视化评估指标，验证几何保真与热信息保持方面的优势。

Conclusion: 区域感知+SVE驱动的联合多曝光多模态融合框架能在极端环境下实现鲁棒IR-VIS融合，提升清晰度与测量可靠性，并具备对单曝光场景的适应性。

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [56] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

TL;DR: 提出Self-Autoregressive Refinement（SAR），通过轻量级自回归展开与对比学生强制损失，缓解尺度级AR图像生成的曝光偏差，显著提升生成质量且训练开销小。


<details>
  <summary>Details</summary>
Motivation: 尺度级自回归（coarse-to-fine）图像生成近年流行但受曝光偏差影响：一是训练-测试不匹配，推理时依赖自身不完美预测；二是不同尺度学习难度不均，部分尺度优化更困难，导致误差放大与质量下降。

Method: 在已有AR模型上引入SAR，包括：1）Stagger-Scale Rollout（SSR）：在训练中进行轻量级、跨尺度分段展开，让模型暴露于自身中间预测，缩小训练-测试分布差距；2）Contrastive Student-Forcing Loss（CSFL）：对模型在自生成上下文中的预测给予对比式监督，稳定优化并平衡不同尺度的学习难度。整体作为高效的后训练方法，无需大幅额外算力。

Result: 在多个预训练AR模型上均有增益，如在ImageNet 256上的FlexVAR-d16，10个epoch（32×A100，5小时）带来FID降低5.2%。计算开销和训练时长增量较小，同时提升生成质量与稳定性。

Conclusion: SAR高效、可扩展且效果稳定，可作为视觉自回归生成的可靠后训练策略，通过对齐训练-测试模式并缓解尺度难度不均来减少曝光偏差、提升图像质量。

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [57] [A Perception CNN for Facial Expression Recognition](https://arxiv.org/abs/2512.06422)
*Chunwei Tian,Jingyuan Xie,Lingjun Li,Wangmeng Zuo,Yanning Zhang,David Zhang*

Main category: cs.CV

TL;DR: 提出PCNN用于表情识别：并行局部子网抓取眼睛/脸颊/嘴部细节，借助多域交互融合局部与全局特征，并引入两阶段损失监督局部感知与重建，全数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN能学到表情特征，但容易忽略基于面部分割的局部细粒度变化，导致微表情与受遮挡/姿态变化下的表达不足，需要显式建模局部器官与整体结构的关联。

Method: 1) 感知CNN（PCNN）：设置5个并行子网络，分别针对眼睛、脸颊、嘴部等局部区域提取细粒度特征；2) 多域交互机制：对齐并融合局部器官特征与全局面部结构特征，实现注册与特征层面的融合；3) 两阶段损失：同时约束局部“感知信息”的准确性及重建的人脸图像质量，以提升识别鲁棒性与判别力。

Result: 在CK+、JAFFE、FER2013、FERPlus、RAF-DB以及遮挡与姿态变化数据集上取得优于现有方法的性能（SOTA或接近SOTA）。代码已开源。

Conclusion: 显式的局部器官感知与全局结构融合、配合两阶段损失，可显著提升FER性能与鲁棒性；PCNN在多基准上验证有效，具有实际应用潜力。

Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.

</details>


### [58] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: DragMesh提出一种解耦“运动学推理+运动生成”的实时三维关节化交互框架：先推断关节类型与几何参数，再用基于双四元数的VAE在FiLM条件下生成满足运动学约束的完整轨迹，实现既快又物理合理的交互式拖拽关节运动。


<details>
  <summary>Details</summary>
Motivation: 现有三维生成方法擅长静态内容，但难以理解与生成对象的可动结构及交互响应；现有可动方法要么物理一致但速度慢、难实时，要么生成式但违背基本运动学约束。需要一种既能实时交互又能保持严格运动学一致性的生成方法。

Method: 1) 解耦的运动学推理：将“语义意图→关节类型”与“几何回归→关节轴与原点”分开，通过KPP-Net推断潜在关节参数。2) 表示与生成：采用双四元数表示刚体运动，构建Dual Quaternion VAE（DQ-VAE）。输入包括用户拖拽与预测到的关节先验，生成完整可行的运动轨迹。3) 约束注入：在非自回归Transformer解码器的每层通过FiLM注入关节先验，实现持续多尺度指导；并加入数值稳定的叉积损失保证轴对齐与运动学一致性。

Result: 在无需对新对象再训练的条件下，实现实时交互的关节化运动生成，轨迹既符合运动学约束又具有生成多样性与合理性；较现有方法在速度与物理一致性之间取得更优权衡。

Conclusion: DragMesh通过“运动学推理与生成解耦+双四元数生成建模+全程条件化约束”，实现可泛化、实时、物理一致的三维关节运动生成，为迈向具备交互理解的生成式3D智能提供了实用进展。

Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [59] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 提出一个结合CLIP的双路径Transformer，用视觉与属性文本提示共同识别远距离场景中的性别，并在新构建的长距离数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 远距离（极端长距离）人像常因分辨率低、视角变化和面部信息缺失，传统基于面部或单一视觉线索的性别识别性能受限，需要一种能利用多模态与可解释属性线索、且对视距/视角/高度变化鲁棒的方法。

Method: 构建语言引导的双路径框架：1) 视觉路径对预训练CLIP图像编码器的高层进行选择性微调；2) 属性路径基于软生物属性（发型、服饰、配件等）生成文本提示，在CLIP图文共同空间推理性别；并加入空间-通道注意力模块以在遮挡和低分辨率下提升判别定位。提出统一数据集U-DetAGReID（由DetReIDx与AG-ReID.v2整合），采用三分类标签（男/女/未知）。

Result: 在宏F1、准确率、AUC等指标上全面超越属性识别与行人重识别SOTA，对距离、角度和高度变化保持稳定鲁棒；可视化显示注意力聚焦于可解释属性，并在不确定时倾向输出“未知”。

Conclusion: 语言引导的双路径学习为非受限长距离场景中的负责性别识别提供了可扩展、可解释且更鲁棒的基础。

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [60] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

TL;DR: 提出一种使用深度学习从2D合成人体图像中自动估计5项关键人体测量指标的方法，最佳模型ResNet50在10万张数据上训练，整体平均MAE约0.668厘米，显示可用于规模化辅助运动员赛前心血管筛查；未来将在真实图像上验证。


<details>
  <summary>Details</summary>
Motivation: PPCE需要识别如马凡综合征等与SCD相关的体态特征，但人工测量耗时、依赖操作者且难以大规模实施，因此需要自动化、可扩展且精确的人体测量方法来支持运动员筛查。

Method: 构建10万张由3D人体网格生成的2D合成图像数据集；选择VGG19、ResNet50、DenseNet121作为骨干，添加全连接回归头以预测五项人体测量（如腰围、四肢长度、躯干比例等）；以回归方式训练与评估，比较模型在绝对误差上的表现。

Result: 三种网络均达到了亚厘米级精度；ResNet50表现最佳，五项指标的平均MAE为0.668厘米。

Conclusion: 深度学习可从2D图像中准确、可扩展地估计关键人体测量值，为运动员赛前筛查提供实用工具；需在真实世界图像上进一步验证以提升外部可用性与泛化。

Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [61] [AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars](https://arxiv.org/abs/2512.06438)
*Ramazan Fazylov,Sergey Zagoruyko,Aleksandr Parkin,Stamatis Lefkimmiatis,Ivan Laptev*

Main category: cs.CV

TL;DR: AGORA将3D高斯泼洒与GAN结合，加入轻量FLAME条件的形变分支与双判别器，实时生成可动画、可控且高保真的人头头像，并在GPU/CPU上均实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF类方法渲染慢且动态不一致；3DGS方案多局限静态或缺乏可控动画能力。需求是获得高保真、实时、可精细表情控制并可在有限算力（含CPU）上运行的数字人头像。

Method: 提出AGORA：在3DGS框架中嵌入GAN训练。核心是FLAME驱动的轻量形变分支，预测每个高斯的残差以实现身份保持的表情形变；采用基于合成网格渲染的双判别器约束表达真实度；以条件生成方式实现可控表情与实时推理。

Result: 在表情准确度上优于SOTA NeRF方法；渲染速度在单GPU上达250+ FPS，CPU仅推理约9 FPS；生成头像既真实又可精确控制表情。

Conclusion: AGORA显著推进可动画数字人的实用化：在保证高保真与可控性的同时实现极高帧率，并首次展示了实用的CPU-only可动画3DGS头像合成能力。

Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/

</details>


### [62] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 提出SCD-MLLM，一个统一且对缺失模态稳健的多模态大模型框架，实现跨数据源抑郁识别，并在5个数据集、完整与缺失模态条件下优于SOTA与商用LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有音频/视频自动抑郁检测缺乏统一、可泛化的框架，且对真实场景常见的模态缺失不稳定；需要一种能整合异构数据、跨域泛化、并对缺失模态稳健的方法。

Method: 提出SCD-MLLM，包含两大组件：(i) 多源数据输入适配器MDIA：用掩码机制与任务提示将异构抑郁相关输入统一为token序列，缓解跨源不一致；(ii) 模态感知自适应融合模块MAFM：通过共享投影对音频与视觉特征进行自适应融合，提高缺失模态时的鲁棒性。框架支持多数据集联合训练。

Result: 在CMDC、AVEC2014、DAIC-WOZ、DVlog、EATD五个异构数据集上进行多数据集联合训练实验，在完整与部分模态设置下均优于SOTA与商用LLMs（Gemini、GPT），表现出更强的跨域泛化、多模态抑郁线索捕获能力与缺失模态稳定性。

Conclusion: SCD-MLLM为跨域抑郁识别提供统一且稳健的多模态LLM框架，能有效整合异构数据并在模态缺失条件下保持性能，适用于真实应用场景。

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [63] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

TL;DR: Sanvaad是一个轻量级、多模态、实时双向无障碍沟通框架，面向聋人与视障用户：用MediaPipe手势地标实现ISL手语识别，语音转手语GIF/字母可视化；为视障者提供免屏幕的多语种语音界面，集成语音识别、文本摘要与TTS，并通过Streamlit在桌面与移动端运行。


<details>
  <summary>Details</summary>
Motivation: 现有工具多只支持单向交互，限制了聋人、视障者与健听人群之间的双向沟通，亟需一个在边缘设备上可运行、低算力、实用的双向无障碍交流解决方案。

Method: - 采用MediaPipe手部/姿态地标实现轻量级ISL手语识别，适配边缘设备。
- 语音到手语：将手机语音识别结果映射到预定义短语，输出对应GIF或字母序列可视化。
- 面向视障者：提供免屏幕语音界面，集成多语种ASR、文本摘要、TTS。
- 利用Streamlit实现统一前端，兼容桌面与移动端。

Result: 构建了一个可在通用设备上运行的原型系统，支持聋人与视障用户的实时双向交流：手语识别与语音转手语可用，语音交互链路（ASR-摘要-TTS）贯通，整体在低算力环境下流畅运行。

Conclusion: 将轻量级计算机视觉与语音处理模块统一在同一框架内，可在不依赖专用硬件的前提下实现包容性、双向实时交流；Sanvaad为无障碍沟通提供了实用路径，具备跨平台可用性与扩展潜力。

Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [64] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

TL;DR: 提出一套端到端、智能化的多模态光伏电站自动巡检框架，解决热谱调色偏差、告警冗余与高带宽等痛点，在PVF-10上mAP@0.5达0.903，并在实地验证中显著降低重复告警与通信量。


<details>
  <summary>Details</summary>
Motivation: 传统光伏巡检依赖单一模态与人工流程，存在热成像调色板依赖导致识别不稳、重复告警造成误报与维护成本上升、以及原始数据回传导致通信带宽高等问题，亟需一个从采集到告警全流程自动化、鲁棒且高效的系统。

Method: 构建多模态协同架构：1) 通过一致性约束学习调色板不变的热像嵌入；2) 与经对比度归一化的RGB流通过门控机制进行特征融合；3) 闭环自适应重采集控制器（基于Rodrigues更新）针对不明确异常进行定向复核；4) 基于地理空间去重模块，使用哈弗辛距离上的DBSCAN聚类去除冗余告警；最终输出带地理位置的维护告警。

Result: 在PVF-10公开基准上获得mAP@0.5=0.903，较单模态基线提升12–15%；实地测试召回率96%；地理去重将因重复导致的误报降低15–20%；“只传相关”遥测策略将空中数据传输减少60–70%。

Conclusion: 该系统验证了面向光伏巡检的主动式多模态自动化范式，可显著提升检测精度与运维效率，并在实地具备部署可行性（高召回、低误报、低通信量）。

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [65] [ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images](https://arxiv.org/abs/2512.06521)
*Jens Dede,Anna Förster*

Main category: cs.CV

TL;DR: 人口增长导致人兽接触增多，传统AI难以适应多变环境。论文提出统一自适应框架“ShadowWolf”，通过动态重训与集成标注-训练-评估流程，降低标注成本、提升野生动物监测准确性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 野生动物栖息地被挤压，人兽互动频次与风险上升，亟需自动化、鲁棒的监测手段。然而真实场景中景观、天气、光照、距离等强变化让传统固定训练流程的模型泛化与部署维护困难且昂贵。

Method: 构建统一的一体化框架ShadowWolf，将数据采集、标注、训练与评估闭环打通；支持根据环境与需求变化进行动态模型再训练与在线适配；在流程层面优化以减少人工标注量并实现现场快速更新。

Result: 通过该框架可在多变条件下持续提升识别准确性和效率，减少标注工作量，实现更稳健的野生动物监测（摘要未给出具体数值）。

Conclusion: ShadowWolf将AI训练与评估统一并可自适应更新，能在野外复杂多变环境中提升监测性能与可扩展性，为保护实践提供更有效的技术路径。

Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.

</details>


### [66] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

TL;DR: 论文探讨“可学习k-space采样模式”在MRI加速重建中的跨域泛化能力，并提出通过在训练中引入采集不确定性（随机扰动k-space轨迹）来提升域鲁棒性。结果显示：学习型采样不仅在源域有效，还能在域偏移下提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作多在单一数据集/模态上优化采样模式，忽视跨扫描仪、协议、模态等域迁移时的可转移性与鲁棒性；临床实际存在显著域偏移，需提升重建模型与采样策略的泛化。

Method: 1) 系统性跨数据集与采集范式评测：比较学习型k-space采样与传统采样在跨域重建中的表现；2) 提出“采集不确定性训练”：在训练阶段对k-space轨迹进行随机扰动，模拟不同扫描仪与成像条件的变化，从而学习到更鲁棒的采样-重建组合。

Result: 学习型采样在跨域设置下较传统/固定采样表现更佳；引入轨迹随机扰动进一步提升对域偏移的鲁棒性与重建质量。

Conclusion: k-space轨迹设计应被视为提升MRI重建跨域泛化的重要自由度；通过在训练中建模采集不确定性，可获得在域外也表现优异的重建系统。

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [67] [Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images](https://arxiv.org/abs/2512.06531)
*Sayan Das,Arghadip Biswas*

Main category: cs.CV

TL;DR: 提出两个自注意力深度学习模型：SAETCN用于MRI脑肿瘤分类（四类：胶质瘤、脑膜瘤、垂体瘤与非肿瘤），验证准确率99.38%；SAS-Net用于肿瘤分割，像素准确率99.23%。


<details>
  <summary>Details</summary>
Motivation: 手工判读MRI费时、主观且在儿童与青少年发病率上升背景下数据量激增；现有CAD模型泛化性不足、在验证集表现不佳，亟需更准确且可泛化的自动化方法。

Method: 构建两种含自注意力机制的深度网络：1）SAETCN用于多类别脑肿瘤分类，引入自注意力增强判别能力；在包含三种肿瘤与正常的公开/自建数据集上训练与验证。2）SAS-Net用于精细分割，采用自注意力模块提升肿瘤区域特征聚合与边界刻画。

Result: SAETCN在验证集上达到99.38%分类准确率；SAS-Net达到99.23%整体像素准确率。

Conclusion: 自注意力增强的架构在脑肿瘤分类与分割任务上取得极高验证性能，显示其用于早期自动化检测与临床辅助诊断的潜力，但仍需验证其泛化与临床可用性。

Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.

</details>


### [68] [Bridging spatial awareness and global context in medical image segmentation](https://arxiv.org/abs/2512.06560)
*Dalia Alzu'bi,A. Ben Hamza*

Main category: cs.CV

TL;DR: 提出U-CycleMLP：一种轻量级U形编码器-解码器，用位置注意、密集空洞与CycleMLP通道块结合，兼顾局部/全局上下文，提升医学图像分割精度与边界细节，在三数据集上优于SOTA，并有消融验证。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型在同时捕获局部与全局上下文方面不足，导致边界像素丢失与分割误差；需要在保持计算效率的同时提升精度与细粒度结构刻画能力。

Method: 设计U形网络U-CycleMLP。编码器：下采样+位置注意权重激励块（增强空间相关性）+密集空洞块（多尺度感受野）以提取多尺度上下文。解码器：上采样+密集空洞块+特征融合重建高分辨率掩码。沿跳跃连接在解码侧引入通道CycleMLP块，在线性复杂度下强化跨尺度特征整合与边界细化。

Result: 在三个基准数据集上，定量与定性实验均显示U-CycleMLP较多种SOTA方法取得更高的分割精度，能更好捕捉细粒度解剖结构，并在不同成像模态上具有鲁棒性。

Conclusion: U-CycleMLP在保持轻量级的同时有效融合局部与全局信息，显著提升医学分割与边界刻画；其核心组件（位置注意、密集空洞、通道CycleMLP与跳连融合）经消融验证有效。

Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.

</details>


### [69] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

TL;DR: SUGAR是一种可扩展的生成式“遗忘”框架，在无需整体重训的情况下，可同时或持续地从3D感知生成模型中移除多达数百个特定人脸身份，同时保持生成质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 3D感知生成模型对人脸身份的高保真合成带来隐私与同意问题；需要一种能按请求移除特定个体（合规/伦理）且不显著损伤模型实用性的技术。

Method: 为每个需遗忘的身份学习个性化替代潜变量（surrogate latent），在重建时将其引导至视觉连贯但非目标身份的输出；并提出持续的实用性保持目标（continual utility preservation），在不断遗忘更多身份时抑制模型退化；支持同时或顺序遗忘，无需从头训练。

Result: 在移除多达200个身份的任务上达到SOTA；与现有基线相比，保留实用性（retention utility）最高提升达700%，同时维持生成质量与多样性并避免不真实的投射或依赖固定模板脸。

Conclusion: SUGAR可在大规模与持续场景下有效、稳定地遗忘多身份，兼顾隐私合规和模型可用性，提供实证上的先进性能与开源实现。

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [70] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

TL;DR: GNC-Pose是一个无需学习的单目6D姿态估计管线，通过渲染初始化、基于几何一致性的对应点加权和GNC鲁棒优化，结合最终的LM细化，在YCB数据集上在无训练与类先验条件下取得与主流方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计要么依赖学习和数据、要么在高外点比例下不稳定。作者希望在不使用训练数据与类先验的前提下，仍能获得稳健、精确的姿态估计，特别是在存在严重外点与对应误差的情况下。

Method: 1) 通过特征匹配与基于渲染的对齐获得粗略2D-3D对应；2) 基于Graduated Non-Convexity原则进行鲁棒优化；3) 引入几何感知的簇式加权：依据3D结构一致性为每个对应分配置信度，从而抑制外点；4) 最后用LM优化进一步细化姿态。

Result: 在YCB Object and Model Set上验证，在无需学习特征、训练数据或类别先验的条件下，精度与多种学习式与非学习式方法具有竞争力，表现稳健。

Conclusion: 几何先验驱动的加权与GNC鲁棒优化相结合，可在无学习的框架下实现简单、稳健且实用的单目6D姿态估计，并在标准数据集上达到竞争性表现。

Abstract: We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [71] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

TL;DR: 提出MedVidBench大规模医学视频理解基准与MedGRPO强化学习框架，解决多数据集奖励失衡，显著提升VL模型在医学视频定位与生成任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在医学视频上表现不佳，因需要精细空间定位、复杂时间推理与临床语义把握；缺少高质量大规模数据与稳健的多数据集训练方法。

Method: 1) 构建MedVidBench：来自8个医学来源、覆盖视频/片段/帧级任务，共531,850条视频指令对，采用专家引导提示与双模型验证的质控流程；2) 监督微调(SFT) Qwen2.5-VL-7B于该基准；3) 提出MedGRPO：跨数据集奖励归一化（将各数据集的中位性能映射到统一奖励尺度）+ 医学LLM裁判（在5个临床维度进行比较式相似度打分），用于平衡多数据集RL训练。

Result: 在MedVidBench上，SFT后的Qwen2.5-VL-7B在全部任务上显著优于GPT-4.1与Gemini-2.5-Flash；在此基础上，MedGRPO进一步提升了定位与描述任务性能，避免了标准RL在奖励失衡下的训练崩溃。

Conclusion: MedVidBench为医学视频理解建立了权威数据与评测基线；MedGRPO提供了稳健的多数据集RL训练方法，推进医学领域视觉语言模型的发展。

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [72] [From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain](https://arxiv.org/abs/2512.06598)
*Muhammad Adil,Patrick J. Clemins,Andrew W. Schroth,Panagiotis D. Oikonomou,Donna M. Rizzo,Peter D. F. Isles,Xiaohan Zhang,Kareem I. Hannoun,Scott Turnbull,Noah B. Beckage,Asim Zia,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出一种仅依赖遥感数据的预测框架，融合Transformer与BiLSTM，对佛蒙特州尚普兰湖北部区域的蓝绿藻水华在最远14天内进行强度预报。在严重缺测（CI缺失>30%、温度缺失~90%）条件下，通过两阶段像元级时序插补与平滑、特征离散化与统计提取，模型在多预见期保持较高F1与AUC，证明了从稀疏卫星时序中捕捉复杂时空动态并实现早期预警的能力。


<details>
  <summary>Details</summary>
Motivation: CyanoHABs威胁生态与公共健康，尚普兰湖北部频发且点位监测稀缺；需要可扩展、连续覆盖的早期预警手段，遥感可满足但受缺测与时空异质性制约，亟需能从稀疏卫星时序中学习有效信号的预测方法。

Method: 构建“Transformer + BiLSTM”遥感时序预测框架：数据源为Cyanobacteria Index（CyAN）与MODIS温度；像元级两阶段预处理（前向填充+加权时间插补；随后平滑以降低事件不连续）；将CI做等频分箱、温度提取统计特征；模型用Transformer捕捉长程依赖，BiLSTM刻画序列动态，输出未来1–14天的水华强度类别/概率。

Result: 在Missisquoi Bay、St. Albans Bay与Northeast Arm多时域验证：F1为1天89.5%、2天86.4%、3天85.5%；14天保持F1 78.9%、AUC 82.6%，显示在高缺测条件下仍有稳健预警性能。

Conclusion: 仅遥感驱动的Transformer-BiLSTM框架能从稀疏卫星数据中学习CyanoHABs时空模式，提供可靠的1–14天预警，支持湖泊管理与风险缓解；预处理与特征工程对提升性能关键。

Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.

</details>


### [73] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

TL;DR: 该论文提出用相对表达而非绝对表达来训练从病理图像预测基因表达的模型，并引入对噪声与批次效应更稳健的排名式损失STRank，经合成与真实数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 绝对基因表达受测序技术复杂性、细胞内在变异、批次效应与随机噪声影响，导致点对点回归损失难以稳定学习准确的绝对值；作者希望绕开不可靠的绝对尺度，利用在不同实验中更一致的相对表达模式。

Method: 提出假设：不同实验间基因的相对表达关系更稳定。基于此构建关系建模与新的排名损失函数STRank，以学习样本内或基因间的相对顺序/比较，而非拟合绝对数值，从而对噪声与批次偏移具有鲁棒性；在合成与真实数据集上训练与评估。

Result: 在合成数据和真实病理-转录组数据上，采用STRank的模型在受噪声与批次效应干扰的场景下优于使用点式回归损失的基线方法，表现为更好的预测一致性与稳健性。

Conclusion: 学习相对表达模式并使用STRank损失可显著缓解批次效应与随机噪声带来的问题，提升从病理图像估计基因表达的鲁棒性，适合在多实验/多批次场景下应用。

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [74] [Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach](https://arxiv.org/abs/2512.06613)
*Yueying Ke*

Main category: cs.CV

TL;DR: 提出一种层级卷积网络用于硅藻图像的多层级分类，利用级联头与层级掩码，在物种层与平坦模型持平，同时在上层级显著更优，并显著减少错误的层级距离并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统硅藻分类高度依赖专家且多为单一层级预测，忽视了生物分类学的层级结构，导致错误不可控且可解释性弱。希望通过在模型结构中显式编码层级，提升准确率、限制错误范围并增强生物学一致性。

Method: 构建一个共享骨干+五个级联预测头（纲/目/科/属/种）的层级CNN。每个头接收上层概率分布；用二值掩码在训练与推理中将候选限制为合法后代；进行渐进式训练以探索自上而下的约束与自下而上的梯度共同作用。与同设定的平坦模型在1,456张、82物种数据集上对比。

Result: 在物种层准确率69.4%，与平坦基线持平；在更高层级全面超越。物种误分时，92.5%的样本仍正确到属（基线为67.2%）；平均分类学距离降低38.2%（1.209对1.955）。渐进训练将class准确率由96.2%提升到99.5%，并在上层级带来6–8%的增益。

Conclusion: 在网络中编码分类学层级能在不牺牲细粒度准确率的前提下，显著提升上层级性能、限制错误在局部并增强可解释性。层级约束与细粒度梯度的双向机制共同改进特征学习，适用于多层级生物分类与生态监测等任务。

Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.

</details>


### [75] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

TL;DR: 用MAE在强引力透镜模拟图上做自监督预训练，再微调同一ViT编码器用于两任务（暗物质模型分类与超分辨），在合适遮挡比（90%）下分类显著优于从零训练，超分辨小幅提升；遮挡比越高分类越好但重建略降。


<details>
  <summary>Details</summary>
Motivation: 噪声高、分辨率低的强引力透镜图像中弱小的暗物质次结构信号难以提取，传统有监督训练对标注/任务特定数据依赖强、泛化差。希望通过物理模拟上的自监督学习获取可迁移特征，兼顾多任务（分类与重建）。

Method: 在DeepLense ML4SCI模拟透镜图上采用Masked Autoencoder对ViT编码器做掩码图像建模预训练，系统扫描掩码比例；随后将同一编码器分别微调用于：(i) 多类暗物质模型分类（CDM/类轴子/无子结构），(ii) 图像超分辨（16×16→64×64）。进行与从零训练的ViT基线对比，并做掩码比例消融。

Result: 当掩码比例为90%时，分类微调取得AUC 0.968、准确率88.65%，超过从零训练的AUC 0.957、准确率82.46%；超分辨方面，MAE预训练模型得到约PSNR 33 dB、SSIM 0.961，相比从零训练有小幅提升。消融显示更高掩码比例提升分类性能但略损重建质量。

Conclusion: 基于物理模拟的MAE预训练能学到通用、可复用的透镜图像表示，在多任务上至少匹配并多处优于从零训练；掩码比例是关键调参，存在分类与重建之间的权衡。该策略为强引力透镜分析提供了灵活的共享编码器框架。

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [76] [TextMamba: Scene Text Detector with Mamba](https://arxiv.org/abs/2512.06657)
*Qiyan Zhao,Yue Yan,Da-Han Wang*

Main category: cs.CV

TL;DR: 提出一种基于Mamba与注意力融合的场景文本检测器，通过显式Top-k选择、双尺度FFN与嵌入金字塔增强，缓解Transformer长程依赖中的遗忘与干扰问题，在CTW1500/TotalText/ICDAR19ArT上达SOTA或竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在全局依赖提取上受限，Transformer虽能建模长程依赖，但原生注意力在跨域泛化与长序列建模时可能遗忘关键信息或关注无关表示，影响场景文本检测效果。Mamba的线性复杂度选择机制在长依赖建模上表现更优，适合改进文本检测编码器。

Method: 将Mamba的选择机制与注意力层融合：在编码器中引入Top-k显式筛选关键特征，抑制无关信息干扰；设计双尺度前馈网络以增强高维隐状态交互；提出嵌入金字塔增强模块实现多尺度特征融合，提升对不同形态文本的检测能力。

Result: 在多个基准上取得领先或竞争性成绩：CTW1500 F=89.7%，TotalText F=89.2%，ICDAR19ArT F=78.5%。

Conclusion: 融合Mamba选择机制与注意力可有效缓解Transformer在文本检测中的长程依赖缺陷，显式Top-k、双尺度FFN和多尺度融合共同提升编码与检测性能，达到SOTA/竞争性结果；代码将开源。

Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.

</details>


### [77] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出DEPER方法，将个体观看行为融入个性化图像描述，通过辅助注意力预测学习同时编码语言风格与观视模式，在冻结VLM上用轻量适配器实现小样本个性化，跨四数据集平均提升24%。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像描述只关注语言风格，忽视不同个体对同一图像的观看差异（关注区域/顺序等），导致描述高方差且与人类不一致，需将个体的观看行为显式建模以提升对齐与质量。

Method: 提出DEPER：学习“主体嵌入”同时捕捉语言风格与观看行为；通过辅助任务预测个体注意力（视线/关注分布）以指导嵌入学习；使用轻量Adapter将该嵌入与冻结的视觉-语言模型对齐，实现少样本个性化，无需重训。

Result: 在四个涵盖不同观看任务与描述粒度（短/细节）的数据集上，DEPER相对基线平均提升24%，产生更符合人类关注模式且更高质量的描述。

Conclusion: 建模“人如何看”能更好预测“人如何说”。在多模态系统中显式建模感知多样性可同时提升性能与人类对齐度；DEPER验证了利用个体注意力模式进行个性化描述的有效性与数据/训练效率。

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [78] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出CoT4Det，把检测等感知任务拆为“分类—计数—定位”三步，使LVLM在COCO2017上mAP由19%升至33%，并在RefCOCO与Flickr30k实体等基准上显著提升。


<details>
  <summary>Details</summary>
Motivation: LVLM在通用VQA/OCR上强，但在检测、分割、深度估计等感知任务上明显落后于专家模型，尤其密集场景与小目标召回差；需要一种利用LVLM推理优势而非端到端检测的策略。

Method: 提出Chain-of-Thought for Detection（CoT4Det）：将感知任务重构为三步可解释流程——(1) 分类：识别类别；(2) 计数：估计每类目标数量；(3) 定位/对齐（grounding）：输出对应区域/框；以此更契合LVLM的推理与逐步分解能力，无需改动底层模型。

Result: 在Qwen2.5-VL-7B-Instruct上，COCO2017 val的mAP从19.0%提升到33.0%；在RefCOCO系列上超基线+2%，在Flickr30k Entities上提升19%，同时保留通用视觉语言能力。

Conclusion: 将感知任务链式分解显著增强LVLM的检测与指代性能，尤其对密集场景和小目标有效，且不牺牲通用能力；是一种简单高效、可泛化到多种感知基准的策略。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [79] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出DEViL：将视频LLM与开放词汇检测器耦合，通过参考语义token（RST）实现端到端时空定位与推理，并引入管道挖掘的时间正则（TTReg）提升跨帧一致性，缓解自回归框解码的漂移与误差累积，在STVG与GroundedVQA上表现强。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型将边界框当作文本token自回归生成，导致输出序列极长、空间误差随时间累积、跨帧定位漂移，难以稳健完成时空定位与涉及因果、时序、动作关系的推理。

Method: 构建Detector-Empowered Video LLM（DEViL），将视频LLM与开放词汇检测器（OVD）通过参考语义token（RST）耦合：LLM把用户查询蒸馏为RST，RST既作为控制信号也替代OVD的文本嵌入，实现对参考理解与空间定位的联合端到端学习；并在OVD中加入tube-mined temporal regularization（TTReg），从候选tube中挖掘并约束时序一致的目标查询，增强跨帧关联与稳定性。

Result: 在多种细粒度视频理解任务上取得强性能，尤其在时空指代定位（STVG）与带定位的VQA（GroundedVQA）上优于现有方法（具体数值未在摘要中给出）。

Conclusion: 以检测器增强的视频LLM，通过RST实现语义-定位耦合、通过TTReg保障时序一致性，从根本上缓解自回归框解码的长序列与漂移问题，提升时空指代与推理能力；代码将开源。

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [80] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

TL;DR: 提出RunawayEvil，多模态I2V模型的首个自进化越狱框架，在商业模型上显著提升攻击成功率，用于暴露安全脆弱性并推动更稳健的视频生成防护。


<details>
  <summary>Details</summary>
Motivation: I2V图像到视频生成在创作上强大，但其多模态越狱风险与防护研究不足，需要系统化方法评估并揭示安全薄弱点，以促进模型稳健性。

Method: 基于“策略-战术-行动”范式构建自进化攻击框架RunawayEvil：1) 策略感知指挥单元：通过强化学习的策略定制与LLM策略探索，持续进化高层攻击策略；2) 多模态战术规划单元：将选定策略转化为协调的文本越狱指令与图像篡改指南；3) 战术行动单元：执行并评估多模态协同攻击，形成闭环反馈以自我放大。

Result: 在Open-Sora 2.0、CogVideoX等商业I2V模型上取得SOTA攻击成功率；在COCO2017上较现有方法提升58.5%~79%。

Conclusion: RunawayEvil为I2V模型提供系统化、可自我强化的多模态越狱评测工具，有助于识别和量化脆弱性，推动更安全、更稳健的视频生成系统的构建。

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [81] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出EMGauss：用高斯splatting把切片序列当作随时间演化的2D高斯点云来重建3D，从而避免各向同性假设；并用Teacher-Student自举在稀疏数据上提升质量，相比扩散/GAN更好插值、可连续合成且无需大规模预训练。


<details>
  <summary>Details</summary>
Motivation: vEM体成像受采集权衡限制导致轴向分辨率差，各向异性体数据普遍；现有方法多依赖“横向先验→各向同性重建”的假设，但对形态各向异性的结构失败，亟需一种不依赖各向同性假设、能从2D切片可靠重建3D的通用框架。

Method: 把切片到三维的重建重构为“3D动态场景渲染”问题：将轴向切片的推进视作时间维度上2D高斯点云的演化，采用Gaussian splatting进行体表达与渲染；在观测稀疏时，引入Teacher-Student自举，用教师对未观测切片的高置信预测作为伪监督，引导学生模型训练，提升保真度与稳定性。

Result: 相较扩散与GAN重建：显著提升切片间插值质量；实现连续切片合成（非离散层级）；训练不依赖大规模预训练；在vEM数据上更稳健，特别对形态各向异性的结构表现更好。

Conclusion: EMGauss提供了绕开各向同性假设的切片到3D重建新范式，利用高斯splatting与教师-学生自举在数据稀疏场景下亦能获得高保真3D；方法有望推广到vEM之外的多种成像领域。

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [82] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: UniVoiceLite 提出一个统一且轻量的音视融合模型，同时处理语音增强与语音分离，并在无监督设置下取得强健表现。


<details>
  <summary>Details</summary>
Motivation: 现实场景常同时存在环境噪声与多人重叠说话，传统将SE与SS割裂处理、或采用多阶段重模型且依赖有监督配对数据，导致参数臃肿、可扩展性与泛化性受限。

Method: 构建单一轻量的音频-视觉框架：利用唇动与人脸身份特征引导目标语音提取；引入Wasserstein距离正则以稳定潜空间，从而在无需成对干净-带噪数据的无监督训练下学习到可分离/增强的表示。

Result: 在含噪与多人混讲的多种场景中，模型表现强劲，同时保持较低参数量与较高效率，并显示良好的跨场景泛化能力。

Conclusion: UniVoiceLite 在统一SE与SS方面实现兼顾效率与泛化的无监督轻量方案，证明音视融合与分布正则可在缺乏配对数据时有效支撑鲁棒语音提取。

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [83] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 论文提出ECVGPO，一种针对视觉定位任务的可解释熵控制强化学习算法，通过更好地平衡探索与利用，在多项基准与多种MLLM上取得全面提升。


<details>
  <summary>Details</summary>
Motivation: 现有使用强化学习微调MLLM的工作在推理任务上借助各类熵控制技巧获益明显，但对以感知为主的视觉定位（visual grounding）任务中“熵”扮演的角色、特性及如何有效控制缺乏系统研究。

Method: 对比分析视觉定位与推理任务中的策略熵特性与作用；据此设计ECVGPO算法：显式、可解释地调节策略熵以控制探索-利用权衡，并在MLLM的视觉定位训练中进行策略优化（类似RL的policy optimization框架并加入熵调度/约束机制）。

Result: 在多个视觉定位基准与不同模型上，应用ECVGPO获得普遍性能提升，显示其熵控制策略有效。

Conclusion: 感知导向任务中熵的角色与需求不同于推理任务；通过ECVGPO进行有针对性的熵调控可更好地平衡探索与利用，从而稳定且显著提升视觉定位性能。

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [84] [Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data](https://arxiv.org/abs/2512.06736)
*Jiaxing Fan,Jiaojiao Liu,Wenkong Wang,Yang Zhang,Xin Ma,Jichen Zhang*

Main category: cs.CV

TL;DR: 提出一种基于骨架数据的GCN-LSTM-注意力网络用于脑卒中后代偿动作检测，在16名患者数据上获得0.8580准确率，优于SVM/KNN/RF，消融证实各模块有效。


<details>
  <summary>Details</summary>
Motivation: 上肢运动功能障碍常伴随代偿动作，长期不利于恢复；现有检测方法精度有限或对设备/标注依赖高，需开发更精准、可推广的自动检测模型以优化康复训练。

Method: 采集16名卒中患者在特定康复动作中的Kinect骨架序列；数据预处理后，构建融合图卷积(建模关节-骨架拓扑)、LSTM(时序依赖)与注意力(关键关节/时刻加权)的GCN-LSTM-ATT模型；并与SVM、KNN、RF对比；做消融实验评估各组件贡献。

Result: GCN-LSTM-ATT在代偿动作检测任务上达到0.8580准确率，显著优于传统机器学习基线；消融显示GCN、LSTM与注意力模块均带来性能提升。

Conclusion: 基于骨架序列的GCN-LSTM-注意力模型能更准确检测卒中患者代偿动作，有望为个性化、精细化康复策略提供技术支撑。

Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.

</details>


### [85] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 提出FedSCAl，用服务器-客户端对齐机制在联邦源数据不可用的域自适应中抑制客户端漂移，提升伪标签质量并在视觉分类基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FFreeDA场景下，服务器只能提供预训练源模型，无法访问源数据；各客户端未标注数据分布差异大，直接将SFDA方法用于联邦学习会因极端异质性导致客户端漂移和伪标签不可靠，影响适配效果。

Method: 提出联邦框架FedSCAl，引入Server-Client Alignment（SCAl）机制：在本地训练中对齐客户端与服务器模型的预测，对客户端更新施加正则，缓解因域间差异带来的漂移；在全局聚合中利用对齐后的更新提升伪标签准确性与稳定性。

Result: 在多个视觉分类基准上进行广泛实验，FedSCAl在FFreeDA设定下持续优于当前最先进联邦方法，并观察到对齐后客户端伪标签准确率提升。

Conclusion: 通过SCAl正则化客户端更新，可有效缓解FFreeDA中的客户端漂移，提升伪标签质量，从而在无源数据的联邦域自适应任务中取得稳定且领先的性能。

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [86] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: 论文将AIGI检测拆解为“语义一致性检查”和“像素伪迹检测”两类互补任务，并提出两分支模型AlignGemini：VLM仅用高层语义监督微调，像素专家仅用低层伪迹监督训练，以实现任务-模型对齐，显著提升泛化与准确率（平均+9.5）。


<details>
  <summary>Details</summary>
Motivation: 现有做法直接把VLM改造成检测器，资源开销大且易幻觉；用像素伪迹微调VLM迁移性差，而用语义监督微调VLM能泛化但对细粒度伪迹迟钝。传统像素检测器能抓低层伪迹却缺乏语义感知。存在任务-模型错配导致系统性盲点。

Method: 提出“任务-模型对齐”原则：将AIGI检测形式化为两互补子任务。构建两分支检测器AlignGemini：1) 语义分支：基于VLM，仅用纯语义一致性监督微调，增强语义判别与跨域泛化；2) 像素分支：像素伪迹专家，仅用纯低层伪迹监督训练，敏感于细粒度伪迹。通过对两个简化数据集施加正交监督，使两分支各自专注并互补。

Result: 在5个真实场景基准上，AlignGemini的平均准确率提升+9.5，显示两分支互补能有效覆盖语义与像素线索，减少盲点并提升泛化。

Conclusion: AIGI检测应同时处理语义一致性与像素伪迹两任务，且应选用与任务匹配的模型进行正交训练。AlignGemini验证了“任务-模型对齐”的有效性，为可泛化的AIGI检测提供了实证路径。

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [87] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

TL;DR: 提出UARE：首个统一视觉-语言模型，将图像质量评估(IQA)、图像恢复与增强在同一框架中联合，通过两阶段训练与多任务协同，使IQA信号直接指导恢复，显著提升多退化场景下的恢复与增强效果。


<details>
  <summary>Details</summary>
Motivation: IQA与图像恢复在概念上紧密相关，但研究多各自为政；多模态统一理解-生成模型显示更强理解能提升生成质量，启发将IQA与恢复统一，并系统研究“让质量评估指导恢复”的价值与方法，尤其在多重退化、混合失真场景下。

Method: 建立在预训练的统一理解-生成多模态基础上，提出两阶段训练：1) 逐步课程学习，从单一失真到高阶混合退化，培养多退化适应能力；2) 统一微调，将质量理解(IQA)与恢复以交织的文本-图像数据共同训练，使IQA信号与恢复目标对齐，通过多任务协同学习利用IQA提升恢复/增强。

Result: 在IQA、恢复、增强多个任务与数据集上进行广泛实验，UARE在多退化条件下取得优异表现，相比分离训练或无IQA引导的基线显著提升恢复与增强质量。

Conclusion: 统一IQA与恢复的多模态框架是有效的；通过课程式多退化训练与统一微调，IQA可直接引导并增强恢复性能。UARE验证了“更强理解促更好生成”的理念，适用于实际复杂退化场景。

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [88] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

TL;DR: VisChainBench 提出一个面向多图、多轮情境的LVLM基准，强调低语言线索下的逐步视觉推理评估，涵盖三大领域、1457个任务、2万余张图像，并通过多智能体生成流程降低语言偏置、提高视觉多样性。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM评测多为静态或单步比较，强依赖文本提示，难以衡量在连续任务中基于上下文的逐步视觉到视觉推理能力。实际应用常需跨图像、跨回合、依赖前序结果的决策过程，缺乏相应严格基准。

Method: 构建VisChainBench：包含1457个多步任务、覆盖2万+图像、跨日常与工程排障等三类场景；任务按现实决策链条设计，最小化语言引导；采用多代理生成管线以确保视觉多样性、控制语言偏置；提供数据与构建代码（链接至HuggingFace）。

Result: 得到一个大规模、结构化的基准数据集，能够系统评估LVLM在多图多轮、依赖上下文的逐步视觉推理能力；任务覆盖面广、难度递进，并在设计上降低了语言暗示对模型表现的影响。

Conclusion: VisChainBench填补了LVLM多步视觉推理评测空白，为研究与对比提供标准化平台；其多代理与低语言偏置设计更贴近现实决策流程，适合作为后续模型训练与评测的参考。

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [89] [JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms](https://arxiv.org/abs/2512.06763)
*Chengyang Yan,Mitch Bryson,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 提出联合优化相机硬件与自适应控制以提升下游视觉任务表现，结合梯度与无导数优化，并用DF-Grad处理不可微效应（如运动模糊）；在弱光与快速运动等场景优于分别优化的方法。


<details>
  <summary>Details</summary>
Motivation: 传统相机-感知共设计多优化制造时固定参数，忽视运行时需自适应的参数（如曝光）。不可微的成像过程与离散/连续混合参数和控制策略的学习，使得端到端优化困难。需要一个能同时处理硬件、控制算法和下游任务的统一框架。

Method: 构建统一优化框架，融合梯度法与无导数法，支持连续/离散参数以及不可微成像过程；提出DF-Grad：用无导数优化器产生的信号训练自适应控制网络，同时进行无监督、任务驱动的学习，实现对曝光等动态参数的在线控制，并与硬件参数联合优化。

Result: 在弱光、快速运动等挑战场景中，联合优化在感知任务上优于仅优化静态参数或将静/动参数分开优化的基线；证明DF-Grad能有效应对运动模糊等不可微效应。

Conclusion: 硬件参数与自适应控制的联合、任务驱动优化是提升感知性能的有效途径；所提统一框架为相机系统与视觉任务共设计提供通用方法，尤其在复杂动态环境中优势明显。

Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

</details>


### [90] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

TL;DR: 提出SiTe（Stitch and Tell）方法，通过拼接图像并生成具空间约束的文本，向训练数据注入结构化空间监督，无需人工标注或昂贵模型，显著缓解视觉语言模型的空间幻觉并提升空间理解，同时保持通用性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型常产生空间幻觉，错误描述物体相对位置。作者认为根源在于图像与文本的非对称性（文本缺乏显式空间结构），因此需要在训练中引入更强的空间结构信号且成本低、可扩展。

Method: 提出无标注、可插拔的数据增强方法SiTe：沿空间轴（水平/垂直）拼接多张图像，依据拼接布局自动生成空间感知的描述或问答（如“左图/右图/上图/下图”关系），形成结构化的图文对；将其用于不同VL架构和数据集的训练/微调，无需人力或高级模型。

Result: 在LLaVA-v1.5-7B、LLaVA-Qwen2-1.5B、HALVA-7B三种架构、两种训练集、八个基准上评测：空间理解指标显著提升（如MME_Position +5.50%、Spatial-MM +4.19%），通用VL基准同样持平或提升（COCO-QA +1.02%、MMBench +4.76%）。

Conclusion: 向训练数据显式注入空间结构是缓解空间幻觉、提升空间理解的有效途径，且不牺牲通用能力。SiTe简单、无标注、可插拔，具有较强泛化与实用性。

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [91] [RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06774)
*Longjie Zhao,Ziming Hong,Zhenyang Ren,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 提出RDSplat，一个针对扩散编辑具有鲁棒性的3D Gaussian Splatting(3DGS)水印方法，通过面向低频高斯与扩散代理对抗训练，在保持不可见性的同时大幅提升在扩散编辑下的可检测性与稳定性，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS水印在扩散式编辑（会抹除溯源信息）下易失效，实际应用中3D资产常被扩散模型编辑，急需对扩散编辑本质保留成分具有内在鲁棒性的水印方案。

Method: 多域框架在3DGS原生空间嵌入水印，关键包括：(1) 主动定位并利用扩散编辑天然保留的低频高斯成分；(2) 协同的协方差正则与2D滤波实现对低频成分的水印注入；(3) 以高斯模糊作为扩散编辑的高效训练替代，进行对抗式微调，提升对扩散的鲁棒性。

Result: 在三个基准数据集上进行定量与定性评估，RDSplat在扩散编辑场景下的鲁棒性显著优于现有方法，同时保持水印不可见性，整体达到SOTA表现。

Conclusion: 针对扩散编辑破坏水印的问题，RDSplat通过低频高斯定位与扩散代理对抗训练实现内在鲁棒的3DGS水印，在不牺牲视觉质量的前提下显著增强可恢复性，适合数字资产版权保护等应用。

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.

</details>


### [92] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

TL;DR: 提出一个实时后处理算法，将BlazePose的2D与3D估计通过加权优化融合，并加入骨骼长度与生物力学约束，使用自适应信任的卡尔曼滤波个性化骨长，实测在Physio2.2M上显著降低3D MPJPE与关节夹角误差，适用于消费级设备的自动康复与运动指导。


<details>
  <summary>Details</summary>
Motivation: 现有单目视频姿态估计（如BlazePose）虽实时但缺乏解剖约束，易出现骨长漂移与生物力学不合理的关节姿态，限制其在物理治疗等需要稳定、可信结果场景中的应用。

Method: 对BlazePose的2D与3D输出进行加权优化融合；在目标函数中对偏离期望骨长与生物力学模型的解添加惩罚；利用带自适应测量可信度的卡尔曼滤波在线估计个体化骨长；整体作为后处理模块实时运行，计算高效。

Result: 在Physio2.2M数据集上，相比原始BlazePose 3D，3D MPJPE降低10.2%，身体段间角度误差降低16.6%。

Conclusion: 融合2D/3D估计并引入解剖与生物力学先验的实时后处理，可在消费级设备上提供更稳健、解剖一致的3D姿态，适用于自动物理治疗、医疗与运动教练；后端仅处理匿名数据以保障隐私。

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [93] [Generalized Geometry Encoding Volume for Real-time Stereo Matching](https://arxiv.org/abs/2512.06793)
*Jiaxin Liu,Gangwei Xu,Xianqi Wang,Chengliang Zhang,Xin Yang*

Main category: cs.CV

TL;DR: 提出GGEV，一种实时立体匹配网络，通过深度感知特征与深度感知动态代价聚合，在保持实时性的同时显著提升跨域泛化，达成多个基准SOTA并优于现有实时方法的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有实时立体匹配强调在域内精度，忽视真实应用中的跨域泛化；而利用单目大模型提升泛化的“立体基础模型”往往推理延迟大，难以实时。需要一种既具强泛化又能实时的立体匹配方案。

Method: 1) 提取深度感知特征，编码域不变的结构先验，为代价聚合提供指导；2) 设计深度感知动态代价聚合（DDCA），将上述先验自适应注入每个视差假设，强化在未知场景中的脆弱匹配关系；两步轻量且互补，构成具泛化力的几何编码体（GGEV）。

Result: 在零样本跨域评测中超越所有现有实时方法；在KITTI 2012、KITTI 2015 与 ETH3D 上达到SOTA。推理保持实时且延迟显著低于依赖单目基础模型的方法。

Conclusion: GGEV在不牺牲实时性的前提下实现强跨域泛化，通过深度感知先验与动态代价聚合提升立体匹配鲁棒性，适合真实世界部署，并在主流基准上取得领先性能。

Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.

</details>


### [94] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

TL;DR: 论文提出VDOT，一个统一高效的视频生成模型，通过少步生成实现与百步基线相当或更优的质量。核心在于将分布匹配蒸馏框架与计算型最优传输距离结合，并引入判别器学习真实视频分布，同时构建自动化多任务数据标注/筛选流水线与统一评测基准UVCBench。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成要么只支持有限条件（如文本、图像条件等之一），要么推理步骤过多、耗时长，难以落地。需要一个既统一多条件、又高效稳定、质量可靠的生成方案。

Method: 1) 采用Distribution Matching Distillation（DMD）框架进行蒸馏，将长步扩散生成压缩为少步生成；2) 在传统KL最小化之外，引入计算型最优传输（OT）距离来度量真实与生成的score分布差异，利用其几何约束缓解KL在少步场景的零强制与梯度塌陷问题；3) 集成判别器，使模型接触真实视频数据以提升逼真度；4) 构建全自动视频数据多任务标注与过滤流水线；5) 提出统一评测基准UVCBench。

Result: 在4步去噪的设定下，VDOT在多个视频创建任务上达到或超过需要100步去噪的基线方法，生成质量与效率兼优，并在统一基准上验证其有效性。

Conclusion: 通过将OT距离融入DMD蒸馏并结合判别器学习，VDOT实现了稳定高效的少步视频生成，统一多条件任务，配套自动化数据管线与评测基准，为实际应用提供更可行的解决方案。

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [95] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 提出MMDuet2：让视频多模态大模型在流式观看过程中“何时开口、何时沉默”进行自决，并以多轮强化学习训练以实现及时且准确的主动回应，在ProactiveVideoQA上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLM多为回合式，被动在用户发言后作答，不适合实时应用；以阈值/手工规则决定何时回应或需要精确标注回复时刻，均不稳定或成本高。

Method: 提出文本到文本的主动交互范式：在每个时间步基于对话历史与当前帧之前的视觉上下文，模型输出“回应/保持沉默”的文本决策与内容；训练上采用两阶段：SFT构建基础能力；随后用多轮强化学习（无需精确标注回复时刻）对时机与质量联合优化，鼓励及时且准确的响应。使用52k视频、两类对话数据。

Result: 在响应时机与回答质量上优于现有主动式视频MLLM基线；在ProactiveVideoQA基准上达到SOTA表现。

Conclusion: 多轮RL驱动的文本式主动交互可有效解除人工阈值与精确时标依赖，使视频MLLM在流式场景中更及时、准确地发言，验证了MMDuet2在主动回应时机与质量上的优势。

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [96] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

TL;DR: 提出RMAdapter：一种双分支重构式多模态适配器，在少样本微调VLM时同时增强任务适应性与泛化能力，并以极低开销超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 少样本场景下微调VLM需在“任务特化”与“通用泛化”间权衡；现有方法多偏重提示学习（prompt），适配器（adapter）路线研究不足且性能落后。

Method: 设计双分支适配器：1) 适应分支进行参数高效微调，注入任务特定知识；2) 重构分支将潜在特征重建回原特征空间以保留通用知识。层内局部重构损失、共享投影模块降低计算与参数；加入一致性约束在判别性与泛化间调节。

Result: 在三类评测（新类别泛化、新数据集迁移、域泛化）上，无需数据增强或多重提示，RMAdapter在各项指标上稳定优于SOTA方法。

Conclusion: 通过双分支重构与一致性约束，RMAdapter在保持轻量的同时，实现少样本微调对任务适应与通用能力的动态平衡，取得全面领先表现。

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [97] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 提出MeshSplatting：以网格为核心的可微渲染重建方法，在保持实时性的同时生成可在3D引擎中直接使用的高质量连续网格；在Mip-NeRF360上超越现有网格化SOTA（MiLo）0.69 dB，训练更快、占用更低。


<details>
  <summary>Details</summary>
Motivation: 点基元（如3D Gaussian Splatting）虽有实时渲染与高视图合成质量，但与AR/VR与游戏引擎依赖的网格化管线不兼容；现有网格化方法在质量、效率或可编辑性上存在折衷，亟需一种既能端到端优化外观与几何、又能在工业引擎中无缝运行的网格表示。

Method: 采用可微分渲染框架，在训练中联合优化几何与外观；通过受限Delaunay三角化强制表面连通性，并配合表面一致性细化策略，生成平滑、无裂缝的高质量网格；利用网格渲染的高效性实现实时可视化与交互。

Result: 在Mip-NeRF360基准上，相比当前网格化新视图合成SOTA（MiLo）提升PSNR +0.69 dB；同时训练速度提升约2倍，显存占用减半；渲染可实时，并可直接在3D引擎中运行。

Conclusion: MeshSplatting把神经渲染与交互式3D图形连接起来：在保持或提升视觉质量的同时产出可编辑、可部署的网格，兼顾训练效率与内存占用，为AR/VR和游戏场景提供无缝的实时场景交互能力。

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [98] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

TL;DR: SparseCoop提出一个完全稀疏的协同感知框架，用显式运动状态实例查询、粗到细聚合与协同实例去噪，摆脱BEV中间表示，在V2X-Seq与Griffin上实现SOTA，且通信、算力和时延鲁棒性更优。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知常共享密集BEV特征，通信成本随参与体数平方增长，且跨异步/异视角对齐缺乏灵活与可解释性；稀疏查询方法虽更轻量，但几何表示不足、融合策略欠佳且训练不稳定。急需一种既高效通信又具精确时空对齐和稳定训练的框架。

Method: 提出SparseCoop：1) 运动学约束的实例查询，以包含3D几何与速度的显式状态向量进行精准时空对齐；2) 粗到细的聚合模块，实现稳健的跨车体融合；3) 协同实例去噪任务，加速并稳定训练；全流程丢弃中间BEV表示，端到端用于3D检测与跟踪。

Result: 在V2X-Seq与Griffin数据集上达到SOTA，同时表现出更高的计算效率、更低的通信开销，并对通信时延具有更强鲁棒性。

Conclusion: 以显式状态实例为核心的全稀疏协同感知可替代BEV密集共享，实现更精确的时空对齐与稳健融合，带来SOTA性能与显著效率优势。

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [99] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

TL;DR: 提出CADE，将持续学习融入弱监督视频异常检测，通过双生成器处理数据不平衡与标签不确定、并用多判别器集成缓解遗忘与异常模式不完整性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: WVAD标注成本低但多数方法假设静态数据域；在多场景/随时间变化的真实环境中会产生域移，若仅用新数据增量训练会遗忘旧知识并偏向部分异常模式，导致漏检。因此需要将持续学习思想引入WVAD以同时应对域移、遗忘与弱标签问题。

Method: 提出CADE：1) Dual-Generator（DG）用于缓解WVAD中的数据不平衡与标签不确定（弱标签），提升对异常样本的合成/重加权与鲁棒性；2) Multi-Discriminator（MD）集成，将多个判别器/模型对不同异常模式的专长进行互补，以捕获因持续学习遗忘而遗漏的异常；整体在多场景顺序任务下进行训练以适应域移。

Result: 在多场景视频异常检测基准（如ShanghaiTech与Charlotte Anomaly）上进行大量实验，CADE在性能上显著超过现有VAD/WVAD方法。

Conclusion: 将持续学习与弱监督VAD首度结合，通过DG处理弱监督固有问题、MD集成缓解遗忘引发的异常模式不完整性，实现对跨场景域移的更稳健检测并取得SOTA表现。

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [100] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

TL;DR: 提出PA-VAD：无需真实异常视频，通过少量真实正常图像驱动合成伪异常视频，与真实正常视频配对训练，在弱监督设定下取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实部署受限于真实异常视频稀缺和采集成本高；希望在不收集真实异常的情况下仍能获得高精度异常检测，以提升可扩展性与隐私合规。

Method: 生成驱动的弱监督框架。1) 合成阶段：用CLIP从少量正常图像集中挑选与场景/类相关的初始图像；再用视觉语言模型优化文本提示，提升合成的语义保真和场景一致性；随后调用视频扩散模型生成伪异常视频。2) 训练阶段：使用域对齐正则模块，结合域对齐和“内存使用感知”的更新策略，缓解合成异常在时空幅度上的过强偏差，从而稳定学习异常检测器。训练数据为合成伪异常视频+真实正常视频。

Result: 在标准弱监督划分上：ShanghaiTech AUC 98.2%，比最强使用真实异常的方法高0.6%；UCF-Crime AUC 82.5%，比UVAD SOTA高1.9%。

Conclusion: 无需收集真实异常也能实现高精度视频异常检测。PA-VAD通过高保真合成与域对齐正则，提供了可扩展、实用的部署路径。

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [101] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

TL;DR: 提出一种仅用椎体级“健康/恶性”弱标注训练的CT骨转移分割方法，通过生成式编辑+选择性遮挡实现精确的溶骨/成骨病灶分割，性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 椎体转移灶分割对治疗规划重要，但逐体素标注稀缺且溶骨/成骨病灶易与退变改变混淆，导致可扩展自动分割难以实现。希望用更廉价的椎体级标签，仍能得到可靠病灶掩膜。

Method: 1) 仅用椎体级健康/恶性标签训练。2) 采用扩散自编码器(DAE)对每个椎体进行分类器引导的“健康编辑”，并与原图做逐像素差值得到候选病灶区域。3) 提出“Hide-and-Seek Attribution”：对每个候选区域，单独显露该区域、遮蔽其余候选；将编辑后图像经DAE投回数据流形；用潜空间分类器量化该区域对“恶性”判定的独立贡献。4) 对高评分区域输出最终溶骨/成骨分割。

Result: 在独立放射科医师标注上，无掩膜监督条件下仍取得高性能：溶骨/成骨F1=0.91/0.85，Dice=0.87/0.78，均优于基线(F1=0.79/0.67，Dice=0.74/0.55)。

Conclusion: 椎体级弱标签可经生成式健康编辑与选择性遮挡转化为可靠的病灶掩膜。将生成模型与归因策略结合，可在CT中实现准确的弱监督溶骨/成骨分割，具备临床可扩展性潜力。

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [102] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出Omni-Referring Image Segmentation（OmniRIS），能用文本与多种视觉提示（掩码/框/涂鸦）作为“全能提示”进行高泛化分割，并提供数据集OmniRef与基线OmniSegNet，实验显示其在多设置下具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有指代分割多为单模态条件（仅文本或仅视觉），难以同时利用文本的属性细粒度指代与视觉提示对罕见目标的定位能力，且难覆盖一对多、多对多等复杂场景。因此需要一种统一框架与基准，推动更实用、泛化更强的分割。

Method: 1）提出新任务OmniRIS：输入可为文本指令与参考图像的多种提示（掩码/框/涂鸦），支持多种实例关系设置（one-vs-many、many-vs-many）。2）构建大规模数据集OmniRef：30,956张图、186,939条全能提示，并配套评价体系。3）设计基线模型OmniSegNet，解决“全能提示编码”等关键问题，使模型能遵循跨模态指令完成分割。

Result: 在广泛实验中，OmniSegNet有效理解并执行跨模态全能提示，较现有单模态或特定设置方法展现更强的泛化性与性能，验证OmniRIS任务与方法的可行性与优越性。

Conclusion: OmniRIS作为统一、通用的指代分割范式，结合文本与视觉提示，能更好地应对复杂多样的分割需求；OmniRef与OmniSegNet为该方向提供了数据与强基线，推动高度泛化图像分割的发展。

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [103] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 提出AutoQ-VIS：通过质量引导的自训练闭环，从合成视频逐步适配到真实视频，在YouTubeVIS-2019 val上达52.6 AP50，无需人工标注，超越VideoCutLER 4.4%。


<details>
  <summary>Details</summary>
Motivation: VIS标注昂贵且难：既要像素级分割又要时序一致标注。现有无监督方法（如VideoCutLER）虽用合成数据并摆脱光流，但受制于合成到真实的域差距，性能受限。需要一种能自动评估伪标签质量并实现从合成到真实自适应的机制。

Method: 提出质量感知自训练框架AutoQ-VIS：构建“伪标签生成—自动质量评估”闭环。先在合成数据上训练，生成真实视频的伪标签；再通过自动质量评估模块为伪标签打分与筛选，选取高质量样本进行自训练，逐步迭代，缩小域差距。整个流程无需光流及人工标注。

Result: 在YouTubeVIS-2019 val集上达到52.6 AP50，较此前SOTA VideoCutLER提升4.4个百分点；全程零人工标注。

Conclusion: 质量感知的自训练闭环能有效弥合合成与真实域差距，使无监督VIS达到SOTA，验证了该范式的可行性与实用性；代码将开源（AutoQ-VIS）。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [104] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文提出在自动驾驶中加入“空间检索”范式：将离线检索的地理图像（如Google Maps航片/底图）与车载传感器数据融合，以弥补能见度差、遮挡和短时视野的不足；在nuScenes上扩展并验证，部分任务性能提升。


<details>
  <summary>Details</summary>
Motivation: 车载感知受限于行驶时视野、遮挡和极端天气/光照，难以像人类司机在低能见度下“回忆”道路结构。引入可离线获取且无需新增硬件的地理图像，赋予模型跨时空先验与环境“记忆”。

Method: 提出空间检索范式：从离线缓存（如Google Maps或已存数据集）检索与自车轨迹对齐的地理图像，作为额外输入，与现有AD任务管线进行多模态融合。构建扩展版nuScenes：通过API检索地理图像、配准到自车轨迹；在五类核心任务上建立基线：目标检测、在线建图、占用预测、端到端规划、生成式世界建模。

Result: 在多个任务上进行大量实验，显示加入地理图像这一扩展模态能提升若干任务的性能（并非所有任务均显著提升）。提供数据、代码与基准以促进研究。

Conclusion: 离线地理图像作为即插即用的先验通道，可在能见度受限或遮挡条件下补足车载传感器短板，验证了空间检索范式的潜力；开源资源将推动该方向进一步研究与标准化评测。

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [105] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DyToK是一种训练-free的动态视觉token压缩方法，利用VLLM内部注意力自动形成的“查询条件化关键帧先验”，按帧自适应保留比例，优先保留语义丰富帧、抑制冗余，在保持精度的同时显著加速长视频推理，并可与现有压缩方法即插即用叠加。


<details>
  <summary>Details</summary>
Motivation: 长视频使VLLM的视觉token序列很长，注意力计算呈二次增长，导致推理效率瓶颈。传统关键帧采样在特征编码前引入额外代价且二元帧选择过于粗糙，难以在效率与精度间兼顾。

Method: 提出DyToK：不训练、依赖VLLM内在注意力层提取随查询变化的关键帧先验；据此为每帧分配动态token保留率（而非二元保留/丢弃），在语义丰富帧保留更多token、在冗余帧压缩更多。该机制可与VisionZip、FastV等压缩技术组合。

Result: 在多种VLLM（如LLaVA-OneVision、Qwen2.5-VL）上取得SOTA效率-精度折衷；与VisionZip、FastV叠加可达约4.3倍推理加速，同时基本保持准确率。

Conclusion: 利用VLLM注意力中的查询条件化关键帧先验进行动态token压缩是一条高效且通用的路径，能在不训练、不修改模型的前提下大幅提升长视频推理效率，并与现有压缩方法互补、可即插即用。

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [106] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 提出ECOCSeg：用ECOC编码替代一热标签，进行位级别伪标签去噪与分类，缓解伪标签错误放大，在UDA与SSL分割任务上显著提升。


<details>
  <summary>Details</summary>
Motivation: 伪标签在语义分割（尤其UDA/SSL）中常含错误，一热编码训练会放大错误并导致不稳定与泛化差。需要一种能容错、可分解类别属性并对噪声鲁棒的监督方式。

Method: 引入基于错误纠正输出码（ECOC）的分割框架：1）为每类构建细粒度多比特编码，用ECOC分类器将类别解耦为属性位；2）设计位级别的伪标签去噪机制，能识别和校正不可靠比特，从而对未标注图像提供更稳健监督；可无缝集成到现有UDA/SSL方法与多种分割架构中。

Result: 在多个UDA与SSL基准与不同分割网络上，ECOCSeg持续带来显著改进（具体数值未给出），显示出更高质量伪标签与更强稳定性/泛化。

Conclusion: 用ECOC替代一热的位级表示与去噪策略可有效抑制伪标签噪声，提升语义分割在低标注场景中的鲁棒性与性能，并具备良好通用性与易集成性。

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [107] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: 提出一种轻量级“卷积混合器”用于遥感场景分类，在AID与EuroSAT上以较少参数实现较优准确-效率平衡。


<details>
  <summary>Details</summary>
Motivation: CNN/ViT在遥感场景分类上易受分辨率、视角、朝向与背景变化影响，泛化能力受限；需要在保持精度的同时大幅降低参数量与计算，提升跨场景鲁棒性与部署效率。

Method: 基于Convolutional Mixer范式，交替执行：1) 多尺度深度可分离卷积进行空间混合，聚合局部与上下文信息；2) 逐点卷积进行通道混合。整体网络轻量化，参数与算力开销低；在AID与EuroSAT上进行广泛实验评估。

Result: AID：OA 74.7%，AA 74.57%，Kappa 73.79；EuroSAT：OA 93.90%，AA 93.93%，Kappa 93.22。与主流CNN/Transformer相比在精度与效率间取得更佳权衡。

Conclusion: 所提轻量级卷积混合器能有效提取多尺度空间与通道特征，提升遥感场景分类的精度-效率权衡，具备良好泛化与部署潜力；代码将开源。

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [108] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

TL;DR: 提出分层的图像引导3D分割框架：先实例级，再部件级；用YOLO-World提示SAM在渲染视图上分割并回投到点云，多视角分割后用贝叶斯融合保证跨视角一致，显著提升在工业场景的mIoU与泛化、且省标注。


<details>
  <summary>Details</summary>
Motivation: 工业等复杂场景中物体密集、多尺度且强遮挡，导致3D几何边界模糊；端到端3D方法难兼顾粗细粒度，点云方法标注昂贵，图像引导法跨视角语义不一致。需要一种既高效又一致的3D分割方案。

Method: 分层两阶段：1) 实例级：渲染顶视图，用YOLO-World生成提示，引导SAM得到实例掩码，将掩码回投到3D点云获得实例分割；2) 部件级：对每个实例渲染多视图，重复YOLO-World+SAM得到部件掩码并回投；3) 用贝叶斯更新融合多视角结果，确保语义一致。

Result: 在真实工厂数据上对强遮挡与结构复杂场景表现稳健，各类别mIoU稳定且高；在公开数据集上同样取得良好表现，显示良好泛化。

Conclusion: 该分层图像引导3D分割框架在不依赖大量3D标注的前提下，兼顾实例与部件粒度，通过多视图与贝叶斯融合缓解语义不一致与遮挡问题，具备鲁棒性、标注效率与跨场景适应性。

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [109] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

TL;DR: 提出JoPano：在单一DiT骨干中统一文本到全景和视图到全景生成，利用Joint-Face Adapter在立方体贴图上联合建模各面并结合Poisson融合减缝，新增Seam-SSIM/Seam-Sobel评估缝一致性，指标上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法：1）以U-Net为主，限制大模型可迁移性与生成质量；2）将两项核心任务分离建模，造成冗余与低效；3）全景立方体面间缝隙不一致问题常见且缺少针对性度量。

Method: - 模型：采用DiT作为统一生成骨干；
- 适配器：提出Joint-Face Adapter，基于cubemap表示，联合建模各个面视图并把从自然图像学到的DiT能力迁移到全景域；
- 缝处理：使用Poisson Blending在立方体面边界做无缝融合；
- 评估：提出Seam-SSIM与Seam-Sobel量化缝一致性；
- 任务统一：条件切换机制在同一模型内统一text-to-panorama与view-to-panorama。

Result: 在两种任务上均能生成高质量全景图，在FID、CLIP-FID、IS、CLIP-Score等通用指标上达到SOTA，并在缝一致性上通过新指标展现改进。

Conclusion: DiT+Joint-Face Adapter可有效迁移自然图像生成能力到全景域；统一的条件切换机制减少建模冗余并提升效率；配合Poisson融合与新缝一致性指标，实现更高质量、缝更一致的全景生成。

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [110] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 提出BLDA，通过分析并校正各类别logits分布，缓解UDA语义分割中的类别不均衡与偏置；离线后处理对齐+在线分布估计与损失校正，并用累积分布作跨域结构先验；在多基线与两个基准上显著提升，尤其改善低预测类。


<details>
  <summary>Details</summary>
Motivation: 自训练在UDA语义分割有效，但受源/目标域的类别不均衡与分布偏移影响，模型倾向于过度预测多数类、忽视少数或难类，导致伪标签偏置和迁移性能受限。需要一种无需先验分布信息、可直接识别并纠正类别偏置的方法。

Method: 提出BLDA：1) 通过分析预测logits的分布来识别“过预测”和“欠预测”类别；2) 引入共享“锚”分布，采用后处理方式对齐不同类别的logits分布，实现跨类平衡；3) 在自训练过程中在线估计各类logits分布，将logits校正项纳入训练损失，促使网络生成无偏伪标签；4) 将由此得到的累积分布作为跨域共享的结构知识，连接源域与目标域，增强一致性。方法可无缝集成到现有UDA框架。

Result: 在两个标准UDA语义分割基准上做了大量实验，BLDA集成到多种现有方法中均带来稳定增益，尤其显著提升欠预测类别的性能。

Conclusion: 直接基于logits分布评估并矫正类别偏置是有效的；后处理对齐与在线校正结合、以及利用累积分布作为跨域结构先验，可在不依赖先验分布信息的前提下，持续改善UDA语义分割，特别是对少数/难类。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [111] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

TL;DR: 提出AIR-400婴儿无接触呼吸监测数据集与首个可复现实验基线，并开源数据与模型。


<details>
  <summary>Details</summary>
Motivation: 婴儿呼吸异常与神经发育障碍及SIDS相关，但现有成人算法/数据不适用于婴儿，公开婴儿呼吸视频数据稀缺、缺少可复现有效算法与基准。

Method: 收集并精标注共400段婴儿视频（新增275段、10名受试者）；设计婴儿特定ROI检测；结合光流增强的时空神经网络管线进行呼吸估计；在数据集上系统实验建立基准；开源数据、代码和模型。

Result: 构建AIR-400数据集；提出可复现的婴儿呼吸估计算法流水线；通过全面实验给出当前SOTA的可复现基准性能。

Conclusion: AIR-400与提出的方法为婴儿无接触呼吸估计提供了首个系统化、可复现的数据与基线，有望推动早期检测与干预研究，并促进该领域的公平比较与复现。

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [112] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

TL;DR: Saber 是一个零样本的参考到视频生成框架，只用视频-文本对训练，通过掩码训练与注意力设计实现对参考主体身份的一致保留，并用掩码增强减轻拷贝粘贴伪影；在OpenS2V-Eval上优于需R2V三元组训练的方法。


<details>
  <summary>Details</summary>
Motivation: 现有R2V方法依赖昂贵难扩展的参考图像-视频-文本三元组数据，限制了规模化与泛化能力；需要一种不依赖显式R2V数据、仍能保持主体身份一致性的生成方案。

Method: 仅使用视频-文本对进行训练：1) 掩码化训练策略，让模型学习在缺失参考信息时利用上下文与注意力聚合到隐式的主体表征；2) 定制的基于注意力的模型结构，学习身份一致且具参考感知的表示；3) 掩码增强（mask augmentation）以缓解R2V常见的拷贝-粘贴伪影；4) 可处理可变数量的参考。

Result: 在无需显式R2V数据的前提下，实现对参考主体的身份一致视频生成；在OpenS2V-Eval基准上表现优于使用R2V三元组训练的方法，并展现对参考数量变化的强泛化。

Conclusion: Saber打破对R2V三元组的依赖，通过掩码训练与注意力设计实现可扩展、零样本的参考到视频生成，在标准基准上取得更优效果并具备良好泛化性。

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [113] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 提出NeuroABench：首个针对神经外科解剖理解的多模态评测基准；包含9小时、89种手术、标注68个解剖结构。SOTA MLLM最佳仅40.87%准确；4名住院医平均46.5%，最佳56%。模型接近最低水平学生但显著落后于人类平均。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM外科视频研究多聚焦流程/步骤识别，忽视临床实践中至关重要的解剖理解能力；需要一个标准化基准来衡量模型在解剖层面的理解与识别。

Method: 构建NeuroABench：收集并标注9小时神经外科视频，覆盖89种手术；采用多模态（视频、可能含文本/帧级）标注流程与多轮审核，定义并评测68个临床解剖结构的识别任务；在10余种SOTA MLLM上进行评测，并从数据集中抽取子集对4名神经外科学员进行对照测试。

Result: 最佳MLLM在解剖识别任务上仅达40.87%准确；人类学员中最佳56%，最低28%，平均46.5%。模型表现与最低分学员相当，但低于人类平均水平。

Conclusion: NeuroABench为神经外科解剖理解提供了首个系统化、多模态评测框架，揭示当前MLLM在解剖识别上的显著不足；虽显示进展，但仍远未达到人类平均水平，需进一步方法改进与数据增强。

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [114] [Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology](https://arxiv.org/abs/2512.06949)
*Shravan Venkatraman,Muthu Subash Kavitha,Joe Dhanith P R,V Manikandarajan,Jia Wu*

Main category: cs.CV

TL;DR: 提出NTRM，将CNN与组织级图神经网络结合，通过建图与消息传递显式建模组织间关系，显著提升皮肤癌病理图像分割性能（Dice提升4.9%–31.25%）。


<details>
  <summary>Details</summary>
Motivation: 传统CNN主要依赖局部纹理、将组织视为独立区域，难以在边界密集或形态相似的组织中捕捉生物学上下文与组织间依赖，导致分割不连贯、语义不一致。

Method: 在CNN初始分割基础上，对预测组织区域构建图（节点为组织区域/类型，边表示空间与功能关系），通过图神经网络进行消息传递，融合上下文语义，再将图级推理结果投影回像素空间以细化分割；形成“CNN特征→区域建图→GNN关系推理→空间投影精炼”的闭环。

Result: 在Histopathology Non-Melanoma Skin Cancer Segmentation基准上，较现有SOTA方法Dice提升4.9%–31.25%，在边界密集区域获得更结构一致的预测。

Conclusion: 显式的组织关系建模能提供更具上下文与可解释性的组织学分割，优于仅依赖局部感受野的架构；NTRM为病理分割提供了可推广的关系推理范式。

Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.

</details>


### [115] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

TL;DR: 提出一种用于语义分割的自监督预训练：选择性掩码重建（SMIR），通过迭代选择重建误差最高的图像块来掩码，较随机掩码与ImageNet监督预训在多个数据集上显著提升下游分割精度，并对难类提升尤为明显。


<details>
  <summary>Details</summary>
Motivation: 现有掩码图像建模多用随机掩码，未充分利用模型在预训练过程中的“已学知识”，且在算力/模型容量受限场景下难以获得最优下游分割性能。作者希望通过更有针对性的掩码策略提高预训练质量，从而提升端到端语义分割效果，尤其是低资源和难分类别。

Method: 将图像重建式自监督预训练分解为多步迭代：先训练初始模型，计算各图像块的重建损失，再在后续迭代中优先掩码损失最高的块（选择性掩码），不断利用当前模型的知识聚焦困难区域，替代传统随机掩码。随后将预训练权重用于下游语义分割微调。

Result: 在Pascal VOC与Cityscapes，以及Nassar 2020与Sugarbeets 2016除草分割数据集上，选择性掩码优于随机掩码与ImageNet监督预训练：通用数据集mIoU提升约2.9%，杂草分割数据集提升约2.5%；对最低表现类别的精度提升尤为显著。同数据集上进行预训练与下游训练在低预算情形下效果最佳。

Conclusion: 选择性掩码图像重建是一种实用、高效的自监督预训练策略，能在有限模型容量与计算资源约束下显著提升语义分割，尤其改善弱势类别表现，适合端到端分割工作流与低算力应用场景。

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [116] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: TransCues 提出以边界与反射双线索增强的金字塔Transformer分割框架，显著提升透明/镜面目标分割的mIoU，在多数据集上大幅超过SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有分割方法难以区分透明材质与不透明物体，因其透明与强反射导致纹理/颜色线索不可靠。人类识别依赖边界与反射线索，但以往方法未同时充分建模两者，因此需设计能联合利用边界与反射特征的模型。

Method: 构建金字塔式Transformer编码-解码网络TransCues，包含两个关键模块：Boundary Feature Enhancement（增强轮廓/边缘与轮廓一致性）与 Reflection Feature Enhancement（显式捕获镜面反射与高光 cues）。二者以互补方式交互，提升透明/镜面目标的表征；在多尺度金字塔中融合以输出分割。

Result: 在多基准上显著超越SOTA：Trans10K-v2 +4.2% mIoU，MSD +5.6%，RGBD-Mirror +10.1%，TROSD +13.1%，Stanford2D3D +8.3%。泛化到玻璃与镜面以及通用分割数据集均有提升。

Conclusion: 同时建模边界和反射的双线索增强与金字塔Transformer框架互补有效，能稳定、显著提升透明/镜面目标分割性能，展现良好跨数据集泛化能力。

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [117] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

TL;DR: 论文关注超分辨率模型在视觉质量高但可能产生内容幻觉的问题，提出以“高层语义保真度”作为补充评估标准，构建带保真评分的数据集，分析现有指标相关性，利用大模型改进评估，并基于保真反馈微调SR模型以同时提升语义保真与感知质量。


<details>
  <summary>Details</summary>
Motivation: 当前SR模型生成能力强但会改变图像语义内容，现有低层次画质指标（如PSNR/SSIM/LPIPS）难以识别，缺乏衡量“高层语义一致性/保真度”的工具与基准，影响模型可靠性评估与优化。

Method: 1) 构建首个带高层保真评分的SR数据集：收集多种SOTA SR模型输出并进行标注；2) 评估各模型在语义保真方面的表现；3) 研究各类现有图像质量指标与保真度的相关性；4) 探索使用基础/大模型来更好地度量高层保真；5) 将保真反馈用于SR模型微调，验证其对语义保真与感知质量的双重提升。

Result: 得到一个含多模型输出与人工保真标注的数据集；发现多种SOTA SR在高层保真上存在差异与不足；现有低层指标与保真度相关性有限；基础模型在保真评估上表现更好；基于保真反馈的微调同时提高了语义保真和主观视觉质量。

Conclusion: 高层语义保真是评价生成式SR模型可靠性的关键补充维度；构建的数据集与评估方案揭示现有指标的不足，基础模型可更好地执行该评估；利用保真反馈可优化SR模型，兼顾保真与观感，具有评测与训练双重价值。

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [118] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

TL;DR: 提出DAUNet：在U-Net中引入Deformable V2卷积与无参注意力SimAM，实现更强空间自适应与上下文融合，以轻量参数在两数据集上取得SOTA分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法在几何形变、低对比与缺失上下文条件下鲁棒性不足；同时临床部署需要轻量、高效而不牺牲精度的模型。

Method: 设计轻量化U-Net变体DAUNet：在瓶颈处使用Deformable V2卷积以动态采样应对形状与位姿变化；在解码器与跳连路径中插入无参数注意力模块SimAM以突出显著特征与抑制噪声，实现语义与空间信息的高效融合；总体不增加可训练参数复杂度。

Result: 在FH-PS-AoP超声与FUMPE胸部CT数据集上，DAUNet在Dice、HD95与ASD指标优于现有SOTA，同时参数量更小；消融实验验证了可变形卷积与SimAM各自的增益。

Conclusion: DAUNet兼具精度、效率与鲁棒性，对低对比与缺失上下文具有稳健表现，适合实时与资源受限的临床场景部署。

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [119] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 提出一种对3D高斯点渲染(3DGS)的可变码率压缩方案，在给定上下限之间可连续调节速率，无需为不同码率重新训练，且保持接近原始渲染质量。方法计算开销小，适合实际沉浸式应用部署。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽可实时逼真渲染，但存在显著内存占用与训练成本高的问题；现有压缩方法多为固定码率，无法适应不同带宽与设备限制的动态需求。

Method: 设计一个在预设码率上下限之间可插值调节的压缩框架：通过轻量化表示与参数插值/控制机制，在无需重新训练的前提下对同一模型实现从低到高的连续码率调节，同时保持渲染质量。

Result: 实验显示该方法在广泛码率范围内都能实现高效高质的3DGS压缩；在不同运行点上保持渲染质量，且具备动态码率控制能力和较低计算开销。

Conclusion: 所提方案兼顾灵活性、效率与质量，为3DGS在沉浸式多媒体中的实际部署提供了可行路径；代码将在论文接收后开源。

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [120] [$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction](https://arxiv.org/abs/2512.07062)
*Changliang Xia,Chengyou Jia,Minnan Luo,Zhuohang Dang,Xin Shen,Bowen Ping*

Main category: cs.CV

TL;DR: 论文提出D^3-Predictor：将预训练扩散模型改造成无噪声、一步推理的确定性密集预测器，避免扩散采样中的随机噪声破坏几何细节；通过自监督聚合多时间步先验并结合任务监督，在多种密集任务上达SOTA或具竞争力，且训练数据与推理成本更低。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为强视觉先验的骨干已用于密集预测，但其核心随机噪声与需要确定性映射的几何/密集任务不匹配，会损伤细粒度空间线索并引导模型去拟合与时间步相关的噪声目标，破坏有意义的几何结构映射。

Method: 将预训练扩散网络重构为无随机性的确定性框架：把扩散网络视为“时间步相关的视觉专家”集合，通过自监督聚合这些异质先验，得到单一、干净且完整的几何先验；再用任务特定监督将该无噪先验适配到密集预测任务。推理为单步、无噪声。

Result: 在多种密集预测任务上取得与现有方法竞争或SOTA的表现；相比以往需要不到一半的训练数据；推理高效，仅需一步。

Conclusion: 扩散模型中的随机噪声会削弱密集预测的几何对齐。通过去噪声、确定性地聚合多时间步先验并结合任务监督的D^3-Predictor，可在更低数据与计算成本下实现强性能，验证了将扩散先验以确定性方式用于密集预测的有效性。

Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.

</details>


### [121] [Persistent Homology-Guided Frequency Filtering for Image Compression](https://arxiv.org/abs/2512.07065)
*Anil Chintapalli,Peter Tenholder,Henry Chen,Arjun Rao*

Main category: cs.CV

TL;DR: 用DFT结合持久同调，从频域中挑选与拓扑特征对应的频率，实现在噪声下仍保留关键信息的压缩与重构；压缩性能接近JPEG，并在与CNN结合时提升二分类鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 噪声数据集下的图像特征提取会影响模型可靠性。作者希望在压缩与特征保留之间取得平衡，利用拓扑信息指导从频域筛选有效成分，从而在噪声环境中提升识别与分类的稳健性。

Method: 先对图像做离散傅里叶变换，将频谱与持久同调分析相结合：计算不同频段对应的拓扑特征持久性，依据持久度选择/过滤频率分量（频率滤波），据此进行有指导的压缩与重构；再以六种指标评估压缩效果，并将该频率过滤作为特征增强模块，辅以CNN做二分类实验。

Result: 在压缩方面，所提方法在六项指标上达到与JPEG相当的压缩质量；在分类方面，作为CNN的增强/预处理步骤，可在二分类任务中较传统特征提取与压缩方法获得更好表现，且对噪声明显更稳健。

Conclusion: 持久同调引导的频率筛选能在噪声条件下保留与拓扑相关的关键信息，实现接近JPEG的压缩率与质量，同时提升下游二分类的性能与可靠性，表明该方法可作为鲁棒图像压缩与特征提取的实用方案。

Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.

</details>


### [122] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出一种面向伪装目标分割的“上下文化评估范式”Context-measure，通过像素级相关概率框架显式建模空间依赖与伪装程度，以更符合人类感知，并在三大数据集上比传统无上下文指标更可靠。


<details>
  <summary>Details</summary>
Motivation: 现有伪装场景评测多沿用通用/显著性目标指标，默认空间上下文独立，难以反映伪装的本质（与背景高度相关），导致评估与人类感知不一致、可靠性不足。

Method: 构建一个概率化、像素感知的相关性框架：显式引入空间依赖，逐像素量化伪装程度，并将这些上下文关联纳入评价，形成新的Context-measure指标/范式。

Result: 在三个具有挑战性的伪装目标分割数据集上，Context-measure较现有不考虑上下文的指标表现出更高的一致性与可靠性，能更准确地反映模型在伪装场景中的真实能力。代码开源（GitHub: pursuitxi/Context-measure）。

Conclusion: 考虑空间上下文与像素级伪装量化的评测更贴近人类感知，可作为涉及伪装模式的计算机视觉任务（农业、工业、医疗等）的基础评测基准。

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [123] [DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection](https://arxiv.org/abs/2512.07078)
*Bo Gao,Jingcheng Tong,Xingsheng Chen,Han Yu,Zichen Li*

Main category: cs.CV

TL;DR: 提出DFIR-DETR，通过动态稀疏注意力与频域处理，提升小目标检测与表面缺陷识别，达SOTA性能且模型轻量。


<details>
  <summary>Details</summary>
Motivation: 小目标/表面缺陷检测普遍存在特征稀弱、背景杂乱、尺度变化大；现有Transformer检测器面临三难：下采样致特征退化、空间卷积难以建模长程依赖、传统上采样导致特征“虚胖”与冗余。

Method: 构建DFIR-DETR，含三模块：1) DCFA：动态K稀疏注意力将复杂度由O(N^2)降至O(NK)，并用空间门控线性单元（SGLU）增强非线性表征；2) DFPN：幅度归一化上采样抑制特征膨胀，双路shuffle卷积在多尺度间保留空间细节；3) FIRC3：在频域中处理，实现高效全局感受野。整体进行动态特征聚合+频域建模。

Result: 在NEU-DET与VisDrone上mAP50分别达92.9%与51.6%，均为SOTA；模型仅11.7M参数、41.2 GFLOPs，兼顾精度与效率，并在跨场景小目标检测中表现稳健。

Conclusion: DFIR-DETR通过动态稀疏注意力、频域全局建模与无膨胀上采样，有效缓解特征退化与长程依赖建模难题，在资源受限场景下实现跨领域小目标检测的高准确与轻量化，具备良好泛化与实用性。

Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.

</details>


### [124] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

TL;DR: COREA 是首个在统一框架下同时学习可重光照的 3D 高斯与 SDF，直接在3D空间对齐与监督，实现更准确几何与稳定PBR分解。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS虽能扩展到网格重建与PBR，但主要从2D渲染监督学习几何，导致表面粗糙、BRDF与光照分解不可靠。需要一种能在3D中直接学习几何并提升重建与重光照质量的方法。

Method: 提出 coarse-to-fine 的双向 3D-to-3D 对齐：用深度进行粗对齐，再用深度梯度与法线精修，实现3D高斯与SDF之间的几何信号互相约束，得到高保真几何并支撑稳定的BRDF-光照分解；同时引入密度控制机制，稳定高斯增长，在几何细节与内存效率间平衡。

Result: 在标准基准上，统一框架下同时在新视角合成、网格重建与物理基渲染(PBR)任务上优于现有方法。

Conclusion: 直接在3D空间对齐并联合学习3D高斯与SDF，可显著提升几何精度与重光照质量；密度控制带来稳定训练与更高效率，使框架在多任务上取得SOTA表现。

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [125] [MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection](https://arxiv.org/abs/2512.07110)
*Liangwei Jiang,Jinluo Xie,Yecheng Huang,Hua Zhang,Hongyu Yang,Di Huang*

Main category: cs.CV

TL;DR: 提出多方向相似性网络（MSN），通过多方向层级特征与2D相似度解码器，实现更鲁棒的复制-粘贴篡改检测与定位，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有复制-粘贴篡改检测在两方面受限：1）表示不足，难以在多尺度、多旋转与复杂变换下稳健衡量相似性；2）定位不足，多依赖1D相似向量，未充分利用全图空间关系，导致细粒度篡改区域难以精准分割。深度生成与精细手工操作的兴起进一步加剧检测难度，需新的模型与数据基准。

Method: 提出两流多方向相似性网络（MSN）。表示端：用多方向CNN分层编码图像，两流取样补丁并在多尺度、多旋转增强下提取特征，提高跨变换的相似性度量。定位端：设计基于2D相似度矩阵的解码器，替代传统1D相似向量方案，显式建模整幅图像的空间相似关系，以更好地恢复篡改区域。并构建由多种深度网络生成的复制-粘贴伪造数据集，作为新基准。

Result: 在CASIA CMFD、CoMoFoD以及新提出的数据集上取得最新最优性能（SOTA），在检测与定位精度上显著优于现有方法。

Conclusion: 多方向层级表示与2D相似度解码的联合提升了复制-粘贴伪造的鲁棒检测与精确定位；新数据集为深度合成复制-粘贴检测提供了更具挑战的评测基准。

Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.

</details>


### [126] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

TL;DR: 提出一种在VTON生成过程中用能量函数约束注意力的策略，并引入新评估指标VTID，在VITON-HD与DressCode上超越SOTA，并提升下游CC-ReID性能。


<details>
  <summary>Details</summary>
Motivation: 现有VTON方法常出现生成服饰与目标服饰在图案、纹理、边界上的不一致，同时评估指标多侧重写实性而忽视与目标要素的一致性，缺乏能直接度量“对齐程度”的指标。

Method: 在扩散/生成流程中对注意力图施加能量函数约束，使注意力聚焦于服装感兴趣区域，促使生成结果更贴合目标服饰细节；同时提出虚拟试穿版的Inception距离VTID，综合衡量生成与目标在特征空间的对齐与逼真度。

Result: 在VITON-HD与DressCode上，相比SOTA分别提升LPIPS 1.4%、FID 2.3%、KID 12.3%、VTID 5.8%；将生成数据用于服装更换重识别（CC-ReID）下游任务，在LTCC、PRCC、VC-Clothes上Rank-1分别提高2.5%、1.1%、1.6%。代码公开于GitHub。

Conclusion: 通过对注意力的能量约束有效强化了目标服饰细节的保真与边界一致性；新指标VTID弥补了评估对齐性的缺口。方法在图像质量与下游识别上均显著优于现有方法，具备实用价值。

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [127] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

TL;DR: MulCLIP提出端到端多层次对齐框架，使CLIP类模型更好处理长文本描述，无需区域提议，兼顾全局与细粒度对齐，显著提升多项下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP在短标注上训练，对长而细致的描述理解不足。基于区域提议的方法能改进细粒度对齐但带来额外部署成本与复杂度，需要一种无需外部检测器、能高效处理长文本并加强图文细粒度对齐的方案。

Method: MulCLIP包含多层次对齐：1) 全局对齐：保持图像与摘要/长描述的全局对比学习，并通过扩展位置编码以支持更长文本序列；2) 细粒度对齐两策略：a) 基于局部校准特征的token重构对齐，增强词与图像patch的语义联系；b) 子句聚合patch对齐，自动为每个子句提取并聚合具上下文的图像patch，建立子句-区域级对齐。整体为端到端训练，无需区域提议器。

Result: 在多个多样化基准上持续优于现有方法；消融实验证明多尺度（全局-子句-词/patch）对齐是性能提升的关键，并在细粒度理解上优于依赖区域提议的方法。

Conclusion: MulCLIP以端到端多层次对齐有效弥补CLIP对长文本与细粒度语义的不足，降低部署成本，提升广泛下游任务表现，适合真实世界多样化应用场景。

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [128] [TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning](https://arxiv.org/abs/2512.07135)
*Zebin Xing,Pengxuan Yang,Linbo Wang,Yichen Zhang,Yiming Hu,Yupeng Zheng,Junli Wang,Yinfeng Gao,Guang Li,Kun Ma,Long Chen,Zhongpu Xia,Qichao Zhang,Hangjun Ye,Dongbin Zhao*

Main category: cs.CV

TL;DR: 提出一种结合MoE先验与RL精炼评估的端到端自动驾驶规划方法，在不同场景自适应选择轨迹先验，并用强化学习微调轨迹打分机制，融合多种感知骨干，ICCV navsim得分51.08，排名第三。


<details>
  <summary>Details</summary>
Motivation: 现有端到端规划虽可由传感器直接映射到轨迹，但依赖固定的轨迹先验且只用一阶段监督训练，导致：1）不同驾驶场景下先验不匹配；2）轨迹评估缺乏策略驱动的迭代优化，难以突破监督数据与标注偏差的限制。

Method: 1）采用Mixture-of-Experts（MoE）为不同驾驶情境提供可切换/组合的轨迹先验，由门控网络选择合适专家。2）在轨迹评估/打分模块上引入强化学习，使用环境反馈（如安全、进度、舒适度等）对评分机制进行微调，超越单纯的监督损失。3）融合多种感知骨干（不同视觉/多模态特征）以增强感知质量，从而提高规划输入特征的判别力。

Result: 在ICCV navsim基准上获得51.08分，排名第三，显示方法在标准化模拟任务上的有效性；相较于仅监督训练或固定先验的方法，规划性能提升（摘要未给出具体提升幅度）。

Conclusion: 针对端到端自动驾驶规划中先验不适配与评估机制僵化的问题，MoE实现场景自适应轨迹先验，RL对轨迹评分进行策略驱动的精炼，配合更强的感知表征，在公共基准上取得竞争性成绩。

Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.

</details>


### [129] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

TL;DR: 提出CUHK-X多模态数据集与基准，覆盖HAR/HAU/HARn三任务，并用LLM驱动的场景生成+人工校验获得一致性更强的文本描述，填补非RGB模态缺少大规模数据-描述资源的空缺。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs/LVLMs难以处理深度、IMU、毫米波等非RGB模态，主要因缺乏大规模数据-描述资源；现有HAR数据集多为粗粒度分类标签，难支撑细粒度的人类动作理解（HAU）与因果推理（HARn）。

Method: 构建CUHK-X：包含深度、IMU、毫米波等多模态，40类动作、30名被试、两处室内环境，共58,445样本；提出基于提示的场景创建流程：用LLM生成逻辑连贯的活动序列与文本描述，再经人工验证，以提升文本与时空逻辑一致性；提供三大基准与六个评测任务。

Result: 在CUHK-X上的实验：HAR平均准确率76.52%，HAU 40.76%，HARn 70.25%；证明任务难度梯度与基准可行性。

Conclusion: CUHK-X为多模态人类活动分析提供了数据与评测基座，促进面向非RGB模态的理解与推理研究，支持数据密集型方法的发展；代码与项目页已开放。

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [130] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

TL;DR: 论文提出Think-Reflect-Revise(TRR)三阶段框架，通过先思考、再反思、再修订的流程，让多模态大模型在生成前后进行安全自检，从而显著提升对越狱与不安全内容的抵抗力，并保持通用任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有“单次思考后直接作答”的安全推理方法虽能提升安全意识与可解释性，但仍易受上下文或视觉越狱攻击；其关键缺陷在于：单次推理可能忽视自身输出中的明显有害内容，错失自纠契机。作者希望利用首轮推理暴露的恶意信号，通过反思触发真正的自我修正，提升LVLM安全对齐能力。

Method: 提出TRR三阶段训练：1) 构建包含5,000条“思考-反思-修订”(think-reflect-revise)流程示例的ReSafe数据集；2) 用ReSafe对目标LVLM微调，初始化反思式行为；3) 通过强化学习进行“策略引导的反思”，强化在检测到潜在不安全内容时的自我修正策略。整个框架以策略指导反思，促使模型在第一轮推理后利用已暴露的恶意线索进行修订。

Result: 在安全意识基准与越狱攻击评测上显著提升：以Qwen2.5-VL-7B为例，总体安全响应率由42.8%提升至87.7%，同时在MMMU、MMStar等通用基准上保持稳定性能。

Conclusion: 单次推理的安全方案存在系统性盲点；通过TRR的“思考-反思-修订”并配合策略引导与强化学习，可有效利用首轮推理暴露的恶意信号实现自校正，大幅增强LVLM的安全性且不牺牲通用能力。

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [131] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

TL;DR: CHIMERA提出一种零样本扩散模型图像变形（morphing）框架，通过缓存反演引导的去噪流程，实现在两张差异较大的图像间产生平滑、语义一致的过渡；核心在自适应缓存注入（ACI）与语义锚提示（SAP），并提出同时评估全局与局部过渡的指标GLCS，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽强，但现有图像变形方法常出现突兀过渡与过饱和问题，根本原因是缺乏对结构与语义的自适应对齐机制；需要一种既能在深度与时间维度上对齐特征、又能跨大语义差异保持一致性的无训练（零样本）方案与客观评价指标。

Method: 将变形表述为“缓存反演（DDIM inversion）引导的去噪”过程：1）在两张输入图像上分别执行DDIM反演，缓存U-Net下/中/上各层特征；2）在前向生成中通过自适应缓存注入（ACI），按深度与时间自适应地重注入两侧特征，实现空间与语义对齐及自然融合；3）提出语义锚提示（SAP），借助视觉-语言模型生成两图共享的锚提示，作为语义桥梁引导去噪；4）提出全局-局部一致性分数（GLCS）作为客观评测指标，同时衡量两图的全局协调与局部过渡平滑度。

Result: 在大量实验与用户研究中，CHIMERA相较现有扩散式或其他变形方案，获得更平滑的过渡、更强的语义一致性与更自然的外观融合；在新提出的GLCS及主观评价上均达到SOTA。

Conclusion: 通过ACI与SAP，CHIMERA在无需微调的条件下实现对大语义/外观差异图像的平滑变形，并提供GLCS用于客观评估，整体确立了扩散模型图像变形的新SOTA。

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [132] [MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation](https://arxiv.org/abs/2512.07165)
*Muyu Xu,Fangneng Zhan,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: MuSASplat提出一种轻量pose-free前馈式稀视角3D高斯点渲染框架，在保持或接近SOTA画质的同时显著降低训练参数量与GPU资源开销。核心包括多尺度适配器用于高效微调ViT，以及特征融合聚合器取代记忆库，实现跨视角一致几何融合并降低内存与计算。


<details>
  <summary>Details</summary>
Motivation: 现有依赖预训练3D先验的前馈式方法多需要对大型ViT全量微调，训练参数与GPU开销巨大；同时常用的记忆库式多视角融合带来高内存与复杂度，并可能导致几何整合不一致。需要一种在稀视角下仍具高质量新视角合成、但训练高效且资源友好的方法。

Method: 1) 设计轻量级Multi-Scale Adapter嵌入ViT多层级，冻结大部分主干，仅微调少量适配器参数，高效注入任务特定能力；2) 提出Feature Fusion Aggregator跨输入视角有效融合特征，避免记忆库，确保几何一致性并降低内存/计算；3) 构建pose-free、前馈式3D Gaussian Splatting管线以从少量图像快速生成新视角渲染。

Result: 在多数据集上达到SOTA或接近SOTA的新视角渲染质量；相比现有方法显著减少训练参数量与GPU/内存占用，并降低训练复杂度与成本。

Conclusion: MuSASplat通过多尺度适配器与高效特征融合，在稀视角场景下以极低训练开销实现高质量3D Gaussian Splatting渲染，为pose-free前馈式方法提供了更实用的高效替代方案。

Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.

</details>


### [133] [When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing](https://arxiv.org/abs/2512.07166)
*Siyuan Xu,Yibing Liu,Peilin Chen,Yung-Hui Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出SPPE数据集与统一的隐私恢复方法，评估并重建在多模态大模型编辑流程中被“代理替换”的隐私信息，同时保持编辑真实性与可用性，在SPPE与InstructPix2Pix上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM隐私保护多聚焦于隐藏或替换隐私，却缺乏对“被保护/替代后隐私能否被真实、完整恢复”的评估与方法，忽视了在真实应用中既要保护隐私又要保留可编辑性的需求。

Method: 1) 构建SPPE数据集：涵盖多种隐私类别、用户指令与MLLM编辑结果，为每个样本提供受保护的代理数据及其多样编辑版本，用于直接评估恢复质量。2) 将隐私恢复表述为受引导的生成问题：以互补的多模态信号（如受保护代理、编辑后的内容、指令等）为条件，设计统一的重建框架，在保持MLLM生成编辑忠实度的同时最大化私密内容的可恢复性。

Result: 在SPPE与InstructPix2Pix上，方法对多样视觉内容与编辑任务具有良好泛化能力，能够在隐私保护强度与模型可用性之间取得较佳平衡，并实现高质量的隐私内容重建与编辑一致性。

Conclusion: 通过数据集与统一方法的提出，工作首次系统地评估并解决代理驱动的隐私保护下的恢复问题，证明可在不牺牲编辑保真度的前提下有效重建隐私信息，为MLLM安全与可用性的协同优化提供新路径。

Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.

</details>


### [134] [Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach](https://arxiv.org/abs/2512.07170)
*Jiayang Li,Chengjie Jiang,Junjun Jiang,Pengwei Liang,Jiayi Ma,Liqiang Nie*

Main category: cs.CV

TL;DR: DiTFuse提出一个将指令与多模态图像联合编码的Diffusion-Transformer，用无GT的多退化掩码建模训练，实现端到端、可控、具语义感知的图像融合，在红外-可见、多焦点、多曝光等基准上表现领先并具零样本泛化与多级用户控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法鲁棒性与可控性不足，通常为特定任务定制；难以融合用户意图，在低照、偏色、曝光不均等复杂场景下表现欠佳。缺乏真实融合GT与小规模数据集阻碍了端到端同时学习高层语义与细粒度对齐。

Method: 提出DiTFuse：单一模型内的指令驱动Diffusion-Transformer。将两幅图像与自然语言指令联合编码到共享潜空间，实现分层与细粒度的融合控制。训练采用多退化掩码图像建模，联合学习跨模态对齐、模态不变复原与任务感知特征选择；并构建多粒度指令数据集以增强交互式融合能力。

Result: 在IVIF（红外-可见）、MFF（多焦点）、MEF（多曝光）等公开基准上获得更优定量与定性结果，纹理更锐利、语义保留更好。支持文本控制的细化和下游任务，并在其他多图像融合场景上实现零样本泛化，如指令条件分割。

Conclusion: DiTFuse将多任务图像融合与文本可控性统一到一个端到端框架中，通过指令与多模态联合编码及无GT的自监督训练，提升鲁棒性、适应性与可控性，带来更优质量与更强泛化能力。

Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.

</details>


### [135] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

TL;DR: 提出TIDE：一个两阶段的水下图像复原框架，先对多种退化进行显式分解与专家化修复，再进行渐进式细化，适应局部空间变化，兼顾保真与感知质量，尤其提升颜色与对比度。


<details>
  <summary>Details</summary>
Motivation: 现有方法常对整幅图像施加统一复原，难以处理随空间与水体条件变化的多种共存退化（颜色偏移、散射雾化、细节缺失、噪声）。需要一种能显式建模并局部自适应处理多退化的方案。

Method: 提出TIDE两阶段逆退化估计框架：阶段一进行“专长先验分解”，将退化分为颜色失真、雾化、细节损失、噪声四因子，为每因子设计对应的复原专家，生成多条“专门化复原假设”；通过根据局部退化模式的自适应融合来平衡各因素。阶段二做渐进式细化，修正残余伪影。

Result: 在标准基准与高度浑浊水体数据上，参考型保真指标达到有竞争力水平；在无参考感知质量指标上优于SOTA，颜色校正与对比度增强显著提升。

Conclusion: 显式退化分解+专家化假设与自适应融合，结合后续细化，可更好处理空间变化的多退化水下图像，兼顾保真与感知质量，尤其在困难场景下表现突出。

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [136] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

TL;DR: 提出START框架，通过空间与文本双重学习（元素定位与图表到代码生成）提升MLLM的细粒度图表理解，并构建START数据集与CS-Bench评测，显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 图表不仅是自然图像，还包含结构化布局与底层数据表示。现有方法难以同时精准把握空间结构与数据细节，缺乏针对空间理解的系统评测。

Method: 1) 设计START：包括(i) 图表元素grounding以学习空间布局；(ii) 图表到代码生成以恢复数据细节。2) 数据：提出START-Dataset，先用MLLM把真实图表转译为可执行绘图代码以还原数据与保留视觉分布，再用LLM演化代码并标注元素位置，获得高质量空间标注。3) 评测：提出CS-Bench衡量图表空间理解能力。

Result: 在多种模型规模与多项基准上相较基础模型均有一致提升，并以明显优势超过已有SOTA。

Conclusion: 空间+文本联合学习能显著增强MLLM对图表的精细推理；START的数据与评测填补空间理解评估空白，方法通用且将开源。

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [137] [Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification](https://arxiv.org/abs/2512.07190)
*Pengfei Gu,Huimin Li,Haoteng Tang,Dongkuan,Xu,Erik Enriquez,DongChul Kim,Bin Fu,Danny Z. Chen*

Main category: cs.CV

TL;DR: 提出一个将多尺度、多滤波的拓扑特征（持久同调）与视觉骨干融合的医学影像分类框架，通过“葡萄园”算法整合多尺度立方持久图，借助跨注意力网络编码拓扑表示，并与CNN/Transformer特征融合，三数据集上优于强基线与SOTA，提升鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深网往往偏重像素强度，忽视或仅粗略利用解剖结构的拓扑信息；单参数持久同调难以捕获复杂多尺度与多滤波的结构信号。需要一种能系统提取并融合更丰富拓扑不变量以改进医学影像分类性能与可解释性的方案。

Method: 1) 对输入影像在多种分辨率/尺度上计算立方复形的持久同调，得到多尺度持久图(PD)；2) 设计“葡萄园”算法将多尺度PD稳定地整合为单一图，兼顾从全局解剖到局部异常的签名；3) 通过跨注意力神经网络直接处理整合后的PD以获得拓扑嵌入；4) 将拓扑嵌入与CNN或Transformer的特征图融合，形成端到端训练的拓扑引导分类框架。

Result: 在三个公开数据集上，相比强基线与最新方法取得稳定且显著的性能提升，表明多尺度、多滤波拓扑特征的引入能增强复杂解剖结构识别与模型稳健性。

Conclusion: 多尺度与多滤波的持久拓扑特征，通过葡萄园整合与跨注意力编码，并与视觉骨干融合，可在医学影像分类中带来显著、可解释的收益；拓扑先验为构建更鲁棒的临床AI提供了有效途径。

Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.

</details>


### [138] [RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction](https://arxiv.org/abs/2512.07191)
*Wenqi Zhao,Jiacheng Sang,Fenghua Cheng,Yonglu Shu,Dong Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出RefLSM：把Retinex反射率分解融入水平集，直接在反射率域分割，并结合线性结构先验与松弛二值水平集，通过ADMM高效求解，在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统水平集在强非均匀照明、噪声和模糊边界下依赖偏置场近似，易失效；医学图像常见强度不均、细节易丢失，需要对照明不变且结构友好的分割框架。

Method: 将图像分解为反射率与偏置场，直接对反射率进行分割；引入平滑反射率梯度的线性结构先验提供几何引导；采用凸松弛与符号投影的松弛二值水平集避免重初始化并稳定演化；整体以变分模型表述并用ADMM求解。

Result: 在多类医学影像数据集上，分割精度、鲁棒性与计算效率均优于最新水平集方法，实验广泛验证了优势。

Conclusion: Retinex式反射率分解结合结构先验与松弛二值水平集可有效应对非均匀照明与噪声，带来稳定高效且精确的医学图像分割。

Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.

</details>


### [139] [HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression](https://arxiv.org/abs/2512.07192)
*Niu Yi,Xu Tianyi,Ma Mingming,Wang Xinkun*

Main category: cs.CV

TL;DR: 提出HVQ-CGIC：在VQ生成式图像压缩中引入可控的超先验以自适应建模码本索引熵，实现更优RD和码率可控；在Kodak上以相同LPIPS节省约61.3%比特。


<details>
  <summary>Details</summary>
Motivation: 现有VQ类生成式压缩通常用静态全局分布估计VQ索引熵，无法适应图像内容，导致码率未被充分压缩且难以灵活控率。需要一种自适应、可控的熵模型来提升RD并实现精准码率控制。

Method: 构建HVQ-CGIC框架：从理论上推导在VQ索引熵模型中引入超先验（hyperprior）的合理性；设计新损失将率失真（RD）平衡与控制显式引入VQ生成式压缩；配合轻量级超先验估计网络，对VQ索引的条件分布进行自适应建模，从而实现可控码率与更优RD。

Result: 在Kodak数据集上，相比SOTA生成式压缩（Control-GIC、CDC、HiFiC），在达到相同LPIPS时，平均比特降低约61.3%；整体RD曲线显著优于对比方法。

Conclusion: 引入VQ超先验的HVQ-CGIC实现了对VQ索引熵的自适应与控率，显著提升RD表现；有望成为VQGAN系图像压缩中的基础组件，类似HyperPrior在传统神经压缩中的地位。

Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.

</details>


### [140] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文综述“高效高斯喷溅（Efficient Gaussian Splatting）”在3D/4D中的研究进展，围绕降低存储与计算开销、保持重建质量，系统归纳参数压缩与结构重构两大路线，并给出数据集、指标与基准对比，同时讨论局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽在实时高保真重建与新视角合成上表现突出，但需要数百万高斯，带来巨大的显存和渲染计算负担，在4D动态场景更为严峻。迫切需要系统化梳理如何在保证质量的前提下压缩与加速，以推动可扩展、紧凑、实时的场景表示。

Method: 提出首个统一综述框架：分别在3D与4D设定下，将现有工作划分为两大类——参数压缩（如量化、剪枝、共享/因子分解、哈希/低秩等）与结构式压缩（如层次/块化/稀疏结构、网格/体素/点云混合结构、时空因子化与轨迹建模等）；并汇总常用数据集、评测指标与代表性基准对比。

Result: 形成系统分类与方法学脉络，归纳各类技术在内存、速度与质量上的权衡，展示在多数据集与指标上的代表性对比，显示多种压缩方案可在显著降低资源占用的同时保持接近甚至匹配的重建质量，部分方法在4D场景也取得可观加速。

Conclusion: 高效GS是解决3D/4D重建可用性瓶颈的关键方向。尽管已有进展，但在大规模场景、动态时空一致性、通用性与鲁棒性、标准化评测和软硬件协同方面仍存挑战。未来应面向可扩展、紧凑、实时的统一框架，探索更强的压缩与加速策略并完善基准体系。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [141] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

TL;DR: 提出“故事性图像生成”任务，构建两阶段管线StorytellingPainter：LLM负责故事与推理链构思，T2I模型负责视觉合成；并配套三类评估器（语义复杂度、KNN多样性、故事-图像对齐）。同时提出轻量训练策略得到Mini-Storytellers以缩小开源与闭源LLM差距。实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 故事性图像可通过图中多层次、因果相连的视觉线索（CoRs）传达复杂叙事，但这类图像因语义复杂而稀缺、难以自动生成。需要探索如何利用生成式AI系统性创作并客观评估此类图像，同时提升开源LLM在故事生成中的能力。

Method: 提出两阶段StorytellingPainter：阶段一用LLM进行创意与推理驱动的故事设计，产出结构化叙事与视觉要素；阶段二用T2I模型据此合成图像。配套三种评估：1) 语义复杂度评估器衡量推理链与信息层级；2) 基于KNN的多样性评估器衡量图像多样性；3) 故事-图像对齐评估器衡量叙事与视觉一致性。并通过定制训练策略训练轻量Mini-Storytellers以提升开源LLM表现。

Result: 实验显示所提管线能生成具备丰富语义与推理链的故事性图像，三类评估器可有效度量复杂度、多样性与对齐度；Mini-Storytellers在故事生成上显著缩小与专有LLM的性能差距。

Conclusion: 生成故事性图像是可行的：结合LLM的创意推理与T2I合成可产生高质量叙事图像；提出的评估框架提供客观度量；轻量化训练使开源LLM成为实用替代。开源代码与模型促进后续研究与应用。

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [142] [Understanding Diffusion Models via Code Execution](https://arxiv.org/abs/2512.07201)
*Cheng Yu*

Main category: cs.CV

TL;DR: 论文提供一个约300行的最小可运行实现，用代码视角讲清扩散模型的关键流程（前向扩散、反向采样、噪声预测网络与训练循环），弥合公式与开源实现的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有教程多停留在方程推导，忽视代码层面的操作细节，导致研究者难以将理论与实际实现对齐。作者希望以最小实现帮助读者快速理解“代码如何对应理论”。

Method: 给出精简到约300行的DDPM/DDIM与Classifier-Free Guidance相关实现：包括前向加噪过程、反向去噪采样、噪声预测网络结构与训练流程；剔除工程性细节，以可读、可运行代码为核心阐释。

Result: 产出一套最小实现与预训练模型仓库，验证该最小实现能工作并复现扩散模型的关键行为；读者可直接运行并对照代码理解核心机制。

Conclusion: 该技术报告通过实现优先、代码驱动的讲解，清晰展示扩散模型从理论到实践的映射，为研究与教学提供轻量、可复用的参考实现与学习路径。

Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.

</details>


### [143] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

TL;DR: 提出MMRPT：一种将强化学习直接融入多模态预训练的“掩码—重建—奖励”框架，通过视觉依赖估计与掩码、视觉语义奖励引导的重建，促使模型从图像中进行视觉落地与推理而非模仿描述；在零样本评测与监督微调鲁棒性上均取得一致提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态预训练高度依赖图文对，存在描述偏置：模型更倾向于抓取表层语言线索而非真正基于图像理解与推理，导致泛化性与鲁棒性不足。需要一种训练信号能直接奖励“视觉落地与推理”，而非“字幕仿真”。

Method: 1) 估计句子级视觉依赖：利用对视觉token的注意力评估文本片段对视觉的依赖度；2) 掩码多模态数据：对高视觉依赖片段进行掩码；3) 重建任务：模型需通过图像信息推断被掩码的文本；4) 语义-视觉奖励：设计强化学习的奖励函数，鼓励重建内容与图像语义对齐、而非语言表面相似；5) 将RL直接融入大规模视觉-语言模型的预训练流程，实现“强化驱动的掩码式推理”。

Result: 在多种基准上的零样本表现获得一致提升；在后续监督微调场景下表现出显著更强的鲁棒性，表明预训练得到的能力更可迁移与稳健。

Conclusion: 以强化学习驱动的掩码式视觉推理作为预训练目标，能有效克服图文对描述偏置，提升MLLM在视觉落地、零样本泛化与微调鲁棒性上的表现；该范式为多模态预训练提供更可靠与可泛化的目标。

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [144] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

TL;DR: 提出AutoLugano：从基线FDG-PET/CT端到端自动完成淋巴瘤病灶分割、解剖定位与Lugano分期，外部验证显示区域受累检测与治疗分层均具高准确度。


<details>
  <summary>Details</summary>
Motivation: 临床Lugano分期依赖人工在PET/CT上逐步判读病灶、定位淋巴结区域并整合规则，耗时、主观且难以大规模应用；缺乏能直接从影像自动产出完整Lugano分期与治疗分层（局限/进展期）的端到端工具。

Method: 构建三阶段自动化流水线：1) 解剖先验的3D nnU-Net进行多通道病灶分割；2) 借助TotalSegmentator及确定性规则将病灶映射到21个预定义淋巴结区域；3) 将区域受累模式转换为Lugano分期与治疗组（局限vs进展）。在autoPET（n=1007）训练，在独立67例外部队列验证，评估区域受累检测（准确率、灵敏度、特异度、F1）和分期一致性/治疗分层表现。

Result: 外部验证中，区域受累检测准确率88.31%，灵敏度74.47%，特异度94.21%，F1 80.80%，优于基线模型；治疗分层（局限vs进展）准确率85.07%，特异度90.48%，灵敏度82.61%。

Conclusion: AutoLugano为首个可从单次基线FDG-PET/CT自动生成完整Lugano分期的全自动端到端系统，显示出在初始分期、治疗分层与临床决策支持中的应用潜力。

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [145] [Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds](https://arxiv.org/abs/2512.07211)
*Frederik Hagelskjær,Dimitrios Arapis,Steffen Madsen,Thorbjørn Mosekjær Iversen*

Main category: cs.CV

TL;DR: 提出一种仅用无色3D数据（点云/深度）估计物体位姿不确定性的深度学习方法，避免RGB依赖，在实际抓取场景验证，对对称物体给出分布而非单一位姿。


<details>
  <summary>Details</summary>
Motivation: 单一位姿估计无法反映由视觉歧义与对称性导致的不确定性，容易产生不可靠的机器人行为；现有分布式位姿估计多依赖RGB，在工业环境常缺失色彩信息，亟需仅用几何数据的方案。

Method: 设计基于神经网络的模型，从3D无色传感数据学习输出位姿分布（考虑反射/旋转对称），由训练策略与表示方式对不确定性进行建模；框架可扩展到SE(3)全空间分布。

Result: 在真实料箱抓取实验中，对不同几何歧义对象成功估计位姿不确定性分布，展示对工业无RGB场景的有效性。

Conclusion: 无需RGB即可进行位姿不确定性分布估计，适用于工业应用；当前支持反射与旋转对称，方法具备扩展到完整SE(3)分布的潜力；代码已开源。

Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io

</details>


### [146] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 本文比较CLIP与DINOv2在手-物体抓取场景中的6D位姿估计表现：CLIP语义更强、语义一致性更好；DINOv2几何密集特征更优、几何精度更高，二者具有互补性。


<details>
  <summary>Details</summary>
Motivation: VFMs与VLMs为视觉任务提供强语义与几何表示，但在机器人抓取中的3D位姿估计究竟应选语义更强的CLIP还是几何更强的DINOv2缺乏直观对比与选择依据。

Method: 在手-物体抓取场景中，基于CLIP和DINOv2分别构建/调用对应特征提取与匹配管线，进行6D物体位姿估计；在基准数据集上做广泛实验，比较语义一致性与几何精度等指标，并进行可视化对照分析。

Result: CLIP在语言对齐带来的语义理解、目标辨识与语义一致性上更优；DINOv2输出的稠密几何特征带来更强的几何匹配与位姿估计精度，整体性能具竞争力。两者在不同维度上互补。

Conclusion: 针对机器人操作与抓取任务：若需语义驱动的目标选择与稳健语义一致性，优选CLIP；若追求更高几何精度与稠密匹配，优选DINOv2。可考虑融合两者以兼得语义与几何优势。

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [147] [Towards Robust Protective Perturbation against DeepFake Face Swapping](https://arxiv.org/abs/2512.07228)
*Hengyang Yao,Lin Li,Ke Sun,Jianing Qiu,Huiping Chen*

Main category: cs.CV

TL;DR: 论文针对DeepFake人脸置换防护中对抗扰动易被压缩/缩放等破坏的问题，提出将“变换分布”从固定的均匀采样(EOT)升级为可学习分布(EOLT)：用策略网络通过强化学习学习关键变换并生成实例自适应扰动，从而显著提升在多类变换下的鲁棒性（平均+26%，难类最高+30%）。


<details>
  <summary>Details</summary>
Motivation: 现有隐形防护扰动在常见图像变换（压缩、缩放、裁剪、滤波等）下极易失效；EOT通常用均匀采样变换做训练，但作者系统性评估30种变换发现鲁棒性高度依赖采样选择，均匀EOT在资源有限时本质上次优，需能重点关注“瓶颈”变换并自适应到具体样本。

Method: 提出EOLT：将变换分布作为可学习组件。用策略网络（强化学习）学习对不同变换的权重/选择，优先采样关键且致命的变换；同时根据实例特征自适应生成扰动。训练过程中通过防护成功率/鲁棒性作为回报，显式建模“防御瓶颈”，并兼顾跨变换与跨场景可迁移性。

Result: 在六大类、共30种变换的基准上，相比SOTA防护方法取得显著提升：平均鲁棒性提高约26%，在最具挑战的变换类别上最高提升约30%，表明学到的变换分布与实例自适应扰动均带来稳定增益。

Conclusion: 固定、均匀的EOT采样对鲁棒防护并非最优。将变换分布学习化，并结合强化学习的策略网络与实例自适应扰动，可更有效地针对关键变换，显著提升在多种图像变换下的防护稳健性与可迁移性。

Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.

</details>


### [148] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

TL;DR: 提出ReLKD，一个端到端的GCD框架，通过隐式类间关系提升未知类识别：目标粒度表示学习+粗粒度层级关系建模+蒸馏迁移。多数据集验证，少标注场景尤佳。


<details>
  <summary>Details</summary>
Motivation: GCD需要在只有已知类标签的条件下，对混含已知/新类的无标注数据进行分类。以往方法将各类独立处理，忽视类间关系，但在现实中直接获取显式类间关系困难，因而需要一种可从数据中挖掘隐式关系并利用其提升新类识别的方案。

Method: 设计ReLKD，包含三模块：1) 目标粒度模块（target-grained）：学习判别性特征与细粒度类别表示；2) 粗粒度模块（coarse-grained）：捕获层级化类间关系与群组结构；3) 知识蒸馏模块：将粗粒度模块的关系知识迁移到目标粒度模块，细化其表示学习。整体端到端训练，在无显式关系标注下挖掘并利用隐式类间关系。

Result: 在四个基准数据集上进行大量实验，ReLKD显著优于现有方法，尤其在标注数据有限的设定中表现更强。

Conclusion: 通过联合细/粗粒度建模并以蒸馏桥接，ReLKD有效利用隐式类间关系，提升GCD中未知类的分类能力，具有在弱监督与低标注场景中的实用价值。

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [149] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

TL;DR: STRinGS 提出一种面向文本的选择性细化框架，专为3DGS在含文字场景中的重建弱点而设，通过先精修文本区域、再与非文本区域融合，显著提升文字清晰度与可读性，并给出OCR字符错误率作为评价指标与一个新数据集STRinGS-360。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS虽具高保真，但对细小文字等高频细节重建不佳，轻微误差会导致严重语义损失；缺乏专门的可读性度量与基准数据集来评估文本重建效果。

Method: 提出STRinGS：区分文本与非文本区域，先对文本区域进行选择性精细化（text-aware selective refinement），再与非文本区域合并进行全局优化；引入OCR字符错误率（CER）作为文本可读性指标；并构建包含多样文本场景的数据集STRinGS-360用于评测。

Result: 在7K迭代下，相比3DGS在文本区域的可读性上获得63.6%的相对提升，能够在复杂配置中重建出锐利、可读的文字。

Conclusion: 面向文本的分区优化能有效弥补3DGS在文字重建上的短板；配套的CER指标与STRinGS-360数据集为文本可读性的衡量与研究提供了标准化基础，推动文本感知的3D重建进展。

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [150] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

TL;DR: 提出“Dropout Prompt Learning”，在视觉-语言模型中对文本与图像的token进行自适应dropout，并配以残差熵正则以兼顾对齐与多样性，在低样本、长尾与OOD等任务上显著提升泛化，base-to-novel实验相较KgCoOp和PromptSRC分别提升5.10%与2.13%。


<details>
  <summary>Details</summary>
Motivation: 传统dropout提升神经网络泛化能力，但主要针对神经元，对多模态VLM中token级表征的鲁棒性与对齐不足；现有提示学习在低样本、长尾与分布外场景易过拟合或对齐受损，需要一种既保留通用语义对齐又能引入表征多样性的正则化方法。

Method: 在文本与视觉分支对token施加自适应dropout：通过度量token重要性时同时考虑模态内上下文关系与跨模态对齐强度，为每个token设定不同的drop概率；为抵消dropout带来的潜在语义漂移并促进知识迁移，引入残差熵正则化（对残差分布或表示不确定性施加熵约束），在保持语义对齐的同时鼓励表示多样化。

Result: 在15个基准上验证：在低样本学习、长尾分类、分布外泛化等挑战场景中取得一致增益；在base-to-novel泛化上，相比正则化方法KgCoOp提升5.10%，相比PromptSRC提升2.13%。

Conclusion: 基于token级、跨模态感知的dropout提示学习结合残差熵正则，可在不破坏语义对齐的前提下提升VLM鲁棒性与泛化，适用于低样本、长尾与OOD等现实困难设置。

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [151] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

TL;DR: 提出UCPE：一种统一相机位置编码，将相机完整几何（6DoF位姿、内参与畸变）以相对光线编码+绝对方位组件注入视频扩散Transformer，实现更强相机可控视频生成，参数开销<1%，SOTA质量与可控性。


<details>
  <summary>Details</summary>
Motivation: Transformer在3D、视频与世界模型中普及，但现有相机编码多基于简化的小孔成像，难以覆盖真实相机多样的内参与镜头畸变，限制跨设备泛化与几何一致性；需要统一、几何一致、可控性强的相机表示。

Method: 1) Relative Ray Encoding：以像素相对光线为核心，统一表达完整相机信息（6DoF位姿、内参、镜头畸变），确保几何一致性与跨相机泛化；2) Absolute Orientation Encoding：识别俯仰(pitch)与横滚(roll)为有效绝对方位信号，实现初始相机朝向的可控；3) UCPE=相对光线编码+绝对朝向编码，通过轻量空间注意力adapter注入到预训练视频扩散Transformer，新增参数<1%；4) 构建覆盖多类镜头与运动的大规模视频数据集，用于系统训练与评测。

Result: 在相机可控文本到视频生成上达到SOTA的相机可控性与视觉保真度；在多样相机内参与畸变场景下表现稳健；小参数开销实现显著增益。

Conclusion: UCPE作为统一相机表示可无缝集成到Transformer，提供几何一致与强可控的相机条件，对未来多视图、视频与3D任务具通用性与推广潜力；代码将开源。

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [152] [Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture](https://arxiv.org/abs/2512.07241)
*Md. Srabon Chowdhury,Syeda Fahmida Tanzim,Sheekar Banerjee,Ishtiak Al Mamoon,AKM Muzahidul Islam*

Main category: cs.CV

TL;DR: 提出一种结合SqueezeNet v1与EfficientNet-B0并融合手工放射组学特征（HOG、LBP、Gabor、小波）的混合模型，用于Nickparvar脑肿瘤MRI四分类，在TTA下测试准确率达99.08%，参数<2.1M、算力<1.2 GFLOPs，兼顾效率与准确性，具备临床辅助潜力。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤诊断依赖MRI，但人工分割/判读耗时、主观且易产生观测者间差异；现有深度模型在算力与泛化间权衡不足。作者希望在保持高诊断性能的同时显著降低模型规模与计算成本，并通过融合纹理特征增强对肿瘤微观结构的敏感性。

Method: 构建混合深度学习框架：以轻量的SqueezeNet v1与高性能的EfficientNet-B0为骨干，融合手工放射组学特征（HOG、LBP、Gabor、小波），并采用测试时增强（TTA）。仅在公开Nickparvar脑肿瘤MRI数据集（7023张CE T1轴位切片，四类：神经胶质瘤、脑膜瘤、垂体瘤、无肿瘤）上训练与测试。模型参数<2.1M，计算量<1.2 GFLOPs。

Result: 在测试集上获得98.93%准确率，使用TTA达到99.08%，显示出良好的泛化能力与高诊断性能；手工纹理特征提升了对组织纹理的敏感度，EfficientNet-B0捕获层级表征。

Conclusion: 该混合网络在计算效率与诊断准确性间实现良好折中，接近临床可靠性，适合用作自动化MRI肿瘤分类的临床决策支持工具，但验证仍局限于单一公开数据集。

Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.

</details>


### [153] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: 提出TEXTER：先定位并增强“决策关键特征”，再与CLIP语言空间对齐，从而生成更忠于分类器因果依据的文本解释；并用稀疏自编码器提升可解释性与稀疏性。实验显示优于现有零样本方法。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型生成的描述更偏向“看到了什么”，而非“为何做出该分类”，缺乏与特定分类器决策因果一致的文本解释。零样本方法通常对齐全局图像特征，忽视了驱动预测的关键内部表示，导致解释不忠实。

Method: 1) 决策神经元识别：在分类器内部定位对当前预测贡献最大的神经元。2) 特征强调：放大这些神经元编码的特征，得到“决策关键特征”表征。3) 跨模态映射：将强调后的特征映射到CLIP的文本-图像共同空间，检索/生成与之对齐的文本解释，反映模型推理。4) 稀疏自编码器：用于得到稀疏、可分解的概念单元，尤其适配Transformer结构，进一步提升解释可读性与可定位性。

Result: 在多项数据集与模型上，TEXTER生成的解释在忠实度与可解释性指标上显著优于现有零样本对齐方法；对Transformer架构的解释质量因引入稀疏自编码器而进一步提升。

Conclusion: 隔离并强调决策关键内部特征，再与语言空间对齐，可显著提高文本解释对分类器真实决策依据的忠实度与可读性；TEXTER在实验中验证有效，代码将开源。

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [154] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: 提出AdLift：将2D对扩散编辑的对抗扰动“提升”到3D高斯点云以保护3DGS资产免受指令驱动编辑；通过“截断-投影-拟合”循环优化，兼顾跨视角泛化与不可感知性，实验证明对2D与3D编辑均有效。


<details>
  <summary>Details</summary>
Motivation: 3D高斯渲染（3DGS）已能结合扩散模型进行指令驱动编辑，极大便利创作但引入未授权篡改风险。现有2D对抗保护难以直接用于3D：需在任意视角下仍能防护，且要在保护强度与视觉不可见性间平衡。

Method: 提出AdLift：以严格受限的2D图像级对抗扰动为目标，通过“提升”到3D的方式在3DGS中注入少量“保护高斯”。核心是“Lifted PGD”优化：1) 在由编辑模型反传的图像梯度上进行梯度截断；2) 在图像空间做投影以严格约束扰动幅度；3) 将得到的图像扰动通过图像到高斯的拟合反传到保护高斯参数；4) 交替执行截断与拟合，跨训练视角联合优化以实现多视角一致与新视角泛化。

Result: 在定性与定量实验上，AdLift能有效阻止最先进的指令驱动2D图像编辑与3DGS编辑对目标资产的篡改，同时保持可视化不可感知或极低可感知度，且对新颖视角具有稳健保护。

Conclusion: 通过将受限2D对抗扰动系统性地提升至3D高斯表示，并以Lifted PGD交替优化，AdLift实现了跨视角泛化、强保护与不可感知性的统一，为3DGS资产提供首个有效编辑防护方案。

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [155] [See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement](https://arxiv.org/abs/2512.07251)
*Junqi Liu,Zejun Wu,Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Ibrahim E. Hamamci,Sezgin Er,Tianyu Lin,Yi Luo,Szymon Płotka,Bjoern Menze,Daguang Xu,Kai Ding,Kang Wang,Yang Yang,Yucheng Tang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: 提出SMILE：一种解剖感知的扩散式增强模型，仅在临床相关部位进行增强，避免过度编辑；在6个外部数据集上显著提升图像质量与临床可用性，并提升非增强CT的肿瘤检测F1达10%。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像增强模型常“过度编辑”——不理解解剖结构与对比剂动力学，导致器官变形、伪影与小病灶漏检，影响临床决策。因此需要既懂解剖、又能尊重多期动态对比信息的增强方法。

Method: 构建SMILE解剖感知扩散模型，核心包含：1) 结构感知监督：遵循真实器官边界与对比摄取模式；2) 无配准学习：直接利用未对齐的多期CT数据训练；3) 统一推理：对各对比期实现快速一致的增强，仅增强临床相关区域，其余保持不变。

Result: 在6个外部数据集上，相比现有方法，SSIM提升14.2%、PSNR提升20.6%、FID提升50%，并在临床有用性上更优；在非增强CT的癌症检测任务中将F1最高提高约10个百分点。

Conclusion: SMILE能以解剖与对比动力学为先导，实现选择性、可信赖的图像增强，兼顾图像质量与诊断意义，并能作为下游检测的有效前处理。

Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.

</details>


### [156] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

TL;DR: 提出一种面向退化感知的内镜视频增强框架，通过跨帧传播退化表示，实现实时高质量增强，兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 内镜手术高度依赖视频质量，但受不均匀光照、组织散射、遮挡、运动模糊等退化影响，关键解剖细节被掩盖，影响安全与操作。现有深度方法多计算开销大，难以实时临床应用，需要一种既高效又鲁棒的增强方案。

Method: 1) 用对比学习从单帧中提取“退化表示”。2) 设计跨帧传播与融合机制，用退化表示调制图像特征，指导一个轻量单帧增强子网络。3) 训练时加入退化-复原之间的循环一致性约束，提高鲁棒性与泛化。整体实现实时处理。

Result: 在多组实验中，相比多种SOTA方法，本框架在速度与画质上取得更优折中，能实时增强并稳定恢复细节。

Conclusion: 退化感知建模与隐式退化表示的跨帧传播是实现实时内镜视频增强的有效途径，具有临床应用潜力。

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [157] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

TL;DR: 提出一种基于摄影测量与深度学习的低成本管线，利用RGB与立体相机深度生成关键基础设施（如水/能源厂）对象关系图，效果接近真实且可定制、透明，适用于高风险决策。


<details>
  <summary>Details</summary>
Motivation: 激光扫描获取3D点云成本高、操作复杂，不利于在关键基础设施仿真与数字孪生中普及。因此需要更经济、易用、仍具可靠性的建模方法来支持韧性与连续性评估。

Method: 采用摄影测量获取影像与立体相机深度；利用深度学习做对象检测与实例分割；通过用户定义的启发式/规则推断对象间关系；最终生成描述系统组件与连接的图结构。

Result: 在两个水力系统的案例中，生成的关系图与地面真实标注接近，展示了方法的有效性；同时显示出可根据具体应用灵活调整规则。

Conclusion: 该方法以较低成本、较高透明度与可解释性构建接近真实的系统关系图，适合关键基础设施中的高风险决策支持，并可根据需求定制。

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [158] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

TL;DR: 提出RVLF：一个将手语特化LVLM与强化学习结合的三阶段框架，在无gloss的SLT中显著提升BLEU-4（在CSL-Daily、PHOENIX-2014T、How2Sign、OpenASL上分别+5.1、+1.11、+1.4、+1.61）。


<details>
  <summary>Details</summary>
Motivation: 现有无gloss手语翻译存在两大瓶颈：1) 视觉表征不足，难以捕获细粒度动作与语义线索；2) 依赖LLM的句级语义对齐不足，导致译文语义偏差与不完整。作者希望通过更强的视觉-语言表征与基于奖励的句级优化，提高翻译质量与语义一致性。

Method: 三阶段RVLF：1) 视觉语义表征学习：融合骨架动作线索与DINOv2提取的高语义视觉特征，构建面向手语的LVLM；2) 指令微调（SFT）：获得稳健的SLT-SFT基线；3) GRPO强化优化：以结合BLEU（忠实度）与ROUGE（句子完整性）的奖励函数，对SLT-SFT进行GRPO微调，得到SLT-GRPO，无需外部大规模手语预训练数据。

Result: 在无gloss设定下，RVLF在多个数据集上显著优于现有方法：BLEU-4在CSL-Daily +5.1、PHOENIX-2014T +1.11、How2Sign +1.4、OpenASL +1.61；消融与大量实验显示GRPO优化能提升翻译质量与语义一致性。

Conclusion: 将手语特化的LVLM与GRPO强化学习相结合，能有效缓解表征不足与句级语义错配问题，在多数据集取得稳定提升；据称为首次在SLT中引入GRPO，方法简单、无需外部大规模预训练且可扩展。

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [159] [Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation](https://arxiv.org/abs/2512.07275)
*Siyu Wang,Hua Wang,Huiyu Li,Fan Zhang*

Main category: cs.CV

TL;DR: 提出一种用于皮肤病变分割的多尺度残差编码器-解码器网络，结合MRCF跨尺度特征融合、CMAM交互注意力和外部注意力桥EAB，显著提升低对比、形状不规则病灶的分割精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理皮肤病变形状不规则、边界模糊和低对比度导致的特征缺失与误分割问题；传统U-Net的跳跃连接在上采样中信息损失明显。

Method: 设计多尺度残差编码器-解码器框架；加入MRCF模块实现多分辨率多通道的跨尺度特征融合；提出CMAM跨混合注意力模块，重定义注意力范围并在多上下文间动态加权；在编码器与解码器间引入EAB外部注意力桥，缓解U-Net跳跃连接的信息丢失并强化解码器利用全局信息。

Result: 在多个皮肤病变分割数据集上，较现有Transformer与CNN方法取得显著更优的分割性能与稳定性，达到更高的准确率与鲁棒性（文中未给具体数值）。

Conclusion: 多尺度残差结构结合跨尺度融合、跨混合注意力与外部注意力桥有效提升了皮肤病变分割的边界识别与细节捕获能力，适用于复杂形状和低对比场景，具有良好的泛化性与实用价值。

Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.

</details>


### [160] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

TL;DR: Geo3DVQA提出一个仅用RGB遥感影像进行高程感知与3D地理空间推理的VLM评测基准，含11万问答、16任务、三层复杂度；现有通用VLM在RGB到3D推理上表现较差，领域微调显著提升，指向更可及、可扩展的3D分析新挑战。


<details>
  <summary>Details</summary>
Motivation: 现实城市与环境应用需要3D要素（高程、天空视域、地表覆盖）参与的推理，但现有方法依赖昂贵传感器（LiDAR/多光谱）与规则系统，难以整合多3D线索、应对多样查询并提供可解释推断，且限制全球可获得性。

Method: 构建Geo3DVQA基准：基于RGB遥感图像，设计覆盖16类任务、三种复杂度（单特征推断、多特征推理、应用级空间分析）的约11万问答数据集；对十个SOTA VLM进行系统评测，比较通用模型与领域微调模型在高度感知3D推理上的表现。

Result: 通用VLM在RGB→3D推理任务上准确率低：GPT-4o 28.6%、Gemini-2.5-Flash 33.0%；对Qwen2.5-VL-7B进行领域微调后达49.6%（提升24.8个百分点），显示领域适配的有效性。

Conclusion: RGB-only条件下进行可扩展、可获取的3D地理空间推理仍具挑战；Geo3DVQA为评估与推动VLM在高程感知、多要素融合和应用级分析方面提供了新基准，揭示现有模型局限并鼓励通过领域适配提升性能；数据与代码将发布以促进研究。

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [161] [Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts](https://arxiv.org/abs/2512.07302)
*Mingning Guo,Mengwei Wu,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 提出AerialVP：面向无人机(UAV)图像感知的“任务提示增强”智能体框架，通过从图像中主动提取多维辅助信息来改写/强化原始文本任务提示，在AerialSense基准（包含推理、问答、指代定位）上显著提升多种VLM的稳健性与性能。


<details>
  <summary>Details</summary>
Motivation: VLM通常依赖用户给定的文本任务提示与视觉-文本语义对齐来理解图像。但UAV图像存在目标易混淆、尺度变化大、背景复杂等问题；当提示过于简单而图像内容复杂时，难以形成有效对齐，导致模型难以聚焦于与任务相关的信息，性能受限。

Method: 提出AerialVP三阶段增强流程：1) 解析任务提示以识别任务类型与增强需求；2) 从工具库中选择合适工具以提取多维辅助信息（如场景属性、尺度、候选目标、空间关系等）；3) 基于分析结果与工具输出生成增强后的任务提示，再喂给VLM执行感知任务。并构建AerialSense基准，覆盖Aerial Visual Reasoning、VQA与Visual Grounding，多分辨率、光照、城市/自然场景。

Result: 在开源与商用VLM上，AerialVP带来稳定且显著的性能提升，提示增强能更好引导模型关注任务相关区域与信息，跨多任务、多场景均有效。

Conclusion: 通过将“主动提取辅助信息→任务提示增强”引入UAV图像感知，AerialVP缓解了VLM在复杂航拍场景中的对齐与聚焦难题；AerialSense为评价泛化与鲁棒性提供统一基准。方法通用，可作为VLM在航拍领域的即插即用前端。

Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.

</details>


### [162] [Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset](https://arxiv.org/abs/2512.07305)
*Tobias Abraham Haider*

Main category: cs.CV

TL;DR: 复现实验用公开资源与不同数据集（900张、90物种）重做Inception‑ResNet‑v2相机陷阱图像分类；总体准确率62%（接近原文71%），但宏F1仅0.28，类别间差异大。结果表明：预训练CNN可作基线，但需物种特定适配/迁移学习以提升一致性与质量。


<details>
  <summary>Details</summary>
Motivation: 检验Carl等人方法的可复现性与跨数据集泛化能力，评估直接使用ImageNet预训练模型在野生动物相机陷阱任务上的实际效果与局限。

Method: 从零重实现原流程，使用公开可得资源与新的相机陷阱数据集（900张、90物种），进行最小预处理，直接利用预训练Inception‑ResNet‑v2进行多类分类，并报告总体准确率与宏F1以衡量整体与类别均衡性能。

Result: 总体准确率62%，与原文71%相近；宏F1为0.28，显示类别不均衡与标签与ImageNet类目不对齐导致的显著性能差异。

Conclusion: 预训练CNN能提供可用的基线，但若要获得稳定且高质量的物种识别，需要进行物种特定的适配或迁移学习（如微调、数据扩充、更贴合标签映射）。

Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.

</details>


### [163] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

TL;DR: 提出ContextAnyone：一种从文本和单张参考图生成角色一致视频的上下文感知扩散框架，通过联合重建参考图与合成视频、强调注意力模块、双重引导损失与Gap-RoPE位置编码，显著提升跨帧身份与上下文一致性与画质。


<details>
  <summary>Details</summary>
Motivation: T2V已很强，但跨镜头保持角色一致仍难，现有个性化多局限于脸部，忽视发型、服装、体型等上下文要素，导致身份漂移与不连贯。需要一种能充分利用单参考图的全身与上下文一致性方案。

Method: 以DiT为骨干：1) 训练时联合重建参考图并生成视频帧，使模型充分“看懂”并绑定参考信息；2) Emphasize-Attention模块在跨注意中选择性强化参考感知特征，抑制身份漂移；3) 双重引导损失=扩散目标+参考重建目标，提高外观逼真度；4) Gap-RoPE位置编码区分参考token与视频token，稳定时序建模与对齐。输入为文本+单参考图，输出多场景、多动作一致角色视频。

Result: 在参考到视频任务上，相较现有方法在身份一致性与视觉质量上均更优，能在多动作、多场景下生成保持发型、服装、体型等上下文的连贯视频。

Conclusion: ContextAnyone通过上下文感知注意力、联合重建训练、双重损失与分隔式位置编码，实现单参考图驱动的角色一致文本转视频生成，显著缓解身份漂移并提升时序稳定与画质。

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [164] [The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers](https://arxiv.org/abs/2512.07331)
*Kanishk Awadhiya*

Main category: cs.CV

TL;DR: 本文研究ViT在层间表示维度上的“U型”变化，指出中层信息压缩（“归纳瓶颈”）是由数据分布引发的自适应现象，而非架构限制。通过测量DINO训练的ViT在不同数据集上的有效编码维度（EED），发现瓶颈深度与任务所需语义抽象程度强相关：纹理主导数据保持高秩，目标中心数据在中层抑制高频信息以突出语义特征。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT理论上不具CNN的层级归纳偏置，能在各层维持高维表示，但实证上常出现中层熵/维度降低的“U型”现象。需要澄清该现象是架构固有还是由数据/任务驱动，并理解其与语义抽象需求的关系。

Method: 使用自监督DINO训练的ViT，对不同组合复杂度的数据集（UC Merced、Tiny ImageNet、CIFAR-100）逐层计算与分析有效编码维度（EED），比较各数据集的层间EED曲线形态，量化“瓶颈”深度与任务语义抽象需求之间的相关性。

Result: 发现所谓“归纳瓶颈”在不同数据集上程度不同：纹理重的数据集EED在全层保持较高；以对象为中心的数据集在中层显著降低EED，随后在后层重新扩大，形成U型。瓶颈深度与所需语义抽象程度呈强相关。

Conclusion: ViT的中层信息压缩并非架构缺陷，而是对数据语义结构的自适应结果：当任务强调语义对象时，模型在中层抑制高频/细节以提取语义；当任务更依赖纹理时，模型维持高秩表示。该发现解释了ViT的表示动态，并提示数据属性可调控模型层级表征形态。

Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.

</details>


### [165] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Luís Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: 提出Aerial-D：一个大规模航拍场景的指代表达分割数据集（37,288张图，1,522,523条表达，覆盖259,709个目标与21类），并在RSRefSeg上联合训练，兼顾现代与历史影像，在常见退化条件下保持强鲁棒性且在当代基准上具竞争力。


<details>
  <summary>Details</summary>
Motivation: 指代表达分割在航拍影像中更具挑战：分辨率跨数据集差异大、色彩不一致、目标极小且高密度、部分遮挡，以及需要兼顾现代与历史（单色/褐色/颗粒）影像。现有数据与方法难以覆盖这些特点与退化环境，缺少统一、规模化、跨时代的语义与实例层面的文本到分割训练资源。

Method: 构建Aerial-D数据集：完全自动管线，先用规则系统生成指代表达，再用大语言模型增强以提升语言多样性与视觉细节关注；并对每个场景施加滤波模拟历史成像条件（单色、褐色、颗粒）。选用RSRefSeg框架，在Aerial-D与既有航拍数据联合训练，实现从文本到实例与语义分割的一体化。

Result: 数据集规模：37,288图像、1,522,523条表达，覆盖259,709个标注目标，21个类别（车辆、基础设施、地表覆盖等），目标粒度涵盖个体、成组与区域。联合训练在当代基准上达到有竞争力的指标，同时在单色、褐色、颗粒退化下保持较高精度与鲁棒性，适用于现代与历史航拍影像。

Conclusion: Aerial-D填补了航拍指代表达分割的大规模、多样化、跨时代数据空白；结合LLM增强与退化模拟，提升模型在多种成像条件下的泛化与稳健性。公开数据、模型与全流程代码，促进文本到分割在航拍场景的研究与应用。

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [166] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

TL;DR: 论文提出TD-Attn框架，通过3D感知注意力和层级注意力调制，缓解T2I扩散模型的先验视角偏置，显著提升多视角一致性并支持可控3D编辑。


<details>
  <summary>Details</summary>
Motivation: 基于T2I扩散蒸馏的3D任务无需大量3D数据，但存在先验视角偏置：主语词在跨注意力中倾向于触发固定视角特征，导致不同视角外观冲突。作者希望揭示偏置成因并设计通用插件提升多视角一致性。

Method: 1) 理论分析：数学上解释T2I模型中先验视角偏置的根源，并观察UNet不同层在跨注意力对视角的响应不同。2) 3D-AAG：为主语词构建跨视角一致的3D注意力高斯分布，引导关注区域在空间上保持一致，弥补单张2D CA地图的空间不足。3) HAM：通过语义引导树(SGT)指导语义响应分析器(SRP)定位并调制对视角高度敏感的CA层，增强其注意力并反哺3D高斯构建；同时支持语义级干预实现精确3D编辑。

Result: 在多种3D任务上进行大量实验，TD-Attn作为通用插件显著提升多视角一致性，并展示可控、精细的3D编辑能力。

Conclusion: TD-Attn系统性缓解T2I扩散模型的先验视角偏置，通过3D注意力与层级调制协同提升跨视角一致性，具有通用性与可控编辑价值。

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [167] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: 提出MICo-150K数据集、De&Re子集与MICo-Bench评测以及新指标Weighted-Ref-VIEScore，并基于此微调与评估模型（含Qwen-MICo），显著提升多图合成能力。


<details>
  <summary>Details</summary>
Motivation: 多参考图像的可控生成（多图合成MICo）缺乏高质量训练数据与系统评测，导致模型难以在身份一致与跨任务场景中稳定合成。

Method: 1) 将MICo系统化划分为7类代表性任务；2) 大规模收集高质源图与构建多样化合成提示；3) 利用强大专有模型生成平衡的合成图，并进行人审过滤与精炼，形成MICo-150K；4) 构建De&Re子集：将1.1万真实复杂图像分解为组件并重构，覆盖真实与合成组合；5) 搭建MICo-Bench（每任务100例+300个De&Re难例）与新评价指标Weighted-Ref-VIEScore；6) 在MICo-150K上微调多种模型并评测。

Result: MICo-150K显著提升无MICo能力模型，并进一步增强已有相关能力的模型；基线Qwen-MICo（由Qwen-Image-Edit微调）在三图合成上可匹敌Qwen-Image-2509，且突破后者固定多图数量限制，支持任意多图输入。

Conclusion: 提供数据集、基准与基线模型，系统推进多图合成研究；所提资源与指标为后续方法开发与公平评测奠定基础。

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [168] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

TL;DR: 提出DeepAgent多智能体框架，用独立视觉与音频-文字-OCR链路并以随机森林进行层级融合，提高深伪检测鲁棒性与跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 单一模型整合多模态在实际中易受模态错配、噪声与针对性操控攻击，缺乏对不同操纵类型与跨数据集的稳健性，因此需要一种能分别建模并协作决策的多智能体框架。

Method: 设计两个互补智能体并进行层级融合：Agent-1 用轻量化AlexNet式CNN对视频帧学习视觉深伪痕迹；Agent-2 融合声学特征、Whisper转写文本与EasyOCR的逐帧读字序列以捕捉音画不一致；最终用随机森林作为元分类器，融合两代理的判决边界。进行组件级与融合级评测，并做跨数据集验证。

Result: Agent-1 在Celeb-DF+FakeAVCeleb上测试准确率94.35%；在FakeAVCeleb上，Agent-2准确率93.69%，元分类器81.56%；在DeepFakeTIMIT跨数据集验证中，元分类器达到97.49%准确率，显示较强鲁棒与泛化。

Conclusion: 多智能体分而治之并以层级融合能缓解单模态弱点，增强对不同操纵类型与跨数据集的稳健检测效果，证实了该协作式多模态策略在深伪检测中的有效性。

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [169] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出一种结构感知的特征校正方法，用图像自身的实例先验（基于颜色/纹理的区域邻接图）来细化CLIP特征，提升OVSS在局部区域的区分性与一致性，从而降低噪声并在多基准上取得强性能。


<details>
  <summary>Details</summary>
Motivation: OVSS依赖CLIP等VLM，但CLIP因在图文对上训练，偏重全局语义对齐，难以准确关联细粒度局部区域与文本，导致局部预测噪声大、不一致。此分散偏置由对比学习范式带来，仅靠CLIP特征难以缓解，因此需要引入图像内在结构先验来矫正。

Method: 从图像低层特征（颜色、纹理）构建区域邻接图RAG，捕获局部结构关系；利用该图对CLIP特征进行结构感知的特征整流/强化，提升局部判别性与区域级一致性，从而抑制噪声并改进开集语义分割。

Result: 在多个开放词汇分割基准上取得强性能，相比直接用CLIP的OVSS方法显著降低噪声、提升区域一致性。

Conclusion: 引入基于图像自身的实例先验与RAG的结构感知特征校正，可有效弥补CLIP全局对齐带来的局部不足，提升OVSS细粒度分割质量与整体性能。

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [170] [Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency](https://arxiv.org/abs/2512.07379)
*Mahila Moghadami,Mohammad Ali Keyvanrad,Melika Sabaghian*

Main category: cs.CV

TL;DR: 论文提出在大尺度航拍图像中检测小目标的方法：以SW-YOLO为基线，优化滑窗裁剪策略并修改网络（颈部特征增强、骨干CBAM注意力、新检测头），在VisDrone2019上将mAP@0.5 从35.5提升到61.2，优于SAHI与CZDet。


<details>
  <summary>Details</summary>
Motivation: 航拍图像中小目标尺寸小、密集、尺度变化大，直接全图检测容易漏检。现有方法多依赖裁剪和网络改造，但仍存在精度与速度的权衡、特征保真不足以及对超大图处理效率的局限。需要一种既高效又高精度的通用框架。

Method: 以SW-YOLO为基线：1) 改进滑窗裁剪（窗口尺寸与重叠率优化）以兼顾召回与冗余；2) 网络结构改造：在骨干引入CBAM以保持通道与空间信息；在颈部加入更强的特征提取/融合模块提升高分辨率特征；3) 设计新的检测头，强化小目标分支；4) 与SAHI和CZDet对比评估。

Result: 在VisDrone2019数据集上，相比YOLOv5L基线mAP@0.5=35.5，所提模型达61.2；超过CZDet的58.36，并宣称对SAHI也有显著精度提升。

Conclusion: 优化的滑窗策略结合注意力与颈部增强及新检测头，可显著提升航拍小目标检测精度，在大型图像处理上优于现有裁剪框架（如SAHI、CZDet）。

Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.

</details>


### [171] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

TL;DR: 提出Tessellation GS：把3D高斯点云改为锚定网格面的结构化2D高斯，并结合自适应细分与基础模型先验，实现单相机动态场景重建，显著提升外推与稀视角泛化；LPIPS降29.1%，Chamfer降49.2%。


<details>
  <summary>Details</summary>
Motivation: 传统3D Gaussian Splatting各向异性易过拟合，视角外推差，尤其在稀疏视角与动态场景中表现不佳；单静态相机的动态重建更是对优化式方法极具挑战。需要一种既能结构化约束又能利用先验的表示。

Method: 提出Tessellation GS：以网格面为锚，将场景表示为局部受限的2D高斯；在网格面上用分层神经特征预测高斯属性；通过细节感知损失驱动的自适应面细分进行高斯细化；利用重建基础模型提供的先验来初始化高斯形变，从而稳健处理单静态相机下的通用动态对象。

Result: 在外观与几何重建任务上，相比SOTA显著提升：LPIPS降低29.1%，Chamfer距离降低49.2%；在单移动或静态相机下的动态场景重建中取得更稳健与清晰的结果。

Conclusion: 结构化的2D高斯与网格面层级特征、适应性细分及基础模型先验结合，增强了动态和稀视角情况下的泛化与外推能力，实现单相机动态重建的突破，并在感知与几何指标上超越当前方法。

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [172] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 提出LogicCBM：在概念瓶颈模型上加入可微命题逻辑模块，用逻辑算子组合概念，提升表达力、可解释性与干预效果，并在基准与合成数据上更准确。


<details>
  <summary>Details</summary>
Motivation: 传统CBM多把最终预测视为概念的线性加权和，线性表达受限，难以捕捉概念间的布尔关系与组合结构；需要一种既可端到端训练又能利用逻辑结构的方式来增强表达和可解释性。

Method: 在CBM中新增一个可微“逻辑模块”，用与、或、非等逻辑运算（及其可微近似）对学习到的概念进行组合，从而替代或扩展线性读出层；模块端到端训练，可表达概念间的命题逻辑关系，并支持干预分析。

Result: 在多项公开基准与合成数据集上，所提模型相比线性CBM获得更高准确率；在干预实验中效果更好；同时保持高可解释性。

Conclusion: 以命题逻辑增强CBM能超越简单线性组合，捕捉概念间关系，兼顾端到端可训练、准确率提升、可解释性与干预能力。

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [173] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

TL;DR: 提出首个从追踪无人机视角进行多模态反制无人机（UAV-Anti-UAV）跟踪任务与大规模数据集，并给出基于Mamba的时空语义融合基线MambaSTS；实验显示当前方法在该场景仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有反制无人机研究多依赖地面固定相机的RGB/IR/RGB-IR视频，忽视了由另一架移动无人机追踪目标无人机的真实应用场景；双运动带来的强烈时空扰动使传统跟踪器难以稳健工作，亟需新的任务定义、数据和方法推动研究。

Method: 1) 构建UAV-Anti-UAV多模态跟踪任务与百万级标注数据集（1810段视频，框、文本描述、15类属性）。2) 提出MambaSTS：用Transformer学习空间特征、用Mamba学习全局语义；基于状态空间模型的长序列建模，通过时间token传播建立视频级长时上下文，实现时空语义一体化学习。3) 系统评测50个主流深度跟踪算法。

Result: 在新数据集上验证MambaSTS有效；广泛评测显示现有先进跟踪器在该双动态、多模态场景下性能不足，仍有明显改进空间。

Conclusion: UAV-Anti-UAV任务更贴近实战但更具挑战；所构建数据集与MambaSTS基线为社区提供基准与方法起点，期望推动面向双动态无人机对抗场景的多模态长时序跟踪研究。

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [174] [GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring](https://arxiv.org/abs/2512.07391)
*Đorđe Nedeljković*

Main category: cs.CV

TL;DR: 提出超轻量CNN——GlimmerNet，通过分离感受野多样性与特征重组，在极低参数/算力下保持全局感知，AIDERv2上以31K参数与更低FLOPs达SOTA（加权F1=0.966）。


<details>
  <summary>Details</summary>
Motivation: 现有增强CNN全局上下文的做法多借助自注意力（Transformer），但计算与参数开销大，不适合边缘/无人机等资源受限场景。需要一种无需昂贵注意力模块、同时具备多尺度与全局感知能力的极简架构。

Method: 提出GlimmerNet：1) GDBlocks（分组膨胀深度可分离卷积），将通道分组并赋予不同膨胀率，零额外参数引入多尺度感受野；2) Aggregator模块，用分组逐点卷积高效重组跨组特征，降低参数；3) 整体遵循“感受野多样性”和“特征重组”解耦设计，实现超轻量网络。

Result: 在AIDERv2（UAV应急场景分类/检测数据集）上，以仅31K参数、比最新基线少29% FLOPs，取得加权F1=0.966的新SOTA；展现优异的精度-效率折中，适配实时无人机平台。

Conclusion: 无需自注意力亦可获得强全局感知。GlimmerNet通过GDBlocks与Aggregator在极低复杂度下实现多尺度建模与高效特征融合，刷新AIDERv2性能并推动资源受限视觉任务的SOTA。

Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.

</details>


### [175] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

TL;DR: 提出ROHIT任务：沿手部交互时间线传播物体位姿，利用受约束优化与传播（COP）框架在稳定抓握片段中提升刚体重建，凭2D投影误差评估，无需3D真值，在HOT3D与EPIC-Kitchens上显著提高重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有视频中刚体物体在手部交互过程中的重建困难，因物体被抓握和使用时位姿快速变化、遮挡严重且缺少3D真值标注。需要一种能利用交互先验、在无3D真值条件下评估与提升重建的方法。

Method: 1) 定义手部交互时间线（HIT）：静止→接触持握→稳定抓握使用→释放静止。2) 在HIT上对物体位姿施加阶段性约束。3) 提出COP框架：通过受约束优化与沿时间线的位姿传播，在稳定抓握段保持手-物体接触一致性与刚体约束，从而增强多帧信息融合。4) 使用2D投影误差作为度量以避开3D真值需求；在两个自采与野外数据集上标注稳定抓握片段以训练/评估。

Result: 在HOT3D与EPIC-Kitchens上，稳定抓握重建提升6.2–11.3%；对完整HIT的重建在使用约束传播时提升最高24.5%。共整理/标注：HOT3D 1.2K稳定抓握片段；EPIC-Kitchens 2.4K片段，390物体实例，9类，141环境。

Conclusion: 将手-物体交互时间结构化，并以受约束位姿传播进行重建，可在无3D真值下稳健提升视频中刚体物体的重建质量，尤其在稳定抓握场景中效果显著，为后续手物体交互理解与重建研究提供可标注、可评估的任务与基准。

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [176] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: InterAgent提出一个端到端、文本驱动、物理可控的人形多智能体控制框架，通过自回归扩散Transformer与多流设计，实现多模态解耦与协同，结合交互图外感知与稀疏边注意力，生成符合语义且物理可行的多体行为，并在实验中超越强基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦单智能体控制，忽略多智能体间物理上可行的交互协同；需要一个能从文本指令直接生成多主体、人形体间复杂社交与协作行为的通用方法。

Method: 1) 架构：自回归扩散Transformer，加入多流模块将本体感知（proprioception）、外感知（exteroception）与动作（action）解耦以减少跨模态干扰并提升协同。
2) 外感知表示：提出“交互图”来显式编码关节到关节的精细空间依赖。
3) 注意力机制：基于稀疏边的注意力，动态剪枝冗余连接，突出关键跨主体空间关系，增强交互建模鲁棒性。
4) 文本驱动：从文本提示生成物理一致、语义对齐的多智能体人形动作序列。

Result: 在广泛实验中，InterAgent稳定优于多种强基线，达到SOTA；可从文本生成连贯、物理可信、语义忠实的多智能体人形行为。

Conclusion: 将扩散与自回归Transformer结合的多流解耦框架，加上交互图外感知与稀疏边注意力，有效解决了多智能体人形控制中的跨模态干扰和关键交互捕捉问题，实现了从文本到物理可行多主体行为的SOTA性能；代码与数据将开源以促进后续研究。

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [177] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

TL;DR: 本文从数据出发挖掘个体间“相互影响”的移动事件与模式，用于理解和改进人群/交通动力学模拟，并在车辆与行人两案例中验证。


<details>
  <summary>Details</summary>
Motivation: 物理层面的人类动力学建模需要刻画个体间的相互影响。现有方法多依赖先验的行为模型，可能偏离真实行为。为改进如人群仿真、应急管理等应用，需要从真实数据中直接发现个体交互的证据与规律。

Method: 采取数据挖掘视角：1) 在轨迹数据中检测可能表征个体互相作用的“移动事件”；2) 在这些事件之上挖掘复杂、持续的模式及其时间演化配置；3) 对方法进行性能评估与参数敏感性分析；4) 在两类真实数据（车辆与行人）上实例化。

Result: 方法能够从真实车辆与行人数据中自动发现持久且演化的交互模式；实验展示了算法的有效性、运行性能与对参数变化的稳健性，并给出具有代表性的结果解释。

Conclusion: 从数据驱动的交互模式挖掘能揭示个体移动相互作用的机制，可为现有的人群与交通模拟模型提供校准与改进依据。

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [178] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

TL;DR: 论文指出：深度学习驱动的WSI颜色/风格归一化在真实临床数据上易产生“幻觉”内容（非原组织的伪结构），常被传统指标忽视；作者提出一种新图像比较度量，用于自动检测归一化输出中的幻觉，并在重训多种主流方法后系统评估，发现显著不一致与失效，呼吁更稳健、可解释的归一化与更严格验证。


<details>
  <summary>Details</summary>
Motivation: 现有WSI归一化方法多学习训练分布的平均外观，可能抹平关键诊断特征，更严重的是引入肉眼难察的“幻觉”而危害下游分析；公开数据集上的良好表现掩盖了在临床真实分布中的风险，缺乏专门检测幻觉的评价工具。

Method: 1) 在真实临床数据上重训练多种被广泛引用的归一化方法；2) 提出一种新的图像比较度量，可自动识别归一化后与原始组织不一致的“幻觉”区域；3) 用该度量系统评测各方法，并与常规指标对比。

Result: 在真实临床数据上，多个主流归一化模型频繁产生视觉难以察觉的幻觉与不一致；所提出的度量能检测到这些失败，而传统指标无法充分反映；不同方法在临床数据上表现不稳定、结果不一致。

Conclusion: WSI归一化存在被低估的幻觉风险；需要更稳健、可解释的方法及更严格、包含幻觉检测的验证协议，方可安全用于临床部署。

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [179] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

TL;DR: VideoCoF 提出“框链式”视频编辑：先预测编辑区域的推理标记，再生成视频，从而在无需掩码的情况下实现精确的指令到区域对齐与细粒度编辑，并通过RoPE对齐实现运动一致与长度外推，在仅50k数据上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前方法要么依赖掩码等先验（精度高但不统一），要么统一的时序ICL模型无显式空间线索（定位不准）。需要一种既不依赖用户掩码又能精确空间对齐、能统一处理多任务的视频编辑框架。

Method: 提出VideoCoF（Chain-of-Frames）：在扩散视频生成前强制模型先输出“推理标记/编辑区域潜变量”，实现“看-推理-再编辑”的流程；利用这些推理标记进行显式区域对齐和细粒度编辑；并设计RoPE对齐策略，借助推理标记保证运动对齐并支持超出训练时长的长度外推。使用约5万视频对进行训练与评测。

Result: 在自建基准VideoCoF-Bench上达到SOTA；在无需用户掩码的前提下，实现更强的指令到区域映射与精确定位，并支持更长视频的稳定编辑。

Conclusion: 显式“推理先行”的Chain-of-Frames范式能在低数据量下提升视频编辑的准确性与一致性，兼顾统一性与精度；RoPE对齐进一步带来时序运动一致和长度外推能力。

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [180] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: S2VC是一种将条件编码与单步扩散生成器结合的视频编解码器，在低码率下以极低采样成本实现高真实感重建，并较现有感知方法平均节省约52.73%码率。


<details>
  <summary>Details</summary>
Motivation: 传统与神经视频编解码在率失真方面表现优异，但在低码率下的主观质量仍差：基于感知/对抗损失的方法受限于生成能力，易产生伪影；引入预训练扩散模型的方法虽提升质量却采样开销巨大。需要一种既能提升感知质量又具有高效率的解决方案。

Method: 提出S2VC：基于单步扩散的视频编解码框架。1) 条件编码：以缓冲特征为条件驱动生成器，避免高开销多步采样。2) 语义引导（Contextual Semantic Guidance）：自缓冲特征提取逐帧自适应、细粒度语义，替代文本条件以提升真实感与效率。3) 时序一致性引导（Temporal Consistency Guidance）：在扩散U-Net中加入约束，强化跨帧时序连贯与稳定生成。

Result: 在广泛实验中，S2VC在感知质量上达SOTA，相较既有感知优化方法实现平均52.73%的码率节省，且保持低采样复杂度。

Conclusion: 单步扩散与条件编码的结合能在低码率下实现高感知质量、低复杂度的视频压缩；语义与时序引导对提升真实感和稳定性至关重要。

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [181] [Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior](https://arxiv.org/abs/2512.07498)
*Chih-Chung Hsu,Shao-Ning Chen,Chia-Ming Lee,Yi-Fang Wang,Yi-Shiuan Chou*

Main category: cs.CV

TL;DR: 提出LR-GCN，通过无序时序图嵌入与谱先验，鲁棒检测噪声/打乱/缺失的人脸序列中的DeepFake，并在多数据集上达SOTA与强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中人脸序列常因压缩、遮挡、检测失败与对抗扰动而不稳定，导致现有依赖时序连续与干净人脸的检测器失效，亟需能在无序、缺失与噪声条件下依然可靠的DeepFake检测方法。

Method: 1) 构建无序时序图嵌入（OF-TGE）：依据帧级CNN特征的语义相似度自适应建立稀疏图，不依赖严格的时间顺序；2) 双重稀疏：同时在图结构与节点特征上施加稀疏约束，抑制无效/误检人脸的影响；3) 引入图拉普拉斯谱先验：作为谱域高通，突出结构异常与伪造痕迹；随后使用GCN低通聚合，将其组合为任务驱动的带通机制，保留操控线索、抑制背景与随机噪声；4) 仅用干净数据训练，但在噪声/打乱输入上保持鲁棒。

Result: 在FF++、Celeb-DFv2与DFDC上获得SOTA表现；在缺帧、遮挡及对抗扰动等全局与局部破坏条件下显著提升鲁棒性。

Conclusion: 通过OF-TGE与谱域拉普拉斯先验结合GCN聚合，实现对无序和受扰人脸序列的鲁棒DeepFake检测，既能抑制噪声与无效样本，又能突出伪造线索，效果与稳健性均优于现有方法。

Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.

</details>


### [182] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

TL;DR: 提出MultiMotion：在DiT中实现多对象视频运动迁移的统一框架，核心是基于SAM2掩码的AMF进行对象级运动解耦与控制，并配合高阶RectPC采样器；还构建首个DiT多对象运动迁移基准。方法在多对象上实现精确、语义对齐、时序一致且高质量的运动转移。


<details>
  <summary>Details</summary>
Motivation: 现有DiT在视频运动迁移时存在运动纠缠、缺乏对象级控制，难以在多对象场景下精准转移不同主体的运动；同时缺少针对这类任务的标准化评测数据。

Method: 1) Mask-aware Attention Motion Flow (AMF)：引入SAM2生成的对象掩码，将注意力与运动特征按对象显式解耦，实现多对象级的可控运动建模与融合；2) RectPC：提出高阶predictor-corrector采样求解器，提高采样效率与稳定性，尤其适用于多实体生成；3) 构建DiT多对象运动迁移基准数据与评测协议。

Result: 在所构建的基准上，MultiMotion实现了多对象的精确、语义一致且时序连贯的运动迁移，同时保持DiT的画质与可扩展性；多对象场景中采样效率与质量因RectPC得到提升。

Conclusion: 通过AMF的对象级解耦控制与RectPC的高效采样，MultiMotion克服了DiT在多对象运动迁移中的关键瓶颈，并为社区提供首个标准基准；方法在质量、对齐与时序一致性上均表现出色。

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [183] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

TL;DR: SJD++是一种面向自回归图像生成的训练免并行解码算法，通过在每次前向中进行多token预测并结合草稿-验证机制，大幅减少步数与时延，同时保持画质。


<details>
  <summary>Details</summary>
Motivation: 大规模自回归文本到图像模型画质高但推理慢，主要瓶颈在于需要数百到上千次串行的下一token预测。需要一种无需额外训练、可显著并行化、多token级别的生成加速方法，且不牺牲视觉质量。

Method: 提出Speculative Jacobi Decoding++（SJD++）：1）采用Jacobi式迭代多token并行预测，在单次前向中预测一段序列；2）结合speculative sampling的草稿-验证框架，用高精度模型对草稿多token并行校验；3）关键改进为在每次验证后复用高置信草稿token而非全部重采样，减少无效计算与回退；整体为训练免、概率并行的解码流程。

Result: 在多种代表性自回归文生图模型上，SJD++实现2×–3×的推理时延降低和2×–7×的生成步数压缩，同时视觉质量无明显下降。

Conclusion: SJD++在不改动模型与不额外训练的前提下，将Jacobi多token预测与推测采样有效融合，并通过高置信复用进一步加速，实证表明可显著提升自回归图像生成的效率且保持画质，适合作为通用推理加速方案。

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [184] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 提出ControlVP：在扩散模型生成图像中，通过用户引导和几何约束纠正消失点不一致，提升全局几何一致性且保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型（如Stable Diffusion）虽有高视觉质量，但在几何结构上常出错，尤其平行线在2D投影不正确收敛，导致建筑等场景的空间真实感下降；需要能显式控制和校正消失点与透视结构的方法。

Method: 在预训练扩散模型基础上加入结构化控制：1) 从建筑轮廓提取结构引导，作为控制信号；2) 设计几何约束，使图像边缘与透视线条/消失点对齐；3) 用户可交互指定或调整消失点（或相关结构），模型在生成/重绘时遵循该引导。

Result: 在不损失（或可比于基线的）视觉保真度下，显著提升全局几何一致性与消失点正确性；对需要精确空间结构的任务（如图到3D重建）表现更优。

Conclusion: ControlVP有效缓解扩散图像中的消失点不一致问题，实现结构与美观兼顾的生成；方法通用、可与现有模型结合，资源与代码已开源。

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [185] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Nießner,Wei Yang*

Main category: cs.CV

TL;DR: 提出MeshRipple，一种面向网格的自回归生成框架，通过“前沿扩张”式生成避免长程依赖被截断，显著提升表面连贯性与拓扑完整性。


<details>
  <summary>Details</summary>
Motivation: 现有自回归网格生成将面序列化并依赖滑动窗口训练/推理，内存可控但打断长程几何与拓扑依赖，导致破洞与碎片化组件。需要既能保持全局依赖又可扩展的生成方法。

Method: 1) 前沿感知的BFS标记/序列化，使生成顺序与表面拓扑一致；2) 以“生成前沿”为中心的外扩式预测策略，保证连贯的表面增长与组件连通性；3) 带稀疏注意力的全局记忆模块，提供近似无界感受野以处理远程拓扑关系。三者集成于自回归框架中，实现逐步扩张的网格生成。

Result: 在与近期强基线的比较中，实现更高的表面保真度与更完整的拓扑结构，减少孔洞与碎片组件。

Conclusion: 通过对序列化、预测策略与注意力记忆的协同设计，MeshRipple有效恢复长程依赖，支持连贯、完整的网格生成，优于现有自回归网格生成方法。

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [186] [From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images](https://arxiv.org/abs/2512.07527)
*Fei Yu,Yu Liu,Luyang Tang,Mingchao Sun,Zengye Ge,Rui Bu,Yuchao Jin,Haisen Zhao,He Sun,Yangyan Li,Mu Xu,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: 论文提出一种从稀疏卫星影像重建城市级3D并合成地面视角的新方法：以2.5D高度图（Z单调SDF）建模几何，结合可微渲染进行外观赋予，并用生成式纹理修复提升细节；在大范围城市上实现稳健、可扩展且具SOTA的地面视图合成和可用资产生成。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF/3DGS在极端视角外推（近90°）和弱视差、遮挡、透视压缩、纹理退化的卫星影像下易失败，无法可靠合成地面新视角与得到可用城市模型。需要一种既稳几何、又能从劣质纹理恢复细节且适配城市结构的方案。

Method: 1) 几何：将城市建模为2.5D高度图，具体实现为Z单调SDF，使其与顶视城市布局一致；该设计稳定了稀疏、斜视卫星视角下的几何优化，并生成密闭网格，屋顶清晰、立面垂直外挤。2) 外观：用可微渲染从卫星图“上色”网格；为应对长距离、模糊、退化输入，训练生成式纹理恢复网络，补全高频、合理的细节。

Result: 在大规模城市重建实验中展现出可扩展性与鲁棒性；用少量卫星图重建约4平方公里真实区域，地面视图合成达到或超过SOTA，生成视觉逼真、几何整洁的模型。

Conclusion: 针对卫星到地面极端视角外推，2.5D（Z单调SDF）+可微渲染+生成式纹理修复的组合有效稳定几何并提升外观，能以少量输入产出高保真、可下游应用（如城市规划、仿真）的城市级3D资产。

Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.

</details>


### [187] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

TL;DR: 提出一种无需训练的自纠框架，通过不确定性引导的视觉再注意，迭代修正VLM回答，显著降低幻觉并提升对象存在判断准确率。


<details>
  <summary>Details</summary>
Motivation: VLM在图像理解与问答中常产生“看似合理但错误”的幻觉，尤其在对局部目标或细节缺乏充分注意时；现有方法多需微调或改模型，成本高、迁移性差，因此需要一种无需训练、可直接用于现有预训练VLM的自纠机制。

Method: 在冻结的VLM上运行一个迭代循环：1) 多维不确定性度量——基于生成token熵、注意力分散度、语义一致性、陈述置信度等量化回答的不确定性；2) 将高不确定性关联到图像空间，进行注意力引导的裁剪与再编码，重点关注先前未充分观察的区域；3) 用新的视觉证据重新生成或编辑回答，直至不确定性降低或达到迭代上限。无需梯度更新，仅推理态运行。

Result: 在POPE与MMHAL BENCH上（使用Qwen2.5-VL-7B）相较基线降低幻觉率9.8个百分点；在对抗划分上对象存在准确率提升4.7个百分点；定性结果显示再注意能把修正锚定到真实视觉证据，标准解码难以做到。

Conclusion: 训练自由的、不确定性引导再注意框架能有效缓解VLM幻觉并提升可信多模态推理，可直接套用到现有模型；作者已开源并计划扩展到更多架构以验证通用性。

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [188] [Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation](https://arxiv.org/abs/2512.07568)
*Xuecheng Li,Weikuan Jia,Alisher Kurbonaliev,Qurbonaliev Alisher,Khudzhamkulov Rustam,Ismoilov Shuhratjon,Eshmatov Javhariddin,Yuanjie Zheng*

Main category: cs.CV

TL;DR: 提出DSRSD-Net：通过双流残差分解与语义去相关，显式分离模态共享与私有信息，以抑制模态主导、冗余耦合及虚假相关，提升多模态预测的鲁棒性与可解释性，并在教育预估任务上优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常遭遇模态主导（高方差模态压制弱信号）、共享与私有因素缠绕、虚假跨模态相关，导致泛化差与可解释性弱，且对噪声/缺失模态敏感。需要一种能解耦共享与私有信息、减少冗余与虚假相关、并保持鲁棒与可解释的表示学习框架。

Method: 提出Dual-Stream Residual Semantic Decorrelation Network（DSRSD-Net）。核心包括：1）双流表示学习：通过残差投影将各模态分解为私有（intra-modal）与共享（inter-modal）潜因子；2）残差语义对齐头：用对比学习与回归式目标将各模态的共享因子对齐到统一语义空间；3）去相关与正交约束：对共享空间的协方差结构进行正则，并强制共享与私有流正交，抑制跨模态冗余、防止表示坍塌。

Result: 在两个大规模教育领域基准上，DSRSD-Net在下一步预测与最终结果预测上均稳定优于强力单模态、早/晚融合与协同注意力等基线。

Conclusion: 通过残差式解耦与显式语义去相关，DSRSD-Net有效缓解模态主导与虚假相关，提升可解释性与鲁棒性，适用于噪声或缺失模态场景，并在实际基准中取得一致性能提升。

Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.

</details>


### [189] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

TL;DR: 论文发现视觉大模型在深层对视觉token的信息逐渐“消失”，导致训练免剪枝在深层不优于随机剪；据此提出基于“信息地平线”的剪枝策略：在地平线之后可安全随机剪，结合现有方法能在保持性能的同时大幅降算。


<details>
  <summary>Details</summary>
Motivation: VLLM推理成本高，因需处理上百视觉token。现有训练免剪枝在深层效果不佳且接近随机。作者怀疑是“token信息消失”现象导致，需要度量并理解不同层的token信息分布，以指导更高效的剪枝策略。

Method: 提出基于输出概率敏感度的token信息度量：移除单个token，测量模型输出概率变化量以量化其信息含量；在不同层计算视觉token信息分布，分析其随深度、任务类型（OCR vs VQA）和模型规模（Qwen2.5-VL vs LLaVA-1.5）的变化；据此在较深层采用随机剪，并与现有方法（如DivPrune）结合。

Result: 发现：1）随深度加深，视觉token信息趋于均匀并在某中间层后快速消失，形成“信息地平线”，之后token基本冗余；2）地平线位置随任务而变，视觉密集任务（OCR）地平线更深；3）地平线与模型能力正相关，强模型的视觉token有效深度更深。实证显示：深层随机剪既高效又稳健，并能稳步增强现有剪枝法。使用DivPrune+深层随机剪，在Qwen2.5-VL-7B上剪50%视觉token仍保留96.9%性能，达SOTA。

Conclusion: 视觉token在深层的信息“地平线”决定了可安全剪枝的范围；在地平线之后采用简单随机剪即可兼顾精度与效率，并且能增强现有剪枝方法。论文为任务自适应与模型规模相关的剪枝策略提供了经验依据与度量工具，代码将开源。

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [190] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

TL;DR: LongCat-Image 是一个中英双语、开源的图像生成基础模型，聚焦多语言文本渲染、拟真度、推理效率与可用性，通过分阶段数据策划+奖励模型强化，取得中文字符渲染与图像编辑SOTA，同时以6B扩散核心实现低显存、快推理，并完整开放模型与训练工具链。


<details>
  <summary>Details</summary>
Motivation: 现有主流图像生成模型在多语言（尤其中文）文本渲染不稳、写实度与美学质量不均、MoE体量大导致部署昂贵、开源生态不完整、开发者门槛高。作者希望在保证高保真与文本可控性的同时压缩模型规模，并彻底开放训练流程以促进社区研究与落地。

Method: 多阶段训练策略：预训练/中期训练/指令微调阶段进行严格数据策划；在RL阶段联合使用精心构建的奖励模型以优化文本渲染、美学与写实等目标。模型采用约6B参数的紧凑扩散架构；同时提供文本到图像与图像编辑两类模型版本，并开放中期与后期检查点和完整训练工具链。

Result: 实现SOTA文本渲染与高拟真度，特别是在中文复杂与生僻字覆盖与准确性上优于主流开源与商业方案；图像编辑在标准基准上达SOTA，编辑一致性优于其他开源模型；在仅6B参数下显存占用与推理时延大幅降低，相比常见近20B+ MoE方案更易部署。

Conclusion: LongCat-Image在保持高质量生成与编辑能力的同时，实现多语言（重在中文）文本渲染的行业新标准与高效推理，并以全面开放的模型与工具链强化社区可复现与二次开发潜力，推动视觉内容生成前沿。

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [191] [Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation](https://arxiv.org/abs/2512.07590)
*Kaili Qi,Zhongyi Huang,Wenli Yang*

Main category: cs.CV

TL;DR: 提出VM_TUNet：把变分PDE（改进的Cahn–Hilliard，含物理先验、边缘检测和平均曲率项）与深度网络结合，用频域预处理与稳定的局部计算模块，提升在噪声与边界模糊场景下的分割鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 纯CNN在噪声、边界模糊/断裂条件下易陷入差的局部最优、边界粗糙且解释性弱；纯变分模型可解释但表达力有限、计算昂贵。需要一个同时具备鲁棒边界处理、可解释性与深度网络表征能力，并兼顾效率的混合框架。

Method: 构建变分-深度混合框架VM_TUNet：以含物理先验、边缘检测器与平均曲率项的改进Cahn–Hilliard方程作为先验约束；网络架构含两模块：F模块在频域做高效预处理，缓解差局部极小值；T模块进行稳定的局部计算，提供稳定性估计；整体形成可协同训练的Tailored U-Net。

Result: 在三个基准数据集上，达成性能与计算效率的折中：相较纯CNN获得更优的定量指标与视觉边界质量；在合理计算开销下，性能接近Transformer方法。

Conclusion: 将变分PDE先验（边缘与曲率）嵌入深度网络、并通过频域预处理和稳定局部计算实现鲁棒分割，可在噪声和边界退化场景中取得竞争性结果与更平滑、准确的边界，同时维持较好的效率。

Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.

</details>


### [192] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

TL;DR: 论文评测SAM 3在机器人辅助手术中的表现：在点/框等空间提示下较SAM/SAM2显著提升，视频跟踪更稳；语言提示零样本在手术域仍弱；具备从2D到3D的重建与单目深度估计能力，在多数据集上有亮点但对高度动态复杂场景仍有限。


<details>
  <summary>Details</summary>
Motivation: SAM 3引入语言分割与更强3D感知，声称可零样本、跨模态与视频场景适用。手术场景要求精确、鲁棒与三维理解，需系统评估其在临床相关数据集上的实效与局限。

Method: 对SAM 3在手术图像与视频上进行实证评估：1) 使用点、框和语言提示做零样本分割；2) 测试动态视频跟踪能力；3) 评测其从2D图像进行3D重建与单目深度估计的能力；4) 在MICCAI EndoVis 2017/2018进行对比评测，并在SCARED、StereoMIS、EndoNeRF进行零样本3D相关测试；与SAM/SAM2对比。

Result: 在空间提示（点/框）下，SAM 3在图像与视频分割上较SAM/SAM2有明显提升；语言提示分割在手术域表现不佳；在SCARED、StereoMIS、EndoNeRF上展现出较强的单目深度估计和器械3D重建的真实性；但在高度动态、复杂的手术场景中仍存在稳定性与精度不足。

Conclusion: SAM 3在手术场景的空间提示分割与3D能力上优于前代，具备临床潜力；然而语言提示需域适配训练，复杂动态场景下鲁棒性仍待改进。建议结合手术域微调、数据增强与时空建模提升性能。

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [193] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出AutoSeg3D：将在线3D语义/实例分割重构为实例跟踪问题，用稀疏对象查询在时间上传播信息，结合空间一致性学习，提升实时、细粒度3D感知；在ScanNet200上比ESAM高2.8 AP，并在多数据集一致增益。


<details>
  <summary>Details</summary>
Motivation: 现有基于对象查询的在线3D分割多依赖将VFM输出提升到点云并在查询间传播空间信息，但忽略了感知的时间维度；视角变化导致物体部分可见，VFM碎片化问题严重，限制了时空一致理解与实时性。

Method: 将任务表述为实例跟踪：用对象查询作为时间信息载体。包含两条时间路径：1) 长期实例关联，维持跨帧的特征与ID一致性；2) 短期实例更新，快速注入当前观测细节。为缓解VFM碎片化，引入空间一致性学习，得到更完整的实例表征。通过稀疏查询实现时序信息交换，避免稠密点云时序交互的高计算成本。

Result: 在ScanNet200上超过ESAM 2.8 AP；在ScanNet、SceneNN、3RScan等数据集上也有稳定提升，达成SOTA。

Conclusion: 将在线3D分割转化为实例跟踪并以对象查询进行时序传播，可在不增加昂贵时序点云交互的情况下增强时空一致性与整体物体理解；空间一致性学习缓解VFM碎片化，带来跨数据集的SOTA性能。

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [194] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias Öttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: 提出DECOMP主动学习策略，通过类分解与置信度引导，在医学影像等密集预测任务中更高效地选取标注区域，尤其提升少数类表现，优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 密集预测（如分割、ROI分类）标注昂贵且耗时，现有主动学习多聚焦于图像级选择，区域级方法又存在计算/内存开销大、容易选到无关区域、过度依赖不确定性等问题。需要一种既高效又能覆盖少数类、并降低对不确定性依赖的采样策略。

Method: 提出DECOMP：用伪标签将图像分解为按类别的成分（类特定区域）；在每个类别内进行区域采样以提升多样性与代表性；再用类别层面的预测置信度（难度感知）调节采样强度，使困难/少数类获得更多标注；适用于ROI分类、2D与3D分割。

Result: 在多个任务（ROI分类、2D分割、3D分割）与数据设置中，DECOMP稳定优于基线方法，尤其在少数类区域的采样与性能提升方面更显著。

Conclusion: 类分解+类别置信度引导的区域级主动学习能以较低成本获得更具信息量与多样性的标注，缓解少数类难题并提升密集预测性能；方法通用、可扩展，代码已开源。

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [195] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

TL;DR: MoCA提出一种可扩展的部件级3D生成方法，通过稀疏全局注意和未选部件压缩，显著降低复杂度并提升组合生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有部件感知的3D生成在组件数量增多时，需要对所有部件做全局注意，导致二次方开销，限制了大规模、细粒度的组合式对象/场景生成。

Method: 引入两项关键设计：1) 基于重要性的组件路由，按相关性选择top-k部件参与稀疏全局注意；2) 对未选的重要性较低部件进行压缩表示，保留上下文先验同时减少注意计算量。整体形成可扩展的组合式3D生成框架（MoCA）。

Result: 在组合对象与场景生成任务上进行大量实验，MoCA在质量与效率上均优于基线，能在更多组件规模下稳定工作。

Conclusion: 通过重要性路由与未选部件压缩，MoCA在保持上下文信息的同时大幅降低全局注意成本，实现高效、细粒度且可扩展的3D组合生成。

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [196] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 提出LiQA肝纤维化量化数据集与基线方法，用于复杂真实场景下的肝分割与纤维化分期基准评测，展示多源数据与解剖先验能显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 肝纤维化流行且危害大，临床管理需要精准分期；现实MRI存在多中心多期相、域移、缺失模态与错位等挑战，缺乏统一基准来公平比较与推动算法在真实条件下的表现。

Method: 构建包含440名患者、来自多中心多期相MRI的LiQA数据集，设立两项任务：肝脏分割（LiSeg）与纤维化分期（LiFS）。基线/冠军方案：分割采用半监督框架并引入外部数据增强鲁棒性；分期采用多视图一致性（multi-view consensus）并结合基于CAM的正则化以强化判别和解剖约束。

Result: 在挑战评测中，使用多源数据与解剖（CAM）约束的方案在分割与分期上取得领先和稳健表现，对域移、缺失模态与空间错位等复杂因素更具鲁棒性。

Conclusion: LiQA为真实世界条件下的肝分割与纤维化分期提供了标准化基准；结合半监督、多源数据与解剖先验/多视图一致性的策略能显著提升临床场景中的泛化与可靠性。

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [197] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

TL;DR: 提出一套以YOLOv12 Nano、ResNet50+PCA+K-Means++与LLM（GPT-4o Mini）集成的AUV系统，实现海底目标自动检测、聚类与报告生成；在DeepFish与OzFish共5.5万+图像上评测，mAP@0.5=0.512，精确率0.535，召回率0.438，PCA保留98%方差，集群质量与LLM摘要有效提升任务效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 传统海洋勘探受极端环境、能见度低与高成本限制，导致大量海域未被探索；需要一种能在水下自主、低风险且高效率地感知、分析与汇报海洋目标的方法，以减少人工潜水风险并加速科研。

Method: 在AUV上集成多模块流水线：1）YOLOv12 Nano进行实时目标检测；2）ResNet50提取视觉特征；3）PCA降维以保留主要信息并提升后续聚类效率；4）K-Means++按视觉特征对目标聚类；5）LLM（GPT-4o Mini）结合检测与位置信息生成结构化报告与摘要；数据来自DeepFish与OzFish合并数据集（>55k张、澳大利亚多样海域）。

Result: 检测性能：mAP@0.5=0.512，精确率0.535，召回率0.438；PCA在显著降维下仍保留98%方差；K-Means++能将检测到的对象按外观相似性有效分群；LLM可产出有用的摘要和基于位置的报告，提升可解释性与工作流程效率。

Conclusion: 该AUV多模组AI系统在复杂水下环境中实现了从检测到聚类再到自动报告的端到端流程，降低潜水风险、提高任务效率与数据处理速度，为海洋科学调查提供实用工具；但检测召回仍有限，未来可在模型鲁棒性、数据多样性与在线自适应方面改进。

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [198] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

TL;DR: 提出OMEGA：一种无需训练、由优化引导的扩散式多智能体驾驶场景生成框架，通过在每个反向扩散步求解约束优化来保证物理/社会一致性，并以博弈视角生成逼真的对抗性安全关键情景；在nuPlan与Waymo上显著提升可控性与有效性并增加近碰撞样本。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶评测需要真实且多样的多车交互场景，尤其是罕见但关键的安全事件；现有数据驱动生成方法要么难以控制，要么违反物理/社交约束，难以用于严肃评测与对抗测试。

Method: 在扩散模型的每一步反向采样中，引入带物理与交互约束的优化重锚定（re-anchoring），强制轨迹满足结构一致性和交互合理性；进一步将自车与攻击者交互建模为分布空间的博弈优化，近似求解纳什均衡以合成真实的安全关键对抗场景；整套流程训练无关、以优化引导采样实现可控生成。

Result: 在nuPlan与Waymo上，真实度、一致性与可控性显著提升：自由探索场景的物理/行为有效率从32.35%提升到72.27%；可控生成从11%提升到80%。在保持整体真实感的同时，生成的TTC<3s近碰撞帧数提升约5倍。

Conclusion: 优化引导的训练无关扩散采样可有效约束场景物理/社会一致性并提升可控性；将自车-攻击者交互表述为博弈优化能产生真实的安全关键情境，为自动驾驶评测与鲁棒性测试提供更高效的场景生成手段。

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [199] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

TL;DR: 提出EgoCampus数据集与EgoCampusNet模型，用于预测户外校园导航场景中的行人自我视角凝视；基于Aria眼镜采集80+行人、25条路径、6km数据，强调真实世界注意力；方法与数据共同推动凝视预测在导航领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现有自我视角数据多为室内或缺少凝视标注，难以刻画真实户外导航中的人类视觉注意；需要大规模、带眼动信息的户外数据与相应模型来提升凝视预测能力，服务于导航、人机交互与安全。

Method: 构建EgoCampus数据集：使用Meta Project Aria眼镜同步采集眼动、前视RGB、IMU与GPS，覆盖校园25条户外路径、6公里、80+被试，生成带凝视注释的视频。提出EgoCampusNet：面向行人导航过程的凝视预测模型（细节未在摘要中给出），利用多模态时空线索进行眼睛注视位置估计。

Result: 基于EgoCampus数据训练与评测，EgoCampusNet能在户外行人导航场景中有效预测凝视（摘要未给出具体指标与对比，但宣称取得良好效果）。

Conclusion: 提供首个强调真实户外导航视觉注意的大规模自我视角凝视数据集，以及相应预测方法，为研究与应用（如导航与注意建模）提供基础；数据与代码将公开于项目仓库。

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [200] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: 提出DIST-CLIP：一种可用目标图像或DICOM元数据引导的MRI和谐化框架，利用CLIP提取对比度表示并与解耦的解剖内容通过自适应风格迁移融合，在多临床数据上优于SOTA，兼顾风格一致性与解剖保真。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像的临床泛化受限，主要因不同扫描仪、协议、序列参数导致的域偏移与数据异质性。现有影像级和文本引导的和谐化方法要么依赖目标图像、要么用过于简单的标签/受限数据，难以覆盖真实临床环境的复杂采集差异。

Method: 提出DIST-CLIP框架：1) 显式解耦解剖内容与图像对比度；2) 使用预训练CLIP编码器从目标图像或DICOM元数据中提取“对比度”嵌入；3) 通过新型自适应风格迁移(AST)模块，将对比度嵌入融入解剖内容以实现风格标准化与保持结构；4) 在多样真实临床数据上训练与评估。

Result: 在多源临床MRI数据上，相比当前SOTA方法，DIST-CLIP在风格翻译保真度与解剖结构保留方面均显著提升，展示了更灵活的引导与更强的泛化能力。

Conclusion: DIST-CLIP通过CLIP引导的解耦风格迁移，在无需严格依赖目标图像且可利用DICOM元数据的前提下，实现MRI和谐化与标准化，兼顾风格一致与解剖保真，适用于真实临床异质性场景。

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [201] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

TL;DR: 提出首个从手持单目视频同时预测部件分割与关节参数的端到端数据驱动方法，训练只用合成数据，却能迁移到真实场景，支持随手拍实时应用。


<details>
  <summary>Details</summary>
Motivation: 机器人操作与数字孪生需要理解可动物体，不仅要知道有哪些部件，还需恢复铰链/滑轨等关节的类型与参数。以往多依赖多视角、扫描或静态相机，限制了实用性与部署成本。

Method: 以随手录制的单目视频为输入，设计一个联合学习框架，同时输出部件语义/实例分割与关节类型和参数（如旋转轴、平移方向、角度/位移）。采用纯合成数据进行训练并进行sim2real迁移，模型直接在视频上运行，适配自由移动相机。

Result: 在未见过的真实物体与场景上表现稳健，能够从普通手持视频中可靠恢复部件分割和关节参数，展示出良好的泛化与实时性。

Conclusion: 单目、随手拍条件下的关节化物体理解可通过合成数据训练的端到端模型实现，具备可扩展、实用且适用于动态环境的潜力。

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [202] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

TL;DR: NPC（Negative Prompting for Image Correction）提出一套自动化负向提示词生成与筛选流程，通过抑制“不要的内容”来显著提升文本-图像对齐，在GenEval++与Imagine-Bench上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前扩散式文生图模型在复杂组合或想象性强的提示上易产生错配与多余元素。已有方法多依赖正向提示工程或后验重采样，缺乏系统性的“该不生成什么”的机制与自动化发现途径。作者动机是用可解释分析与自动化管线，系统地发现并应用负向提示以纠正错配。

Method: 1) 机制分析：基于跨注意力（cross-attention）模式解释负向提示的作用，指出“定向负样”（与错配直接相关）与“非定向负样”（与原提示无关但在图中出现的词）均可改善对齐。2) 候选生成：提出“验证器-描述器-提议器”（verifier–captioner–proposer）框架，从生成图像与提示中自动产出负向候选。3) 排序选择：设计显著性文本空间评分（salient text-space score）无须额外图像合成即可对候选负样排序筛选。4) 推理：将选出的负向提示与原提示共同输入扩散模型以抑制不期望内容。

Result: 在GenEval++上NPC得分0.571（对比强基线0.371），在Imagine-Bench上获得最佳总体性能，显示其在多样化、复杂组合提示下的对齐提升。

Conclusion: 通过“指导不该生成什么”，NPC提供了一条有理论动机、可解释且全自动的负向提示管线，可稳健提升扩散模型的文本-图像对齐；代码已开源。

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [203] [PVeRA: Probabilistic Vector-Based Random Matrix Adaptation](https://arxiv.org/abs/2512.07703)
*Leo Fillioux,Enzo Ferrante,Paul-Henry Cournède,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: 提出PVeRA：在VeRA的基础上引入概率化低秩矩阵的参数高效适配器，在VTAB-1k上优于VeRA与多种适配器。


<details>
  <summary>Details</summary>
Motivation: 大模型微调需要大量数据与算力，参数高效适配器可仅训练少量参数以适配新任务。现有VeRA通过在各层共享一对冻结的随机低秩矩阵实现高效，但难以表达输入的不确定性与多样性。

Method: 在VeRA的框架上，将共享的低秩矩阵进行概率化建模（如对低秩表示的随机化/采样），在训练与测试时允许不同的采样配置；主干权重冻结，仅训练新增模块的少量参数，实现参数高效适配。

Result: 在VTAB-1k基准上，系统评测7种适配器；PVeRA在总体性能上超过VeRA及其他适配器，显示更强的适配能力。

Conclusion: 对VeRA进行概率扩展能更好处理输入固有歧义，并在参数高效设定下取得更优性能；代码已开源，便于复现与比较。

Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.

</details>


### [204] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 提出一个三阶段预处理管线，通过分割-修复-再估计来消除笼网遮挡，使动物姿态估计与跟踪在有笼环境中接近无遮挡条件的性能。


<details>
  <summary>Details</summary>
Motivation: 现有动物跟踪与姿态估计（如 STEP、ViTPose）在有笼条和系统性遮挡的图像/视频上性能显著下降；需要一种方法在不改变下游模型的情况下缓解笼网带来的结构化遮挡。

Method: 三阶段管线：1) 笼结构分割：使用加入可调方向滤波的 Gabor 增强型 ResNet-UNet，采用72个取向核以提取方向敏感特征，精准识别笼条；2) 笼区域修复：用 CRFill 进行内容感知补全，对被笼条遮挡的区域进行图像修复；3) 在修复后的“去笼”帧上评估并运行姿态估计与多目标跟踪（如对 STEP、ViTPose）。

Result: 实验表明：经该预处理去除笼网后，姿态估计与跟踪性能可达到与无遮挡环境相当的水平；关键点检测精度与轨迹一致性显著提升。

Conclusion: 结构化遮挡（笼条）是导致现有方法失效的主要因素；通过方向感知的分割与内容感知修复组成的预处理管线，可普适地提升下游姿态与跟踪模型在有笼场景中的表现。

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [205] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出一种将3D重建与视频扩散相结合的一次性输入上半身3D人像生成方法，既保留结构稳定性又生成高频细节与流畅动态，实现实时高保真头像并显著减少伪影。


<details>
  <summary>Details</summary>
Motivation: 单张图像生成高保真上半身3D头像困难：3D重建法结构稳定但纹理模糊、动作僵硬；视频生成法外观逼真、动态好，但易结构不稳与身份漂移。需要兼具几何稳定与生成细节/动态能力的统一框架。

Method: 以3D重建模型提供稳健的几何与外观先验，作为条件引导一个实时自回归视频扩散渲染器。通过该引导，模型在保持结构一致性的同时合成高频纹理与流畅时序动态，实现实时渲染并降低模糊与僵硬。

Result: 与主流方法相比，显著减少伪影与结构不一致，视觉质量大幅提升；实现实时生成、时序一致的高仿真外观与运动。

Conclusion: 融合3D重建的几何稳定性与视频生成的高保真/动态能力，可在一次性输入下生成高质量、时序连贯的上半身数字人像，适用于游戏与VR等实时应用。

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [206] [Improving action classification with brain-inspired deep networks](https://arxiv.org/abs/2512.07729)
*Aidas Aglinskas,Stefano Anzellotti*

Main category: cs.CV

TL;DR: 研究比较DNN与人类在动作识别中对“身体”和“背景”的依赖，提出脑启发的双通道架构（身体/场景）以更贴近人类表现。


<details>
  <summary>Details</summary>
Motivation: 动作识别可从身体姿态与背景场景两源获取信息，但现有DNN可能因数据相关性而过度依赖单一来源；相较之下，人脑有分别处理身体与场景的专属区域，提示分域处理或能提升鲁棒性与人类相似性。

Method: 1) 使用HAA500数据集训练标准DNN，评估其在三种刺激版本（身体+背景、去身体仅背景、去背景仅身体）上的准确率；2) 进行人类实验（N=28）在同三类刺激上测性能；3) 设计脑启发架构：分离身体与背景两条处理流并融合，评估整体性能及在三类刺激上的准确率分布。

Result: 标准DNN：身体+背景与仅背景上准确率相近；去背景仅身体时接近随机水平。人类：三种条件均能较准确识别，且“仅身体”显著优于“仅背景”。新架构：总体性能提升，其在三种条件下的准确率模式更接近人类。

Conclusion: 传统DNN过度依赖背景线索而忽视身体信息；引入与人脑相似的分域处理（身体/背景双流）可提升动作识别，并产生更人类化的线索利用模式。

Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.

</details>


### [207] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出SAVE：用稀疏自编码器(SAE)潜变量引导多模态大模型的推理方向，强化视觉理解、降低物体幻觉；在CHAIR_S提升约10个百分点，并在POPE与MMHal-Bench稳定获益。


<details>
  <summary>Details</summary>
Motivation: MLLM仍易受语言先验与视觉信息丢失影响，产生物体幻觉。需要一种无需再训练、可泛化地增强视觉扎根与减少幻觉的方法。

Method: 训练或采用现成的稀疏自编码器，对模型的中间表征进行稀疏分解；通过一个二分类“是否存在该物体”的QA探针，筛选最能指示视觉信息处理的SAE潜特征（称为视觉理解特征）；在推理时沿这些特征方向对模型进行“steering”（调控/引导），强化与真实图像相关的表征，抑制受语言先验驱动的错误生成。

Result: 在标准无训练方法对比中表现最好：CHAIR_S提升约10个百分点，POPE与MMHal-Bench一致增益；在多模型与不同层位进行广泛评测，结果稳健且可泛化。

Conclusion: 沿“视觉理解特征”进行引导可抑制不确定物体词元的生成、提升对图像词元的注意力，从而缓解幻觉；方法简单、训练开销低、泛化强，并已开源。

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [208] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出SpatialDreamer：一种通过闭环主动探索、世界模型视觉想象与证据支撑推理来提升MLLM空间推理能力的强化学习框架，并配套GeoPO用于长链推理的几何一致性分步奖励优化；在多项基准上表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在复杂空间推理（需心智模拟）上受限，主要因其被动观察、缺乏主动心像与精细奖励监督，难以在长程横向推理中稳定优化。

Method: 1) SpatialDreamer：闭环流程=主动探索动作→世界模型生成视觉想象→根据证据进行推理与决策迭代。2) GeoPO（Geometric Policy Optimization）：树状采样策略，结合几何一致性约束的步级奖励估计，缓解长链推理奖励稀疏与信用分配问题。

Result: 在多个具有挑战性的空间推理基准上取得高度竞争性结果，证明方法有效。

Conclusion: 主动探索+世界模型心像+几何一致性奖励优化可显著提升MLLM的人类般主动空间心智模拟与推理能力。

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [209] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

TL;DR: 他们提出一种面向TREC VQA“答案生成(AG)”任务的“列表式(listwise)学习”框架：先由多模态生成模型产出多候选答案，再用带“掩码指针+排序权重”的交叉熵损失进行重排序，显著提升语义精度与排序稳定性，尤其在时间推理与语义消歧问题上有效。


<details>
  <summary>Details</summary>
Motivation: 现有VQA答案生成往往只优化单一最佳答案或逐条生成，缺少对整份候选列表的整体优化，导致语义细粒度区分不足、排序不稳定，并难以兼顾生成与判别的优势。

Method: 两阶段：1) 基础多模态模型对视频-问题生成多个候选答案；2) 以“Masked Pointer Cross-Entropy Loss with Rank Weights”训练的重排序模型进行列表式优化。该目标结合：指针机制在候选中选择、按目标名次加权、在受限词表下的掩码交叉熵，从而稳定、可解释地进行listwise训练，并实现生成-判别桥接。

Result: 实验显示整体准确率与排序稳定性均有一致性提升，对需要时间推理与语义消歧的题型收益更明显。

Conclusion: 将生成式候选与判别式列表重排结合，通过带排序权重的掩码指针交叉熵实现稳定的listwise优化，可输出连贯、细粒度的答案列表，在TREC VQA AG任务上取得持续增益。

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [210] [DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07745)
*Jialv Zou,Shaoyu Chen,Bencheng Liao,Zhiyu Zheng,Yuehao Song,Lefei Zhang,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出DiffusionDriveV2：在扩散式端到端自动驾驶中结合强化学习与锚点化GMM，缓解模式塌缩，兼顾多样性与高质量，并在NAVSIM取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有生成式扩散驾驶模型易模式塌缩，行为保守单一。DiffusionDrive虽用“锚点意图”划分动作空间带来多样性，但基于纯模仿学习的约束不足，难以同时保证多样性与稳定高质量，产生两难。

Method: 在锚点化GMM扩散规划框架上引入强化学习：1）尺度自适应的乘性噪声促进广泛探索，适配轨迹规划；2）锚点内的GRPO用于对同一锚点样本进行优势估计与更新；3）锚点间的截断式GRPO引入全局视角但避免不同意图（如直行与转弯）之间的不当优势比较，从而防止跨意图的模式塌缩。

Result: 在闭环评测中，以对齐的ResNet-34骨干在NAVSIM v1上达91.2 PDMS、在NAVSIM v2上达85.5 EPDMS，创下新纪录；实验显示在截断扩散模型中兼顾了多样性与稳定高质量。

Conclusion: 通过将RL与锚点化扩散相结合，DiffusionDriveV2用探索与约束双机制提升整体轨迹质量，同时保持多模态；提出的尺度自适应乘性噪声与锚点内/锚点间（截断）GRPO有效抑制模式塌缩，取得SOTA并给出最佳权衡。

Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2

</details>


### [211] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: 提出Unison：在两阶段范式下实现统一多模态理解与生成，低成本训练下覆盖多任务，并可自动解析意图与元信息，取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态系统有两路：自回归端到端需巨量数据算力；两阶段对齐成本低但任务覆盖有限、生成质量不足。同时，两类方法普遍缺乏对任务类型、分辨率、时长等元信息的自动解析，需手工配置，影响实用性与智能化。

Method: 采用两阶段方案：保持预训练理解与生成模型能力，通过对齐微调连接二者；设计模块自动解析用户意图，判定目标任务（理解/生成及子任务），并从输入中提取必要元信息（图像分辨率、视频时长等）用于自动配置推理与生成参数；在仅50万样本和50 GPU小时的低成本设置下完成训练。

Result: 在多种理解（文本、图像、视频）与生成任务（文生图/视频、编辑、可控生成、基于IP参考生成）上实现优异性能；可准确自动识别任务并提取相关参数；在极低训练成本下仍保持预训练模型优势。

Conclusion: Unison以极低成本实现统一的多模态理解与生成，并通过自动意图与元信息解析消除繁琐手动配置，兼顾广泛任务覆盖与高质量生成，显示出实用与可扩展潜力。

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [212] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

TL;DR: UltrasODM是一套面向临床自由手扫超声的双流重建与反馈框架，提供逐帧不确定性、显著性诊断与操作提示，相比基线（UltrasOM）显著降低漂移、距离与Hausdorff误差，提升重建可靠性与临床可用性。


<details>
  <summary>Details</summary>
Motivation: 临床超声获取高度依赖操作者，探头快速运动与亮度波动导致重建误差与不可信结果，影响诊疗与工作流程；缺乏能在采集中即时量化不确定性、提供可解释反馈并提示纠正动作的系统。

Method: 提出UltrasODM：1）对比排序模块按运动相似度分组帧；2）光流分支与Dual-Mamba时序模块融合，进行鲁棒6自由度位姿估计；3）人机协同层结合贝叶斯不确定性、临床校准阈值与显著性图，定位低置信区域并在超阈时发出不干扰式提醒（如重扫或减速）。提供逐帧不确定性与显著性可视化。

Result: 在临床自由手扫数据集上，相较UltrasOM：漂移降低15.2%，距离误差降低12.1%，Hausdorff距离降低10.1%；同时输出逐帧不确定性与显著性。

Conclusion: 通过透明度与临床反馈闭环，UltrasODM在采集中提供可解释的不确定性与可操作提示，显著提升超声重建的可靠性与信任度，支持更安全的临床工作流程；代码已开源。

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [213] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

TL;DR: 提出一种无监督可见-红外行人重识别方法，通过模态感知的Jaccard距离进行全局跨模态关联，并用“拆分-对比”策略对齐模态特定原型，实现模态不变且身份可区分的表示，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: USVI-ReID需在无标注下跨可见光与红外匹配个体，模态差异大导致跨模态关联难。现有多用最优传输在类内聚类层面配对，易放大局部聚类误差且忽视全局实例关系，难以获得可靠的跨模态学习信号。

Method: 1) 提出模态感知的Jaccard距离，对齐并修正因模态差异带来的距离偏置，使全局聚类时的跨模态关联更可信；2) 设计“拆分-对比”策略：从数据中构建模态特定的全局原型，并在全局关联的指导下显式对齐这些原型，进行对比学习，从而学习到模态不变且身份判别的表示。

Result: 在标准VI-ReID基准上取得SOTA，较现有方法有显著性能提升，证明方法有效。

Conclusion: 通过在全局层面缓解模态偏置并对齐模态特定原型，可在无监督设置下实现更可靠的跨模态关联与模态不变表示学习，从而显著提升USVI-ReID性能。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [214] [GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring](https://arxiv.org/abs/2512.07776)
*Maximilian Schall,Felix Leonard Knöfel,Noah Elias König,Jan Jonas Kubeler,Maximilian von Klinski,Joan Wilhelm Linnemann,Xiaoshi Liu,Iven Jelle Schlegelmilch,Ole Woyciniuk,Alexandra Schild,Dante Wasmuht,Magdalena Bermejo Espinet,German Illera Basas,Gerard de Melo*

Main category: cs.CV

TL;DR: 提出GorillaWatch管线与三大数据集，利用多帧自监督和可解释性验证，实现野外大猩猩检测、跟踪、重识别及无监督计数的基准与方法


<details>
  <summary>Details</summary>
Motivation: 野外相机陷阱视频海量且个体重识别需人工逐帧比对，缺乏大规模“野外”视频数据集，导致深度模型难以自动化、泛化与可解释地用于极危西部低地大猩猩监测

Method: 1) 构建三套数据集：Gorilla-SPAC-Wild（最大野外灵长类ReID视频集）、Gorilla-Berlin-Zoo（跨域泛化评估）、Gorilla-SPAC-MoT（用于多目标跟踪）；2) 提出端到端GorillaWatch，将检测、跟踪、重识别整合；3) 多帧自监督预训练：利用tracklet时序一致性、无标注学习领域特征；4) 可解释性：将AttnLRP做可微改造，验证模型关注生物计量特征而非背景；5) 基准实验：比较大尺度图像主干的特征聚合与视频专用架构；6) 无监督群体计数：在聚类中引入时空约束减轻过分割

Result: 实验显示聚合大型图像骨干特征优于常见视频架构；多帧自监督预训练在无标注条件下提升ReID；可解释性分析表明模型聚焦可辨识生物特征；基于时空约束的聚类改进了无监督个体计数的准确性

Conclusion: 提供首个系统性基准与开源数据/代码，结合自监督、可解释性与时空聚类，实现可扩展、非侵入的极危物种（西部低地大猩猩）自动监测，促进跨域泛化与实际部署

Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species

</details>


### [215] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: DMVAE 显式将编码器潜变量分布对齐到任意参考分布（不限于高斯），从而在保持重建质量的同时提升下游生成建模效率；在 ImageNet 上以仅 64 个 epoch 达到 gFID 3.2。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成通常先压缩到潜空间再建模，但 VAE或与大模型对齐的编码器并未显式塑造潜变量分布，默认或隐式先验（如各向同性高斯）可能并非最利于扩散/自回归建模的分布，因而需要一种机制去探索并匹配更合适的潜分布。

Method: 提出 Distribution-Matching VAE（DMVAE）：在 VAE 编码器上加入分布级对齐约束，使其输出潜分布与给定参考分布显式匹配。参考分布可来自：自监督特征（SSL）、扩散噪声、或其他先验。由此突破传统 VAE 的高斯先验限制，并系统评估不同潜分布对生成建模的影响。

Result: 采用 SSL 派生的参考分布时，取得重建质量与建模效率的良好平衡；在 ImageNet 上仅训练 64 个 epoch 即达到 gFID=3.2，显示潜分布选择显著影响下游生成器性能与效率。

Conclusion: 固定高斯先验并非最优；通过分布级对齐选择并匹配更适合建模的潜分布（如 SSL 派生分布）是提升潜空间生成质量与效率的关键。

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [216] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

TL;DR: OneStory将多镜头视频生成重构为“下一镜头生成”问题，借助全局但紧凑的跨镜头上下文（选帧全局记忆+自适应补丁化条件）与预训练I2V模型，实现更长程一致的叙事生成，并在自建60K多镜头数据上取得SOTA叙事一致性。


<details>
  <summary>Details</summary>
Motivation: 现有MSV方法只能在有限时间窗或单关键帧条件下建模跨镜头语义，难以处理复杂长程叙事，导致角色/场景/情节一致性差、可控性不足与可扩展性受限。

Method: 1) 问题重构：将MSV视作自回归“下一镜头”预测，逐镜生成并复用历史上下文；2) 视觉条件：在强大的预训练I2V模型上微调；3) Frame Selection：从先前镜头中选取信息量高、语义相关的关键帧，构建全局记忆；4) Adaptive Conditioner：基于重要度引导的patchification，将全局记忆压缩为紧凑条件，直接喂入生成器；5) 数据与训练：构建带指代性caption的高质量多镜头数据（60K），并设计适配“下一镜头”范式的训练策略。

Result: 在文本与图像条件两种设置下，于多种复杂场景上取得更强的叙事连贯性与可控性，达到SOTA；展示了可扩展、稳定的长视频故事生成能力。

Conclusion: 通过全局且紧凑的跨镜头上下文建模与下一镜头自回归范式，OneStory显著提升多镜头叙事视频生成的一致性与可控性，为长篇叙事视频生成提供实用可扩展的解决方案。

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [217] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

TL;DR: MVP 提出一种可扩展的多视角金字塔 Transformer，可在一次前向中从数十到数百张图像直接重建大规模 3D 场景；通过跨视角“由局部到整体”和单视角“由细到粗”的双层级设计，结合 3D 高斯点云（Gaussian Splatting）表示，实现高效且高质量的通用化重建。


<details>
  <summary>Details</summary>
Motivation: 现有多视角重建方法在大场景、海量视角下计算/显存成本高、推理慢，且难以兼顾细节与全局一致性。需要一种既能扩展到许多视角、又能保持重建质量与效率的模型结构。

Method: 设计 Multi-view Pyramid Transformer：1) 交互式的跨视角层级（local→groups→scene）逐步扩大上下文范围，建立全局一致性；2) 视角内的细到粗层级，将精细空间表征逐步聚合为紧凑高信息密度 token，以降计算。采用 3D Gaussian Splatting 作为底层 3D 表示，并在一次前向传播中进行重建。

Result: 在多样数据集上验证，MVP 在广泛的视角设置下保持高效率与可扩展性；与 3D Gaussian Splatting 结合时，达到了通用化重建的 SOTA 质量，同时速度与可扩展性突出。

Conclusion: 双层级（跨视角由局到全、视角内由细到粗）的 Transformer 架构，使得在一次前向中对大型复杂场景进行快速、高质量、多视角重建成为可能，并在效率与可扩展性上优于现有方法。

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [218] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出一种在3D高斯表示中嵌入极低维语义瓶颈特征，并结合多分辨率哈希编码与新的下采样和正则机制，从而在大规模场景中高效、对齐地学习语言场景语义，性能与效率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通过蒸馏2D语义特征到3D的做法，在大规模互联网场景上存在两大问题：1) 语义特征跨视角/跨模态错配，导致语义对齐差；2) 训练与推理的显存与时间开销高，限制了实际应用，尤其是大规模场景的语言查询、编辑、导航等。

Method: - 在3D Gaussian表示中引入极低维“语义瓶颈”特征，先渲染再经多分辨率、基于特征的哈希编码，提高显存与运行时效率。
- 设计Attenuated Downsampler模块，并提出多种正则项，专门缓解由2D语义监督带来的语义错配问题（跨视角一致性/置信度衰减等）。
- 以大规模、野外HolyScenes数据集进行训练与评测。

Result: 在HolyScenes上，方法在语义质量与效率（显存占用与运行时间）上均优于现有特征蒸馏类基线，显示出更好的大规模场景语言-几何对齐与检索/编辑等下游潜力。

Conclusion: 通过低维语义瓶颈+哈希编码的高效语义场表示，以及针对2D监督错配的下采样与正则策略，可在大规模3D场景中实现更高效、更对齐的语言场嵌入，优于现有方法，并有望促进自然语言驱动的3D检索、编辑与导航等应用。

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [219] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

TL;DR: WorldReel 是一个原生4D一致的视频生成器，同时输出RGB视频与4D场景表示（点图、相机轨迹、稠密流），在大幅非刚性运动与显著相机运动下保持跨视角与时间的一致性，并在几何一致性、运动连贯性与观感伪影上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成器虽具备高逼真度，但缺乏稳健的3D/4D一致性，难以在多视角与动态场景中维持统一的场景与外观，产生时空不一致与视时伪影，限制了可渲染、交互与推理等下游能力。

Method: 提出显式4D表示的视频生成框架WorldReel：联合生成RGB帧与点图(pointmaps)、相机轨迹、稠密光流/对应映射；通过“单一底层场景”约束确保跨时间与视角共享几何与外观。训练上采用合成+真实数据混合：合成数据提供精确几何/运动/相机监督，真实视频提供外观多样性与逼真度，从而在保持强几何保真的同时泛化到野外。

Result: 在动态场景与移动相机条件下，较现有方法显著提升几何一致性与运动连贯性，降低视时伪影；在多项指标上达成SOTA。

Conclusion: WorldReel将视频生成推进到更接近4D一致的世界建模：用单一稳定的时空表示来渲染、交互与推理复杂场景，兼具真实感与几何稳定性，并具备野外泛化能力。

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [220] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

TL;DR: 提出OpenVE-3M大型开源视频编辑指令数据集与OpenVE-Bench基准，并训练5B模型OpenVE-Edit，在基准上达SOTA，超越更大开源模型。


<details>
  <summary>Details</summary>
Motivation: 图像编辑指令数据集质量与多样性快速提升，但视频编辑指令领域缺乏大规模高质量数据与统一评测基准，限制了模型训练与比较。

Method: 构建OpenVE-3M数据集，涵盖空间对齐编辑（全局风格、背景替换、局部修改/删除/添加、字幕编辑）与非空间对齐编辑（多机位/镜头切换与创意编辑）。通过精心设计的数据生成流水线与严格质量过滤获得高质量多样数据；同时构建包含431组视频-编辑对的OpenVE-Bench，设计三个与人类评判高度一致的指标。基于数据训练5B参数模型OpenVE-Edit。

Result: OpenVE-3M在规模、编辑类型多样性、指令长度和整体质量上均优于现有开源数据集；OpenVE-Edit在OpenVE-Bench上取得新SOTA，超过所有先前开源模型，包括一个14B基线。

Conclusion: 提供了首个同时具备大规模与高质量的开源视频编辑指令数据与统一评测基准，并验证中小规模模型在优质数据支持下可达或超越更大模型的性能。

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [221] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

TL;DR: 提出FAE，将预训练视觉特征适配为低维生成潜空间：用一个简洁的特征自编码器，双解码器分别还原特征与生成图像，在扩散与流模型中均奏效，实现高质量且快速学习。


<details>
  <summary>Details</summary>
Motivation: 高质量生成通常在低维潜空间进行以提高效率与稳定性，而自监督预训练表征偏向高维、理解导向；两者在维度、噪声承载及语义假设上存在结构性不匹配，导致以往方法需复杂的目标与架构，限制了泛化与易用性。

Method: 提出Feature Auto-Encoder(FAE)：以预训练编码器(如DINO、SigLIP)获得高维特征；用一个极简瓶颈(可仅一层注意力)压缩为低维latent；配对两个深解码器——(1)特征解码器重建原始高维特征；(2)生成解码器以重建后的特征为输入驱动生成模型(扩散或正规化流)。此耦合训练既保持理解相关信息，又使latent适配生成需求。

Result: 在ImageNet 256×256等基准上，FAE与扩散模型结合在有CFG时FID达1.29(800轮)与1.70(80轮)，无CFG达到SOTA：FID 1.48(800轮)与2.08(80轮)。同时适用于类别条件与文生图任务，并可嵌入扩散与流模型。

Conclusion: FAE以简洁架构桥接高维理解特征与低维生成潜空间，通过双解码器训练实现信息保真与生成友好，获得接近或达到SOTA的质量与更快收敛，具备通用性与可插拔性。

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [222] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

TL;DR: UnityVideo提出一个统一的多模态条件视频生成框架，通过联合学习分割、骨架、DensePose、光流、深度等多模态与异构训练范式，实现更快收敛、强零样本泛化与更符合物理约束的视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成多依赖单一模态（如文本或单种结构先验），跨模态交互不足、模态多样性有限，导致对世界知识和物理约束的建模不充分，生成质量与泛化受限。

Method: 1) 动态加噪(Dynamic Noising)：统一不同训练范式，使来自不同模态与任务的信号在同一噪声学习框架下对齐；2) 模态切换器与情境学习(In-Context Learner)：通过可切换的模块化参数与上下文提示，在同一模型内实现多模态统一处理与条件对齐。并构建包含130万样本的大规模统一数据集，进行联合优化训练。

Result: 联合优化后，模型收敛更快；在未见数据上的零样本泛化显著提升；生成视频质量更高、时空一致性更好，并更契合物理世界约束。

Conclusion: 多模态联合与统一范式训练能够提升视频生成的世界感知与物理一致性。UnityVideo在质量、稳定性和泛化上优于以往单模态方法，并提供可复现的代码与数据。

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [223] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: 论文指出现有图像相似度度量（LPIPS、CLIP、DINO等）只关注可感知属性相似，忽略人类能够感知的“关系相似”。作者将“关系图像相似”形式化为：当图像内部视觉要素之间的关系/函数对应时，即使表面属性不同也相似；并构建11.4万张图片-匿名关系描述数据集，微调视觉-语言模型以度量关系相似，证明现有模型在该维度存在明显缺口。


<details>
  <summary>Details</summary>
Motivation: 人类能识别跨内容的关系类比（如地球与桃子的层次对应），而主流视觉相似度模型仅衡量表面属性相似，无法捕捉这种关系结构，影响检索、推理等应用。作者想建立能体现“关系逻辑”的图像表示与度量。

Method: 1) 定义关系相似：关注图像内部元素间关系/函数的对应性，而非外观属性。2) 构建114k图像-文本数据集：文本为“匿名化”关系描述（不指明具体实体，专注关系逻辑）。3) 基于该数据集微调视觉-语言模型，使其学习以关系逻辑为对齐目标，从而在嵌入空间拉近关系相似图像。

Result: 微调后的模型可度量并区分关系相似，即使图像在外观上差异明显；在多种测试与应用场景中优于LPIPS、CLIP、DINO等感知/语义相似模型，揭示传统模型在关系维度的缺陷。

Conclusion: 提出并量化了“关系图像相似”概念，提供首个大规模匿名关系数据集与相应的视觉-语言微调方法，迈出以关系结构而非外观连接图像的第一步，同时展示了现实应用潜力与当前视觉计算的关键空白。

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [224] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Voxify3D：把3D网格优化与2D像素艺术监督结合，用可微两阶段流程把网格转为高保真体素艺术，兼顾语义、几何抽象与离散调色板一致性。


<details>
  <summary>Details</summary>
Motivation: 自动从3D网格生成“像素化”的体素艺术很难：要在极度离散化下保留语义，又要保持像素级（无透视）对齐与受限调色板的色彩一致；现有方法要么几何过度简化，要么达不到像素艺术的调色板/像素精度。

Method: 可微两阶段框架：1）正交投影的像素艺术监督，去除透视畸变以实现体素-像素精确对齐；2）基于patch的CLIP对齐，在不同离散化水平上保持语义；3）受调色板约束的Gumbel-Softmax量化，使在离散色彩空间中端到端可微优化，并可控调色板策略。通过体渲染将体素外观与2D像素监督关联。

Result: 在多样角色与不同抽象强度（2-8色、20x-50x分辨率）下表现优于现有方法，报告CLIP-IQA 37.12、用户偏好 77.90%。

Conclusion: 三要素协同（正交像素监督+patch CLIP+调色板量化）解决了体素艺术生成中的语义保持、像素美学和离散优化难题，实现可控抽象与高主观偏好，适合将网格自动转化为高质量体素艺术。

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [225] [Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules](https://arxiv.org/abs/2512.06575)
*Fariza Dahes*

Main category: eess.IV

TL;DR: 验证并拓展一套用于医学影像分类的方法框架：在乳腺X光上评估改进版ConvNeXt Tiny（加入GAGM、SEVector、FSL），发现GAGM与SEVector显著提升判别力并降低恶性漏检，但FSL无明显收益；并补充多指标评估、可解释性与交互式仪表盘。


<details>
  <summary>Details</summary>
Motivation: 原框架在阿尔茨海默MRI、且CPU友好设定下已见成效，但其可迁移性与通用性尚未在乳腺X光分类上验证。乳腺癌筛查强调降低恶性漏诊、提升类别可分性，需要验证与增强该框架并提供临床可解释与交互工具。

Method: 基于Kaggle汇合集（INbreast、MIAS、DDSM）进行乳腺X光分类实验；对比基线CNN、ConvNeXt Tiny、InceptionV3等骨干，并在这些骨干上集成GAGM（GAP+GMP融合）与轻量通道注意力SEVector，以及特征平滑损失FSL；采用多指标评估（宏F1、各类召回方差、ROC/AUC），辅以Grad-CAM解释与临床交互式仪表盘。

Result: GAGM与SEVector稳定提升特征判别性并减少假阴性，特别是在恶性类别上；在本任务与设定下，FSL未带来可测改进，提示其效果依赖架构与算力假设。

Conclusion: 该框架在乳腺X光任务上部分验证：结构与注意力改造有效、FSL不稳健；补充的多维评估与可解释性工具有助临床应用。未来需探索更有效的类内紧致与类间分离策略，以进一步提升良恶性区分能力。

Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.

</details>
