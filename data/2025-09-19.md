<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Class-invariant Test-Time Augmentation for Domain Generalization](https://arxiv.org/abs/2509.14420)
*Zhicheng Lin,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 提出一种轻量级的“类别不变”测试时增强（CI-TTA），通过弹性与网格形变生成同类变体、用置信度过滤并聚合预测，在PACS与Office-Home上对多种DG算法与骨干一致提升。


<details>
  <summary>Details</summary>
Motivation: 深度模型在分布偏移下性能显著下降，现有DG方法常需多域联合训练或昂贵的测试时自适应。作者希望通过成本更低、易集成的方法在测试阶段提升对未见域的泛化。

Method: 在测试时对每张输入图像施加弹性变形与网格形变，保证语义类别不变；对得到的多重预测采用置信度引导的过滤策略剔除不可靠输出，再对剩余预测进行聚合，得到最终决策。该策略可插件式应用于不同DG算法与网络骨干。

Result: 在PACS与Office-Home数据集上，CI-TTA在多种域泛化基线与不同骨干网络上带来一致性能增益，说明方法有效且通用。

Conclusion: 无需昂贵的训练或测试时自适应，利用类别不变的轻量增强与置信度过滤即可稳健提升跨域泛化；方法通用、易部署。

Abstract: Deep models often suffer significant performance degradation under
distribution shifts. Domain generalization (DG) seeks to mitigate this
challenge by enabling models to generalize to unseen domains. Most prior
approaches rely on multi-domain training or computationally intensive test-time
adaptation. In contrast, we propose a complementary strategy: lightweight
test-time augmentation. Specifically, we develop a novel Class-Invariant
Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple
variants of each input image through elastic and grid deformations that
nevertheless belong to the same class as the original input. Their predictions
are aggregated through a confidence-guided filtering scheme that remove
unreliable outputs, ensuring the final decision relies on consistent and
trustworthy cues. Extensive Experiments on PACS and Office-Home datasets
demonstrate consistent gains across different DG algorithms and backbones,
highlighting the effectiveness and generality of our approach.

</details>


### [2] [AToken: A Unified Tokenizer for Vision](https://arxiv.org/abs/2509.14476)
*Jiasen Lu,Liangchen Song,Mingze Xu,Byeongjoo Ahn,Yanjun Wang,Chen Chen,Afshin Dehghan,Yinfei Yang*

Main category: cs.CV

TL;DR: AToken提出首个统一视觉tokenizer，在图像、视频、3D三类资产上同时兼顾高保真重建与语义理解，将输入编码到共享4D潜空间，依托纯Transformer与4D旋转位置编码、无对抗感知+Gram损失和渐进式课程训练，并支持连续/离散tokens，在多项基准与下游生成/理解任务上取得竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉tokenizer多为单一模态、在重建或理解二者择一，难以统一多模态表征且兼顾重建质量与语义对齐。作者希望构建一个跨图像/视频/3D、可服务于生成与理解的统一token化基础设施，以支撑下一代多模态AI系统。

Method: 1) 统一4D潜空间编码（空间x,y,z/时间t维度），输入为图像、视频、3D数据。2) 纯Transformer骨干，配4D旋转位置编码支持任意分辨率与时长。3) 训练目标无对抗：感知损失+Gram矩阵损失，稳定训练并提升重建。4) 渐进式课程：从单图像到视频再到3D逐步扩展；同时学习连续与离散潜表示。

Result: 图像：rFID 0.21，ImageNet线性/微调准确率82.2%；视频：rFVD 3.01，MSRVTT检索32.6%；3D：PSNR 28.19，分类准确率90.9%。在多项生成（图像/文本到视频/图到3D）与理解（多模态LLM）下游任务上具竞争力。

Conclusion: AToken将重建与理解、单帧与时序与3D统一到一个tokenizer与4D潜空间中，展示了统一视觉token化对多模态生成与理解任务的广泛适用性，为构建下一代多模态系统奠定基础。

Abstract: We present AToken, the first unified visual tokenizer that achieves both
high-fidelity reconstruction and semantic understanding across images, videos,
and 3D assets. Unlike existing tokenizers that specialize in either
reconstruction or understanding for single modalities, AToken encodes these
diverse visual inputs into a shared 4D latent space, unifying both tasks and
modalities in a single framework. Specifically, we introduce a pure transformer
architecture with 4D rotary position embeddings to process visual inputs of
arbitrary resolutions and temporal durations. To ensure stable training, we
introduce an adversarial-free training objective that combines perceptual and
Gram matrix losses, achieving state-of-the-art reconstruction quality. By
employing a progressive training curriculum, AToken gradually expands from
single images, videos, and 3D, and supports both continuous and discrete latent
tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01
rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%
classification accuracy for 3D. In downstream applications, AToken enables both
visual generation tasks (e.g., image generation with continuous and discrete
tokens, text-to-video generation, image-to-3D synthesis) and understanding
tasks (e.g., multimodal LLMs), achieving competitive performance across all
benchmarks. These results shed light on the next-generation multimodal AI
systems built upon unified visual tokenization.

</details>


### [3] [MemEvo: Memory-Evolving Incremental Multi-view Clustering](https://arxiv.org/abs/2509.14544)
*Zisen Kong,Bo Zhong,Pengyuan Li,Dongxia Chang,Yiming Wang*

Main category: cs.CV

TL;DR: 提出MemEvo，一种受海马-前额叶协同记忆机制启发的增量多视图聚类方法，以在新增视图场景中兼顾稳定性与可塑性，显著提升知识保持与聚类性能。


<details>
  <summary>Details</summary>
Motivation: 增量多视图聚类在不断接入新视图时面临稳定-可塑两难：既要快速适应新信息（可塑性），又要避免遗忘历史知识（稳定性）。现有方法在处理新增视图、对齐跨视图结构与防止灾难性遗忘方面仍不足。

Method: MemEvo包含三大模块：1）海马体启发的视图对齐模块，通过对齐连续表示中的结构获取新视图的增益信息；2）认知遗忘机制，模拟人类记忆衰退模式，自适应调节历史知识权重；3）前额叶启发的知识巩固记忆模块，利用时间张量稳定性逐步巩固历史知识。三者协同在视图数量增长的场景下实现稳定-可塑平衡。

Result: 在多组数据上的广泛实验显示，MemEvo在增量视图聚类任务上显著优于现有SOTA，表现出强知识保持与更稳定的聚类性能（具体数值未在摘要中给出）。

Conclusion: 通过神经科学启发的对齐、遗忘与巩固机制融合，MemEvo在增量多视图聚类中有效缓解SPD，兼顾适应新视图和保留长期知识，具有明显的实践与研究价值。

Abstract: Incremental multi-view clustering aims to achieve stable clustering results
while addressing the stability-plasticity dilemma (SPD) in incremental views.
At the core of SPD is the challenge that the model must have enough plasticity
to quickly adapt to new data, while maintaining sufficient stability to
consolidate long-term knowledge and prevent catastrophic forgetting. Inspired
by the hippocampal-prefrontal cortex collaborative memory mechanism in
neuroscience, we propose a Memory-Evolving Incremental Multi-view Clustering
method (MemEvo) to achieve this balance. First, we propose a
hippocampus-inspired view alignment module that captures the gain information
of new views by aligning structures in continuous representations. Second, we
introduce a cognitive forgetting mechanism that simulates the decay patterns of
human memory to modulate the weights of historical knowledge. Additionally, we
design a prefrontal cortex-inspired knowledge consolidation memory module that
leverages temporal tensor stability to gradually consolidate historical
knowledge. By integrating these modules, MemEvo achieves strong knowledge
retention capabilities in scenarios with a growing number of views. Extensive
experiments demonstrate that MemEvo exhibits remarkable advantages over
existing state-of-the-art methods.

</details>


### [4] [Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution](https://arxiv.org/abs/2509.14550)
*Penghao Rao,Tieyong Zeng*

Main category: cs.CV

TL;DR: 提出一种边缘引导的注意力与轻量残差架构相结合的SISR方法，通过边缘-特征联合调制提升结构清晰度与感知质量，并在不增加复杂度的情况下优于SRGAN/ESRGAN和以往边缘注意力基线。


<details>
  <summary>Details</summary>
Motivation: SISR从单张低分辨率图像恢复高频与结构信息高度不适定。已有做法把边缘先验或注意力“外挂”到复杂骨干上，融合方式零散，易引入冗余与不稳定，结构增益有限。因此需要一种参数高效、稳定、能显式强化结构/边缘区域的机制。

Method: 提出边缘引导注意力机制：将边缘特征与中间特征联合编码，生成自适应调制图，对特征进行归一化与重加权，放大结构显著区域、抑制伪纹理；并将其融入轻量级残差网络。训练采用复合损失：像素损失+感知损失+对抗损失，以平衡保真度、感知真实感与训练稳定性。

Result: 在标准SISR基准上，相比SRGAN、ESRGAN及既有边缘注意力方法，在相近模型复杂度下，结构锐度和感知质量持续提升，表明方法在边缘/结构保真与感知效果上更优。

Conclusion: 边缘条件调制是一条参数高效的路径，可稳定地引入边缘先验并强化结构保真，无需更深或过度参数化的网络，即可推进感知型超分辨性能。

Abstract: Single-image super-resolution (SISR) remains highly ill-posed because
recovering structurally faithful high-frequency content from a single
low-resolution observation is ambiguous. Existing edge-aware methods often
attach edge priors or attention branches onto increasingly complex backbones,
yet ad hoc fusion frequently introduces redundancy, unstable optimization, or
limited structural gains. We address this gap with an edge-guided attention
mechanism that derives an adaptive modulation map from jointly encoded edge
features and intermediate feature activations, then applies it to normalize and
reweight responses, selectively amplifying structurally salient regions while
suppressing spurious textures. In parallel, we integrate this mechanism into a
lightweight residual design trained under a composite objective combining
pixel-wise, perceptual, and adversarial terms to balance fidelity, perceptual
realism, and training stability. Extensive experiments on standard SISR
benchmarks demonstrate consistent improvements in structural sharpness and
perceptual quality over SRGAN, ESRGAN, and prior edge-attention baselines at
comparable model complexity. The proposed formulation provides (i) a
parameter-efficient path to inject edge priors, (ii) stabilized adversarial
refinement through a tailored multiterm loss, and (iii) enhanced edge fidelity
without resorting to deeper or heavily overparameterized architectures. These
results highlight the effectiveness of principled edge-conditioned modulation
for advancing perceptual super-resolution.

</details>


### [5] [Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model](https://arxiv.org/abs/2509.14560)
*Zhaonan Wang,Manyi Li,ShiQing Xin,Changhe Tu*

Main category: cs.CV

TL;DR: 提出一种基于得分扩散模型的自适应迭代点云去噪：先估计噪声变化，生成自适应步长的去噪日程，结合特制网络与两阶段采样训练，实现特征与梯度融合，较现有方法更好地平滑且保边保细节，并在合成与实采数据上定量定性领先。


<details>
  <summary>Details</summary>
Motivation: 现有深度方法通常固定重复若干次迭代去噪，但不同强度/模式的噪声需要不同的迭代策略与步长，缺乏高效统一的自适应安排，导致效率与质量不稳定。

Method: 以score-based diffusion为核心：1) 对输入噪声点云估计噪声变化，生成自适应去噪日程（自适应步长与迭代次数）；2) 依据该日程迭代调用训练网络更新点位置；3) 设计专用网络架构并在训练中采用“两阶段采样策略”，实现特征融合与梯度融合，从而更适配迭代去噪。

Result: 在多种噪声模式的合成数据与真实扫描数据上，较SOTA方法在定量指标与视觉效果均有提升，得到更干净、光滑且能更好保留形状边界与细节的点云。

Conclusion: 自适应步长的迭代扩散去噪结合特制网络与两阶段采样，可有效适配不同噪声水平/模式，取得稳定且优于现有方法的去噪效果。

Abstract: Point cloud denoising task aims to recover the clean point cloud from the
scanned data coupled with different levels or patterns of noise. The recent
state-of-the-art methods often train deep neural networks to update the point
locations towards the clean point cloud, and empirically repeat the denoising
process several times in order to obtain the denoised results. It is not clear
how to efficiently arrange the iterative denoising processes to deal with
different levels or patterns of noise. In this paper, we propose an adaptive
and iterative point cloud denoising method based on the score-based diffusion
model. For a given noisy point cloud, we first estimate the noise variation and
determine an adaptive denoising schedule with appropriate step sizes, then
invoke the trained network iteratively to update point clouds following the
adaptive schedule. To facilitate this adaptive and iterative denoising process,
we design the network architecture and a two-stage sampling strategy for the
network training to enable feature fusion and gradient fusion for iterative
denoising. Compared to the state-of-the-art point cloud denoising methods, our
approach obtains clean and smooth denoised point clouds, while preserving the
shape boundary and details better. Our results not only outperform the other
methods both qualitatively and quantitatively, but also are preferable on the
synthetic dataset with different patterns of noises, as well as the
real-scanned dataset.

</details>


### [6] [DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising](https://arxiv.org/abs/2509.14565)
*Li Gao,Hongyang Sun,Liu Liu,Yunhao Li,Yang Cai*

Main category: cs.CV

TL;DR: 提出DiffVL：用扩散模型把视觉定位转化为GPS去噪问题，结合视觉BEV特征与SD地图，对噪声GPS轨迹迭代精炼，达到亚米级精度、无需HD地图，优于BEV匹配基线。


<details>
  <summary>Details</summary>
Motivation: HD地图定位精度高但构建维护成本高，限制规模化；SD地图可扩展但现有方法多做BEV匹配，忽视随处可得却含多路径误差的GPS信号。需要一种既利用SD地图又充分挖掘GPS的信息、实现高精度且可扩展的定位方法。

Method: 将视觉定位重新表述为GPS去噪任务：以扩散模型为核心，联合建模GPS观测、SD地图（如OSM）和图像BEV特征。训练时学习反向去噪过程；推理时以噪声GPS轨迹为初始，条件于视觉与地图特征进行迭代扩散反演，逐步恢复真实位姿分布。不同于BEV匹配或Transformer配准，直接学习噪声扰动的逆过程。

Result: 在多个数据集上优于OrienterNet等BEV匹配基线，实现亚米级定位精度；无需依赖HD地图即可达到SOTA水平。

Conclusion: 将噪声GPS视为生成先验，结合扩散模型与SD地图/视觉条件，可实现可扩展的高精度定位，标志着从传统匹配范式向生成式去噪范式的转变。

Abstract: Accurate visual localization is crucial for autonomous driving, yet existing
methods face a fundamental dilemma: While high-definition (HD) maps provide
high-precision localization references, their costly construction and
maintenance hinder scalability, which drives research toward
standard-definition (SD) maps like OpenStreetMap. Current SD-map-based
approaches primarily focus on Bird's-Eye View (BEV) matching between images and
maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily
available, it suffers from multipath errors in urban environments. We propose
DiffVL, the first framework to reformulate visual localization as a GPS
denoising task using diffusion models. Our key insight is that noisy GPS
trajectory, when conditioned on visual BEV features and SD maps, implicitly
encode the true pose distribution, which can be recovered through iterative
diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g.,
OrienterNet) or transformer-based registration approaches, learns to reverse
GPS noise perturbations by jointly modeling GPS, SD map, and visual signals,
achieving sub-meter accuracy without relying on HD maps. Experiments on
multiple datasets demonstrate that our method achieves state-of-the-art
accuracy compared to BEV-matching baselines. Crucially, our work proves that
diffusion models can enable scalable localization by treating noisy GPS as a
generative prior-making a paradigm shift from traditional matching-based
methods.

</details>


### [7] [DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction](https://arxiv.org/abs/2509.14566)
*Leon Suarez-Rodriguez,Roman Jacome,Romario Gualdron-Hurtado,Ana Mantilla-Dulcey,Henry Arguello*

Main category: cs.CV

TL;DR: 提出DICE框架，将扩散模型的采样过程与两代理共识平衡（数据一致性与先验）融合，用于稀疏视角CT重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角CT因采样不足导致病态逆问题，传统迭代法依赖手工或学习式先验但难以刻画医学图像复杂结构；扩散模型具备强生成先验，但需与测量一致性结合以提升重建质量与稳健性。

Method: 引入两代理共识平衡（Consensus Equilibrium）：(i) 数据一致性代理，通过近端算子强制与测量匹配；(ii) 先验代理，由扩散模型在每个采样步估计“干净”图像。通过在采样过程中交替并平衡两者，实现先验能力与数据一致性的协同。

Result: 在180视角全采样中的稀疏设置下（15/30/60视角，含均匀与非均匀采样），DICE在CT重建质量上显著优于最先进基线，呈现更高质量与更强鲁棒性。

Conclusion: DICE成功将扩散先验与数据一致性通过共识平衡融合，在极稀疏视角CT中实现高质量重建，且对不同采样模式具有鲁棒性。

Abstract: Sparse-view computed tomography (CT) reconstruction is fundamentally
challenging due to undersampling, leading to an ill-posed inverse problem.
Traditional iterative methods incorporate handcrafted or learned priors to
regularize the solution but struggle to capture the complex structures present
in medical images. In contrast, diffusion models (DMs) have recently emerged as
powerful generative priors that can accurately model complex image
distributions. In this work, we introduce Diffusion Consensus Equilibrium
(DICE), a framework that integrates a two-agent consensus equilibrium into the
sampling process of a DM. DICE alternates between: (i) a data-consistency
agent, implemented through a proximal operator enforcing measurement
consistency, and (ii) a prior agent, realized by a DM performing a clean image
estimation at each sampling step. By balancing these two complementary agents
iteratively, DICE effectively combines strong generative prior capabilities
with measurement consistency. Experimental results show that DICE significantly
outperforms state-of-the-art baselines in reconstructing high-quality CT images
under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out
of a total of 180), demonstrating both its effectiveness and robustness.

</details>


### [8] [Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses](https://arxiv.org/abs/2509.14573)
*Takamasa Yamaguchi,Brian Kenji Iwana,Ryoma Bise,Shota Harada,Takumi Okuo,Kiyohito Tanaka,Kaito Shiku*

Main category: cs.CV

TL;DR: 提出一种针对溃疡性结肠炎(UC)严重程度估计的弱监督域自适应方法，利用目标域常规可得的患者级诊断作为弱标签，通过共享聚合token和“最大严重度三元组损失”实现跨域按类别分布对齐，在存在设备/医院差异的域迁移场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有UC严重度估计受跨医院设备与流程差异导致的域偏移影响，现有DA方法或缺乏目标域监督、或标注成本高，难以充分利用目标域可获得但弱的监督信号（如患者级诊断）。

Method: 提出弱监督域自适应框架：1）使用Shared Aggregation Tokens对源域与目标域进行类别级别的分布对齐；2）设计Max-Severity Triplet Loss，利用“患者级诊断由患者内最严重片段决定”的先验，将患者内最严重区域作为正例/负例构建三元组，促进类内聚合与类间分离；3）以患者级弱标签驱动对齐，无需像素/片段级精细标注。

Result: 在存在显著域偏移的实验设置中，该方法在UC严重度估计任务上优于多种对比域自适应基线，表现出更高的准确性/鲁棒性。

Conclusion: 利用临床常规可得的患者级弱标签，结合共享聚合token与最大严重度三元组损失，可有效缓解UC严重度估计中的跨域偏移，提升跨域泛化表现，降低目标域精细标注成本。

Abstract: The development of methods to estimate the severity of Ulcerative Colitis
(UC) is of significant importance. However, these methods often suffer from
domain shifts caused by differences in imaging devices and clinical settings
across hospitals. Although several domain adaptation methods have been proposed
to address domain shift, they still struggle with the lack of supervision in
the target domain or the high cost of annotation. To overcome these challenges,
we propose a novel Weakly Supervised Domain Adaptation method that leverages
patient-level diagnostic results, which are routinely recorded in UC diagnosis,
as weak supervision in the target domain. The proposed method aligns class-wise
distributions across domains using Shared Aggregation Tokens and a Max-Severity
Triplet Loss, which leverages the characteristic that patient-level diagnoses
are determined by the most severe region within each patient. Experimental
results demonstrate that our method outperforms comparative DA approaches,
improving UC severity estimation in a domain-shifted setting.

</details>


### [9] [Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark](https://arxiv.org/abs/2509.14574)
*Rashid Mushkani*

Main category: cs.CV

TL;DR: 提出一个小型基准，用100张蒙特利尔街景（实拍与高保真合成各半），用社区参与者在30个维度上的标注来评测VLM零样本理解城市场景的能力；结果显示模型更擅长客观可见属性而非主观感受，最佳模型在多标签任务上均值Jaccard为0.48，并发布数据与评测工具以支持可重复与不确定性感知的参与式城市分析。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在一般图文理解上进展迅速，但针对城市规划/设计所需的“人如何读城市”的主客观维度理解缺少专门评测，且需要能反映人群多样性的、可重复且考虑不确定性的基准。

Method: 构建包含100张蒙特利尔街景（照片与拟真合成各50）的数据集；邀请来自7个社区群体的12名参与者，提供230份在30个维度（客观物理属性与主观印象）上的标注；法语标注规范化为英语；设定结构化提示与确定性解析器，对7个VLM进行零样本评测；单选项用准确率，多标签用Jaccard；人类一致性用Krippendorff’s alpha与成对Jaccard进行度量；对实拍与合成图分别分析。

Result: 模型在可见、客观属性上的对齐更强，对主观审美/感受维度较弱；最佳系统（claude-sonnet）在多标签项达到宏平均0.31与平均Jaccard 0.48；人类一致性越高的维度，模型得分也越高；合成图像使分数略降。

Conclusion: 该基准可用于更准确、可重复、具不确定性意识地评估VLM在参与式城市分析中的表现；当前VLM仍与人类主观感知存在差距，尤其在主观评价维度；公开基准、提示与评测框架以推动后续研究。

Abstract: Understanding how people read city scenes can inform design and planning. We
introduce a small benchmark for testing vision-language models (VLMs) on urban
perception using 100 Montreal street images, evenly split between photographs
and photorealistic synthetic scenes. Twelve participants from seven community
groups supplied 230 annotation forms across 30 dimensions mixing physical
attributes and subjective impressions. French responses were normalized to
English. We evaluated seven VLMs in a zero-shot setup with a structured prompt
and deterministic parser. We use accuracy for single-choice items and Jaccard
overlap for multi-label items; human agreement uses Krippendorff's alpha and
pairwise Jaccard. Results suggest stronger model alignment on visible,
objective properties than subjective appraisals. The top system (claude-sonnet)
reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human
agreement coincides with better model scores. Synthetic images slightly lower
scores. We release the benchmark, prompts, and harness for reproducible,
uncertainty-aware evaluation in participatory urban analysis.

</details>


### [10] [Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression](https://arxiv.org/abs/2509.14591)
*Xuan Deng,Xiandong Meng,Longguang Wang,Tiange Zhang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 提出FMT框架，用隐式特征对齐替代显式运动向量，实现动态点云高效压缩，并配合随机访问双向参考与分层编码，达到更低BD-Rate与更快编解码。


<details>
  <summary>Details</summary>
Motivation: 动态点云压缩依赖精确的运动估计与补偿，但点云结构不规则、局部变化大，显式运动矢量难以刻画复杂时空动态且编码冗余，难以充分利用时间相关性与并行性。

Method: 以Feature-aligned Motion Transformation(FMT)为核心：在潜空间进行时空对齐，利用对齐后的特征作为条件上下文进行熵编码，避免显式运动矢量。并设计随机访问(RA)参考策略，支持双向参考与分层编码，使帧级并行压缩成为可能。

Result: 在大量实验中，相较D-DPCC与AdaDPCC，编码与解码效率均提升；客观压缩指标上，BD-Rate分别降低约20%与9.4%。

Conclusion: 隐式特征对齐的FMT可更好建模连续时序变化，配合RA双向参考与分层并行策略，实现兼顾压缩率与速度的动态点云压缩，优于现有方法。

Abstract: Dynamic point clouds are widely used in applications such as immersive
reality, robotics, and autonomous driving. Efficient compression largely
depends on accurate motion estimation and compensation, yet the irregular
structure and significant local variations of point clouds make this task
highly challenging. Current methods often rely on explicit motion estimation,
whose encoded vectors struggle to capture intricate dynamics and fail to fully
exploit temporal correlations. To overcome these limitations, we introduce a
Feature-aligned Motion Transformation (FMT) framework for dynamic point cloud
compression. FMT replaces explicit motion vectors with a spatiotemporal
alignment strategy that implicitly models continuous temporal variations, using
aligned features as temporal context within a latent-space conditional encoding
framework. Furthermore, we design a random access (RA) reference strategy that
enables bidirectional motion referencing and layered encoding, thereby
supporting frame-level parallel compression. Extensive experiments demonstrate
that our method surpasses D-DPCC and AdaDPCC in both encoding and decoding
efficiency, while also achieving BD-Rate reductions of 20% and 9.4%,
respectively. These results highlight the effectiveness of FMT in jointly
improving compression efficiency and processing performance.

</details>


### [11] [HybridMamba: A Dual-domain Mamba for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.14609)
*Weitong Wu,Zhaohu Xing,Jing Gong,Qin Peng,Lei Zhu*

Main category: cs.CV

TL;DR: 提出HybridMamba用于3D医学图像分割，通过结合轴向扫描与局部自适应路径、以及空间-频域门控模块，在保持全局依赖建模优势的同时强化局部结构信息；在MRI与CT上优于SOTA，并发布多中心肺癌CT数据。


<details>
  <summary>Details</summary>
Motivation: CNN建模长程依赖受限，Transformer虽能捕获全局但在高分辨率体数据上计算/内存代价高；Mamba在长程依赖与效率上更佳，但过度强调全局可能削弱局部结构，导致边界模糊与区域形变，需要一种同时兼顾局部与全局的高效架构。

Method: 提出HybridMamba：1) 特征扫描策略，沿轴向遍历与局部自适应两条路径逐步融合，协调局部—全局表征；2) 结合空间与频域分析的门控模块，实现更全面的上下文建模。另构建多中心肺癌CT数据集用于评估。

Result: 在MRI与CT数据集上，HybridMamba显著优于现有SOTA方法（摘要未给出具体指标与提升幅度）。

Conclusion: 融合轴向扫描与局部自适应路径并配合空间-频域门控的HybridMamba可在保持效率的同时兼顾全局与局部信息，缓解边界与形变问题，实现3D医学分割的性能提升；新多中心数据集亦促进评测与研究。

Abstract: In the domain of 3D biomedical image segmentation, Mamba exhibits the
superior performance for it addresses the limitations in modeling long-range
dependencies inherent to CNNs and mitigates the abundant computational overhead
associated with Transformer-based frameworks when processing high-resolution
medical volumes. However, attaching undue importance to global context modeling
may inadvertently compromise critical local structural information, thus
leading to boundary ambiguity and regional distortion in segmentation outputs.
Therefore, we propose the HybridMamba, an architecture employing dual
complementary mechanisms: 1) a feature scanning strategy that progressively
integrates representations both axial-traversal and local-adaptive pathways to
harmonize the relationship between local and global representations, and 2) a
gated module combining spatial-frequency analysis for comprehensive contextual
modeling. Besides, we collect a multi-center CT dataset related to lung cancer.
Experiments on MRI and CT datasets demonstrate that HybridMamba significantly
outperforms the state-of-the-art methods in 3D medical image segmentation.

</details>


### [12] [Enhancing Feature Fusion of U-like Networks with Dynamic Skip Connections](https://arxiv.org/abs/2509.14610)
*Yue Cao,Quansong He,Kaishen Wang,Jianlong Xiong,Tao He*

Main category: cs.CV

TL;DR: 提出可插拔的动态跳跃连接（DSC）模块，含TTT与DMSK两组件，分别在推理时自适应特征与多尺度核选择，缓解U形网络跳跃连接的跨特征与内特征约束，显著提升多种U-like架构的医学图像分割效果。


<details>
  <summary>Details</summary>
Motivation: 传统U形网络的跳跃连接是静态融合，无法根据特征内容自适应地传递与融合信息（跨特征约束）；同时对多尺度特征交互建模不足，难以有效聚合全局上下文（内特征约束）。需要一种既能在推理阶段自适应调整特征，又能动态处理多尺度上下文的机制。

Method: 设计动态跳跃连接（DSC）块，包含：1）测试时训练（TTT）模块：在推理阶段对隐藏表示进行小步自适应更新，使跨层信息传递具备内容感知；2）动态多尺度卷积核（DMSK）模块：依据全局上下文自适应选择核大小，增强多尺度特征集成能力。DSC与网络无关，可插入CNN、Transformer、混合与Mamba等U-like结构。

Result: 在多类U-like网络与数据集上进行广泛实验，显示DSC作为即插即用模块可稳定提升分割性能，相较基线具有一致增益（摘要未给出具体数值）。

Conclusion: DSC通过推理期自适应与动态多尺度建模，缓解传统跳跃连接的两类约束，提升跨层连接与上下文融合能力，且具备架构无关与可插拔性，在多种U-like分割模型中验证有效。

Abstract: U-like networks have become fundamental frameworks in medical image
segmentation through skip connections that bridge high-level semantics and
low-level spatial details. Despite their success, conventional skip connections
exhibit two key limitations: inter-feature constraints and intra-feature
constraints. The inter-feature constraint refers to the static nature of
feature fusion in traditional skip connections, where information is
transmitted along fixed pathways regardless of feature content. The
intra-feature constraint arises from the insufficient modeling of multi-scale
feature interactions, thereby hindering the effective aggregation of global
contextual information. To overcome these limitations, we propose a novel
Dynamic Skip Connection (DSC) block that fundamentally enhances cross-layer
connectivity through adaptive mechanisms. The DSC block integrates two
complementary components. (1) Test-Time Training (TTT) module. This module
addresses the inter-feature constraint by enabling dynamic adaptation of hidden
representations during inference, facilitating content-aware feature
refinement. (2) Dynamic Multi-Scale Kernel (DMSK) module. To mitigate the
intra-feature constraint, this module adaptively selects kernel sizes based on
global contextual cues, enhancing the network capacity for multi-scale feature
integration. The DSC block is architecture-agnostic and can be seamlessly
incorporated into existing U-like network structures. Extensive experiments
demonstrate the plug-and-play effectiveness of the proposed DSC block across
CNN-based, Transformer-based, hybrid CNN-Transformer, and Mamba-based U-like
networks.

</details>


### [13] [LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition](https://arxiv.org/abs/2509.14619)
*Feng Ding,Haisheng Fu,Soroush Oraki,Jie Liang*

Main category: cs.CV

TL;DR: 提出LSTC-MDA统一框架，通过改进时序建模与数据增强，缓解骨架动作识别中样本稀缺与长短期依赖建模难题；在NTU60/120与NW-UCLA上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 骨架动作识别面临两大痛点：标注数据稀缺导致模型泛化弱；传统时序卷积/降采样难以同时捕获短期细节与长期依赖，stride-2易丢失关键长程信息。

Method: 1) LSTC模块：并行短期与长期两条分支，后以学习到的相似度权重进行对齐与自适应融合，缓解常规降采样带来的长程线索丢失。2) 数据增强MDA：在已有JMDA基础上增加输入级Additive Mixup，并限制只在同一相机视角内混合，提升样本多样性同时避免分布偏移。3) 通过消融验证各组件贡献。

Result: 在多个基准上取得SOTA：NTU60 X-Sub 94.1%、X-View 97.5%；NTU120 X-Sub 90.4%、X-Set 92.0%；NW-UCLA 97.2%。

Conclusion: 统一框架同时强化时序建模与数据多样性；LSTC有效保留长程信息，视角约束的Additive Mixup提升泛化且避免分布漂移；整体显著优于现有方法。

Abstract: Skeleton-based action recognition faces two longstanding challenges: the
scarcity of labeled training samples and difficulty modeling short- and
long-range temporal dependencies. To address these issues, we propose a unified
framework, LSTC-MDA, which simultaneously improves temporal modeling and data
diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC)
module with parallel short- and long-term branches, these two feature branches
are then aligned and fused adaptively using learned similarity weights to
preserve critical long-range cues lost by conventional stride-2 temporal
convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an
Additive Mixup at the input level, diversifying training samples and
restricting mixup operations to the same camera view to avoid distribution
shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves
state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4%
and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code:
https://github.com/xiaobaoxia/LSTC-MDA.

</details>


### [14] [MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks](https://arxiv.org/abs/2509.14638)
*Mingsong Li,Lin Liu,Hongjun Wang,Haoxing Chen,Xijun Gu,Shizhan Liu,Dong Gong,Junbo Zhao,Zhenzhong Lan,Jianguo Li*

Main category: cs.CV

TL;DR: MultiEdit 提出一个包含10.7万+高质量样本的指令驱动图像编辑数据集，覆盖6类任务、18种非风格编辑与38种风格迁移操作，并用两阶段多模态LLM管线生成指令与高保真编辑图。用该数据集微调开源模型后，在新基准上显著提升复杂编辑性能，同时不损伤标准基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有IBIE数据集编辑类型少、样本规模有限且含噪声图文对，导致模型在复杂编辑（如人物参照、图中文字编辑、复杂风格迁移）上能力不足并易受偏差影响。

Method: 构建MultiEdit数据集：1）设计覆盖广的编辑类别体系（6任务、18非风格编辑、38风格迁移）；2）采用两套MLLM：一套根据原图生成视觉自适应的编辑指令，另一套产出高保真的编辑结果；3）据此形成训练集与测试基准（MultiEdit-Train/Test）；并以开源基础模型在该数据上微调与评测。

Result: 在MultiEdit-Test基准上，使用MultiEdit-Train微调的开源模型在复杂编辑任务上显著优于基线；同时在标准编辑基准上维持或基本保持原有性能。

Conclusion: MultiEdit是一个大规模、高质量且覆盖复杂编辑场景的IBIE数据集，可有效提升模型在多样且困难编辑任务上的能力，并为后续研究提供新的评测与训练资源；数据已开源至Hugging Face。

Abstract: Current instruction-based image editing (IBIE) methods struggle with
challenging editing tasks, as both editing types and sample counts of existing
datasets are limited. Moreover, traditional dataset construction often contains
noisy image-caption pairs, which may introduce biases and limit model
capabilities in complex editing scenarios. To address these limitations, we
introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality
image editing samples. It encompasses 6 challenging editing tasks through a
diverse collection of 18 non-style-transfer editing types and 38 style transfer
operations, covering a spectrum from sophisticated style transfer to complex
semantic operations like person reference editing and in-image text editing. We
employ a novel dataset construction pipeline that utilizes two multi-modal
large language models (MLLMs) to generate visual-adaptive editing instructions
and produce high-fidelity edited images, respectively. Extensive experiments
demonstrate that fine-tuning foundational open-source models with our
MultiEdit-Train set substantially improves models' performance on sophisticated
editing tasks in our proposed MultiEdit-Test benchmark, while effectively
preserving their capabilities on the standard editing benchmark. We believe
MultiEdit provides a valuable resource for advancing research into more diverse
and challenging IBIE capabilities. Our dataset is available at
https://huggingface.co/datasets/inclusionAI/MultiEdit.

</details>


### [15] [Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model](https://arxiv.org/abs/2509.14664)
*Shinnosuke Hirano,Yuiga Wada,Tsumugi Iida,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出一种针对视觉基础模型的可解释性生成方法，通过引入Attention Lattice Adapter（ALA）与Alternating Epoch Architect（AEA），在生成视觉解释的同时部分更新模型参数，提高适应性与解释质量；在CUB-200-2011与ImageNet-S上显著优于基线，IoU等指标全面提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉解释方法在复杂、庞大的视觉基础模型上适配性差，常需手动层选择且注意力区域过小，导致解释不稳定、欠全面。需要一种可在大模型上自动适配、并提升解释覆盖度与可靠性的机制。

Method: 提出两大机制：1）Attention Lattice Adapter（ALA）：在多层注意力结构上以“格”形式自适应插入/适配，无需手动选层，允许在解释过程中部分更新参数以增强可解释性；2）Alternating Epoch Architect（AEA）：采用隔代（隔一个epoch）更新ALA参数的训练日程，缓解注意力过于集中的问题，扩大合理注意区域。整体方法在视觉基础模型上以解释生成为目标并联动参数微调。

Result: 在CUB-200-2011与ImageNet-S上，相比多种基线，IoU、insertion、deletion及其综合分数均明显提升；其中CUB-200-2011的mean IoU最高提升53.2点，显示出显著优势。

Conclusion: 所提ALA+AEA框架在无需复杂手工配置的前提下，为视觉基础模型提供高质量、适应性强的视觉解释，并通过交替更新策略扩大注意范围、提升解释指标，优于现有基线。

Abstract: In this study, we consider the problem of generating visual explanations in
visual foundation models. Numerous methods have been proposed for this purpose;
however, they often cannot be applied to complex models due to their lack of
adaptability. To overcome these limitations, we propose a novel explanation
generation method in visual foundation models that is aimed at both generating
explanations and partially updating model parameters to enhance
interpretability. Our approach introduces two novel mechanisms: Attention
Lattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanism
simplifies the process by eliminating the need for manual layer selection, thus
enhancing the model's adaptability and interpretability. Moreover, the AEA
mechanism, which updates ALA's parameters every other epoch, effectively
addresses the common issue of overly small attention regions. We evaluated our
method on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our results
showed that our method outperformed the baseline methods in terms of mean
intersection over union (IoU), insertion score, deletion score, and
insertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets.
Notably, our best model achieved a 53.2-point improvement in mean IoU on the
CUB-200-2011 dataset compared with the baselines.

</details>


### [16] [DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images](https://arxiv.org/abs/2509.14685)
*Kazuma Nagata,Naoshi Kaneko*

Main category: cs.CV

TL;DR: 提出DACoN：融合基础模型语义与CNN空间细节的动漫线稿自动上色框架，支持任意数量参考图，显著提升颜色传播的稳健性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习上色方法在遮挡、姿态变化与视角变化下易失败，且多依赖少量（1-2张）参考图的匹配或复杂的Multiplex Transformer，限制了泛化与实用性。需要一种能在线稿中获取部件级语义、并可利用多参考的鲁棒上色方案。

Method: 利用基础模型（foundation models）提取低分辨率的部件级语义特征，即使在仅有线稿的情况下也能识别语义区域；并与CNN提取的高分辨率空间特征进行融合，实现细粒度且鲁棒的特征表示。摒弃依赖Multiplex Transformer的结构设计，框架天然支持任意数量的参考图像，进行多参考特征聚合与颜色迁移。

Result: 在定量与定性实验中，多参考设定带来显著收益，DACoN的上色质量优于现有方法（更准确的颜色一致性与边界细节、对遮挡与姿态/视角变化更稳健）。

Conclusion: 通过融合基础模型语义与CNN细节并解耦对参考数量的限制，DACoN在复杂场景下实现更稳健、精细的线稿自动上色，为动漫制作提供更高效可靠的工具。

Abstract: Automatic colorization of line drawings has been widely studied to reduce the
labor cost of hand-drawn anime production. Deep learning approaches, including
image/video generation and feature-based correspondence, have improved accuracy
but struggle with occlusions, pose variations, and viewpoint changes. To
address these challenges, we propose DACoN, a framework that leverages
foundation models to capture part-level semantics, even in line drawings. Our
method fuses low-resolution semantic features from foundation models with
high-resolution spatial features from CNNs for fine-grained yet robust feature
extraction. In contrast to previous methods that rely on the Multiplex
Transformer and support only one or two reference images, DACoN removes this
constraint, allowing any number of references. Quantitative and qualitative
evaluations demonstrate the benefits of using multiple reference images,
achieving superior colorization performance. Our code and model are available
at https://github.com/kzmngt/DACoN.

</details>


### [17] [FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction](https://arxiv.org/abs/2509.14739)
*Jinlong Fan,Bingyu Hu,Xingguang Li,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: FMGS-Avatar提出将2D高斯原语绑定到模板网格面，并结合大模型先验与选择性梯度隔离训练，以在单目视频中重建高保真、可动画的人体头像，实现更精确几何与外观并具一致语义渲染。


<details>
  <summary>Details</summary>
Motivation: 单目视频因视角与几何信息不足，难以获得高保真的可动画人体；3D高斯splatting虽高效但自由形态导致表面细节对齐差。需要一种既提升表示对齐与细节保持、又能补充单目信息匮乏的方案。

Method: 1) Mesh-Guided 2D Gaussian Splatting：将2D高斯原语直接附着在模板网格面上，限制其位置、朝向与运动，使其紧贴表面以提升几何对齐与细节。2) 利用大规模预训练基础模型（如Sapiens）的多模态先验进行蒸馏；针对多模态目标间的梯度冲突，提出协调训练与选择性梯度隔离，让不同损失只更新相关参数以避免互扰。3) 在共享规范（canonical）空间中进行表示与蒸馏，支持跨视角与跨姿态的一致渲染。

Result: 相较现有方法，重建质量显著提升：几何精度更高、外观保真度更好，并具备丰富语义信息。通过实验展示在新视角与新姿态下保持时空一致性。

Conclusion: 通过网格引导的2D高斯表示与基础模型先验的协调蒸馏，FMGS-Avatar有效克服单目信息不足与高斯表示自由度过高的问题，实现高保真、可动画的人体重建，并自然支持一致的新视角/新姿态渲染。

Abstract: Reconstructing high-fidelity animatable human avatars from monocular videos
remains challenging due to insufficient geometric information in single-view
observations. While recent 3D Gaussian Splatting methods have shown promise,
they struggle with surface detail preservation due to the free-form nature of
3D Gaussian primitives. To address both the representation limitations and
information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that
integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian
Splatting, where 2D Gaussian primitives are attached directly to template mesh
faces with constrained position, rotation, and movement, enabling superior
surface alignment and geometric detail preservation. Second, we leverage
foundation models trained on large-scale datasets, such as Sapiens, to
complement the limited visual cues from monocular videos. However, when
distilling multi-modal prior knowledge from foundation models, conflicting
optimization objectives can emerge as different modalities exhibit distinct
parameter sensitivities. We address this through a coordinated training
strategy with selective gradient isolation, enabling each loss component to
optimize its relevant parameters without interference. Through this combination
of enhanced representation and coordinated information distillation, our
approach significantly advances 3D monocular human avatar reconstruction.
Experimental evaluation demonstrates superior reconstruction quality compared
to existing methods, with notable gains in geometric accuracy and appearance
fidelity while providing rich semantic information. Additionally, the distilled
prior knowledge within a shared canonical space naturally enables spatially and
temporally consistent rendering under novel views and poses.

</details>


### [18] [Chain-of-Thought Re-ranking for Image Retrieval Tasks](https://arxiv.org/abs/2509.14746)
*Shangrong Wu,Yanghong Zhou,Yang Chen,Feng Zhang,P. Y. Mok*

Main category: cs.CV

TL;DR: 提出CoTRR：用多模态大模型（MLLM）的链式思维进行列表式重排，显著提升三类图像检索的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像检索多把MLLM仅用于评测或匹配打分，未让其直接参与排序，未充分利用其多模态推理与全局对比能力，导致性能欠佳与解释性不足。

Method: 设计三种提示：1) 列表式排名提示，让MLLM同时比较候选图像进行全局重排；2) 图像评估提示，衡量候选与查询的一致性；3) 查询分解提示，将查询拆为多个语义要素做细粒度对齐。整体流程：先检索得到候选→用查询分解生成要素→依据评估与列表式提示让MLLM链式思维推理，输出一致、可解释的排名。

Result: 在五个数据集、三类任务（TIR/CIR/Chat-IR）上取得SOTA；相较现有方法在多个基准上有明显提升，并提供可解释的重排理由。

Conclusion: 让MLLM以链式思维进行列表式重排，可实现全局比较、稳定一致的推理与可解释决策，显著提升多种图像检索任务性能；代码已开源。

Abstract: Image retrieval remains a fundamental yet challenging problem in computer
vision. While recent advances in Multimodal Large Language Models (MLLMs) have
demonstrated strong reasoning capabilities, existing methods typically employ
them only for evaluation, without involving them directly in the ranking
process. As a result, their rich multimodal reasoning abilities remain
underutilized, leading to suboptimal performance. In this paper, we propose a
novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.
Specifically, we design a listwise ranking prompt that enables MLLM to directly
participate in re-ranking candidate images. This ranking process is grounded in
an image evaluation prompt, which assesses how well each candidate aligns with
users query. By allowing MLLM to perform listwise reasoning, our method
supports global comparison, consistent reasoning, and interpretable
decision-making - all of which are essential for accurate image retrieval. To
enable structured and fine-grained analysis, we further introduce a query
deconstruction prompt, which breaks down the original query into multiple
semantic components. Extensive experiments on five datasets demonstrate the
effectiveness of our CoTRR method, which achieves state-of-the-art performance
across three image retrieval tasks, including text-to-image retrieval (TIR),
composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our
code is available at https://github.com/freshfish15/CoTRR .

</details>


### [19] [Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks](https://arxiv.org/abs/2509.14755)
*Ahmed Sheta,Mathias Zinnen,Aline Sindel,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 利用扩散模型合成数据增强，在历史艺术品中检测“气味相关”物体；缓解标注稀疏与类别极不平衡，实验证明能提升检测性能，少量数据也有效，规模化潜力大。


<details>
  <summary>Details</summary>
Motivation: 历史艺术作品中“气味”相关目标（如香炉、香囊等）识别困难：风格多变、需要极细粒度类别、标注稀疏且类别分布极不均衡。传统监督学习难以获得足够高质量标注，成本高。

Method: 探索基于扩散模型的合成数据生成与数据增强策略：用扩散模型生成包含目标类别的合成图像（及相应标注/伪标注），将其与真实小规模数据共同训练检测模型；比较多种扩散式增强方案并评估对检测性能的影响。

Result: 将合成数据纳入训练后，目标检测性能显著提升；在数据量较小情境下亦有效；不同扩散增强策略均有收益，且进一步扩大合成数据规模有望带来更多提升。

Conclusion: 大规模预训练的扩散模型能有效缓解小众领域的标注稀缺与类别不平衡问题，提升“气味相关”目标检测准确率；方法对小数据友好，具有良好的可扩展性与推广潜力。

Abstract: Finding smell references in historic artworks is a challenging problem.
Beyond artwork-specific challenges such as stylistic variations, their
recognition demands exceptionally detailed annotation classes, resulting in
annotation sparsity and extreme class imbalance. In this work, we explore the
potential of synthetic data generation to alleviate these issues and enable
accurate detection of smell-related objects. We evaluate several
diffusion-based augmentation strategies and demonstrate that incorporating
synthetic data into model training can improve detection performance. Our
findings suggest that leveraging the large-scale pretraining of diffusion
models offers a promising approach for improving detection accuracy,
particularly in niche applications where annotations are scarce and costly to
obtain. Furthermore, the proposed approach proves to be effective even with
relatively small amounts of data, and scaling it up provides high potential for
further enhancements.

</details>


### [20] [Frame Sampling Strategies Matter: A Benchmark for small vision language models](https://arxiv.org/abs/2509.14769)
*Marija Brkic,Anas Filali Razzouki,Yannis Tevissen,Khalil Guetari,Mounim A. El Yacoubi*

Main category: cs.CV

TL;DR: 提出首个在受控取帧策略下、对小型视频视觉语言模型进行帧精确评测的基准，验证并量化了现有视频评测中的取帧偏差，并开源可复现实验协议。


<details>
  <summary>Details</summary>
Motivation: 现有视频VLM评测常使用不同的取帧策略，导致结果受取帧偏差影响，难以公平比较模型视觉表征与QA能力；需要一个可控、可复现且无偏的评测框架。

Method: 构建“帧精确”的视频问答基准：在相同、受控的取帧策略（多种策略可配置）下，对一组最先进的小型VLM进行评测；系统化比较不同数据集、任务与取帧策略下的性能差异；提供开源基准代码与评测协议以确保可复现性。

Result: 实验证实现有基准存在显著取帧偏差；不同小型VLM对取帧策略呈现数据集依赖与任务依赖的表现差异；在统一策略下能够更公正地区分模型能力。

Conclusion: 视频VLM评测需采用标准化且数据集适配的取帧策略；所提基准和开源代码为社区提供了可复现、无偏的评测协议，以推动更公平、可靠的模型比较与后续研究。

Abstract: Comparing vision language models on videos is particularly complex, as the
performances is jointly determined by the model's visual representation
capacity and the frame-sampling strategy used to construct the input. Current
video benchmarks are suspected to suffer from substantial frame-sampling bias,
as models are evaluated with different frame selection strategies. In this
work, we propose the first frame-accurate benchmark of state-of-the-art small
VLMs for video question-answering, evaluated under controlled frame-sampling
strategies. Our results confirm the suspected bias and highlight both
data-specific and task-specific behaviors of SVLMs under different
frame-sampling techniques. By open-sourcing our benchmarking code, we provide
the community with a reproducible and unbiased protocol for evaluating video
VLMs and emphasize the need for standardized frame-sampling strategies tailored
to each benchmarking dataset in future research.

</details>


### [21] [A Real-Time Multi-Model Parametric Representation of Point Clouds](https://arxiv.org/abs/2509.14773)
*Yuan Gao,Wei Dong*

Main category: cs.CV

TL;DR: 提出一种多模型参数化点云表示，实现实时面检测与曲面拟合：先用GMM聚类，选取并合并平坦簇为平面，曲率簇用B样条曲面拟合，均用2D体素边界表示；在多数据集上较SOTA更鲁棒、效率提升3.78倍，相比纯GMM精度提升约2倍，并在低功耗平台达36.4 fps。


<details>
  <summary>Details</summary>
Motivation: 现有高自适应模型（样条、二次曲面）检测/拟合开销大；实时方法（GMM、平面）自由度低、难以用少量基元高精度表示。需要一种兼顾实时性、精度与表达能力的点云参数化表示与检测/拟合流程。

Method: 1) 以GMM对点云进行聚类分割；2) 识别“平坦”簇并合并为全局平面；3) 平面用简易拟合并通过2D体素化边界描述限定范围；4) 非平坦簇用B样条曲面拟合，并用同样的2D体素边界描述；形成多模型（平面+B样条）混合表示，实现实时检测与拟合。

Result: 在多公共数据集上，所提检测方法比SOTA更鲁棒且效率提升约3.78倍；整体表示对比GMM基线精度提升约2倍；在低功耗车载/机载计算平台可达36.4帧每秒。

Conclusion: 多模型（平面+B样条）与GMM驱动的分割加2D体素边界描述，在保证实时性的同时显著提升表示精度与鲁棒性，适合用于内存高效建图与多机器人协作等应用。

Abstract: In recent years, parametric representations of point clouds have been widely
applied in tasks such as memory-efficient mapping and multi-robot
collaboration. Highly adaptive models, like spline surfaces or quadrics, are
computationally expensive in detection or fitting. In contrast, real-time
methods, such as Gaussian mixture models or planes, have low degrees of
freedom, making high accuracy with few primitives difficult. To tackle this
problem, a multi-model parametric representation with real-time surface
detection and fitting is proposed. Specifically, the Gaussian mixture model is
first employed to segment the point cloud into multiple clusters. Then, flat
clusters are selected and merged into planes or curved surfaces. Planes can be
easily fitted and delimited by a 2D voxel-based boundary description method.
Surfaces with curvature are fitted by B-spline surfaces and the same boundary
description method is employed. Through evaluations on multiple public
datasets, the proposed surface detection exhibits greater robustness than the
state-of-the-art approach, with 3.78 times improvement in efficiency.
Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussian
mixture models, operating at 36.4 fps on a low-power onboard computer.

</details>


### [22] [Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models](https://arxiv.org/abs/2509.14777)
*Sunwoo Cho,Yejin Jung,Nam Ik Cho,Jae Woong Soh*

Main category: cs.CV

TL;DR: 提出一种无需类别标签和预训练SR模型的数据蒸馏方法，用CLIP分组与高梯度补丁筛选，微调扩散模型合成小而有效的训练集，在仅用0.68%数据时训练SR性能几乎不降（约0.3 dB），显著节省训练时长。


<details>
  <summary>Details</summary>
Motivation: 现有SISR训练依赖大数据与高算力；已有基于GAN反演的蒸馏方案依赖预训练SR与类别信息，泛化差、适用性有限。需要一种更通用、对先验依赖更少的数据高效方法。

Method: (1) 从原始图像中提取高梯度补丁；(2) 基于CLIP特征进行无标签的图像/补丁聚类与类别划分；(3) 在所选补丁上微调扩散模型以学习其数据分布；(4) 由微调后的扩散模型合成蒸馏训练图像；(5) 用合成数据训练SR模型（如Transformer基线）。

Result: 在极小数据量（仅原始数据的0.68%）下训练SR模型，性能仅下降约0.3 dB；扩散模型微调约4小时，SR模型训练约1小时，总训练时间远低于使用完整数据（约11小时）。达到与或优于现有方法的SOTA数据效率与性能权衡。

Conclusion: 通过高梯度筛选+CLIP聚类与扩散模型微调的无监督数据蒸馏，可在极小数据和低算力条件下训练SISR模型而仅带来极小性能损失，较既有依赖预训练与标签的方法更通用高效。

Abstract: Training deep neural networks has become increasingly demanding, requiring
large datasets and significant computational resources, especially as model
complexity advances. Data distillation methods, which aim to improve data
efficiency, have emerged as promising solutions to this challenge. In the field
of single image super-resolution (SISR), the reliance on large training
datasets highlights the importance of these techniques. Recently, a generative
adversarial network (GAN) inversion-based data distillation framework for SR
was proposed, showing potential for better data utilization. However, the
current method depends heavily on pre-trained SR networks and class-specific
information, limiting its generalizability and applicability. To address these
issues, we introduce a new data distillation approach for image SR that does
not need class labels or pre-trained SR models. In particular, we first extract
high-gradient patches and categorize images based on CLIP features, then
fine-tune a diffusion model on the selected patches to learn their distribution
and synthesize distilled training images. Experimental results show that our
method achieves state-of-the-art performance while using significantly less
training data and requiring less computational time. Specifically, when we
train a baseline Transformer model for SR with only 0.68\% of the original
dataset, the performance drop is just 0.3 dB. In this case, diffusion model
fine-tuning takes 4 hours, and SR model training completes within 1 hour, much
shorter than the 11-hour training time with the full dataset.

</details>


### [23] [Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model](https://arxiv.org/abs/2509.14780)
*Sina Amirrajab,Zohaib Salahuddin,Sheng Kuang,Henry C. Woodruff,Philippe Lambin*

Main category: cs.CV

TL;DR: Report2CT提出一种以完整放射学报告为条件的3D胸部CT潜变量扩散生成框架，融合多医疗文本编码器与体素间距条件，在大规模数据上训练，较现有方法在视觉质量与文本对齐上显著提升，并在MICCAI 2025 VLM3D赛道夺冠。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的扩散模型在医疗CT 3D生成场景中应用受限，通常仅用简化提示词，无法利用报告中的细粒度语义，导致文本-图像对齐和临床可信度不足。

Method: 构建Report2CT：以完整放射学报告（包含findings与impression）和体素间距为条件的3D潜变量扩散模型；并行整合三种预训练医疗文本编码器（BiomedVLP CXR-BERT、MedEmbed、ClinicalBERT）进行多编码器条件；采用Classifier-Free Guidance增强对齐。基于CT-RATE数据集的2万例CT体积训练；评估使用FID与CLIP类指标，并与GenerateCT进行质与量的对比。

Result: 生成的3D胸部CT体积具有解剖一致性、视觉质量高，文本-图像语义对齐优；多编码器条件显著提升CLIP分数；CFG进一步提升对齐，FID仅小幅下降；在MICCAI 2025 VLM3D文本条件CT生成赛道排名第一，并在全部评测指标上达到SOTA。

Conclusion: 充分利用完整放射学报告并引入多文本编码器条件，能显著提升3D CT合成的临床忠实度与质量；Report2CT为医学影像合成提供了更高保真度的生成手段，可用于高质量合成数据的构建与相关应用。

Abstract: Text to image latent diffusion models have recently advanced medical image
synthesis, but applications to 3D CT generation remain limited. Existing
approaches rely on simplified prompts, neglecting the rich semantic detail in
full radiology reports, which reduces text image alignment and clinical
fidelity. We propose Report2CT, a radiology report conditional latent diffusion
framework for synthesizing 3D chest CT volumes directly from free text
radiology reports, incorporating both findings and impression sections using
multiple text encoder. Report2CT integrates three pretrained medical text
encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced
clinical context. Radiology reports and voxel spacing information condition a
3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset.
Model performance was evaluated using Frechet Inception Distance (FID) for real
synthetic distributional similarity and CLIP based metrics for semantic
alignment, with additional qualitative and quantitative comparisons against
GenerateCT model. Report2CT generated anatomically consistent CT volumes with
excellent visual quality and text image alignment. Multi encoder conditioning
improved CLIP scores, indicating stronger preservation of fine grained clinical
details in the free text radiology reports. Classifier free guidance further
enhanced alignment with only a minor trade off in FID. We ranked first in the
VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved
state of the art performance across all evaluation metrics. By leveraging
complete radiology reports and multi encoder text conditioning, Report2CT
advances 3D CT synthesis, producing clinically faithful and high quality
synthetic data.

</details>


### [24] [Fracture interactive geodesic active contours for bone segmentation](https://arxiv.org/abs/2509.14817)
*Liheng Wang,Licheng Zhang,Hailin Xu,Jingxin Zhao,Xiuyun Su,Jiantao Li,Miutian Tang,Weilu Gao,Chong Chen*

Main category: cs.CV

TL;DR: 提出一种针对骨分割的“骨折交互式”测地有源轮廓(GAC)算法，通过新型边缘检测函数与嵌入距离/提示的自适应步长，提高在骨折与软组织干扰下的稳健性与精度。


<details>
  <summary>Details</summary>
Motivation: 经典GAC对特征提取不加区分，易受边缘遮挡、泄漏与骨折导致的断裂影响，出现误分割与不稳定；骨影像中软组织干扰强，需结合骨科领域知识提升对骨边界与骨折区域的识别与停靠能力。

Method: 1) 设计结合强度与梯度范数的边缘检测函数，引导轮廓靠近骨边缘、抑制软组织干扰；2) 将距离信息（可嵌入骨折提示）作为轮廓演化的自适应步长，稳定演化并在骨边与骨折处停靠，实现对骨折区域的交互与修正；3) 在骨盆与踝部数据上验证。

Result: 在骨盆与踝骨分割实验中，有效缓解边缘遮挡、泄漏和骨折带来的错误，分割表现准确、稳定且一致，相比传统方法更鲁棒。

Conclusion: 该方法能更好捕获骨特征、在骨折与软组织干扰下保持稳健，具有推广到其他骨解剖部位的潜力，并为结合领域先验与深度网络提供思路。

Abstract: For bone segmentation, the classical geodesic active contour model is usually
limited by its indiscriminate feature extraction, and then struggles to handle
the phenomena of edge obstruction, edge leakage and bone fracture. Thus, we
propose a fracture interactive geodesic active contour algorithm tailored for
bone segmentation, which can better capture bone features and perform robustly
to the presence of bone fractures and soft tissues. Inspired by orthopedic
knowledge, we construct a novel edge-detector function that combines the
intensity and gradient norm, which guides the contour towards bone edges
without being obstructed by other soft tissues and therefore reduces
mis-segmentation. Furthermore, distance information, where fracture prompts can
be embedded, is introduced into the contour evolution as an adaptive step size
to stabilize the evolution and help the contour stop at bone edges and
fractures. This embedding provides a way to interact with bone fractures and
improves the accuracy in the fracture regions. Experiments in pelvic and ankle
segmentation demonstrate the effectiveness on addressing the aforementioned
problems and show an accurate, stable and consistent performance, indicating a
broader application in other bone anatomies. Our algorithm also provides
insights into combining the domain knowledge and deep neural networks.

</details>


### [25] [Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation](https://arxiv.org/abs/2509.14827)
*Patrick Madlindl,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 提出最小能量形变（MED）损失，用于学习式皮层表面重建中的形变轨迹正则化，提升训练一致性与可复现性且不降低重建精度与拓扑正确性。


<details>
  <summary>Details</summary>
Motivation: 学习式CSR通过快速将模板变形到个体MRI上，但常见以Chamfer距离为主的监督无法约束形变能量与轨迹稳定性，导致不同训练运行之间结果不一致、形变不优。需要一种机制同时降低形变能量、提升训练稳定性与可复现性。

Method: 设计MED损失，对形变场的时序/路径进行能量正则化，作为Chamfer距离的补充；将其集成到V2C-Flow模型的模板到皮层表面形变流程中，约束形变轨迹的平滑与最小能量属性。

Result: 在V2C-Flow上加入MED后，相比仅用Chamfer的基线，训练一致性和可复现性显著提升，同时维持甚至不降低重建精度与拓扑正确性。

Conclusion: MED作为通用正则化可与Chamfer配合，生成能量更低、轨迹更稳定的形变，解决了学习式CSR中一致性与复现性问题，而不牺牲精度与拓扑。

Abstract: Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI)
is fundamental to neuroimage analysis, enabling morphological studies of the
cerebral cortex and functional brain mapping. Recent advances in learning-based
CSR have dramatically accelerated processing, allowing for reconstructions
through the deformation of anatomical templates within seconds. However,
ensuring the learned deformations are optimal in terms of deformation energy
and consistent across training runs remains a particular challenge. In this
work, we design a Minimal Energy Deformation (MED) loss, acting as a
regularizer on the deformation trajectories and complementing the widely used
Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and
demonstrate considerable improvements in previously neglected training
consistency and reproducibility without harming reconstruction accuracy and
topological correctness.

</details>


### [26] [ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification](https://arxiv.org/abs/2509.14830)
*Alvaro Lopez Pellicer,Andre Mariucci,Plamen Angelov,Marwan Bukhari,Jemma G. Kerns*

Main category: cs.CV

TL;DR: 提出ProtoMedX：一种原型驱动、可解释的多模态模型（DEXA影像+患者记录）用于骨健康分类，兼顾SOTA性能与可视化解释，在4160名NHS患者数据上达89.8%（多模态）/87.58%（仅视觉）。


<details>
  <summary>Details</summary>
Motivation: 现有骨质减少/骨质疏松诊断多基于DEXA与病史；AI研究多依赖仅视觉的深度网络，追求准确率却忽视内生可解释性，解释多为事后手段，难以满足临床与合规（如EU AI Act）对可解释与可审计的需求。

Method: 设计ProtoMedX：原型（prototype）为核心的可解释架构，输入为腰椎DEXA影像与患者结构化记录（多模态）。模型在视觉-only与多模态两种设置下训练/评估；通过原型匹配提供局部可视解释，并可分析错误决策。

Result: 在4160例真实NHS患者数据集上，ProtoMedX在仅视觉任务上达到87.58%准确率，在多模态变体上达到89.8%，均优于已发表方法；同时产出可被临床医生直观理解的可视解释。

Conclusion: ProtoMedX在骨健康分类上实现SOTA并内生可解释，满足临床可解释与合规需求；多模态融合优于单模态，且原型机制支持对正确与错误预测的透明审查。

Abstract: Bone health studies are crucial in medical practice for the early detection
and treatment of Osteopenia and Osteoporosis. Clinicians usually make a
diagnosis based on densitometry (DEXA scans) and patient history. The
applications of AI in this field are ongoing research. Most successful methods
rely on deep learning models that use vision alone (DEXA/X-ray imagery) and
focus on prediction accuracy, while explainability is often disregarded and
left to post hoc assessments of input contributions. We propose ProtoMedX, a
multi-modal model that uses both DEXA scans of the lumbar spine and patient
records. ProtoMedX's prototype-based architecture is explainable by design,
which is crucial for medical applications, especially in the context of the
upcoming EU AI Act, as it allows explicit analysis of model decisions,
including incorrect ones. ProtoMedX demonstrates state-of-the-art performance
in bone health classification while also providing explanations that can be
visually understood by clinicians. Using a dataset of 4,160 real NHS patients,
the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8%
in its multi-modal variant, both surpassing existing published methods.

</details>


### [27] [MapAnything: Mapping Urban Assets using Single Street-View Images](https://arxiv.org/abs/2509.14839)
*Miriam Louise Carnot,Jonas Kunze,Erik Fastermann,Eric Peukert,André Ludwig,Bogdan Franczyk*

Main category: cs.CV

TL;DR: 提出MapAnything模块：用单幅图像结合度量深度估计与相机参数，自动推断城市物体（如交通标志、道路损坏等）的地理坐标，并在城市场景中用LiDAR作参照评估精度，给出自动化建库建议与案例验证。


<details>
  <summary>Details</summary>
Motivation: 城市管理需要持续更新包含交通设施、树木、事件（涂鸦、道路破损等）的地理数据库。数字化程度提升带来更高数据时效与规模要求，传统人工采集/标注代价高昂，亟需从图像自动获取物体地理坐标以降低成本、提高效率。

Method: 构建MapAnything模块：基于先进的度量深度估计（Metric Depth Estimation）从单张图像预测目标到相机的真实尺度距离；结合几何成像模型与已知相机内外参，将图像中目标的像素位置与深度回投到地理坐标系，得到目标经纬度/地理坐标；并按距离区间与语义区域（道路、植被等）分析性能。

Result: 以城市环境LiDAR点云为真值对比，评估单目估距与地理定位误差；对不同距离区间和不同语义区域给出精度差异分析；通过交通标志与道路损坏两个实际用例展示模块的有效性。

Conclusion: MapAnything可用单幅图像自动推断城市对象地理坐标，能在多类城市场景中取得可用精度，并为城市设施与事件的自动化建库与更新提供可行路径；同时给出在不同距离与语义区域下的使用建议与局限。

Abstract: To maintain an overview of urban conditions, city administrations manage
databases of objects like traffic signs and trees, complete with their
geocoordinates. Incidents such as graffiti or road damage are also relevant. As
digitization increases, so does the need for more data and up-to-date
databases, requiring significant manual effort. This paper introduces
MapAnything, a module that automatically determines the geocoordinates of
objects using individual images. Utilizing advanced Metric Depth Estimation
models, MapAnything calculates geocoordinates based on the object's distance
from the camera, geometric principles, and camera specifications. We detail and
validate the module, providing recommendations for automating urban object and
incident mapping. Our evaluation measures the accuracy of estimated distances
against LiDAR point clouds in urban environments, analyzing performance across
distance intervals and semantic areas like roads and vegetation. The module's
effectiveness is demonstrated through practical use cases involving traffic
signs and road damage.

</details>


### [28] [Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](https://arxiv.org/abs/2509.14841)
*Hongjun Wang,Jiyuan Chen,Zhengwei Yin,Xuan Song,Yinqiang Zheng*

Main category: cs.CV

TL;DR: 提出面向泛化超分的“定向去噪”框架：先检测噪声、再对特征去噪，抑制模型对噪声退化的过拟合，可无缝接入现有SR模型，并在多基准（含合成与真实）上优于常见正则化类方法。


<details>
  <summary>Details</summary>
Motivation: 以往泛化SR方法默认模型会同等地过拟合各种退化（模糊、噪声、JPEG等），通过Dropout、特征对齐等抑制过拟合。但作者实证发现，模型主要对“噪声”过拟合，因为噪声的退化模式与其他类型更为不同。这一偏差导致现有通用抑制策略可能效率不高，亟需针对性解决。

Method: 提出“定向特征去噪”框架，包括：1）噪声检测模块：识别输入或中间特征中由噪声导致的成分；2）特征去噪模块：对被判定为噪声相关的特征分量进行抑制/净化，避免网络学习到与内容无关的噪声模式。该框架作为通用插件，可无缝集成到现有SR网络中，无需改动主干结构。

Result: 在五个传统基准与数据集上（涵盖合成与真实场景），相较Dropout、特征对齐等正则化类泛化方法取得更优性能；展示了更强的跨退化泛化能力，尤其对未知噪声退化。

Conclusion: SR模型的过拟合主要来源于噪声退化，针对性地检测并去除噪声相关特征比一刀切的正则化更有效。所提框架易于集成、无需结构修改，并在多基准上验证了优越性。

Abstract: Generalizable Image Super-Resolution aims to enhance model generalization
capabilities under unknown degradations. To achieve this goal, the models are
expected to focus only on image content-related features instead of overfitting
degradations. Recently, numerous approaches such as Dropout and Feature
Alignment have been proposed to suppress models' natural tendency to overfit
degradations and yield promising results. Nevertheless, these works have
assumed that models overfit to all degradation types (e.g., blur, noise, JPEG),
while through careful investigations in this paper, we discover that models
predominantly overfit to noise, largely attributable to its distinct
degradation pattern compared to other degradation types. In this paper, we
propose a targeted feature denoising framework, comprising noise detection and
denoising modules. Our approach presents a general solution that can be
seamlessly integrated with existing super-resolution models without requiring
architectural modifications. Our framework demonstrates superior performance
compared to previous regularization-based methods across five traditional
benchmarks and datasets, encompassing both synthetic and real-world scenarios.

</details>


### [29] [[Re] Improving Interpretation Faithfulness for Vision Transformers](https://arxiv.org/abs/2509.14846)
*Izabela Kurek,Wojciech Trejter,Stipe Frkovic,Andro Erdelez*

Main category: cs.CV

TL;DR: 复现FViT与多种ViT可解释性方法，评估DDS对分割与分类任务中可解释性鲁棒性的提升，并扩展到更多方法与代价评估；总体结果与原文一致但有小差异。


<details>
  <summary>Details</summary>
Motivation: 原论文声称通过Diffusion Denoised Smoothing提升ViT可解释性在攻击与扰动下的鲁棒性，但缺乏独立复现与对更广方法、成本与环境影响的系统评估。本文动机是验证这些主张的可重复性、外推性与实际代价。

Method: - 复现实验：重现实验设置与指标，覆盖分割与分类两类任务。
- 对比方法：原论文中的解释方法，以及新增的基线方法与Attribution Rollout。
- 干预：在各方法上加入DDS，测试在对抗攻击与随机扰动下的可解释性鲁棒性。
- 评估：定量比较鲁棒性指标，并记录计算开销与碳排放/能耗估计。
- 分析：与原文结果对齐，定位差异与可能原因。

Result: - 大体重现了DDS带来鲁棒性提升：在分割攻击、分类扰动与攻击下，解释热图更稳健。
- 将DDS应用于多种方法（含Attribution Rollout）亦有提升，但幅度依方法与任务而异。
- 发现与原文有轻微不一致之处，并给出讨论与可能解释。
- 获得FViT需显著计算成本与环境影响。

Conclusion: DDS普遍能强化ViT可解释性的鲁棒性，且可作为可插拔模块用于多种解释方法；然而收益存在方法/任务依赖性，并以更高的计算与环境成本为代价。总体支持原论文结论，但需关注实现细节与资源权衡。

Abstract: This work aims to reproduce the results of Faithful Vision Transformers
(FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for
Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate
claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised
Smoothing (DDS) improves interpretability robustness to (1) attacks in a
segmentation task and (2) perturbation and attacks in a classification task. We
also extend the original study by investigating the authors' claims that adding
DDS to any interpretability method can improve its robustness under attack.
This is tested on baseline methods and the recently proposed Attribution
Rollout method. In addition, we measure the computational costs and
environmental impact of obtaining an FViT through DDS. Our results broadly
agree with the original study's findings, although minor discrepancies were
found and discussed.

</details>


### [30] [MARIC: Multi-Agent Reasoning for Image Classification](https://arxiv.org/abs/2509.14860)
*Wonduk Seo,Minhyeong Yu,Hyunjin An,Seunghyun Lee*

Main category: cs.CV

TL;DR: 提出MARIC多智能体图像分类框架：由大纲代理生成全局主题与提示，三类“方面代理”沿不同视觉维度提取细粒度描述，推理代理做反思式综合，形成统一表征用于分类；在4个基准上显著优于基线，兼顾性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统图像分类需大量标注与参数密集训练；即便VLM减少数据/调参需求，也因单次前向与单一视角而难以捕捉互补信息，表现与可解释性受限。作者希望通过多视角、协同推理缓解训练与表示的局限。

Method: 将分类重构为多智能体协作推理流程：1) Outliner Agent把握图像全局主题并生成针对性提示；2) 三个Aspect Agents沿不同视觉维度（如形状/纹理/语义或场景/对象/属性）抽取细粒度描述；3) Reasoning Agent通过“反思式”整合这些描述，形成统一表示并输出类别。无需参数重训练，强调分解—综合与反思。

Result: 在4个多样化图像分类基准上，MARIC显著优于若干基线（含单体VLM与传统方法），并展现更稳健与可解释的预测。

Conclusion: 多智能体、分解与反思式综合可弥补单体VLM与重训练方法的不足，提升分类准确与可解释性；MARIC验证了多代理视觉推理在图像分类中的有效性与通用性。

Abstract: Image classification has traditionally relied on parameter-intensive model
training, requiring large-scale annotated datasets and extensive fine tuning to
achieve competitive performance. While recent vision language models (VLMs)
alleviate some of these constraints, they remain limited by their reliance on
single pass representations, often failing to capture complementary aspects of
visual content. In this paper, we introduce Multi Agent based Reasoning for
Image Classification (MARIC), a multi agent framework that reformulates image
classification as a collaborative reasoning process. MARIC first utilizes an
Outliner Agent to analyze the global theme of the image and generate targeted
prompts. Based on these prompts, three Aspect Agents extract fine grained
descriptions along distinct visual dimensions. Finally, a Reasoning Agent
synthesizes these complementary outputs through integrated reflection step,
producing a unified representation for classification. By explicitly
decomposing the task into multiple perspectives and encouraging reflective
synthesis, MARIC mitigates the shortcomings of both parameter-heavy training
and monolithic VLM reasoning. Experiments on 4 diverse image classification
benchmark datasets demonstrate that MARIC significantly outperforms baselines,
highlighting the effectiveness of multi-agent visual reasoning for robust and
interpretable image classification.

</details>


### [31] [Controllable Localized Face Anonymization Via Diffusion Inpainting](https://arxiv.org/abs/2509.14866)
*Ali Salar,Qing Liu,Guoying Zhao*

Main category: cs.CV

TL;DR: 提出基于潜空间扩散模型的统一匿名化框架，用属性引导与梯度校正实现可控、可局部的人脸匿名化，无需额外训练，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 人像数据在视觉任务中广泛使用，但需在保护隐私（身份不可识别）与保持下游任务可用性之间取得平衡；现有方法要么失真、难以控制、需再训练，或影响实用性。

Method: 利用潜变量扩散模型的修复（inpainting）能力：1) 生成目标（匿名）人脸；2) 在反向去噪过程中引入自适应属性引导模块，对梯度进行校正，使生成图像的面部属性对齐于合成的目标图像；3) 支持局部匿名化，用户可指定保持不变的区域；整个流程无需对模型再训练。

Result: 在CelebA-HQ与FFHQ上广泛实验，匿名化后的图像逼真、属性一致且对下游任务保持高可用性，客观评价与对比显示优于当前SOTA，同时无需额外训练成本。

Conclusion: 所提框架实现可控、逼真、可局部的人像匿名化，在隐私保护与实用性之间取得更佳权衡，并具备零训练开销与易用性；代码已开源。

Abstract: The growing use of portrait images in computer vision highlights the need to
protect personal identities. At the same time, anonymized images must remain
useful for downstream computer vision tasks. In this work, we propose a unified
framework that leverages the inpainting ability of latent diffusion models to
generate realistic anonymized images. Unlike prior approaches, we have complete
control over the anonymization process by designing an adaptive
attribute-guidance module that applies gradient correction during the reverse
denoising process, aligning the facial attributes of the generated image with
those of the synthesized target image. Our framework also supports localized
anonymization, allowing users to specify which facial regions are left
unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ
datasets show that our method outperforms state-of-the-art approaches while
requiring no additional model training. The source code is available on our
page.

</details>


### [32] [Temporal Representation Learning of Phenotype Trajectories for pCR Prediction in Breast Cancer](https://arxiv.org/abs/2509.14872)
*Ivana Janíčková,Yen Y. Tan,Thomas H. Helbich,Konstantin Miloserdov,Zsuzsanna Bago-Horvath,Ulrike Heber,Georg Langs*

Main category: cs.CV

TL;DR: 提出一种从乳腺MRI纵向影像中学习早期治疗反应动态表征，以预测NACT后的病理完全缓解（pCR）；通过将影像随时间的变化映射为潜在空间中的轨迹，并用线性分类器进行判别，显著提升了早期阶段的预测性能。


<details>
  <summary>Details</summary>
Motivation: 个体化治疗需要在疗程早期准确预测患者对治疗的反应，但疾病进展与疗效高度异质，传统基线特征或单时点模型难以捕捉动态反应模式。MRI在NACT期间的多时点随访包含丰富的时序信息，若能有效表征其动力学，有望更早、更稳健地预测pCR，辅助治疗决策与自适应治疗调整。

Method: 构建一个多任务学习模型：同时建模影像外观与时间连续性，将多时点乳腺MRI数据编码到潜在空间；患者在不同时间点的影像被映射为潜在空间中的轨迹。模型特别针对非应答者群体的高异质性进行正则或分布建模。随后在该潜在轨迹空间中训练一个简单的线性分类器来预测pCR。

Result: 在ISPY-2公开数据上，基于潜在轨迹的线性分类器达到：仅使用治疗前（T0）平衡准确率0.761；加入早期一次随访（T0+T1）达0.811；使用四个时间点（T0→T3）达0.861。表明纳入更完整的时序动态能持续提升预测性能。

Conclusion: 利用MRI纵向动态构建潜在轨迹，并在该空间进行简洁的线性判别，可有效、渐进地提升对乳腺癌NACT后pCR的早期预测。该方法兼顾外观重构、时间连续性与非应答者异质性，具备在临床早期决策与自适应治疗中的潜在应用价值。

Abstract: Effective therapy decisions require models that predict the individual
response to treatment. This is challenging since the progression of disease and
response to treatment vary substantially across patients. Here, we propose to
learn a representation of the early dynamics of treatment response from imaging
data to predict pathological complete response (pCR) in breast cancer patients
undergoing neoadjuvant chemotherapy (NACT). The longitudinal change in magnetic
resonance imaging (MRI) data of the breast forms trajectories in the latent
space, serving as basis for prediction of successful response. The multi-task
model represents appearance, fosters temporal continuity and accounts for the
comparably high heterogeneity in the non-responder cohort.In experiments on the
publicly available ISPY-2 dataset, a linear classifier in the latent trajectory
space achieves a balanced accuracy of 0.761 using only pre-treatment data (T0),
0.811 using early response (T0 + T1), and 0.861 using four imaging time points
(T0 -> T3). The code will be made available upon paper acceptance.

</details>


### [33] [NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation](https://arxiv.org/abs/2509.14890)
*Antoine Legrand,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 提出一种可视化航天器相对位姿估计网络所依赖3D视觉线索的方法，通过将位姿网络的梯度回传到NeRF式图像生成器，迫使其渲染网络真正利用的三维特征；实验表明该方法能恢复相关3D线索并揭示监督与隐式目标表征的关系。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的航天器6D相对位姿估计在任务中受限于“不可解释性”，任务方需要理解模型依据哪些视觉证据做决策以提升可信度与调试能力。

Method: 训练一个NeRF风格的图像生成器，并将位姿估计网络的梯度反传至该生成器，使其生成与位姿网络决策最相关的三维结构与纹理，从而显式化网络关注的3D线索。

Result: 实验显示生成器能重建出与位姿估计相关的关键三维几何/外观线索，验证了可视化的有效性，并能对网络内部表征提供信息。

Conclusion: 该方法为在轨相对位姿估计提供了可解释性工具，能恢复并展示网络使用的3D视觉线索，并揭示监督方式如何影响网络对目标航天器的隐式表征。

Abstract: On-orbit operations require the estimation of the relative 6D pose, i.e.,
position and orientation, between a chaser spacecraft and its target. While
data-driven spacecraft pose estimation methods have been developed, their
adoption in real missions is hampered by the lack of understanding of their
decision process. This paper presents a method to visualize the 3D visual cues
on which a given pose estimator relies. For this purpose, we train a NeRF-based
image generator using the gradients back-propagated through the pose estimation
network. This enforces the generator to render the main 3D features exploited
by the spacecraft pose estimation network. Experiments demonstrate that our
method recovers the relevant 3D cues. Furthermore, they offer additional
insights on the relationship between the pose estimation network supervision
and its implicit representation of the target spacecraft.

</details>


### [34] [Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS 2025 VOS Track](https://arxiv.org/abs/2509.14901)
*An Yan,Leilei Cao,Feng Lu,Ran Hong,Youhai Jiang,Fengjie Zhu*

Main category: cs.CV

TL;DR: 该工作面向复杂视频目标分割（VOS），在SAM2基础上提出“伪标注训练+级联多模型推理”的方案：用SAM2Long生成MOSE测试集伪标注以再训练，推理时并行SAM2Long与SeC并通过级联决策融合，获得J&F 0.8616（较基线+1.4），获LSVOS 2025赛道亚军。


<details>
  <summary>Details</summary>
Motivation: 复杂VOS中存在小且相似目标、频繁遮挡、快速运动和复杂交互，单一模型难以兼顾时间稳定性与类别/概念鲁棒性；公开训练数据有限且分布与竞赛数据存在差异，需提升域适应与鲁棒性。

Method: 1) 伪标注训练：先训练SAM2，在SAM2Long框架内对MOSE测试集自动生成伪标注，并与现有标注数据合并进行再训练；2) 推理双路并行：SAM2Long提供时间一致、长视频友好的主结果；开源SeC模型输出概念层面鲁棒的补充结果；3) 级联决策融合：依据时序稳定性与概念一致性动态选择/融合两路输出。

Result: 在MOSE测试集取得J&F=0.8616，相比SAM2Long基线提升+1.4分，在LSVOS 2025 VOS赛道排名第二，显示出对长时复杂场景的鲁棒性与精度优势。

Conclusion: 通过伪标注增强训练分布与样本多样性，并以级联多模型在推理时互补优势，可有效提升复杂长视频目标分割的稳定性与准确性；方案在实际竞赛中验证有效。

Abstract: Complex Video Object Segmentation (VOS) presents significant challenges in
accurately segmenting objects across frames, especially in the presence of
small and similar targets, frequent occlusions, rapid motion, and complex
interactions. In this report, we present our solution for the LSVOS 2025 VOS
Track based on the SAM2 framework. We adopt a pseudo-labeling strategy during
training: a trained SAM2 checkpoint is deployed within the SAM2Long framework
to generate pseudo labels for the MOSE test set, which are then combined with
existing data for further training. For inference, the SAM2Long framework is
employed to obtain our primary segmentation results, while an open-source SeC
model runs in parallel to produce complementary predictions. A cascaded
decision mechanism dynamically integrates outputs from both models, exploiting
the temporal stability of SAM2Long and the concept-level robustness of SeC.
Benefiting from pseudo-label training and cascaded multi-model inference, our
approach achieves a J\&F score of 0.8616 on the MOSE test set -- +1.4 points
over our SAM2Long baseline -- securing the 2nd place in the LSVOS 2025 VOS
Track, and demonstrating strong robustness and accuracy in long, complex video
segmentation scenarios.

</details>


### [35] [Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications](https://arxiv.org/abs/2509.14921)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 研究评估了CLIP在微调用于人脸识别(FR)、变形攻击检测(MAD)和展示攻击检测(PAD)后，在专门任务与通用视觉任务间的权衡：微调带来专长性能提升但显著削弱跨域泛化，尤其是复杂的FR任务。较大模型与合适的头部设计有助于减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 基础模型具备强零/小样本迁移能力，但在生物识别等高专门领域微调时可能过度专化，失去跨域泛化这一核心优势。缺乏系统量化这些权衡与影响因素（任务复杂度、分类头设计、模型规模）的研究，亟需在统一框架下评估。

Method: 以CLIP为基础，分别微调用于FR、MAD、PAD三任务；在14个通用视觉数据集上进行零样本与线性探针评估，并在各自领域基准上测试。比较不同任务复杂度（多类FR vs 二分类MAD/PAD）、不同分类头设计及不同模型大小（ViT-L等）的影响。

Result: 微调后的模型在各自专门任务上提升，但在通用数据集上出现明显性能下降，FR微调的遗忘最严重；多类头（FR）较二分类头（MAD/PAD）更易引发灾难性遗忘。ViT-L的FRoundation在IJB-C上最高可提升58.52%，但在ImageNetV2上从CLIP基线的69.84%降至51.63%。更大的CLIP架构比小模型更能保留原始泛化能力。

Conclusion: 生物识别微调存在显著专化-泛化权衡；任务复杂度与头部设计与遗忘程度正相关。增大模型容量有助于缓解过专化，但仍需设计更好的微调策略以兼顾专门任务性能与跨域泛化。

Abstract: Foundation models such as CLIP have demonstrated exceptional zero- and
few-shot transfer capabilities across diverse vision tasks. However, when
fine-tuned for highly specialized biometric tasks, face recognition (FR),
morphing attack detection (MAD), and presentation attack detection (PAD), these
models may suffer from over-specialization. Thus, they may lose one of their
foundational strengths, cross-domain generalization. In this work, we
systematically quantify these trade-offs by evaluating three instances of CLIP
fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the
original CLIP baseline on 14 general vision datasets under zero-shot and
linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our
results indicate that fine-tuned models suffer from over-specialization,
especially when fine-tuned for complex tasks of FR. Also, our results pointed
out that task complexity and classification head design, multi-class (FR) vs.
binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The
FRoundation model with the ViT-L backbone outperforms other approaches on the
large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%.
However, it experiences a substantial performance drop on ImageNetV2, reaching
only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover,
the larger CLIP architecture consistently preserves more of the model's
original generalization ability than the smaller variant, indicating that
increased model capacity may help mitigate over-specialization.

</details>


### [36] [GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation](https://arxiv.org/abs/2509.14927)
*Tan-Hiep To,Duy-Khang Nguyen,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: GenKOL 是一套交互式系统，用生成式AI快速生成虚拟KOL形象，支持服装生成、妆容迁移、背景合成、发型编辑等模块化能力，可本地或云端部署，以降低内容制作成本并加速营销流程。


<details>
  <summary>Details</summary>
Motivation: 人类KOL合作成本高、协调复杂，限制品牌在多渠道快速产出高质量内容的能力。需要一种低成本、可扩展、可控的虚拟替代方案，以提升营销效率与一致性。

Method: 设计并实现一个模块化的交互式系统GenKOL：通过可视化界面整合多种生成式AI能力（服装生成、妆容迁移、背景合成、发型编辑）。各能力作为可替换服务，支持本地与云端灵活部署，便于根据场景与算力配置组合与扩展。

Result: 系统能让营销人员动态组合推广视觉素材，高效生成高质量虚拟KOL图像，显著简化品牌内容生产流程，降低成本并提升产出速度。

Conclusion: 模块化、可部署灵活的生成式AI系统能够在多样营销场景中可扩展地创建虚拟KOL，改善工作流与成本结构，具备实际应用价值。

Abstract: Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping
consumer perceptions and enhancing brand credibility. However, collaborating
with human KOLs often involves high costs and logistical challenges. To address
this, we present GenKOL, an interactive system that empowers marketing
professionals to efficiently generate high-quality virtual KOL images using
generative AI. GenKOL enables users to dynamically compose promotional visuals
through an intuitive interface that integrates multiple AI capabilities,
including garment generation, makeup transfer, background synthesis, and hair
editing. These capabilities are implemented as modular, interchangeable
services that can be deployed flexibly on local machines or in the cloud. This
modular architecture ensures adaptability across diverse use cases and
computational environments. Our system can significantly streamline the
production of branded content, lowering costs and accelerating marketing
workflows through scalable virtual KOL creation.

</details>


### [37] [DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection](https://arxiv.org/abs/2509.14957)
*Zhuokang Shen,Kaisen Zhang,Bohan Jia,Yuan Fang,Zhou Yu,Shaohui Lin*

Main category: cs.CV

TL;DR: 提出DF-LLaVA：从MLLM提取潜在判别知识并以提示注入训练，使LLaVA在合成图像检测中兼具高准确率与可解释性、超越专家模型。


<details>
  <summary>Details</summary>
Motivation: 现有图像真伪检测多仅给出概率/二分类，缺乏可解释性；基于多模态大模型的方法虽更可解释，但在纯分类准确率上落后于专家模型。需要一个同时兼顾准确性与可解释性的方案。

Method: 构建DF-LLaVA框架：先从MLLM（如LLaVA）中抽取“潜在知识”（判别线索/特征），再通过提示工程将这些知识注入训练流程，促使模型在检测任务上发挥内在判别能力，同时保持生成式解释输出能力。

Result: 在多项实验中，DF-LLaVA在合成图像检测准确率上超过专家模型，同时提供定位与解释信息；跨数据集评测显示其兼具高精度与可解释性。

Conclusion: DF-LLaVA证明可通过知识提取与提示注入激发MLLM的判别潜力，使其在合成图像检测中实现“高准确+高可解释”。

Abstract: With the increasing prevalence of synthetic images, evaluating image
authenticity and locating forgeries accurately while maintaining human
interpretability remains a challenging task. Existing detection models
primarily focus on simple authenticity classification, ultimately providing
only a forgery probability or binary judgment, which offers limited explanatory
insights into image authenticity. Moreover, while MLLM-based detection methods
can provide more interpretable results, they still lag behind expert models in
terms of pure authenticity classification accuracy. To address this, we propose
DF-LLaVA, a simple yet effective framework that unlocks the intrinsic
discrimination potential of MLLMs. Our approach first extracts latent knowledge
from MLLMs and then injects it into training via prompts. This framework allows
LLaVA to achieve outstanding detection accuracy exceeding expert models while
still maintaining the interpretability offered by MLLMs. Extensive experiments
confirm the superiority of our DF-LLaVA, achieving both high accuracy and
explainability in synthetic image detection. Code is available online at:
https://github.com/Eliot-Shen/DF-LLaVA.

</details>


### [38] [Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification](https://arxiv.org/abs/2509.14958)
*Xiang Tuo,Xu Xuemiao,Liu Bangzhen,Li Jinyi,Li Yong,He Shengfeng*

Main category: cs.CV

TL;DR: 提出CMGR框架，用CLIP层级空间语义校正3D几增学习中的几何错位与纹理偏置；通过结构感知几何校正、最小判别纹理放大、基-新判别器稳定原型，显著提升少样本增量的几何一致性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 3D开放世界与类增量场景中数据稀缺常见，现有方法因3D-2D对齐差与纹理偏置，导致语义模糊、原型不稳与灾难性遗忘；需要一种既利用2D大模型语义、又能保持3D几何忠实性的方案。

Method: CMGR三模块：1) 结构感知几何校正（SAGR）：用注意力将3D部件层级与CLIP中间层空间先验对齐，进行跨模态几何融合以纠正错位；2) 纹理放大（TAM）：生成最小但判别性的纹理信号，抑制噪声并增强跨模态一致性；3) 基-新判别器（BND）：在增量学习中分离几何变化，稳定类别原型，缓解遗忘。

Result: 在跨域与域内的3D少样本类增量实验中，相较现有方法显著提升：更强的几何一致性、更抗纹理偏置与更稳定的增量原型，整体性能优于SOTA。

Conclusion: 通过利用CLIP层级空间语义进行几何校正并控制纹理偏置，CMGR在极端数据稀缺下实现稳健的3D类增量识别，兼具几何保真与跨模态一致性，适用于开放世界的持续扩展。

Abstract: The rapid growth of 3D digital content necessitates expandable recognition
systems for open-world scenarios. However, existing 3D class-incremental
learning methods struggle under extreme data scarcity due to geometric
misalignment and texture bias. While recent approaches integrate 3D data with
2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by
texture-biased projections and indiscriminate fusion of geometric-textural
cues, leading to unstable decision prototypes and catastrophic forgetting. To
address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a
framework that enhances 3D geometric fidelity by leveraging CLIP's hierarchical
spatial semantics. Specifically, we introduce a Structure-Aware Geometric
Rectification module that hierarchically aligns 3D part structures with CLIP's
intermediate spatial priors through attention-driven geometric fusion.
Additionally, a Texture Amplification Module synthesizes minimal yet
discriminative textures to suppress noise and reinforce cross-modal
consistency. To further stabilize incremental prototypes, we employ a
Base-Novel Discriminator that isolates geometric variations. Extensive
experiments demonstrate that our method significantly improves 3D few-shot
class-incremental learning, achieving superior geometric coherence and
robustness to texture bias across cross-domain and within-domain settings.

</details>


### [39] [Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis](https://arxiv.org/abs/2509.14965)
*Junhao Jia,Yunyou Liu,Cheng Yang,Yifei Sun,Feiwei Qin,Changmiao Wang,Yong Peng*

Main category: cs.CV

TL;DR: 提出Brain-HGCN：基于洛伦兹模型的双曲几何GNN，用于更低失真地表示fMRI脑功能网络的层级结构，并在两大精神疾病分类数据集上显著优于欧氏基线。


<details>
  <summary>Details</summary>
Motivation: fMRI构建的脑功能网络具有强层级性，欧氏空间中的GNN由于空间曲率与维度限制，难以在不显著失真的情况下刻画这种层级拓扑，影响临床判别性能；需要能自然容纳层级与树状结构的几何空间与模型。

Method: 在洛伦兹（双曲）模型上构建几何深度学习框架Brain-HGCN：1）设计超曲率图注意力层（hyperbolic graph attention），在流形上进行注意力与消息传递；2）引入“带符号聚合”机制区分兴奋性与抑制性连接并分别处理；3）采用几何上合理的Fréchet均值作为图级读出，学习稳健的图表示。

Result: 在两个大规模fMRI精神障碍分类数据集上，Brain-HGCN显著优于多种SOTA欧氏基线，显示更强的判别力与稳健性。

Conclusion: 双曲几何为刻画脑网络层级提供了契合的表示空间；Brain-HGCN验证了双曲GNN在计算精神病学中的有效性，开启了fMRI分析的几何深度学习新范式。

Abstract: Functional magnetic resonance imaging (fMRI) provides a powerful non-invasive
window into the brain's functional organization by generating complex
functional networks, typically modeled as graphs. These brain networks exhibit
a hierarchical topology that is crucial for cognitive processing. However, due
to inherent spatial constraints, standard Euclidean GNNs struggle to represent
these hierarchical structures without high distortion, limiting their clinical
performance. To address this limitation, we propose Brain-HGCN, a geometric
deep learning framework based on hyperbolic geometry, which leverages the
intrinsic property of negatively curved space to model the brain's network
hierarchy with high fidelity. Grounded in the Lorentz model, our model employs
a novel hyperbolic graph attention layer with a signed aggregation mechanism to
distinctly process excitatory and inhibitory connections, ultimately learning
robust graph-level representations via a geometrically sound Fr\'echet mean for
graph readout. Experiments on two large-scale fMRI datasets for psychiatric
disorder classification demonstrate that our approach significantly outperforms
a wide range of state-of-the-art Euclidean baselines. This work pioneers a new
geometric deep learning paradigm for fMRI analysis, highlighting the immense
potential of hyperbolic GNNs in the field of computational psychiatry.

</details>


### [40] [RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching](https://arxiv.org/abs/2509.14966)
*Xingwu Zhang,Guanxuan Li,Zhuocheng Zhang,Zijun Long*

Main category: cs.CV

TL;DR: 提出RoboEye，两阶段识别框架：先用大模型提取2D特征候选排序，再按需触发3D重排序，结合几何感知密集特征与关键点匹配；在无需显式3D传感的RGB条件下，Recall@1较SOTA（RoboLLM）提升7.1%。


<details>
  <summary>Details</summary>
Motivation: 电商品类爆炸导致同类差异增大、长尾与外观相似增多，且包装多样、遮挡、视角变化等使查询与参考图差异加剧，纯2D外观方法性能骤降；需要在不增加昂贵3D硬件的前提下引入稳健的几何推理与部署可行的域自适应。

Method: 两阶段框架：1）阶段一用大视觉模型提取2D语义特征进行候选排序，并引入轻量的“3D特征可用性评估”模块，预测是否需要3D重排序，兼顾性能与计算成本。2）若需要，阶段二使用“机器人3D检索Transformer”：从RGB估计几何感知的密集3D特征，并通过关键点级匹配计算对应置信度进行重排序（替代余弦相似度），配合轻量适配器做训练-部署域差距弥合。

Result: 在仅用RGB输入、无需显式3D数据情况下，较前SOTA（RoboLLM）在Recall@1上提升7.1%；同时通过按需触发的3D重排序避免了不必要计算与潜在性能退化。

Conclusion: RoboEye通过动态融合2D与域自适应3D推理，在复杂仓储检索中显著提高识别准确率与部署效率，且成本更低、泛化更强；代码已开源，具备实际落地潜力。

Abstract: The rapidly growing number of product categories in large-scale e-commerce
makes accurate object identification for automated packing in warehouses
substantially more difficult. As the catalog grows, intra-class variability and
a long tail of rare or visually similar items increase, and when combined with
diverse packaging, cluttered containers, frequent occlusion, and large
viewpoint changes-these factors amplify discrepancies between query and
reference images, causing sharp performance drops for methods that rely solely
on 2D appearance features. Thus, we propose RoboEye, a two-stage identification
framework that dynamically augments 2D semantic features with domain-adapted 3D
reasoning and lightweight adapters to bridge training deployment gaps. In the
first stage, we train a large vision model to extract 2D features for
generating candidate rankings. A lightweight 3D-feature-awareness module then
estimates 3D feature quality and predicts whether 3D re-ranking is necessary,
preventing performance degradation and avoiding unnecessary computation. When
invoked, the second stage uses our robot 3D retrieval transformer, comprising a
3D feature extractor that produces geometry-aware dense features and a
keypoint-based matcher that computes keypoint-correspondence confidences
between query and reference images instead of conventional cosine-similarity
scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior
state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,
avoiding reliance on explicit 3D inputs and reducing deployment costs. The code
used in this paper is publicly available at:
https://github.com/longkukuhi/RoboEye.

</details>


### [41] [Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders](https://arxiv.org/abs/2509.14975)
*Xuanhua Yin,Dingxin Zhang,Yu Feng,Shunqi Mao,Jianhui Yu,Weidong Cai*

Main category: cs.CV

TL;DR: 提出一种用于点云旋转不变MAE的双流掩码策略：空间网格掩码+渐进语义掩码，通过课程学习动态权重协同，显著提升多数据集、多旋转场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 随机掩码忽视几何结构与语义连贯性，无法建模跨朝向一致的空间关系，也无法保持旋转下稳定的语义部件，从而限制旋转不变自监督学习效果。

Method: 双流掩码：1) 3D空间网格掩码，以坐标排序形成结构化掩码，捕获在不同朝向下仍可保持的一致几何关系；2) 渐进语义掩码，基于注意力驱动的聚类发现语义部件并在掩码时保持部件连贯；两者以课程学习动态加权，从几何优先过渡到语义优先；设计为即插即用，无需改动现有旋转不变框架架构。

Result: 在ModelNet40、ScanObjectNN、OmniObject3D的多种旋转设定下，相比现有旋转不变方法均有一致且显著的性能提升。

Conclusion: 结构化几何掩码与语义一致性掩码的互补、并辅以课程学习，可有效克服随机掩码的局限，提升旋转不变点云MAE的泛化与鲁棒性，且方法通用、易集成。

Abstract: Existing rotation-invariant point cloud masked autoencoders (MAE) rely on
random masking strategies that overlook geometric structure and semantic
coherence. Random masking treats patches independently, failing to capture
spatial relationships consistent across orientations and overlooking semantic
object parts that maintain identity regardless of rotation. We propose a
dual-stream masking approach combining 3D Spatial Grid Masking and Progressive
Semantic Masking to address these fundamental limitations. Grid masking creates
structured patterns through coordinate sorting to capture geometric
relationships that persist across different orientations, while semantic
masking uses attention-driven clustering to discover semantically meaningful
parts and maintain their coherence during masking. These complementary streams
are orchestrated via curriculum learning with dynamic weighting, progressing
from geometric understanding to semantic discovery. Designed as plug-and-play
components, our strategies integrate into existing rotation-invariant
frameworks without architectural changes, ensuring broad compatibility across
different approaches. Comprehensive experiments on ModelNet40, ScanObjectNN,
and OmniObject3D demonstrate consistent improvements across various rotation
scenarios, showing substantial performance gains over the baseline
rotation-invariant methods.

</details>


### [42] [EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence](https://arxiv.org/abs/2509.14977)
*Chaoyin She,Ruifang Lu,Lida Chen,Wei Wang,Qinghua Huang*

Main category: cs.CV

TL;DR: EchoVLM 是面向超声影像的专用视觉-语言模型，基于MoE架构，覆盖七个解剖部位，可同时执行报告生成、诊断与VQA，在报告生成上显著优于通用VLM（BLEU-1 +10.15，ROUGE-1 +4.77）。


<details>
  <summary>Details</summary>
Motivation: 通用VLM在超声场景下知识不足、对多器官病灶泛化差、且多任务诊断效率低；临床超声诊断依赖医生经验，主观性强、效率低，需要一个专用、可泛化、可多任务的模型来提升客观性与效率。

Method: 提出EchoVLM：以Mixture of Experts（MoE）为核心的VLM，使用覆盖七个解剖区域的数据进行训练；面向多任务（报告生成、诊断、VQA）设计，使模型在不同超声任务间共享与专门化表征。

Result: 在超声报告生成任务上，相比Qwen2-VL，BLEU-1提升10.15点、ROUGE-1提升4.77点；体现了在多器官、多任务上的更强表现（摘要未给出更多数值）。

Conclusion: EchoVLM在超声领域展现出优于通用VLM的文本生成与诊断相关能力，有望提高超声诊断准确性与效率，具备临床应用潜力；代码与模型已开源，便于复现与扩展。

Abstract: Ultrasound imaging has become the preferred imaging modality for early cancer
screening due to its advantages of non-ionizing radiation, low cost, and
real-time imaging capabilities. However, conventional ultrasound diagnosis
heavily relies on physician expertise, presenting challenges of high
subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer
promising solutions for this issue, but existing general-purpose models
demonstrate limited knowledge in ultrasound medical tasks, with poor
generalization in multi-organ lesion recognition and low efficiency across
multi-task diagnostics. To address these limitations, we propose EchoVLM, a
vision-language model specifically designed for ultrasound medical imaging. The
model employs a Mixture of Experts (MoE) architecture trained on data spanning
seven anatomical regions. This design enables the model to perform multiple
tasks, including ultrasound report generation, diagnosis and visual
question-answering (VQA). The experimental results demonstrated that EchoVLM
achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and
ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report
generation task. These findings suggest that EchoVLM has substantial potential
to enhance diagnostic accuracy in ultrasound imaging, thereby providing a
viable technical solution for future clinical applications. Source code and
model weights are available at https://github.com/Asunatan/EchoVLM.

</details>


### [43] [SPATIALGEN: Layout-guided 3D Indoor Scene Generation](https://arxiv.org/abs/2509.14981)
*Chuan Fang,Heng Li,Yixun Liang,Jia Zheng,Yongsen Mao,Yuan Liu,Rui Tang,Zihan Zhou,Ping Tan*

Main category: cs.CV

TL;DR: 提出SpatialGen：基于新发布的大规模合成室内数据集的多视角多模态扩散模型，可从给定3D布局与参考图生成一致的外观、几何与语义，多视角一致，优于现有方法，并开源数据与模型。


<details>
  <summary>Details</summary>
Motivation: 现有自动室内场景生成方法在视觉质量、多样性、语义一致性与用户可控性间难以兼顾，且缺乏针对该任务的大规模高质量数据集成为主要瓶颈。

Method: 构建包含12,328个注释场景、57,440个房间与470万张照片级渲染图的大规模合成数据集；基于此提出SpatialGen，多视角多模态扩散框架：输入3D布局与由文本提示得到的参考图，从任意视角同时生成颜色图（外观）、场景坐标图（几何）与语义分割图（三种模态），在跨视角与跨模态上保持空间一致。

Result: 在多项实验中，SpatialGen生成的室内3D场景在真实感、语义一致性与多视角一致性方面均优于现有方法。

Conclusion: 大规模高质量数据集与多视角多模态扩散建模能显著提升室内3D场景生成的质量与一致性；开源数据与模型将推动室内场景理解与生成研究。

Abstract: Creating high-fidelity 3D models of indoor environments is essential for
applications in design, virtual reality, and robotics. However, manual 3D
modeling remains time-consuming and labor-intensive. While recent advances in
generative AI have enabled automated scene synthesis, existing methods often
face challenges in balancing visual quality, diversity, semantic consistency,
and user control. A major bottleneck is the lack of a large-scale, high-quality
dataset tailored to this task. To address this gap, we introduce a
comprehensive synthetic dataset, featuring 12,328 structured annotated scenes
with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this
dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model
that generates realistic and semantically consistent 3D indoor scenes. Given a
3D layout and a reference image (derived from a text prompt), our model
synthesizes appearance (color image), geometry (scene coordinate map), and
semantic (semantic segmentation map) from arbitrary viewpoints, while
preserving spatial consistency across modalities. SpatialGen consistently
generates superior results to previous methods in our experiments. We are
open-sourcing our data and models to empower the community and advance the
field of indoor scene understanding and generation.

</details>


### [44] [PRISM: Product Retrieval In Shopping Carts using Hybrid Matching](https://arxiv.org/abs/2509.14985)
*Arda Kabadayi,Senem Velipasalar,Jiajing Chen*

Main category: cs.CV

TL;DR: PRISM提出一种三阶段混合检索：先用SigLIP粗检Top-35，再用YOLO-E分割去背景，最后用LightGlue做精细像素级匹配，在ABV数据集Top-1提升4.21%，并保持接近实时。


<details>
  <summary>Details</summary>
Motivation: 零售商品检索难在同类不同品牌外观极相似且拍摄视角差异大；CLIP/SigLIP等全局模型难捕捉细微局部差异，而像素级匹配又计算昂贵，需要兼顾效率与精细辨别力的解决方案。

Method: 三阶段混合框架PRISM：1) 以SigLIP从固定图库检回最相似的35个候选，缩小搜索空间；2) 用YOLO-E进行目标分割，去除背景干扰；3) 在候选上用LightGlue进行像素级特征匹配，实现细粒度区分。

Result: 在ABV数据集上，PRISM较当前SOTA提升Top-1精度4.21%，总体推理速度满足实用部署的近实时需求。

Conclusion: 结合全局语义检索与局部像素匹配的分级策略能有效应对高类别间相似度与视角变化问题，既提升准确率又保持可部署效率。

Abstract: Compared to traditional image retrieval tasks, product retrieval in retail
settings is even more challenging. Products of the same type from different
brands may have highly similar visual appearances, and the query image may be
taken from an angle that differs significantly from view angles of the stored
catalog images. Foundational models, such as CLIP and SigLIP, often struggle to
distinguish these subtle but important local differences. Pixel-wise matching
methods, on the other hand, are computationally expensive and incur
prohibitively high matching times. In this paper, we propose a new, hybrid
method, called PRISM, for product retrieval in retail settings by leveraging
the advantages of both vision-language model-based and pixel-wise matching
approaches. To provide both efficiency/speed and finegrained retrieval
accuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)
is employed first to retrieve the top 35 most semantically similar products
from a fixed gallery, thereby narrowing the search space significantly; 2) a
segmentation model (YOLO-E) is applied to eliminate background clutter; 3)
fine-grained pixel-level matching is performed using LightGlue across the
filtered candidates. This framework enables more accurate discrimination
between products with high inter-class similarity by focusing on subtle visual
cues often missed by global models. Experiments performed on the ABV dataset
show that our proposed PRISM outperforms the state-of-the-art image retrieval
methods by 4.21% in top-1 accuracy while still remaining within the bounds of
real-time processing for practical retail deployments.

</details>


### [45] [UCorr: Wire Detection and Depth Estimation for Autonomous Drones](https://arxiv.org/abs/2509.14989)
*Benedikt Kolbeinsson,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: 提出一种基于单目、端到端的细线（电线/拉线）分割与深度估计联合模型，核心是引入在合成数据上训练的时间相关层，显著优于现有方法，提升无人机避障安全性。


<details>
  <summary>Details</summary>
Motivation: 无人机全自主飞行中，细线类障碍物因细长、对比度低、纹理弱而极难被检测与测距，导致潜在碰撞风险；现有方法要么只做分割/检测，要么深度估计不稳健，缺乏对时间信息的有效利用。

Method: 设计单目端到端网络同时输出线缆分割与深度，加入“时间相关（temporal correlation）层”以聚合相邻帧时序信息，从而增强对细薄目标的可见性与几何推断。使用合成数据对该层进行训练，再迁移至真实场景。

Result: 在联合任务（线缆检测+深度估计）上，相比现有竞争方法取得更优性能（文中宣称“superiority”），证明时序相关层与合成数据训练带来收益。

Conclusion: 该方法能更可靠地识别并测距细线障碍，提升无人机导航安全与精度，具备面向真实场景应用的潜力。

Abstract: In the realm of fully autonomous drones, the accurate detection of obstacles
is paramount to ensure safe navigation and prevent collisions. Among these
challenges, the detection of wires stands out due to their slender profile,
which poses a unique and intricate problem. To address this issue, we present
an innovative solution in the form of a monocular end-to-end model for wire
segmentation and depth estimation. Our approach leverages a temporal
correlation layer trained on synthetic data, providing the model with the
ability to effectively tackle the complex joint task of wire detection and
depth estimation. We demonstrate the superiority of our proposed method over
existing competitive approaches in the joint task of wire detection and depth
estimation. Our results underscore the potential of our model to enhance the
safety and precision of autonomous drones, shedding light on its promising
applications in real-world scenarios.

</details>


### [46] [Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation](https://arxiv.org/abs/2509.15011)
*Vasiliki Ismiroglou,Malte Pedersen,Stefan H. Bengtson,Andreas Aakerberg,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: 提出改进的水下图像合成管线，显式建模前向散射并允许介质非均匀；并发布在可控浊度下采集、带参考真值的BUCKET数据集。实验在高浊度场景下相较基准模型有更佳质感与可见性，用户主观选择率82.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理成像模型的水下合成多聚焦于颜色失真，忽略高浊度环境中随距离变化的能见度损失，且常省略前向散射项与介质非均匀性，导致对真实浊水条件的逼真度不足，限制算法训练与评测。

Method: 1) 在水下成像模型中补充前向散射项并建模非均匀介质，使合成图像同时体现颜色衰减与距离相关的散射模糊；2) 搭建合成数据生成管线；3) 采集BUCKET数据集：在受控浊度条件下拍摄真实浊水视频，并提供对应无浊参考图像以作真值。

Result: 相较参考模型，在浊度升高条件下表现出更真实的可视性衰减与成像特性；主观评测中有82.5%的样本被受试者偏好；同时给出数据与代码公开。

Conclusion: 将前向散射与非均匀介质纳入生成模型显著提升了高浊度水下合成数据的逼真度；BUCKET数据集为真实对照评测提供了基础，支持后续算法在浊水条件下的训练与验证。

Abstract: In recent years, the underwater image formation model has found extensive use
in the generation of synthetic underwater data. Although many approaches focus
on scenes primarily affected by discoloration, they often overlook the model's
ability to capture the complex, distance-dependent visibility loss present in
highly turbid environments. In this work, we propose an improved synthetic data
generation pipeline that includes the commonly omitted forward scattering term,
while also considering a nonuniform medium. Additionally, we collected the
BUCKET dataset under controlled turbidity conditions to acquire real turbid
footage with the corresponding reference images. Our results demonstrate
qualitative improvements over the reference model, particularly under
increasing turbidity, with a selection rate of 82. 5\% by survey participants.
Data and code can be accessed on the project page:
vap.aau.dk/sea-ing-through-scattered-rays.

</details>


### [47] [No Modality Left Behind: Adapting to Missing Modalities via Knowledge Distillation for Brain Tumor Segmentation](https://arxiv.org/abs/2509.15017)
*Shenghao Zhu,Yifei Chen,Weihong Chen,Shuo Jiang,Guanyu Zhou,Yuanhan Wang,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: 提出AdaMM框架，在缺失模态MRI情况下进行脑肿瘤分割，通过图引导自适应特征精炼、双瓶颈蒸馏与病灶存在引导的可靠性模块，提升对单/弱模态的鲁棒性与精度，在BraTS2018/2024上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态MRI可提供互补肿瘤信息，但临床常有模态缺失，现有深度模型依赖完整输入且对非优势模态组合泛化差，需一种在缺失模态下仍稳健高效的分割方案。

Method: 构建以知识蒸馏为核心的三模块框架AdaMM：1) 图引导自适应精炼模块（Graph-guided Adaptive Refinement）显式建模可泛化与模态特异特征的语义关联，适配模态缺失；2) 双瓶颈蒸馏模块（Bi-Bottleneck Distillation）通过全局风格匹配与对抗特征对齐，将教师模型的结构与纹理知识迁移给学生；3) 病灶存在引导的可靠性模块（Lesion-Presence-Guided Reliability）借助辅助分类预测各病灶类型的先验存在概率，抑制不完整输入下的假阳性。并系统评估六类缺失模态策略。

Result: 在BraTS 2018与BraTS 2024上，AdaMM在完整与缺失模态、尤其单模态与弱模态配置下均取得显著更高分割精度与鲁棒性，稳定超越现有方法。

Conclusion: 知识蒸馏驱动的AdaMM能在缺失模态场景下有效提升脑肿瘤分割，三模块协同带来更强适应性与更低误检；对比研究验证蒸馏策略的优势，并为方法选择与未来研究提供实践指南。

Abstract: Accurate brain tumor segmentation is essential for preoperative evaluation
and personalized treatment. Multi-modal MRI is widely used due to its ability
to capture complementary tumor features across different sequences. However, in
clinical practice, missing modalities are common, limiting the robustness and
generalizability of existing deep learning methods that rely on complete
inputs, especially under non-dominant modality combinations. To address this,
we propose AdaMM, a multi-modal brain tumor segmentation framework tailored for
missing-modality scenarios, centered on knowledge distillation and composed of
three synergistic modules. The Graph-guided Adaptive Refinement Module
explicitly models semantic associations between generalizable and
modality-specific features, enhancing adaptability to modality absence. The
Bi-Bottleneck Distillation Module transfers structural and textural knowledge
from teacher to student models via global style matching and adversarial
feature alignment. The Lesion-Presence-Guided Reliability Module predicts prior
probabilities of lesion types through an auxiliary classification task,
effectively suppressing false positives under incomplete inputs. Extensive
experiments on the BraTS 2018 and 2024 datasets demonstrate that AdaMM
consistently outperforms existing methods, exhibiting superior segmentation
accuracy and robustness, particularly in single-modality and weak-modality
configurations. In addition, we conduct a systematic evaluation of six
categories of missing-modality strategies, confirming the superiority of
knowledge distillation and offering practical guidance for method selection and
future research. Our source code is available at
https://github.com/Quanato607/AdaMM.

</details>


### [48] [AutoEdit: Automatic Hyperparameter Tuning for Image Editing](https://arxiv.org/abs/2509.15031)
*Chau Pham,Quan Dao,Mahesh Bhosale,Yunjie Tian,Dimitris Metaxas,David Doermann*

Main category: cs.CV

TL;DR: 提出用强化学习自动调参，加速并稳定扩散模型文本引导图像编辑中的超参数搜索。


<details>
  <summary>Details</summary>
Motivation: 现有扩散编辑方法需要人工暴力搜索多个相互依赖的超参数（如反演步数、注意力修改强度等），搜索空间巨大、成本高、效率低，严重影响实际落地。

Method: 将“在扩散去噪过程中的超参数选择”建模为序贯决策问题，构建MDP：状态随去噪步推进，动作为动态调整各超参数；以编辑质量/一致性等目标构造奖励。采用PPO实现高效策略学习，在推理时按步自适应设定超参数。

Result: 在多组实验中，相比暴力搜索显著减少搜索时间与计算开销，同时保持或提升编辑质量；在实际应用中更稳定、可复用。

Conclusion: 用RL在扩散去噪过程中自适应调参可替代昂贵的网格/随机搜索，实现时间效率与编辑表现的兼得，推动扩散编辑方法走向实用。

Abstract: Recent advances in diffusion models have revolutionized text-guided image
editing, yet existing editing methods face critical challenges in
hyperparameter identification. To get the reasonable editing performance, these
methods often require the user to brute-force tune multiple interdependent
hyperparameters, such as inversion timesteps and attention modification,
\textit{etc.} This process incurs high computational costs due to the huge
hyperparameter search space. We consider searching optimal editing's
hyperparameters as a sequential decision-making task within the diffusion
denoising process. Specifically, we propose a reinforcement learning framework,
which establishes a Markov Decision Process that dynamically adjusts
hyperparameters across denoising steps, integrating editing objectives into a
reward function. The method achieves time efficiency through proximal policy
optimization while maintaining optimal hyperparameter configurations.
Experiments demonstrate significant reduction in search time and computational
overhead compared to existing brute-force approaches, advancing the practical
deployment of a diffusion-based image editing framework in the real world.

</details>


### [49] [Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies](https://arxiv.org/abs/2509.15045)
*Luisa Torquato Niño,Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: 论文利用纯合成数据与域随机化训练YOLOv11检测特定目标（汤罐），通过多种增强、数据集配置与模型尺度实验，在Kaggle隐藏测试集上取得mAP@50=0.910，证明纯合成训练可行但仍存在真实域泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 合成数据易获取、可控且廉价，但与真实世界存在域差距，导致在真实场景的检测性能下降。作者希望验证在仅用合成数据的前提下，能否通过提升数据多样性和合理的增强/模型设置，缩小合成到真实的性能差距，并在客观竞赛评测中验证。

Method: 以YOLOv11为基线，对单类目标（汤罐）进行检测。采用域随机化生成多样化的合成数据，系统研究：1）数据增强策略强度与组合；2）数据集构成（视角、背景复杂度等多样性）；3）模型尺度（如YOLOv11l）。评估上，发现合成验证指标与真实性能不一致，因而引入：- 定性可视化检查预测；- 在人工标注的真实测试集上量化评估；- 以Kaggle官方隐藏测试集mAP@50为最终指标。

Result: 关键发现：提升合成数据的多样性（多视角与复杂背景）并配合精调的数据增强，显著缩小域差距。最佳配置为使用更大规模且更多样的合成数据训练的YOLOv11l，最终在Kaggle隐藏测试集上达到mAP@50=0.910。合成验证分数 consistently 高但对真实表现预测性较差。

Conclusion: 纯合成数据结合域随机化可以在特定目标检测任务上实现强竞争力的真实域性能（mAP@50=0.910），但完全覆盖真实世界变异性仍具挑战。实践上应优先增加数据多样性、谨慎调参增强，并用真实小规模标注集与定性检查校准合成验证的偏差。

Abstract: This paper addresses the synthetic-to-real domain gap in object detection,
focusing on training a YOLOv11 model to detect a specific object (a soup can)
using only synthetic data and domain randomization strategies. The methodology
involves extensive experimentation with data augmentation, dataset composition,
and model scaling. While synthetic validation metrics were consistently high,
they proved to be poor predictors of real-world performance. Consequently,
models were also evaluated qualitatively, through visual inspection of
predictions, and quantitatively, on a manually labeled real-world test set, to
guide development. Final mAP@50 scores were provided by the official Kaggle
competition. Key findings indicate that increasing synthetic dataset diversity,
specifically by including varied perspectives and complex backgrounds, combined
with carefully tuned data augmentation, were crucial in bridging the domain
gap. The best performing configuration, a YOLOv11l model trained on an expanded
and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's
hidden test set. This result demonstrates the potential of a synthetic-only
training approach while also highlighting the remaining challenges in fully
capturing real-world variability.

</details>


### [50] [Transplant-Ready? Evaluating AI Lung Segmentation Models in Candidates with Severe Lung Disease](https://arxiv.org/abs/2509.15083)
*Jisoo Lee,Michael R. Harowicz,Yuwen Chen,Hanxue Gu,Isaac S. Alderete,Lin Li,Maciej A. Mazurowski,Matthew G. Hartwig*

Main category: cs.CV

TL;DR: 评估三种公开深度学习肺分割模型（Unet-R231、TotalSegmentator、MedSAM）在肺移植候选患者不同病情严重度与病理类别下的表现；Unet-R231总体最好，但所有模型在中-重度病变时性能明显下降，提示需在重症场景中进行专项微调。


<details>
  <summary>Details</summary>
Motivation: 肺移植术前计划需要可靠的肺分割，但公开模型多在普通人群或轻症数据上训练，重症、多病理共存患者中的鲁棒性未知。本研究旨在系统量化这些模型在移植候选人群（复杂病变、不同严重度、双侧肺）中的适用性与局限。

Method: 回顾性纳入2017–2019年32名在Duke行胸部CT的移植候选患者（3645张轴位切片），选择含≥2种不同严重度病理的标准轴扫。采用三种现有模型（Unet-R231、TotalSegmentator、MedSAM）自动分割肺。用体积相似度、Dice、Hausdorff距离等量化指标与四分临床可接受度进行评估，并分严重度、病理类别、左右肺分层比较；统计显著性以p<0.05判定。

Result: Unet-R231在总体、不同严重度与病理类别分层中均显著优于TotalSegmentator与MedSAM（p<0.05）。所有模型从轻度到中-重度病例性能显著下降，尤其在体积相似度上（p<0.05）。左右肺或具体病理类型间无显著差异。TotalSegmentator表现次之，MedSAM相对较弱。

Conclusion: 在移植候选人复杂病变人群中，Unet-R231是三者中最准确的自动分割方案，TotalSegmentator紧随其后；但中-重度病变会显著削弱各模型表现。临床应用（特别是术前计划）需对重症场景进行专门微调或再训练，并结合人工复核以确保可靠性。

Abstract: This study evaluates publicly available deep-learning based lung segmentation
models in transplant-eligible patients to determine their performance across
disease severity levels, pathology categories, and lung sides, and to identify
limitations impacting their use in preoperative planning in lung
transplantation. This retrospective study included 32 patients who underwent
chest CT scans at Duke University Health System between 2017 and 2019 (total of
3,645 2D axial slices). Patients with standard axial CT scans were selected
based on the presence of two or more lung pathologies of varying severity. Lung
segmentation was performed using three previously developed deep learning
models: Unet-R231, TotalSegmentator, MedSAM. Performance was assessed using
quantitative metrics (volumetric similarity, Dice similarity coefficient,
Hausdorff distance) and a qualitative measure (four-point clinical
acceptability scale). Unet-R231 consistently outperformed TotalSegmentator and
MedSAM in general, for different severity levels, and pathology categories
(p<0.05). All models showed significant performance declines from mild to
moderate-to-severe cases, particularly in volumetric similarity (p<0.05),
without significant differences among lung sides or pathology types. Unet-R231
provided the most accurate automated lung segmentation among evaluated models
with TotalSegmentator being a close second, though their performance declined
significantly in moderate-to-severe cases, emphasizing the need for specialized
model fine-tuning in severe pathology contexts.

</details>


### [51] [OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation](https://arxiv.org/abs/2509.15096)
*Bo-Wen Yin,Jiao-Long Cao,Xuying Zhang,Yuming Chen,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: 提出OmniSegmentor：在ImageNet基础上构建多模态预训练数据集ImageNeXt（含5种视觉模态），并给出通用高效的多模态预训练机制，使模型能在任意模态组合下进行鲁棒语义分割，取得多数据集SOTA。


<details>
  <summary>Details</summary>
Motivation: 多模态线索能提升语义分割鲁棒性，但缺少一个可灵活“预训练-微调”的统一范式，尤其能适配多种视觉模态及其任意组合。

Method: 1) 构建ImageNeXt：基于ImageNet扩展出含五种视觉模态的大规模预训练数据；2) 设计通用高效的多模态预训练策略，使单一模型学习并编码不同模态信息，支持任意模态组合在下游分割任务中协同使用。

Result: 在多模态语义分割基准（NYU Depthv2、EventScape、MFNet、DeLiVER、SUNRGBD、KITTI-360）上取得新的SOTA表现，显示对多种场景与模态组合具有一致增益。

Conclusion: 通用多模态预训练框架OmniSegmentor与ImageNeXt数据集显著提升了跨场景与模态组合下的感知能力，为多模态语义分割提供了灵活、可扩展的预训练-微调范式。

Abstract: Recent research on representation learning has proved the merits of
multi-modal clues for robust semantic segmentation. Nevertheless, a flexible
pretrain-and-finetune pipeline for multiple visual modalities remains
unexplored. In this paper, we propose a novel multi-modal learning framework,
termed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, we
assemble a large-scale dataset for multi-modal pretraining, called ImageNeXt,
which contains five popular visual modalities. 2) We provide an efficient
pretraining manner to endow the model with the capacity to encode different
modality information in the ImageNeXt. For the first time, we introduce a
universal multi-modal pretraining framework that consistently amplifies the
model's perceptual capabilities across various scenarios, regardless of the
arbitrary combination of the involved modalities. Remarkably, our OmniSegmentor
achieves new state-of-the-art records on a wide range of multi-modal semantic
segmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER,
SUNRGBD, and KITTI-360.

</details>


### [52] [RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes](https://arxiv.org/abs/2509.15123)
*Fang Li,Hao Zhang,Narendra Ahuja*

Main category: cs.CV

TL;DR: 提出一种仅用单目RGB视频监督，在动态场景中高效准确优化相机参数的方法，包含补丁跟踪过滤、鲁棒联合优化与两阶段策略，并在多数据集上优于依赖多种先验的传统/改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有如COLMAP在静态场景表现好，但在动态场景需GT运动掩码且运行耗时；许多改进依赖焦距、掩码、点云、位姿、度量深度等先验，随手拍视频通常不可得。因此需要一种仅依赖RGB视频、能鲁棒处理运动并高效优化相机参数的方法。

Method: 1) Patch-wise Tracking Filters：在视频中进行基于补丁的稳健、稀疏“铰链式”关联，提升对应关系质量并降低规模。2) Outlier-aware Joint Optimization：联合优化相机参数，自动下调运动异常点权重，无需运动先验。3) Two-stage Optimization：通过在损失中软阈值（Softplus）界与凸极小之间折中，先稳后快的两阶段策略提升稳定性与速度。随后将估计相机用于4D重建以间接验证准确性。

Result: 在4个真实数据集（NeRF-DS、DAVIS、iPhone、TUM-dynamics）和1个合成数据集（MPI-Sintel）上，方法仅用单个RGB视频即可更高效、更准确地估计相机参数，视觉与数值指标均优于对比；将其输入4D重建后得到更高质量的3D场景与渲染RGB/深度图。

Conclusion: 该方法摆脱对多种外部先验与GT掩码的依赖，仅凭单目视频即可在动态场景中稳健、快速地优化相机参数，并在多数据集上验证了准确性与效率，具有实践价值。

Abstract: Although COLMAP has long remained the predominant method for camera parameter
optimization in static scenes, it is constrained by its lengthy runtime and
reliance on ground truth (GT) motion masks for application to dynamic scenes.
Many efforts attempted to improve it by incorporating more priors as
supervision such as GT focal length, motion masks, 3D point clouds, camera
poses, and metric depth, which, however, are typically unavailable in casually
captured RGB videos. In this paper, we propose a novel method for more accurate
and efficient camera parameter optimization in dynamic scenes solely supervised
by a single RGB video. Our method consists of three key components: (1)
Patch-wise Tracking Filters, to establish robust and maximally sparse
hinge-like relations across the RGB video. (2) Outlier-aware Joint
Optimization, for efficient camera parameter optimization by adaptive
down-weighting of moving outliers, without reliance on motion priors. (3) A
Two-stage Optimization Strategy, to enhance stability and optimization speed by
a trade-off between the Softplus limits and convex minima in losses. We
visually and numerically evaluate our camera estimates. To further validate
accuracy, we feed the camera estimates into a 4D reconstruction method and
assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform
experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)
and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates
camera parameters more efficiently and accurately with a single RGB video as
the only supervision.

</details>


### [53] [MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation](https://arxiv.org/abs/2509.15154)
*Gengliang Li,Rongyu Chen,Bin Li,Linlin Yang,Guodong Ding*

Main category: cs.CV

TL;DR: MEDFACT-R1通过“外部知识对齐 + 强化学习”两阶段，显著提升医学视觉-语言模型的事实一致性与推理可靠性，在3个医学QA基准上最高提升22.5%事实准确率。


<details>
  <summary>Details</summary>
Motivation: 医学多模态问答常出现事实错误与不稳定推理，缺乏可靠的医学知识对齐与一致性约束。作者希望将权威外部知识纳入训练，并用强化学习塑造更自洽、可验证的医学推理过程。

Method: 两阶段框架：1) 伪标签监督微调（SFT），将外部医学知识接入模型，完成“冷启动”知识对齐；2) 基于Group Relative Policy Optimization (GRPO) 的强化学习，引入四类面向事实的奖励信号，鼓励自洽、可核验的推理。并进行消融分析验证各组件作用。

Result: 在三个公开医学QA基准上，相比SOTA，事实准确率最高提升22.5%。消融显示：没有伪标签SFT冷启动将显著降效；四个GRPO奖励各自均有贡献，组合最优。

Conclusion: 知识落地（grounding）与RL驱动的推理相辅相成，可明显提升医学VLM的事实一致性与可信度；框架通用、可复现（代码已开源）。

Abstract: Ensuring factual consistency and reliable reasoning remains a critical
challenge for medical vision-language models. We introduce MEDFACT-R1, a
two-stage framework that integrates external knowledge grounding with
reinforcement learning to improve the factual medical reasoning. The first
stage uses pseudo-label supervised fine-tuning (SFT) to incorporate external
factual expertise; while the second stage applies Group Relative Policy
Optimization (GRPO) with four tailored factual reward signals to encourage
self-consistent reasoning. Across three public medical QA benchmarks,
MEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over
previous state-of-the-art methods. Ablation studies highlight the necessity of
pseudo-label SFT cold start and validate the contribution of each GRPO reward,
underscoring the synergy between knowledge grounding and RL-driven reasoning
for trustworthy medical AI. Codes are released at
https://github.com/Garfieldgengliang/MEDFACT-R1.

</details>


### [54] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang,Minghao Guo,Dequan Yang,Wenyu Wang*

Main category: cs.CV

TL;DR: 将经典几何视觉错觉作为辅助任务融入ImageNet训练，能提升CNN与Transformer在复杂纹理与细轮廓等困难样例上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度模型主要利用数据统计规律，缺乏来自知觉心理学的结构性归纳偏置。作者希望验证：将人类知觉中成熟研究的几何错觉引入训练，是否能作为有效的感知先验，提升模型的结构敏感性与泛化。

Method: 构建可控参数化的合成几何错觉数据集；设计三种多源学习策略，把“错觉识别”与ImageNet分类联合（多任务/多源）训练；在CNN与Transformer架构上系统评测其影响。

Result: 两点核心发现：(1) 将几何错觉作为辅助监督可系统性提升泛化，尤其在复杂轮廓与细粒度纹理等困难场景；(2) 即使这些合成、看似与自然图像无关的刺激，也能增强模型的结构敏感性，适用于CNN与Transformer。

Conclusion: 将知觉科学中的几何错觉嵌入训练流程，为在视觉模型中注入感知先验提供了可行路径，打通知觉心理与机器学习，提示未来可系统探索更多感知驱动的归纳偏置以改进视觉模型设计。

Abstract: Contemporary deep learning models have achieved impressive performance in
image classification by primarily leveraging statistical regularities within
large datasets, but they rarely incorporate structured insights drawn directly
from perceptual psychology. To explore the potential of perceptually motivated
inductive biases, we propose integrating classic geometric visual illusions
well-studied phenomena from human perception into standard image-classification
training pipelines. Specifically, we introduce a synthetic, parametric
geometric-illusion dataset and evaluate three multi-source learning strategies
that combine illusion recognition tasks with ImageNet classification
objectives. Our experiments reveal two key conceptual insights: (i)
incorporating geometric illusions as auxiliary supervision systematically
improves generalization, especially in visually challenging cases involving
intricate contours and fine textures; and (ii) perceptually driven inductive
biases, even when derived from synthetic stimuli traditionally considered
unrelated to natural image recognition, can enhance the structural sensitivity
of both CNN and transformer-based architectures. These results demonstrate a
novel integration of perceptual science and machine learning and suggest new
directions for embedding perceptual priors into vision model design.

</details>


### [55] [AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt](https://arxiv.org/abs/2509.15159)
*Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: 论文提出一种针对RAG系统的“对抗性指令提示（AIP）”攻击，通过篡改共享/复用的系统级指令提示，悄然改变检索行为，从而操纵生成结果；借助多样化查询生成与遗传算法联合优化，兼顾攻击成功率、任务可用性与隐蔽性，实验ASR最高达95.23%。


<details>
  <summary>Details</summary>
Motivation: 现有RAG攻击多依赖操纵用户查询，但真实场景中用户输入常受限或受保护；而系统/公开共享的指令提示被广泛复用、少审计且被默认信任，是更现实且隐蔽的攻击入口，需揭示其风险。

Method: 1) 将攻击面从用户查询转移到“指令提示”；2) 设计多样化查询生成，模拟真实语言变化，筛选能在改写与复述下仍生效的对抗提示；3) 基于遗传算法的联合优化，综合优化攻击成功率、正常任务效用与自然度（隐蔽性），迭代进化对抗提示。

Result: 在多组实验中，所提出AIP在保持良好正常功能（可用性）的同时，最高取得95.23%的攻击成功率（ASR），对不同查询变体具鲁棒性，能稳定诱导检索偏移并操纵RAG输出。

Conclusion: RAG系统存在被“可信指令提示”武器化的关键且被忽视的漏洞。应重新审视与审计共享/复用的指令提示，并开发针对提示层面的防御机制，以维护RAG系统完整性与可验证性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources to improve factual accuracy
and verifiability. However, this reliance introduces new attack surfaces within
the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have
exposed such vulnerabilities, they largely rely on manipulating user queries,
which is often infeasible in practice due to fixed or protected user inputs.
This narrow focus overlooks a more realistic and stealthy vector: instructional
prompts, which are widely reused, publicly shared, and rarely audited. Their
implicit trust makes them a compelling target for adversaries to manipulate RAG
behavior covertly.
  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that
exploits adversarial instructional prompts to manipulate RAG outputs by subtly
altering retrieval behavior. By shifting the attack surface to the
instructional prompts, AIP reveals how trusted yet seemingly benign interface
components can be weaponized to degrade system integrity. The attack is crafted
to achieve three goals: (1) naturalness, to evade user detection; (2) utility,
to encourage use of prompts; and (3) robustness, to remain effective across
diverse query variations. We propose a diverse query generation strategy that
simulates realistic linguistic variation in user queries, enabling the
discovery of prompts that generalize across paraphrases and rephrasings.
Building on this, a genetic algorithm-based joint optimization is developed to
evolve adversarial prompts by balancing attack success, clean-task utility, and
stealthiness. Experimental results show that AIP achieves up to 95.23% ASR
while preserving benign functionality. These findings uncover a critical and
previously overlooked vulnerability in RAG systems, emphasizing the need to
reassess the shared instructional prompts.

</details>


### [56] [Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model](https://arxiv.org/abs/2509.15167)
*Pak-Hei Yeung,Jayroop Ramesh,Pengfei Lyu,Ana Namburete,Jagath Rajapakse*

Main category: cs.CV

TL;DR: 从2D自然图像预训练的通用视觉模型向3D医学图像分割迁移知识，在少量标注+大量未标注的半监督场景中，通过双模型协同与自适应采样实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D医学分割标注昂贵且稀缺，现有半监督方法有限；而强大的2D预训练模型信息丰富但与3D域存在模态差异与维度鸿沟，如何有效、稳健地把2D知识迁移给3D模型以提升小样本分割性能，是关键问题。

Method: 提出模型无关的M&N框架：初始化一个2D预训练模型与一个从零开始训练的3D分割模型，进行迭代式协同训练（co-training），两者互相为对方生成伪掩码以扩充监督；并引入“学习率引导采样”，根据训练中预测准确性与稳定性动态调整每个batch中标注/未标注样本比例，以降低不准伪标的负面影响。

Result: 在多个公开数据集、不同标注比例设置下，M&N均优于13种现有半监督分割方法，达到SOTA；消融实验显示方法对具体网络架构不敏感，能与不同模型无缝集成。

Conclusion: M&N能有效将2D预训练知识渐进蒸馏到3D分割模型，在半监督场景中显著提升性能且具有架构无关的可适配性，随更强模型出现可继续受益。

Abstract: This paper explores the transfer of knowledge from general vision models
pretrained on 2D natural images to improve 3D medical image segmentation. We
focus on the semi-supervised setting, where only a few labeled 3D medical
images are available, along with a large set of unlabeled images. To tackle
this, we propose a model-agnostic framework that progressively distills
knowledge from a 2D pretrained model to a 3D segmentation model trained from
scratch. Our approach, M&N, involves iterative co-training of the two models
using pseudo-masks generated by each other, along with our proposed learning
rate guided sampling that adaptively adjusts the proportion of labeled and
unlabeled data in each training batch to align with the models' prediction
accuracy and stability, minimizing the adverse effect caused by inaccurate
pseudo-masks. Extensive experiments on multiple publicly available datasets
demonstrate that M&N achieves state-of-the-art performance, outperforming
thirteen existing semi-supervised segmentation approaches under all different
settings. Importantly, ablation studies show that M&N remains model-agnostic,
allowing seamless integration with different architectures. This ensures its
adaptability as more advanced models emerge. The code is available at
https://github.com/pakheiyeung/M-N.

</details>


### [57] [A Race Bias Free Face Aging Model for Reliable Kinship Verification](https://arxiv.org/abs/2509.15177)
*Ali Nazari,Bardiya Kariminia,Mohsen Ebrahimi Moghaddam*

Main category: cs.CV

TL;DR: 提出RA-GAN以缓解亲属验证中的年龄差与种族偏差问题，通过生成无种族偏差的同龄人脸来提升验证准确率；在种族准确率与身份保持上优于现有方法，并在KinFaceW数据集上显著提升多类亲属关系的验证准确率。


<details>
  <summary>Details</summary>
Motivation: 亲子照片往往存在显著拍摄时间差，同龄照片难以获得；现有人脸老化模型存在种族偏差，影响生成图像的相似度与亲属验证效果。因此需要一种能在年龄变换中减少种族偏差并更好保留身份特征的方法，以更公平有效地进行亲属身份验证。

Method: 提出RA-GAN，包括两个新模块：RACEpSp与特征混合器（feature mixer），用于生成种族无偏的人脸老化/返老还童图像。将亲子图像转换到相同年龄，再用于亲属验证实验，评估在不同年龄段的种族准确率与身份保持能力以及对验证准确率的提升。

Result: RA-GAN在所有年龄段的平均种族准确率上比SAM-GAN高13.14%，在60+年龄段比CUSP-GAN高9.1%；在身份保持方面也优于SAM-GAN与CUSP-GAN。将KinFaceW-I/II中的亲子图像转换到同龄后，亲属验证准确率全面提升：KinFaceW-I上父-子、父-女、母-子、母-女分别提升5.22、5.12、1.63、0.41；KinFaceW-II上父-女、父-子、母-子分别提升2.9、0.39、1.6。

Conclusion: 通过RA-GAN进行无种族偏差的年龄变换，可有效缓解亲属验证中的年龄差影响并提升公平性与准确率；方法在多年龄段与多关系上优于现有GAN老化模型，验证了同龄化变换对亲属验证的实际益处，并开源代码以促进复现。

Abstract: The age gap in kinship verification addresses the time difference between the
photos of the parent and the child. Moreover, their same-age photos are often
unavailable, and face aging models are racially biased, which impacts the
likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN,
consisting of two new modules, RACEpSp and a feature mixer, to produce racially
unbiased images. The unbiased synthesized photos are used in kinship
verification to investigate the results of verifying same-age parent-child
images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an
average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by
9.1\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects'
identities better than SAM-GAN and CUSP-GAN across all age groups.
Additionally, we demonstrate that transforming parent and child images from the
KinFaceW-I and KinFaceW-II datasets to the same age can enhance the
verification accuracy across all age groups. The accuracy increases with our
RA-GAN for the kinship relationships of father-son and father-daughter,
mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41,
respectively, on KinFaceW-I. Additionally, the accuracy for the relationships
of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on
KinFaceW-II, respectively. The code is available
at~\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}

</details>


### [58] [Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding](https://arxiv.org/abs/2509.15178)
*Zaiquan Yang,Yuhao Liu,Gerhard Hancke,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 提出一个基于多模态大模型（MLLM）的零样本时空视频指代（STVG）框架，通过“分解式时空高亮（DSTH）”与“时间增强组装（TAS）”两项策略，利用MLLM的潜在“指代token”并缓解其整合文本线索不足的问题，在三大基准上超过SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM具备一定零样本指代能力，但存在两点问题：1）会自发分配用于定位的特殊“指代token”，但未被系统化利用；2）难以充分整合文本中的属性与动作等多线索，导致指代不稳定、时空一致性差。作者希望在不额外监督的前提下，激发MLLM的时空推理与稳定定位能力。

Method: 提出零样本STVG框架：
- DSTH：将原始文本查询解耦为“属性子查询”和“动作子查询”，分别在空间与时间维度探测目标；引入“logit引导再注意（LRA）”模块，通过对每个子查询的token预测进行正则化，学习到用于空间与时间的潜在提示（prompts），高亮属性与动作相关区域/时刻。
- TAS：考虑属性子查询的空间定位应具备时间一致性，使用原始帧与“时间增强帧”双路输入，组装预测以提升跨帧一致性与稳健性。
框架可无缝套用多种MLLM。

Result: 在三大常见STVG基准上，基于不同MLLM的实现均超越现有SOTA，显示出优越的零样本时空指代性能与更好的时序一致性。（摘要未给出具体数值）

Conclusion: MLLM在STVG中的零样本潜力可通过显式利用“指代token”与分解式线索融合被显著激活。DSTH与TAS共同提升了空间与时间维度的定位质量与一致性，方法通用且可移植，优于现有方法。

Abstract: Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal
tube of a video, as specified by the input text query. In this paper, we
utilize multimodal large language models (MLLMs) to explore a zero-shot
solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to
dynamically assign special tokens, referred to as \textit{grounding tokens},
for grounding the text query; and (2) MLLMs often suffer from suboptimal
grounding due to the inability to fully integrate the cues in the text query
(\textit{e.g.}, attributes, actions) for inference. Based on these insights, we
propose a MLLM-based zero-shot framework for STVG, which includes novel
decomposed spatio-temporal highlighting (DSTH) and temporal-augmented
assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH
strategy first decouples the original query into attribute and action
sub-queries for inquiring the existence of the target both spatially and
temporally. It then uses a novel logit-guided re-attention (LRA) module to
learn latent variables as spatial and temporal prompts, by regularizing token
predictions for each sub-query. These prompts highlight attribute and action
cues, respectively, directing the model's attention to reliable spatial and
temporal related visual regions. In addition, as the spatial grounding by the
attribute sub-query should be temporally consistent, we introduce the TAS
strategy to assemble the predictions using the original video frames and the
temporal-augmented frames as inputs to help improve temporal consistency. We
evaluate our method on various MLLMs, and show that it outperforms SOTA methods
on three common STVG benchmarks.
  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.

</details>


### [59] [Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11, YOLOv12 and Faster-RCNN](https://arxiv.org/abs/2509.15181)
*Dewi Endah Kharismawati,Toni Kazic*

Main category: cs.CV

TL;DR: 提出MSDD高质量航拍玉米苗期数据集，用于单/双/三株识别与站位计数；在V4–V6、垂直视角下检测最佳；YOLO11最快、YOLOv9单株最准；单株精度最高但多株受稀有与类不平衡影响难度大，支持实时决策与精准农业。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要准确的苗期站位计数以指导补栽、投入调整和产量预测，但人工方法费时易错，且缺少高质量、覆盖多场景的公开数据集。

Method: 构建并发布MSDD航拍数据集，标注三类（单株、双株、三株），覆盖多生长阶段、种植方式、土壤、光照、相机角度与密度；对多种检测模型进行基准评测，比较不同生育期与视角下的性能与速度。

Result: 检测在V4–V6阶段和垂直（nadir）视角最可靠；YOLO11推理最快（35 ms/图，另存结果约120 ms），YOLOv9在单株精度最高；单株检测精度最高（P至0.984，R至0.873），双株与三株因稀有与形态不规则、类不平衡导致性能显著下降。

Conclusion: MSDD为玉米苗期自动检测与站位计数提供坚实基础，可提升模型鲁棒性与实时应用能力，促进资源优化配置与精准农业决策。但多株识别仍受类不平衡与外观多样性限制，需进一步方法改进。

Abstract: Accurate maize seedling detection is crucial for precision agriculture, yet
curated datasets remain scarce. We introduce MSDD, a high-quality aerial image
dataset for maize seedling stand counting, with applications in early-season
crop monitoring, yield prediction, and in-field management. Stand counting
determines how many plants germinated, guiding timely decisions such as
replanting or adjusting inputs. Traditional methods are labor-intensive and
error-prone, while computer vision enables efficient, accurate detection. MSDD
contains three classes-single, double, and triple plants-capturing diverse
growth stages, planting setups, soil types, lighting conditions, camera angles,
and densities, ensuring robustness for real-world use. Benchmarking shows
detection is most reliable during V4-V6 stages and under nadir views. Among
tested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for
single plants. Single plant detection achieves precision up to 0.984 and recall
up to 0.873, but detecting doubles and triples remains difficult due to rarity
and irregular appearance, often from planting errors. Class imbalance further
reduces accuracy in multi-plant detection. Despite these challenges, YOLO11
maintains efficient inference at 35 ms per image, with an additional 120 ms for
saving outputs. MSDD establishes a strong foundation for developing models that
enhance stand counting, optimize resource allocation, and support real-time
decision-making. This dataset marks a step toward automating agricultural
monitoring and advancing precision agriculture.

</details>


### [60] [Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation](https://arxiv.org/abs/2509.15185)
*Xiaoyu Yue,Zidong Wang,Yuqing Wang,Wenlong Zhang,Xihui Liu,Wanli Ouyang,Lei Bai,Luping Zhou*

Main category: cs.CV

TL;DR: 提出ST-AR：在自回归图像生成中加入自监督目标，缓解视觉语义学习受限问题，显著提升理解与生成质量（对LlamaGen提升约42%-49%的FID）。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）模型源自文本领域，将其用于图像生成时在高层视觉理解上受限；现有生成模型也显示理解能力不足，导致生成质量与一致性欠佳。因此需要系统性分析AR在视觉域的机制缺陷，并寻找无需外部预训练表征即可提升语义建模的方法。

Method: 提出Self-guided Training for AutoRegressive models（ST-AR）：在传统下一 token 预测训练中，加入自监督学习目标作为辅导信号，以应对三类关键问题——（1）局部与条件依赖导致的短视；（2）跨步语义不一致；（3）空间不变性缺失。该框架不依赖外部预训练表征，直接在训练阶段联合优化AR损失与自监督目标，从而引导模型学习更稳健的高层视觉语义。

Result: 在相同采样策略下，ST-AR显著提升了LlamaGen系列的生成质量：LlamaGen-L的FID约提升42%，LlamaGen-XL约提升49%；同时增强了模型对图像语义的理解能力。

Conclusion: 通过将自监督目标融入AR训练，可系统性缓解视觉域AR模型的结构性缺陷，提升高层语义建模与生成质量，无需依赖外部表征模型；该范式为改进AR类图像生成提供了通用训练框架。

Abstract: Recent studies have demonstrated the importance of high-quality visual
representations in image generation and have highlighted the limitations of
generative models in image understanding. As a generative paradigm originally
designed for natural language, autoregressive models face similar challenges.
In this work, we present the first systematic investigation into the mechanisms
of applying the next-token prediction paradigm to the visual domain. We
identify three key properties that hinder the learning of high-level visual
semantics: local and conditional dependence, inter-step semantic inconsistency,
and spatial invariance deficiency. We show that these issues can be effectively
addressed by introducing self-supervised objectives during training, leading to
a novel training framework, Self-guided Training for AutoRegressive models
(ST-AR). Without relying on pre-trained representation models, ST-AR
significantly enhances the image understanding ability of autoregressive models
and leads to improved generation quality. Specifically, ST-AR brings
approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for
LlamaGen-XL, while maintaining the same sampling strategy.

</details>


### [61] [Geometric Image Synchronization with Deep Watermarking](https://arxiv.org/abs/2509.15208)
*Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 提出SyncSeal，一种专为图像几何同步（估计并逆转裁剪、旋转等变换）的水印增强方法，通过嵌入器+提取器端到端训练预测变换参数，并用判别器保持感知质量；可叠加在现有水印上，显著提升其对几何与强度（valuemetric）变换的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有数字水印在面对几何变换（如旋转、裁剪、缩放、平移等）时易失效。为保证水印在实际传播中经常遭遇的几何与强度变换后仍可被正确解码，需要一种能够恢复或“对齐”图像到嵌入坐标系的同步机制。

Method: 设计SyncSeal：包含（1）不可感知地修改图像的嵌入器网络；（2）预测图像所受几何变换参数的提取器网络；（3）对抗式判别器以维持高感知质量。通过端到端训练最小化预测与真值变换参数之间的误差，实现对变换的准确估计；推理时先用提取器估计并逆变换，从而恢复同步后再进行常规水印解码。

Result: 在多种几何与亮度/对比度等valuemetric变换上进行实验，SyncSeal可准确估计并纠正变换，实现稳健同步；将其叠加到现有水印方案上，显著提升这些方法对先前脆弱的几何攻击的鲁棒性。

Conclusion: SyncSeal通过学习式几何同步模块为水印系统提供通用的前端防护，可与现有方法无缝结合，显著增强其在实际复杂变换场景下的可靠性与可用性。

Abstract: Synchronization is the task of estimating and inverting geometric
transformations (e.g., crop, rotation) applied to an image. This work
introduces SyncSeal, a bespoke watermarking method for robust image
synchronization, which can be applied on top of existing watermarking methods
to enhance their robustness against geometric transformations. It relies on an
embedder network that imperceptibly alters images and an extractor network that
predicts the geometric transformation to which the image was subjected. Both
networks are end-to-end trained to minimize the error between the predicted and
ground-truth parameters of the transformation, combined with a discriminator to
maintain high perceptual quality. We experimentally validate our method on a
wide variety of geometric and valuemetric transformations, demonstrating its
effectiveness in accurately synchronizing images. We further show that our
synchronization can effectively upgrade existing watermarking methods to
withstand geometric transformations to which they were previously vulnerable.

</details>


### [62] [RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](https://arxiv.org/abs/2509.15212)
*Yuming Jiang,Siteng Huang,Shengke Xue,Yaxi Zhao,Jun Cen,Sicong Leng,Kehan Li,Jiayan Guo,Kexiang Wang,Mingxiu Chen,Fan Wang,Deli Zhao,Xin Li*

Main category: cs.CV

TL;DR: 提出RynnVLA-001：基于大规模人类示范视频的视觉-语言-行动模型，通过两阶段预训练（自我视角视频生成+人类关键点轨迹联合建模）并引入ActionVAE压缩动作空间，微调后在机器人任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型初始化不足、视频到行动的桥接弱、动作表示维度高导致学习困难与泛化受限。作者希望通过生成式视频预训练与轨迹/动作表征压缩来提升下游机器人任务性能。

Method: 两阶段预训练：1) 自我视角视频生成预训练，用12M自我视角操作视频，训练图像到视频模型，根据初始帧和语言指令预测未来帧；2) 人类中心的轨迹感知建模，在视频预测的同时联合预测未来关键点轨迹，连接视觉预测与动作预测。另提出ActionVAE，将动作序列压缩为低维潜变量，简化VLA输出空间。之后在机器人数据集上微调。

Result: 在相同下游机器人数据集上微调时，RynnVLA-001在多项指标上超过当代最佳基线，显示预训练策略带来更有效的初始化。

Conclusion: 大规模自我视角生成式视频预训练结合轨迹联合建模与动作潜变量压缩，能显著提升VLA在机器人任务上的表现，验证了所提预训练范式的有效性。

Abstract: This paper presents RynnVLA-001, a vision-language-action(VLA) model built
upon large-scale video generative pretraining from human demonstrations. We
propose a novel two-stage pretraining methodology. The first stage, Ego-Centric
Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric
manipulation videos to predict future frames conditioned on an initial frame
and a language instruction. The second stage, Human-Centric Trajectory-Aware
Modeling, extends this by jointly predicting future keypoint trajectories,
thereby effectively bridging visual frame prediction with action prediction.
Furthermore, to enhance action representation, we propose ActionVAE, a
variational autoencoder that compresses sequences of actions into compact
latent embeddings, reducing the complexity of the VLA output space. When
finetuned on the same downstream robotics datasets, RynnVLA-001 achieves
superior performance over state-of-the-art baselines, demonstrating that the
proposed pretraining strategy provides a more effective initialization for VLA
models.

</details>


### [63] [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](https://arxiv.org/abs/2509.15219)
*Haichao Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 提出OST任务：用含噪传感数据预测不可见目标的无噪视觉轨迹；引入基于相机标定的视觉-定位去噪模块，扩展到行人与车辆，在Vi-Fi与JRDB上实现去噪与预测SOTA，并与卡尔曼等传统方法及改造的预测模型对比，提供基准与代码。


<details>
  <summary>Details</summary>
Motivation: 现实系统常受相机视野受限、遮挡和传感噪声影响，导致部分目标不可见且轨迹含噪；现有方法多假设观测完整且干净，难以在真实场景可靠预测，带来安全风险。

Method: 定义Out-of-Sight Trajectory (OST)任务与其预测框架OOSTraj；提出增强的视觉-定位去噪模块：利用相机标定建立视觉到定位的映射，在缺乏视觉参考时无监督地对含噪传感轨迹去噪；将该模块与轨迹预测模型结合，适配行人与车辆；构建与卡尔曼滤波和近期轨迹预测模型的对比基准。

Result: 在Vi-Fi与JRDB数据集上，方法在轨迹去噪与预测上均达SOTA，显著优于以往基线；对比显示相机几何投影与无监督去噪策略带来明显收益。

Conclusion: 首次将视觉-定位投影用于不可见体的传感轨迹去噪，并扩展到多类主体，验证了在真实受限可视条件下的有效性；提供代码与数据，奠定后续研究与应用基础。

Abstract: Trajectory prediction is a critical task in computer vision and autonomous
systems, playing a key role in autonomous driving, robotics, surveillance, and
virtual reality. Existing methods often rely on complete and noise-free
observational data, overlooking the challenges associated with out-of-sight
objects and the inherent noise in sensor data caused by limited camera
coverage, obstructions, and the absence of ground truth for denoised
trajectories. These limitations pose safety risks and hinder reliable
prediction in real-world scenarios. In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.
Building on our previous research, we broaden the scope of Out-of-Sight
Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending
its applicability to autonomous driving, robotics, surveillance, and virtual
reality. Our enhanced Vision-Positioning Denoising Module leverages camera
calibration to establish a vision-positioning mapping, addressing the lack of
visual references, while effectively denoising noisy sensor data in an
unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.
Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark. This work represents the first initiative
to integrate vision-positioning projection for denoising noisy sensor
trajectories of out-of-sight agents, paving the way for future advances. The
code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

</details>


### [64] [Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model](https://arxiv.org/abs/2509.15220)
*Fangjinhua Wang,Qingshan Xu,Yew-Soon Ong,Marc Pollefeys*

Main category: cs.CV

TL;DR: 将多视图深度细化视为条件扩散过程，引入条件编码器、轻量U-Net+ConvGRU的扩散网络与基于置信度的自适应采样，提出DiffMVS与CasDiffMVS，在效率与精度上分别达到SOTA或具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有学习式MVS通常先做多视图深度估计再融合，但为提高效率常用粗到细精化；同时扩散模型在生成任务中表现卓越且具迭代细化特性。作者动机是把扩散模型的逐步去噪范式引入深度细化，以获得更稳健、可控且高效的MVS。

Method: - 将深度细化建模为条件扩散：从噪声深度逐步去噪，条件由多视图线索提供。
- 设计条件编码器：聚合多视图/几何一致性信息，引导扩散步骤中的深度判别式更新。
- 轻量扩散网络：结合2D轻量U-Net与ConvGRU，在多步迭代中共享与累积状态，降低算力与显存开销。
- 置信度自适应采样：依据扩散模型估计的置信度自适应选择/采样深度假设，聚焦不确定区域。
- 两个实例：DiffMVS侧重效率；CasDiffMVS级联/层级式设计以达更高精度。

Result: DiffMVS在运行时间与GPU显存上具有与SOTA相当甚至更优的效率，同时保持有竞争力的精度；CasDiffMVS在DTU、Tanks & Temples、ETH3D上取得SOTA表现。

Conclusion: 把扩散模型引入MVS的深度细化是有效且高效的；条件编码器、轻量U-Net+ConvGRU以及置信度驱动的自适应采样共同提升了MVS的精度与效率。开源代码提供以促进复现与拓展。

Abstract: To reconstruct the 3D geometry from calibrated images, learning-based
multi-view stereo (MVS) methods typically perform multi-view depth estimation
and then fuse depth maps into a mesh or point cloud. To improve the
computational efficiency, many methods initialize a coarse depth map and then
gradually refine it in higher resolutions. Recently, diffusion models achieve
great success in generation tasks. Starting from a random noise, diffusion
models gradually recover the sample with an iterative denoising process. In
this paper, we propose a novel MVS framework, which introduces diffusion models
in MVS. Specifically, we formulate depth refinement as a conditional diffusion
process. Considering the discriminative characteristic of depth estimation, we
design a condition encoder to guide the diffusion process. To improve
efficiency, we propose a novel diffusion network combining lightweight 2D U-Net
and convolutional GRU. Moreover, we propose a novel confidence-based sampling
strategy to adaptively sample depth hypotheses based on the confidence
estimated by diffusion model. Based on our novel MVS framework, we propose two
novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive
performance with state-of-the-art efficiency in run-time and GPU memory.
CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and
ETH3D. Code is available at: https://github.com/cvg/diffmvs.

</details>


### [65] [ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data](https://arxiv.org/abs/2509.15221)
*Zhaoyang Liu,JingJing Xie,Zichen Ding,Zehao Li,Bowen Yang,Zhenyu Wu,Xuehui Wang,Qiushi Sun,Shi Liu,Weiyun Wang,Shenglong Ye,Qingyun Li,Zeyue Tian,Gen Luo,Xiangyu Yue,Biqing Qi,Kai Chen,Bowen Zhou,Yu Qiao,Qifeng Chen,Wenhai Wang*

Main category: cs.CV

TL;DR: 提出ScaleCUA：一个跨6个操作系统、3个任务领域的大规模开源计算机使用代理（CUA）数据与模型套件，基于闭环人机协作采集与训练，显著提升多项基准并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前VLM驱动的计算机使用代理在自动操作GUI方面前景广阔，但受制于缺乏大规模、开源的计算机使用数据与通用基础模型，限制了跨平台与通用性的发展。

Method: 构建ScaleCUA：1）闭环数据管线，自动化代理与人类专家协同收集与精炼数据；2）覆盖6个操作系统与3个任务域的大规模数据集；3）在该数据上训练统一的跨平台CUA模型；4）公开数据、模型与代码。

Result: 在多个基准上显著超越基线与刷新SOTA：+26.6（WebArena-Lite-v2）、+10.7（ScreenSpot-Pro），并达到94.4%（MMBench-GUI L1-Hard）、60.6%（OSWorld-G）、47.4%（WebArena-Lite-v2）。模型实现跨平台无缝操作。

Conclusion: 数据规模化与闭环人机协作能显著提升通用CUA性能。ScaleCUA为开源社区提供数据与模型基础，推动跨平台、通用GUI代理研究发展。

Abstract: Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that
operate GUIs autonomously, showing great potential, yet progress is limited by
the lack of large-scale, open-source computer use data and foundation models.
In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It
offers a large-scale dataset spanning 6 operating systems and 3 task domains,
built via a closed-loop pipeline uniting automated agents with human experts.
Trained on this scaled-up data, ScaleCUA can operate seamlessly across
platforms. Specifically, it delivers strong gains over baselines (+26.6 on
WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art
results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on
WebArena-Lite-v2). These findings underscore the power of data-driven scaling
for general-purpose computer use agents. We will release data, models, and code
to advance future research: https://github.com/OpenGVLab/ScaleCUA.

</details>


### [66] [Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation](https://arxiv.org/abs/2509.15224)
*Luca Bartolomei,Enrico Mannocci,Fabio Tosi,Matteo Poggi,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出一种利用视觉基础模型进行跨模态蒸馏的方法，为事件相机的单目深度估计生成致密代理标签，并基于VFM适配出面向事件数据的模型，在无需昂贵真值深度的情况下取得与全监督相当乃至SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高速运动与强光照变化场景下优势明显，但缺乏大规模致密深度真值，限制了基于学习的单目深度估计。需要一种无需昂贵标注、能充分利用现有RGB资源与大模型鲁棒性的策略。

Method: 1) 跨模态蒸馏：利用空间对齐的事件流与同步RGB帧，调用鲁棒的视觉基础模型（如Depth Anything v2）在RGB上推断深度，将其作为事件数据的致密代理标签；2) 模型适配：直接采用DAv2作为教师或基础，并提出从DAv2衍生的递归结构，使其能从单目事件流推断深度；3) 训练在合成与真实数据上进行，使用代理标签监督事件深度网络。

Result: 在合成和真实数据集上评测：i) 用跨模态蒸馏生成的代理标签训练的模型，在无真实深度标注的条件下，达到可与全监督方法竞争的性能；ii) 基于VFM适配的事件深度模型达到当前最优（SOTA）表现。

Conclusion: 通过利用VFM进行跨模态蒸馏与结构适配，可以绕过昂贵的深度标注瓶颈，使事件相机单目深度估计在多场景中获得强鲁棒与领先性能，为低标注成本的事件视觉学习提供有效范式。

Abstract: Event cameras capture sparse, high-temporal-resolution visual information,
making them particularly suitable for challenging environments with high-speed
motion and strongly varying lighting conditions. However, the lack of large
datasets with dense ground-truth depth annotations hinders learning-based
monocular depth estimation from event data. To address this limitation, we
propose a cross-modal distillation paradigm to generate dense proxy labels
leveraging a Vision Foundation Model (VFM). Our strategy requires an event
stream spatially aligned with RGB frames, a simple setup even available
off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,
we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),
or deriving from it a novel recurrent architecture to infer depth from
monocular event cameras. We evaluate our approach with synthetic and real-world
datasets, demonstrating that i) our cross-modal paradigm achieves competitive
performance compared to fully supervised methods without requiring expensive
depth annotations, and ii) our VFM-based models achieve state-of-the-art
performance.

</details>


### [67] [Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2509.15225)
*Silvio Mazzucco,Carl Persson,Mattia Segu,Pier Luigi Dovesi,Federico Tombari,Luc Van Gool,Matteo Poggi*

Main category: cs.CV

TL;DR: 提出VocAlign：用于开放词汇语义分割的VLM源无域自适应框架，结合学生-教师与词汇对齐，配合LoRA与Top-K选择，显著提升伪标签与效率，在Cityscapes上+6.11 mIoU并刷新零样本分割表现。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割的VLM在跨域迁移时，通常缺乏源数据（隐私/合规限制）且受限于固定词表，导致伪标签噪声大、适配效率低、显存开销高，难以在新域上保持或提升性能。

Method: 1) 学生-教师框架：教师生成目标域伪标签，学生学习；2) 词汇对齐（Vocabulary Alignment）：在伪标签生成中引入额外类别概念，缓解词表错配并提升标签质量；3) LoRA微调：低秩适配冻结大部分参数，保留原能力并降低计算/显存成本；4) 学生Top-K类别选择：仅保留每像素Top-K候选，减少内存并抑制噪声；整体实现源无（仅用目标域未标注数据）自适应。

Result: 在CityScapes上带来6.11 mIoU的提升；在多项零样本分割基准上优于现有方法，展现更强的泛化与适配能力。

Conclusion: VocAlign通过词汇对齐与高效低秩微调，结合Top-K筛选，提升源无开放词汇语义分割的伪标签质量与效率，并在标准数据集与零样本场景中取得SOTA。

Abstract: We introduce VocAlign, a novel source-free domain adaptation framework
specifically designed for VLMs in open-vocabulary semantic segmentation. Our
method adopts a student-teacher paradigm enhanced with a vocabulary alignment
strategy, which improves pseudo-label generation by incorporating additional
class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to
fine-tune the model, preserving its original capabilities while minimizing
computational overhead. In addition, we propose a Top-K class selection
mechanism for the student model, which significantly reduces memory
requirements while further improving adaptation performance. Our approach
achieves a notable 6.11 mIoU improvement on the CityScapes dataset and
demonstrates superior performance on zero-shot segmentation benchmarks, setting
a new standard for source-free adaptation in the open-vocabulary setting.

</details>


### [68] [Calibration-Aware Prompt Learning for Medical Vision-Language Models](https://arxiv.org/abs/2509.15226)
*Abhishek Basu,Fahad Shamshad,Ashshak Sharifdeen,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出CalibPrompt：在提示微调阶段对医疗视觉语言模型进行校准的首个框架，通过可学习提示与校准目标，在少标注数据下显著提升置信度校准且几乎不损伤准确率。


<details>
  <summary>Details</summary>
Motivation: Med-VLM虽在多种医学影像任务上表现卓越，但其置信度校准问题基本未被系统研究。临床环境中，过度自信的错误会削弱信任与决策可靠性，因此需要在不依赖大量标注的条件下提升模型置信度的可信度。

Method: 在提示微调时仅优化少量可学习提示，引入两类校准目标：1) 平滑准确率-置信度对齐正则项，使预测置信度与平滑后的准确率相匹配；2) 角度分离损失，增强文本特征的角度间隔与聚合，从而提升多模态置信度估计的可分性与可靠性。整个方法在标注稀缺设定下进行。

Result: 在4个公开Med-VLM与5个多样化医学影像数据集上，CalibPrompt在不显著降低“干净”准确率的前提下，一致性提升了校准质量（例如更低的校准误差/更可靠的置信度）。

Conclusion: CalibPrompt为医疗VLM提供了一种轻量、数据高效的校准途径，在维持性能的同时显著提升置信度可靠性，增强其临床可用性。

Abstract: Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable
performance across diverse medical imaging tasks by leveraging large-scale
image-text pretraining. However, their confidence calibration is largely
unexplored, and so remains a significant challenge. As such, miscalibrated
predictions can lead to overconfident errors, undermining clinical trust and
decision-making reliability. To address this, we introduce CalibPrompt, the
first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt
optimizes a small set of learnable prompts with carefully designed calibration
objectives under scarce labeled data regime. First, we study a regularizer that
attempts to align the smoothed accuracy with the predicted model confidences.
Second, we introduce an angular separation loss to maximize textual feature
proximity toward improving the reliability in confidence estimates of
multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs
and five diverse medical imaging datasets reveal that CalibPrompt consistently
improves calibration without drastically affecting clean accuracy. Our code is
available at https://github.com/iabh1shekbasu/CalibPrompt.

</details>
