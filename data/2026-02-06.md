<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 107]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR提出一种将人脸图像拆分为“机器可识别的身份特征向量”和“人类可见的语义外观”的框架，通过扩散模型潜空间的语义引导重组，生成在视觉上匿名、但在机器层面保持身份一致的对抗人脸；支持无约束扰动优化、语义-视觉权衡、多样自然样本合成，并可在授权下用密码恢复原图；在CelebA-HQ与FFHQ上黑盒攻击成功率达99%，恢复质量（PSNR）较基线提升41.28%。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别广泛用于线上银行与身份核验，如何在图像存储与传输中将“身份信息”与“可视外观”有效解耦以保护隐私成为关键难题；现有方法往往难以同时兼顾视觉匿名性、机器身份一致性、可恢复性与无约束高自然度。

Method: 1）语义解耦：将人脸分解为身份特征向量（供机器识别）与可视语义外观；2）扩散模型潜空间语义引导重组，生成对抗样本，使视觉匿名且机器身份一致；3）动量驱动的无约束扰动优化提升黑盒迁移与自然度；4）语义-视觉平衡因子，权衡匿名强度与外观保真/多样性；5）受控恢复机制：基于密码可逆还原保护图像至原图。

Result: 在CelebA-HQ与FFHQ上实现黑盒攻击成功率99%；恢复质量以PSNR计较基线提升41.28%；可生成多样且自然的对抗样本，同时保持机器层面的身份一致性。

Conclusion: SIDeR通过语义解耦与扩散潜空间重组，实现“视觉匿名+机器一致+可恢复”的人脸隐私保护，在黑盒攻击与恢复质量上显著优于现有方法，适合用于需要隐私保护且可授权恢复的实际场景。

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [2] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: UniTrack 提出一种可即插即用的图理论损失，用统一可微学习同时优化检测、身份与时空一致性，在不改动架构的前提下全面提升多目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的MOT方法多需要重新设计/更换网络架构，且训练目标往往分别优化检测与关联，难以直接对应跟踪评测指标（如IDF1、MOTA），导致身份切换多、时空一致性弱、泛化差。需要一个可与任意现有MOT模型无缝结合、直接对齐跟踪特定目标的统一可微训练目标。

Method: 提出 UniTrack：将检测准确性、身份保持与时空连贯性融合为一个端到端可训练的图损失。通过可微的图表示学习，将跨帧的运动连续性与身份关系编码进统一损失中，作为“训练目标层”插入现有MOT系统，无需改动网络结构。适配多种架构（如Trackformer、MOTR、FairMOT、ByteTrack、GTR、MOTE），以统一损失驱动特征与关联学习。

Result: 在多个基准与多种模型上稳定提升：身份切换（IDSW）最高减少53%，IDF1最高提升12%，在SportsMOT上以GTR为例MOTA提升9.7%。

Conclusion: 统一的可微图损失能在不改动模型架构的情况下显著提升MOT，对齐评测指标、减少ID切换并增强时空一致性，具有强通用性与可移植性。

Abstract: We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

</details>


### [3] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: 提出一种增强VLA模型“视觉依赖性”的训练框架，通过偏好优化与潜空间蒸馏，减少视觉-动作失配，在不改架构不增数据的情况下提升OpenVLA（离散与连续）的视觉条件化与任务表现。


<details>
  <summary>Details</summary>
Motivation: 将大规模VLM扩展到动作空间常出现视觉-动作失配：动作对当前视觉状态依赖弱，导致执行不稳。作者观察到成功轨迹较失败轨迹更强“视觉依赖”，据此希望通过训练显式强化这种依赖以提升可靠性与性能。

Method: 两阶段：1）在“跟踪轨迹（track-following）”代理任务上做偏好优化（preference optimization），使动作预测更紧密对齐视觉输入；2）在指令跟随的监督微调中，通过潜空间蒸馏把第一阶段学到的强视觉对齐迁移到目标任务。无需改模型结构或新增数据。适配离散OpenVLA并扩展到连续OpenVLA-OFT。

Result: 实证显示：成功rollout的视觉依赖更强；所提方法提高了VLA的视觉条件化度量与下游操控任务成功率。在OpenVLA离散与OpenVLA-OFT连续设置均取得一致收益。

Conclusion: 强化视觉条件化是缓解视觉-动作失配、提升VLA可靠性与性能的有效途径。该无架构改动、无额外数据的训练策略在多设置下稳定增益，表明可作为通用改进方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [4] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: 论文综述图像驱动的饮食评估中“份量估计”的方法与挑战，比较单目、深度/多视角辅助与模板等模型途径，并讨论深度学习如何提升从2D到3D体积推断的精度。


<details>
  <summary>Details</summary>
Motivation: 基于图像的饮食评估便捷且有潜力用于慢性病与肥胖的预防与管理，但核心瓶颈在于从2D图像准确估计食物的三维体积/份量，需要系统梳理与评估现有策略。

Method: 对现有份量估计策略进行综述性分析：使用辅助输入（深度图、多视角）、模型驱动（模板匹配）、以及深度学习（单目或与辅助信息融合）的方法，比较其原理与适用性。

Result: 总结各路线的能力与权衡：辅助传感能提升几何精度但成本更高；模板/模型法在特定菜品有效但泛化受限；深度学习可在单目条件下逼近真实体积，融合多模态进一步提升，但依赖大规模标注与跨域鲁棒性仍是挑战。

Conclusion: 准确的份量估计需在精度、成本与易用性间平衡。未来方向包括：弱/自监督学习以减标注、跨域鲁棒与不确定性估计、多模态轻量化传感融合、标准化数据集与评测协议，以及与临床与行为干预的端到端集成。

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [5] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 提出VCR方法审计LMM在医学影像任务中的偏差与依赖特征，发现示例提示下不同人群间性能差距，并通过人工干预验证VCR生成的视觉概念依赖假设。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，LMM的可靠性与公平性至关重要，但其对哪些视觉概念在决策中起关键作用往往不透明，且可能对不同人群表现不一致，需要一种可审计的方法揭示模型的概念依赖与潜在偏差。

Method: 提出Visual Concept Ranking（VCR）：在给定提示与任务（主要为皮肤病变恶性分类）下，从图像中发现并排序对模型决策重要的视觉概念；据此形成关于模型依赖哪些视觉特征的可检验假设，并通过手动干预（例如修改或遮挡特征）来验证这些假设。研究以临床皮肤科图像为主，补充胸片与自然图像实验；同时分析示例式提示导致的人群子组性能差异。

Result: 发现当以示例提示驱动时，LMM在不同人口统计子组间存在意外的性能落差；VCR能够产出关于模型依赖的视觉特征（概念）的可操作假设，并通过人工干预得到验证，表明模型确实利用了这些特征。

Conclusion: VCR为审计LMM在医疗影像任务中的行为提供了系统方法，能定位并验证模型对特定视觉概念的依赖，从而揭示偏差与脆弱性；该方法适用于皮肤病变分类并可扩展到其他医学与自然图像场景。

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [6] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: 提出CLEAR-HPV：在注意力引导的潜在空间中进行概念发现，将高维MIL嵌入压缩为少量可解释的形态学概念（如角化、基底样、间质），在保持HPV预测性能的同时提升可解释性，并在多数据集上泛化良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的MIL可在全切片病理图像上准确预测HPV状态，但缺乏对形态学要素的可解释性，难以在临床与病理学中建立可信的生物学联系。

Method: 提出CLEAR-HPV：利用注意力权重重构MIL潜在空间，无需概念标签即可自动发现形态学概念；生成空间概念图，并以“概念占比向量”紧凑表示每张切片。框架与骨干无关，可接入任意注意力型MIL模型。

Result: 自动发现角化、基底样、间质等概念；将原1536维特征压缩为10个可解释概念，概念占比向量几乎保留原MIL的预测信息；在TCGA-HNSCC、TCGA-CESC、CPTAC-HNSCC数据集上稳定泛化。

Conclusion: CLEAR-HPV实现从黑盒MIL到概念级可解释的转变：以少量可解释概念保持HPV预测性能并提供空间可视化；作为通用、骨干无关的框架，适用于全切片病理HPV相关任务并具备跨数据集泛化能力。

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [7] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: ARGaze将在线第一人称注视估计转化为自回归序列预测：用当前视觉特征+固定长度的历史注视窗口，因果地解码当前注视点，实现流式、资源受限推理并在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 在线第一人称注视估计缺乏显式头眼信号，只能依赖手-物交互与显著性等间接线索；而注视在目标导向行为中具有强时序连续性，近期注视位置对下一时刻有强先验，因此需要能有效利用受限历史的因果模型。

Method: 提出ARGaze：受视觉条件的自回归Transformer解码器。每一步以当前帧视觉特征和固定长度的Gaze Context Window（近期注视目标估计）为条件，因果地预测当前注视；设计为在线、流式、资源受限推理；通过消融展示有界历史+自回归建模的关键作用。

Result: 在多个人称视角数据集的在线评估上取得SOTA；消融实验表明使用有界注视历史和自回归机制显著提升鲁棒性与精度。

Conclusion: 将注视估计表述为有界历史的自回归序列预测可有效利用注视的时序连续性，适用于因果、在线场景，并在多基准上优于现有方法；代码与预训练模型将开源。

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [8] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: 评估视觉手部追踪在传感手套上的表现，并提出AirGlove以跨手套外观差异实现更好泛化，显著提升姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统依赖手套传感器信号（角速度、重力方向、触觉等）来估计3D手势，但易受噪声与标定影响；而现有大型预训练的视觉手部模型在裸手上强，但对外观差异巨大的手套泛化差。需要系统评估并提升视觉方法在传感手套场景的能力。

Method: 1) 首次系统性基准：在多种传感手套上，对现成视觉手部追踪模型进行零样本与微调两种设定评测，量化外观域差带来的退化。2) 提出AirGlove：利用已有手套数据学习“手套表示”，并通过少量新手套数据进行适配，使手势姿态模型可跨手套设计泛化。

Result: 现有裸手模型在手套上显著退化；AirGlove在多种传感手套上相较基线取得显著性能提升，在少量新手套数据下也能有效泛化。

Conclusion: 视觉模型在手套场景存在明显域差问题；通过AirGlove的表示迁移与少样本适配，可有效弥合外观差异，显著提升手部姿态估计在不同手套上的鲁棒性与准确度。

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [9] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: 提出SHaSaM，通过子模硬样本挖掘与组合式损失提升公平性与准确率，在CelebA与UTKFace上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 深度模型从带偏见标注数据中继承对种族、性别、年龄等敏感属性的偏见，且现有方法因属性组不均衡与对敏感属性过度关注而加剧不公平与性能下降，需要一种同时缓解不均衡、弱化敏感属性影响且不牺牲准确率的方法。

Method: 将公平驱动的表示学习建模为子模硬样本挖掘问题，包含两阶段：1) SHaSaM-MINE 以子模子集选择挖掘困难正负样本，缓解组间不均衡；2) SHaSaM-LEARN 基于子模条件互信息构造一族组合式损失，在扩大目标类别决策边界的同时最小化敏感属性影响，实现统一的表示学习框架。

Result: 在CelebA与UTKFace上达SOTA：Equalized Odds公平性提升最高2.7点，准确率提高3.5%，且训练轮数更少。

Conclusion: 子模优化驱动的困难样本挖掘与基于条件互信息的组合损失能有效抑制模型对敏感属性的依赖，在不牺牲甚至提升准确率的同时显著提升公平性。

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

</details>


### [10] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的水下图像复原管线，通过合成退化学习逆过程，使用约2.5k高质量意识摄影数据从零训练，11M参数，能以512×768分辨率生成与感知一致的图像。


<details>
  <summary>Details</summary>
Motivation: 水下成像受对比度降低、空间模糊与波长依赖性色偏影响，导致海洋生物色彩与细节被掩盖，摄影师需繁重后期校正；需要一种自动化、泛化性强的复原方法以减轻后处理负担并提升图像可用性。

Method: 构建合成退化（corruption）管线模拟水下劣化，再用扩散式图像到图像模型学习逆向还原；在Keith Ellenbogen的小型高质量意识摄影数据上进行训练与评估；模型参数量约11M，从零开始训练，目标输出分辨率512×768。

Result: 在约2.5k张训练图像上训练后，模型在感知一致性方面表现良好，具有较强的泛化能力，能稳定合成512×768尺寸的复原图像。

Conclusion: 基于合成退化+扩散逆过程的小模型可在小数据集上实现高感知质量的水下图像复原，为减少繁杂后期与提升水下摄影可视化提供有效方案。

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

</details>


### [11] [ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification](https://arxiv.org/abs/2602.05175)
*Zhe Li,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出ShapePuri，一种通过形状先验引导的对抗净化方法，利用SDF形状编码与全局外观去偏，在保持推理高效的同时显著提升鲁棒性，在AutoAttack上达成84.06%干净准确率与81.64%鲁棒准确率，首次突破80%。


<details>
  <summary>Details</summary>
Motivation: 现有对抗防御（对抗训练、扩散净化等）要么计算开销大、要么带来信息丢失与外观偏置，且易受外观微扰影响。作者希望利用对抗扰动难以改变的“稳定结构不变量”（形状/几何）来提升鲁棒性，同时避免扩散类方法的高成本。

Method: 提出ShapePuri框架，由两部分组成：1）形状编码模块（SEM）：通过有符号距离函数（SDF）提供稠密几何引导，将输入映射到稳定的形状表征，从而对外观噪声不敏感；2）全局外观去偏（GAD）：通过随机变换与正则，削弱模型对纹理/颜色等外观特征的依赖，强化对形状表征的对齐。整体以对齐策略在训练期建立鲁棒表示，推理时无需额外辅助模块与计算开销。

Result: 在AutoAttack协议下取得84.06%干净准确率与81.64%鲁棒准确率，成为首个在该基准上鲁棒准确率突破80%的防御方法；同时宣称具备可扩展性与高效性。

Conclusion: 通过形状引导与外观去偏的协同，模型学到与人感知一致、对对抗扰动不敏感的结构表示，从而在不增加推理成本的情况下显著提升鲁棒性，为高效对抗防御提供了新思路。

Abstract: Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\%$ clean accuracy and $81.64\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

</details>


### [12] [PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction](https://arxiv.org/abs/2602.05190)
*Ju Shen,Chen Chen,Tam V. Nguyen,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 提出PoseGaussian：利用人体姿态引导的高保真高斯泼洒法，实现实时(100 FPS)的人体新视角合成，并在多个数据集上达SOTA(PSNR 30.86/SSIM 0.979/LPIPS 0.028)。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼洒等方法在动态人体场景中易受关节运动与自遮挡影响，且通常仅将姿态作为条件或用于变形，难以同时提升几何深度精度与跨帧时序一致性，限制了鲁棒性与泛化。

Method: 设计PoseGaussian：1) 将人体姿态与颜色编码器融合，用作结构先验以细化深度估计与几何；2) 专门的姿态编码器作为时间线索，增强跨帧时序一致性；3) 端到端、全可微训练，将姿态信号同时注入几何与时间两个阶段；4) 维持标准Gaussian Splatting的高效渲染。

Result: 在ZJU-MoCap、THuman2.0和自建数据集上达到SOTA感知质量与结构精度，指标：PSNR 30.86、SSIM 0.979、LPIPS 0.028；同时实现约100 FPS实时渲染。

Conclusion: 将姿态深度融合进几何与时序两端能显著提升动态人体新视角合成的鲁棒性、泛化与时序一致性，且不牺牲渲染效率，优于仅将姿态作条件/变形的既有方法。

Abstract: We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is processed by a dedicated pose encoder to enhance temporal consistency across frames. These components are integrated into a fully differentiable, end-to-end trainable pipeline. Unlike prior works that use pose only as a condition or for warping, PoseGaussian embeds pose signals into both geometric and temporal stages to improve robustness and generalization. It is specifically designed to address challenges inherent in dynamic human scenes, such as articulated motion and severe self-occlusion. Notably, our framework achieves real-time rendering at 100 FPS, maintaining the efficiency of standard Gaussian Splatting pipelines. We validate our approach on ZJU-MoCap, THuman2.0, and in-house datasets, demonstrating state-of-the-art performance in perceptual quality and structural accuracy (PSNR 30.86, SSIM 0.979, LPIPS 0.028).

</details>


### [13] [GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling](https://arxiv.org/abs/2602.05202)
*Shivanshu Shekhar,Uttaran Bhattacharya,Raghavendra Addanki,Mehrab Tanjim,Somdeb Sarkhel,Tong Zhang*

Main category: cs.CV

TL;DR: 论文提出将视频生成模型改造为具有时序感知能力的奖励/评价模型，通过能量基模型视角与对比学习，精细判别视频质量，并以合成“难负样本”避免取巧。以极少人工标注在GenAI-Bench与MonteBench上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 基于VLM的奖励模型难以捕捉细微时间动态，导致与人类偏好对齐效果受限；需要一种更能理解时序结构且标注需求更低的评价方式。

Method: 将先进视频生成模型重构为能量基模型，把高质量视频赋低能量、退化视频赋高能量；用对比目标训练。为避免模型利用“真/假”表面差异，设计在潜空间的受控扰动构造难负样本：时间切片、特征交换、帧乱序，模拟真实且细微的时空退化，迫使模型学习有意义的时空特征。

Result: 在仅使用约3万条人工标注下，在GenAI-Bench与MonteBench上取得SOTA，所需标注量比VLM方法少6×到65×。

Conclusion: 将视频生成模型作为时序感知的能量式奖励模型可显著提升视频质量评估与偏好对齐能力；合成难负样本有效抑制投机取巧，显著降低标注需求并达成SOTA。

Abstract: Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.

</details>


### [14] [Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures](https://arxiv.org/abs/2602.05213)
*Chuqin Zhou,Xiaoyue Ling,Yunuo Chen,Jincheng Dai,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 提出一种在超低码率下结合显式与隐式表示的训练免压缩框架：用扩散模型承载高层语义，用反向信道编码传递细节，并配套可调失真-感知权衡的插件编码器；在多数据集上取得领先DISTS BD-Rate。


<details>
  <summary>Details</summary>
Motivation: 神经编解码器在极低码率时感知质量显著下降。基于生成先验的方法虽有前景，但面临“语义忠实度 vs. 感知真实感”的固有限制：显式表示保结构但缺细节，隐式表示添细节却易语义漂移。需要一种能兼顾二者且无需再训练的方案。

Method: 构建一个训练免（training-free）的统一框架：以显式高层语义（如语义图/特征）去条件化扩散模型，保证内容与结构；同时通过“反向信道编码”隐式传递细粒度纹理信息以增强真实感。并引入可插拔的编码器，通过调制隐式信息量来灵活控制失真-感知折中。

Result: 在Kodak、DIV2K、CLIC2020上实现最先进的率-感知表现，相比DiffC在DISTS BD-Rate上分别提升29.92%、19.33%、20.89%。

Conclusion: 显式与隐式表示的协同、结合扩散条件与反向信道编码，可在超低码率下同时保持语义一致与细节真实，并可通过插件编码器平衡失真与感知，达成SOTA性能。

Abstract: While recent neural codecs achieve strong performance at low bitrates when optimized for perceptual quality, their effectiveness deteriorates significantly under ultra-low bitrate conditions. To mitigate this, generative compression methods leveraging semantic priors from pretrained models have emerged as a promising paradigm. However, existing approaches are fundamentally constrained by a tradeoff between semantic faithfulness and perceptual realism. Methods based on explicit representations preserve content structure but often lack fine-grained textures, whereas implicit methods can synthesize visually plausible details at the cost of semantic drift. In this work, we propose a unified framework that bridges this gap by coherently integrating explicit and implicit representations in a training-free manner. Specifically, We condition a diffusion model on explicit high-level semantics while employing reverse-channel coding to implicitly convey fine-grained details. Moreover, we introduce a plug-in encoder that enables flexible control of the distortion-perception tradeoff by modulating the implicit information. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art rate-perception performance, outperforming existing methods and surpassing DiffC by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on the Kodak, DIV2K, and CLIC2020 datasets, respectively.

</details>


### [15] [E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching](https://arxiv.org/abs/2602.05215)
*Jiahao Nie,Wenbin An,Gongjie Zhang,Yicheng Xu,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 提出E.M.Ground，一种面向时间视频定位（TVG）的Vid-LLM，通过事件级<event>聚合、Savitzky–Golay平滑和多粒度帧特征聚合，提升事件语义连贯感知与时间边界预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有TVG方法常以两个独立token与帧特征比对来定位起止时刻，强依赖精确时间戳，忽视事件在时间上的语义连续性，导致匹配模糊与边界不稳。需要一种能整体建模事件语义并提升时间预测鲁棒性的方案。

Method: 1) 设计<event>特殊token，聚合与查询事件相关的所有帧信息，进行整体事件级匹配；2) 在token-帧相似度时间序列上采用Savitzky–Golay平滑，抑制噪声、稳定峰谷用于边界推断；3) 多粒度帧特征聚合，结合不同时间尺度的表示，补偿压缩导致的信息丢失并增强时序理解。

Result: 在多个TVG基准上，E.M.Ground在指标上较现有Vid-LLMs取得显著且一致的提升（摘要未给出具体数值）。

Conclusion: 事件级语义聚合+时序相似度平滑+多粒度特征融合可有效提升TVG中的事件感知与边界定位精度，E.M.Ground在标准数据集上优于SOTA，证明该思路有效。

Abstract: Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.

</details>


### [16] [Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation](https://arxiv.org/abs/2602.05217)
*Jiahao Nie,Guanqiao Fu,Wenbin An,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 提出一种用于跨域小样本分割的多视角渐进自适应方法（MPA），通过逐步增强的数据视角与双链多视预测策略，提高模型在目标域的少样本能力，实验较SOTA提升约+7.0%。


<details>
  <summary>Details</summary>
Motivation: 现有方法先在大规模源域学少样本能力再迁移到目标域，但目标域样本稀少且多样性不足，且源训模型在目标域的少样本能力弱、域间差异大，导致目标样本难以被有效利用与适配受阻。

Method: 从数据与策略两方面渐进适配：1）数据视角—混合渐进增强（Hybrid Progressive Augmentation），通过累积的强增强生成更丰富复杂的多视图，构造逐步更具挑战性的学习场景；2）策略视角—双链多视预测（Dual-chain Multi-view Prediction），在顺序与并行两条学习路径上对这些多视图进行充分利用，在广泛监督下联合约束不同视图预测一致性，实现稳健而准确的适配。

Result: 在跨域小样本分割任务上进行广泛实验，MPA显著优于现有方法，平均提升约+7.0%。

Conclusion: 通过多视角渐进式的数据增强与双链多视一致性学习，MPA有效提升目标域的小样本分割能力，实现鲁棒且准确的跨域适配，并显著超越SOTA。

Abstract: Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).

</details>


### [17] [Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification](https://arxiv.org/abs/2602.05218)
*Jiahao Nie,Yun Xing,Wenbin An,Qingsong Zhao,Jiawei Shao,Yap-Peng Tan,Alex C. Kot,Shijian Lu,Xuelong Li*

Main category: cs.CV

TL;DR: 提出一种在跨域少样本分割（CD-FSS）场景下改进SAM提示效果的训练免方法：通过条件点稀疏化（CPS）从参考图中匹配到的稠密点中自适应挑选少量更可靠的点作为提示，从而提升跨域（医疗、遥感）分割精度。


<details>
  <summary>Details</summary>
Motivation: SAM在同域少样本提示分割表现强，但跨域（医疗、卫星）时，基于参考-目标点匹配得到的稠密点提示效果显著下降。作者观察到域偏移破坏了SAM学到的点-图像交互，且点的密度在跨域中尤为关键，需要一种能依据参考先验自适应控制点密度与质量的机制。

Method: 提出Conditional Point Sparsification（CPS）：利用参考图的真值掩码和匹配关系，对原本稠密的匹配点进行条件化筛选与稀疏化，保留更具判别性、对SAM更稳健的子集点作为提示；全流程训练免，仅在推理时根据参考示例自适应调整提示点，以更好地引导SAM在目标图像上生成掩码。

Result: 在多种CD-FSS数据集上，CPS较现有基于SAM的训练免方法取得更优性能，显著提升跨域分割的准确性与稳健性。

Conclusion: 跨域少样本分割中，直接使用稠密匹配点会因域移位导致提示失效；通过依据参考掩码进行条件化点稀疏化，可有效恢复并增强SAM的提示-图像交互，带来稳定且更准确的跨域分割表现。

Abstract: Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.

</details>


### [18] [PatchFlow: Leveraging a Flow-Based Model with Patch Features](https://arxiv.org/abs/2602.05238)
*Boxiang Zhang,Baijian Yang,Xiaoming Wang,Corey Vian*

Main category: cs.CV

TL;DR: 提出一种结合邻域感知补丁特征与正规化流，并通过适配器缩短预训练特征到工业影像域差距的无监督异常检测方法，在MVTec AD与VisA上将错误率分别降至较SOTA低20%与28.2%，自研压铸数据集无需异常样本训练也达95.77%准确率。


<details>
  <summary>Details</summary>
Motivation: 压铸件表面缺陷影响质量与良率，人工或传统规则法检测效率低且不稳。现有计算机视觉方法在跨域泛化、细粒度缺陷定位与无异常样本条件下的检测仍有不足，需要一种既能利用通用预训练特征又能适配工业域、并保持高精度与高效率的方案。

Method: 1) 从图像提取局部补丁，并构建“邻域感知”特征，融合补丁与其周围上下文以增强对微小/局部异常的敏感度；2) 在特征层使用正规化流建模正常样本分布，实现基于似然的无监督异常检测；3) 在通用预训练特征提取器与工业图像之间加入轻量适配器（adapter），以缩小域差距并提升表示质量；4) 训练仅使用正常样本，不需要异常标注。

Result: 在MVTec AD上图像级AUROC=99.28%，相对SOTA错误率下降20%；在VisA上图像级AUROC=96.48%，错误率相对下降28.2%；在自有压铸数据集上，无需异常样本训练即可达到95.77%异常检测准确率。

Conclusion: 结合邻域感知补丁特征、正规化流与域适配器的无监督框架能有效提升压铸表面缺陷检测的准确性与泛化，验证了利用预训练特征并做轻量域适配在工业质检中的价值，适合部署于实际生产的自动化外观检验流程。

Abstract: Die casting plays a crucial role across various industries due to its ability to craft intricate shapes with high precision and smooth surfaces. However, surface defects remain a major issue that impedes die casting quality control. Recently, computer vision techniques have been explored to automate and improve defect detection. In this work, we combine local neighbor-aware patch features with a normalizing flow model and bridge the gap between the generic pretrained feature extractor and industrial product images by introducing an adapter module to increase the efficiency and accuracy of automated anomaly detection. Compared to state-of-the-art methods, our approach reduces the error rate by 20\% on the MVTec AD dataset, achieving an image-level AUROC of 99.28\%. Our approach has also enhanced performance on the VisA dataset , achieving an image-level AUROC of 96.48\%. Compared to the state-of-the-art models, this represents a 28.2\% reduction in error. Additionally, experiments on a proprietary die casting dataset yield an accuracy of 95.77\% for anomaly detection, without requiring any anomalous samples for training. Our method illustrates the potential of leveraging computer vision and deep learning techniques to advance inspection capabilities for the die casting industry

</details>


### [19] [Active Label Cleaning for Reliable Detection of Electron Dense Deposits in Transmission Electron Microscopy Images](https://arxiv.org/abs/2602.05250)
*Jieyun Tan,Shuo Liu,Guibin Zhang,Ziqi Li,Jian Geng,Lei Zhang,Lei Cao*

Main category: cs.CV

TL;DR: 提出一种主动标签清洗方法，用少量专家重标驱动高精度清洗模型，显著提升拥噪众包数据下的EDD检测性能，接近全专家标注效果且大幅降本。


<details>
  <summary>Details</summary>
Motivation: EDD检测依赖高质量标注，但医学影像专家稀缺、成本高；众包可降本却带来噪声标签，直接训练会显著降性能。因此需要在有限专家资源下，高效识别并纠正关键噪声样本，提升模型可靠性与性价比。

Method: 设计“主动标签清洗”框架：1) 主动学习策略，从噪声数据中挑选对模型最有价值的样本提交给专家重标；2) 标签选择模块（LSM）基于众包标签与当前模型预测的不一致，进行样本选择与实例级噪声分级；3) 利用重标数据训练高精度清洗模型，对剩余数据进行纠错与加权训练，迭代提升。

Result: 在私有数据集上，方法取得AP50=67.18%，较直接用噪声标签训练提升18.83%。其性能达到全专家标注训练的95.79%，同时节省73.30%的标注成本。

Conclusion: 所提方法在有限专家资源场景下，可有效清洗众包噪声标签，显著提升EDD检测效果，接近全专家标注水平，具备实际落地性与高性价比。

Abstract: Automated detection of electron dense deposits (EDD) in glomerular disease is hindered by the scarcity of high-quality labeled data. While crowdsourcing reduces annotation cost, it introduces label noise. We propose an active label cleaning method to efficiently denoise crowdsourced datasets. Our approach uses active learning to select the most valuable noisy samples for expert re-annotation, building high-accuracy cleaning models. A Label Selection Module leverages discrepancies between crowdsourced labels and model predictions for both sample selection and instance-level noise grading. Experiments show our method achieves 67.18% AP\textsubscript{50} on a private dataset, an 18.83% improvement over training on noisy labels. This performance reaches 95.79% of that with full expert annotation while reducing annotation cost by 73.30%. The method provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources.

</details>


### [20] [RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation](https://arxiv.org/abs/2602.05257)
*Diya He,Qingchen Liu,Cong Zhang,Jiahu Qin*

Main category: cs.CV

TL;DR: 提出RFM-Pose：以流匹配生成与强化学习联合优化的范式，实现高效的类别级6D位姿生成与评估，在REAL275上以更低计算成本达成具竞争力表现，并可扩展到位姿跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有类别级位姿估计受旋转对称歧义影响，尽管基于分数的扩散模型有所缓解，但采样代价高、效率低，限制了实际应用（VR与具身智能需要快速稳定的3D交互理解）。

Method: 1) 用Flow Matching替代扩散：沿最优传输路径从先验到位姿分布生成候选，降低采样步数与成本。2) 将采样过程表述为MDP：把可学习流场视为策略，候选评估器作为价值网络。3) 采用PPO微调策略，实现“生成—评分”联合优化；在同一框架内同时提升候选质量与打分准确性。4) 适配到跟踪任务时复用相同范式。

Result: 在REAL275数据集上获得“favorable performance”（与SOTA相当或更优），同时显著降低计算开销；在位姿跟踪场景也取得有竞争力结果。

Conclusion: 以流匹配+PPO的联合优化框架可在保持或提升精度的同时显著提升类别级6D位姿生成效率，并具备对跟踪任务的可迁移性，显示了替代扩散采样与将生成视为决策过程的价值。

Abstract: Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.

</details>


### [21] [ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network](https://arxiv.org/abs/2602.05262)
*Junzhou Li,Manqi Zhao,Yilin Gao,Zhiheng Yu,Yin Li,Dongsheng Jiang,Li Xiao*

Main category: cs.CV

TL;DR: 提出ReGLA轻量级混合网络，将高效卷积与基于ReLU的门控线性注意力结合，以在高分辨率下兼顾精度与时延；在ImageNet与下游检测/分割任务上达SOTA，512px仅4.98ms延迟。


<details>
  <summary>Details</summary>
Motivation: 轻量模型在高分辨率场景常面临推理时延过高，尤其是Transformer结构的全局建模代价大；需要在保持精度的同时显著降低高分辨率推理延迟。

Method: 提出ReGLA框架：1) ELRF模块，以高效卷积保持/扩展大感受野并提升卷积效率；2) RGMA模块，采用ReLU门控的调制线性注意力，在维持线性复杂度下增强局部表示；3) 多教师蒸馏策略，从多源教师迁移知识以提升下游任务泛化。

Result: 在ImageNet-1K上ReGLA-M于224px达80.85% Top-1；在512px推理延迟仅4.98ms。下游任务优于同规模iFormer：COCO检测+3.1% AP，ADE20K分割+3.6% mIoU。

Conclusion: ReGLA通过卷积+线性注意力和多教师蒸馏，在高分辨率视觉任务中实现更佳的精度-时延权衡，达到当前同级最优性能，适用于高分辨率实时应用。

Abstract: Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.

</details>


### [22] [Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2602.05271)
*Shengqin Jiang,Xiaoran Feng,Yuankai Qi,Haokui Zhang,Renlong Hang,Qingshan Liu,Lina Yao,Quan Z. Sheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出在FSCIL中冻结特征提取器、仅微调原型（类别中心），通过双重校准（类特异与任务感知偏移）将静态原型变为可学习动态原型，以极少可学习参数在多基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FSCIL方法多用冻结骨干+静态原型，或用提示微调少量骨干参数。但在极少数据下，更新骨干难以显著提升全局判别力，且静态原型受表征偏置影响，决策边界次优。因此需要在高质量但静态的特征空间内优化决策区域，而非继续追求特征获取。

Method: 固定预训练特征提取器不变，将每类的静态质心升级为可学习“动态原型”。为此引入双校准：1) 类别特异偏移（class-specific offset），细化每类原型以纠正表征偏置；2) 任务感知偏移（task-aware offset），根据当前增量任务的全局分布调整各类原型间的相对位置。通过联合学习的小规模参数对原型进行微调，从而在增量会话中持续提升原型判别性。

Result: 在多个FSCIL基准上获得优于现有方法的性能，同时需要的可学习参数极少，显示出效率与效果兼具。

Conclusion: FSCIL的关键在于于静态高质量特征空间中优化决策边界。通过冻结骨干并微调带有双重校准的动态原型，可在极少样本和参数条件下显著提升增量识别性能。

Abstract: Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.

</details>


### [23] [Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs](https://arxiv.org/abs/2602.05275)
*Qi Li,Yanzhe Zhao,Yongxin Zhou,Yameng Wang,Yandong Yang,Yuanjia Zhou,Jue Wang,Zuojian Wang,Jinxiang Liu*

Main category: cs.CV

TL;DR: 提出Magic-MM-Embedding，通过视觉token压缩的高效MLLM架构+分阶段渐进式训练，在通用多模态检索上实现SOTA且推理高效。


<details>
  <summary>Details</summary>
Motivation: MLLM在通用多模态检索上有潜力，但视觉输入token多带来高延迟与高显存，限制实际应用，需要兼顾效率与性能的解决方案。

Method: 两大支柱：1) 架构层面进行视觉token压缩，显著降低推理时延与内存占用；2) 多阶段渐进式训练：先继续预训练恢复多模态理解/生成，再进行大规模对比学习与hard negative挖掘提升判别力，最后在"MLLM-as-a-Judge"指导下进行任务感知微调与数据筛选，实现由粗到细的性能强化。

Result: 在广泛实验中，模型在通用多模态嵌入/检索基准上显著优于现有方法，同时推理更高效。

Conclusion: 通过结构压缩与渐进训练的协同，Magic-MM-Embedding在保持/提升SOTA性能的同时显著降低计算开销，验证了高效MLLM在通用多模态检索中的可行性与优势。

Abstract: Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.

</details>


### [24] [Fast-SAM3D: 3Dfy Anything in Images but Faster](https://arxiv.org/abs/2602.05293)
*Weilun Feng,Mingqiang Wu,Zhiliang Chen,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiaokun Liu,Guoxin Fan,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出Fast-SAM3D，通过三种异质性感知机制在无需训练的前提下动态匹配生成复杂度，实现对SAM3D推理的大幅加速（最高2.67×）且几乎无质量损失。


<details>
  <summary>Details</summary>
Motivation: SAM3D在复杂场景的开放世界3D重建能力强，但推理延迟高；通用加速策略在该管线中易失效，原因是忽视了形状与布局运动学差异、纹理细化稀疏性以及几何频谱差异等多层异质性。

Method: 提出训练无关的Fast-SAM3D框架，按时刻生成复杂度自适应分配计算，包含三机制：1) 模态感知的步缓存，将结构演化与对布局敏感的更新解耦；2) 联合时空Token雕刻，将细化集中于高熵区域；3) 频谱感知的Token聚合，按几何频谱自适应解码分辨率。

Result: 在广泛实验中，相比原始SAM3D实现最高2.67×端到端加速，质量损失可忽略，在单视图3D生成的效率-保真权衡上建立新的Pareto前沿。

Conclusion: 识别并利用管线的多层异质性是高效3D生成加速的关键。Fast-SAM3D无需再训练即可显著提速且保持保真度，为后续异质性感知的生成系统设计提供了通用思路。

Abstract: SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.

</details>


### [25] [FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion](https://arxiv.org/abs/2602.05305)
*Zhuokun Chen,Jianfei Cai,Bohan Zhuang*

Main category: cs.CV

TL;DR: FlashBlock 针对块扩散（block diffusion）在长上下文推理中重复计算注意力的问题，复用“块外”注意力的稳定输出，从而减少注意力计算与KV缓存访问，在不改变扩散过程的前提下提速（最高1.44×吞吐、1.6×注意力时间降低），对生成质量影响可忽略，并可与稀疏注意力互补。


<details>
  <summary>Details</summary>
Motivation: 块扩散通过KV缓存与分块因果推理提升长序列生成效率，但在长上下文下仍需对不断增长的KV缓存反复做注意力，计算与内存带宽开销大。作者观察到：在同一块内跨扩散步（time steps），当前块之外的token对注意力输出基本稳定，而块内注意力变化显著，提示可跨步重用块外注意力结果以节省开销。

Method: 提出 FlashBlock：对“块外”注意力输出进行缓存与复用。具体做法是在扩散的多个时间步内，固定并复用对块外tokens的注意力输出，仅对块内tokens重新计算注意力；保持原扩散过程不变，减少对大KV缓存的访问与矩阵乘；该机制与稀疏注意力正交，可作为残差复用策略叠加，缓解激进稀疏化带来的精度损失。

Result: 在扩散语言模型与视频生成任务上，FlashBlock 实现最高1.44× token 吞吐提升、注意力计算时间最高减少1.6×，同时对生成质量影响可忽略；与稀疏注意力结合可显著提升在高稀疏度下的准确性/质量。

Conclusion: 跨步复用块外注意力输出能有效削减长上下文块扩散的注意力与内存带宽开销，在不改动扩散流程的前提下带来显著加速，并与稀疏注意力互补，适合长文本与长视频生成等场景。

Abstract: Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.

</details>


### [26] [Wid3R: Wide Field-of-View 3D Reconstruction via Camera Model Conditioning](https://arxiv.org/abs/2602.05321)
*Dongki Jung,Jaehoon Choi,Adil Qureshi,Somi Jeong,Dinesh Manocha,Suyong Yeon*

Main category: cs.CV

TL;DR: Wid3R 是一个支持广视场（鱼眼/全景/360°）相机模型的前馈式多视图3D重建网络，通过射线的球谐表示与相机模型token实现失真感知重建，在零样本泛化上稳健，显著优于以针孔/去畸变假设为主的以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图几何/3D重建方法大多假设针孔或经整形的透视图像，导致在鱼眼或全景相机等广视场场景中需要额外标定与去畸变，鲁棒性与适用性受限。作者希望建立一个无需严格去畸变、可直接处理广视场输入并能泛化到现实相机的3D重建方法。

Method: 提出 Wid3R：一种可泛化的前馈多视图3D估计网络。核心做法包括：1）以射线为中心的表征，并用球谐函数对射线方向/分布进行编码；2）在网络中引入“相机模型token”，显式注入相机类型/畸变信息，实现失真感知特征融合与几何推理；3）支持360°影像的端到端多视图前馈重建，无需依赖去畸变或针孔近似。

Result: Wid3R 在零样本设定下表现稳健，在多个基准上持续超越既有方法；在 Stanford2D3D 上最高带来约 +77.33 的性能提升（相对提升/指标未在摘要中明确）。

Conclusion: 通过射线球谐编码与相机模型token，Wid3R 将广视场相机纳入统一的前馈多视图3D重建框架，实现对鱼眼与360°图像的失真感知重建，并在零样本迁移与基准评测中显著领先，表明其作为多视图基础模型的实用价值。

Abstract: We present Wid3R, a feed-forward neural network for visual geometry reconstruction that supports wide field-of-view camera models. Prior methods typically assume that input images are rectified or captured with pinhole cameras, since both their architectures and training datasets are tailored to perspective images only. These assumptions limit their applicability in real-world scenarios that use fisheye or panoramic cameras and often require careful calibration and undistortion. In contrast, Wid3R is a generalizable multi-view 3D estimation method that can model wide field-of-view camera types. Our approach leverages a ray representation with spherical harmonics and a novel camera model token within the network, enabling distortion-aware 3D reconstruction. Furthermore, Wid3R is the first multi-view foundation model to support feed-forward 3D reconstruction directly from 360 imagery. It demonstrates strong zero-shot robustness and consistently outperforms prior methods, achieving improvements of up to +77.33 on Stanford2D3D.

</details>


### [27] [MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors](https://arxiv.org/abs/2602.05330)
*Jingdong Zhang,Xiaohang Zhan,Lingzhi Zhang,Yizhou Wang,Zhengming Yu,Jionghao Wang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: 提出MTPano：以“无标注多任务训练”构建的全景多任务基础模型，通过透视切片伪标注与几何感知双分支网络，在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 全景（360°）沉浸式应用需要高质量、密集预测（深度、法线、分割等），但高分辨率、多任务标注稀缺；直接把透视领域的大模型迁移到全景会因球面几何畸变与坐标系差异而失效；不同密集任务在球面空间的关系与相互干扰缺乏系统建模。

Method: 1) 无标注伪标注：将全景图投影为多块透视patch，用成熟透视基础模型生成准确伪标注，再重投影回全景，作为patch级监督，规避域差。2) 任务解耦：将任务分为旋转不变（深度/分割）与旋转相关（法线）两类。提出Panoramic Dual BridgeNet，利用几何感知调制层注入绝对位置与射线方向先验，分别建模两类特征流。3) 处理ERP畸变：引入ERP token mixer，后接双分支BridgeNet并配合梯度截断，使跨任务信息共享有益而阻断属性不兼容的冲突梯度。4) 辅助任务：加入图像梯度、点图等，促进跨任务学习。

Result: 在多项全景基准上取得SOTA；相较各任务的全景领域专用基础模型也具备竞争力，显示该框架在多任务与跨域上的鲁棒性与有效性。

Conclusion: 通过透视先验驱动的无标注伪标注与几何感知的双分支解耦框架，MTPano有效缓解数据与几何差异难题，实现全景多任务的统一、强泛化与SOTA表现。

Abstract: Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.

</details>


### [28] [Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation](https://arxiv.org/abs/2602.05339)
*Yongwoo Kim,Sungmin Cha,Hyunsoo Kim,Jaewon Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: 提出PAIR框架，将“概念擦除”从简单删除转为基于不安全-安全成对数据的语义重对齐，实现只去除目标不安全概念且保持结构与语义一致性，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型中的概念擦除多仅关注移除不安全概念，缺乏面向安全替代的引导，导致结构/语义失真与生成质量下降。需要一种既能有效擦除又能保持原图语义与结构的方案。

Method: 1) 先从不安全输入生成结构与语义保持的安全对应物，构造不安全-安全多模态对；2) 配对语义重对齐：用成对数据把目标不安全概念显式映射到对齐的安全锚点；3) Fisher加权的DoRA初始化：在参数高效的低秩适配中，利用配对数据进行Fisher加权初始化，引导生成安全替代并选择性抑制不安全概念。

Result: 在大量实验中，相比SOTA基线，PAIR实现更细粒度的目标概念擦除，同时更好地保持结构完整性、语义一致性与生成质量。

Conclusion: 通过配对驱动的语义重对齐与Fisher加权的参数高效适配，PAIR将擦除转化为引导式替换，能在不牺牲整体一致性的前提下有效去除不安全概念，并在实验中取得显著优势。

Abstract: With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.

</details>


### [29] [Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection](https://arxiv.org/abs/2602.05349)
*Ningkang Peng,JiuTao Zhou,Yuhao Zhang,Xiaoqian Peng,Qianfeng Yu,Linjing Qian,Tingyu Lu,Yi Chen,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出APEX框架，通过自适应原型数与后验感知打分两大机制，修复原型表示学习在OOD检测中的两类通用缺陷，在CIFAR-100等基准上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的表示学习虽然强，但普遍假设每类共享固定原型容量（静态同质性），且在推理时丢弃了训练中关于原型质量的有用信息（学习-推理脱节），导致容量受限与性能受损。

Method: 两阶段Repair优化特征流形：1) 自适应原型流形（APM），基于最小描述长度（MDL）原则，为每个类别自动选择最优原型复杂度K_c^*，缓解/消除原型碰撞；2) 后验感知OOD评分（PAOS），量化原型凝聚度与可分性，将原型质量引入推理打分。

Result: 在CIFAR-100等OOD基准上全面评测，APEX取得新的SOTA表现，优于现有原型法与主流方法。

Conclusion: 通过自适应原型容量与质量感知打分，APEX从根本上提升表示能力与OOD判别力，修复了原型学习在OOD检测中的结构性缺陷，具有通用与可扩展性。

Abstract: Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.

</details>


### [30] [Multimodal Latent Reasoning via Hierarchical Visual Cues Injection](https://arxiv.org/abs/2602.05359)
*Yiming Zhang,Qiangyu Yan,Borui Jiang,Kai Han*

Main category: cs.CV

TL;DR: 提出HIVE：在对齐的潜在空间内进行多模态“慢思考”推理，通过层次化视觉线索注入与递归Transformer内部循环，实现无文本化CoT的迭代推理与更强的场景理解，并在测试时扩展显示有效。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽有强感知力，但推理多依赖端到端或语言化CoT，存在低效、冗长与幻觉问题；需要一种在潜在空间内、无依赖表层文本解释、可无缝融合多模态信号的稳健推理机制。

Method: 提出HIVE：对Transformer块进行递归扩展，形成内部迭代推理回路；将从全局场景到细粒度区域的层次化视觉提示（cues）注入模型潜在表示，以单射方式将视觉线索与潜在推理过程对齐，使模型在对齐的潜在空间中进行多步、扎根的推理而不依赖显式文本CoT；并验证测试时扩大迭代/视觉知识的有效性。

Result: 在多项评测上，注入视觉知识的测试时扩展有效；引入层次化信息显著提升模型对复杂场景的理解与多步推理表现，相比语言化CoT更稳健、较少冗长与幻觉。

Conclusion: 通过HIVE在潜在空间中实现层次化视觉引导的“慢思考”推理，可在不依赖显式文本链条的情况下获得更强的多模态推理与复杂场景理解能力；测试时扩展与层次化注入均带来持续收益。

Abstract: The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a "fast thinking" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\emph{HIVE}), a novel framework that instills deliberate, "slow thinking" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.

</details>


### [31] [Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection](https://arxiv.org/abs/2602.05360)
*Ningkang Peng,Xiaoqian Peng,Yuhao Zhang,Qianfeng Yu,Feng Xing,Peirong Ma,Xichen Yang,Yi Chen,Tingyu Lu,Yanhui Gu*

Main category: cs.CV

TL;DR: 论文发现基于特征的后验OOD检测存在“简单性悖论”：对语义细微的OOD很敏感，却对几何结构简单或含高频噪声的样本失灵。作者将原因归结为深层特征空间中的“语义霸权”，并用神经坍塌视角给出理论解释。为此提出无需训练、可即插即用的几何解耦框架D-KNN，通过正交分解分离语义与结构残差，并在双空间进行校准，显著提升OOD检测，刷新多项基准。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA OOD方法在面对语义相近但细节差异的样本时表现出色，却对结构明显但语义简单的分布偏移以及传感器噪声极为不敏感，造成高漏检率。作者希望揭示这一反常现象的机制，并提出无需重新训练即可普适增强现有模型的解决方案。

Method: 提出D-KNN：1) 在深特征上进行正交分解，将主子空间（语义成分）与余子空间（结构残差）解耦；2) 指出主子空间方差大导致谱集中偏置，掩盖残差子空间中的结构偏移信号；3) 设计双空间校准机制，同时利用语义空间与残差空间的邻近性/距离度量进行打分；4) 无需额外训练、以即插即用的方式与现有模型结合。

Result: 在CIFAR与ImageNet基准上达到新的SOTA：解决“简单性悖论”时FPR95由31.3%降至2.3%；在高斯噪声等传感器失效场景中，AUROC由79.7%提升到94.9%。

Conclusion: 深特征中的“语义霸权”与由神经坍塌引出的谱集中偏置会掩盖结构分布移位信号，导致现有OOD方法对几何简单或噪声主导的样本失明。通过正交解耦与双空间校准的D-KNN可重新激活对残差信号的敏感性，显著提升OOD检测且具有训练无关、即插即用的优势。

Abstract: While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.

</details>


### [32] [Imagine a City: CityGenAgent for Procedural 3D City Generation](https://arxiv.org/abs/2602.05362)
*Zishan Liu,Zecong Tang,RuoCheng Wu,Xinzhe Zheng,Jingyu Hu,Ka-Hei Hui,Haoran Xie,Bo Dai,Zhengzhe Liu*

Main category: cs.CV

TL;DR: CityGenAgent提出一种以自然语言驱动、分层程序化生成高质量3D城市的框架，通过“街区程序+建筑程序”两级表示，并结合SFT与RL（空间对齐与视觉一致奖励）训练，达到更好的语义对齐、结构正确性与可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有3D城市生成在真实感、可控性与交互编辑方面均受限：高保真资产难以生成，结构约束（如多边形不自交、字段完整）易被破坏，文本到视觉的对齐不足，且缺少对局部/全局的可控操纵能力。

Method: 提出CityGenAgent，以自然语言为输入，采用分层可解释的程序化表示：1) Block Program定义城市街区与道路等宏观布局；2) Building Program定义建筑层级与细节。训练分两阶段：SFT阶段让BlockGen与BuildingGen学会生成满足模式约束的有效程序；RL阶段引入空间对齐奖励增强空间推理，视觉一致奖励提升文本-视觉一致性。借助程序化表示和模型泛化，实现自然语言编辑与操作。

Result: 在多项评测中，相较现有方法，达到更强的语义对齐、视觉质量与可控性；生成结构正确（如无自交、多字段完整），并支持稳定的自然语言编辑/操控。

Conclusion: 分层程序化+语言驱动+SFT+RL的组合为可扩展的3D城市生成奠定基础，既保证结构正确与高保真，又提升可控与可编辑性，优于现有生成与程序化方法。

Abstract: The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.

</details>


### [33] [SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback](https://arxiv.org/abs/2602.05380)
*Xiaoxuan He,Siming Fu,Wanli Li,Zhiyuan Li,Dacheng Yin,Kang Rong,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: 提出SAIL：在极少人工偏好对的前提下，让扩散模型自我生成—自我偏好标注—自我迭代优化，实现无需外部奖励模型的人类偏好对齐；用6%数据超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 偏好对齐通常依赖大规模人工标注与外部奖励模型，成本高且难以获取；问题在于能否仅用极少人类反馈、并不依赖奖励模型，解锁扩散模型的潜在自我改进能力以达成有效对齐。

Method: 提出SAIL框架：1) 用少量人工偏好配对作为种子；2) 闭环迭代：模型生成多样样本→基于自身逐步形成的理解进行自标注偏好→用自增广数据继续训练；3) 引入“排序型偏好mixup”以在探索与遵循初始人类先验之间取得平衡并避免灾难性遗忘。

Result: 在多项基准上稳定优于现有方法，同时仅使用其约6%的偏好数据，显示自我改进能力强。

Conclusion: 扩散模型在合适的自监督式闭环中可自我放大偏好对齐能力，可在很小人类标注和无外部奖励模型的条件下达到甚至超过现有方法效果。

Abstract: Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \textbf{SAIL} (\textbf{S}elf-\textbf{A}mplified \textbf{I}terative \textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.

</details>


### [34] [VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs](https://arxiv.org/abs/2602.05382)
*Tina Khezresmaeilzadeh,Jike Zhong,Konstantinos Psounis*

Main category: cs.CV

TL;DR: VRIQ基准测试显示：当前VLM在抽象视觉推理上接近随机（约28%），在自然图像任务上也仅中等（约45%），工具增强带来有限提升。失败主要源于感知不足而非纯推理。


<details>
  <summary>Details</summary>
Motivation: 评估当代视觉语言模型是否具备稳定、可靠的非语言（视觉）推理能力，并明确其失败源头，为改进多模态推理提供依据。

Method: 提出VRIQ基准，包含两类任务：抽象拼图式与自然图像推理；在此基础上引入诊断探针，分离“感知”与“推理”失误，并进一步细化到具体感知类别（形状、计数、位置、3D/深度等）。还评估了工具增强（如外部推理工具）对性能的影响。

Result: 抽象拼图任务平均准确率约28%（近随机）；自然图像任务约45%。工具增强仅带来温和改进。失误归因：约56%仅由感知问题引起，43%由感知与推理共同导致，只有约1%源于纯推理。部分感知子类别（如形状、计数、位置、3D/深度）更易造成失败。

Conclusion: 当前VLM即便结合视觉推理工具，仍是不可靠的抽象推理者。主要瓶颈在视觉感知而非高层推理。VRIQ提供了细粒度诊断框架与改进方向，为提升多模态系统的视觉推理能力提供了有原则的依据。

Abstract: Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.

</details>


### [35] [Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting](https://arxiv.org/abs/2602.05384)
*Hao Feng,Wei Shi,Ke Zhang,Xiang Fei,Lei Liao,Dingkang Yang,Yongkun Du,Xuecheng Wu,Jingqun Tang,Yang Liu,Hong Chen,Can Huang*

Main category: cs.CV

TL;DR: Dolphin-v2 提出一套两阶段文档图像解析框架：先分类（数码原生/拍摄）并做版面分析，再依据类型采用整页或逐元素并行解析，显著提升拍摄文档鲁棒性、细粒度元素与语义属性抽取、以及代码块识别；在多基准上大幅超越前作并保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 当前文档解析生态割裂：模型众多、各有侧重，选择复杂且难以扩展；主流两阶段方法依赖轴对齐框，面对拍摄/扭曲文档效果差。需要一种统一、可扩展且能处理几何畸变的方案。

Method: 两阶段框架：阶段一联合完成文档类型判别（数码原生vs拍摄）与版面分析；若为数码原生，进行更细粒度（21类）元素检测并预测阅读顺序。阶段二采用混合解析策略：拍摄文档以整页级别整体解析以应对畸变；数码原生则按阶段一提供的锚点进行逐元素并行解析，实现高效内容抽取；同时支持语义属性抽取（作者、元数据）与代码块识别（含缩进保留）。

Result: +14.78 分提升于 OmniDocBench；在拍摄文档上错误率降低 91%；在 DocPTBench、OmniDocBench 与自建 RealDoc-160 上做了全面评测，并在保持并行推理效率的同时显著优于原 Dolphin。

Conclusion: Dolphin-v2 通过“类型感知+混合解析”的两阶段设计统一了多场景文档解析：对拍摄文档具强鲁棒性，对数码原生文档实现细粒度、高效并行抽取，并补齐代码块与语义属性能力，在多基准上显著领先且具可扩展性。

Abstract: Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.

</details>


### [36] [Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning](https://arxiv.org/abs/2602.05387)
*Zolnamar Dorjsembe,Hung-Yi Chen,Furen Xiao,Hsing-Kuo Pao*

Main category: cs.CV

TL;DR: 提出一种用于MRI到合成CT（sCT）生成的3D网络：并行Swin Transformer增强的Med2Transformer，可在MRI-only放疗计划中替代CT，提升影像相似度、几何精度与剂量学可用性。


<details>
  <summary>Details</summary>
Motivation: 放疗剂量计算需要电子密度信息，MRI缺乏此信息，临床常用MRI+CT流程带来配准误差与流程复杂度。通过从MRI生成sCT可实现MRI-only计划，但MRI-CT非线性映射与解剖多样性使之困难，需要既能捕捉局部解剖细节又能表征长程依赖的模型。

Method: 提出Parallel Swin Transformer-Enhanced Med2Transformer：3D架构，卷积编码器结合双Swin Transformer并行分支；利用多尺度、shifted window注意力与分层特征聚合，兼顾局部纹理与全局上下文，增强结构一致性与解剖保真度。

Result: 在公共与临床数据集上，相比基线方法取得更高图像相似度与更好几何准确性；剂量学评估显示临床可接受，靶区平均剂量误差1.69%。

Conclusion: 该并行Swin增强的Med2Transformer能更可靠地从MRI生成sCT，兼顾图像与几何精度并满足放疗剂量学要求，为MRI-only放疗流程提供可行方案；代码已开源。

Abstract: MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.

</details>


### [37] [Dataset Distillation via Relative Distribution Matching and Cognitive Heritage](https://arxiv.org/abs/2602.05391)
*Qianxin Xia,Jiawei Du,Yuhan Zhang,Jielei Wang,Guoming Lu*

Main category: cs.CV

TL;DR: 提出一种高效稳定的数据集蒸馏方法“统计流匹配”，在分类任务中用统计量而非批梯度来优化合成样本；一次加载原始数据统计、单次增强即可，显著降低显存与时间，同时提出“分类器继承”策略，用原模型的分类头+轻量投影进行推理，性能更好、存储更省。


<details>
  <summary>Details</summary>
Motivation: 现有基于自监督骨干+线性分类器的蒸馏常用线性梯度匹配，需要在每步蒸馏时加载大量真实样本并对合成样本做多轮可微增强，带来巨大的计算与显存开销与不稳定性。作者希望在保持或提升精度的同时，显著降低资源需求并提升训练稳定性与可扩展性。

Method: 1) 统计流匹配：从原始数据中提取每类的统计（如类中心等），构造从目标类中心指向非目标类中心的“恒定统计流”，并优化合成图像使其沿这些统计流对齐（而非逐批次梯度对齐）。只需一次性加载原始统计，并对合成数据执行单次数据增强。2) 训练与推理：在自监督骨干上以监督方式学习合成样本。3) 分类器继承：推理时复用原数据上训练过的分类器，仅在其前加一个极轻量线性投影（projector），从而几乎不增加存储与计算。

Result: 在使用自监督骨干进行分类的蒸馏设置中，与最新方法相比，在相当或更优精度下实现约10倍更低GPU显存占用与约4倍更短运行时间。分类器继承策略在几乎不增加存储的前提下带来显著性能提升。

Conclusion: 以统计流替代批梯度对齐可实现更稳定高效的数据集蒸馏；只需一次统计加载与单次增强即可达到SOTA级别性能/效率权衡。结合分类器继承，可在保持极小存储与推理开销的同时进一步提升表现，适合资源受限的实际部署。

Abstract: Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.

</details>


### [38] [Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing](https://arxiv.org/abs/2602.05397)
*Yuechen Yang,Junlin Guo,Ruining Deng,Junchao Zhu,Zhengyi Lu,Chongyu Qu,Yanfan Zhu,Xingyi Guo,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出一种“流形感知扩散”(MAD)框架，用VAE学到的可解缠潜空间约束特征编辑轨迹，再用条件扩散生成高保真细胞核图像，从而在编辑相关的pathomics特征时保持生物学合理与结构一致，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Pathomics提供可解释定量特征，但许多高阶统计特征难以跨情境解释；现有条件扩散做特征编辑往往假设特征独立，实际pathomics特征相关性强，单独编辑会偏离生物流形、生成伪影，限制临床可用性。

Method: 提出MAD：先用VAE学习细胞核图像与pathomics特征的可解缠潜空间，并在潜空间中对目标特征进行正则化的轨迹约束，使相关特征联动调整以留在真实数据分布流形上；随后以这些优化后的特征条件引导扩散模型，合成高保真、结构连贯的图像，实现可控且生物合理的特征编辑。

Result: 实验证明在编辑pathomics特征时，MAD能在特征流形内平滑导航，较基线方法更好地保持结构连贯性并减少不真实伪影，在条件特征编辑任务上取得更优性能。

Conclusion: 通过在可解缠潜空间中进行流形感知的特征编辑并结合条件扩散生成，MAD实现可控、可解释且生物学上可信的细胞核编辑，为pathomics特征的可用性与可解释性提供了有效途径，优于现有方法。

Abstract: Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., "second-order moment") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.

</details>


### [39] [TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions](https://arxiv.org/abs/2602.05414)
*Ngoc Doan-Minh Huynh,Duong Nguyen-Ngoc Tran,Long Hoang Pham,Tai Huu-Phuong Tran,Hyung-Joon Jeon,Huy-Hung Nguyen,Duong Khac Vu,Hyung-Min Jeon,Son Hong Phan,Quoc Pham-Nam Ho,Chi Dai Tran,Trinh Le Ba Khanh,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 提出TSBOW交通监控数据集，聚焦极端天气与强遮挡场景下的车辆/交通参与者检测，含32小时实景、4.8万人工标注与320万半标注帧，并建立检测基准，为智能交通研究提供新挑战与基线。


<details>
  <summary>Details</summary>
Motivation: 全球变暖导致极端天气更频繁更严重，CCTV画质受损、交通扰动加剧、事故率上升；现有数据集主要覆盖轻雾雨雪，缺乏极端天气与强遮挡场景，难以支撑鲁棒检测模型研究。

Method: 构建TSBOW：从高密度城市CCTV采集全年多种天气下的交通视频；整理32小时数据，提供8类交通参与者（大型车辆至微出行与行人）的目标框；结合人工精标（>48k帧）与半监督标注（约320万帧）；覆盖多路型、多尺度与多视角；并建立标准化的目标检测基准与评测协议，分析遮挡和恶劣天气带来的挑战。

Result: 得到一个大规模、覆盖极端天气与严重遮挡的公开数据集；基准实验表明现有检测器在TSBOW中性能显著下降，凸显鲁棒性瓶颈与改进空间。

Conclusion: TSBOW为在复杂天气与遮挡条件下的交通目标检测提供关键资源与评测平台，可推动基于CCTV的智能交通监测研究与应用发展；数据集已公开发布，促进社区复现与拓展研究。

Abstract: Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.

</details>


### [40] [VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection](https://arxiv.org/abs/2602.05415)
*Ningkang Peng,Qianfeng Yu,Yuhao Zhang,Yafei Liu,Xiaoqian Peng,Peirong Ma,Yi Chen,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出一个无需外部数据的长尾分布OOD检测框架，通过在特征空间合成几何引导的虚拟离群点并配合双粒度语义损失，优于依赖外部图像的SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 长尾分布下尾部类别样本稀缺，导致特征空间决策边界模糊，现有依赖外部大规模数据（如80M Tiny Images）的OE方法在成本与隐私上不可行，亟需在不依赖外部数据的前提下维持/提升OOD检测性能。

Method: 提出Geometry-guided virtual Outlier Synthesis (GOS)：在超球面上用vMF分布建模特征统计，定位特征空间中低似然的“环带”区域，并在该区域进行定向采样以合成虚拟离群点；同时设计Dual-Granularity Semantic Loss (DGS)，以对比学习最大化ID特征与合成边界离群点在粗粒度与细粒度层面的区分度。

Result: 在CIFAR-LT等基准上进行大量实验，所提方法在无需外部真实图像的情况下，OOD检测性能超过依赖外部数据的SOTA。

Conclusion: 通过几何引导的虚拟离群点合成与双粒度语义约束，可在数据自由（data-free）设定下有效收紧ID边界、扩大与OOD的间隔，从而在长尾场景实现更优的OOD检测并规避外部数据的成本与隐私问题。

Abstract: Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.

</details>


### [41] [Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring](https://arxiv.org/abs/2602.05420)
*Rui Sun,Yiwen Yang,Kaiyu Guo,Chen Jiang,Dongli Xu,Zhaonan Liu,Tan Pan,Limei Han,Xue Jiang,Wu Wei,Yuan Cheng*

Main category: cs.CV

TL;DR: 提出Disco框架与GBC-FS 2025数据集，系统揭示细胞邻接图多为非二分且含大量奇环（尤其三角形），导致2-染色不足；通过“显式标注+隐式消歧”将拓扑冲突转为可学习分类并在冲突区强制特征可分，提升致密重叠细胞实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有等高线/距离映射方法在高密度、复杂拓扑的细胞区域表现不稳；基于图着色的方法虽有潜力，但其在现实中面对大量重叠、奇环拓扑的有效性未被验证。需要新的数据与方法来刻画并解决真实细胞图的染色复杂性。

Method: 1) 构建并发布大规模高复杂度数据集GBC-FS 2025；2) 跨四个数据集系统分析细胞邻接图的染色性质，发现非二分性与高奇环比例；3) 提出Disco：遵循分而治之的邻接感知协同着色框架。其包含：a) 显式标注（Explicit Marking）：递归分解细胞图，提取“冲突集”，将拓扑难题转为可学习的分类标注；b) 隐式消歧（Implicit Disambiguation）：在冲突区域对不同实例施加特征相异性约束，学习可分离表征以缓解邻接歧义。

Result: 发现多数真实细胞图非二分且充满奇环（以三角形为主），2-染色不足，高染色度直接建模会带来冗余与优化困难。Disco在该背景下有效处理密集重叠与复杂邻接冲突（摘要暗示在多数据集上表现更佳，尽管未给出具体数值）。

Conclusion: 真实病理细胞邻接图的染色结构复杂，简单二染色范式不适用。通过“显式标注+隐式消歧”的分而治之设计，Disco在不引入高染色度冗余的前提下缓解拓扑冲突，提升致密重叠场景下的实例分割可行性与鲁棒性。

Abstract: Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the "divide and conquer" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, "Explicit Marking" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a "conflict set." Second, "Implicit Disambiguation" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.

</details>


### [42] [NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks](https://arxiv.org/abs/2602.05423)
*Pengcheng Chen,Yue Hu,Wenhao Li,Nicole M Gunderson,Andrew Feng,Zhenglong Sun,Peter Beerel,Eric J Seibel*

Main category: cs.CV

TL;DR: NeVStereo将NeRF与多视几何深度/位姿优化融合，在无需外参的多视RGB下同时输出位姿、深度、NVS与可重建表面，零样本下在多基准上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端重建网络缺少高质量NVS输出；NeRF渲染虽画质高但依赖精确相机位姿且易受姿态误差影响，且常见表面叠层与几何不稳。需要一个统一框架，从随手拍多视图中稳健获得姿态、可靠深度、高清渲染与精确表面。

Method: 提出NeVStereo：1) 以NeRF驱动的NVS生成“立体友好”渲染；2) 置信度引导的多视图深度估计；3) 与NeRF耦合的BA进行位姿细化；4) 迭代优化同时更新深度与辐射场以提升几何一致性，缓解NeRF常见的表面堆叠、伪影与位姿-深度耦合。

Result: 在室内、室外、桌面、航拍等基准上实现零样本强表现：相较主流方法，深度误差降低至多36%，位姿准确提升10.4%，NVS保真度提高4.5%，网格质量达SOTA（F1 91.93%，Chamfer 4.35 mm）。

Conclusion: NeVStereo有效统一了位姿估计、深度、NVS与表面重建，缓解NeRF固有问题，在多场景零样本下取得全面领先，展示了面向随手拍重建的一体化可行路径。

Abstract: In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.

</details>


### [43] [Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications](https://arxiv.org/abs/2602.05426)
*Wahyu Rahmaniar,Kenji Suzuki*

Main category: cs.CV

TL;DR: 提出Multi-AD：基于CNN的无监督跨域异常检测框架，结合SE通道注意力、教师-学生蒸馏、多尺度特征与判别器，针对医疗与工业图像取得SOTA水平（图像级AUROC：医疗81.4%、工业99.6%；像素级：医疗97.0%、工业98.4%）。


<details>
  <summary>Details</summary>
Motivation: 跨域异常检测常缺乏标注数据，传统深度学习难以在医疗早筛与工业质检场景鲁棒泛化，需要一种无需标注、能捕捉细微异常且具备跨域泛化能力的方法。

Method: 构建教师-学生T-S架构：1) 在CNN中引入SE通道注意力增强相关特征提取；2) 通过知识蒸馏让学生学习教师对正常/异常差异的表征；3) 设判别器网络强化正常与异常可分性；4) 推理时融合多尺度特征以覆盖不同尺寸异常；保证高维表示一致性的同时自适应增强异常感知。

Result: 在多医疗数据集（脑MRI、肝CT、视网膜OCT）与工业数据集（MVTec AD）上验证，跨域泛化良好，优于现有方法：图像级AUROC（医疗81.4%、工业99.6%），像素级AUROC（医疗97.0%、工业98.4%）。

Conclusion: Multi-AD在无监督设定下通过SE注意力+知识蒸馏+判别器+多尺度融合，实现跨域鲁棒异常检测，显著超越SOTA，具备真实世界应用潜力。

Abstract: Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.

</details>


### [44] [LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects](https://arxiv.org/abs/2602.05434)
*Sanghoon Jeon,Gihyun Jung,Suhyeon Ka,Jae-Sang Hyun*

Main category: cs.CV

TL;DR: 提出一种用于高反射低粗糙物体的潜空间扩散式结构光（LD-SLRO），利用编码器提取反射相关潜特征并作为条件，借助扩散模型抑制镜面/间接反射伪影、补全丢失条纹，提高条纹质量与三维重建精度（RMSE由1.8176 mm降至0.9619 mm）。


<details>
  <summary>Details</summary>
Motivation: 传统条纹投影在高光泽表面上易受镜面反射与间接照明影响，导致条纹畸变或丢失，进而显著降低相位解算与3D重建精度；现有方法在严重反射场景下鲁棒性与灵活性不足。

Method: 将相移条纹图像输入“镜面反射编码器”获取表面反射潜表示；以此为条件驱动潜空间扩散模型，在潜域内迭代去噪与生成，抑制反射伪影并重建缺失条纹；引入时间可变通道仿射层与注意力模块以增强条件控制与细节还原；框架支持可变输入/输出条纹组配置。

Result: 在反射性强样品上，恢复的条纹质量更高，三维重建精度优于SOTA，平均RMSE从1.8176 mm降至0.9619 mm，并展现更强的条纹配置灵活性与泛化性。

Conclusion: 潜扩散条件生成结合专用编码与注意力/仿射调制，可有效抑制高反射表面导致的条纹退化并恢复相位信息，显著提升结构光三维测量的鲁棒性与精度。

Abstract: Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.

</details>


### [45] [Stable Velocity: A Variance Perspective on Flow Matching](https://arxiv.org/abs/2602.05435)
*Donglin Yang,Yongxing Zhang,Xin Yu,Liang Hou,Xin Tao,Pengfei Wan,Xiaojuan Qi,Renjie Liao*

Main category: cs.CV

TL;DR: 论文提出Stable Velocity框架，通过显式刻画并利用流匹配中条件速度估计的方差结构，在靠近先验的高方差区与靠近数据分布的低方差区分别采取策略，达到更稳健训练与更快采样。


<details>
  <summary>Details</summary>
Motivation: 传统flow matching依赖单样本条件速度，目标噪声方差大，导致优化不稳定、收敛慢。作者观察到训练目标在时间/噪声尺度上存在方差分区：靠近先验处方差高、靠近数据分布处方差低且条件/边际速度近似一致，动机是降低方差、稳住训练，并在低方差区利用结构性简化提升采样效率。

Method: 1) 方差刻画：划分高方差（近先验）与低方差（近数据）两类时间区间。2) Stable Velocity Matching (StableVM)：构造无偏的方差降低目标，减小条件速度估计的方差。3) VA-REPA：方差自适应的表示对齐，在低方差区增强辅助监督信号。4) Stable Velocity Sampling (StableVS)：在低方差区利用条件/边际速度近似重合的性质做闭式化简，实现无需微调的推理加速。

Result: 在ImageNet 256×256及多种大规模预训练文生图/文生视频模型（SD3.5、Flux、Qwen-Image、万2.2等）上，StableVM与VA-REPA提升训练稳定性与效率；StableVS在低方差区实现>2×采样加速且不降低样本质量。

Conclusion: 显式利用流匹配训练目标的方差结构，可以在不牺牲样本质量的前提下同时改进训练与推理：StableVM与VA-REPA降低训练方差、稳态收敛，StableVS基于低方差区的闭式近似实现显著加速，形成统一的稳定速度框架。

Abstract: While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.

</details>


### [46] [Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations](https://arxiv.org/abs/2602.05440)
*Natascha Jeziorski,Petra Gospodnetić,Claudia Redenbach*

Main category: cs.CV

TL;DR: 论文提出用参数化3D网格缺陷模型+基于物理的蒙特卡洛NDT仿真，批量生成高质量、可控、带精确标注的合成检测数据，以支持机器学习缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 工业质量控制需要无损检测；真实缺陷数据稀缺且不平衡，尤其是稀有缺陷，且标注成本高。需一种可控方式生成与真实检测相似的数据与标注，支撑自动化缺陷检测模型训练与评估。

Method: 1) 构建数字孪生：在被检对象几何上，以参数化方法建模多类常见（以金属铸造为主）缺陷的3D网格，并将其嵌入对象几何形成带缺陷模型；2) 用基于物理的蒙特卡洛仿真，针对具体NDT方法（示例为可视化表面检测），生成与真实检测相似的合成观测数据；3) 同步输出像素级精确标注；4) 可调参数使得可大规模、可变异地生成数据集，包含稀有缺陷。

Result: 能生成任意规模、覆盖多缺陷类型与形态变化的合成数据，质量接近真实检测数据，并提供像素级标注；能够按需增加稀有缺陷样本，提升训练数据的充分性与均衡性。

Conclusion: 参数化缺陷建模结合物理仿真可在可控环境下高效产生高保真、带精注的NDT合成数据，适用于视觉表面检测并可推广到其他NDT方法，为机器学习缺陷检测提供数据基础。

Abstract: In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.

</details>


### [47] [DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching](https://arxiv.org/abs/2602.05449)
*Chang Zou,Changlin Li,Yang Li,Patrol Li,Jianbing Wu,Xiao He,Songtao Liu,Zhao Zhong,Kailin Huang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出一种与蒸馏兼容的可学习特征缓存和保守的受限MeanFlow蒸馏，使视频扩散模型在质量基本不损失的前提下实现高达约11.8×的加速。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散加速方法要么训练免（特征缓存）但在高压缩下出现语义与细节丢失，要么依赖蒸馏（少步推理）在视频场景下显著退化；两者直接叠加因采样过稀进一步恶化，亟需一种既能兼容蒸馏又能维持质量的加速方案。

Method: 1) 以轻量可学习神经预测器替代传统启发式的训练免特征缓存，学习高维特征随时间的演化，从而更准确地复用与外推中间特征；2) 针对大规模视频模型的高压缩少步蒸馏困难，引入保守的Restricted MeanFlow蒸馏策略，稳定学生模型训练并降低质量损失；3) 二者协同：在少步蒸馏学生上使用可学习缓存，兼容地进一步减少实际推理计算。

Result: 在大幅压缩采样步数与缓存计算的同时，将视频生成加速提升至约11.8×，且基本保持生成质量；大量实验证实方法有效。

Conclusion: 可学习特征缓存与受限MeanFlow蒸馏互补，突破视频扩散生成的加速—质量权衡边界，在不牺牲质量的情况下实现显著加速，并具备良好的稳定性与可扩展性。

Abstract: While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.

</details>


### [48] [Attention Retention for Continual Learning with Vision Transformers](https://arxiv.org/abs/2602.05454)
*Yue Lu,Xiangyu Zhou,Shizhou Zhang,Yinghui Xing,Guoqiang Liang,Wencong Zhang*

Main category: cs.CV

TL;DR: 论文指出ViT在增量学习中因注意力漂移导致遗忘，提出通过二步梯度屏蔽与比例缩放来保留旧任务注意区域，达到SOTA并具良好泛化。


<details>
  <summary>Details</summary>
Motivation: CL中灾难性遗忘依旧突出。作者观察到ViT在学新任务时对旧视觉概念的注意力显著偏移（注意力漂移），怀疑这正是遗忘的重要来源，因而希望从注意力选择性角度加以约束。

Method: 提出注意力保留框架：1）用分层rollout提取上一任务的注意力图，并生成实例自适应二值掩码；2）学习新任务时，将掩码用于反向传播，对旧注意区域对应的梯度置零，避免破坏已学概念；为兼容现代优化器，对被屏蔽后的参数更新进行比例缩放，保持相对幅度。

Result: 实验与可视化显示该方法显著缓解遗忘、保留视觉概念；在多种连续学习场景下达到SOTA，并表现出强鲁棒泛化能力。

Conclusion: 针对ViT的注意力漂移提出梯度级的注意力保留机制，能有效抑制灾难性遗忘并广泛适用。

Abstract: Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.

</details>


### [49] [MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation](https://arxiv.org/abs/2602.05467)
*Dekang Qi,Shuang Zeng,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Mu Xu*

Main category: cs.CV

TL;DR: 提出用于视觉语言导航的Memory-Execute-Review框架，在TF与零样本设置下显著提升成功率，并在部分数据集上同时超越TF与SFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法在成功率与泛化能力间难以兼得：SFT成功率较高但泛化差，TF泛化好但成功率不足。亟需一种同时提升两者的通用方法。

Method: 设计三模块框架：1) 分层记忆模块，汇聚并提供关键信息；2) 执行模块，进行常规决策与动作执行；3) 复盘模块，处理异常并纠正行为。将该框架用于Object Goal Navigation，并在多数据集上评测。

Result: 在4个数据集上，平均SR在TF与ZS设置下分别较全部基线提升7%和5%。在HM3D_v0.1与更具挑战的开放词汇HM3D_OVON上，ZS设置下SR分别提升8%与6%。在MP3D与HM3D_OVON上同时优于所有TF与SFT方法，SR与泛化全面领先（分别高出5%与2%）。

Conclusion: Memory-Execute-Review框架在不依赖监督微调的前提下，实现了更高成功率与更强泛化，缓解了VLN中SR与泛化的权衡，并在多个数据集上取得SOTA表现。

Abstract: Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.

</details>


### [50] [SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing](https://arxiv.org/abs/2602.05480)
*Peihao Wu,Yongxiang Yao,Yi Wan,Wenfei Zhang,Ruipeng Zhao,Jiayuan Li,Yongjun Zhang*

Main category: cs.CV

TL;DR: SOMA-1M 提供130万+对像素级精配准的SAR-光学多分辨率影像（0.5–10 m, 512×512），覆盖全球与12类场景，用于多模态遥感基础模型与四项层级任务评测；在多任务上显著提升性能并达成MRSI匹配SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态遥感数据集规模小、分辨率单一、配准精度低，难以支撑多尺度基础模型的训练与泛化；需要大规模、精确对齐、场景多样的SAR-光学数据资源与系统化基准。

Method: 构建SOMA-1M：汇聚Sentinel-1、PIESAT-1、Capella与Google Earth多源数据；全球0.5–10 m多尺度覆盖与12类地物类别；提出严谨的由粗到细图像匹配/配准框架，解决多模态投影形变与海量数据对齐，实现像素级精配准；建立四层级任务基准（匹配、融合、SAR辅助去云、跨模态翻译），整合30+主流算法进行系统评测。

Result: 在SOMA-1M上监督训练显著提升四类任务表现，尤其多模态遥感影像匹配达到当前SOTA；基准为评估提供统一参考。

Conclusion: SOMA-1M作为高精度、多尺度、全球覆盖的SAR-光学对齐数据集，弥补现有数据短板，能作为鲁棒多模态算法与遥感基础模型的重要底座；数据将公开发布以推动社区研究。

Abstract: Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.

</details>


### [51] [Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014](https://arxiv.org/abs/2602.05487)
*Julien Moreau,S. Ambellouis,Yassine Ruichek*

Main category: cs.CV

TL;DR: 这是一份2014年博士期间形成的技术报告，提供鱼眼相机数据集PFSeq、文献综述与大量实验，用于评测鱼眼图像的特征点检测与描述在自标定中的效果；非同行评审、未更新到当代SOTA、无新算法。


<details>
  <summary>Details</summary>
Motivation: 在城市环境中使用安装在车顶朝天的鱼眼相机进行自标定、视觉里程计与双目重建，需要可靠的特征点与描述子。但存在“先有鸡还是先有蛋”的困境：精确相机投影模型有助于最佳特征提取，而获得该模型又依赖高质量特征。

Method: 构建并发布鱼眼序列数据集PFSeq；收集并实现/评测多种经典特征检测与描述方法，在鱼眼图像与自标定场景下进行对比实验；给出详尽的参考文献与实验报告（作为2014年的基线）。

Result: 报告给出了不同通用特征在鱼眼图像上的性能对比和经验性结论，为鱼眼自标定和后续视觉里程计/立体任务选择特征提供依据；但未与专为全向图像设计的方法直接比较。

Conclusion: 该报告是一个资源与基线：提供PFSeq数据集与系统性评测，帮助研究者在鱼眼自标定框架中选择合适的特征方法；其结论属于2014年的阶段性结果，未经过同行评审且未随后续进展更新。

Abstract: What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).

</details>


### [52] [VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency](https://arxiv.org/abs/2602.05508)
*Zhuang Xiong,Chen Zhang,Qingshan Xu,Wenbing Tao*

Main category: cs.CV

TL;DR: 提出VGGT-Motion：一种无需标定的单目SLAM，通过运动感知子图与锚点驱动的直接Sim(3)配准，以及轻量级子图级位姿图优化，实现公里级全局一致、零样本长程运行，显著降低尺度漂移并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D视觉大模型的无标定单目SLAM在长序列中仍存在严重尺度漂移；基于运动无关的分块破坏上下文一致性并引入零运动漂移；传统几何对齐计算开销大。需要一种既高效又稳健、能在长距离上保持全局一致性的方案。

Method: 1) 运动感知子图构建：利用光流引导自适应分块，剪除静态冗余，并在转弯处封装片段以稳固局部几何；2) 锚点驱动的直接Sim(3)配准：通过上下文均衡的锚点，实现无需搜索的逐像素稠密对齐与高效回环，无需代价高的特征匹配；3) 轻量级子图级位姿图优化：线性复杂度的优化在子图层面施加全局一致性，支持可扩展的长程运行。

Result: 在零样本、长程、无标定的单目SLAM设定下，显著提升轨迹精度与效率，达到当前最先进水平，并有效抑制长序列中的尺度漂移。

Conclusion: VGGT-Motion通过运动感知分块、直接Sim(3)锚点配准和线性复杂度位姿图优化，实现了高效稳健的全球一致性与可扩展长距运行，为无标定单目SLAM在实际大规模场景中的应用提供了可行路径。

Abstract: Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.

</details>


### [53] [Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification](https://arxiv.org/abs/2602.05522)
*Jeongbin You,Donggun Kim,Sejun Park,Seungsang Oh*

Main category: cs.CV

TL;DR: 提出Mapper-GIN：用Mapper把点云分割成重叠区域，建区域重叠图并用GIN做分类；在ModelNet40-C的噪声与变换腐蚀下以仅0.5M参数获得稳定、具竞争力的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云鲁棒分类多依赖更大骨干或复杂增强，代价高且难解释。作者探问：仅凭结构抽象（拓扑启发）是否能带来鲁棒性？

Method: 基于Mapper算法进行拓扑分解：以PCA为lens、立方覆盖（cubical cover）、密度聚类将点云划分为重叠区域；用区域间的重叠关系构建区域图；采用Graph Isomorphism Network在该图上进行图分类，形成轻量端到端流水线（约0.5M参数）。

Result: 在ModelNet40-C腐蚀基准上，对噪声与几何变换类腐蚀取得竞争且稳定的准确率，与更重模型或额外机制相比展现更好的参数效率与稳健性。

Conclusion: 简单的区域级图抽象结合GIN消息传递即可带来强腐蚀鲁棒性；区域图结构为3D视觉识别提供了一种高效、可解释的鲁棒性来源。

Abstract: Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.

</details>


### [54] [Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains](https://arxiv.org/abs/2602.05527)
*Ben Isselmann,Dilara Göksu,Andreas Weinmann*

Main category: cs.CV

TL;DR: 研究比较了在显微图像领域使用DINO预训练ViT的跨域可迁移性：在OpenCell蛋白定位任务上，来自HPA的显微特定预训练表现最佳，甚至略优于直接在OpenCell上预训练的模型，证明大规模、领域相关的SSL预训练能在标注稀缺场景下提供强性能。


<details>
  <summary>Details</summary>
Motivation: 显微成像任务常因数据量小导致监督学习难以学到稳健特征；SSL可用未标注大数据预训练，但不同显微域（染色/通道差异）间的可迁移性尚不清楚，需要系统评估跨域迁移效果以指导模型与数据策略。

Method: 采用三种DINO-预训练ViT骨干（分别在ImageNet-1k、Human Protein Atlas、OpenCell上预训练）提取图像嵌入；在OpenCell上仅训练一个监督分类头进行蛋白定位评估；以宏平均F1及其均值±标准误比较转移表现。

Result: 三者均有良好迁移；HPA预训练的显微特定模型最佳，宏平均F1=0.8221±0.0062；直接在OpenCell上预训练的DINO得分为0.8057±0.0090；说明领域相关但非同域的大规模预训练可优于在小目标域上预训练。

Conclusion: 大规模、与任务领域相关的SSL预训练可跨显微域泛化，在标注有限时实现强下游性能；选择与任务相近的显微来源进行预训练可能优于直接在小目标数据集上预训练。

Abstract: Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.

</details>


### [55] [SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation](https://arxiv.org/abs/2602.05534)
*Youngwoo Shin,Jiwan Hur,Junmo Kim*

Main category: cs.CV

TL;DR: 提出一种在视觉自回归（VAR）图像生成中，纯推理阶段、无需再训练的指导方法SSG，通过频域先验与高频残差信号，使分层从粗到细的一致性在推理时得以维持，带来更高保真与多样性且低延迟。


<details>
  <summary>Details</summary>
Motivation: VAR按尺度递归生成，理论上应先粗后细，但推理时因容量有限与误差累积，层级会漂移，细节层未能只补充高频信息，导致训练-推理不一致与质量下降。作者从信息论角度推导：若每个尺度仅贡献前一尺度未解释的高频（语义残差），即可缓解这种不一致。

Method: 提出Scaled Spatial Guidance（SSG），在推理时引导各尺度学习并注入目标高频语义残差。为获得稳健的粗尺度先验并分离高频，设计离散空间增强（DSE）频域过程，对图像/特征进行频率感知的构造与锐化，提取更纯净的语义残差；然后在任意离散视觉token的VAR模型上，借由该残差对下一尺度采样进行引导，保持全局一致同时强化高频。方法对token化和条件类型无关，无需额外训练。

Result: 在多种VAR模型与条件设定上，一致提升保真度与多样性，同时保持低推理延迟；显示从粗到细生成存在尚未利用的效率空间。

Conclusion: 通过在推理阶段用频域先验与语义残差引导各尺度仅补充高频信息，SSG修正了VAR的层级漂移，提升质量与多样性且几乎不增成本，具有通用性与实用价值。

Abstract: Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.

</details>


### [56] [A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments](https://arxiv.org/abs/2602.05538)
*Malaz Tamim,Andrea Matic-Flierl,Karsten Roscher*

Main category: cs.CV

TL;DR: 对JRDB数据集上三类3D行人检测范式（纯相机BEVDepth、纯激光PointPillars、相机-激光融合DAL）进行系统评估。融合方法整体最好，尤其在遮挡、远距离和复杂场景中；但对传感器外参失配与部分LiDAR扰动仍敏感。纯相机方法最弱，受遮挡、距离与噪声影响最大。


<details>
  <summary>Details</summary>
Motivation: 3D行人检测在机器人、工业监控与安防中与安全直接相关。现有研究多聚焦自动驾驶且常忽视非车载、多场景以及鲁棒性（遮挡、距离、传感器失真与对齐误差）。为填补这一空白，本文在多样化室内外环境中，系统比较不同模态与融合策略的性能与鲁棒性。

Method: 基于JRDB数据集，选取三种代表方法：BEVDepth（相机）、PointPillars（LiDAR）、DAL（相机-激光融合）。在不同遮挡与距离分层下评测，并设计/引入多类传感器扰动（LiDAR噪声、删点、模态失衡）与跨模态外参失配，分析性能退化与鲁棒性差异。

Result: 融合的DAL在总体与挑战场景下均优于单一模态；对多数扰动更稳健，但对跨模态错位与特定LiDAR类腐蚀仍显著下降。PointPillars居中，受LiDAR腐蚀影响显著但对遮挡/距离相对优于相机。BEVDepth表现最低，遮挡、距离与噪声敏感性最高。

Conclusion: 多传感器融合显著提升3D行人检测性能与鲁棒性，应优先考虑于安全关键应用。同时需重点研究跨模态几何对齐鲁棒性、对LiDAR腐蚀的缓解，以及在资源受限条件下的稳健融合策略。

Abstract: Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.

</details>


### [57] [FastVMT: Eliminating Redundancy in Video Motion Transfer](https://arxiv.org/abs/2602.05551)
*Yue Ma,Zhikai Wang,Tianhao Ren,Mingzhe Zheng,Hongyu Liu,Jiayi Guo,Mark Fong,Yuxuan Xue,Zixiang Zhao,Konrad Schindler,Qifeng Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出FastVMT：在视频运动迁移中，通过局部注意力与梯度复用/跳过两类机制，去除运动与梯度冗余，实现平均3.43倍加速且不损失画质与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT的视频运动迁移方法耗时高。以往仅在算力层面加速，但忽视结构性低效：视频帧间运动通常小而平滑，扩散过程中的梯度沿时间步缓慢变化。这两点未被充分利用导致冗余计算。

Method: 1) 运动冗余：在时空注意中采用局部掩码，仅允许局部邻域交互，避免远距离无效注意力计算。2) 梯度冗余：设计优化策略在扩散步骤间复用先前梯度，并在变化不显著时跳过梯度计算，减少反向传播开销。整体嵌入到DiT框架的Video Motion Transfer流程中。

Result: 在不降低生成视频的视觉保真度和时序一致性的前提下，平均达到3.43×推理加速。

Conclusion: 通过针对性削减结构性冗余（局部注意与梯度复用/跳过），可显著提升DiT式视频运动迁移的效率，且保持生成质量与稳定性。

Abstract: Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.

</details>


### [58] [IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools](https://arxiv.org/abs/2602.05555)
*Panagiotis Sapoutzoglou,Orestis Vaggelis,Athina Zacharia,Evangelos Sartinas,Maria Pateraki*

Main category: cs.CV

TL;DR: IndustryShapes 是一个面向工业工具与部件的RGB-D基准数据集，涵盖实例级与新物体6D姿态估计，含经典与扩展两部分，提供真实工业装配场景、多实例与多模态（含静态onboarding序列），并用多种SOTA方法评测显示仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态数据集多集中于家用/消费品、合成或高度受控环境，难以反映真实工业制造场景的挑战。为缩小实验室研究与实际部署之间的鸿沟，需要一个在工业环境中采集、包含具有挑战性属性物体的数据集，以公平评测实例级与无模型/新物体方法。

Method: 构建IndustryShapes数据集：引入5类具有挑战性的工业对象；在真实工业装配环境与多复杂度场景（单/多物体、同类多实例）采集RGB-D数据。数据组织为两部分：1) 经典集，含约4.6k图像与6k姿态标注；2) 扩展集，加入支持无模型与时序方法的额外模态，提供首个RGB-D静态onboarding序列。并选取代表性SOTA方法（实例级与新物体6D姿态、检测、分割）进行基准评测。

Result: 数据集成功覆盖从简单到复杂的多种场景与模态；在SOTA方法上的基准测试显示在工业场景中的6D姿态估计、检测与分割仍存在显著改进空间。

Conclusion: IndustryShapes为工业机器人视觉提供更贴近应用的评测平台，支持实例级与新物体6D姿态研究，尤其对无模型与序列方法有用；初步基准结果表明该领域仍需推进算法鲁棒性与泛化能力。

Abstract: We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.

</details>


### [59] [PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557)
*Michael Schwingshackl,Fabio F. Oberweger,Mario Niedermeyer,Huemer Johannes,Markus Murschitz*

Main category: cs.CV

TL;DR: PIRATR 是一个端到端点云三维目标检测框架，同时输出多类别 6-DoF 姿态与类别特定的参数属性，实现“可参数化”的目标检测，能把几何定位与任务相关属性（如抓具开口）一并估计，并用简单规则调整3D模型；采用模块化、类别专属头，易于扩展；在完全合成数据训练下，对真实户外 LiDAR 具有良好零样本泛化，在叉车场景三类物体上获得 mAP 0.919。


<details>
  <summary>Details</summary>
Motivation: 传统点云三维检测多聚焦几何框/姿态，难以直接提供机器人操作所需的对象参数（如机构开合、平台尺寸），且对新类别扩展成本高；需要一种能桥接几何感知与可执行世界模型的感知范式，并能依赖仿真训练在真实环境泛化。

Method: 在 PI3DETR 基础上扩展：以端到端 transformer 式架构对遮挡点云进行联合预测，输出多类别 6-DoF 姿态与类别特定参数；通过“参数化对象”定义与简单先验规则将估计的参数映射到可调整的3D模型；采用模块化、类别专属检测头，便于插拔扩展到新物体类型；合成域训练、真实 LiDAR 测试。

Result: 在自动叉车平台上评测三类结构/功能差异大的对象（吊装抓具、装载平台、托盘）；模型仅用合成数据训练即可在真实户外 LiDAR 上达到 0.919 的检测 mAP，无需微调，并能输出可用于操作的参数化属性。

Conclusion: PIRATR 提出“姿态感知+参数化”的三维感知新范式，把低层几何推理与可执行世界模型连接起来，证明了以仿真训练、现实部署的可扩展机器人感知路线的可行性，并提供可扩展到新类别的模块化实现。

Abstract: We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.

</details>


### [60] [ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors](https://arxiv.org/abs/2602.05572)
*Zhenxiao Liang,Ning Zhang,Youbao Tang,Ruei-Sung Lin,Qixing Huang,Peng Chang,Jing Xiao*

Main category: cs.CV

TL;DR: ShapeGaussian是一个无需模板的高保真4D人体重建方法，针对随拍单目视频，通过融合2D视觉先验先学习粗糙可变形几何，再用神经形变细化，既避免SMPL模板法对姿态估计的脆弱性，又优于通用方法在大形变下的失真，最终在视觉质量与鲁棒性上显著超越模板法。


<details>
  <summary>Details</summary>
Motivation: 现有通用4D重建（如4DGS）缺乏强视觉先验，单目大形变人体重建易失败；模板法（如HUGS/SMPL）虽能逼真但强依赖姿态估计，误差会造成明显伪影。需要一种既不依赖人体模板、又具备强先验与鲁棒性的4D人体重建方案。

Method: 两阶段管线：1) 通过预训练模型提供的数据驱动2D视觉先验，学习一个粗略的可变形几何（模板无关）；2) 基于神经形变模型对几何进行细粒度动态细化。利用多参考帧缓解2D关键点不可见问题，并以2D先验降低姿态估计误差带来的伪影。

Result: 在随拍单目视频、多样人体动作上进行大量实验；在重建精度、视觉质量与鲁棒性方面优于基于模板的方法（如SMPL系），能更稳定捕获高形变动态细节。

Conclusion: 融合模板无关的2D视觉先验与神经形变的ShapeGaussian，能在单目条件下实现高保真且鲁棒的4D人体重建，避免模板依赖带来的姿态脆弱性，并在多种场景中实现更佳的准确性与观感。

Abstract: We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.

</details>


### [61] [Visual Implicit Geometry Transformer for Autonomous Driving](https://arxiv.org/abs/2602.05573)
*Arsenii Shirokov,Mikhail Kuznetsov,Danila Stepochkin,Egor Evdokimov,Daniil Glazkov,Nikolay Patakin,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: ViGT 是一种面向自动驾驶的几何基础模型，从环视相机直接估计连续 3D 占据场（BEV 表达），无需标定信息，可适配多种传感器配置；以自监督（图像-LiDAR 同步对）训练，跨五大数据集训练后在 pointmap 估计上达 SOTA，并在 Occ3D-nuScenes 上与监督方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有通用几何模型多做像素对齐预测、依赖固定传感器标定或监督标注，难以在多数据集与多传感器配置下扩展。自动驾驶需要统一度量坐标的 BEV 连续占据表征，用于多几何任务，同时降低人工标注成本并具备跨域泛化能力。

Method: 提出 Visual Implicit Geometry Transformer（ViGT）：- 以 Transformer 为核心，直接从多相机视图推断单一度量坐标系下的连续 3D 占据场（BEV）。- 架构“免标定”，同一模型可适配不同相机阵列配置。- 自监督训练：利用同步的图像-LiDAR 对作为训练信号，无需人工标注。- 在混合五个大规模自动驾驶数据集上联合训练，检验可扩展与泛化。

Result: 在 pointmap 估计任务上获得 SOTA，且在所有基线中平均排名最佳；在 Occ3D-nuScenes 基准上取得与监督方法可比的性能。

Conclusion: ViGT 作为面向自动驾驶的几何基础模型原型，兼具可扩展性、架构简单、对多传感器配置的泛化与自监督训练优势；能统一多视角图像到度量 BEV 占据场，为多几何任务提供通用表示，并在多数据集上验证了强竞争力。

Abstract: We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.

</details>


### [62] [A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features](https://arxiv.org/abs/2602.05574)
*Mengyu Li,Ingibjörg Kristjánsdóttir,Thilo van Eimeren,Kathrin Giehl,Lotta M. Ellingsen,the ASAP Neuroimaging Initiative*

Main category: cs.CV

TL;DR: 提出一种融合CNN与传统机器学习的多模态框架，用T1 MRI、12个深部脑结构分割掩膜及其体积特征区分APD（PSP、MSA）与PD及彼此，AUC达0.95/0.86/0.92，显示空间与结构信息融合可提升早期分型诊断。


<details>
  <summary>Details</summary>
Motivation: APD早期与PD症状重叠，临床易误诊，缺乏稳定的影像生物标志物；需构建能在早期准确区分PSP、MSA与PD的模型以指导及时干预。

Method: 构建混合学习框架：CNN从T1加权MRI提取图像特征；结合12个与APD相关深部脑结构的分割掩膜与体积测量；将CNN特征与体积等手工特征融合，以分类器分别完成PSP vs PD、MSA vs PD、PSP vs MSA任务。

Result: 融合多模态与多特征后取得较高分类性能：PSP vs PD AUC=0.95；MSA vs PD AUC=0.86；PSP vs MSA AUC=0.92，表现稳健。

Conclusion: CNN图像特征与体积型ML特征的融合能显著提升APD亚型早期鉴别的准确性，有望作为临床早期诊断与精准干预的辅助工具。

Abstract: Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.

</details>


### [63] [LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization](https://arxiv.org/abs/2602.05577)
*Shiyu Wu,Shuyan Li,Jing Li,Jing Liu,Yequan Wang*

Main category: cs.CV

TL;DR: 论文提出LocateEdit-Bench，一个针对指令驱动图像编辑的篡改定位评测数据集（23.1万张图），涵盖4种前沿编辑模型与3类常见编辑类型，并给出多指标评测协议，以弥补现有方法偏向补全式(inpainting)编辑、难以应对新范式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI篡改定位方法主要针对补全/修复式编辑场景，对迅速兴起的“基于文本指令的图像编辑”缺乏有效的定位评测与数据支持，导致方法在新范式上失效或无法公平比较。因此需要一个大规模、覆盖多模型与多编辑类型的标准数据集与评测协议，推动对指令编辑篡改的鲁棒定位研究。

Method: 构建LocateEdit-Bench：收集使用四个前沿指令编辑模型生成的图像，覆盖三类常见编辑类型，形成23.1万张编辑图的大规模数据集；提供详尽数据分析；设计两套多指标评测协议，用于全面衡量现有定位方法在新编辑范式下的表现。

Result: 得到一个系统化、可复现实验平台：展示数据分布与难例分析；用所提出的评测协议对现有定位方法进行基准测试（摘要未给出具体数值，但表明可全面评估方法在指令编辑上的能力缺口）。

Conclusion: LocateEdit-Bench为指令驱动编辑的篡改定位建立了基础设施与标准评测，填补了数据与评测空白，有助于推动未来针对新型图像编辑的伪造定位方法的发展；数据集将于录用后开源。

Abstract: Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.

</details>


### [64] [LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2602.05578)
*Junyang Chen,Xiangbo Lv,Zhiqiang Kou,Xingdong Sheng,Ning Xu,Yiguo Qiao*

Main category: cs.CV

TL;DR: LoGoSeg 提出一个无需外部候选与额外数据的单阶段开词汇语义分割框架，通过全局类别先验、区域级对齐与双流融合，提升空间对齐与减少幻觉，在六个基准上表现竞争且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM（如CLIP）的OVSS方法多基于图像级预训练，空间对齐不精确；缺少强对象先验与区域级约束，导致在杂乱或歧义场景中出现对象幻觉或漏检，影响分割质量与泛化。

Method: 提出 LoGoSeg：1) 对象存在先验：利用全局图文相似度动态加权相关类别，抑制不相关类别的响应；2) 区域感知对齐：建立区域级视觉-文本对应，提升精确定位与对齐；3) 双流融合：将局部结构信息与全局语义上下文最优融合。整体为单阶段，不依赖外部掩码提案、额外骨干或外部数据集。

Result: 在 A-847、PC-459、A-150、PC-59、PAS-20、PAS-20b 六个基准上取得有竞争力的结果，并展示出在开词汇设定中的强泛化能力。

Conclusion: 通过引入全局先验、区域级对齐与双流融合，LoGoSeg 有效缓解VLM空间对齐不足与对象幻觉问题，在保持效率的同时提升开词汇语义分割的精度与泛化。

Abstract: Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.

</details>


### [65] [Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation](https://arxiv.org/abs/2602.05582)
*Joe-Mei Feng,Sheng-Wei Yu*

Main category: cs.CV

TL;DR: 提出一个统一的算子理论框架，扩展影响函数到SE(3)上的左平凡化M估计，定义几何可观测性指数（GOI），通过曲率算子与李代数结构量化单个特征对位姿估计的影响；GOI的谱分解揭示弱可观测方向上的高敏感性，并在总体极限与Fisher信息几何一致，解释纯旋转/消失视差等退化与动态特征放大，并可在Gauss-Newton管线中作为轻量无训练诊断信号用于检测动态与弱可观测配置。


<details>
  <summary>Details</summary>
Motivation: 现有灵敏度与信息论工具（条件数、欧式扰动、Fisher界）无法解释“逐特征”如何影响位姿估计，也无法说明为何动态/不一致观测会不成比例地扭曲SLAM与SfM。需要一个内在于SE(3)几何的、能刻画单测量影响与动态可检测性的统一理论。

Method: 将影响函数理论推广到矩阵李群SE(3)，针对左平凡化M估计推导内在扰动算子；基于曲率算子与可观测子空间的李代数结构定义GOI；对GOI做谱分解以关联主曲率方向与敏感性；在总体（无限样本）极限下将GOI与SE(3)上的Fisher信息几何对齐，给出单测量版Cramér–Rao界。

Result: 得到几何一致的单测量影响量化指标GOI，其谱结构直接刻画弱可观测性=高敏感性；统一了条件数分析、Fisher信息几何、影响函数与动态场景可检测性的解释；解释了纯旋转、消失视差等经典退化与动态特征沿弱曲率方向的放大；并表明Gauss-Newton中可直接得到曲率谱与GOI，提供轻量、免训练的动态/弱可观测性诊断信号。

Conclusion: GOI与曲率谱为SE(3)位姿估计提供了统一、几何一致的逐特征影响分析框架，既解释退化与动态放大机理，又能在现有SLAM管线中无侵入地作为实时诊断，用于识别动态特征与弱可观测配置。

Abstract: We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).
  The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.
  The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.

</details>


### [66] [A Mixed Reality System for Robust Manikin Localization in Childbirth Training](https://arxiv.org/abs/2602.05588)
*Haojie Cheng,Chang Liu,Abhiram Kanneganti,Mahesh Arjandas Choolani,Arundhati Tushar Gosavi,Eng Tat Khoo*

Main category: cs.CV

TL;DR: 论文提出一种将混合现实与触觉产科模拟相结合的分娩训练系统，通过外置RGB-D相机与HMD透视流的空间标定，实现对实体产妇模型与预扫描胎头的实时定位与配准，以叠加虚拟引导手势和轨迹，支持学员在保留真实触感下独立练习。离线外设与单机头显上实现稳定精准定位。83名四年级医学生的对照研究表明，MR训练在接生、产后与总体表现上显著优于纯VR，并更受学员偏好。


<details>
  <summary>Details</summary>
Motivation: 医学教育中阴道分娩的实操机会被轮转时间缩短、患者顾虑与临产不可预测性所限制；教师示教负担重，学员效率受限。需要一种既保留真实触觉、又能在缺乏持续专家在场的情况下提供有效指导的训练方式。

Method: 在商用HMD上扩展透视功能：将外置RGB-D相机与头显进行空间标定，实现物理教具（产妇模型与胎头）的实时视觉融合。提出逐级定位管线：先用标志点对产妇模型进行粗定位以确定分娩区域，再将预扫描的胎头在该区域内精配准；据此叠加虚拟引导手与轨迹，让学员按专家路径操作并获得触觉反馈。系统在独立头显上运行，无需外接计算。

Result: 实验显示系统在单机头显上对产妇模型的定位准确且稳定。包含83名四年级医学生的大规模用户研究中，四位高年资产科医师按标准量表独立评分：MR组在接生、产后及总体绩效上显著高于VR组；学员主观偏好也更倾向MR。

Conclusion: 结合实体触觉与虚拟引导的MR训练在产科接生教学中优于纯VR，能提高关键环节表现并提升学习体验，同时具备在独立头显上的实用可部署性。

Abstract: Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.

</details>


### [67] [EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality](https://arxiv.org/abs/2602.05590)
*Haojie Cheng,Shaun Jing Heng Ong,Shaoyu Cai,Aiden Tat Yang Koh,Fuxi Ouyang,Eng Tat Khoo*

Main category: cs.CV

TL;DR: 提出EgoPoseVR：通过融合HMD运动信号与头载RGB-D观测，实现实时、稳定、准确的VR自我视角全身姿态追踪，优于现有方法，并在用户研究中获得更高主观评价。


<details>
  <summary>Details</summary>
Motivation: 现有基于头载相机的自我视角姿态估计在VR场景中存在时间不稳定、下肢不准、难以实时等问题，影响沉浸式体验与化身一致性，需要一种能充分利用HMD信号并提升稳定性与精度的方法。

Method: 构建端到端双模态融合框架：以HMD运动线索与RGB-D输入为两路，使用时空编码器提取帧级与关节级表示，通过跨注意力进行多模态融合；随后引入基于HMD信号的运动学优化模块施加约束，提升精度与稳定性。并提供包含180万+对齐的HMD与RGB-D帧的大规模合成数据集用于训练与评测。

Result: 在基准实验中，EgoPoseVR在精度与时序一致性上超过SOTA自我视角姿态估计模型；在真实场景用户研究中，在准确性、稳定性、具身感与未来使用意愿四项主观指标上显著优于基线方法。

Conclusion: EgoPoseVR实现稳健的VR全身姿态跟踪，无需额外穿戴式传感器或房间级定位系统，提供实用的高精度VR具身化解决方案。

Abstract: Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.

</details>


### [68] [CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion](https://arxiv.org/abs/2602.05598)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: 提出CAViT：在每个Transformer块中用“空间自注意+通道自注意”的双注意力替代静态MLP，实现内容自适应的token混合；在5个数据集上较ViT最高+3.6%准确率，同时参数与FLOPs各降30%+。


<details>
  <summary>Details</summary>
Motivation: ViT虽然通过自注意力捕获长程空间关系，但通道维的混合依赖固定MLP，无法根据输入动态调整，限制了表达能力与效率。

Method: 设计CAViT结构：每个块先做空间自注意力（空间token间交互），再做通道自注意力（跨通道动态重标定），以注意力机制取代MLP实现统一、内容感知的token混合；不增加深度与复杂度。提供定量评测与注意力可视化。

Result: 在5个自然与医学基准上，相比标准ViT最高提升准确率+3.6%，同时参数量与FLOPs均降低30%以上；注意力图更清晰且具语义。

Conclusion: 动态、双注意力的通道与空间联合建模优于静态MLP，能在不增深度的情况下提升准确率并降算量与参数，表明注意力驱动的token混合是一种更高效的ViT设计。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.

</details>


### [69] [Multi-instance robust fitting for non-classical geometric models](https://arxiv.org/abs/2602.05602)
*Zongliang Zhang,Shuxiang Li,Xingwang Huang,Zongyue Wang*

Main category: cs.CV

TL;DR: 提出一种针对非经典模型的多实例鲁棒拟合方法：使用无阈值的模型到数据误差估计器，配合元启发式全局优化，在含噪与离群点数据上重建多个实例，并在多类非经典模型上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒拟合多聚焦于直线、圆、平面等经典模型，且多为单实例重建；对螺旋、程序化角色、自由形面等非经典模型的鲁棒且多实例拟合方法匮乏，且离群点处理常需预设阈值、可微性要求限制了优化选择。

Method: 将多实例非经典模型拟合表述为包含估计器与优化器的优化问题：设计基于“模型到数据”误差的全新估计器，能在无需预设误差阈值的情况下抑制离群点；由于该估计器对模型参数不可微，采用元启发式全局优化算法搜索最优解，并以此同时恢复多个模型实例。

Result: 在多种非经典模型数据集上进行实验，显示该方法在噪声与离群点存在时依然能准确重建多个实例，优于或补充现有方法；代码已开源。

Conclusion: 所提无阈值鲁棒估计器与元启发式全局优化的组合，有效解决多实例非经典模型的鲁棒拟合问题，具普适性与实用性。

Abstract: Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.

</details>


### [70] [Unified Sensor Simulation for Autonomous Driving](https://arxiv.org/abs/2602.05617)
*Nikolay Patakin,Arsenii Shirokov,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: XSIM 是一个针对自动驾驶的传感器仿真框架，基于扩展的3DGUT splatting，引入通用滚动快门建模、相位建模以处理球面相机（如激光雷达）的方位角边界不连续，以及双不透明度高斯表示，统一建模外观与几何失真，实现更一致几何与更真实外观，并在多数据集上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3DGUT splatting在自动驾驶场景的多传感器（特别是球面相机/激光雷达）仿真中存在边界时间/投影不连续、滚动快门效应和几何-外观不匹配等问题，导致渲染错误与不足，影响动态环境中失真模拟与真实性。

Method: 1) 将3DGUT扩展为通用滚动快门建模，统一外观与几何传感器成像；2) 识别球面相机在方位角环绕与时间不连续导致的错误投影，提出“相位建模”机制，对方位边界处UT投影的高斯在时间与形状上的不连续进行显式处理；3) 扩展3D高斯表示，引入两种独立的不透明度参数，分别对几何与颜色分布建模，以缓解二者失配；4) 在Waymo、Argoverse2、PandaSet等数据集上系统评估。

Result: XSIM 能渲染复杂的传感器失真，在球面相机/激光雷达边界问题上避免错误投影，获得更一致的几何与更逼真的外观表现。在Waymo、Argoverse2、PandaSet上均优于近期强基线，达到SOTA。

Conclusion: 通过通用滚动快门与相位建模、双不透明度高斯表示，XSIM 为自动驾驶场景提供统一灵活的传感器仿真与渲染框架，实现几何一致性与写实性的显著提升，并在多数据集上验证其领先性能与泛化能力。

Abstract: In this work, we introduce \textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.

</details>


### [71] [ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing](https://arxiv.org/abs/2602.05629)
*Jianlei Chi,Yuzhen Wu,Jiaxuan Hou,Xiaodong Zhang,Ming Fan,Suhui Sun,Weijun Dai,Bo Li,Jianguo Sun,Jun Sun*

Main category: cs.CV

TL;DR: 提出ROMAN：结合多头注意力与交通法规权重的ADS测试场景生成方法，在CARLA对Apollo测试，较ABLE和LawBreaker生成更多高风险违规并覆盖所有法条。


<details>
  <summary>Details</summary>
Motivation: 现有ADS测试难以系统生成复杂高风险、含多车交互与关键情境的“违规”场景，导致对遵法与安全性的评估不充分；需要能面向法规条款、可控地提升违规风险与多样性的生成方法。

Method: ROMAN采用多头注意力网络建模车辆、信号与环境要素交互；引入交通法规权重机制：利用LLM基于“严重性×发生性”对各法条违规进行风险加权，驱动场景搜索/生成；在CARLA中对Baidu Apollo进行仿真评测，与SOTA（ABLE、LawBreaker）对比。

Result: ROMAN在平均违规次数上较ABLE提升7.91%，较LawBreaker提升55.96%；同时保持更高场景多样性；能为输入交通法规的每一条款成功生成相应违规场景，覆盖度优于基线。

Conclusion: ROMAN能面向法规高风险点定向生成复杂违规场景，提升ADS测试的有效性与覆盖度；相较现有工具在违规数量、多样性与法条覆盖上均更优，适合用于更全面的合规与安全评估。

Abstract: Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.

</details>


### [72] [UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos](https://arxiv.org/abs/2602.05638)
*Jinlin Wu,Felix Holm,Chuxi Chen,An Wang,Yaxin Hu,Xiaofan Ye,Zelin Zang,Miao Xu,Lihua Zhou,Huai Liao,Danny T. M. Chan,Ming Feng,Wai S. Poon,Hongliang Ren,Dong Yi,Nassir Navab,Gaofeng Meng,Jiebo Luo,Hongbin Liu,Zhen Lei*

Main category: cs.CV

TL;DR: UniSurg 是一个以视频为中心的外科基础模型，用潜在运动预测替代像素重建，并在大规模外科视频数据集（UniSurg-15M）上预训练，显著提升多项外科视频理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖像素级重建，模型容量被烟雾、反光、液体等低层细节消耗，难以捕获对外科语义理解关键的结构与运动关系。需要一种以语义运动为核心的学习范式。

Method: 基于 V-JEPA 框架，提出三项改进：1）运动引导的潜在预测，聚焦语义相关区域；2）时空亲和力自蒸馏，保证关系一致性；3）特征多样性正则，防止在纹理稀疏场景下表征坍塌。同时构建 UniSurg-15M（3,658 小时、50 来源、13 解剖区域）用于大规模预训练。

Result: 在17个基准上达到或刷新 SOTA：流程识别（EgoSurgery F1 +14.6%，PitVis +10.3%），动作三元组识别（CholecT50 上 mAP-IVT 39.54%），并在技能评估、息肉分割、深度估计等任务上显著领先。

Conclusion: 将学习目标从像素重建转向潜在运动预测，配合针对外科视频的三项技术与大规模数据预训练，UniSurg 成为面向普适、以运动为导向的外科视频理解新标准。

Abstract: While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.

</details>


### [73] [Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances](https://arxiv.org/abs/2602.05650)
*Amir Ansari,Jana Subirana,Bruna Silva,Sergio Escalera,David Gallardo-Pujol,Cristina Palmero*

Main category: cs.CV

TL;DR: 研究探索在个体互动的视听数据上，用更细粒度（Big Five 的“细微项/nuances”）标签训练模型能否优于用传统广义特质或面向（facets）标签。基于UDIVA v0.5，构建具跨模态与跨被试注意力的Transformer，细微项层级显著提升预测，MSE最高降74%。


<details>
  <summary>Details</summary>
Motivation: 传统个性识别多以广义特质分数为“真值”，但同分数可由情境依赖且多样的行为模式产生，加上训练数据有限，导致泛化差。层级更细（面向与细微项）也许能更好地对应具体行为信号，缓解标签-行为多对一的问题。

Method: 使用UDIVA v0.5 视听互动数据，训练一种包含跨模态（音频-视频）与跨被试（考虑对话双方，dyad-aware）的Transformer，比较以三种标签层级（特质、面向、细微项）训练/评估的模型表现。

Result: 细微项层级的模型在各互动情景中稳定优于面向与特质层级，均方误差最高可下降74%。

Conclusion: 在个性识别中，使用Big Five 更细的层级（尤其细微项）作为监督信号，结合跨模态与跨被试注意力建模，可显著提升泛化与预测精度。

Abstract: Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.

</details>


### [74] [ShapeUP: Scalable Image-Conditioned 3D Editing](https://arxiv.org/abs/2602.05676)
*Inbar Gat,Dana Cohen-Bar,Guy Levy,Elad Richardson,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: ShapeUP提出一种可扩展、以图像为条件的3D编辑框架，将编辑建模为原生3D潜空间中的监督式“潜到潜”翻译，用3D DiT在三元组数据上学习，从而在保持几何一致的同时实现细粒度、无掩码的可控编辑，并优于训练式与免训练基线。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑在可控性、几何一致性与可扩展性间难以兼顾：优化法慢，多视图2D传播有漂移，免训练潜操作受冻结先验限制、难以随规模提升。需要一种既能利用3D基础模型强先验又能高效、可扩展地进行精确编辑的方法。

Method: 将编辑视为原生3D表示中的监督式潜空间翻译：使用预训练3D基础模型的潜表示，构建训练三元组（源3D形状、编辑后的2D图像、对应编辑后的3D形状），通过3D Diffusion Transformer学习从源潜表示到目标潜表示的直接映射；以“图像即提示”提供细粒度局部与全局编辑，引入隐式、无掩码的定位，同时保持与原资产的结构一致。

Result: 在广泛评测中，ShapeUP在身份保持（结构与形状一致性）和编辑保真度方面稳定优于现有训练式与免训练基线；实现高保真、可控的本地与全局编辑，并展现良好的扩展性与鲁棒性。

Conclusion: 通过将编辑转化为可监督的3D潜空间翻译，并依托预训练3D基础模型先验，ShapeUP在速度、可控性与一致性上取得平衡，为原生3D内容创作提供了可扩展且精确的编辑范式。

Abstract: Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.

</details>


### [75] [Poster: Camera Tampering Detection for Outdoor IoT Systems](https://arxiv.org/abs/2602.05706)
*Shadi Attarha,Kanaga Shanmugi,Anna Förster*

Main category: cs.CV

TL;DR: 论文提出两种静态图像场景下的摄像头篡改检测方法（基于规则与基于深度学习），并发布含正常、模糊、旋转图像的数据集。深度学习精度更高；规则法更省资源、无须长时间标定。


<details>
  <summary>Details</summary>
Motivation: 户外智慧相机易受人为破坏或恶劣环境影响，导致监控失效。静态照片缺乏时序信息，使篡改检测更难。现有公开数据与方法对静态图像篡改场景不足，因此需要可复现实验的基准与对比方法。

Method: 提出两条技术路线：1）规则法：利用图像质量/几何线索（如清晰度、模糊度、旋转/地平线偏移、遮挡比例等）设阈值检测；2）深度学习法：训练模型从单帧图像判别正常、模糊、旋转等篡改类别；并比较两者在精度、算力/推理开销、训练数据需求上的差异。构建并公开包含正常、模糊、旋转图像的数据集用于评测。

Result: 深度学习模型在准确率上优于规则法；规则法在算力受限、无法长时间标定/收集训练数据的场景下更实用。公开的数据集为社区提供评测与开发资源。

Conclusion: 对静态图像的篡改检测，深度学习更准确但依赖数据与算力；规则法易部署、资源友好。公开的数据集填补资源空缺，可促进实用化与后续研究。

Abstract: Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.

</details>


### [76] [Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2602.05718)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 提出在点监督时序动作定位中引入多任务自监督以显式建模帧间时序一致性，包含完成度、顺序与规律性三项任务，在四个数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有PTAL方法多只用点监督做片段级分类，缺乏对动作帧间时序关系的显式建模，限制了对动作边界与完整性的理解；通过利用仅有的点标注挖掘时序一致性，可提升对动作定义与完整帧范围的定位能力。

Method: 在点监督框架下构建多任务学习：在常规片段级分类头之外，引入三种自监督时序理解任务——(i) 动作完成度（估计或预测从标注点到动作起止的进程/剩余），(ii) 动作顺序理解（判别或重建打乱片段的时间顺序），(iii) 动作规律性理解（识别/度量跨视频的时序模式一致性，区分常规与异常片段）。这些任务共享特征并联合训练，显式约束时序一致性以提升定位。

Result: 在四个基准数据集上进行广泛实验，所提方法整体优于多种最新方法，显示出显著性能增益。

Conclusion: 显式挖掘时序一致性的多任务自监督是对点监督时序动作定位的有效补充，能更好地理解动作定义并提升完整定位性能。

Abstract: Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.

</details>


### [77] [Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification](https://arxiv.org/abs/2602.05729)
*Lexiang Hu,Youze Xue,Dian Li,Gang Liu,Zhouchen Lin*

Main category: cs.CV

TL;DR: 提出AGFF-Embed，通过让MLLM生成多视角嵌入并自适应融合，同时结合EGA进行批内难负样本增强，在MMEB与MMVP-VLM上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP式与MLLM式多模态嵌入多捕捉全局语义，难兼顾复杂场景中的全局+细粒度混合感知需求；缺少无需数据精修即可增强难负样本判别力的机制。

Method: 1) 设计AGFF-Embed：用提示引导MLLM针对不同语义维度（全局与细粒度）生成多组嵌入；2) 提出自适应、平滑的融合机制对这些嵌入加权聚合；3) 结合Explicit Gradient Amplification (EGA)，在批内放大难负样本的梯度贡献，实现无需细粒度数据编辑的难样本增强。

Result: 在MMEB与MMVP-VLM两个基准上，在通用理解与细粒度理解两方面均取得全面SOTA，相比现有多模态嵌入模型显著提升。

Conclusion: 多视角嵌入生成+自适应融合可有效统一全局与细粒度理解；配合EGA的批内难负样本增强进一步提升判别力，实现更强的多模态检索/对齐性能。

Abstract: Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.

</details>


### [78] [Depth as Prior Knowledge for Object Detection](https://arxiv.org/abs/2602.05730)
*Moussa Kassem Sbeyti,Nadja Klein*

Main category: cs.CV

TL;DR: 提出DepthPrior框架，用深度作为先验而非特征融合，通过训练期的深度加权与分层、推理期的深度感知阈值，显著提升小/远目标检测，且无需改动检测器结构，仅需一次性深度估计开销。


<details>
  <summary>Details</summary>
Motivation: 小/远目标因尺度变化、低分辨率与背景干扰使检测器性能显著下降；安全关键场景尤其需要可靠检测。尽管深度信息有帮助，但现有方法多依赖复杂、模型特定的结构改造，部署成本高、泛化差。作者希望用更通用、低侵入的方式利用深度提升检测。

Method: 理论与实证分析深度与检测性能的关系，指出深度引起系统性退化并解释深度监督可缓解的原因。提出DepthPrior：1) 训练期的深度加权损失（DLW）根据目标深度调整样本权重；2) 深度分层损失（DLS）按深度分桶进行分层优化；3) 推理期深度感知置信度阈值（DCT）依据深度自适应调整置信度阈值。框架将深度作为先验信号，不改动检测器架构；唯一额外开销是初次深度估计。

Result: 在KITTI、MS COCO、VisDrone、SUN RGB-D四个基准与YOLOv11、EfficientDet两种检测器上验证：小目标mAP_S最高+9%，mAR_S最高+7%；推理阶段真/假检恢复率可达95:1。总体在不增加传感器、不改结构、无额外推理耗时的条件下取得稳定增益。

Conclusion: 深度作为训练与推理先验可系统性缓解小/远目标检测退化，DepthPrior以最低工程侵入实现跨数据集与架构的显著增益，适合实际部署。代码开源便于复现与扩展。

Abstract: Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.

</details>


### [79] [Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing](https://arxiv.org/abs/2602.05737)
*Luca Ciampi,Ludovico Iannello,Fabrizio Tonelli,Gabriele Lagani,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.CV

TL;DR: 论文提出“生物水库计算”（BRC）：用体外培养的大脑皮层神经元网络作为水库，通过HD-MEA施加输入并读取高维响应，再用线性读出层分类，完成从简单刺激到MNIST的静态视觉识别，表现稳定且准确，展示活体神经网络可作为有效的计算基底。


<details>
  <summary>Details</summary>
Motivation: 传统水库计算多依赖人工递归网络来近似神经动态，难以完全捕获生物神经系统的丰富动力学与稀疏性、噪声、可塑性等特性。作者希望直接利用活体神经网络自身的时空动态作为计算资源，检验其在实际视觉识别中的可用性，并探索将生物原则纳入机器学习与类脑视觉的途径。

Method: - 在HD-MEA上培养皮层神经元，选定部分电极注入输入刺激，其他电极并行读取诱发与自发活动，形成高维特征表征。
- 采用线性读出（单层感知机）对“水库状态”进行监督训练，实现静态视觉图案分类。
- 任务从简单到复杂：点刺激、定向条纹、类似时钟数字的形状、以及MNIST手写数字。
- 评估跨会话与噪声条件下的稳定性和分类精度。

Result: 尽管存在生物响应的噪声、自发活动与会话差异，该BRC系统仍能稳定地产生区分性高维表示，支持各类静态视觉任务的准确分类；在从简单图形到MNIST的多级任务上均获得良好性能。

Conclusion: 体外皮层网络可作为有效水库，用于静态视觉模式识别；活体神经基底与线性读出结合，能够在噪声与变异性下实现稳健分类。这为将活体神经系统纳入神经形态计算提供可行路径，并为受生物启发的视觉与机器学习模型设计提供实践依据。

Abstract: In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.

</details>


### [80] [FMPose3D: monocular 3D pose estimation via flow matching](https://arxiv.org/abs/2602.05755)
*Ti Wang,Xiaohang Yu,Mackenzie Weygandt Mathis*

Main category: cs.CV

TL;DR: 提出FMPose3D：用Flow Matching的条件分布输运框架，从高斯先验高效生成与2D观测一致的多样3D姿态，并用重投影后验期望聚合得到单一精确预测；在人类与动物3D姿态基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态存在深度歧义与遮挡，单一解不可靠；扩散模型虽能生成多假设但推理迭代多、成本高，亟需既能表达多模态不确定性又高效的生成式方法。

Method: 以Flow Matching学习条件速度场，解常微分方程把标准高斯逐步输运到条件于2D关键点的3D姿态分布；通过不同噪声种子产生多样3D假设；提出RPEA模块，基于2D重投影一致性近似贝叶斯后验期望，将多假设聚合为单一预测。

Result: 在Human3.6M与MPI-INF-3DHP上超越现有方法；在Animal3D与CtrlAni3D动物姿态数据集上取得SOTA，显示跨人/动物领域的强泛化与精度。

Conclusion: Flow Matching可在少量积分步高效生成可信的多样3D姿态；结合RPEA能将不确定性样本转化为稳定准确的最终估计，兼具效率与SOTA性能，适用于多域3D姿态估计。

Abstract: Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.

</details>


### [81] [ReText: Text Boosts Generalization in Image-Based Person Re-identification](https://arxiv.org/abs/2602.05785)
*Timur Mamedov,Karina Kvanchiani,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: 提出ReText，通过混合多摄像头Re-ID数据与单摄像头+文本描述的数据，联合学习Re-ID、图文匹配与文本引导重建，在跨域泛化上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 跨域行人重识别需要在未见域无再训练条件下保持识别性能。现有方法多依赖复杂架构以缩小域差，但最新发现表明风格多样的单摄像头数据有助于泛化，然而其缺少跨视角复杂性与语义多样性，难以充分训练鲁棒表征。作者动机是把易获取但欠复杂的单摄数据与更具跨视角信息的多摄数据结合，并用文本增强其语义信息，从而提升泛化。

Method: 提出ReText：在训练时混合多摄Re-ID数据与单摄像头数据；为单摄数据配套文本描述。模型联合优化三项任务：(1) 在多摄数据上进行标准行人Re-ID训练；(2) 图像-文本匹配以对齐多模态语义；(3) 在单摄数据上进行文本引导的图像重建，以利用文本丰富语义并正则化视觉表征。整体为多任务联合学习以提升域外泛化。

Result: 在跨域Re-ID基准上取得强泛化性能，显著优于当前SOTA方法。

Conclusion: 首次在行人Re-ID中探索将多摄与单摄数据混合，并进行多模态联合学习；通过引入文本描述与联合优化，增强语义与鲁棒性，实现跨域显著提升。

Abstract: Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.

</details>


### [82] [Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation](https://arxiv.org/abs/2602.05789)
*Hengyi Wang,Ruiqiang Zhang,Chang Liu,Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出“Allocentric Perceiver”：无需训练的策略，用多视角/单视角几何重建+查询条件化的目标系变换，把空间推理中的“心智旋转”外显为确定性的几何计算，使VLM在他者中心（allocentric）问题上显著提升（约+10%），且不损伤自我中心（egocentric）表现，优于经空间感知微调与多款SOTA开源/闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在需要视角转换的他者中心空间问答上脆弱：答案应在目标物体/目标参照系中推理，而非相机视角。导航与操控等任务对这类能力需求增长，促使寻找能稳健处理视角变换的方案。

Method: 训练免策略：1) 使用成熟几何专家（如多视角几何/单目深度+位姿等）从一张或多张图恢复度量级3D场景；2) 依据指令语义构建“查询条件化”的他者中心参考系（目标为原点/朝向等）；3) 将重建几何确定性地变换到该参考系；4) 用结构化、几何对齐的表示提示底座VLM，从而把隐式心智旋转外包为显式计算。

Result: 在多种VLM骨干和多项空间推理基准上，他者中心任务平均提升约10%，同时保持自我中心任务强劲表现；整体超越经空间感知微调的模型及若干最新开源/商用SOTA。

Conclusion: 显式、几何驱动的参照系对齐与结构化提示，能在无需再训练的前提下，显著增强VLM的他者中心空间推理并兼顾通用性能，显示“把心智旋转外显化”为一条有效、可泛化的路径。

Abstract: With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.

</details>


### [83] [Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning](https://arxiv.org/abs/2602.05809)
*Enwei Tong,Yuanchao Bai,Yao Zhu,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: 提出FSR，一种训练免疫、可插拔的视觉token剪枝框架，通过“聚焦-扫描-精炼”三步在不增token预算下提升VLM推理效率并保持/提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练免疫的token剪枝在高压缩率下难以兼顾局部关键证据与全局上下文，常偏向视觉显著但与指令无关区域，导致精度下降与上下文缺失。需要一种既关注任务相关证据又能补充全局信息、并在有限token预算内优化表示的方法。

Method: FSR三阶段：1) Focus：结合视觉重要性与指令相关性选出关键证据，抑制与问题无关的显著区域偏置；2) Scan：以已聚焦集合为条件，选择与其最“不同”的token以补充互补全局上下文；3) Refine：围绕扫描锚点基于相似度分配与得分加权合并，将临近信息性token聚合进锚点，在不增加token数的前提下提升表示质量。框架训练免疫、可插拔，适配多种VLM骨干。

Result: 在多种VLM与多项视觉语言基准上，FSR在相同或更低token预算下，相比最新SOTA剪枝方法实现更好的准确率-效率权衡，降低延迟与内存占用并保持/提升精度。

Conclusion: 模仿人类“先聚焦、后全局、再精炼”的FSR为VLM训练免疫token剪枝提供有效范式，能在激进压缩下平衡局部证据与全局语境，带来稳定的效率与性能收益；代码已开源。

Abstract: Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR

</details>


### [84] [NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects](https://arxiv.org/abs/2602.05822)
*Musawar Ali,Manuel Carranza-García,Nicola Fioraio,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TL;DR: NVS-HO 提出首个仅用RGB输入、针对真实场景手持物体的新视角合成基准；双序列设计（手持序列用于学习、板载序列提供精确位姿与评测）。对比SfM与VGGT位姿估计，结合NeRF与高斯溅射训练，结果显示在无约束手持条件下现有方法存在显著性能差距，基准可推动更稳健方法发展。


<details>
  <summary>Details</summary>
Motivation: 当前NVS多依赖受控拍摄与可靠位姿，难覆盖真实世界手持物体场景；缺少专门数据集与评测，导致方法在非结构化、位姿不稳定条件下表现不明。需要一个现实、可量化、仅RGB的基准来检验并推动鲁棒NVS。

Method: 构建数据集与评测协议：每个对象采集两段RGB序列——手持序列（静止相机、有人手操控物体）用于学习对象全外观；板载序列（物体固定在ChArUco棋盘上）通过标记检测获得高精度相机位姿，作为评测真值。建立基线：位姿估计采用传统SfM与预训练VGGT；渲染模型用NeRF与Gaussian Splatting进行训练与比较。

Result: 在手持、非受限条件下，使用上述位姿与渲染管线的组合均出现显著性能下滑与差距，表明现有方法难以稳健处理仅RGB、位姿噪声与物体/手交互等挑战。

Conclusion: NVS-HO提供了一个具有挑战性的真实世界基准，能够系统揭示并量化当前RGB-only NVS在手持物体场景中的不足，呼吁并促进更鲁棒的位姿估计与渲染方法发展。

Abstract: We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.

</details>


### [85] [Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation](https://arxiv.org/abs/2602.05827)
*Hai Zhang,Siqi Liang,Li Chen,Yuxian Li,Yukuan Xu,Yichao Zhong,Fu Zhang,Hongyang Li*

Main category: cs.CV

TL;DR: 本文提出Beyond-the-View Navigation（BVN）：仅凭高层意图在未知环境中寻找远距离、不可见目标的问题；为此引入视频生成模型，并提出SparseVideoNav，以稀疏未来视频引导在亚秒级做出轨迹决策，较未优化版本提速27倍，在真实零样本实验中成功率为SOTA LLM的2.5倍，且首次在夜间场景实现该能力。


<details>
  <summary>Details</summary>
Motivation: 现实导航不应依赖冗长、逐步的语言指令；理想的智能体应能根据简洁意图在未知环境中自主探索并到达不可见远目标。现有基于LLM的方法依赖短视野（short-horizon）监督，虽能跟随密集指令，却在BVN中短视、难以长期规划；直接延长监督会导致训练不稳定。作者观察到视频生成模型天生在长时间尺度与语言对齐上具有优势，因而更适合BVN。

Method: 首次将视频生成模型用于BVN：让模型生成覆盖约20秒时域的“稀疏未来”作为长时预测，再据此进行路径规划与控制。为解决视频生成的高延迟问题，提出SparseVideoNav：以稀疏关键帧/低帧率未来视频作为指导，并设计快速的轨迹推理管线，实现亚秒级决策；整体相较未经优化的视频生成推理提速27倍。

Result: 在真实世界零样本BVN实验中，相比SOTA LLM基线，SparseVideoNav在成功率上提升约2.5倍；同时首次在具有挑战性的夜间场景中实现BVN能力；推理速度达亚秒级，显著优于未优化版本（27×加速）。

Conclusion: 长时监督对齐的 视频生成 模型更契合BVN等长视野导航需求。SparseVideoNav通过稀疏未来视频指导实现快速、稳健的长视野导航，既提高成功率又降低延迟，表明将视频生成纳入导航感知—规划—控制闭环是解决高层意图驱动的真实世界导航的有效路径。

Abstract: Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.

</details>


### [86] [Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning](https://arxiv.org/abs/2602.05829)
*Yudi Shi,Shangzhe Di,Qirui Chen,Qinian Wang,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie*

Main category: cs.CV

TL;DR: 提出Weaver：一个端到端可训练的多模态推理代理，通过动态调用工具与强化学习，提升长视频复杂推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的CoT方法用于视频推理时存在表征错配与感知能力不足，限制了模型对复杂、长时视频的理解与推理边界。

Method: 构建名为Weaver的多模态代理系统：在推理过程中由策略模型动态调用多种外部/内部工具以逐步获取关键视觉线索并形成真实的多模态推理轨迹；同时引入强化学习，在无轨迹数据的情况下探索与优化工具使用与组合策略，实现端到端训练。

Result: 在多个复杂视频推理基准上取得优于现有方法的成绩，尤其在长视频任务上提升更显著。

Conclusion: 动态工具调用与RL驱动的策略学习能有效缓解表征错配与感知受限问题，端到端的Weaver提升了复杂与长视频推理能力。

Abstract: Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.

</details>


### [87] [UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents](https://arxiv.org/abs/2602.05832)
*Han Xiao,Guozhi Wang,Hao Wang,Shilong Liu,Yuxiang Chai,Yue Pan,Yufeng Zhou,Xiaoxin Chen,Yafei Wen,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出UI-Mem：在GUI在线强化学习中引入分层经验记忆，以模板化工作流/子技能/失败模式实现跨任务迁移，并通过分层分组采样和自进化循环将记忆指导融入策略学习，显著超越基线且可泛化到新应用。


<details>
  <summary>Details</summary>
Motivation: 在线RL在GUI长时序任务中信用分配低效，且不同任务间无法复用经验，导致重复犯错与学习缓慢；需要一种能结构化沉淀与迁移经验、同时不破坏在线探索多样性的机制。

Method: 1) 分层经验记忆：将高层工作流、子任务技能、失败模式以参数化模板存储，实现跨任务/应用复用；2) 分层分组采样（Stratified Group Sampling）：同一rollout组内注入不同强度的记忆指导，保持结果多样性并推动无指导策略内化受指导行为；3) 自进化循环：持续从新交互中抽象新策略与错误，动态更新记忆以与策略共同演化。

Result: 在在线GUI基准上，UI-Mem显著优于传统RL基线与静态复用方法，并在未见应用上表现出强泛化能力。

Conclusion: 通过将结构化、可迁移的记忆与在线RL深度耦合，并维持探索-指导的平衡，UI-Mem提高了长程信用分配与经验迁移效率，带来更高性能与更强泛化。

Abstract: Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io

</details>


### [88] [Self-Supervised Learning with a Multi-Task Latent Space Objective](https://arxiv.org/abs/2602.05845)
*Pierre-François De Plaen,Abhishek Jha,Luc Van Gool,Tinne Tuytelaars,Marc Proesmans*

Main category: cs.CV

TL;DR: 论文提出为不同视图类型分配独立预测器，稳定多裁剪（multi-crop）训练，并统一全局/局部/遮挡视图于一个不对称Siamese自监督框架，显著提升ImageNet上ResNet与ViT性能。


<details>
  <summary>Details</summary>
Motivation: 多裁剪策略在SSL中有效，但在带预测器的Siamese方法（BYOL、SimSiam、MoCo v3）上引发不稳定；作者想找出不稳定根因并在不更换主干框架的前提下提升性能与稳定性。

Method: 将多裁剪中不同空间变换（全局、局部、cutout遮挡）视为不同对齐任务；关键改动是为每一类视图分配独立的预测器而非共享预测器；在不对称Siamese架构中进行多任务训练，联合对齐全局-全局、全局-局部、以及含遮挡的视图对。

Result: 为每类视图设置独立预测器后，多裁剪训练稳定；在ImageNet上对ResNet和ViT均有一致增益，较原始预测器共享的做法显著提升Top-1精度（具体数值未在摘要中给出）。

Conclusion: 多裁剪不稳定的根源在于跨视图共享预测器；按视图类型拆分预测器并将不同空间变换统一为多任务对齐，可在不改变主干的情况下稳定训练并普适提升性能。

Abstract: Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.

</details>


### [89] [Pathwise Test-Time Correction for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.05871)
*Xunzhi Xiang,Zixuan Duan,Guiyu Zhang,Haiyu Zhang,Zhe Gao,Junta Wu,Shaofeng Zhang,Tengfei Wang,Qi Fan,Chunchao Guo*

Main category: cs.CV

TL;DR: 提出一种针对蒸馏自回归扩散模型在长视频生成中漂移累积的问题的测试时纠偏方法（TTC），以首帧为锚在采样过程中校准中间随机状态，在几乎零开销下延长生成长度，并在30秒基准上达到接近训练密集方法的质量。


<details>
  <summary>Details</summary>
Motivation: 蒸馏自回归扩散模型可实现实时短视频合成，但在长序列生成中会因误差累积导致漂移；现有测试时优化（TTO）对图像或短片有效，却因奖励景观不稳定与蒸馏参数的超敏感性而在长序列中失效，急需一种无需训练、能稳定长时生成的方案。

Method: 提出测试时纠偏（TTC）：不进行额外训练，利用初始帧作为稳定参考锚点；在采样轨迹中对中间的随机状态进行校准，从而抑制随时间扩大的漂移。该方法可无缝集成到多种蒸馏模型中，带来可忽略的计算与时延开销。

Result: 在广泛实验中，TTC显著延长可生成的视频长度，且与多种蒸馏模型兼容；在30秒长视频基准上，生成质量可匹配需要大量训练资源的方法，同时保持几乎不增加的推理开销。

Conclusion: TTC为长序列视频生成的漂移问题提供了训练-free的实用解决方案，通过锚定首帧并在线校准采样状态，稳定了蒸馏自回归扩散模型的长时生成，在效率与质量上取得兼顾，并具备良好的通用性与可扩展性。

Abstract: Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.

</details>


### [90] [Contour Refinement using Discrete Diffusion in Low Data Regime](https://arxiv.org/abs/2602.05880)
*Fei Yu Guan,Ian Keefe,Sophie Wilkinson,Daniel D. B. Perrakis,Steven Waslander*

Main category: cs.CV

TL;DR: 提出一种轻量级离散扩散轮廓精炼方法，在小数据场景下稳健检测不规则/半透明物体边界，速度更快且精度优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 许多医学、环境与制造场景标注稀缺、设备算力受限，现有分割工作偏重掩膜而忽视边界质量；小数据下的边界检测尤为薄弱，亟需一种既节省数据与算力又能提升边界精度的方法。

Method: 以CNN+自注意力为核心，条件输入为分割掩膜；将边界表示为稀疏轮廓并进行迭代去噪的离散扩散精炼。为小数据与高效推理做多项改动：简化扩散过程、定制轻量架构、最小化后处理，将稀疏轮廓转为致密且孤立的边界。训练数据规模<500张。

Result: 在KVASIR医学数据集上优于多种SOTA基线；在HAM10K与自建野火烟雾数据集上表现具竞争力；推理帧率提高约3.5倍。

Conclusion: 在低数据与资源受限条件下，该轻量离散扩散轮廓精炼管线可显著提升边界检测精度与效率，具有跨领域适用潜力。

Abstract: Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.

</details>


### [91] [EoCD: Encoder only Remote Sensing Change Detection](https://arxiv.org/abs/2602.05882)
*Mubashir Noman,Mustansar Fiaz,Hiyam Debary,Abdul Hannan,Shah Nawaz,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 提出EoCD：仅用编码器、早期融合并用无参数多尺度特征融合替代解码器，在多数据集上以更低复杂度实现性能-速度最优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用Siamese双流编码+时序后融合，并配套复杂解码器，导致参数量与计算开销大；少数早期融合方法虽去掉双编码器但仍依赖复杂解码器且性能不及后融合。作者希望在不牺牲精度的前提下显著降低模型复杂度与推理时延。

Method: 提出Encoder-only Change Detection (EoCD)。对时序影像进行早期融合，取消传统解码器，以无参数的多尺度特征融合模块在编码阶段直接完成变化信息聚合。方法可与多种主干编码器兼容，整体结构简单、轻量。

Result: 在四个具有挑战性的变化检测数据集上，EoCD在不同编码器架构下均取得良好精度，并展现优异的速度与复杂度（参数量、计算量）权衡，相较依赖复杂解码器的方法更高效。

Conclusion: 变化检测性能主要取决于编码器能力，解码器并非必要组件。EoCD以早期融合+无参数多尺度特征融合实现低复杂度与高效率的最佳平衡，泛化于多种编码器并在多数据集验证有效。

Abstract: Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.

</details>


### [92] [Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views](https://arxiv.org/abs/2602.05884)
*Gino E. Jansen,Carolina Brás,R. Nils Planken,Mark J. Schuuring,Berto J. Bouma,Ivana Išgum*

Main category: cs.CV

TL;DR: 提出一种从稀疏CTA平面分割重建完整心脏3D形状的方法，并验证其在模拟2D经胸超声视图中的可行性；在保留集上实现高Dice和显著优于Simpson双平面法的容量误差。


<details>
  <summary>Details</summary>
Motivation: 临床2D超声广泛可及但缺乏准确3D定量能力；直接3D成像昂贵或不可得。需要利用少量2D切面重建可靠的3D心腔与心肌形状，以改进容量估计与功能评估。

Method: 以神经隐式表示（MLP + 潜变量）学习来自CTA 3D分割的形状先验；测试时从模拟TTE的稀疏CTA平面分割出发，联合优化潜码与刚体配准，将观测平面嵌入3D空间并解码多类别体素/表面以重建心房、心室及左室心肌。比较四个模拟心尖视图条件下的重建与参考CTA。

Result: 在独立CTA分割测试集上，跨全部结构的Dice为0.86±0.04；左室容量误差4.88±4.26 mL，相比Simpson双平面法8.14±6.04 mL；左房容量误差6.40±7.37 mL，对比37.76±22.96 mL，显著更低。

Conclusion: 基于神经隐式的稀疏平面到3D重建在模拟TTE条件下可行，能较大幅度提升心腔三维定量精度，提示其作为2D超声中更准确3D测量途径的潜力。

Abstract: Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\pm$ 4.26 mL vs. 8.14 $\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\pm$ 7.37 mL vs. 37.76 $\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.

</details>


### [93] [CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression](https://arxiv.org/abs/2602.05909)
*Kangjie Zhang,Wenxuan Huang,Xin Zhou,Boxiang Zhou,Dejia Song,Yuan Xie,Baochang Zhang,Lizhuang Ma,Nemo Chen,Xu Tang,Yao Hu,Shaohui Lin*

Main category: cs.CV

TL;DR: 提出CLIP-Map：用可学习映射矩阵与Kronecker分解对CLIP权重做映射式压缩，并配以对角继承初始化，较选择式继承方法在各压缩率下更优，尤其高压缩下优势明显。


<details>
  <summary>Details</summary>
Motivation: CLIP在多任务上有效，但计算与内存开销大；现有压缩多为“选取并继承”少量权重再微调，极端压缩下表征能力受损。需要一种能尽可能保留原权重信息、在高压缩下仍稳健的方案。

Method: 提出映射式压缩框架CLIP-Map：通过可学习矩阵对预训练权重进行全映射（Full-Mapping），并用Kronecker因子分解降低映射参数量；为缓解映射带来的优化/分布漂移，设计“对角继承初始化”，让映射初始近似恒等映射以稳定训练与高效收敛。

Result: 在多项实验与不同压缩比下，CLIP-Map整体优于基于选择的压缩方法，在高压缩设置下提升尤为显著。

Conclusion: 映射式、带Kronecker分解与对角继承初始化的CLIP-Map可在保持信息的同时显著压缩CLIP模型，优于选择式权重继承，尤其适合资源受限场景。

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.

</details>


### [94] [Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2602.05937)
*Lingrui Li,Yanfeng Zhou,Nan Pu,Xin Chen,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出MGIPT用于医学图像分割的持续测试时自适应，通过多尺度全局与实例级提示调优缓解分布偏移、误差累积与遗忘，较SOTA更稳健。


<details>
  <summary>Details</summary>
Motivation: 医学多中心数据存在显著分布偏移，已训练的分割模型跨域部署性能下降。CTTA可在推理阶段持续适应，但参数增量更新易导致长程误差累积与灾难性遗忘；提示调优虽更稳健，但仍缺乏多尺度多样性、未充分利用实例知识且存在隐私风险。

Method: 提出MGIPT框架，包含两条互补分支：1) 自适应尺度实例提示（AIP）：为每个样本动态学习轻量实例级提示，并通过自适应最优尺度选择机制缓解误差累积；2) 多尺度全局提示（MGP）：在不同尺度上捕获域级知识，提升跨域鲁棒性与抗遗忘。两者通过加权集成融合，实现全局-局部双层适应，无需暴露隐私。

Result: 在多个医学图像分割基准与持续变化目标域上，MGIPT优于现有SOTA方法，表现出更稳健的适应性与更少的遗忘与误差累积。

Conclusion: 多尺度全局与实例级提示的联合调优能在CTTA中有效缓解分布偏移、误差累积与遗忘问题，提升医学分割的跨域与长期适应性能，并兼顾隐私友好。

Abstract: Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.

</details>


### [95] [Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching](https://arxiv.org/abs/2602.05951)
*Junwan Kim,Jiho Park,Seonghu Jeon,Seungryong Kim*

Main category: cs.CV

TL;DR: 论文提出在条件流匹配（flow matching）生成模型中，将“源分布”本身作为可学习、随条件变化的对象，以替代固定标准高斯，从而显著提升文本到图像任务的训练稳定性与效率，并带来更好性能（如FID更快收敛达3倍）。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法多沿用扩散模型的标准高斯源分布，忽略了条件信息（如文本提示）在源端的利用。作者动机在于：若源分布能与条件对齐，或可缩短从源到目标数据分布的流路径，提升训练样本效率、稳定性与收敛速度。

Method: 在流匹配目标下，学习一个依赖条件（文本）的源分布。为避免直接条件化源分布导致的崩塌与不稳定，提出两类关键正则/约束：1）方差正则，防止分布过度收缩（collapse）；2）方向对齐，使源到目标的初始流方向与目标表示空间中的语义方向一致。同时，系统性分析了目标表示空间（像素/特征空间等）的选择如何影响带结构化源分布的流匹配效果，给出哪些设置更有效的适用区间。

Result: 在多个文本到图像基准上，使用条件化、正则化的源分布带来稳定一致的性能提升；训练更高效，FID收敛可达3倍加速，表现出鲁棒改进。

Conclusion: 将源分布作为可学习、随条件变化的设计对象是可行且有效的。配合方差正则与方向对齐，并选择合适的目标表示空间，可在大规模文本到图像流匹配中实现更稳定、快速且更优的生成效果。

Abstract: Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.

</details>


### [96] [LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation](https://arxiv.org/abs/2602.05966)
*Mirlan Karimov,Teodora Spasojevic,Markus Braun,Julian Wiederer,Vasileios Belagiannis,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出一种名为LSA的微调框架，通过在动态目标局部区域对齐语义特征来提升视频生成的时间一致性，无需推理时外部控制信号，并在nuScenes与KITTI上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成方法在推理阶段需依赖控制信号来维持动态目标的时间一致性，限制了其作为通用数据引擎的可扩展性与泛化性；亟需一种训练阶段即可注入时间一致性的方式，避免推理时的依赖与开销。

Method: 提出Localized Semantic Alignment：利用现成特征提取模型，对真实视频与生成视频在围绕动态物体的局部片段上提取语义特征，并计算一致性损失；将该损失与标准扩散损失联合，用于对预训练视频生成模型进行单轮(epoch)微调，从而内化时间一致性。

Result: 仅用一轮微调即可在常见视频生成指标上优于基线；为更全面评测时间一致性，作者将目标检测中的mAP与mIoU改造用于生成视频评估，并在nuScenes与KITTI上取得显著提升；无需推理阶段外部控制与额外计算开销。

Conclusion: LSA以简单高效的局部语义对齐训练策略，显著增强了视频生成的时间一致性与动态目标稳定性，消除了推理时对控制信号的依赖，具有作为可扩展数据引擎的潜力。

Abstract: Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.

</details>


### [97] [RISE-Video: Can Video Generators Decode Implicit World Rules?](https://arxiv.org/abs/2602.05986)
*Mingxin Liu,Shuran Ma,Shibei Meng,Xiangyu Zhao,Zicheng Zhang,Shaofeng Zhang,Zhihang Zhong,Peixian Chen,Haoyu Cao,Xing Sun,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: RISE-Video 提出面向推理能力的 TI2V（文本/图像到视频）评测基准，含 467 条人工标注样本与四维度指标，用于检验生成视频模型在隐含规则下的认知与物理一致性；实验显示 11 个 SOTA 模型在复杂情景与隐式约束下普遍失效。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型虽有高保真视觉效果，但缺乏对隐含世界规则的内化与推理能力，现有评测多偏重美学与表层一致性，无法诊断模型在常识、物理、时序与专业知识等维度的智能缺陷。

Method: 构建 RISE-Video 基准：467 个经人工精标的样本，覆盖 8 类推理任务；设计四项评测指标：Reasoning Alignment、Temporal Consistency、Physical Rationality、Visual Quality；并提出利用大多模态模型（LMM）模拟人工评分的自动化评测流程。

Result: 在 11 个最先进 TI2V 模型上的大量实验表明：模型在包含隐式约束与复杂场景的任务中普遍表现不佳，尤其在物理合理性、时序一致性与隐含条件遵循上存在明显短板。

Conclusion: RISE-Video 将视频生成评估从审美转向认知推理，提供结构化、多维度、可扩展的评测方案；当前模型仍难以模拟隐式世界规则，未来需面向物理与常识约束的生成与推理融合方法。

Abstract: While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.

</details>


### [98] [VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation](https://arxiv.org/abs/2602.05998)
*Jie Deng,Kaichun Yao,Libo Zhang*

Main category: cs.CV

TL;DR: VisRefiner通过“看差异、改代码”的闭环训练，让截图到前端代码生成模型学会把可视化差异映射到相应的代码编辑，并在自我强化阶段迭代修正，从而显著提升一次生成质量、版式保真度与自我改进能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型直接从UI截图生成代码，但训练时从未看到自己代码渲染后的视觉结果；而人类开发者会渲染对比、基于差异迭代。作者希望让模型也能从“视觉差异—代码改动”的因果关系中学习，提高生成质量与可自我修复能力。

Method: 1) 差异对齐监督：将模型预测代码渲染图与目标设计图做视觉对比，构造与具体代码编辑对齐的监督信号，教模型理解外观变化对应哪些实现修改。2) 自我强化学习：在推理式训练环节，模型观察渲染输出与目标设计的差异，自主提出并应用代码更新，形成闭环自我精炼。

Result: 在标准评测中，VisRefiner显著提升单步生成质量与布局一致性，并展示出强的自我迭代优化能力，相比基线模型有明显增益。

Conclusion: 让模型显式学习“视觉差异→代码编辑”的映射并进行自我精炼，是推进截图到代码生成效果与可修复性的有效路径。

Abstract: Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.

</details>


### [99] [GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013)
*Ruihang Li,Leigang Qu,Jingxu Zhang,Dongnan Gui,Mengde Xu,Xiaosong Zhang,Han Hu,Wenjie Wang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文指出现有“绝对分数式”评估视觉生成的做法不稳定且与人类不一致，提出基于成对比较的统一评测框架GenArena，大幅提升评测稳定性与人对齐度，并用其对多任务下的生成模型做基准。


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型发展迅速，传统评测落后；依赖VLM打分成为主流，但主流的单样本绝对评分在随机一致性与人类感知一致性方面存在问题，需要更可靠、可复现、与人类判断对齐的评估方法。

Method: 提出GenArena：把评测从点式绝对评分转为成对比较（pairwise）协议；利用开源VLM作为裁判，通过两两对比生成结果，聚合比较结果得到稳定排名；在多种视觉生成任务上统一实现并自动化评测流程。

Result: 采用成对比较后，开源模型作为裁判即可优于专有模型裁判；评测准确率提升20%+；与权威LMArena榜单的Spearman相关达到0.86，显著高于点式方法的0.36；在多任务上给出统一基准结果。

Conclusion: 点式绝对评分不可靠；成对比较是更稳定且更贴近人类判断的评测范式。GenArena为视觉生成提供了可扩展、自动化且人对齐的评测标准，并能用开源裁判实现强性能，建议社区采用该范式进行基准化评估。

Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.

</details>


### [100] [MambaVF: State Space Model for Efficient Video Fusion](https://arxiv.org/abs/2602.06017)
*Zixiang Zhao,Yukun Cui,Lilun Deng,Haowen Bai,Haotong Qin,Tao Feng,Konrad Schindler*

Main category: cs.CV

TL;DR: 提出MambaVF：基于状态空间模型（SSM）的高效视频融合框架，无需光流与特征扭曲，通过时序状态更新与双向时空扫描实现线性复杂度的长程依赖建模与跨帧信息聚合，在多类视频融合基准上达SOTA，同时大幅降参、降FLOPs并提速。


<details>
  <summary>Details</summary>
Motivation: 现有视频融合方法依赖光流估计与特征对齐，计算与内存开销大、对复杂运动与尺度变化的泛化与可扩展性受限；需要一种能高效建模长程时序关系、摆脱显式运动估计的通用视频融合范式。

Method: 将视频融合重构为序列状态更新问题：采用SSM进行线性复杂度的时序建模；设计轻量级基于SSM的融合模块，以时空双向扫描机制替代传统流引导对齐，实现跨帧高效信息聚合；整体框架MambaVF在不同融合任务上统一适配。

Result: 在多曝光、多焦、红外-可见、医学视频融合等基准上取得SOTA；与现有方法相比，参数最多减少92.25%，FLOPs减少88.79%，推理速度提升约2.1倍。

Conclusion: SSM驱动的MambaVF无需显式光流即可高效捕获长程时序依赖并完成跨帧融合，在多类视频融合任务上兼具精度与效率，展现出良好的可扩展性与实际应用潜力。

Abstract: Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io

</details>


### [101] [Context Forcing: Consistent Autoregressive Video Generation with Long Context](https://arxiv.org/abs/2602.06028)
*Shuo Chen,Cong Wei,Sun Sun,Ping Nie,Kai Zhou,Ge Zhang,Ming-Hsuan Yang,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出“Context Forcing”框架：用具备长上下文的教师监督长上下文学生，并配合Slow-Fast Memory将线性增长的上下文压缩，以可计算成本实现2分钟级生成，显著提升长时一致性与有效上下文长度（>20秒），超越LongLive与Infinite-RoPE。


<details>
  <summary>Details</summary>
Motivation: 现有实时长视频生成多用短上下文教师指导长上下文学生，教师仅看5秒窗口，无法传递全局时序约束，造成学生-教师不匹配并限制学生的有效上下文长度与长时一致性。

Method: 1) Context Forcing：以具有全历史感知的长上下文教师对学生进行监督，消除监督失配；2) 上下文管理：引入Slow-Fast Memory，将随时间线性增长的上下文重构为慢通道（稀疏、长期记忆）与快通道（密集、短期细节），降低冗余与计算；3) 在极长时长（如2分钟）上进行可行训练与推理。

Result: 在多种长视频评测指标上优于SOTA；实现有效上下文>20秒，相比LongLive与Infinite-RoPE提升2–10倍，保持更强的长时一致性。

Conclusion: 用长上下文教师监督并配合Slow-Fast Memory的上下文管理，可稳定训练具备长时依赖建模能力的生成模型，在长视频一致性与可用上下文长度上显著超越现有方法。

Abstract: Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.

</details>


### [102] [Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation](https://arxiv.org/abs/2602.06032)
*David Shavin,Sagie Benaim*

Main category: cs.CV

TL;DR: 提出“Splat and Distill”框架：将2D视觉基础模型的特征前馈式提升为3D高斯表示，并将其从新视角投影回2D进行蒸馏，从而显著提升3D感知与语义能力。


<details>
  <summary>Details</summary>
Motivation: 现有2D视觉基础模型在多任务上成功但缺乏3D意识；以往通过每场景优化获得3D一致性的办法慢、易发生特征平均与伪一致性，难以规模化与动态改进。

Method: 用教师VFM产生2D特征；通过快速前馈的3D重建管线将2D特征“抬升”为显式3D高斯表示；将该3D表示从新视角高斯“splatting”回投生成多视角2D特征图；用这些几何一致的特征监督学生模型进行蒸馏；由于是前馈，无需每场景优化，教师与学生可在训练中共同提高一致性，减少特征平均伪像。

Result: 在单目深度、表面法线、多视图对应、语义分割等任务上显著优于先前方法；不仅3D一致性提升，2D特征的语义丰富度也增强。

Conclusion: 前馈式3D高斯抬升+新视角splat监督提供了高效、可扩展的几何一致蒸馏方案，克服了以往慢速 per-scene 优化与特征平均问题，为2D VFM注入稳健3D感知并提升下游多任务表现。

Abstract: Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/

</details>


### [103] [V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval](https://arxiv.org/abs/2602.06034)
*Dongyang Chen,Chaoyang Wang,Dezhao SU,Xi Xiao,Zeyu Zhang,Jing Xiong,Qing Li,Yuzhang Shang,Shichao Ka*

Main category: cs.CV

TL;DR: 提出V-Retrver：将多模态检索重塑为“假设-视觉核验”往返的代理式推理流程，通过外部视觉工具主动取证，显著提升多基准上的检索准确与推理稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的通用多模态检索多为语言驱动：视觉特征静态、缺乏细粒度证据核验，易在视觉歧义时产生臆测性推理。需要一种能在推理过程中主动查看与验证视觉细节的机制，以提升可依赖性与准确度。

Method: 提出V-Retrver，将检索表述为代理式推理：在生成假设与调用外部视觉工具进行针对性证据采集之间交替的“多模态交织”过程。训练采用课程式策略：1) 监督式推理激活；2) 基于拒绝的优化（过滤/改写不良轨迹）；3) 与证据对齐目标的强化学习，促使模型学会何时与如何取证并据此重排候选。

Result: 在多个多模态检索基准上获得一致增益，平均检索准确率提升约23.0%，并体现出更可靠的以感知为驱动的推理与更好的泛化能力。

Conclusion: 通过把检索转变为“证据驱动的代理推理”，并用课程式+RL的训练范式对齐取证行为，V-Retrver减少臆测、提升细粒度核验能力与检索效果，可作为MLLM检索的新范式。

Abstract: Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.

</details>


### [104] [InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions](https://arxiv.org/abs/2602.06035)
*Sirui Xu,Samuel Schulter,Morteza Ziyadi,Xialin He,Xiaohan Fei,Yu-Xiong Wang,Liangyan Gui*

Main category: cs.CV

TL;DR: InterPrior提出面向人形机器人“整体身体-物体交互”的可扩展生成式控制先验：先大规模模仿预训练，再经物理扰动增强与强化学习微调，形成能在未见目标/初始条件与新物体上泛化的统一策略。


<details>
  <summary>Details</summary>
Motivation: 人类以高层意图驱动，平衡/接触/操作会自然涌现；要让人形机器人在多样环境中复用与组合“行走-操作”技能，需要可扩展、物理一致的运动先验，突破仅依赖模仿在庞大交互空间中难以泛化的问题。

Method: 1) 全参考模仿专家蒸馏：训练目标条件的变分策略，可从多模态观测与高层意图重建动作；2) 物理扰动数据增强，暴露策略于更广分布；3) 强化学习微调，在未见目标与初始条件上提升成功率与稳健性；4) 将潜在技能压实为可行流形，形成统一生成式控制器，可用于交互式控制与机器人部署。

Result: 蒸馏策略重建训练行为但泛化不足；加入物理扰动与RL微调后，策略在未见任务、初始条件与新物体交互上显著提升，展现合成新行为的能力，并支持用户交互控制；展示对真实机器人落地的潜力。

Conclusion: 通过“模仿预训练+物理增强+RL微调”的统一框架，可将大规模人-物交互运动整合为通用运动先验，使人形机器人在保持全身物理协调的同时，对新目标和新物体具备更强泛化与组合能力，并具备实际部署前景。

Abstract: Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.

</details>


### [105] [Thinking with Geometry: Active Geometry Integration for Spatial Reasoning](https://arxiv.org/abs/2602.06037)
*Haoyuan Li,Qihang Cao,Tao Tang,Kun Xiang,Zihan Guo,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: GeoThinker提出从“被动融合”转向“主动感知”的3D几何与多模态大模型融合方式，通过在选定层进行空间落地的跨注意力与重要性门控，使语义特征按需检索与整合任务相关几何信息，显著提升空间智能（VSI-Bench 72.6），并在具身指代与自动驾驶等场景中表现出强泛化与更佳空间感知。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM整合3D几何多为全局、无差别融合，易导致语义与几何错配、冗余信息与低效推理；需要一种可依据推理需求有选择地利用几何证据的机制，以提升空间推理能力与下游泛化。

Method: 提出GeoThinker：1）空间落地融合（Spatial-Grounded Fusion）在精心挑选的VLM层进行，使视觉语义先作为查询，利用帧严格的跨注意力从几何编码中检索与任务相关的结构信息；2）重要性门控（Importance Gating）对每帧注意力进行偏置，突出与任务相关的几何结构，抑制无关或噪声信号；整体实现从按通道混合到按需检索的主动集成。

Result: 在VSI-Bench上取得72.6的新SOTA；在复杂下游任务（如具身指代、自动驾驶）上展现更强的空间感知与稳健泛化。

Conclusion: 主动地、按需整合空间结构而非被动全局融合是提升下一代空间智能的关键路径；GeoThinker验证了该范式在基准与实际场景中的有效性与可推广性。

Abstract: Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.

</details>


### [106] [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/abs/2602.06040)
*Jintao Tong,Shilin Yan,Hongwei Xue,Xiaojun Tang,Kunyu Shi,Guannan Zhang,Ruixuan Li,Yixiong Zou*

Main category: cs.CV

TL;DR: 提出SwimBird：一种可在文本、视觉、交错三种推理模式间自适应切换的多模态大模型，通过统一的混合自回归训练（文本下一token预测+视觉思维的下一embedding预测）与精心构建的SFT数据集，实现对视觉密集任务的大幅提升，同时保持强文本逻辑推理，达到了多项基准SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多依赖文本CoT，难以胜任视觉密集任务；近期用固定数量“视觉思维”提升视觉表现，但牺牲文本逻辑。根因是僵化、预设的推理范式，无法针对不同查询自适应选择最合适的思维模态。

Method: 提出可切换推理的MLLM——SwimBird：1) 三种模式：文本仅推理、视觉仅推理（连续隐状态作为视觉思维）、视觉-文本交错推理；2) 混合自回归建模，统一文本思维的下一token预测与视觉思维的下一embedding预测；3) 设计模式策展策略，构建覆盖三种范式的SFT数据集SwimBird-SFT-92K，以监督微调实现模式选择能力。

Result: 在覆盖文本推理与高难视觉理解的多种基准上，相比固定模式方法实现稳健增益，并在若干任务上达SOTA；在不牺牲文本逻辑的前提下显著提升视觉密集任务表现。

Conclusion: 自适应选择思维模态是提升MLLM通用推理能力的关键。SwimBird通过统一训练框架与多样化SFT数据，实现文本与视觉推理的平衡与互补，优于固定推理模式的多模态方法。

Abstract: Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.

</details>


### [107] [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/abs/2602.06041)
*Xuejun Zhang,Aditi Tiwari,Zhenhailong Wang,Heng Ji*

Main category: cs.CV

TL;DR: 提出CAMCUE，一个基于相机位姿的多图像跨视角推理框架，通过显式利用相机位姿进行跨视角融合、语言到位姿的对齐与目标视角的想象式合成，在新数据集CAMCUE-DATA上显著提升多图像空间推理与视角转换问答性能，并大幅降低推理时延。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多图像空间推理薄弱，尤其是需要从多视角构建一致的3D理解并在新语言指定视角下进行推理（perspective taking）。多数方法缺少对相机几何的显式建模，常依赖代价高的检索/匹配，难以高效准确泛化到人类自然语言描述的视角。

Method: 提出CAMCUE：1) 将每个视角的相机位姿注入视觉token，作为跨视角融合的几何锚点；2) 将自然语言中的目标视角描述显式对齐到目标相机位姿（位姿预测）；3) 条件于预测位姿合成“想象”的目标视角表征以辅助回答。并构建CAMCUE-DATA，含多视图图像与位姿、丰富的目标视角描述与视角转换问题，训练与评测模型（测试集含人工标注描述以评估泛化）。

Result: 在CAMCUE-DATA上整体准确率提升9.06%；从自然语言视角描述预测目标位姿的旋转准确率>90%（20°内），平移准确率在0.5阈值内>90%；相比基于检索/匹配的测试时搜索，推理时间从256.6秒降至1.45秒/例。

Conclusion: 显式位姿注入与语言到位姿的直接对齐可显著提升多图像跨视角空间推理的准确性与效率，避免昂贵的测试时搜索，支持快速交互式应用，并对MLLM中的3D一致性建模提供有效路径。

Abstract: Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.

</details>
