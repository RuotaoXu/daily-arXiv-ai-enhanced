<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 105]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 将顶尖事件相机眼动追踪模型“神经形态化”，用轻量LIF层与深度可分离卷积替代RNN/注意力，在接近Retina专用系统精度的同时，大幅降低参数与计算，预计可在毫瓦功耗、毫秒级时延下实时运行。


<details>
  <summary>Details</summary>
Motivation: 可穿戴AR/VR需要低时延、毫瓦级功耗的眼动追踪；传统帧式方案易受运动模糊、算力高、时间分辨率受限。事件相机+SNN具潜力，但现有SNN要么过于定制化、要么性能不及ANN。希望在保持高精度的同时获得神经形态效率。

Method: 以当前表现最优的事件驱动眼动追踪ANN为蓝本：用轻量LIF脉冲层替换循环与注意力模块，采用深度可分离卷积降复杂度；在事件数据上训练/评估，比较与ANN与专用系统Retina的精度、模型大小与理论计算量，并估算功耗与时延。

Result: SNN变体达到3.7–4.1像素平均误差，接近Retina的3.24像素；相较最接近的ANN版本，模型尺寸缩小约20倍，理论计算量降低约850倍；预计在1 kHz下实现3 ms时延、3.9–4.9 mW功耗。

Conclusion: 高性能事件相机眼动追踪架构可重构为SNN，在保持接近SOTA精度的同时显著提升效率，满足实时可穿戴部署需求。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

</details>


### [2] [ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects](https://arxiv.org/abs/2512.10031)
*Woojin Lee,Hyugjae Chang,Jaeho Moon,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: 提出ABBSPO，一种基于HBox弱监督的定向目标检测框架，通过自适应缩放GT盒和对称先验角度损失，提升尺度与角度学习稳定性与精度，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有HBox监督的定向检测把GT水平框与预测旋转框的最小外接矩形直接比较，导致尺度估计偏差；同时，多视角一致性弱监督在旋转/翻转增强下易出现错误一致、训练坍塌。需要一种既能准确对齐尺度、又能稳定学习角度的弱监督方法。

Method: 1) 自适应边界框缩放（ABBS）：根据每个预测RBox的形状对GT HBox进行可学习/规则化的缩放，使监督尺度与目标真实尺寸更匹配。2) 对称先验角度（SPA）损失：利用航拍目标的旋转/镜像对称性，设计自监督一致性约束，使原图、旋转、翻转三视图的角度预测在对称群作用下相容，避免“错误一致”导致的塌陷。整体形成ABBSPO框架，在HBox弱标注下训练OOD。

Result: 在多个数据集上的大量实验表明，ABBSPO在mAP、角度误差与尺度估计上均优于现有HBox弱监督OOD方法，达到SOTA。

Conclusion: 通过ABBS校正尺度监督并用SPA稳定角度学习，ABBSPO在无需RBox标注的情况下实现更精准的定向检测，证明了对称先验与自适应尺度对WS-OOD的有效性。

Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.

</details>


### [3] [Diffusion Is Your Friend in Show, Suggest and Tell](https://arxiv.org/abs/2512.10038)
*Jia Cheng Hu,Roberto Cavicchioli,Alessandro Capotondi*

Main category: cs.CV

TL;DR: 论文提出将扩散模型用作自回归生成的“建议器”，而非替代者，在图像描述任务上结合扩散模型的双向细化能力与自回归模型的语言结构优势，方法（SST）在COCO上无强化学习达到125.1 CIDEr-D，超过当前自回归与扩散SOTA。


<details>
  <summary>Details</summary>
Motivation: 离散序列生成（如文本）中，扩散模型难以超越自回归方法；但扩散模型具备全局、可双向细化的优点。作者动机是：不完全替换自回归模型，而是让扩散模型提供全局一致性的建议，从而兼得二者优势，改善图像描述质量。

Method: 提出Show, Suggest and Tell (SST)：在图像字幕生成中，扩散模型对序列（例如词或标记分布）进行去噪生成，作为“建议”或候选分布/模板；自回归解码器在生成过程中参考这些建议进行条件化与重打分，实现双向全局信息与局部语言流畅性的结合。进行大量消融以分析建议模块的影响与强度。

Result: 在COCO数据集上，无强化学习即可达到125.1 CIDEr-D，较自回归与扩散SOTA分别提升约1.5与2.5分。实验显示建议强度与描述质量正相关，验证了建议模块的有效性。

Conclusion: 将扩散模型作为自回归生成的辅助建议器是一条有效路径，能带来稳定提升，并揭示了一个被低估的研究方向：融合双向扩散细化与自回归语言建模以提高离散生成质量。

Abstract: Diffusion Denoising models demonstrated impressive results across generative Computer Vision tasks, but they still fail to outperform standard autoregressive solutions in the discrete domain, and only match them at best. In this work, we propose a different paradigm by adopting diffusion models to provide suggestions to the autoregressive generation rather than replacing them. By doing so, we combine the bidirectional and refining capabilities of the former with the strong linguistic structure provided by the latter. To showcase its effectiveness, we present Show, Suggest and Tell (SST), which achieves State-of-the-Art results on COCO, among models in a similar setting. In particular, SST achieves 125.1 CIDEr-D on the COCO dataset without Reinforcement Learning, outperforming both autoregressive and diffusion model State-of-the-Art results by 1.5 and 2.5 points. On top of the strong results, we performed extensive experiments to validate the proposal and analyze the impact of the suggestion module. Results demonstrate a positive correlation between suggestion and caption quality, overall indicating a currently underexplored but promising research direction. Code will be available at: https://github.com/jchenghu/show\_suggest\_tell.

</details>


### [4] [MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata](https://arxiv.org/abs/2512.10041)
*Yihao Liu,Chenyu Gao,Lianrui Zuo,Michael E. Kim,Brian D. Boyd,Lisa L. Barnes,Walter A. Kukull,Lori L. Beason-Held,Susan M. Resnick,Timothy J. Hohman,Warren D. Taylor,Bennett A. Landman*

Main category: cs.CV

TL;DR: MetaVoxel 提出一个联合扩散生成框架，学习影像与临床元数据的联合分布，用单一模型实现图像生成、年龄回归与性别分类，并支持任意子集条件的零样本推断，表现接近专用基线。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI多为面向单一任务/方向的条件模型，需要为不同输入/任务分别训练，限制了跨任务、跨模态与临床应用的灵活性。作者希望用一个模型同时覆盖影像与元数据，统一多任务并支持灵活条件推断。

Method: 构建MetaVoxel：一个联合扩散模型，直接对MRI影像体素与临床元数据的联合变量进行扩散/去噪学习；训练于九个数据集共1万+T1 MRI及配套元数据。通过在同一扩散过程中编码所有变量，推断阶段可对任意已知子集进行条件化，实现零样本多任务推断。

Result: 单一MetaVoxel在三类任务上达到与任务特定模型相当的性能：MRI图像生成、年龄估计、性别预测；并展示了在不同输入子集下灵活推断的能力。

Conclusion: 联合多模态扩散建模能统一传统上需要多模型的医学AI任务，提升模型复用与临床适用性，是迈向通用医学AI的有前景方向。

Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.

</details>


### [5] [Independent Density Estimation](https://arxiv.org/abs/2512.10067)
*Jiahao Liu*

Main category: cs.CV

TL;DR: 提出独立密度估计（IDE）以提升视觉-语言模型的人类式组合泛化：逐词建模词与图像特征的独立关联，并以熵驱动的组合推断聚合词级预测；在多数据集上对未见组合优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型在描述、生成等任务上强，但对未见属性-对象、关系等组合的泛化欠佳。需要能从已学的词级知识组合出新组合的机制，而非端到端整体匹配，避免“粘连”与共现偏差。

Method: 提出Independent Density Estimation：将句子分解为词级单元，学习每个词与图像（或其表征）之间的独立密度/兼容度；给出两种实现：1）以完全可解缠的视觉表征作为输入，直接为每个词估计对应因子；2）用VAE从原始图像学得部分解缠的潜变量，再对词进行独立密度建模。推断阶段用基于熵的组合推断，将各词预测按不确定性自适应融合。

Result: 在多个数据集上对未见组合（compositional splits）显著优于现有模型；展示了词级预测与图像因素更一致，泛化更稳健。

Conclusion: 词级独立密度建模结合熵引导的组合推断能有效提升视觉-语言的组合泛化；解缠表征（显式或VAE获得）是关键。方法通用、可与现有VL系统结合。

Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Neverthe- less, these models still encounter difficulties in achieving human-like composi- tional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connec- tion between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy- based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.

</details>


### [6] [TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing](https://arxiv.org/abs/2512.10095)
*Jiachen Tao,Junyi Wu,Haoxuan Wang,Zongxin Yang,Dawen Cai,Yan Yan*

Main category: cs.CV

TL;DR: TraceFlow提出一种面向动态镜面场景的高保真渲染框架，通过精准反射方向估计与物理一致的反射建模，实现更锐利、更真实的高光与反射效果，并在基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景渲染在镜面反射方面常出现方向估计不准、材料与环境交互欠物理性，导致反射模糊、伪影多、细节缺失。作者旨在同时解决动态几何/材质表征与环境反射建模的难题，实现物理一致且高保真的镜面合成。

Method: 1) 提出“残差材料增强二维高斯泼洒”（Residual Material-Augmented 2D Gaussian Splatting）表示，联合建模动态几何与材质，用于精确计算反射光线；2) 引入“动态环境高斯”（Dynamic Environment Gaussian）并设计混合渲染管线，将渲染分解为漫反射与镜面两支路：漫反通过光栅化、镜面通过光线追踪，保证物理一致；3) 采用粗到细训练策略，提升优化稳定性并促进物理上可解释的分解。

Result: 在多个动态场景基准上，定量和定性均优于现有方法，生成更锐利、更真实的镜面高光与反射，尤其在复杂动态环境中表现突出。

Conclusion: 通过新的场景/材质表示、动态环境建模与混合渲染加粗细化训练，TraceFlow实现了对动态镜面场景的物理一致高保真渲染，并显著超越现有技术。

Abstract: We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.

</details>


### [7] [Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information](https://arxiv.org/abs/2512.10102)
*Neelima Prasad,Jarek Reynolds,Neel Karsanbhai,Tanusree Sharma,Lotus Zhang,Abigale Stangl,Yang Wang,Leah Findlater,Danna Gurari*

Main category: cs.CV

TL;DR: 提出“层级实例跟踪”新任务：同时跟踪物体及其部件所有实例，并保持层级关系；并发布首个支持该任务的基准数据集与评测。


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪/实例分割通常只关注物体层级，忽视物体—部件等层级关系；应用（机器人、视频理解、人机交互）需要细粒度且结构化的跟踪能力，因此需要一个任务定义与基准推动研究。

Method: 1) 定义层级实例跟踪任务：对预定义的物体与部件类别进行全实例跟踪，并维护对象—部件层级关联；2) 构建数据集：552段视频中标注2,765个唯一实体，覆盖40个类别（含物体与部件），为层级关系提供跟踪标注；3) 评测：针对该任务改造4类模型，共7个变体进行基线实验与分析。

Result: 基线模型在新数据集上的表现有限，表明任务具有挑战性；数据与评测基准公开发布。

Conclusion: 层级实例跟踪是有意义且困难的新问题；所提供的数据集与基线揭示研究空间，期望推动维护物体—部件层级关系的多实例视频理解方法发展。

Abstract: We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/

</details>


### [8] [Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization](https://arxiv.org/abs/2512.10151)
*Charles Fanning,Mehmet Emin Aktas*

Main category: cs.CV

TL;DR: 提出将基于波形/小波的持久同调向量化为多尺度空间通道，作为输入条件信号接入乳腺X线两阶段检测流程，以提升跨设备与人群的泛化；在INbreast上将ConvNeXt-Tiny的病人级AUC从0.55提升到0.75。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线筛查虽能降死亡率，但读片误差高、假阳性/假阴性多；现有模型在跨扫描仪、模态与人群迁移时精度明显下降，急需稳健的、可泛化的结构化表征来提升外部数据集性能。

Method: 利用拓扑数据分析（TDA）的持久同调，捕捉图像在不同强度阈值下稳定存在的结构；将其进行小波基的向量化，生成对小强度扰动稳定的空间多尺度“持久性通道”地图；在两阶段病灶检测流水线中通过输入级通道拼接与ConvNeXt Tiny结合训练。训练/验证于CBIS-DDSM（美国胶片数字化），外部评估于INbreast（葡萄牙）与CMMD（中国），以病人级指标汇报。

Result: 在有限训练预算下，将ConvNeXt Tiny加入“波形/小波-持久性通道”后，INbreast病人级AUC从0.55提升至0.75；并在跨域（胶片→数字、跨国家）设置中显示更好的外部泛化。

Conclusion: 稳定的拓扑-小波条件通道可作为简单而有效的输入增强，改善乳腺摄影模型在外部数据集上的表现与鲁棒性，特别是在数据与计算受限时。

Abstract: Breast cancer is the most commonly diagnosed cancer in women and a leading cause of cancer death worldwide. Screening mammography reduces mortality, yet interpretation still suffers from substantial false negatives and false positives, and model accuracy often degrades when deployed across scanners, modalities, and patient populations. We propose a simple conditioning signal aimed at improving external performance based on a wavelet based vectorization of persistent homology. Using topological data analysis, we summarize image structure that persists across intensity thresholds and convert this information into spatial, multi scale maps that are provably stable to small intensity perturbations. These maps are integrated into a two stage detection pipeline through input level channel concatenation. The model is trained and validated on the CBIS DDSM digitized film mammography cohort from the United States and evaluated on two independent full field digital mammography cohorts from Portugal (INbreast) and China (CMMD), with performance reported at the patient level. On INbreast, augmenting ConvNeXt Tiny with wavelet persistence channels increases patient level AUC from 0.55 to 0.75 under a limited training budget.

</details>


### [9] [Feature Coding for Scalable Machine Vision](https://arxiv.org/abs/2512.10209)
*Md Eimran Hossain Eimon,Juan Merlos,Ashan Perera,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: 论文提出并评估MPEG的“面向机器的特征编码”(FCM)与其参考测试模型FCTM，用于在边-云切分推理中高效压缩中间特征，平均节省约85%码率且基本不损精度。


<details>
  <summary>Details</summary>
Motivation: 边缘设备算力受限，完整本地推理或云端推理在时延、带宽与隐私上均有折衷。边-云分割推理需传输中间特征，但其带宽开销仍大，缺乏统一、可互操作的压缩标准。

Method: MPEG制定FCM标准，定义特征比特流语法与编解码流程；论文提出并实现参考测试模型FCTM，对多种视觉任务的中间特征进行编码，评估码率-精度权衡。

Result: 在多类视觉任务上，FCTM实现平均约85.14%的码率降低，同时保持任务精度几乎不变。

Conclusion: FCM/FCTM为带宽受限与隐私敏感场景提供可扩展、可互操作的中间特征压缩路径，促进边-云协同智能部署。

Abstract: Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications.

</details>


### [10] [Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226)
*Shuhan Tan,Kashyap Chitta,Yuxiao Chen,Ran Tian,Yurong You,Yan Wang,Wenjie Luo,Yulong Cao,Philipp Krahenbuhl,Marco Pavone,Boris Ivanovic*

Main category: cs.CV

TL;DR: LCDrive提出用潜在语言而非自然语言来进行链式推理，把“推理+决策”统一到与动作对齐的潜在空间中；通过监督未来轨迹回放冷启动潜在CoT，并用闭环强化学习后训练，实现在端到端自动驾驶上更快推理、更优轨迹与更大RL收益。


<details>
  <summary>Details</summary>
Motivation: 文本CoT在驾驶中推理冗长、效率低且与动作/世界动态对齐不足；需要一种既高效又与行动结果紧密耦合的推理表示，以提升困难场景下的安全与性能。

Method: 设计Latent-CoT-Drive：用两类潜在token交替表达推理过程——(1) 动作提案token（与最终动作共享词表）; (2) 世界模型token（由学习到的潜在世界模型生成，表征这些动作的未来结果）。先用基于真实未来rollout的监督信号对动作与世界token进行冷启动训练；随后采用闭环强化学习进行后训练，增强推理与决策质量。

Result: 在大规模端到端驾驶基准上，相比无推理和文本CoT基线，LCDrive具有：更快推理速度、更高轨迹质量，以及在交互式强化学习中获得更大的性能提升。

Conclusion: 用动作对齐的潜在语言取代自然语言进行CoT可统一推理与决策，并结合监督冷启动与闭环RL带来更高效、更强性能的自动驾驶策略。

Abstract: Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.

</details>


### [11] [Emerging Standards for Machine-to-Machine Video Coding](https://arxiv.org/abs/2512.10230)
*Md Eimran Hossain Eimon,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: 论文比较了用于机器到机器视觉通信的像素编码与特征编码方案，证明压缩中间特征（FCM）在保精度同时显著降码率，并评估了将H.26X作为FCM内核编解码器的影响。


<details>
  <summary>Details</summary>
Motivation: 传统远程推理将面向人眼的视频码流传到云端，带来高带宽、差扩展性与隐私暴露问题。需要一种面向机器任务、节省带宽且保护隐私的通信/编码框架。

Method: 对比MPEG提出的两条管线：VCM（像素域任务感知编码）与FCM（中间神经特征压缩）。在FCM中以H.264/AVC、H.265/HEVC、H.266/VVC作为“内编解码器”压缩特征，评估其对多种机器视觉任务（含跟踪）的码率-任务精度权衡，使用BD-Rate等指标量化差异。

Result: FCM在接近边缘侧推理精度下显著降低比特率；作为FCM内编解码器，HEVC与VVC对任务性能几乎相当（VVC换HEVC平均BD-Rate仅+1.39%），而AVC相较VVC平均+32.28%。在跟踪任务上，编解码器选择影响很小，HEVC甚至优于VVC（BD-Rate -1.81%），AVC为+8.79%。

Conclusion: 面向机器的特征编码（FCM）相较传像素的远程推理更高效且更隐私友好；在现有硬件下，已部署的HEVC即可满足机器到机器通信需求且不降低任务性能，VVC并非必须，AVC在多数任务上劣于HEVC/VVC，但在跟踪任务上差距也较小。

Abstract: Machines are increasingly becoming the primary consumers of visual data, yet most deployments of machine-to-machine systems still rely on remote inference where pixel-based video is streamed using codecs optimized for human perception. Consequently, this paradigm is bandwidth intensive, scales poorly, and exposes raw images to third parties. Recent efforts in the Moving Picture Experts Group (MPEG) redesigned the pipeline for machine-to-machine communication: Video Coding for Machines (VCM) is designed to apply task-aware coding tools in the pixel domain, and Feature Coding for Machines (FCM) is designed to compress intermediate neural features to reduce bitrate, preserve privacy, and support compute offload. Experiments show that FCM is capable of maintaining accuracy close to edge inference while significantly reducing bitrate. Additional analysis of H.26X codecs used as inner codecs in FCM reveals that H.265/High Efficiency Video Coding (HEVC) and H.266/Versatile Video Coding (VVC) achieve almost identical machine task performance, with an average BD-Rate increase of 1.39% when VVC is replaced with HEVC. In contrast, H.264/Advanced Video Coding (AVC) yields an average BD-Rate increase of 32.28% compared to VVC. However, for the tracking task, the impact of codec choice is minimal, with HEVC outperforming VVC and achieving BD Rate of -1.81% and 8.79% for AVC, indicating that existing hardware for already deployed codecs can support machine-to-machine communication without degrading performance.

</details>


### [12] [Multi-dimensional Preference Alignment by Conditioning Reward Itself](https://arxiv.org/abs/2512.10237)
*Jiho Jang,Jinyoung Kim,Kyungjune Baek,Nojun Kwak*

Main category: cs.CV

TL;DR: 论文指出标准DPO在对扩散模型进行人类偏好对齐时因将多维偏好压缩为单一标量奖励而产生“奖励冲突”。作者提出MCDPO：在训练中显式引入多维偏好结果向量作为条件，采用“解耦的Bradley-Terry目标”，并配合维度级奖励dropout，实现在单一网络内对各奖励轴独立学习与平衡优化；推理阶段可用CFG对特定维度进行放大控制，无需额外训练或外部奖励模型。实验在SD1.5与SDXL上优于基线。


<details>
  <summary>Details</summary>
Motivation: 标准DPO用Bradley-Terry将审美、语义一致性等多维指标压缩为一个标量，导致当全局不偏好样本在某一维表现优秀时，模型被迫“反学”该优点，出现奖励冲突与次优对齐。需要一种在不拆分模型的前提下，同时学习并控制多维偏好的方法。

Method: 提出Multi Reward Conditional DPO（MCDPO）：1）解耦BT目标：对每个奖励维度分别建模偏好概率，避免跨维度相互掩蔽；2）在训练中将偏好结果向量作为条件输入，使网络学习各维独立的优化方向；3）维度级reward dropout，随机忽略部分维度信号，防止某些维度主导训练、提升均衡性；4）推理时用Classifier-Free Guidance基于条件向量进行多轴动态控制与放大。

Result: 在Stable Diffusion 1.5与SDXL上的大量实验表明：在多项对齐/审美/语义等基准上优于标准DPO与相关方法；并验证了无需额外训练或外部奖励模型即可在推理期对特定奖励维度执行可控放大。

Conclusion: 多维条件化与解耦偏好建模可消除DPO中的奖励冲突，允许单一扩散模型在训练期平衡各奖励轴、在推理期灵活多轴控制，从而在对齐质量与可控性上同时提升。

Abstract: Reinforcement Learning from Human Feedback has emerged as a standard for aligning diffusion models. However, we identify a fundamental limitation in the standard DPO formulation because it relies on the Bradley-Terry model to aggregate diverse evaluation axes like aesthetic quality and semantic alignment into a single scalar reward. This aggregation creates a reward conflict where the model is forced to unlearn desirable features of a specific dimension if they appear in a globally non-preferred sample. To address this issue, we propose Multi Reward Conditional DPO (MCDPO). This method resolves reward conflicts by introducing a disentangled Bradley-Terry objective. MCDPO explicitly injects a preference outcome vector as a condition during training, which allows the model to learn the correct optimization direction for each reward axis independently within a single network. We further introduce dimensional reward dropout to ensure balanced optimization across dimensions. Extensive experiments on Stable Diffusion 1.5 and SDXL demonstrate that MCDPO achieves superior performance on benchmarks. Notably, our conditional framework enables dynamic and multiple-axis control at inference time using Classifier Free Guidance to amplify specific reward dimensions without additional training or external reward models.

</details>


### [13] [Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective](https://arxiv.org/abs/2512.10244)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.CV

TL;DR: 问题：将开源视觉-语言模型用于半监督小样本学习时，常规SSL微调反而不如FSL。原因：VLM软max分布过“平”，伪标签置信低，导致无标签数据利用率低、监督信号弱。解法：简单但有效的分类器初始化+温度调参，并提出分阶段微调SWIFT，使SSL方法能有效微调VLM并利用任务相关但噪声的预训练数据。效果：在5个基准上比近期FSL/SSL高约5点，甚至接近用真标签的全监督微调。


<details>
  <summary>Details</summary>
Motivation: 现实“自动标注”需要用少量标注+大量未标注样本高效学习。开源VLM与其预训练数据资源丰富，但SSFSL领域基本未充分利用，且直接套用SSL微调VLM表现异常差，需找出原因并给出简单、通用的改进。

Method: 1) 分析：将现有SSL方法直接用于VLM微调，观察到性能显著落后FSL；诊断VLM输出概率分布“过平”，伪标签置信度低。2) 解决：采用（a）分类器初始化（如基于类文本原型/提示或FSL式权重初始化），（b）温度调参提高softmax尖锐度，提升伪标签置信与未标注数据利用率与监督强度。3) 提出SWIFT：分阶段微调+温度调参，使现有SSL在VLM上有效；并引入从VLM预训练集检索到的任务相关但带噪数据，协同训练。

Result: 在五个SSFSL基准上，SWIFT相较近期FSL与SSL方法平均提升约5个准确率点；在某些设置下接近甚至可与使用真实标签进行全监督微调的效果匹敌。

Conclusion: VLM在SSFSL中失效的关键是输出分布过平导致伪标签弱。通过极其简单的初始化与温度调参并进行分阶段微调（SWIFT），即可显著提升未标注数据利用与整体性能，充分发挥开源VLM与预训练数据的价值。

Abstract: Semi-supervised few-shot learning (SSFSL) formulates real-world applications like ''auto-annotation'', as it aims to learn a model over a few labeled and abundant unlabeled examples to annotate the unlabeled ones. Despite the availability of powerful open-source Vision-Language Models (VLMs) and their pretraining data, the SSFSL literature largely neglects these open-source resources. In contrast, the related area few-shot learning (FSL) has already exploited them to boost performance. Arguably, to achieve auto-annotation in the real world, SSFSL should leverage such open-source resources. To this end, we start by applying established SSL methods to finetune a VLM. Counterintuitively, they significantly underperform FSL baselines. Our in-depth analysis reveals the root cause: VLMs produce rather ''flat'' distributions of softmax probabilities. This results in zero utilization of unlabeled data and weak supervision signals. We address this issue with embarrassingly simple techniques: classifier initialization and temperature tuning. They jointly increase the confidence scores of pseudo-labels, improving the utilization rate of unlabeled data, and strengthening supervision signals. Building on this, we propose: Stage-Wise Finetuning with Temperature Tuning (SWIFT), which enables existing SSL methods to effectively finetune a VLM on limited labeled data, abundant unlabeled data, and task-relevant but noisy data retrieved from the VLM's pretraining set. Extensive experiments on five SSFSL benchmarks show that SWIFT outperforms recent FSL and SSL methods by $\sim$5 accuracy points. SWIFT even rivals supervised learning, which finetunes VLMs with the unlabeled data being labeled with ground truth!

</details>


### [14] [RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection](https://arxiv.org/abs/2512.10248)
*Zhuo Wang,Xiliang Liu,Ligang Sun*

Main category: cs.CV

TL;DR: RobustSora构建评测基准，量化AI生成视频检测对数字水印的依赖性，并提出两类任务检验在去水印与伪水印条件下的鲁棒性与误报。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC视频检测基准忽视了一个关键现实：许多SOTA生成模型会给输出嵌入数字水印，检测器可能无意间把水印当作判别线索，从而导致脆弱性与偏差。需要一个系统化框架衡量和隔离“水印因素”的影响。

Method: 构建包含4类视频（A-C、A-S、G-W、G-DeW）的6,500条数据集；设计两项评测任务：Task-I在“去水印AI视频”上测鲁棒性，Task-II在“带伪水印的真实视频”上测误报；选取10个代表性模型（专用检测器、Transformer、MLLM）进行系统实验。

Result: 在水印操控下，多数模型性能出现2–8个百分点波动：Transformer模型呈稳定的中等依赖（约6–8pp），多模态大模型表现多样（2–8pp）。总体显示检测器对水印存在部分依赖。

Conclusion: AIGC视频检测确有对水印的部分依赖，需要引入“水印感知”的训练与评测策略。RobustSora提供基准与工具，促进更鲁棒的检测研究。

Abstract: The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.

</details>


### [15] [THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose](https://arxiv.org/abs/2512.10251)
*Eunho Lee,Chaehyeon Song,Seunghoon Jeong,Ayoung Kim*

Main category: cs.CV

TL;DR: 提出THE-Pose：通过拓扑先验（表面嵌入）与混合图融合（HGF）在类别级6D位姿估计中同时利用2D全局上下文与3D局部结构，显著提升鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D图卷积的类别级位姿估计方法主要依赖局部几何与深度，易受类内变化、复杂外形与视觉歧义（遮挡、纹理缺失）影响；缺乏从图像域获取稳定的全局/拓扑信息的机制。

Method: 1) 从图像域提取一致且对类内变化不敏感的拓扑特征（表面嵌入作为拓扑先验）；2) 设计混合图融合HGF，自适应融合拓扑（2D）与点云（3D）特征，建立2D上下文与3D几何的桥接；3) 用融合特征进行类别级6D位姿估计，并在遮挡/未见物体上保持稳定。

Result: 在REAL275数据集上，相比3D-GC基线HS-Pose提升35.8%，并在所有关键指标上较此前SOTA提升7.2%。

Conclusion: 引入拓扑先验并通过混合图融合整合2D与3D信息，可显著增强类别级6D位姿估计对复杂形状、类内变化与遮挡的鲁棒性并取得新的SOTA表现。

Abstract: Category-level object pose estimation requires both global context and local structure to ensure robustness against intra-class variations. However, 3D graph convolution (3D-GC) methods only focus on local geometry and depth information, making them vulnerable to complex objects and visual ambiguities. To address this, we present THE-Pose, a novel category-level 6D pose estimation framework that leverages a topological prior via surface embedding and hybrid graph fusion. Specifically, we extract consistent and invariant topological features from the image domain, effectively overcoming the limitations inherent in existing 3D-GC based methods. Our Hybrid Graph Fusion (HGF) module adaptively integrates the topological features with point-cloud features, seamlessly bridging 2D image context and 3D geometric structure. These fused features ensure stability for unseen or complicated objects, even under significant occlusions. Extensive experiments on the REAL275 dataset show that THE-Pose achieves a 35.8% improvement over the 3D-GC baseline (HS-Pose) and surpasses the previous state-of-the-art by 7.2% across all key metrics. The code is avaialbe on https://github.com/EHxxx/THE-Pose

</details>


### [16] [GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule](https://arxiv.org/abs/2512.10252)
*Rui Wang,Yimu Sun,Jingxing Guo,Huisi Wu,Jing Qin*

Main category: cs.CV

TL;DR: 提出GDKVM用于超声心动图视频心腔分割，结合线性键值关联建模时序、门控Delta规则存储记忆、关键像素特征融合多尺度整合，兼顾长程依赖与实时性，在CAMUS与EchoNet-Dynamic上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有分割方法在超声序列中受噪声、伪影、器官变形与运动影响，且在捕获长程时空依赖与保持细粒度表示/计算效率之间存在权衡，难以同时达到高精度与实时性。

Method: 构建GDKVM架构：1) 用Linear Key-Value Association（LKVA）高效建模帧间相关；2) 以Gated Delta Rule（GDR）高效存储与更新中间记忆状态；3) 通过Key-Pixel Feature Fusion（KPFF）在多尺度上融合局部与全局特征，增强对边界模糊与噪声的鲁棒性。

Result: 在CAMUS与EchoNet-Dynamic数据集上，与多种SOTA方法对比，GDKVM在分割精度与鲁棒性上更优，并保持实时推理性能。

Conclusion: GDKVM在不牺牲效率的前提下提升了超声心动图序列分割的准确性与稳健性，适用于临床实时应用；代码已开源。

Abstract: Accurate segmentation of cardiac chambers in echocardiography sequences is crucial for the quantitative analysis of cardiac function, aiding in clinical diagnosis and treatment. The imaging noise, artifacts, and the deformation and motion of the heart pose challenges to segmentation algorithms. While existing methods based on convolutional neural networks, Transformers, and space-time memory networks have improved segmentation accuracy, they often struggle with the trade-off between capturing long-range spatiotemporal dependencies and maintaining computational efficiency with fine-grained feature representation. In this paper, we introduce GDKVM, a novel architecture for echocardiography video segmentation. The model employs Linear Key-Value Association (LKVA) to effectively model inter-frame correlations, and introduces Gated Delta Rule (GDR) to efficiently store intermediate memory states. Key-Pixel Feature Fusion (KPFF) module is designed to integrate local and global features at multiple scales, enhancing robustness against boundary blurring and noise interference. We validated GDKVM on two mainstream echocardiography video datasets (CAMUS and EchoNet-Dynamic) and compared it with various state-of-the-art methods. Experimental results show that GDKVM outperforms existing approaches in terms of segmentation accuracy and robustness, while ensuring real-time performance. Code is available at https://github.com/wangrui2025/GDKVM.

</details>


### [17] [VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models](https://arxiv.org/abs/2512.10262)
*Yuetong Su,Baoguo Wei,Xinyu Wang,Xu Li,Lixin Li*

Main category: cs.CV

TL;DR: 提出LLM-NCD，一个融合视觉-文本语义与原型引导聚类的多模态新类发现框架；通过联合优化已知类的图像与文本特征构建聚类中心与语义原型，并以双阶段机制（语义亲和阈值+自适应聚类）区分已知/新类；在CIFAR-100上对未知类精度最高提升25.3%，且对长尾分布具备鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图像NCD方法仅依赖视觉特征，面临特征可分性不足与长尾分布导致的性能退化问题。需要引入更丰富的语义先验与稳健的聚类策略，提升未知类识别与发现能力。

Method: 1) 多模态融合：联合优化已知类的图像与文本特征，学习对应的聚类中心与语义原型；2) 原型引导：利用已知类语义原型引导特征空间结构；3) 双阶段发现：先基于语义亲和阈值动态划分可能的已知/新样本，再对候选新样本进行自适应聚类；4) 在CIFAR-100上评估。

Result: 在CIFAR-100上，相比现有方法，对未知类的准确率最高提升25.3%；在长尾分布情形下表现出独特的鲁棒性。

Conclusion: 融合视觉-文本语义与原型引导的LLM-NCD有效提升新类发现性能，并首次在长尾分布场景下表现出显著鲁棒性，验证了多模态与双阶段机制的有效性。

Abstract: Novel Class Discovery aims to utilise prior knowledge of known classes to classify and discover unknown classes from unlabelled data. Existing NCD methods for images primarily rely on visual features, which suffer from limitations such as insufficient feature discriminability and the long-tail distribution of data. We propose LLM-NCD, a multimodal framework that breaks this bottleneck by fusing visual-textual semantics and prototype guided clustering. Our key innovation lies in modelling cluster centres and semantic prototypes of known classes by jointly optimising known class image and text features, and a dualphase discovery mechanism that dynamically separates known or novel samples via semantic affinity thresholds and adaptive clustering. Experiments on the CIFAR-100 dataset show that compared to the current methods, this method achieves up to 25.3% improvement in accuracy for unknown classes. Notably, our method shows unique resilience to long tail distributions, a first in NCD literature.

</details>


### [18] [Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction](https://arxiv.org/abs/2512.10267)
*Chen Ziwen,Hao Tan,Peng Wang,Zexiang Xu,Li Fuxin*

Main category: cs.CV

TL;DR: 提出Long-LRM++：结合半显式场景表征与轻量解码器，在保持高质量渲染的同时实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 直接一次性预测数百万高斯参数易受误差影响导致模糊；而将场景信息隐式压缩到模型权重虽提升保真度，但逐帧“解压”计算开销大，难以实时。需探索既保真又高效的表示与解码方式。

Method: 采用半显式场景表示，并配合轻量级解码器替代深度顺序解码。模型在多视图输入（扩展至64张，950×540分辨率）下进行前馈重建，输出用于实时渲染的表示；并通过组件级消融验证设计有效性。

Result: 在DL3DV上渲染质量匹配LaCT，同时在A100上达到约14 FPS的实时渲染；相较直接高斯深度渲染，在ScanNetv2的新视角深度预测上表现更优；能从32扩展到64视图且保持泛化能力。

Conclusion: 深度顺序解码并非必要。半显式表示+轻量解码器可在保证隐式方法保真度的同时实现实时渲染，并具备更强的扩展性与深度估计优势。

Abstract: Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\times540$ resolution, achieving 360° scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential "decompression" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.

</details>


### [19] [Sample-wise Adaptive Weighting for Transfer Consistency in Adversarial Distillation](https://arxiv.org/abs/2512.10275)
*Hongsin Lee,Hye Won Chung*

Main category: cs.CV

TL;DR: 提出SAAD：按样本对抗可迁移性自适应加权的对抗蒸馏方法，在不增加计算成本下提升学生模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗蒸馏多未使用最强鲁棒教师，且更强教师不一定带来更强学生（鲁棒饱和）。传统把原因归结为容量差距并不充分，需要找到影响鲁棒迁移的关键因素。

Method: 提出“对抗可迁移性”指标：学生生成的对抗样本对教师同样有效的比例；据此设计样本级自适应加权（SAAD），对训练样本按其可迁移性进行重加权，以强化有效知识转移；实现上无需额外计算开销。

Result: 在 CIFAR-10/100 与 Tiny-ImageNet 上，相比已有方法，SAAD 在 AutoAttack 下的鲁棒准确率持续提高。

Conclusion: 鲁棒蒸馏成效取决于对抗可迁移性而非仅是容量差距；通过样本级自适应加权可稳定增强学生鲁棒性，且高效无额外开销。

Abstract: Adversarial distillation in the standard min-max adversarial training framework aims to transfer adversarial robustness from a large, robust teacher network to a compact student. However, existing work often neglects to incorporate state-of-the-art robust teachers. Through extensive analysis, we find that stronger teachers do not necessarily yield more robust students-a phenomenon known as robust saturation. While typically attributed to capacity gaps, we show that such explanations are incomplete. Instead, we identify adversarial transferability-the fraction of student-crafted adversarial examples that remain effective against the teacher-as a key factor in successful robustness transfer. Based on this insight, we propose Sample-wise Adaptive Adversarial Distillation (SAAD), which reweights training examples by their measured transferability without incurring additional computational cost. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that SAAD consistently improves AutoAttack robustness over prior methods. Our code is available at https://github.com/HongsinLee/saad.

</details>


### [20] [MotionEdit: Benchmarking and Learning Motion-Centric Image Editing](https://arxiv.org/abs/2512.10284)
*Yixin Wan,Lei Ke,Wenhao Yu,Kai-Wei Chang,Dong Yu*

Main category: cs.CV

TL;DR: 提出MotionEdit数据集与MotionEdit-Bench基准，聚焦“动作中心”的图像编辑；并提出后训练框架MotionNFT，通过运动对齐奖励提升扩散式编辑模型的动作编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑数据集多关注静态外观改动，缺少高质量的“动作/交互”编辑样本；现有扩散式编辑模型在动作编辑上表现不足，缺乏专门评测与训练方法。

Method: 1) 构建MotionEdit：从连续视频中提取并人工核验的高保真动作变换图像对，保持身份与结构一致；2) 设计MotionEdit-Bench：包含生成式、判别式与偏好式多维指标，专测动作编辑；3) 提出MotionNFT：对已训练的编辑模型进行负向感知微调，依据输入-编辑图像与真值之间的运动流匹配度计算“运动对齐奖励”，引导模型学习正确的动作变换。

Result: 在FLUX.1 Kontext与Qwen-Image-Edit两种基座上，MotionNFT在不牺牲通用编辑能力的情况下，显著提升动作编辑的画质与运动保真，在MotionEdit-Bench上优于SOTA扩散式编辑方法。

Conclusion: 动作中心图像编辑是重要且困难的任务。MotionEdit与MotionEdit-Bench为该方向提供数据与评测基座；MotionNFT作为通用后训练方案，显著增强现有编辑模型的动作编辑能力并保持通用性。

Abstract: We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.
  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.

</details>


### [21] [ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions](https://arxiv.org/abs/2512.10286)
*Xiaoxue Wu,Xinyuan Chen,Yaohui Wang,Yu Qiao*

Main category: cs.CV

TL;DR: 提出ShotDirector框架，通过参数级相机控制与分层、剪辑模式感知提示，实现可控、电影化的多镜头视频生成，并配套ShotWeaver40K数据集与评测指标，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成多关注跨镜头的低层视觉一致性，忽视镜头衔接与电影剪辑语言，导致仅是顺序拼接、缺乏导演式叙事与有意图的转场设计。

Method: 1) 相机控制模块：引入6-DoF位姿与内参，精确注入相机运动与成像参数；2) 镜头感知掩码与分层提示：依据专业剪辑模式（如镜头规模、景别、镜头组接规律）进行层级化提示控制，实现细粒度内容与转场设计；3) 结合参数级条件与高层语义引导，统一到生成管线；4) 构建ShotWeaver40K数据集，包含电影化剪辑先验；5) 设计一套可控多镜头视频生成评测指标。

Result: 在构建的数据集与多项指标上进行大量实验，显示该框架在可控性、叙事连贯性与电影化转场质量方面优于现有方法。

Conclusion: 通过将可参数化相机控制与剪辑模式感知的分层提示结合，ShotDirector实现了更接近电影语言的可控多镜头生成；配套数据与评测为该方向提供了训练与评价基准。

Abstract: Shot transitions play a pivotal role in multi-shot video generation, as they determine the overall narrative expression and the directorial design of visual storytelling. However, recent progress has primarily focused on low-level visual consistency across shots, neglecting how transitions are designed and how cinematographic language contributes to coherent narrative expression. This often leads to mere sequential shot changes without intentional film-editing patterns. To address this limitation, we propose ShotDirector, an efficient framework that integrates parameter-level camera control and hierarchical editing-pattern-aware prompting. Specifically, we adopt a camera control module that incorporates 6-DoF poses and intrinsic settings to enable precise camera information injection. In addition, a shot-aware mask mechanism is employed to introduce hierarchical prompts aware of professional editing patterns, allowing fine-grained control over shot content. Through this design, our framework effectively combines parameter-level conditions with high-level semantic guidance, achieving film-like controllable shot transitions. To facilitate training and evaluation, we construct ShotWeaver40K, a dataset that captures the priors of film-like editing patterns, and develop a set of evaluation metrics for controllable multi-shot video generation. Extensive experiments demonstrate the effectiveness of our framework.

</details>


### [22] [Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings](https://arxiv.org/abs/2512.10293)
*Karthikeya KV,Narendra Bandaru*

Main category: cs.CV

TL;DR: Disentangled360提出一种在高斯点云渲染框架中显式拆分各向同性与各向异性贡献的3D感知方法，用于单张图像驱动的360°新视角合成，兼顾医学体渲染与自然场景。通过双分支条件、姿态无关锚点与方向解耦体渲染，实现快速、逼真且具有方向性的视图合成，并在多数据集上优于现有方法（SSIM/LPIPS），可交互运行且无需场景特定微调。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成/新视角合成方法要么过度简化各向异性光传播（影响真实感，尤其在医学体渲染），要么在不同场景间泛化性差、需要昂贵的微调或物理仿真。作者希望在单一框架中兼顾医学体数据（CT等）的散射特性与自然RGB场景的方向性与尺度一致性，实现单张输入的360°独特视角合成。

Method: 以高斯Splatting为骨干，引入方向解耦体渲染：将体/表面辐射分为各向同性与各向异性两部分并分别建模；采用双分支条件框架：一支以CT强度驱动体散射（面向体数据），另一支基于归一化相机嵌入（面向真实RGB场景）；提出混合、姿态无关的锚定策略，通过自适应深度与材质转变采样，在蒸馏中作为稳定枢轴以缓解尺度歧义并维持结构真实感；整合术前放射影像模拟与消费级360°渲染于统一推理管线，实现方向性强、快速的单图到多视角合成。

Result: 在Mip-NeRF 360、RealEstate10K、DeepDRR上取得更优的SSIM与LPIPS；运行时评估显示可交互速率；能够在无需场景特定微调或昂贵光子仿真的前提下实现高质量方向性视图合成。

Conclusion: Disentangled360通过在高斯渲染中显式拆分各向同性/各向异性贡献与双分支条件学习，统一支持医学体渲染与自然场景的360°新视角生成，具备更强泛化、结构稳定与实时性，适用于混合现实医疗监督、机器人感知与沉浸式内容创作。

Abstract: We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.

</details>


### [23] [Efficient-VLN: A Training-Efficient Vision-Language Navigation Model](https://arxiv.org/abs/2512.10310)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Main category: cs.CV

TL;DR: 提出Efficient-VLN，在VLN中通过高效记忆与动态混合策略显著降训练成本并达SOTA。


<details>
  <summary>Details</summary>
Motivation: MLLM用于VLN潜力大，但训练代价高。主要因：1) 长时序观察作为大量token带来二次方计算负担；2) DAgger的数据聚合存在探索-效率权衡：更多探索有助于分布偏移恢复，但训练与推理轨迹更长、成本更高。

Method: 提出Efficient-VLN：
- 高效记忆机制缓解token负担：
  (a) 渐进式记忆：对近期观察分配更多token，远期压缩，动态分配序列预算；
  (b) 可学习递归记忆：用可学习token的KV-cache作为记忆状态，递归更新，减少重复编码。
- 动态混合策略：在DAgger框架下自适应平衡探索与效率，动态调配专家与代理策略以控制轨迹长度与探索度。

Result: 在R2R-CE达64.2% SR、在RxR-CE达67.0% SR，均为SOTA；训练仅耗约282个H800 GPU小时，大幅降低开销。

Conclusion: 通过两类高效记忆与动态混合策略，Efficient-VLN在显著降低训练成本的同时提升VLN性能，缓解长序列计算与DAgger探索权衡问题。

Abstract: Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.

</details>


### [24] [DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation](https://arxiv.org/abs/2512.10314)
*Anh M. Vu,Khang P. Le,Trang T. K. Vo,Ha Thach,Huy Hung Nguyen,David Yang,Han H. Huynh,Quynh Nguyen,Tuan M. Pham,Tuan-Anh Le,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出一种利用视觉-语言对齐与原型库的弱监督病理分割方法，结合可学习文本提示与图像原型并配合多尺度金字塔以缓解ViT过平滑与CAM收缩，实验在BCSS-WSSS上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: WSSS在病理图像中受限于类间相似、类内差异大，以及基于CAM的监督导致区域收缩，难以准确定位病灶区域并降低标注成本。作者希望利用文本语义与视觉原型互补，提升弱监督条件下的区域发现能力。

Method: 构建原型驱动框架：1）采用CoOp式可学习提示，生成文本原型；2）联合可学习的图像原型，形成双模态原型库，捕获语义与外观信息；3）引入多尺度金字塔模块缓解ViT表示过平滑、提升空间精度与定位质量；4）在弱监督下以原型匹配/对齐指导分割学习。

Result: 在BCSS-WSSS基准上优于现有SOTA；消融显示文本描述多样性、上下文长度、以及文本与图像原型的互补性均带来增益。

Conclusion: 在数字病理WSSS中，联合利用文本语义与视觉原型学习、并配合多尺度特征，有效缓解CAM收缩与ViT过平滑问题，提升区域定位与分割性能。

Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.

</details>


### [25] [ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation](https://arxiv.org/abs/2512.10316)
*Khang Le,Ha Thach,Anh M. Vu,Trang T. K. Vo,Han H. Huynh,David Yang,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出一种用于病理图像弱监督语义分割的原型学习框架，融合CONCH的形态/语义能力、SegFormer的细粒度结构信息与文本引导，对无像素标注生成高质量伪掩码，并在BCSS-WSSS上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有WSSS依赖分类骨干，往往只定位最判别区域，难以覆盖完整组织结构；而视觉-语言模型与现代分割骨干分别擅长语义对齐与空间细节，但在弱监督条件下难以有效结合。

Method: 构建原型学习框架：1) 文本引导的原型初始化，利用病理描述提升伪掩码的语义完整性；2) 结构蒸馏，将SegFormer的多尺度空间/边界知识迁移到原型学习以保持细粒度形态；3) 融合CONCH的形态感知表示与SegFormer结构特征，训练轻量适配器，冻结基础模型。

Result: 在BCSS-WSSS数据集上优于现有WSSS方法，提升定位完整性与跨组织类型的语义一致性，同时保持较高计算效率（冻结大模型+轻量可训练模块）。

Conclusion: 通过文本引导原型与结构蒸馏，成功整合VLM与分割骨干的互补优势，在无像素级标注下生成更高质量伪掩码并提升WSSS性能与泛化性。

Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology relies heavily on classification backbones, yet these models often localize only the most discriminative regions and struggle to capture the full spatial extent of tissue structures. Vision-language models such as CONCH offer rich semantic alignment and morphology-aware representations, while modern segmentation backbones like SegFormer preserve fine-grained spatial cues. However, combining these complementary strengths remains challenging, especially under weak supervision and without dense annotations. We propose a prototype learning framework for WSSS in histopathological images that integrates morphology-aware representations from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment to produce prototypes that are simultaneously semantically discriminative and spatially coherent. To effectively leverage these heterogeneous sources, we introduce text-guided prototype initialization that incorporates pathology descriptions to generate more complete and semantically accurate pseudo-masks. A structural distillation mechanism transfers spatial knowledge from SegFormer to preserve fine-grained morphological patterns and local tissue boundaries during prototype learning. Our approach produces high-quality pseudo masks without pixel-level annotations, improves localization completeness, and enhances semantic consistency across tissue types. Experiments on BCSS-WSSS datasets demonstrate that our prototype learning framework outperforms existing WSSS methods while remaining computationally efficient through frozen foundation model backbones and lightweight trainable adapters.

</details>


### [26] [Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset](https://arxiv.org/abs/2512.10321)
*Hyunsoo Lee,Daeum Jeon,Hyeokjae Oh*

Main category: cs.CV

TL;DR: 提出Point2Pose：基于时空点云与姿态历史的注意力生成式回归器，用于3D人体姿态估计，并发布多模态室内数据集MVPose3D；在多数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 3D人体姿态估计因人体几何复杂、自遮挡严重、且缺乏大规模真实运动数据而困难；需要一种能建模条件分布并充分利用多模态序列信息的方法与数据支撑。

Method: 提出Point2Pose框架：1) 时空点云编码器+姿态特征编码器，提取关节级特征；2) 注意力驱动的生成式回归器，建模条件姿态分布（由序列点云与姿态历史条件化）；3) 构建MVPose3D大规模室内多模态数据集（IMU、稠密多视角点云、RGB）。

Result: 在多数据集上优于多种基线模型，显示更强的3D姿态估计性能。

Conclusion: 生成式、注意力驱动并结合时空点云与姿态历史的Point2Pose在3D姿态估计上更稳健有效；新数据集MVPose3D为复杂动作与多模态研究提供支持。

Abstract: We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.

</details>


### [27] [EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs](https://arxiv.org/abs/2512.10324)
*Chao Gong,Depeng Wang,Zhipeng Wei,Ya Guo,Huijia Zhu,Jingjing Chen*

Main category: cs.CV

TL;DR: EchoingPixels提出在音视频联合流上做自适应跨模态选 Token，以极少Token保持性能并提速降显存。核心是跨模态语义筛（CS2）从单一池中共同选择音视频Token，并配合Sync-RoPE维持稀疏令牌的时序关系。用原始5-20%的Token即可接近强基线，带来2-3倍速度与内存收益。


<details>
  <summary>Details</summary>
Motivation: AV-LLMs因音频与视频产生海量Token而计算与内存成本过高。现有视频专用的Token压缩方法无法利用音视频协同且通常按模态静态配额分配，难以处理两种模态信息密度随时间的动态变化。需要一种能在联合音视频流上进行早期跨模态交互、动态分配预算并保留时序建模能力的Token缩减方案。

Method: 提出EchoingPixels框架：1) 跨模态语义筛CS2在早期对音视频联合Token序列进行共注意力，从单一Token池中自适应选择最有信息的Token，而非对每个模态独立/固定配额压缩；2) 同步增强的相对位置编码Sync-RoPE，针对被稀疏选中的Token强化并保持关键的跨时间对齐关系，减缓激进采样带来的时序丢失；整体可插拔用于AV-LLM前端的Token缩减。

Result: 在多项实验中，EchoingPixels以仅5-20%的原始Token数仍达成与强基线可比的任务性能，同时实现约2-3倍推理速度提升与显存占用降低。

Conclusion: 跨模态、单一池自适应选Token结合时序保持机制，是缓解AV-LLMs计算瓶颈的有效路径；EchoingPixels在显著降本的同时维持性能，适合作为通用前端模块拓展到更广的音视频理解与多模态生成场景。

Abstract: Audio-Visual Large Language Models (AV-LLMs) face prohibitive computational overhead from massive audio and video tokens. Token reduction, while extensively explored for video-only LLMs, is insufficient for the audio-visual domain, as these unimodal methods cannot leverage audio-visual cross-modal synergies. Furthermore, the distinct and dynamic information densities of audio and video render static budgets per modality suboptimal. How to perform token reduction on a joint audio-visual stream thus remains an unaddressed bottleneck. To fill this gap, we introduce EchoingPixels, a framework inspired by the coexistence and interaction of visuals and sound in real-world scenes. The core of our framework is the Cross-Modal Semantic Sieve (CS2), a module enabling early audio-visual interaction. Instead of compressing modalities independently, CS2 co-attends to the joint multimodal stream and reduces tokens from an entire combined pool of audio-visual tokens rather than using fixed budgets per modality. This single-pool approach allows it to adaptively allocate the token budget across both modalities and dynamically identify salient tokens in concert. To ensure this aggressive reduction preserves the vital temporal modeling capability, we co-design a Synchronization-Augmented RoPE (Sync-RoPE) to maintain critical temporal relationships for the sparsely selected tokens. Extensive experiments demonstrate that EchoingPixels achieves performance comparable to strong baselines using only 5-20% of the original tokens, with a 2-3x speedup and memory reduction.

</details>


### [28] [StainNet: A Special Staining Self-Supervised Vision Transformer for Computational Pathology](https://arxiv.org/abs/2512.10326)
*Jiawen Li,Jiali Hu,Xitong Ling,Yongqiang Lv,Yuxuan Chen,Yizhi Wang,Tian Guan,Yifei Liu,Yonghong He*

Main category: cs.CV

TL;DR: 提出StainNet：面向特殊染色病理图像的ViT基础模型，基于自蒸馏SSL，在>1.4M特殊染色patch（来自20,231张WSI）上预训练；在肝恶性分类与两个公开ROI数据集等多项评估（含小样本与检索）中表现强，优于或可比近期更大PFMs；权重已开源。


<details>
  <summary>Details</summary>
Motivation: 现有病理大模型多以H&E染色为主，临床常用的特殊染色（如IHC等）在预训练中代表性不足，导致在涉及特殊染色的实际任务上迁移性能受限，难以满足广泛临床应用需求。

Method: 构建面向特殊染色的基础模型StainNet：以ViT为骨干，采用自监督的自蒸馏（self-distillation SSL）策略；使用HISTAI数据库中20,231张特殊染色WSI裁剪得到的>140万patch进行预训练。作为骨干/特征提取器用于MIL或ROI分析；随后在多任务上微调或评估。

Result: 在院内切片级肝恶性分类任务与两个公开ROI级数据集上取得强竞争力；在小比例学习（few-ratio）与图像检索评测中表现出色；与近期更大规模PFMs比较时显示出明显优势或相当性能。

Conclusion: 针对特殊染色的专用PFM能显著提升相关病理任务表现；StainNet在多场景中验证有效，具备数据高效与可迁移性强等优势，并已开放模型权重以促进研究与应用。

Abstract: Foundation models trained with self-supervised learning (SSL) on large-scale histological images have significantly accelerated the development of computational pathology. These models can serve as backbones for region-of-interest (ROI) image analysis or patch-level feature extractors in whole-slide images (WSIs) based on multiple instance learning (MIL). Existing pathology foundation models (PFMs) are typically pre-trained on Hematoxylin-Eosin (H&E) stained pathology images. However, images with special stains, such as immunohistochemistry, are also frequently used in clinical practice. PFMs pre-trained mainly on H\&E-stained images may be limited in clinical applications involving special stains. To address this issue, we propose StainNet, a specialized foundation model for special stains based on the vision transformer (ViT) architecture. StainNet adopts a self-distillation SSL approach and is trained on over 1.4 million patch images cropping from 20,231 publicly available special staining WSIs in the HISTAI database. To evaluate StainNet, we conduct experiments on an in-house slide-level liver malignancy classification task and two public ROI-level datasets to demonstrate its strong ability. We also perform few-ratio learning and retrieval evaluations, and compare StainNet with recently larger PFMs to further highlight its strengths. We have released the StainNet model weights at: https://huggingface.co/JWonderLand/StainNet.

</details>


### [29] [Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering](https://arxiv.org/abs/2512.10327)
*Cai Xu,Jinlong Liu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 提出一种针对不完整多视图聚类的选择性填补方法ISMVC：先基于信息量评估是否值得填补，再用带高斯混合先验的VAE做分布级填补与聚类表示学习，兼顾稳健与性能，在不平衡缺失下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 不完整多视图数据常见且各视图缺失程度不均。全量填补会在信息不足时引入噪声与偏差；完全不填补又在严重缺失下缺乏跨视图互补而性能下降。需要一种既能避免盲目填补又能在有依据时利用互补信息的方案。

Method: 提出ISMVC：1) 设计“与填补相关的信息量”度量，结合视内相似性与跨视一致性，对每个缺失位置评估支持强度，仅在信息充足时选择性填补；2) 将该选择机制与带高斯混合先验的VAE结合，学习有利于聚类的潜表示；3) 进行分布级填补，稳定后验分布聚合并显式建模填补不确定性，避免过度自信的重构；4) 作为轻量、数据驱动、与模型无关的插件，可集成于现有IMC框架。

Result: 在更现实且更具挑战的不平衡缺失设置下的多基准数据集实验中，ISMVC整体优于现有的填补型与免填补型多视图聚类方法。

Conclusion: 基于信息量的选择性填补结合MoG-VAE的分布级建模，能在不完整多视图场景中兼顾稳健性与聚类判别性，提供通用可插拔的改进路径，并在不平衡缺失情形下显著提升性能。

Abstract: Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.

</details>


### [30] [A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images](https://arxiv.org/abs/2512.10334)
*Yi Liu,Yichi Zhang*

Main category: cs.CV

TL;DR: 提出基于Pix2Pix的条件生成框架，从二值掩膜合成显微图像中的细丝结构，并引入“丝状感知”的结构损失；用合成数据增强训练，可提升细丝分割性能。


<details>
  <summary>Details</summary>
Motivation: 生物显微图像中微管、肌动蛋白等细长丝状结构对定量分析很关键，但像素级标注代价高且难，导致可用训练数据不足，限制了深度学习分割模型性能。

Method: 以Pix2Pix为骨干的条件生成模型，输入二值掩膜生成逼真的显微图像；同时设计“filament-aware”结构损失，强调细丝几何与拓扑一致性，提高合成图像与真实图像在结构上的相似度；用生成的合成图像与真实数据联合训练分割网络。

Result: 实验显示：使用该生成框架产生的合成数据进行数据增强，分割模型性能优于不使用合成数据的基线；结构损失进一步提升生成质量与下游分割效果。

Conclusion: 合成细丝显微图像结合结构感知损失可有效缓解标注数据稀缺问题，并显著提升丝状结构分割的准确性，优于仅用真实数据训练的方案。

Abstract: Thin and elongated filamentous structures, such as microtubules and actin filaments, often play important roles in biological systems. Segmenting these filaments in biological images is a fundamental step for quantitative analysis. Recent advances in deep learning have significantly improved the performance of filament segmentation. However, there is a big challenge in acquiring high quality pixel-level annotated dataset for filamentous structures, as the dense distribution and geometric properties of filaments making manual annotation extremely laborious and time-consuming. To address the data shortage problem, we propose a conditional generative framework based on the Pix2Pix architecture to generate realistic filaments in microscopy images from binary masks. We also propose a filament-aware structural loss to improve the structure similarity when generating synthetic images. Our experiments have demonstrated the effectiveness of our approach and outperformed existing model trained without synthetic data.

</details>


### [31] [Zero-shot Adaptation of Stable Diffusion via Plug-in Hierarchical Degradation Representation for Real-World Super-Resolution](https://arxiv.org/abs/2512.10340)
*Yi-Cheng Liao,Shyang-En Weng,Yu-Syuan Xu,Chi-Wei Hsiao,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出HD-CLIP，将真实退化图像分解为语义嵌入与序等级退化嵌入，并以CFG/CFPG融入扩散模型，作为即插即用模块在多种SR框架中无训练提升细节与真实感。


<details>
  <summary>Details</summary>
Motivation: 真实世界超分存在未知且耦合的多种退化，扩散模型需要更丰富、可量化的引导；现有方法假设已知退化强度且依赖CLIP文本编码，无法表达数值等级，泛化弱、易幻觉。

Method: 构建HD-CLIP：将低质图像编码为两支嵌入—语义嵌入（内容）与序等级退化嵌入（有序关系、可插值）；并通过classifier-free guidance与提出的classifier-free projection guidance将两类嵌入引入扩散恢复过程，用语义引导生成、用退化信号抑制幻觉与伪影；作为即插即用模块，无需训练可接入多种SR框架。

Result: 在多种真实世界数据集上显著提升细节保真与感知真实度；对未见退化等级具有良好插值与泛化能力，减少幻觉和伪影。

Conclusion: HD-CLIP提供了能表征退化等级的分层引导，并通过CFG/CFPG有效作用于扩散式SR，作为即插即用模块在不同框架与数据上带来稳定收益与更强泛化。

Abstract: Real-World Image Super-Resolution (Real-ISR) aims to recover high-quality images from low-quality inputs degraded by unknown and complex real-world factors. Real-world scenarios involve diverse and coupled degradations, making it necessary to provide diffusion models with richer and more informative guidance. However, existing methods often assume known degradation severity and rely on CLIP text encoders that cannot capture numerical severity, limiting their generalization ability. To address this, we propose \textbf{HD-CLIP} (\textbf{H}ierarchical \textbf{D}egradation CLIP), which decomposes a low-quality image into a semantic embedding and an ordinal degradation embedding that captures ordered relationships and allows interpolation across unseen levels. Furthermore, we integrated it into diffusion models via classifier-free guidance (CFG) and proposed classifier-free projection guidance (CFPG). HD-CLIP leverages semantic cues to guide generative restoration while using degradation cues to suppress undesired hallucinations and artifacts. As a \textbf{plug-and-play module}, HD-CLIP can be seamlessly integrated into various super-resolution frameworks without training, significantly improving detail fidelity and perceptual realism across diverse real-world datasets.

</details>


### [32] [CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates](https://arxiv.org/abs/2512.10342)
*Shresth Grover,Priyank Pathak,Akash Kumar,Vibhav Vineet,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 提出用于纠错型视觉顺序规划的评测基准CoSPlan，并给出训练-free方法SGI，通过引入中间场景图增量推理，提升VLM在错误检测与步骤补全上的表现，平均提升5.2%，并可迁移至Plan-Bench与VQA。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型虽在复杂推理上强，但在多步视觉顺序规划上研究不足；真实规划会包含非最优/错误步骤，需要模型具备发现并纠正错误的能力。缺少系统性基准与有效方法评估/提升VLM在此场景的可靠性。

Method: 1) 构建CoSPlan基准，涵盖迷宫导航、方块重排、图像重建、物体重组织四个视觉顺序规划域；评估两项能力：错误检测与步骤补全。2) 系统评测包含CoT与场景图等最先进技巧的VLM（如Intern-VLM、Qwen2）。3) 提出训练无关的Scene Graph Incremental updates（SGI）：在初始与目标状态之间插入中间场景图增量推理步骤，帮助模型逐步推演行动序列并修正错误。

Result: 即便结合CoT与场景图，现有VLM在CoSPlan上仍难以利用上下文达成目标；采用SGI后，平均性能提升5.2%，并在传统规划（Plan-Bench）与VQA任务上也有泛化收益。

Conclusion: CoSPlan揭示了VLM在纠错型顺序规划中的明显短板；SGI作为一种无需训练的中间推理增强策略，能稳定提升错误检测与步骤补全能力，并对更广泛的规划与问答任务具有可迁移性。

Abstract: Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.

</details>


### [33] [Topology-Agnostic Animal Motion Generation from Text Prompt](https://arxiv.org/abs/2512.10352)
*Keyi Chen,Mingze Sun,Zhenyu Liu,Zhangquan Chen,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出OmniZoo跨140物种的多模态动物运动数据集，并基于此构建可处理任意骨架拓扑的文本驱动自回归运动生成框架，核心为拓扑感知骨架嵌入，实现跨物种、物理合理、时间连贯、语义对齐的运动合成与风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成多依赖固定骨骼模板，难以泛化到不同或扰动的骨架拓扑；同时缺乏大规模、异构动物运动数据和能统一建模任意骨架与文本条件的生成框架。

Method: 1) 构建OmniZoo数据集：覆盖140物种、32,979段序列，提供多模态标注；2) 设计通用自回归运动生成框架：输入文本与目标骨架；3) 提出拓扑感知骨架嵌入模块（TSEM）：将任意骨架的几何与结构属性编码到共享token空间，并与文本语义融合；4) 生成时间连贯、物理可行的骨架运动，并支持跨物种风格迁移。

Result: 在OmniZoo上，模型可对任意目标骨架根据文本生成语义对齐、物理合理、时间一致的动作，并实现跨物种风格迁移，显示出对不同及扰动骨架拓扑的强泛化能力。

Conclusion: 通过大规模异构数据集与拓扑感知嵌入的统一自回归框架，突破固定骨架模板限制，实现任意骨架拓扑下的文本驱动运动生成与风格迁移，为广泛应用场景提供通用解决方案。

Abstract: Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.

</details>


### [34] [Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation](https://arxiv.org/abs/2512.10353)
*Yiheng Lyu,Lian Xu,Mohammed Bennamoun,Farid Boussaid,Coen Arrow,Girish Dwivedi*

Main category: cs.CV

TL;DR: TranSamba 是一种用于体数据弱监督医学分割的混合 Transformer-Mamba 架构，通过在ViT中插入跨切片的Mamba块，在线性时间与常量内存下建模跨切片3D上下文，从而提升基于注意力的目标定位并在三数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督体分割多依赖2D编码器，忽略体数据的3D上下文，导致跨切片信息利用不足、定位不稳且计算/内存成本高。需要一种既高效又能显式捕获体数据纵深维上下文的方案。

Method: 以标准ViT为骨干，在其Transformer块之间增设Cross-Plane Mamba（跨切片）模块：在每个体数据的相邻切片间利用状态空间模型（SSM）的线性复杂度进行信息交换；Mamba模块输出与切片内的自注意力特征融合，增强pairwise self-attention，从而直接改进弱监督定位所依赖的注意力图。整体复杂度随体深度线性扩展，并支持批处理的常量内存。

Result: 在三个不同模态与病灶的数据集上，TranSamba均超过现有方法，建立新的SOTA表现（文中强调稳健、跨数据集一致的性能提升）。

Conclusion: 将Mamba的高效跨切片建模与Transformer的切片内自注意力结合，可在弱监督体分割中以线性时间、常量内存有效捕获3D上下文，显著提升定位与分割表现，并具有良好的可扩展性与通用性。

Abstract: Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.

</details>


### [35] [mmCounter: Static People Counting in Dense Indoor Scenarios Using mmWave Radar](https://arxiv.org/abs/2512.10357)
*Tarik Reza Toha,Shao-Jung,Lu,Shahriar Nirjon*

Main category: cs.CV

TL;DR: 提出mmCounter，通过提取<1 Hz的微弱生命体征（呼吸、微动）并进行多阶段信号处理，在高密静止人群中实现毫米波雷达人数统计，熟悉环境F1=87%、MAE=0.6，未知环境F1=60%、MAE=1.1，最多可在3 m²内统计7人。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波雷达在高密、静止人群中计数困难，因空间分辨率有限且依赖运动回波；既有工作多假设已知人数（如仅估计呼吸率），无法在未知人数与强静态杂波下稳健计数。

Method: 设计mmCounter，多阶段低频信号提取与空间映射流程：从原始雷达回波中提取超低频（<1 Hz）源（呼吸与细微躯干移动），抑制静态背景与近邻物体干扰；进行源分离、去噪、稳健特征提取，并将低频源与空间位置关联，实现一一映射与人数估计。

Result: 在多种室内环境评估：熟悉环境平均F1 87%、MAE 0.6；未知环境平均F1 60%、MAE 1.1；在3 m²、前后1 m间距、无并排的高密场景下可统计至多7人。

Conclusion: 通过利用生命体征级超低频微动与新型信号处理管线，毫米波雷达可在高密静止人群中实现有效计数，但在未见过环境中性能下降，表明对环境泛化与鲁棒性仍有改进空间。

Abstract: mmWave radars struggle to detect or count individuals in dense, static (non-moving) groups due to limitations in spatial resolution and reliance on movement for detection. We present mmCounter, which accurately counts static people in dense indoor spaces (up to three people per square meter). mmCounter achieves this by extracting ultra-low frequency (< 1 Hz) signals, primarily from breathing and micro-scale body movements such as slight torso shifts, and applying novel signal processing techniques to differentiate these subtle signals from background noise and nearby static objects. Our problem differs significantly from existing studies on breathing rate estimation, which assume the number of people is known a priori. In contrast, mmCounter utilizes a novel multi-stage signal processing pipeline to extract relevant low-frequency sources along with their spatial information and map these sources to individual people, enabling accurate counting. Extensive evaluations in various environments demonstrate that mmCounter delivers an 87% average F1 score and 0.6 mean absolute error in familiar environments, and a 60% average F1 score and 1.1 mean absolute error in previously untested environments. It can count up to seven individuals in a three square meter space, such that there is no side-by-side spacing and only a one-meter front-to-back distance.

</details>


### [36] [Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task](https://arxiv.org/abs/2512.10359)
*Sunqi Fan,Jiashuo Cui,Meng-Hao Guo,Shuojin Yang*

Main category: cs.CV

TL;DR: 提出一套可扩展的视频工具箱与时空推理框架（STAR），为MLLM提供时序+空间工具调度，显著提升视频问答的时空理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频问答中难以同时处理帧内空间关系与跨帧因果时序，导致对复杂、推理密集型VideoQA表现不足；需要系统化工具与合理调度机制提升时空推理。

Method: 构建“视频工具箱”，涵盖时间与空间两类轻量工具；提出STAR时空推理框架，策略性编排工具调用顺序，先时间后空间（或交替）逐步定位关键片段与区域，避免工具链捷径与错误链式调用；将该框架接入GPT-4o，实现可控的工具调用与逐步推理。

Result: 在VideoMME上提升8.2%，在LongVideoBench上提升4.6%，表明在多样与长视频场景中均有效。代码开源于https://github.com/fansunqi/VideoTool。

Conclusion: 通过可扩展工具箱与STAR调度策略，MLLM的视频时空推理能力得到显著增强，向自主智能视频分析助手迈进一步。

Abstract: Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.

</details>


### [37] [Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models](https://arxiv.org/abs/2512.10362)
*Woojun Jung,Jaehoon Go,Mingyu Jeon,Sunjae Yoon,Junyeong Kim*

Main category: cs.CV

TL;DR: 提出“视觉漏斗”（Visual Funnel），一种训练无关的两步法，通过上下文锚定与基于熵的分层多裁剪组合，缓解MLLM在精细视觉感知中的“情境盲视”，优于单裁剪与无结构多裁剪基线。


<details>
  <summary>Details</summary>
Motivation: MLLM具备推理能力但对细粒度视觉细节感知不足。现有通过裁剪显著区域的方法虽能提细节，却打破与全局的结构关联，导致即使信息齐备也无法正确理解，即“情境盲视”。问题不在信息量不足，而在输入缺乏结构多样性。

Method: 提出“Visual Funnel”，无需训练、两步实现：1) 上下文锚定（Contextual Anchoring）：用一次前向传递定位兴趣区域；2) 熵尺度组合（Entropy-Scaled Portfolio）：依据注意力熵自适应确定多尺度裁剪大小并细化裁剪中心，形成从焦点到更广环境的层级式裁剪集合，保留层级上下文结构。

Result: 在多项实验中，Visual Funnel显著优于朴素单裁剪与无结构多裁剪基线；简单增加无结构裁剪收益有限甚至有害，证明分层结构是缓解情境盲视的关键。

Conclusion: 通过引入结构化、分层的多裁剪组合而非堆叠信息量，Visual Funnel有效缓解MLLM的情境盲视，提升精细视觉感知与上下文一致性，且无需额外训练。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: "Contextual Blindness". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.

</details>


### [38] [Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos](https://arxiv.org/abs/2512.10363)
*Mingyu Jeon,Jisoo Yang,Sungjin Han,Jinkwon Hwang,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 提出零样本长视频时刻检索框架P2S，通过“自适应跨度生成”避免候选爆炸，并用“查询分解”替代高成本VLM校验，实现无需训练即可在小时级视频中进行时间定位，性能超越多项有监督SOTA。


<details>
  <summary>Details</summary>
Motivation: LVMR难点在于无法一次性处理超长视频，主流“先搜索再精炼”范式要么依赖重监督、泛化差、成本高，要么零样本方法在搜索阶段候选爆炸、精炼阶段依赖昂贵且易语义偏差的VLM校验，导致效率与效果兼顾困难。

Method: 构建无训练框架P2S：1) 自适应跨度生成（Adaptive Span Generator）在搜索阶段动态生成更精炼的候选时间段，抑制爆炸式候选；2) 查询分解（Query Decomposition）将自然语言查询拆解为可执行子目标/要点，用于对候选段进行语义细化与过滤，避免依赖大型VLM的昂贵验证。

Result: 在小时级视频零样本时间定位任务上，P2S首次实现有效的时间锚定，且在MAD等基准上优于有监督SOTA（如R5@0.1提升约+3.7%）。

Conclusion: P2S在无需任务特定训练的前提下，解决了搜索阶段的候选爆炸与精炼阶段的高成本验证问题，实现高效、可扩展的长视频零样本时间定位，并显著超越有监督方法。

Abstract: Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).

</details>


### [39] [Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views](https://arxiv.org/abs/2512.10369)
*Zhankuo Xu,Chaoran Feng,Yingtao Li,Jianbin Zhao,Jiashu Yang,Wangbo Yu,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: CoherentGS 提出在稀疏且运动模糊的输入下，实现高保真 3D 高斯点渲染重建。核心是“去模糊光度先验 + 扩散几何先验”的双先验框架，并配合一致性引导的相机探索与深度正则，显著优于现有方法，在仅有 3/6/9 张图像时也能稳定重建。


<details>
  <summary>Details</summary>
Motivation: 3DGS 在新视角合成上领先，但依赖密集清晰影像。实际中常见稀疏视角与运动模糊相互强化：稀疏导致多视约束不足难以解模糊；模糊又抹去高频细节使稀疏视角难以对齐，造成重建破碎与低频偏置。需要能同时处理稀疏与模糊的鲁棒重建方法。

Method: 提出 CoherentGS：1) 双先验策略——引入预训练去模糊网络提供锐化与光度引导；引入扩散模型提供几何先验以补全不可见区域。2) 一致性引导的相机探索模块，自适应引导生成过程，提升多视一致性。3) 深度正则化损失，约束几何合理性。方法在 3D 高斯点框架中联合优化。

Result: 在合成与真实场景上，用极少视角（3/6/9）进行评测，定量与可视化均显著优于现有方法，达成新的 SOTA；能缓解破碎重建与低频偏置，生成连贯且细节丰富的场景。

Conclusion: 双先验（光度去模糊 + 几何扩散）结合一致性相机探索与深度正则，可有效打破稀疏与模糊的恶性循环，使 3DGS 在稀疏、模糊输入下仍能实现高保真重建，并刷新该任务的 SOTA。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.

</details>


### [40] [RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds](https://arxiv.org/abs/2512.10376)
*Jingyun Fu,Zhiyu Xiang,Na Zhao*

Main category: cs.CV

TL;DR: 提出首个联合4D毫米波雷达与激光雷达的场景流学习框架RaLiFlow，并构建配套Radar-LiDAR场景流数据集；通过动态感知的双向跨模态融合模块和针对雷达噪声的损失设计，显著优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 现有场景流融合多为图像+LiDAR，鲜有探索4D雷达+LiDAR；4D雷达价格低、抗恶劣天气且可测点级速度，能补充LiDAR，但其噪声大、分辨率低、稀疏且缺乏联合数据集与标注。

Method: 1) 基于公开自动驾驶数据构建Radar-LiDAR场景流数据集；提出雷达去噪与标签生成的预处理策略，尤其为目标边界外的雷达点提供更可靠的流真值。2) 提出RaLiFlow框架，设计动态感知的双向跨模态融合（DBCF）模块：将雷达的动态线索注入局部跨注意力中，实现跨模态上下文传播。3) 设计一组损失函数，降低不可靠雷达数据对训练的不利影响，并增强跨模态、实例级一致性，尤其关注动态前景。

Result: 在重构的数据集上，RaLiFlow较现有LiDAR或雷达单模态方法取得显著性能提升，证明融合与损失设计的有效性。

Conclusion: 4D雷达与LiDAR的联合学习在场景流估计上可显著受益；数据与方法（DBCF与鲁棒损失）共同缓解雷达噪声与稀疏性问题，为复杂天气和动态场景下的精确运动估计提供新路径。

Abstract: Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.

</details>


### [41] [Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching](https://arxiv.org/abs/2512.10379)
*Alberto Rota,Elena De Momi*

Main category: cs.CV

TL;DR: 提出一个用于内窥镜图像配准的自监督深度学习管线：用新视角合成生成真值内点对应，挖掘对比学习三元组，基于DINOv2并加一层Transformer以产出可用余弦相似度直接匹配的嵌入；在SCARED数据集上优于现有方法（更高匹配精度与更低极线误差）。


<details>
  <summary>Details</summary>
Motivation: 微创手术依赖视觉，需在内镜帧间建立精确像素级对应以支持3D重建、相机跟踪与场景理解。然而手术场景有弱透视线索、非朗伯反射、组织形变等，传统CV与直接迁移的自然场景深度特征在细粒度匹配上效果差，需面向手术域的特征与训练策略。

Method: 构建自监督流水线：1）利用新视角合成生成高置信度内点对应作为伪真值；2）据此进行三元组挖掘并开展对比学习；3）在DINOv2主干上添加额外Transformer层，优化其输出嵌入使之可通过余弦相似度阈值直接匹配；4）在SCARED数据集上评测匹配与几何一致性（极线误差）。

Result: 与现有SOTA相比，在SCARED上获得更高的匹配精度与更低的极线误差，显示该特征在内镜图像配准场景中更稳健。

Conclusion: 该框架为手术内镜中的特征对应提供了有效自监督方案，能提升下游高层视觉任务（如3D重建、相机跟踪）的准确性，表明针对手术域适配的表示学习是必要且可行的。

Abstract: Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness. In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation. However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques. While Deep Learning models have shown strong performance in natural scenes, their features are not inherently suited for fine-grained matching in surgical images and require targeted adaptation to meet the demands of this domain. This research presents a novel Deep Learning pipeline for establishing feature correspondences in endoscopic image pairs, alongside a self-supervised optimization framework for model training. The proposed methodology leverages a novel-view synthesis pipeline to generate ground-truth inlier correspondences, subsequently utilized for mining triplets within a contrastive learning paradigm. Through this self-supervised approach, we augment the DINOv2 backbone with an additional Transformer layer, specifically optimized to produce embeddings that facilitate direct matching through cosine similarity thresholding. Experimental evaluation demonstrates that our pipeline surpasses state-of-the-art methodologies on the SCARED datasets improved matching precision and lower epipolar error compared to the related work. The proposed framework constitutes a valuable contribution toward enabling more accurate high-level computer vision applications in surgical endoscopy.

</details>


### [42] [Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies](https://arxiv.org/abs/2512.10384)
*Cong Pang,Hongtao Yu,Zixuan Chen,Lewei Lu,Xin Lou*

Main category: cs.CV

TL;DR: 提出FROW基准与两类数据/训练优化以提升LVLM在细粒度识别上的表现，实验证明对类别与内容准确率有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM评测多偏重推理，忽视对实际应用关键的细粒度识别能力，缺乏系统化基准与对应训练优化。

Method: 1) 构建FROW基准，基于GPT-4o设计细粒度识别评测；2) 数据构造：引入马赛克数据（多短答组合）与开放世界数据（真实问题由GPT-4o生成/整理）；3) 训练流程优化：在预训练阶段纳入细粒度数据并调整训练策略；4) 用FROW进行系统评测。

Result: 马赛克数据使类别识别准确率提升约1%；开放世界数据使FROW基准准确率提升10%-20%、内容准确率提升6%-12%；在预训练中加入细粒度数据可使类别识别准确率最高提升10%。

Conclusion: FROW填补了LVLM细粒度识别评测空白，并通过数据与训练双向优化显著提升LVLM的细粒度识别与开放世界泛化，具备实际应用价值；基准与资源将开源。

Abstract: Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.

</details>


### [43] [Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method](https://arxiv.org/abs/2512.10386)
*Ge Zhang,Chunyang Wang,Bo Xiao,Xuelian Liu,Bin Liu*

Main category: cs.CV

TL;DR: 提出一种自适应双权重“引力”点云去噪方法：先用八叉树并行分块与体素/密度快速粗筛，再用结合密度与距离的引力评分精细判别；在多数据集上同时提升F1、PSNR、CD并降低单帧时延。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR点云含噪多，影响检测重建；去噪方法常在精度、边界保留与实时性三者难以兼顾。需要一种既准确稳健、又能保边且实时的去噪方案。

Method: 1) 用八叉树对全局点云空间划分，实现并行加速；2) 在叶节点内基于自适应体素占据统计与kNN密度估计，快速剔除孤立/低密度噪点以缩小候选集；3) 构建结合密度权与自适应距离权的“引力”评分函数，细粒度区分噪点与目标点。

Result: 在Stanford 3D Scanning、CADC及自采FMCW LiDAR数据上，较现有方法在多种噪声条件下取得更高的F1、PSNR和更低的CD，同时减少单帧处理时间。

Conclusion: 该方法在多噪声场景下兼顾高精度、强边缘保留与实时性，具有鲁棒性与工程实用价值。

Abstract: High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.

</details>


### [44] [MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos](https://arxiv.org/abs/2512.10408)
*Qiyue Sun,Tailin Chen,Yinghui Zhang,Yuchen Zhang,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出MultiHateLoc，弱监督下实现多模态仇恨言论的时间定位，通过模态感知时序编码、动态跨模态融合与对比对齐、以及模态感知MIL目标，在HateMM与MultiHateClip上达SOTA并产生可解释的帧级预测。


<details>
  <summary>Details</summary>
Motivation: 短视频平台多模态仇恨言论增长，仇恨线索往往跨视觉、声学、文本异步且隐蔽；现有研究多停留在视频级分类，忽略“何时发生”的定位任务，尤其在仅有视频级弱标注条件下，传统静态融合或分类式架构难以捕捉跨模态与时序动态。

Method: 提出MultiHateLoc：1) 模态感知时序编码器，含文本特定预处理增强；2) 动态跨模态融合，按时刻自适应强调信息量最大模态，并加入跨模态对比对齐以提升一致性；3) 模态感知的多示例学习（MIL）目标，在视频级监督下挖掘判别性片段，输出帧级（或片段级）预测。

Result: 在HateMM与MultiHateClip数据集上进行定位评测，MultiHateLoc在弱监督设置下实现SOTA，且预测细粒度、可解释。

Conclusion: 仅用粗粒度视频级标签也能通过模态感知时序建模与动态融合实现有效的多模态仇恨定位；方法在基准上达SOTA，并提供帧级可解释性，为实际内容审核提供可操作方案。

Abstract: The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.

</details>


### [45] [Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction](https://arxiv.org/abs/2512.10416)
*Wenfei Guan,Jilin Mei,Tong Shen,Xumin Wu,Shuo Wang,Cheng Min,Yu Hu*

Main category: cs.CV

TL;DR: 提出WildRoad离路网数据集与MaGRoad路径中心方法，解决现有节点中心方法在野外场景易断裂、拓扑错误问题；在WildRoad上达SOTA并具备较好泛化与2.5倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 城市道路矢量提取已有进展，但野外（off-road）环境缺数据且方法结构脆弱，域间差异导致先进模型失效，特别是节点中心范式在遮挡与模糊路口下易出错，因此需要新的数据与更稳健的建模。

Method: 1) 构建WildRoad：全球离路网数据集，借助专用交互式标注工具高效标注路网；2) 提出MaGRoad：基于路径的mask-aware geodesic提取框架，沿候选路径聚合多尺度视觉证据以稳健推断连通性，相比节点中心推理更抗遮挡与歧义；3) 精简推理流水线以提升速度。

Result: 在WildRoad基准上达成SOTA，同时对城市数据具备良好泛化；推理速度约提升2.5倍。

Conclusion: 数据集与路径中心范式共同为“野外道路”制图提供更坚实基础；MaGRoad在准确性、鲁棒性与效率上优于现有节点中心方法，并具有跨域泛化潜力。

Abstract: Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.

</details>


### [46] [TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning](https://arxiv.org/abs/2512.10419)
*Phu Pham,Damon Conover,Aniket Bera*

Main category: cs.CV

TL;DR: 提出TransLocNet，通过跨模态注意力将地面LiDAR与航拍影像融合，实现亚米级、亚度级定位，较SOTA误差降至多63%，在CARLA与KITTI均验证有效。


<details>
  <summary>Details</summary>
Motivation: 地空定位面临视角差异大与模态差异（几何vs语义纹理）显著的问题，现有方法难以在真实与合成场景中稳健泛化，需一种能对齐LiDAR几何与航拍语义的统一框架。

Method: 将地面LiDAR投影为鸟瞰表示，与航拍影像特征通过双向跨模态注意力对齐；随后用似然图解码器输出位置与朝向的空间概率分布；并加入对比学习，构建共享嵌入空间以强化跨模态对齐与判别性。

Result: 在CARLA与KITTI上优于SOTA，定位误差最高降低63%，并达到亚米级、亚度级精度，显示在合成与真实数据上的鲁棒与泛化能力。

Conclusion: TransLocNet通过几何-语义跨模态注意力与概率式解码，实现稳健的地空定位，具有良好泛化性与高精度，适用于多场景实用部署。

Abstract: Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.

</details>


### [47] [Neural Collapse in Test-Time Adaptation](https://arxiv.org/abs/2512.10421)
*Xiao Chen,Zhongjing Du,Jiazhen Huang,Xu Jiang,Li Lu,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: 论文提出NCTTA，通过在测试时自适应中对特征与分类器权重进行样本级对齐，提高OOD鲁棒性；基于新发现的样本级对齐塌陷(NC3+)，用混合目标（几何接近+置信度）缓解伪标签不可靠问题，显著优于Tent（ImageNet-C提升14.52%）。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在域移位下常退化，但缺乏关于退化根因的理论解释。神经塌陷(NC)揭示了训练后DNN的几何结构，但尚未被样本级地系统利用来指导TTA。作者动机：解释TTA退化的根源并据此设计更稳健的适应策略。

Method: 1) 将NC扩展到样本级，提出并实证“样本级对齐塌陷”(NC3+)：单样本特征与对应分类器权重高度对齐。2) 诊断：域移位下TTA过程会引入样本级错位（misalignment），且随分布偏移加大而加剧。3) 提出NCTTA：以“特征-分类器对齐”为核心目标，构造混合目标=几何接近(与权重对齐) + 预测置信度（缓解不可靠伪标签），在推理时对模型参数进行更新。

Result: 在多种域移数据集上取得显著提升，尤其在ImageNet-C上相对Tent提升14.52%，显示NCTTA在强分布偏移下的鲁棒性优势。

Conclusion: 样本级特征与分类器权重的对齐是TTA鲁棒性的关键。退化源于适应时的样本级错位；通过NCTTA的对齐与混合目标设计，可有效缓解伪标签噪声并显著提升OOD表现。

Abstract: Test-Time Adaptation (TTA) enhances model robustness to out-of-distribution (OOD) data by updating the model online during inference, yet existing methods lack theoretical insights into the fundamental causes of performance degradation under domain shifts. Recently, Neural Collapse (NC) has been proposed as an emergent geometric property of deep neural networks (DNNs), providing valuable insights for TTA. In this work, we extend NC to the sample-wise level and discover a novel phenomenon termed Sample-wise Alignment Collapse (NC3+), demonstrating that a sample's feature embedding, obtained by a trained model, aligns closely with the corresponding classifier weight. Building on NC3+, we identify that the performance degradation stems from sample-wise misalignment in adaptation which exacerbates under larger distribution shifts. This indicates the necessity of realigning the feature embeddings with their corresponding classifier weights. However, the misalignment makes pseudo-labels unreliable under domain shifts. To address this challenge, we propose NCTTA, a novel feature-classifier alignment method with hybrid targets to mitigate the impact of unreliable pseudo-labels, which blends geometric proximity with predictive confidence. Extensive experiments demonstrate the effectiveness of NCTTA in enhancing robustness to domain shifts. For example, NCTTA outperforms Tent by 14.52% on ImageNet-C.

</details>


### [48] [An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time](https://arxiv.org/abs/2512.10437)
*Stylianos Kandylakis,Christos Orfanopoulos,Georgios Siolas,Panayiotis Tsanakas*

Main category: cs.CV

TL;DR: 提出一套在移动端实时识别、分类并评估物理治疗动作的高效框架：用姿态估计网络取关键点→转为关节角特征→轻量分类器给出帧级姿态与准确度→基于改进Levenshtein距离的动态规划做序列匹配与误差定位；全端侧运行，实验有效，适用于远程康复与移动健康。


<details>
  <summary>Details</summary>
Motivation: 远程康复与m-health场景需要在资源受限设备上，对患者动作进行实时、可扩展、可靠的识别与质量评估；现有方法要么算力需求高、要么缺乏对完整动作序列的鲁棒匹配与误差定位能力。

Method: 1) 摄像头输入经姿态估计网络得到人体关键点；2) 将关键点转换为基于三角函数的关节角特征；3) 轻量级有监督分类器输出帧级姿态类别与准确度；4) 为识别完整运动与偏差，使用基于改进Levenshtein距离的动态规划进行序列匹配与不准确位置定位；5) 全流程在客户端本地执行以保证实时与可扩展。

Result: 实验表明该方法能有效识别物理治疗动作、评估动作准确性并定位偏差，同时在移动端实现实时性能与良好可扩展性。

Conclusion: 基于角度特征+轻量分类器+改进Levenshtein的端侧框架可在移动设备上鲁棒地完成康复运动识别与误差检测，适合远程物理治疗监督及m-health应用。

Abstract: This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.

</details>


### [49] [Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment](https://arxiv.org/abs/2512.10450)
*Han Li,Shaohui Li,Wenrui Dai,Chenglin Li,Xinlong Pan,Haipeng Wang,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: 提出一个统一变换的学习视频压缩框架，结合像素域粗对齐与潜变量域细对齐（FGDT实现LTMR），并用质量条件化混合专家(QCMoE)进行连续码率控制，实现无误差传播且具竞争性的率失真表现。


<details>
  <summary>Details</summary>
Motivation: 现有学习型视频压缩在运动估计/补偿上两难：分离变换框架R-D好但会误差传播；统一变换框架无误差传播但共享潜空间导致时域对齐不准、ME/MC弱。需要既避免误差传播又提升对齐质量，并支持连续稳定的码率控制。

Method: 设计统一变换框架下的双域逐级时域对齐：先用单参考帧的光流做像素域粗对齐，再在潜变量域上引入多参考的Flow-Guided Deformable Transformer（FGDT）进行长时程运动细化（LTMR）。同时提出质量条件化混合专家（QCMoE），根据目标质量与内容对每像素动态选择专家以调节量化步长，实现连续码率自适应。

Result: 实验显示在率失真上达到与SOTA竞争的性能，并且在流式编码中成功消除误差传播，提供质量一致性与连续码率控制能力。

Conclusion: 通过双域渐进对齐与QCMoE，统一变换视频压缩可同时获得高质量时域建模、无误差传播与灵活稳定的码率控制，为学习式视频压缩提供更优的流式传输方案。

Abstract: Existing frameworks for learned video compression suffer from a dilemma between inaccurate temporal alignment and error propagation for motion estimation and compensation (ME/MC). The separate-transform framework employs distinct transforms for intra-frame and inter-frame compression to yield impressive rate-distortion (R-D) performance but causes evident error propagation, while the unified-transform framework eliminates error propagation via shared transforms but is inferior in ME/MC in shared latent domains. To address this limitation, in this paper, we propose a novel unifiedtransform framework with dual-domain progressive temporal alignment and quality-conditioned mixture-of-expert (QCMoE) to enable quality-consistent and error-propagation-free streaming for learned video compression. Specifically, we propose dualdomain progressive temporal alignment for ME/MC that leverages coarse pixel-domain alignment and refined latent-domain alignment to significantly enhance temporal context modeling in a coarse-to-fine fashion. The coarse pixel-domain alignment efficiently handles simple motion patterns with optical flow estimated from a single reference frame, while the refined latent-domain alignment develops a Flow-Guided Deformable Transformer (FGDT) over latents from multiple reference frames to achieve long-term motion refinement (LTMR) for complex motion patterns. Furthermore, we design a QCMoE module for continuous bit-rate adaptation that dynamically assigns different experts to adjust quantization steps per pixel based on target quality and content rather than relies on a single quantization step. QCMoE allows continuous and consistent rate control with appealing R-D performance. Experimental results show that the proposed method achieves competitive R-D performance compared with the state-of-the-arts, while successfully eliminating error propagation.

</details>


### [50] [Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network](https://arxiv.org/abs/2512.10498)
*Khurram Ashfaq,Muhammad Tariq Mahmood*

Main category: cs.CV

TL;DR: 提出一种混合式SFF框架：用手工DDL核构建多尺度焦聚体，再用轻量多尺度GRU迭代提深度并通过学习型凸上采样恢复高分辨率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习SFF两阶段流程中，特征编码器计算的焦聚体重、代价高，且后续一步式聚合易产生伪影并放大噪声、泛化性不足。需要一种既稳健又高效、能保留细节且减少噪声伪影的方法。

Method: 1) 传统方式以手工设计的方向性膨胀拉普拉斯(DDL)核在多尺度上计算焦聚体，捕获长程与方向性的焦点变化；2) 设计轻量多尺度GRU深度提取模块，从低分辨率初始深度出发迭代细化，提高计算效率与鲁棒性；3) 在同一递归网络内加入学习型凸上采样模块，将低分辨率深度重建为高分辨率，保持细节与边界锐利。

Result: 在合成与真实数据集上进行大量实验，方法在准确率与泛化性上均优于最先进的深度学习与传统SFF方法，在多种焦距条件下表现稳健。

Conclusion: 混合式“手工焦聚体+轻量递归深度推断+学习型凸上采样”框架有效缓解一步聚合带来的伪影与噪声问题，实现高精度、强泛化且边界友好的SFF深度估计。

Abstract: Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.

</details>


### [51] [3D Blood Pulsation Maps](https://arxiv.org/abs/2512.10517)
*Maurice Rohr,Tobias Reinhardt,Tizian Dege,Justus Thies,Christoph Hoog Antink*

Main category: cs.CV

TL;DR: Pulse3DFace 是首个用于估计三维血液搏动图的公开数据集，包含多视角RGB视频、参考脉搏、单目SfM生成的3D人脸以及与FLAME纹理空间对齐的三维搏动图（含SNR、振幅、相位等），用于支持合成数据生成与多视角抗光照研究，并验证其生理意义与一致性。


<details>
  <summary>Details</summary>
Motivation: 远程脉搏（rPPG）估计在真实复杂光照与视角变化下鲁棒性不足、缺少标准化三维搏动标注和可用于生成合成视频的数据基准，限制了方法评估与泛化。需要一个多视角、带参考脉搏与三维对齐的面部/颈部搏动数据集来推动模型、合成与抗光照研究。

Method: 采集15名受试者的23视角、30Hz RGB原始视频与脉搏参考；使用单目SfM与FLAME建立3D头部模型与纹理空间；从视频中估计并投射三维搏动图，提供SNR、局部振幅、相位及辅助数据；系统评估光照条件、三维映射一致性以及对面部/颈部生理特征的捕获能力。

Result: 得到与FLAME纹理空间对齐的三维搏动图数据集，含多视角视频、参考信号与质量度量；实验表明数据在不同光照下具有可分析性，三维图在空间与视角间保持一致，并能反映生理上有意义的面部与颈部搏动特征。

Conclusion: Pulse3DFace为三维血液搏动研究提供首个系统性数据资源，可用于合成视频生成、评估与提升rPPG方法，尤其支持多视角与抗光照研究；其标注与质量评估显示数据具有生理有效性与跨视角一致性。

Abstract: We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps. These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging. Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis. Pulse3DFace consists of raw videos from 15 subjects recorded at 30 Hz with an RGB camera from 23 viewpoints, blood pulse reference measurements, and facial 3D scans generated using monocular structure-from-motion techniques. It also includes processed 3D pulsation maps compatible with the texture space of the 3D head model FLAME. These maps provide signal-to-noise ratio, local pulse amplitude, phase information, and supplementary data. We offer a comprehensive evaluation of the dataset's illumination conditions, map consistency, and its ability to capture physiologically meaningful features in the facial and neck skin regions.

</details>


### [52] [Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA](https://arxiv.org/abs/2512.10521)
*Pasquale De Marinis,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: TaP 用 LoRA 在支持集上快速微调编码器，显著提升（跨域）小样本语义分割性能，低秩高效、模型无关、易集成。


<details>
  <summary>Details</summary>
Motivation: FSS 的主要瓶颈在于编码器对未见类别的表征不足，既往工作多优化解码器而忽视编码器适应性；在跨域设置下该问题更严重，需要一种既能快速适配又避免灾难性遗忘、计算开销低的方法。

Method: 提出 Take a Peek（TaP）：在推理/训练时对支撑集进行低秩适配（LoRA）以微调编码器，冻结主干参数，仅学习低秩插入的适配层，从而以极低参数量实现对新类的快速适配；方法与现有 FSS 流水线解耦、可即插即用；并进行秩敏感性分析以平衡性能与计算。

Result: 在 COCO 20^i、Pascal 5^i 及跨域数据集（DeepGlobe、ISIC、Chest X-ray）上，对多种模型与不同 shot 数均有稳定提升，尤其在多类复杂场景中增益显著；低秩配置即可获得强性能。

Conclusion: 通过对编码器进行低秩自适应微调，TaP 有效提升了 FSS 与 CD-FSS 的泛化与效率，为更稳健、可拓展的分割系统提供了简洁实用的路径。

Abstract: Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder's limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder's generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.

</details>


### [53] [Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding](https://arxiv.org/abs/2512.10548)
*Yuchen Feng,Zhenyu Zhang,Naibin Gu,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CV

TL;DR: 提出“Blink”框架：在MLLM前向一次中，基于层间注意力进行显著性引导扫描与动态分辨率分配，对显著视觉token做超分辨扩展并在失焦后丢弃，以更高效地提升视觉感知与多模态理解。


<details>
  <summary>Details</summary>
Motivation: MLLM在多模态任务上进步显著，但视觉感知仍受限；人类通过“眨眼式”顺序扫描聚焦显著区域高效理解场景。作者希望检验MLLM是否具备类似层间区域转移关注的行为，并利用这一特性改进模型在不显著增加计算的情况下的视觉感知。

Method: 1) 先做探索性分析：观察MLLM不同层对不同视觉区域的自然关注变化，并测试对显著token分配更多计算是否有益。2) 提出Blink框架：单次前向中的动态视觉token分辨率机制，包括两部分：a) 显著性引导扫描：利用当前层的注意力图估计每个视觉token的显著性；b) 动态token分辨率：通过可插拔的TokenSR模块对高显著token进行超分辨扩展（细粒度化），在后续层若这些扩展token显著性下降则将其丢弃。整体在层间循环，实现“广泛探索+精细聚焦”的权衡。

Result: 广泛实验显示：Blink能提升视觉感知与多模态理解表现（较基线更好），且计算开销自适应、效率较高。

Conclusion: MLLM天然存在层间关注区域迁移；在此基础上，Blink通过显著性驱动的动态token分辨率在单次前向中有效模拟“眨眼式”扫描，兼顾效率与精度，显著增强多模态视觉理解能力。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential "blink-like" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.

</details>


### [54] [Grounding Everything in Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2512.10554)
*Xiangxuan Ren,Zhongdao Wang,Liping Hou,Pin Tang,Guoqing Wang,Chao Ma*

Main category: cs.CV

TL;DR: 提出GETok：通过可学习的空间词汇（网格token+偏移token）让MLLM在不改自回归架构下更好完成2D定位与推理，SFT与RL场景的指代任务均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM需将图像离散为语言token，导致在原生2D空间中难以精确定位与落地（grounding）。需要一种兼容自回归流程、又能提升2D空间对齐与推理能力的表示方式。

Method: 设计空间表示GETok：1) 网格token将图像平面划分为结构化空间锚点；2) 偏移token对初始位置进行精细、可迭代的局部化修正；把空间关系直接编码进token序列，无需改动自回归Transformer。

Result: 在多种指代定位相关任务上，GETok在监督微调与强化学习两种训练范式下均显著超越现有SOTA，体现更强的2D空间推理与定位能力。

Conclusion: 通过将空间关系融入可学习token，MLLM可在保持架构不变的前提下实现更精确的2D grounding与推理，验证了基于token的空间表达对视觉-语言模型的有效性。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.

</details>


### [55] [Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks](https://arxiv.org/abs/2512.10562)
*Meher Md Saad*

Main category: cs.CV

TL;DR: 提出用于骨架序列的少样本原型网络来做孤立手语识别，通过度量学习与原型分类替代固定分类器，结合ST-GCN与多尺度时间聚合模块，在WLASL上显著优于相同骨干的标准分类基线，并在跨数据集上具备零样本泛化。


<details>
  <summary>Details</summary>
Motivation: ISLR受制于数据稀缺与长尾类别分布，标准分类易过拟合高频类、难以泛化稀有类；采集成千上万手语样本成本高，需要一种能在少样本条件下鲁棒泛化的学习范式。

Method: 采用少样本原型网络+度量学习：在训练中以episode方式构建支持集与查询集，学习语义度量空间，基于与类原型的距离进行分类。编码器使用骨架驱动的ST-GCN，并引入多尺度时间聚合(MSTA)以同时捕获快速与平滑动作动态。

Result: 在WLASL测试集上取得Top-1 43.75%、Top-5 77.10%，相比相同骨干的标准分类基线Top-1提升>13%。跨数据集零样本迁移到SignASL（未微调）约30%准确率。

Conclusion: 原型度量学习在数据稀缺和长尾场景下比标准分类更有效；结合ST-GCN与MSTA能更好建模时空骨架动态，且具备有意义的跨数据集零样本泛化，指向可扩展的手语大词表识别路径。

Abstract: Isolated Sign Language Recognition (ISLR) is critical for bridging the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. However, robust ISLR is fundamentally constrained by data scarcity and the long-tail distribution of sign vocabulary, where gathering sufficient examples for thousands of unique signs is prohibitively expensive. Standard classification approaches struggle under these conditions, often overfitting to frequent classes while failing to generalize to rare ones. To address this bottleneck, we propose a Few-Shot Prototypical Network framework adapted for a skeleton based encoder. Unlike traditional classifiers that learn fixed decision boundaries, our approach utilizes episodic training to learn a semantic metric space where signs are classified based on their proximity to dynamic class prototypes. We integrate a Spatiotemporal Graph Convolutional Network (ST-GCN) with a novel Multi-Scale Temporal Aggregation (MSTA) module to capture both rapid and fluid motion dynamics. Experimental results on the WLASL dataset demonstrate the superiority of this metric learning paradigm: our model achieves 43.75% Top-1 and 77.10% Top-5 accuracy on the test set. Crucially, this outperforms a standard classification baseline sharing the identical backbone architecture by over 13%, proving that the prototypical training strategy effectively outperforms in a data scarce situation where standard classification fails. Furthermore, the model exhibits strong zero-shot generalization, achieving nearly 30% accuracy on the unseen SignASL dataset without fine-tuning, offering a scalable pathway for recognizing extensive sign vocabularies with limited data.

</details>


### [56] [Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner](https://arxiv.org/abs/2512.10571)
*Haojie Zheng,Shuchen Weng,Jingqi Liu,Siqi Yang,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: 提出AVI-Edit：聚焦音频同步的视频实例级编辑框架，通过精细化掩码与自反馈音频代理，实现时空上可控、与音频高度对齐的编辑；并发布大规模实例对应数据集，实验表明在视觉质量、条件遵循与音视同步上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑忽视音画同步，且缺乏细粒度的空间（实例级）与时间控制，难以进行精确的对象级编辑，影响生成内容的真实感与可用性。

Method: 1) 粒度感知的掩码优化器：在用户提供的粗掩码基础上迭代细化，定位到精确的实例级区域；2) 自反馈音频代理：从音频中自监督/自回路地提炼高质量时序引导，实现细粒度时间控制；3) 构建大规模、含实例对应与全面标注的数据集，支撑训练与评估。

Result: 在多项实验中，AVI-Edit在视觉质量、条件遵循度以及音画同步指标上均优于现有最先进方法；定性与定量评估均显示更好的实例级编辑准确度与时序对齐。

Conclusion: 通过联合精细空间掩码与高质量音频时序引导，AVI-Edit实现了音画同步的实例级视频编辑，并以新数据集与实验验证其有效性，为更可控、更真实的视频编辑提供了新范式。

Abstract: Recent advancements in video generation highlight that realistic audio-visual synchronization is crucial for engaging content creation. However, existing video editing methods largely overlook audio-visual synchronization and lack the fine-grained spatial and temporal controllability required for precise instance-level edits. In this paper, we propose AVI-Edit, a framework for audio-sync video instance editing. We propose a granularity-aware mask refiner that iteratively refines coarse user-provided masks into precise instance-level regions. We further design a self-feedback audio agent to curate high-quality audio guidance, providing fine-grained temporal control. To facilitate this task, we additionally construct a large-scale dataset with instance-centric correspondence and comprehensive annotations. Extensive experiments demonstrate that AVI-Edit outperforms state-of-the-art methods in visual quality, condition following, and audio-visual synchronization. Project page: https://hjzheng.net/projects/AVI-Edit/.

</details>


### [57] [Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration](https://arxiv.org/abs/2512.10581)
*Wenlong Jiao,Heyang Lee,Ping Wang,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 提出SymUNet：依靠对称U-Net与精心设计的特征对齐/传播，在无需MoE、扩散或复杂提示的情况下实现全能图像复原SOTA；并通过SE-SymUNet用冻结CLIP语义特征进一步增强。


<details>
  <summary>Details</summary>
Motivation: 当前“一体化图像复原”常依赖复杂专家混合、扩散模型与退化提示，带来高算力与实现复杂度。作者观察到：高质量的特征提取本身就蕴含退化信息，如果结构设计能充分保留与传播这些线索，或可用更简单的网络取得更佳效果。

Method: 1) 对称U-Net（SymUNet）：在编码器-解码器多尺度上严格对齐通道与分辨率，简化跨尺度信息传递；跳连采用简单的加法融合。2) 设计跨尺度流与尺度对齐，确保退化相关特征稳健保真地在网络中传播。3) 语义增强变体（SE-SymUNet）：引入冻结CLIP特征，通过简洁的交叉注意力进行“直接语义注入”，以显式放大退化先验。

Result: 在多项基准数据集上优于现有SOTA，同时计算成本更低；SE-SymUNet在此基础上进一步提升。代码开源。

Conclusion: 只要保证特征尺度对齐与高效跨尺度传播，对称U-Net配合简单跳连即可充分利用隐含的退化信息，建立简单而强大的全能图像复原基线；加入CLIP语义能进一步增强退化先验并带来额外收益。

Abstract: All-in-one image restoration aims to handle diverse degradations (e.g., noise, blur, adverse weather) within a unified framework, yet existing methods increasingly rely on complex architectures (e.g., Mixture-of-Experts, diffusion models) and elaborate degradation prompt strategies. In this work, we reveal a critical insight: well-crafted feature extraction inherently encodes degradation-carrying information, and a symmetric U-Net architecture is sufficient to unleash these cues effectively. By aligning feature scales across encoder-decoder and enabling streamlined cross-scale propagation, our symmetric design preserves intrinsic degradation signals robustly, rendering simple additive fusion in skip connections sufficient for state-of-the-art performance. Our primary baseline, SymUNet, is built on this symmetric U-Net and achieves better results across benchmark datasets than existing approaches while reducing computational cost. We further propose a semantic enhanced variant, SE-SymUNet, which integrates direct semantic injection from frozen CLIP features via simple cross-attention to explicitly amplify degradation priors. Extensive experiments on several benchmarks validate the superiority of our methods. Both baselines SymUNet and SE-SymUNet establish simpler and stronger foundations for future advancements in all-in-one image restoration. The source code is available at https://github.com/WenlongJiao/SymUNet.

</details>


### [58] [Salient Object Detection in Complex Weather Conditions via Noise Indicators](https://arxiv.org/abs/2512.10592)
*Quan Chen,Xiaokai Yang,Tingyu Wang,Rongfeng Lu,Xichun Sheng,Yaoqi Sun,Chenggang Yan*

Main category: cs.CV

TL;DR: 提出一种针对多种天气噪声的显著性目标检测框架：在编码器阶段引入“噪声指示器”和NIFM进行自适应特征调制，并与主流解码器即插即用，显著提升恶劣天气下分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有SOD多假设低噪声环境，忽视雨雪雾等天气噪声导致的分割退化；需要一个能感知并适应多种天气条件、且对主流解码器友好的方法。

Method: 设计一个“特定编码器+可替换解码器”的框架：用一维one-hot噪声指示向量标识天气类型；在编码器相邻阶段间插入噪声指示融合模块NIFM，以语义特征与噪声指示为双输入，通过自适应特征调制（如通道/空间重标定）注入天气先验；保持与多种SOD解码器兼容。

Result: 在WXSOD数据集上，跨不同训练规模（100%/50%/30%）、3种编码器与7种解码器配置进行实验；相较于普通编码器，加入NIFM的特定编码器在复杂天气下显著提高分割精度，表现稳健且普适。

Conclusion: 在编码阶段显式引入天气噪声先验（通过噪声指示器+NIFM）能稳健提升多天气场景的SOD性能；该特定编码器与主流解码器兼容，具备通用性与实际部署价值。

Abstract: Salient object detection (SOD), a foundational task in computer vision, has advanced from single-modal to multi-modal paradigms to enhance generalization. However, most existing SOD methods assume low-noise visual conditions, overlooking the degradation of segmentation accuracy caused by weather-induced noise in real-world scenarios. In this paper, we propose a SOD framework tailored for diverse weather conditions, encompassing a specific encoder and a replaceable decoder. To enable handling of varying weather noises, we introduce a one-hot vector as a noise indicator to represent different weather types and design a Noise Indicator Fusion Module (NIFM). The NIFM takes both semantic features and the noise indicator as dual inputs and is inserted between consecutive stages of the encoder to embed weather-aware priors via adaptive feature modulation. Critically, the proposed specific encoder retains compatibility with mainstream SOD decoders. Extensive experiments are conducted on the WXSOD dataset under varying training data scales (100%, 50%, 30% of the full training set), three encoder and seven decoder configurations. Results show that the proposed SOD framework (particularly the NIFM-enhanced specific encoder) improves segmentation accuracy under complex weather conditions compared to a vanilla encoder.

</details>


### [59] [Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.10596)
*J. Xiao,Y. Guo,X. Zi,K. Thiyagarajan,C. Moreira,M. Prasad*

Main category: cs.CV

TL;DR: 提出RSRT数据集与无训练文本检索方法TRSLLaVA，把跨模态检索改为T2T匹配，在RSITMD/RSICD上零样本表现接近/超越监督模型。


<details>
  <summary>Details</summary>
Motivation: 遥感图像语义检索受“语义鸿沟”困扰；现有基于大规模视觉语言模型的方法依赖昂贵的领域训练，且缺乏用于评估零样本文本生成实用性的基准。

Method: 构建包含每图多条结构化描述的新基准RSRT；将跨模态检索重构为纯文本的T2T匹配：用丰富查询文本在统一文本嵌入空间中与VLM自动生成的图像字幕库进行比对；完全无需训练或微调。

Result: 在RSITMD与RSICD上，训练自由的方法与SOTA监督模型竞争：例如在RSITMD上mean Recall达42.62%，显著高于零样本CLIP的23.86%，并超过若干顶级监督方法。

Conclusion: 高质量、结构化文本可以有效弥补语义鸿沟，文本到文本的检索范式在遥感检索中具有强竞争力与高性价比，无需额外训练即可取得优异效果。

Abstract: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.

</details>


### [60] [Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos](https://arxiv.org/abs/2512.10607)
*Bishoy Galoaa,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: TCAM提出一种以运动为中心、无需查询的视频理解框架，利用运动场注意力将语言描述与轨迹对齐，通过联合全局对齐与细粒度空间对应，自动发现并定位多种运动，并在MeViS上取得强泛化与跨任务表现。


<details>
  <summary>Details</summary>
Motivation: 在遮挡、伪装、快速运动等困难场景中，外观线索不可靠，运动动态更关键；现有方法多依赖用户查询或外观特征，难以自动发现与描述多种动作，缺乏统一的语义-运动对齐能力。

Method: 提出Track and Caption Any Motion (TCAM)：1) 无查询自动观察视频并发现多种运动活动；2) 通过“运动场注意力”在时空上将自然语言描述与对应轨迹进行空间绑定；3) 利用对比式视觉-语言表示，将运动模式与语义对齐；4) 统一训练同时进行视频-文本全局对齐与细粒度空间对应；5) 采用多头交叉注意力，实现多运动表达的并行发现与描述。

Result: 在MeViS基准上，视频到文本检索58.4%，空间定位JF 64.9；平均每段视频发现4.8条相关表达，精度84.7%，显示出较强的跨任务泛化能力。

Conclusion: 将运动模式与对比式视觉-语言表示结合，并通过运动场注意力进行空间绑定，可在无需查询的前提下自动发现、描述并定位多种动作，实现跨检索与定位任务的强泛化。

Abstract: We propose Track and Caption Any Motion (TCAM), a motion-centric framework for automatic video understanding that discovers and describes motion patterns without user queries. Understanding videos in challenging conditions like occlusion, camouflage, or rapid movement often depends more on motion dynamics than static appearance. TCAM autonomously observes a video, identifies multiple motion activities, and spatially grounds each natural language description to its corresponding trajectory through a motion-field attention mechanism. Our key insight is that motion patterns, when aligned with contrastive vision-language representations, provide powerful semantic signals for recognizing and describing actions. Through unified training that combines global video-text alignment with fine-grained spatial correspondence, TCAM enables query-free discovery of multiple motion expressions via multi-head cross-attention. On the MeViS benchmark, TCAM achieves 58.4% video-to-text retrieval, 64.9 JF for spatial grounding, and discovers 4.8 relevant expressions per video with 84.7% precision, demonstrating strong cross-task generalization.

</details>


### [61] [Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation](https://arxiv.org/abs/2512.10608)
*Mohammad Sadegh Gholizadeh,Amir Arsalan Rezapour*

Main category: cs.CV

TL;DR: 研究提出将深度特征提取与可解释图像处理结合，用视网膜血管分割辅助分类，以提升眼病自动筛查的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 眼底致盲性疾病发病率上升，临床需要可扩展且准确的自动筛查；传统CNN“黑箱”难以获得医生信任、易产生误报，影响落地。

Method: 构建一条包含深度学习特征提取与可解释图像处理模块的流水线：以高保真视网膜血管分割作为辅助任务，利用血管形态等临床相关特征引导疾病分类；通过多任务/注意力或特征对齐，将分割结果与分类网络融合以提升可解释性。

Result: 与仅用标准CNN相比，模型在降低误报（false positives）方面更优，并能以血管形态等可视证据解释预测，提高临床可用性；实现可扩展筛查的潜力。

Conclusion: 通过将可解释的血管分割融入分类流程，模型在准确性与可解释性上取得平衡，缓解“黑箱”问题并提升临床部署可行性。

Abstract: In recent years, the incidence of vision-threatening eye diseases has risen dramatically, necessitating scalable and accurate screening solutions. This paper presents a comprehensive study on deep learning architectures for the automated diagnosis of ocular conditions. To mitigate the "black-box" limitations of standard convolutional neural networks (CNNs), we implement a pipeline that combines deep feature extraction with interpretable image processing modules. Specifically, we focus on high-fidelity retinal vessel segmentation as an auxiliary task to guide the classification process. By grounding the model's predictions in clinically relevant morphological features, we aim to bridge the gap between algorithmic output and expert medical validation, thereby reducing false positives and improving deployment viability in clinical settings.

</details>


### [62] [Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces](https://arxiv.org/abs/2512.10617)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: Lang2Motion提出一种将轨迹运动流形与CLIP联合嵌入对齐的语言引导点轨迹生成框架，从真实视频点跟踪提取任意物体运动，采用Transformer自编码器并用文本和轨迹渲染双重监督学习表示；在文本到轨迹检索与运动精度上显著优于视频生成基线，并展现跨领域迁移与可编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于人体动作或视频合成，难以直接生成可用于下游任务的显式物体轨迹，且对任意对象运动的语言控制与可编辑性不足。作者希望构建一个可泛化到多对象的、由自然语言驱动、且具备良好检索与生成质量的轨迹生成框架，并与通用多模态语义空间对齐以获得迁移与编辑能力。

Method: 从真实世界视频通过点跟踪提取对象运动轨迹；设计基于Transformer的自编码器学习轨迹潜表示；采用双重监督：将文本描述与轨迹可视化渲染分别送入冻结的CLIP文本与图像编码器，使轨迹潜表示与CLIP联合嵌入空间对齐；支持文本到轨迹检索、生成与潜空间操作（风格迁移、语义插值、编辑）。

Result: 在文本到轨迹检索上Recall@1=34.2%，比视频方法高12.5个百分点；运动精度（ADE）12.4，相比视频生成基线18.3–25.3提升33–52%；仅在多样物体运动上训练即可在人类动作识别上Top-1达到88.3%，显示跨运动域有效迁移；展示风格迁移、语义插值与潜空间编辑能力。

Conclusion: 通过将轨迹表示对齐到CLIP嵌入，Lang2Motion实现语言可控的任意对象点轨迹生成与检索，显著优于视频基线并具备强迁移与可编辑性，表明对齐至通用多模态语义空间是实现通用运动表示与跨任务应用的有效途径。

Abstract: We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.

</details>


### [63] [DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM](https://arxiv.org/abs/2512.10619)
*Qintong Zhang,Junyuan Zhang,Zhifei Ren,Linke Ouyang,Zichen Wen,Junbo Niu,Yuan Qu,Bin Wang,Ka-Ho Chow,Conghui He,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出DOCR-Inspector：用VLM充当“裁判”，对文档解析输出进行细粒度错误检测与质量评估，含28类错误、链式清单推理、配套200K训练集与882案例基准，7B模型优于商用与开源基线，并能指导结果修复。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析评测依赖通用基准的整体分数，存在数据集偏差、与真实场景相关性弱、无法揭示细粒度错误模式的问题，难以可靠评估并改进解析系统。

Method: 将评测形式化为细粒度错误检测：给定文档图像与解析结果，VLM-as-a-Judge识别并标注错误并归类到28种类型，输出综合质量报告；构建DOCRcase-200K训练集；提出Chain-of-Checklist分层推理范式以覆盖结构化检查清单；并发布DOCRcaseBench用于实证验证。

Result: 在DOCRcaseBench（882真实案例、人工标注）上，DOCR-Inspector-7B超过Gemini 2.5 Pro等商用与主流开源模型；其评估还能用于引导解析结果修复，提升下游质量。

Conclusion: 细粒度、可解释的评估框架能更可靠地反映真实场景的解析质量并推动系统改进；公开模型与代码具实用价值并可作为业界规模化迭代的驱动器。

Abstract: Document parsing aims to transform unstructured PDF images into semi-structured data, facilitating the digitization and utilization of information in diverse domains. While vision language models (VLMs) have significantly advanced this task, achieving reliable, high-quality parsing in real-world scenarios remains challenging. Common practice often selects the top-performing model on standard benchmarks. However, these benchmarks may carry dataset-specific biases, leading to inconsistent model rankings and limited correlation with real-world performance. Moreover, benchmark metrics typically provide only overall scores, which can obscure distinct error patterns in output. This raises a key challenge: how can we reliably and comprehensively assess document parsing quality in the wild? We address this problem with DOCR-Inspector, which formalizes document parsing assessment as fine-grained error detection and analysis. Leveraging VLM-as-a-Judge, DOCR-Inspector analyzes a document image and its parsed output, identifies all errors, assigns them to one of 28 predefined types, and produces a comprehensive quality assessment. To enable this capability, we construct DOCRcase-200K for training and propose the Chain-of-Checklist reasoning paradigm to enable the hierarchical structure of parsing quality assessment. For empirical validation, we introduce DOCRcaseBench, a set of 882 real-world document parsing cases with manual annotations. On this benchmark, DOCR-Inspector-7B outperforms commercial models like Gemini 2.5 Pro, as well as leading open-source models. Further experiments demonstrate that its quality assessments provide valuable guidance for parsing results refinement, making DOCR-Inspector both a practical evaluator and a driver for advancing document parsing systems at scale. Model and code are released at: https://github.com/ZZZZZQT/DOCR-Inspector.

</details>


### [64] [K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices](https://arxiv.org/abs/2512.10628)
*Bishoy Galoaa,Pau Closas,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: K-Track通过在关键帧用深度模型、在中间帧用卡尔曼滤波预测，实现5–10倍加速，同时保留原追踪器85%以上精度，使点跟踪在资源受限设备上可实时运行。


<details>
  <summary>Details</summary>
Motivation: 现有最优深度点追踪需要逐帧GPU推理，算力/能耗/带宽消耗大，难在边缘设备实时部署，迫切需要在不大幅牺牲精度的前提下降低推理开销。

Method: 提出通用、与追踪器无关的加速框架K-Track：对稀疏关键帧进行深度网络推理获取高置信度观测；对关键帧之间的中间帧，采用轻量级卡尔曼滤波进行状态预测与更新，并使用贝叶斯不确定性传播维持时间一致性。该混合策略在不同SOTA点追踪器上可直接套用。

Result: 在多种SOTA点追踪器上评估，K-Track相对原方法实现5–10倍速度提升，同时保留超过85%的原始追踪精度；在NVIDIA Jetson Nano与RTX Titan等边缘平台上达到实时性能。

Conclusion: K-Track在显著降低计算需求的同时基本保持精度，为资源受限场景下的高质量点追踪提供可部署路径，缩小先进算法与实用系统之间的落差。

Abstract: Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.

</details>


### [65] [TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection](https://arxiv.org/abs/2512.10652)
*Jian-Yu Jiang-Lin,Kang-Yang Huang,Ling Zou,Ling Lo,Sheng-Ping Yang,Yu-Wen Tseng,Kun-Hsiang Lin,Chia-Ling Chen,Yu-Ting Ta,Yan-Tsung Wang,Po-Ching Chen,Hongxia Xie,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: TriDF 是一个面向可解释 DeepFake 检测的多模态基准，覆盖图像/视频/音频的16类伪造，评估“感知-检测-幻觉”三要素的联动，并用以推动可信的合成媒体识别系统。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型易制造以人为中心的逼真伪造，威胁安全与信任。检测系统不仅要分辨真伪，还需提供明确、可靠的证据与解释；现有评测缺少同时覆盖多模态、多类型伪造、并考察证据感知与解释可靠性的统一框架。

Method: 构建TriDF基准：收集来自先进合成模型的高质量伪造，涵盖图像、视频、音频16种DeepFake类型；设计三维评估指标——感知（基于人工标注证据衡量对细粒度伪造痕迹的识别）、检测（跨伪造家族与生成器的分类表现）、幻觉（度量模型解释的可靠性）。用多模态大模型开展系统实验。

Result: 实验证明：准确的证据感知对可靠检测至关重要；但解释幻觉会显著干扰决策，三者相互依赖。SOTA多模态大模型在TriDF上暴露出在证据定位与解释稳定性方面的不足。

Conclusion: TriDF为理解检测准确性、证据识别与解释可靠性的相互作用提供统一框架，为构建应对真实世界合成媒体威胁的可信检测系统奠定基础。

Abstract: Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.

</details>


### [66] [NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation](https://arxiv.org/abs/2512.10660)
*Hanfeng Wu,Marlon Steiner,Michael Schmidt,Alvaro Marcos-Ramiro,Christoph Stiller*

Main category: cs.CV

TL;DR: NaviHydra 是一个可控、可遵循高层导航指令的端到端自动驾驶模型，由规则基模拟器蒸馏而来，借助BEV轨迹聚合与新提出的“导航服从度”指标，在NAVSIM基准上达到了SOTA并显著提升可控性与安全性。


<details>
  <summary>Details</summary>
Motivation: 传统规则系统在动态场景中脆弱，端到端方法又难以严格遵循显式导航指令，导致可控性与安全性不足；需要一种既能理解高层指令又能生成符合意图且安全的轨迹的方案。

Method: 提出 NaviHydra：以高层导航命令作为控制信号的端到端模型，从规则基模拟器进行知识蒸馏；采用基于鸟瞰图（BEV）的轨迹采集/聚合以强化轨迹特征提取；提出新的导航服从度评估指标；设计覆盖多种导航指令的可控性测试方案。

Result: 在 NAVSIM 基准上显著优于基线并达成 SOTA；在多种导航命令下表现出更好的可控性，对预期路线的遵循度更高，提升安全性。

Conclusion: 将可控导航信号融入端到端框架，并结合BEV轨迹特征与导航服从度评估，可有效提升自动驾驶在动态环境中的指令遵循与安全性；NaviHydra 为可控端到端自动驾驶提供了有力路径。

Abstract: The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.

</details>


### [67] [XDen-1K: A Density Field Dataset of Real-World Objects](https://arxiv.org/abs/2512.10668)
*Jingxuan Zhang,Tianqi Yu,Yatu Zhang,Jinze Wu,Kaixin Yao,Jingyang Liu,Yuyao Zhang,Jiayuan Gu,Jingyi Yu*

Main category: cs.CV

TL;DR: 提出XDen-1K数据集与从稀疏双平面X射线重建体密度场的方法，显著提升质心估计、体分割与机器人抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉/三维模型只关注外形与外观，忽略内部物理属性（如体密度），而这些属性决定质心、稳定性与交互动力学。缺乏大规模真实数据是瓶颈，阻碍了具身智能与物理仿真的发展。

Method: 构建XDen-1K：包含1000个真实物体、148类，多模态数据（高分辨率3D几何与部件标注、双平面X射线）。在此基础上提出优化框架：从稀疏X射线视角反演高保真体密度场；并将X射线作为条件信号接入现有分割网络实现体积分割；在下游机器人任务中评估。

Result: 用XDen-1K训练/条件化后，质心估计更准确、体分割更好，机器人操作（抓取/操控）成功率提升。

Conclusion: XDen-1K为真实世界物理属性估计提供首个大规模多模态基准与密度重建方法，可作为具身智能与物理感知研究的基础资源与挑战性基准。

Abstract: A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.

</details>


### [68] [Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching](https://arxiv.org/abs/2512.10674)
*Javier Villena Toro,Mehdi Tarkian*

Main category: cs.CV

TL;DR: 提出Geo6DPose：在本地、无需训练的零样本6D位姿估计管线，以几何约束替代大模型规模，实现子秒级推理并与更大基线持平的召回率。


<details>
  <summary>Details</summary>
Motivation: 现有零样本6D位姿多依赖大模型与云端推理，带来高时延、高能耗与联网/合规风险，不适合计算受限、需本地推理的机器人场景。

Method: 以“特征+几何过滤”为核心：1) 采用基础模型（如DINO）抽取模板与场景特征；2) 计算模板与场景patch的相似度图；3) 将场景patch中心投到3D，将模板描述子映射到对象模型坐标系，建立双向互为匹配的对应点；4) 用对应点驱动RANSAC恢复候选位姿；5) 以兼顾重投影一致性与空间支持的加权几何对齐指标对候选位姿排序。全流程轻量、在设备端运行，无需训练/微调/联网。

Result: 在单张商品级GPU上实现约1.08 FPS、平均召回53.7，与显著更大的零样本基线相当，同时更鲁棒于噪声、遮挡与局部可见性。

Conclusion: Geo6DPose以几何可靠性替代模型规模，提供轻量、全本地、零训练的6D位姿估计方案，兼容不断演进的基础特征骨干，提升机器人落地的可行性。

Abstract: Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.

</details>


### [69] [Optimal transport unlocks end-to-end learning for single-molecule localization](https://arxiv.org/abs/2512.10683)
*Romain Seailles,Jean-Baptiste Masson,Jean Ponce,Julien Mairal*

Main category: cs.CV

TL;DR: 提出一种基于最优传输损失和迭代神经网络的SMLM定位方法，摒弃NMS，实现端到端训练，在中高密度发射体下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SMLM依赖单分子逐帧定位构建超分辨图像，但高效采集需要避免荧光体重叠，导致时间长且不利于活细胞成像。现有深度学习虽能处理高密度，但依赖非可微的NMS，训练与推理不一致且可能丢失真阳性，限制性能。

Method: 将SMLM的训练目标重构为集合匹配问题，用最优传输(OT)定义预测与真值之间的匹配与距离，作为可微损失，避免NMS；并提出一类迭代式神经网络，在模型中显式融入显微镜光学系统（PSF/成像物理）的先验，通过多步更新逐渐精化定位。

Result: 在合成数据基准和真实生物数据上，新损失与新架构在中高发射体密度下均超越当前SOTA，提升定位准确率与召回，降低假阳性；代码公开（RSLLES/SHOT）。

Conclusion: 以OT为核心的端到端训练和物理先验驱动的迭代网络能够在无需NMS的情况下稳健处理高密度SMLM，显著缩短采集需求并提升重建质量，适合更快、更可靠的活细胞超分辨成像。

Abstract: Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.

</details>


### [70] [Sharp Monocular View Synthesis in Less Than a Second](https://arxiv.org/abs/2512.10685)
*Lars Mescheder,Wei Dong,Shiwei Li,Xuyang Bai,Marcel Santos,Peiyun Hu,Bruno Lecouat,Mingmin Zhen,Amaël Delaunoy,Tian Fang,Yanghai Tsin,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TL;DR: SHARP从单张照片直接回归场景的3D高斯表示，用一次前向推理在<1秒内完成，并可实时渲染附近视角，达到高分辨率、逼真效果；具有绝对尺度，支持度量级相机运动；零样本泛化强，显著优于SOTA且速度快三个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有单张图像新视角合成方法要么质量有限、尺度不确定，要么依赖耗时的优化或多视图输入，难以实时、鲁棒和具有度量尺度的渲染。作者希望实现从单图出发、一次前向推理即可得到可实时渲染、具备绝对尺度的3D表示，并在零样本场景上稳健泛化。

Method: 提出SHARP：训练神经网络直接从单张图像回归场景的3D Gaussian Splatting参数（位置、各向异性、颜色、密度等），得到带绝对尺度的度量化表示。推理阶段通过一次前向传播在标准GPU上<1秒生成该表示，随后使用高效的高斯渲染器进行实时新视角合成。

Result: 在多数据集上零样本评测，较最佳先前方法LPIPS降低25–34%，DISTS降低21–43%，同时合成速度提升约三数量级；可生成高分辨率、照片级质量的近邻视角图像。

Conclusion: 单图输入即可快速预测可实时渲染、具绝对尺度的3D高斯表示，实现高质量新视角合成并大幅优于SOTA和速度；方法具有良好的零样本泛化与实用性（代码与权重已开源）。

Abstract: We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp

</details>


### [71] [CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images](https://arxiv.org/abs/2512.10715)
*Matias Cosarinsky,Nicolas Gaggion,Rodrigo Echeveste,Enzo Ferrante*

Main category: cs.CV

TL;DR: 本文在胸部X光解剖标志点分割中提出并系统评估不确定性估计：利用混合架构（卷积编码器+图生成解码器）的变分潜空间，给出“潜在不确定性”和“预测不确定性”两种度量；两者在受控扰动下随严重度上升，能指示不可靠预测与OOD样本；并发布含节点级不确定性的超大规模数据集CheXmask-U与开源代码/演示。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割的安全落地需要知道模型何时不可靠。以往多做像素级不确定性，而基于解剖标志点（landmark）的分割具备拓扑一致性优势，却少被研究其不确定性。胸片在临床广泛应用，若能在该范式中量化不确定性，将有助于稳健性与人机协作。

Method: 采用混合神经体系：卷积图像编码器+图生成解码器（具变分潜变量）。提出两类互补度量：1）潜在不确定性：直接由学习到的潜变量分布参数（如均值/方差）计算；2）预测不确定性：从潜空间采样多次，生成随机输出图（标志点/拓扑），统计输出变异。通过受控图像腐蚀/扰动试验评估两者行为。

Result: 两种不确定性均与扰动强度正相关，可同时反映全局与局部退化；能通过与人工真值对比识别不可靠预测，并在CheXmask数据上实现OOD检测。发布CheXmask-U数据集（657,566例胸片标志点分割，含每节点不确定性），并提供交互式演示与源码。

Conclusion: 在解剖标志点分割中引入并验证不确定性估计是可行且有效的，可提升鲁棒性与安全部署；节点级不确定性标签的数据资源将促进后续研究与下游使用时对空间质量差异的建模。

Abstract: Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.

</details>


### [72] [SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving](https://arxiv.org/abs/2512.10719)
*Peizheng Li,Zhenghao Zhang,David Holtz,Hang Yu,Yutong Yang,Yuzhi Lai,Rui Song,Andreas Geiger,Andreas Zell*

Main category: cs.CV

TL;DR: SpaceDrive 是一种面向端到端自动驾驶的空间感知VLM框架，用通用三维位置编码替代数字化文本输入/输出，实现对语义与空间关系的联合推理与直接轨迹回归，在 nuScenes 开放环和 Bench2Drive 闭环上取得SOTA/次优成绩。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽具强通用理解与推理，但对精细三维空间关系理解薄弱，而这对具身交互（自动驾驶）至关重要；现有方法将空间信息以数字文本形式处理，难以对齐视觉语义、效率低且误差累积。

Method: 提出 SpaceDrive：将来自多视角深度估计、历史自车状态与文本提示的所有三维坐标统一编码为通用位置编码（PE）；这些3D PEs一方面叠加到对应的2D视觉token上以增强空间语义对齐，另一方面作为任务无关的坐标表示替代数字token，既用于输入也用于输出；由此VLM可直接回归轨迹坐标而非逐位生成数字，提升空间索引与规划精度。

Result: 在 nuScenes 上取得开放环SOTA；在 Bench2Drive 闭环基准上达到 78.02 的 Driving Score，位列VLM方法第二；总体较现有VLM驱动框架在规划精度与空间推理上有显著提升。

Conclusion: 将空间信息显式化为三维位置编码并与语义token联合建模，可显著提升VLM在自动驾驶中的空间推理与规划能力；以坐标级回归替代数字生成减少误差与延迟，推动VLM端到端驾驶性能达到新水平。

Abstract: End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.

</details>


### [73] [Video Depth Propagation](https://arxiv.org/abs/2512.10725)
*Luigi Piccinelli,Thiemo Wandel,Christos Sakaridis,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出VeloDepth：一种高效在线视频深度估计方法，利用前帧深度先验与特征传播，通过光流引导的变形与残差校正，实现时序一致、精度具竞争力且推理速度显著更快。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么逐帧单目估计，导致时序不稳定与误差累积；要么引入复杂的时序建模（如重型卷积/Transformer/3D模块），难以实时部署，限制在真实场景中的普适性与性能。

Method: 提出VeloDepth流水线：1) 使用上一帧的深度预测和深层特征作为时空先验；2) 设计Propagation Module，基于光流进行特征与深度的warp，并辅以可学习残差进行校正；3) 在结构上显式约束时序一致性，使连续帧的深度稳定；4) 在线处理、轻量高效。

Result: 在多项基准上进行零样本评测，展示出领先的时序一致性与具有竞争力的深度精度，同时推理速度显著快于现有视频深度估计方法。

Conclusion: VeloDepth在保证准确性的同时实现高时序一致性与实时性，为多种感知任务提供实用的在线视频深度估计方案，并已开源代码与模型。

Abstract: Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth

</details>


### [74] [IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation](https://arxiv.org/abs/2512.10730)
*Yuan-Ming Li,Qize Yang,Nan Lei,Shenghao Fu,Ling-An Zeng,Jian-Fang Hu,Xihan Wei,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: IRMoGen 通过将生成、评估与细化以“文本-动作对话”的方式交替进行，打通了动作理解与生成之间的双向知识流，构建 IRG-MotionLLM，并在多基准上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有动作领域的大语言模型通常把“理解(assessment)”与“生成(generation)”割裂开来，无法让两者互相校正与增益；缺少能把评估与细化作为中间桥梁的统一范式。

Method: 提出 Interleaved Reasoning for Motion Generation (IRMoGen)：在一次生成后，模型自评估文本-动作对齐度、定位问题并给出细化建议，再进行动作细化；如此迭代的“文本-动作对话”。实现为 IRG-MotionLLM，并采用三阶段训练逐步注入并强化这种交错能力；同时构建自动化数据引擎，从现有文本-动作数据合成带有交错推理标注的训练样本。

Result: 大量实验显示：(i) 加入评估与细化显著提升文本-动作对齐；(ii) 交错执行“生成-评估-细化”在各训练阶段均带来稳定增益；(iii) IRG-MotionLLM 明显优于基线并在标准 T2M 基准上取得先进性能；跨评测器测试也验证了其有效性。

Conclusion: 评估与细化是连接理解与生成的关键桥梁；将其与生成交错迭代可系统性提升动作生成质量与对齐度。IRG-MotionLLM 证明了该范式的有效性，并提供代码与数据以促进复现与扩展。

Abstract: Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.

</details>


### [75] [LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation](https://arxiv.org/abs/2512.10750)
*Tianyu Zhou,Junyi Tang,Zehui Li,Dahong Qian,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出LDP框架，基于多模态大语言模型生成高质量结肠镜息肉诊断报告；构建MMEndo数据集，并用LoRA+ DPO对Qwen2-VL-7B进行高效对齐；在自动与临床评估上优于基线，训练成本较全量微调降至1/833，并在IU-XRay上验证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动化结肠镜报告存在数据稀缺、跨模态对齐差、幻觉和不一致等问题，难以满足临床标准与基层医疗落地需求，需要一个可扩展、成本可控且可信的多模态生成方案。

Method: 1) 构建MMEndo：专家标注的结肠镜图文对；2) 选用Qwen2-VL-7B为骨干；3) 采用LoRA进行参数高效微调；4) 通过DPO与临床偏好/标准对齐；5) 在自动指标与专家评估上进行系统实验，并进行跨数据集（IU-XRay）验证。

Result: LDP在自动指标与专家严谨打分中均优于现有方法，医生评分达7.2/10；与全量微调相比，训练计算成本降低约833倍；在IU-XRay数据集上保持良好表现，显示出跨场景鲁棒性。

Conclusion: LDP为多模态医疗报告生成提供了一条可扩展、临床可行的路径：通过MMEndo数据、LoRA高效微调与DPO临床对齐，能减轻幻觉并提升一致性，且成本显著降低，适用于基层医疗与其他医学影像场景。

Abstract: Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.

</details>


### [76] [Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography](https://arxiv.org/abs/2512.10765)
*Rene Lisasi,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 提出一个从CCTA到冠脉压力分布的端到端管线：自动几何重建+标准化CFD数据生成+扩散回归模型，推理时无需CFD，达SOTA（R2=64.42%，RMSE=0.0974，nRMSE=0.154）。


<details>
  <summary>Details</summary>
Motivation: CFD可提供诊断CAD所需的血流动力学指标，但计算昂贵、耗时且难以规模化，限制了带标注的血流动力学数据与临床落地。需要一种既能批量生成一致训练数据又能在推理时摆脱CFD的方案。

Method: 1) 自动化CCTA冠脉几何提取，减少人工操作；2) 流水线式生成一致的模拟（CFD）数据作为训练集；3) 设计扩散式回归模型，从CCTA派生特征直接预测冠脉压力分布，推理时不再运行CFD。

Result: 在模拟冠脉血流动力学数据集上，相比多种基线方法取得最佳表现：R2=64.42%，RMSE=0.0974，归一化RMSE=0.154。

Conclusion: 该端到端框架在保持精度的同时显著提高了效率与可扩展性，支持快速、非侵入式的冠脉压力预测，有望促进基于生理的CAD评估在临床中的普及。

Abstract: Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis.

</details>


### [77] [What matters for Representation Alignment: Global Information or Spatial Structure?](https://arxiv.org/abs/2512.10794)
*Jaskirat Singh,Xingjian Leng,Zongze Wu,Liang Zheng,Richard Zhang,Eli Shechtman,Saining Xie*

Main category: cs.CV

TL;DR: 论文发现：在REPA中，用于指导扩散模型生成的目标表示，决定性能的关键不是分类等“全局语义”强弱，而是表示的空间结构（patch 之间的相似关系）。基于此，作者提出iREPA：用卷积替代MLP投影，并对外部表示做空间归一化，<4行代码即可集成，在多种编码器、模型规模与训练范式上都能更快收敛、提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为：更强的全局语义（如ImageNet准确率更高的编码器）作为蒸馏目标能带来更好的生成。作者质疑这一点：在表示对齐中，究竟是全局语义还是空间结构更关键？若是空间结构，则可用更直接的机制加强其迁移，从而改进生成训练效率与效果。

Method: 1) 大规模实证：跨27个视觉编码器与不同扩散模型规模，系统评估REPA蒸馏目标的“全局语义表现”与“空间结构质量”（patch token 余弦相似矩阵）对生成性能的影响。2) 方法改进iREPA：a) 将REPA中的MLP投影层替换为简单卷积层，以更好保留/对齐空间局部结构；b) 对外部表示加入空间归一化层，凸显相对空间关系并稳定对齐训练。实现极简（<4行）。

Result: 实验显示：目标表示的空间结构与最终生成性能强相关，而与其全局语义（如ImageNet-1K准确率）关系弱。iREPA在多种编码器（27个）、不同模型规模及多种训练变体（REPA, REPA-E, Meanflow, JiT等）中，一致带来更快收敛和更好或不差的生成质量。

Conclusion: 生成训练中的表示对齐应更加重视空间结构的传递而非仅追求更强的全局语义分类性能。iREPA提供了一个简单有效的实现路径，提示需要重新审视表示对齐的工作机理，以改进生成模型训练。

Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa

</details>


### [78] [Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading](https://arxiv.org/abs/2512.10808)
*Masum Shah Junayed,John Derek Van Vessem,Qian Wan,Gahie Nam,Sheida Nabavi*

Main category: cs.CV

TL;DR: 提出GLAT+IRM用于前列腺WSI分级：迭代精选关键斑块+图拉普拉斯注意力建模空间连通并凸加权聚合，跨6数据集优于SOTA且更一致高效。


<details>
  <summary>Details</summary>
Motivation: 随机/静态取样常包含冗余或无信息区域，WSI尺度大、组织异质、难挑诊断区域，导致分级不稳、空间不一致与计算浪费，需一种能自适应筛选关键区域并保持空间连贯性的框架。

Method: 两部分：1) IRM：用预训练ResNet50提局部特征，调用基础模型（无梯度）进行重要性评分，迭代式更新保留集，逐步去除不相关斑块。2) GLAT：将斑块作图节点，构造组织连通图，引入图拉普拉斯约束提升空间一致性；通过可学习滤波增强判别性结构；并用凸聚合机制动态调整权重，生成鲁棒的WSI级表示。

Result: 在5个公开与1个私有数据集上，较现有方法取得更高分级性能与更强空间一致性，同时保持较好的计算效率。

Conclusion: 通过迭代精选关键区域与图拉普拉斯注意力的联合，模型在保证空间一致性的同时提升分级准确度与效率，适合大规模WSI的高效、稳健病理分级。

Abstract: Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency.

</details>


### [79] [Self-Ensemble Post Learning for Noisy Domain Generalization](https://arxiv.org/abs/2512.10818)
*Wang Lu,Jindong Wang*

Main category: cs.CV

TL;DR: 论文针对域泛化在噪声标签场景下易放大伪特征、导致性能退化的问题，提出自集成后学习（SEPL）：在模型的中间层训练多个探测分类器，并在推理时集成其预测；为抵抗噪声，用半监督训练这些探测器，并用众包式推断融合结果。实验显示SEPL提升鲁棒性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉与机器学习方法在分布移位和标签噪声下鲁棒性不足；域泛化方法在遇到噪声标签时，深层伪特征被放大，性能显著下降。作者希望让既有DG方法在含噪环境下“重获新生”。

Method: 提出SEPL，包括：1) 特征探测训练：利用预训练模型的中间层特征，训练多个探测（probing）分类头；在噪声标签下，使用半监督学习策略优化这些探测器。2) 预测集成推理：不同探测器关注图像不同区域，采用众包式推断（类似加权投票/置信聚合）融合多头输出，得到最终预测。

Result: 广泛实验表明，SEPL在含噪标签和分布移位场景中能提升现有方法的鲁棒性与总体性能，并显示出较强的灵活性与落地潜力。

Conclusion: 通过挖掘模型中间表示并进行自集成，SEPL缓解噪声引发的伪特征放大问题，提升域泛化在现实含噪环境中的可靠性；方法通用、可与现有预训练模型结合，具有较强扩展性。

Abstract: While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.

</details>


### [80] [PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning](https://arxiv.org/abs/2512.10840)
*Jianqi Chen,Biao Zhang,Xiangjun Tang,Peter Wonka*

Main category: cs.CV

TL;DR: 提出PoseGAM：一个几何感知的多视角框架，从查询图像和多张模板图像直接回归6D物体位姿，无需显式特征匹配；融合点级几何与几何表示网络学习的特征，并构建19万+对象的大规模合成数据以提升泛化；在多基准上SOTA，平均AR提升5.1%，单数据集最高+17.6%。


<details>
  <summary>Details</summary>
Motivation: 现有对未见物体的6D位姿估计依赖查询图与模型/模板间的显式特征对应，遇到外观变化、遮挡、背景干扰与类别泛化时脆弱。需要一种能跨对象泛化、避免易错匹配、并充分利用物体几何的端到端方法。

Method: 基于多视角类基础模型架构，输入为一张查询图和多张模板图。通过两条互补几何通道：1) 显式点级几何（如从模板/模型采样点云或可渲染几何，与图像特征融合）; 2) 几何表示网络学习到的隐式几何特征。多视角融合后直接回归6D位姿（旋转+平移），不再构造显式对应关系。同时构建包含>19万对象、覆盖多样环境条件的大规模合成数据，用于训练以提升鲁棒性与泛化。

Result: 在多个基准上达到SOTA，平均AR比先前方法提升5.1%，在部分数据集最高提升17.6%，显示对未见物体有很强的泛化能力。

Conclusion: 通过几何感知的多视角直接回归框架与大规模多样化合成数据，PoseGAM在无需显式匹配的前提下显著提升未见物体的6D位姿估计性能与泛化能力。

Abstract: 6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .

</details>


### [81] [SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation](https://arxiv.org/abs/2512.10860)
*Kehong Gong,Zhengyu Wen,Mingxi Xu,Weixia He,Qi Wang,Ning Zhang,Zhengyu Li,Chenbin Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: SWiT-4D提出一种无损、无需额外参数的滑动窗口Transformer，将任意DiT式图像到3D生成器扩展到时空建模，实现从单目视频生成显式4D网格；用极少视频微调即可获得高保真几何与时间一致性，并在多基准上优于现有方法的平滑度。


<details>
  <summary>Details</summary>
Motivation: 单目视频转高质量显式4D网格难，缺乏大规模自然采集的4D网格数据限制了端到端训练；而图像到3D已有强大先验与数据基础，如何在极少4D监督下有效利用这些先验实现视频到4D成为关键。

Method: 提出SWiT-4D：在任意DiT型image-to-3D生成器上以滑动窗口Transformer进行时空建模，保持单帧前向过程（“无损、零新增参数”整合），支持任意时长视频的4D网格重建；并针对静态相机视频引入优化式轨迹模块以恢复全局平移。仅需单个<10秒视频进行微调即可。

Result: 在域内（动物园测试集）和域外（C4D、Objaverse、野外视频）基准上，SWiT-4D在时间平滑度上持续优于现有方法，同时获得高保真几何与稳定的时间一致性，体现出强数据效率。

Conclusion: SWiT-4D有效将图像到3D先验迁移到视频到4D网格生成，在几乎无4D监督下实现高质量、时间稳定的4D资产构建，并具备实际可部署性。

Abstract: Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/

</details>


### [82] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出MMSI-Video-Bench，用于全面评测MLLM在视频空间智能上的感知、规划、预测与跨视频推理能力；覆盖多源数据与三大子基准，实测25个模型显示显著人机差距与系统性失败，常见微调与技巧无明显增益。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM缺乏对连续视频中空间理解的系统评测；需要一个高质量、覆盖全面、可细分领域能力的基准来客观衡量和推动视频空间智能进展。

Method: 构建全人工标注的MMSI-Video-Bench：从25个数据集与自采视频中选取1,278段剪辑，设计1,106个问题，按四级框架（感知、规划、预测、跨视频推理）组织；每项由3D视觉专家审校并给出解释性理由；并提供三类子基准（室内感知、机器人、指代/定位）。评测25个开源与闭源MLLM，并进行细粒度误差分析与对常见策略（帧采样、3D线索、CoT提示等）的检验。

Result: 多数模型接近随机水平，最佳推理模型仍比人类低近60%；空间微调模型在该基准上泛化不佳；发现几类系统性错误：几何推理、运动落地、长时预测与跨视频对应；常见帧采样策略迁移差，3D空间线索与链式思维提示未带来显著提升。

Conclusion: MMSI-Video-Bench为视频空间智能提供了覆盖全面、严格标注的评测平台，暴露当前MLLM在空间与时序推理上的显著短板，可作为推进该方向研究与模型改进的坚实测试床。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [83] [From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models](https://arxiv.org/abs/2512.10867)
*Zongzhao Li,Xiangzhe Kong,Jiahui Su,Zongyang Ma,Mingze Li,Songyou Li,Yuelin Zhang,Yu Rong,Tingyang Xu,Deli Zhao,Wenbing Huang*

Main category: cs.CV

TL;DR: 提出MiSI概念与MiSI-Bench基准，以评估VLM在微观空间智能上的能力；结果显示现有SOTA显著落后于人类，微调7B在空间变换上可超人，但在氢键等科学知识任务上薄弱，提示需引入显式领域知识。


<details>
  <summary>Details</summary>
Motivation: 科学发现常依赖对不可见微观实体的空间关系感知与推理；现有VLM是否具备此类“微观空间智能”缺乏系统评估，因而需要一个覆盖多任务、可量化的人机对比基准。

Method: 提出MiSI（Microscopic Spatial Intelligence）概念，并构建MiSI-Bench：基于约4000个分子结构生成约58.7万张图像与16.3万+问答，设计9类任务，跨度从基础空间变换到复杂关系识别；用多种SOTA VLM进行评测，并训练/微调一个7B模型进行对比。

Result: SOTA VLM整体显著低于人类；微调7B模型在空间变换类任务上达到甚至超过人类，但在与科学知识紧密相关的任务（如氢键识别）表现较差。

Conclusion: 当前VLM缺乏稳健的微观空间智能；通过微调可提升几何推理，但要在科学任务上取得突破需融入显式领域知识与先验，迈向科学AGI。

Abstract: This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.

</details>


### [84] [MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos](https://arxiv.org/abs/2512.10881)
*Kehong Gong,Zhengyu Wen,Weixia He,Mingxi Xu,Qi Wang,Ning Zhang,Zhengyu Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: MoCapAnything提出“类别无关动作捕捉（CAMoCap）”：给定单目视频与任意绑定骨骼的3D资产作为提示，直接输出能驱动该资产的旋转动画（如BVH）。方法以参考资产为引导，先预测资产无关的3D关节轨迹，再通过带约束的逆向运动学恢复资产特定的关节旋转。系统包括参考提示编码器、视频特征提取与粗4D重建、统一运动解码器，以及轻量IK。作者还构建Truebones Zoo数据集（1038段，含骨架-网格-渲染三联）。实验表明：在域内与野外视频上均能高质量动画驱动，并可跨物种/异构骨架有效重定向。


<details>
  <summary>Details</summary>
Motivation: 现有动作捕捉与重定向流程多为物种或模板特定（如只适用于人类或固定骨架），限制了对任意资产（不同拓扑、比例、关节配置）的可扩展内容创作需求。缺乏一个统一框架可从单目视频直接得到可驱动任意资产的旋转动画。

Method: 提出参考引导、因素分解的两阶段方案：1）先预测与资产无关的3D关节轨迹；2）再用考虑骨骼/关节约束的逆运动学求解资产特定旋转。架构包含三可学习模块与轻量IK：- 参考提示编码器：从资产骨架、网格及渲染图提取逐关节查询embedding；- 视频特征提取器：计算稠密视觉描述并重建粗糙4D可变形网格，作为视频与关节空间的桥梁；- 统一运动解码器：融合参考与视频线索，生成时间一致的关节轨迹；- 约束感知IK：将轨迹转化为特定资产的旋转（BVH等）。并构建Truebones Zoo数据集（1038条，标准化骨架-网格-渲染三联）。

Result: 在多个基准与野外视频中，能生成高质量骨骼动画，表现出跨物种、异构骨架的有意义重定向能力；在用户给定不同资产作为提示时，系统稳定输出可直接驱动该资产的旋转序列。

Conclusion: MoCapAnything将单目视频动作捕捉推广到类别无关、提示驱动的设定，通过先轨迹后旋转的分解与参考引导设计，显著提升对异构资产的泛化与可控性，并配套新数据集支撑训练与评测，推动可扩展三维动作捕捉。

Abstract: Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/

</details>


### [85] [PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction](https://arxiv.org/abs/2512.10888)
*Brandon Smock,Valerie Faucon-Morin,Max Sokolov,Libin Liang,Tayyibah Khanam,Maury Courtland*

Main category: cs.CV

TL;DR: 提出PubTables-v2大规模数据集，覆盖多页表格结构识别等任务；用其评测领域专用VLM并展示进展；并据此提出用于页级表格抽取的POTATR（图像到图的Table Transformer扩展）。


<details>
  <summary>Details</summary>
Motivation: 现有表格抽取方法多为先检测再结构识别，但缺乏能在整页/多页上下文中直接抽取的模型评测基准，尤其缺少带标注的大规模数据来推动和量化进展，导致难以比较与验证新方法。

Method: 构建大规模带标注的PubTables-v2数据集，覆盖多种表格抽取任务，重点包含首个大规模多页表格结构识别基准；用该数据集系统评测领域专用视觉-语言模型（VLM）在相关任务上的表现；基于Table Transformer扩展出Page-Object Table Transformer（POTATR），将图像到图的建模用于页级综合表格抽取。

Result: 证明PubTables-v2可用于评测并揭示当前方法（含域专用VLM）的能力与不足；给出在多项任务上的基线与对比结果（摘要未给出具体数值）；展示POTATR在页级表格抽取上的有效性。

Conclusion: PubTables-v2为多页与页级表格抽取提供了首个大规模基准，推动VLM等方法的可比评估；POTATR进一步扩展了表格变换器到页级图建模。数据、代码与模型将开源，促进该领域发展。

Abstract: Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.

</details>


### [86] [DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance](https://arxiv.org/abs/2512.10894)
*Peiying Zhang,Nanxuan Zhao,Matthew Fisher,Yiran Xu,Jing Liao,Difan Liu*

Main category: cs.CV

TL;DR: 提出DuetSVG：一个同时生成图像token与SVG token的统一多模态模型，并在推理时用模型自带的视觉预测作为引导以提升SVG解码质量，显著优于现有VLM仅文本生成的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的SVG生成方法在解码阶段只输出文本、缺少视觉信号，导致对复杂语义理解不足、图形不美观、几何一致性差与语法错误等问题，亟需一种能在生成过程中结合视觉与向量表示的方案。

Method: 构建端到端统一模型DuetSVG，联合训练图像与SVG数据，同时在推理阶段提出“测试时扩展/缩放”策略：利用模型原生的图像token预测作为引导信号，协同约束与引导SVG token的解码，从而提升语义与几何一致性以及语法正确性。

Result: 在多种应用与数据集上进行广泛实验，DuetSVG较现有方法在视觉忠实度、语义对齐度与SVG语法洁净度上均取得更优结果。

Conclusion: 联合生成图像与SVG并在推理中用视觉预测引导SVG解码是有效范式，可显著提升SVG生成的视觉质量、语义一致与语法正确性，优于仅文本解码的VLM方法。

Abstract: Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.

</details>


### [87] [FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos](https://arxiv.org/abs/2512.10927)
*Yulu Gan,Ligeng Zhu,Dandan Shan,Baifeng Shi,Hongxu Yin,Boris Ivanovic,Song Han,Trevor Darrell,Jitendra Malik,Marco Pavone,Boyi Li*

Main category: cs.CV

TL;DR: 提出FoundationMotion：一个自动化构建大规模、细粒度运动数据集的流水线，通过视频目标检测与跟踪提取轨迹，并借助LLM从轨迹与帧生成细粒度描述与多样化运动/空间推理问答，用于微调开源多模态/视频模型，显著提升运动理解能力，并在多项基准上超越强闭源与大规模开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型在运动理解与物理推理基准上表现受限，核心瓶颈是缺乏大规模、细粒度且可扩展的运动数据集；人工标注代价高、难扩展，亟需自动化、高质量的数据构建方案。

Method: 构建全自动数据管线：1）在视频中进行目标检测与多目标跟踪，提取物体轨迹；2）结合轨迹与关键帧，借助LLM生成细粒度运动描述与多样化运动/空间推理问答；3）使用该数据对开源视频/多模态模型（如 NVILA-Video-15B、Qwen2.5-7B）进行微调，同时评估对其他任务的影响。

Result: 微调后的模型在多个运动理解数据集与基准上显著提升，且未损害其他任务表现；在与强闭源（如 Gemini-2.5 Flash）和大规模开源（如 Qwen2.5-VL-72B）模型的对比中取得领先。

Conclusion: FoundationMotion提供了可扩展、低成本的细粒度运动数据集构建方案，能有效增强多样模型的运动理解与空间推理能力，并在广泛基准上实现SOTA级表现。

Abstract: Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.

</details>


### [88] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM‑V2 是一个以婴幼儿发展为灵感的视觉‑语言预训练框架：用纵向的婴儿视听语料和多模态任务（DevCV Toolbox）进行训练与评测，小模型从零开始也能在这些认知对齐任务上取得有竞争力结果，部分超过 GPT‑4o。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型的预训练多以大规模但与人类早期经验不匹配的数据为主，缺少对婴幼儿认知发展轨迹的对齐与可量化评估。作者希望通过发展学动机：用婴儿真实接触的多模态、低样本、纵向数据与与其能力匹配的测评，探索更样本高效、发展合理的预训练路径。

Method: 提出 BabyVLM‑V2 框架：1) 数据：构建覆盖广但低人工筛选的婴儿中心纵向视听语料，包含视频‑话语、图像‑话语与多轮对话，模拟婴儿经历；2) 模型：设计一个紧凑且通用的视觉‑语言模型，从零开始预训练；3) 评测：推出 DevCV Toolbox，将 NIH Baby Toolbox 中所有与视觉相关的量表改编为10个多模态任务，覆盖空间推理、记忆、词汇理解等早期能力。

Result: 在 DevCV Toolbox 上，小规模从零预训练的模型取得有竞争力成绩，在若干任务上超过 GPT‑4o；显示了发展学对齐的数据与任务能够带来样本效率与性能的优势。

Conclusion: 以儿童发展为导向的统一预训练与评测框架是可行且有效的。BabyVLM‑V2 为研究“发展可行”的视觉基础模型提供了数据、模型与基准三位一体的路径，有望加速该方向进展。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [89] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D 是一个可扩展的多视角 Transformer，用于度量尺度的稠密前馈 4D 重建，直接对 N 帧输出逐像素几何与运动（场景流），可灵活融合 RGB、RGB-D、IMU 位姿、雷达多普勒等多模态传感，基于“自我坐标系+世界坐标系”的模块化表征，实现更高精度（误差降 2–3 倍）与更高效率（加速约 15 倍）。


<details>
  <summary>Details</summary>
Motivation: 现有 4D 重建方法常局限于：仅做两视图稠密场景流或稀疏 3D 点跟踪；或仅依赖单目 RGB，难以融合多传感器；推理效率低。需要一种既可端到端稠密预测、又能无缝接入多模态、且在度量尺度上稳定的统一框架。

Method: 提出 Any4D：一个可扩展的多视角 Transformer。核心是将 4D 场景拆分为两类因素的模块化表征：- 以相机为中心（egocentric）的局部因素：逐视图深度图、相机内参；- 以世界为中心（allocentric）的全局因素：相机外参、场景流。模型直接对 N 帧进行逐像素几何与运动预测，可选接入 RGB-D、IMU 里程计、雷达多普勒等观测。采用前馈式推断实现多帧并行和高效计算。

Result: 在多种设置与数据模态下，相比现有方法精度提升 2–3 倍（误差更低），计算效率提升约 15 倍（推理更快），并支持更广泛的输入组合与场景规模。

Conclusion: Any4D 以模块化、坐标系分离的设计实现了对多模态输入的统一 4D 重建，能以度量尺度产生稠密的多帧几何与场景流预测，兼具高精度与高效率，为多种下游应用（如视频理解、机器人定位与导航、AR/VR）提供基础。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [90] [GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting](https://arxiv.org/abs/2512.10939)
*Madhav Agarwal,Mingtian Zhang,Laura Sevilla-Lara,Steven McDonagh*

Main category: cs.CV

TL;DR: 提出一种基于3DMM参数驱动的高斯点云（Gaussian Splatting）说话人头像生成方法，用音频直接预测时序一致的表情与口型，实现单目视频建模后的人物特定（person-specific）实时说话头像，兼顾视觉质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动头像方法存在两难：扩散模型画质高但单样本/快速生成困难；高斯点云实时但因人脸跟踪误差和高斯映射不一致导致抖动与伪影，难以稳定用于真实交互场景。需要一种同时具备实时性、视觉保真度与时间稳定性的系统。

Method: 1）以3D Morphable Model（3DMM）作为中间表示，将高斯点云与3DMM对齐，生成人物特定的可渲染头像；2）设计基于Transformer的音频到3DMM参数预测网络，直接从语音估计表情/口型等参数以保证时序一致；3）从单目视频构建头像，再用独立音频驱动渲染，实现实时说话头生成。

Result: 在实时推理速度下生成说话头像，报告了与现有方法相比具有竞争力的定量与定性结果，显著减少了抖动和视频伪影，提升时间一致性与视觉稳定性。

Conclusion: 将3DMM与Gaussian Splatting结合，并以Transformer从音频预测参数，可在单样本场景中实现人物特定、实时且时序稳定的说话头像；为交互式虚拟人提供更实用的方案。

Abstract: Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.

</details>


### [91] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView 提出一个统一的扩散模型框架，解耦空间、时间与视角条件，覆盖并泛化到多种4D一致性任务（NVS、文本/图像到视频、相机轨迹控制等），在多项基准上超越或匹配专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法各自针对NVS、相机可控T2V、I2V等子任务，训练数据切分零散，难以跨任务泛化与共享能力；需要一个能在单一模型中统一处理多种4D一致性任务、并充分利用分布式3D/4D数据的通用框架。

Method: 以扩散模型为基础，分离建模空间（3D场景）、时间（动态演化）与视角/相机条件（位姿、轨迹）。通过可组合的条件设计，让模型在输入上灵活组合：静态/动态/多视图作为空间条件、时间外推/回溯作为时间条件、完整相机控制作为视角条件。统一训练覆盖多种数据形态（静态、多视角、动态视频），实现单模型多任务。

Result: 在多项基准上优于或匹配专用模型：LLFF多视图NVS图像质量提升至多33%；Neural 3D Video动态NVS提升约60%；RE-10K静态相机控制提升约20%；文本条件视频生成中的相机轨迹误差降低4倍。

Conclusion: 通过对空间、时间与视角条件的解耦与统一训练，OmniView作为单一通用4D视频模型，实现跨任务泛化与强竞争力，证明了构建“通才型”4D一致性扩散模型的可行性。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [92] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 提出“Mull-Tokens”：可在文本与图像间承载中间推理信息的模态无关潜在token，通过先监督再无监督微调，提升多模态空间推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理方法在图像-文本间切换依赖外部工具、昂贵图像生成或手工标注推理轨迹，难以扩展且脆弱。需要一种无需专门工具、能自由在模态间“思考”的通用中间表示。

Method: 引入Mull-Tokens作为模态无关潜在token：1) 监督阶段用交错的图文推理轨迹训练，使Mull-Tokens学习承载中间信息；2) 随后仅用最终答案进行无监督式微调（无轨迹），鼓励模型在自由形式的图文潜在空间中进行推理。与文本-only和图文交替推理基线对比。

Result: 在四个空间推理基准（拼图、视角变换等）上，平均提升约+3%，在以推理为主的拼图子集上较最强基线最高提升+16%。

Conclusion: Mull-Tokens为抽象地在多模态间进行中间推理提供简单、可扩展方案，缓解视觉与文本推理的对齐/落地难题，并优于多种现有基线。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [93] [VL-JEPA: Joint Embedding Predictive Architecture for Vision-language](https://arxiv.org/abs/2512.10942)
*Delong Chen,Mustafa Shukor,Theo Moutakanni,Willy Chung,Jade Yu,Tejaswi Kasarla,Allen Bolourchi,Yann LeCun,Pascale Fung*

Main category: cs.CV

TL;DR: VL-JEPA 是一种基于联合嵌入预测架构（JEPA）的视觉语言模型，不再自回归生成文本，而是预测目标文本的连续嵌入；在相同视觉编码器和数据下，相比传统 token 空间训练，性能更强、可训练参数减少约 50%。推理时仅在需要时用轻量文本解码器将嵌入转成文本，并可进行选择性解码，将解码次数减少约 2.85 倍，性能几乎不损。其嵌入空间天然支持开放词汇分类、文本-视频检索和判别式 VQA。跨 8 个视频分类与 8 个视频检索数据集，平均表现优于 CLIP、SigLIP2、Perception Encoder；在 GQA、TallyQA、POPE、POPEv2 上与 InstructBLIP、QwenVL 等经典 VLM 相当，且仅 1.6B 参数。


<details>
  <summary>Details</summary>
Motivation: 现有 VLM 主要在 token 空间进行自回归生成，受表面语言变异性影响大、训练和推理成本高，且在非生成类任务上需额外适配。作者希望在抽象表示空间学习语义，降低解码负担，提升效率与泛化，并用同一模型无缝支持生成与判别任务。

Method: 提出 VL-JEPA：以 JEPA 方式直接预测目标文本的连续嵌入而非 token。训练时与标准 token 空间 VLM 做严格对照（相同视觉编码器与数据）。推理阶段仅在需要文本输出时调用轻量解码器以把预测嵌入转文本，并引入“选择性解码”策略，按需要触发解码以减少解码频次。模型的联合嵌入空间用于开放词汇分类、文本-视频检索、判别式 VQA 等，无需额外架构改动。

Result: 在等设置对照下，VL-JEPA 以约 50% 更少可训练参数获得更强性能；选择性解码将解码操作减少约 2.85 倍且性能近似不变；在 8 个视频分类与 8 个视频检索数据集的平均表现超过 CLIP、SigLIP2、Perception Encoder；在 GQA、TallyQA、POPE、POPEv2 上与 InstructBLIP、QwenVL 等经典生成式 VLM 表现相当；整体模型规模仅约 1.6B 参数。

Conclusion: 在抽象嵌入空间学习并以按需解码替代全量自回归，可同时提升效率与效果。VL-JEPA 统一支持生成与判别任务，减少参数与推理成本，同时在多项视频与 VQA 基准上达到或超过主流方法，显示出 JEPA 风格 VLM 的实用潜力。

Abstract: We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.

</details>


### [94] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT在主体驱动视频生成中加入“显式时间戳”条件与新型时间区间位置编码，并配合主体描述文本token，以极小开销实现多主体在不同时间段的精准出现/消失控制，同时保持与SOTA相当的画质与身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化（subject-driven）视频扩散模型虽能按用户主体生成，但无法细粒度控制每个主体何时出现或消失，限制了分镜制作、可控动画与合成式视频创作等应用。需要一种既兼容预训练视频扩散模型、又能精确时间调度多主体的机制。

Method: 提出AlcheMinT框架：1) 显式时间戳条件：为每个主体引入可编码的时间区间；2) 新型位置编码：设计能表达“时间区间-主体身份”绑定的时序位置编码，并与原视频模型的pos embedding无缝叠加；3) 主体描述文本token：在文本侧强化视觉身份与caption的绑定，减少生成歧义；4) token级拼接：通过token-wise concatenation集成上述条件，无需新增cross-attention模块，参数开销可忽略；5) 构建评测基准，覆盖身份保持、视频保真度与时间遵从度。

Result: 在所建基准与实验中，AlcheMinT的视觉质量与现有SOTA个性化视频方法相当，同时首次在多主体场景中实现精确的时间段级出现/消失控制，并显著提升时间遵从度与身份一致性。

Conclusion: AlcheMinT以低开销为主体驱动视频生成引入可控的时间维度，解决多主体精准时序调度难题，而不牺牲画质与个性化效果；为分镜、可控动画和合成视频提供实用的统一方案。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [95] [MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation](https://arxiv.org/abs/2512.10945)
*Henghui Ding,Chang Liu,Shuting He,Kaining Ying,Xudong Jiang,Chen Change Loy,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出MeViS，多模态“动作指代”视频分割数据集，并基于此系统评测现有方法与提出LMPM++，在RVOS/AVOS/RMOT上创SOTA，凸显运动表达在像素级视频理解中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有指代视频分割数据集偏向显著目标与静态属性描述，使目标可在单帧被锁定，弱化了视频与语言中的“运动”信息，难以检验模型对运动表达与运动推理的真正理解。

Method: 构建MeViS：2006段复杂场景视频、8171个对象、33072条人工标注的运动表达（含文本与音频两模态）。围绕MeViS设立4类任务并基准评测15种现有方法：6个RVOS、3个AVOS、2个RMOT、4个视频字幕/生成方法（用于新设RMEG任务）。进一步提出统一框架LMPM++，用于RVOS/AVOS/RMOT。

Result: 系统基准显示现有方法在运动表达引导的视频理解上存在明显短板。LMPM++在MeViS上的RVOS/AVOS/RMOT任务取得新的SOTA表现。并公开数据集与代码。

Conclusion: MeViS为研究“基于运动表达”的像素级视频理解提供标准平台，揭示当前方法的不足并推动面向复杂场景的算法发展；所提LMPM++验证了统一框架在多任务上的有效性。

Abstract: This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/

</details>


### [96] [Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving](https://arxiv.org/abs/2512.10947)
*Jiawei Yang,Ziyu Chen,Yurong You,Yan Wang,Yiming Li,Yuxiao Chen,Boyi Li,Boris Ivanovic,Marco Pavone,Yue Wang*

Main category: cs.CV

TL;DR: Flex提出一种高效场景编码器，用少量可学习场景token跨多相机多时刻联合汇聚视觉信息，几何无关、无显式3D先验，强力压缩供下游LLM政策模型使用；在2万小时数据上实现2.2倍吞吐并显著提升驾驶表现，且场景token呈现出自发的场景分解能力。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶需处理高吞吐的多相机时序图像，现有方法多依赖BEV/占据/tri-plane等显式3D归纳偏置，计算昂贵、扩展性受限。需要一种更高效、可扩展、对3D先验不依赖的场景表征方法，兼顾性能与速度。

Method: 引入少量可学习的全局“场景token”，通过与所有相机与时间步的图像token交互，联合编码整场景；不显式构建3D几何表示，采用数据驱动的紧凑表示以供下游LLM政策模型消费；整体是几何无关的跨相机/时序的holistic编码与激进压缩策略。

Result: 在专有2万小时驾驶数据上，相比SOTA推理吞吐提升2.2倍，同时驾驶性能显著提升；紧凑场景token在无监督信号下出现“场景分解”的涌现能力。

Conclusion: 无需依赖3D先验也可高效、有效地编码多相机时序场景；数据驱动的联合编码策略更具可扩展性与效率，为端到端自动驾驶提供了更优路径。

Abstract: We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.

</details>


### [97] [ClusIR: Towards Cluster-Guided All-in-One Image Restoration](https://arxiv.org/abs/2512.10948)
*Shengkai Hu,Jiaqi Ma,Jun Wan,Wenwen Min,Yongcheng Jing,Lefei Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出ClusIR：通过“可学习聚类+频域调制”实现自适应的多退化统一图像复原；用PCGRM做稳健的聚类感知路径选择，用DAFMM在频域进行针对性调制，显著提升复杂/混合退化下的恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有一体化图像复原模型难以显式刻画退化语义，遇到复杂或混合退化时难以自适应调整；专家路由不稳、退化识别与激活耦合、空间与频域的信息协同不足。

Method: 1) 通过可学习聚类显式建模退化语义；2) 概率式聚类引导路由机制（PCGRM）：将退化识别与专家激活解耦，以聚类概率实现稳定、可区分的专家路由；3) 退化感知频率调制模块（DAFMM）：利用聚类先验进行自适应频率分解与目标向调制，联合优化结构与纹理；4) 在空间-频域间传播聚类线索，形成聚类引导的跨域协同。

Result: 在多种基准与多种退化场景下取得有竞争力甚至显著的复原指标与视觉效果（文中表明在广泛退化类型上达到较强性能）。

Conclusion: 显式的聚类语义建模与频域调制的协同，可带来稳健的专家路由和针对性复原，提升复杂/混合退化下的统一图像复原效果；方法通用，可扩展到多场景。

Abstract: All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.

</details>


### [98] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 提出首个系统性研究将强化学习用于自回归文本到3D生成，并据此推出RL增强模型AR3D-R1与新基准MME-3DR。


<details>
  <summary>Details</summary>
Motivation: RL已提升LLM与2D生成，但3D生成对全局几何一致性与局部纹理细节更敏感，现有基准也难以评估隐式推理能力，缺乏系统方法与算法/奖励设计指引。

Method: 从四个维度系统研究：1) 奖励设计：比较不同维度与模型来源的奖励，强调对齐人类偏好与使用通用多模态模型提供稳健3D属性信号；2) RL算法：研究GRPO及其变体，验证token级优化有效，并分析数据规模与迭代扩展规律；3) 基准：提出MME-3DR以评测3D生成模型的隐式推理；4) 高级RL范式：提出层级式Hi-GRPO，针对从全局到局部的3D生成过程配套分层奖励集成。最终实现AR3D-R1，从粗形状到纹理精修的RL增强自回归3D模型。

Result: 实证表明：与人类偏好对齐的奖励最关键；通用多模态模型能提供稳健的3D属性评估信号；token级GRPO更优且具可扩展性；新基准MME-3DR能够揭示隐式推理差异；Hi-GRPO在全局-局部层级优化上带来显著效果，促成首个RL强化的文本到3D模型AR3D-R1达到更好几何与纹理一致性。

Conclusion: 将RL引入文本到3D自回归生成可行且有效，关键在于合适的奖励设计与层级优化策略。MME-3DR与Hi-GRPO为后续研究提供评测与方法论基础，AR3D-R1展示了从形状到纹理的端到端性能提升。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [99] [E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training](https://arxiv.org/abs/2512.10950)
*Qitao Zhao,Hao Tan,Qianqian Wang,Sai Bi,Kai Zhang,Kalyan Sunkavalli,Shubham Tulsiani,Hanwen Jiang*

Main category: cs.CV

TL;DR: E-RayZer 是一种自监督的大规模3D视觉预训练模型，直接在3D空间以显式几何进行重建学习，借助细粒度课程学习实现稳定可扩展训练，显著优于RayZer，在姿态估计上更强，部分指标可比或超越全监督重建模型，并在多种3D下游任务迁移中优于主流视觉预训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督预训练在语言、2D图像与视频上成功，但对多视角图像的3D感知表示仍欠缺。先前方法（如RayZer）多通过潜空间视图合成间接学习3D，易走捷径、几何约束弱，难以得到真正3D感知表征。作者希望构建能直接学习几何、一致性强、可扩展到大规模数据的3D自监督框架。

Method: 提出E-RayZer：在显式3D空间进行自监督重建，直接预测并优化几何（如体素/网格/显式体表示与相机参数的一致性），避免潜空间合成的歧义与捷径。为保证训练收敛与扩展性，设计无监督细粒度课程学习，将样本按难度（纹理、视角基线、遮挡等）与数据源异质性组织为由易到难的训练计划，实现不同来源数据的协调学习。

Result: 在姿态估计上显著优于RayZer；在3D重建等任务上可匹敌或部分超越全监督模型（如VGGT）；其学到的表示在3D下游任务迁移上优于DINOv3、CroCo v2、VideoMAE V2及RayZer。

Conclusion: 直接在3D空间以显式几何进行自监督预训练能学得更具几何约束的表示，并在多项3D任务中带来实质收益。E-RayZer确立了3D感知视觉预训练的新范式，兼具收敛性、可扩展性与强迁移性能。

Abstract: Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.

</details>


### [100] [Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration](https://arxiv.org/abs/2512.10954)
*Sicheng Mo,Thao Nguyen,Richard Zhang,Nick Kolkin,Siddharth Srinivasan Iyer,Eli Shechtman,Krishna Kumar Singh,Yong Jae Lee,Bolei Zhou,Yuheng Li*

Main category: cs.CV

TL;DR: 提出“Group Diffusion”，在扩散模型推理时让多张图像共享注意力共同去噪，显著提升生成质量，ImageNet-256 上 FID 最多提升32.2%。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型推理将样本彼此独立，未利用跨样本信息；作者探究是否能在推理阶段让样本协同生成，通过共享注意力捕捉跨图像关联以提升质量。

Method: 在基于Diffusion Transformer的架构中，将自注意力从“单图像内patch”扩展为“跨多图像+单图像”的共享注意力；在推理过程中将若干样本组成组同时去噪，学习组内的内外部对应关系。提出定性度量来评估跨样本注意力强度，并与FID相关性分析。

Result: 随着组大小增大，跨样本注意力增强且生成质量提升；在ImageNet-256×256上，GroupDiff相对基线FID最高提升32.2%，定性度量与FID变化高度相关。

Conclusion: 跨样本共享注意力的协同推理是一条有效且此前未充分探索的生成途径，可在不改变训练数据的情况下显著提升扩散模型生成质量，组规模是关键调节因素。

Abstract: In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.

</details>


### [101] [Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization](https://arxiv.org/abs/2512.10955)
*Tsai-Shien Chen,Aliaksandr Siarohin,Guocheng Gordon Qian,Kuan-Chieh Jackson Wang,Egor Nemchinov,Moayed Haji-Ali,Riza Alp Guler,Willi Menapace,Ivan Skorokhodov,Anil Kag,Jun-Yan Zhu,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 提出Omni-Attribute：一个开放词汇的图像属性编码器，通过成对数据与双目标训练，学习可分解、属性特定的表征，实现高保真个性化与组合生成并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法使用整体图像嵌入，多个视觉因素纠缠，难以只转移单一属性，造成信息泄漏与不一致合成，缺乏对属性级别的细粒度控制。

Method: 数据与模型联合设计：1）构建带正/负属性标注的语义关联图像对，显式指示需保留与抑制的成分；2）采用“生成保真度+对比解耦”的双目标训练，学习高保真且可分解的属性特征；得到开放词汇的属性编码器Omni-Attribute。

Result: 在开放词汇属性检索、个性化与属性组合生成等任务上，取得多项基准的SOTA表现，验证了表征的可分解性与保真度。

Conclusion: 通过专门的数据构造与双目标学习，可获得高保真、可解耦的属性级表示，显著减少信息泄漏并提升个性化与组合生成质量。

Abstract: Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.

</details>


### [102] [Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision](https://arxiv.org/abs/2512.10956)
*Wentao Zhou,Xuweiyi Chen,Vignesh Rajagopal,Jeffrey Chen,Rohan Chandra,Zezhou Cheng*

Main category: cs.CV

TL;DR: 提出StereoWalker：在端到端导航基础模型中加入双目输入与中层视觉（深度、稠密跟踪），显著减少监督数据需求并提升在动态无结构场景的导航性能。


<details>
  <summary>Details</summary>
Motivation: 纯单目端到端导航模型（NFM）假设视觉能力可隐式涌现，但需要大量像素到动作监督，且在动态、无结构环境中受单目尺度-深度歧义限制，难以获得可靠的几何与运动理解。

Method: 在NFM框架中显式引入双目视觉以消除尺度-深度歧义，并融合现代中层视觉模块（深度估计、稠密像素跟踪）。同时构建大规模互联网双目视频导航数据集，利用自动动作标注进行训练。

Result: 使用仅1.5%的训练数据即可达到与SOTA相当的性能，使用全部数据时超越SOTA；双目输入在导航性能上优于单目。

Conclusion: 忽视中层视觉与仅依赖单目输入是低效的。将双目与中层视觉先验纳入端到端导航可显著提升数据效率与在动态场景中的鲁棒性。

Abstract: The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.
  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.

</details>


### [103] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 提出SceneMaker：解耦“去遮挡”和“3D对象生成”，并以统一姿态估计模型提升在严重遮挡与开放集场景下的几何质量与姿态精度；同时构建开放集3D场景数据集，实验在室内与开放集场景均优。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法在严重遮挡与开放集条件下，缺乏足够的去遮挡与姿态先验，导致几何质量差、姿态不准。需要一种能在开放集、多样遮挡情况下泛化良好的框架。

Method: 1) 解耦：将去遮挡模型从3D对象生成中独立出来。2) 去遮挡增强：利用通用图像数据集与专门收集的去遮挡数据集学习更多样的开放集遮挡模式。3) 统一姿态估计：在自注意力与交叉注意力中同时融合全局与局部机制，提高姿态精度。4) 数据集构建：搭建开放集3D场景数据集，扩展姿态模型的泛化。5) 实验：在室内与开放集场景上全面评测。

Result: 解耦框架在几何质量与姿态估计精度上均优于现有方法；在室内和开放集场景实验中表现突出。

Conclusion: 解耦去遮挡与3D生成、并结合统一全局-局部注意的姿态估计与开放集数据集，可显著提升在严重遮挡与开放集设置下的3D场景生成质量与姿态准确性；代码与数据已开源。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


### [104] [WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World](https://arxiv.org/abs/2512.10958)
*Ao Liang,Lingdong Kong,Tianyi Yan,Hongsi Liu,Wesley Yang,Ziqi Huang,Wei Yin,Jialong Zuo,Yixuan Hu,Dekai Zhu,Dongyue Lu,Youquan Liu,Guangfeng Jiang,Linfeng Li,Xiangtai Li,Long Zhuo,Lai Xing Ng,Benoit R. Cottereau,Changxin Gao,Liang Pan,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: WorldLens提出一个面向生成式世界模型的全谱基准与配套数据与评测代理，系统评估生成环境在视觉、几何、物理与可控性上的真实度，并以人类标注蒸馏的评测模型实现可扩展、可解释的打分。


<details>
  <summary>Details</summary>
Motivation: 当前生成式世界模型可合成“看起来逼真”的4D驾驶场景，但常在物理、行为或几何一致性上失真；领域缺乏统一、可比、与人类判断对齐的评测框架来衡量“看起来像”与“行为像”的差距。

Method: 提出WorldLens基准，从五个维度评估：生成、重建、动作跟随、下游任务与人类偏好，覆盖视觉真实、几何一致、物理可行与功能可靠；构建含评分与文本理由的WorldLens-26K人类标注视频数据集；基于该数据蒸馏出WorldLens-Agent评测模型，实现自动化、可解释评分，并与人类偏好对齐。

Result: 跨五维评测显示现有世界模型无一全面领先：纹理强的模型更易违背物理，几何稳健的模型在行为忠实度上不足；WorldLens-Agent可用数值与解释对模型进行一致性更高的打分。

Conclusion: WorldLens提供了统一生态（基准+数据集+代理）来衡量世界忠实度，推动评价从“外观真实”扩展到“行为真实”，为未来世界模型的标准化比较与改进奠定基础。

Abstract: Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.

</details>


### [105] [StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space](https://arxiv.org/abs/2512.10959)
*Tjark Behrens,Anton Obukhov,Bingxin Ke,Fabio Tosi,Matteo Poggi,Konrad Schindler*

Main category: cs.CV

TL;DR: StereoSpace 是一种将单目图像直接合成立体对的扩散式框架，只用视点条件建模几何，无需显式深度或扭曲。它在规范化的校正空间内端到端推断对应关系并补全遮挡，同时提出无需任何几何真值/代理的评测协议，以iSQoE与MEt3R作为核心指标，结果在结构与感知上均优于现有类别方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目转立体方法常依赖显式深度估计、光流/扭曲与修补，存在深度误差传播、非Lambert场景不鲁棒以及评测“泄漏”几何信息等问题。作者希望：1）用更泛化、可扩展的生成式方法直接从视点条件学习几何一致性与视差；2）建立不依赖任何测试时几何监督/代理的公平评测协议，突出对下游VR/3D观看体验相关的指标。

Method: 提出基于扩散模型的视点条件生成：在一个规范化的校正（rectified）立体空间中，以目标视点为条件生成右目图像，不显式估计深度或进行像素级扭曲。模型通过条件引导学习跨视图对应与遮挡补全，端到端训练。为评测，提出端到端协议：测试时不使用GT深度或代理几何；采用iSQoE衡量立体舒适度、MEt3R衡量几何一致性。

Result: 在多类基线（warp&inpaint、latent-warping、warped-conditioning）上，StereoSpace在iSQoE和MEt3R上均显著领先，生成具有清晰视差、在分层与非Lambert场景上鲁棒，细节锐利且遮挡处理更好。

Conclusion: 基于视点条件的扩散生成能够在无深度显式建模下实现高质量、可扩展的单目到立体合成；配套的无几何泄漏评测协议更贴近实际使用价值。该方向为深度无关的立体生成提供了有效范式。

Abstract: We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.

</details>
