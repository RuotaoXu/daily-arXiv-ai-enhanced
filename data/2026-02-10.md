<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 204]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 论文提出用于量化鞋印中“偶发特征”（磨损切痕等）稀有度的分层贝叶斯模型，通过潜在高斯模型与INLA高效推断，并引入空间变系数刻画鞋底纹路与偶发位置关系；在留出数据上性能更优，有助提升法庭证据力度评估的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 单凭鞋印匹配品牌/型号不足以排除大量同款鞋；“偶发特征”可能具有高度区分度，但缺乏稳健且可扩展的统计方法来量化其稀有度与证据强度。

Method: 构建分层贝叶斯框架：以潜在高斯模型表述偶发特征的空间分布；采用集成嵌套拉普拉斯近似（INLA）实现对大规模标注鞋印数据的高效推断；在模型中引入空间变系数以捕捉鞋底纹理/胎面图案与偶发位置之间的空间相关与异质性。

Result: 在留出测试数据上优于现有方法（更准确地估计偶发特征的分布与稀有度），展现出更高的预测性能与稳健性。

Conclusion: 该方法能高效、可扩展地量化偶发特征的稀有程度，并将胎面图案与磨损位置的关系纳入建模，从而提升法庭鞋印证据的量化评估与可靠性。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出一种基于归因的人类先验对齐方法，用人类标注的关注区域约束模型在训练中的证据归因，减少捷径依赖，提升准确率与决策可解释性，并在图像分类与MLLM GUI点击任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 监督学习常仅有类别标签，模型容易依赖与任务无关的捷径相关性，导致高准确但证据不可接受；而将人类先验（人类认为应依赖的区域/证据）融入训练可抑制捷径，但模型内部表征与人类感知不一致，使先验对齐困难。需要一种能在训练中显式曝露并约束模型决策证据的机制。

Method: 将人类先验编码为输入上的期望关注区域（如框/掩码）。采用高保真“子集选择式”归因方法在训练中获取模型当前用于决策的证据区域；当归因与先验偏离时，对“先验外归因”施加惩罚，形成带有归因约束的训练目标，推动模型把决策依据向先验区域迁移。方法适用于分类与自回归生成（点击决策）两类设定。

Result: 在图像分类和基于MLLM的GUI智能体点击任务上，加入人类先验对齐后，模型不仅任务准确率提升，而且归因更集中于人类标注的合理区域，表现出更好的决策合理性与解释一致性。

Conclusion: 通过在训练中引入基于归因的人类先验约束，可有效减少模型对非期望证据的依赖，实现性能与可解释性的双提升，对多模态与不同预测范式具有通用性。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [3] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: 提出工业多类型异常理解数据集MAU-Set与评测协议，并基于此构建含AMoE-LoRA的多模态大模型MAU-GPT，在多领域超越SOTA，提升异常理解与推理能力，助力可扩展自动化质检。


<details>
  <summary>Details</summary>
Motivation: 现有工业视觉异常检测受限于数据覆盖不足、异常模式多样导致泛化差、任务层级单一与评测不一致，难以满足大规模质检自动化需求。

Method: 1) 构建MAU-Set：覆盖多工业领域，设计从二分类到复杂推理的分层任务结构；2) 制定严格统一的评测协议，保障公平全面对比；3) 提出MAU-GPT：面向工业异常理解的多模态大模型，引入AMoE-LoRA，将“异常感知专家”和“通用专家”的低秩适配进行统一与协同，以提升对多种缺陷类别的理解与推理。

Result: 在多个工业领域的广泛实验中，MAU-GPT在所有任务与领域上稳定优于既有SOTA，体现更强的跨缺陷类别泛化、理解与推理能力。

Conclusion: 高覆盖数据集与规范评测为工业异常理解提供坚实基础；结合AMoE-LoRA的MAU-GPT有效统一专家适配，显著提升模型在多类型工业缺陷上的性能，为可扩展、自动化工业质检展示出强大潜力。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [4] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: RetSAM提出一个面向眼底照相的通用分割与量化框架，统一多目标分割并自动生成30+标准化生物标志物，在17个公开数据集上显著超越现有方法，促进大规模眼科与全身疾病相关性研究。


<details>
  <summary>Details</summary>
Motivation: 眼底成像便捷且可量化，但缺乏多标签公开数据与统一的“分割到量化”流程，限制了对结构与血管表型与疾病关系的规模化研究。

Method: 构建RetSAM：在20万+眼底图上以多阶段训练，覆盖三类任务，分割5种解剖结构、4种表型模式、20+病变类型；将分割结果标准化为30+生物标志物，表征结构形态、血管几何与退行性改变；在多域设备与人群上评测泛化。

Result: 在17个公开数据集上获得最优或领先表现，平均DSC较之前最佳提升3.9个百分点，在多任务基准上最高提升15个百分点；表现对不同人群、成像设备与临床场景具有良好泛化。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的量化表型，为糖网、黄斑变性、青光眼、病理性近视等疾病的系统相关性分析与大规模眼科研究提供基础。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [5] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: 提出CR-VLM，一种通过激活操控实现可配置拒绝的VLM方法，兼顾安全性与实用性，减少过度/不足拒绝，并可跨模型与数据集稳定工作。


<details>
  <summary>Details</summary>
Motivation: 现有VLM拒绝机制“一刀切”，无法根据用户需求与上下文灵活调节，常出现过拒或欠拒；需要一种可按场景/策略调节的拒绝能力，实现用户自适应的安全对齐。

Method: 基于激活操控的三模块框架：1) 教师强制提取“可配置拒绝向量”，放大拒绝信号以实现策略可调；2) 门控机制，在保留就绪域内查询的接受能力同时抑制过拒；3) 反事实视觉增强模块，使视觉表征与拒绝需求对齐。

Result: 在多数据集与多种VLM上实证，CR-VLM实现有效、效率高、鲁棒的可配置拒绝，相比基线更少过拒/欠拒，且具可扩展性。

Conclusion: CR-VLM为VLM提供可配置、稳健的拒绝能力，推动面向用户自适应的安全对齐，适用于不同模型与场景。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [6] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: 提出Vectra：首个面向电商图像内机器翻译(IIMT)的、无参考、由多模态大模型驱动的视觉质量评估框架，含可解释14维度评分、DAR空间缺陷量化、百万级数据集与4B参数模型，相关性SOTA并优于主流MLLM。


<details>
  <summary>Details</summary>
Motivation: 现有IIMT研究偏重翻译文本评估，忽视视觉呈现质量；在信息密集商品图与多模态缺陷场景下，参考式指标(SSIM/FID)难解释，模型裁判法缺乏细粒度、贴合业务的奖励信号。需要一个可解释、细粒度、与人类感知高度相关且可用于对齐的视觉质量评估体系。

Method: 构建三部分：1) Vectra Score：将视觉质量分解为14个可解释维度，并用空间感知的缺陷区域占比(DAR)降低标注歧义；2) Vectra Dataset：从110万真实商品图经多样性采样，包含2K基准集、3万条推理式标注用于指令微调、3.5K专家偏好用于对齐与评测；3) Vectra Model：4B参数MLLM，同时输出量化评分与诊断性推理。

Result: 在与人工排序的一致性上达到SOTA；在评分能力上优于领先MLLM（包括GPT-5与Gemini-3）。

Conclusion: Vectra为电商IIMT提供首个无参考、可解释、可对齐的视觉质量评估方案，数据与模型将开源，能更好指导系统优化与模型训练。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [7] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 提出一种混合CNN（MobileNetV3-Large + EfficientNetB0 + MLP）用于孟加拉纸币识别，并构建含受控与真实场景的新数据集，融合四个外部数据集，取得最高97.95%（受控）、92.84%（复杂背景）、94.98%（合并）准确率，并用五折与七种指标评估，结合LIME/SHAP增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 视障人士在识别纸币时依赖他人，易受欺骗与剥削；现有模型泛化差、对复杂背景/真实场景鲁棒性不足，缺乏覆盖本地化（孟加拉）纸币的高质量数据集与可在低算力设备上部署的高性能模型。

Method: 1) 构建新的孟加拉纸币数据集，涵盖受控与真实场景；2) 融合四个额外数据集（含公共基准）以提升多样性与泛化；3) 设计混合CNN：并行/级联地利用MobileNetV3-Large与EfficientNetB0做高效特征提取，随后用轻量MLP分类；4) 用五折交叉验证与七项指标（Accuracy/Precision/Recall/F1/Kappa/MCC/AUC）全面评估；5) 引入LIME与SHAP解释模型决策。

Result: 在受控数据集上97.95%准确率，在复杂背景下92.84%，在合并全部数据集上94.98%；模型在多指标上表现稳定，并在可解释性分析中展示了对票面关键区域的关注。

Conclusion: 所提混合网络在效率与精度间取得良好平衡，适合资源受限设备与辅助技术场景；多数据集训练提升了对真实环境的泛化。未来可扩展到更多币种/伪钞检测与更强的域自适应。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [8] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 论文在IMC2025任务下，探索对图像嵌入施加各向同性高斯约束的LeJEPA风格表示，以提升场景发现与位姿估计稳健性；通过三套递进管线的实证对比，显示高斯约束可在混合与含歧义数据中改进聚类一致性和位姿可信度，优于启发式基线。


<details>
  <summary>Details</summary>
Motivation: 无监督从非结构化、多场景、含大量干扰的图像集中重建3D场景极具挑战；IMC2025同时要求场景发现与相机位姿估计，强调真实噪声与混合内容。作者受LeJEPA启发，假设在表示空间施加理论动机的分布约束（高斯）或能提高聚类与SfM稳健性，填补自监督表征与实际SfM流程间的落差。

Method: 提出三条递进式管线，最终方案在学习到的图像嵌入上施加各向同性高斯约束（LeJEPA风格），以鼓励分布可分与尺度/各向性受控；随后以这些嵌入进行场景聚类（场景发现）并驱动相机位姿估计（SfM），与启发式驱动基线对比，主要做实证评测而非提供新的理论保证。

Result: 在IMC2025数据上，高斯约束嵌入提升了场景分离度与聚类一致性，并在视觉歧义强、混合内容多的设置下，得到更可信/稳健的位姿估计；总体优于启发式基线。

Conclusion: 受理论动机的高斯约束表征在实践中有助于无监督多场景重建的关键步骤（聚类与位姿估计），为连接自监督学习与实际SfM流程提供了可行方向，但贡献主要为经验性验证而非新理论。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [9] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出XAI-CLIP：用多模态视觉-语言嵌入引导ROI的扰动式解释框架，为医学图像分割生成更清晰、边界感知的显著图；在FLARE22与CHAOS上，相比传统扰动法，运行时降至40%（最高降60%）、Dice提升44.6%、IoU提升96.7%，解释更解剖一致、伪影更少。


<details>
  <summary>Details</summary>
Motivation: Transformer分割虽性能优于CNN，但可解释性弱，难以临床落地。现有XAI（梯度显著图、扰动法）往往计算昂贵、需多次前向、且解释噪声大且与解剖结构不匹配，缺乏临床可信度。

Method: 构建XAI-CLIP：利用多模态视觉-语言模型（如CLIP）对解剖部位进行语言引导的区域定位（ROI），将该ROI用于引导分割模型的解释；在这些语义一致的区域内施加有针对性的、区域感知的扰动，生成边界敏感的显著图。通过语言信息与分割网络输出联合，减少无关区域搜索，显著降低前向次数与计算成本。

Result: 在FLARE22与CHAOS数据上，XAI-CLIP相对传统遮挡/扰动式解释：运行时间减少最高60%，Dice提升44.6%，IoU提升96.7%；定性上，显著图更干净、边界更清晰、与解剖结构更一致、伪影更少。

Conclusion: 将多模态视觉-语言表示融入扰动式XAI，可在维持/提升分割解释质量的同时显著降本增效，得到更透明、可临床部署的医学图像分割解释系统。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [10] [Deep Learning Based Multi-Level Classification for Aviation Safety](https://arxiv.org/abs/2602.07019)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Jonathan King*

Main category: cs.CV

TL;DR: 提出一种基于相机的CNN框架，识别鸟种并估计群形与群规模，为航迹预测与风险评估提供输入，以提升机场防鸟击安全。


<details>
  <summary>Details</summary>
Motivation: 现有依赖鸟类雷达的防鸟击系统难以区分鸟种；而不同鸟种在飞行行为与高度偏好上差异显著，影响风险评估与路径预测，因此需要能在实时视觉下进行物种与群体特征识别的方法。

Method: 构建面向自治视觉检测的CNN分类体系：1) 物种识别CNN；2) 群体形态（编队类型）分类CNN；3) 群规模估计CNN。将物种识别结果作为物种特定预测模型的输入，用于更准确的飞行轨迹预测；群形与规模作为补充特征提升安全相关判断。

Result: 框架层面实现了多任务CNN分类：可识别鸟种、区分编队类型并估计群规模，从而为后续的物种特定轨迹预测与风险评估提供关键输入。摘要未给出具体精度或对比数值。

Conclusion: 通过将视觉CNN引入机场鸟击防控流程，并结合物种、群形与规模信息，可更好地表征飞行行为与轨迹离散度，进而改进风险评估（包括多鸟动能叠加导致的潜在损害）。

Abstract: Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.

</details>


### [11] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 论文通过分析开放权重VLM的表示几何，提出用“概念向量”解释与操纵模型在多目标视觉任务中的错误，并用向量重叠度量来预测失误模式。


<details>
  <summary>Details</summary>
Motivation: VLM在多物体场景中常出现幻觉、错误匹配等失误，类似人类“绑定问题”，但其在人工系统中的内部机制尚不清楚。作者希望提供可验证、可量化的机制解释。

Method: 选取开源VLM（Qwen、InternVL、Gemma），提出并比较数种提取潜在“概念向量”的方法；通过干预（steering）在简化与自然任务中注入或抑制概念以操纵模型感知（如将“红花”感知为“蓝色”）；度量不同概念向量的几何关系（如夹角/重叠）并与特定错误模式做相关分析。

Result: 提取的概念向量可稳定操纵模型输出，证明其语义有效；不同概念向量之间的几何重叠程度与特定错误（如颜色绑定错误、相似物体混淆）显著相关，相关性为理解失败提供量化依据。

Conclusion: VLM的多物体失败可由内部表示几何解释：概念向量的重叠驱动特定失误。基于表示几何的分析与干预为诊断、预测和缓解VLM视觉失败提供了可操作框架。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [12] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 提出Fixed-frame Modality Gap理论，分解跨模态表示错位为稳定偏置与各向异性残差；据此给出无需训练的ReAlign对齐流程（Anchor/Trace/Centroid），并据此构建可扩展预训练范式ReVision，用未配对大规模数据替代昂贵图文对，实现MLLM高效扩展。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习虽对齐了视觉与语言，但仍存在“模态鸿沟”：同义语义在不同模态中的嵌入呈系统性偏移。既有方法常假设各向同性、简化几何，难以在大规模场景适用。需要更精确刻画几何形状并将其用于高效扩展MLLM。

Method: 1) 固定参考系下提出“固定框架模态间隙”理论，将间隙分解为稳定偏置与各向异性残差；2) 基于该建模提出训练-free的ReAlign：利用大规模未配对数据的统计，将文本表示对齐到图像分布，分三步：Anchor、Trace、Centroid Alignment，显式校正几何错位；3) 在此之上提出ReVision预训练范式：在视觉指令微调前，把ReAlign融入预训练，使模型先从未配对文本学习视觉表示分布，无需大量高质图文对。

Result: 通过统计对齐的未配对数据，能够在无大规模图文对的条件下有效学习跨模态对齐，并为MLLM提供可扩展的性能提升与更高训练效率（摘要未给出具体数值）。

Conclusion: 精确刻画并校正模态几何错位（稳定偏置+各向异性残差）可支持训练-free对齐与可扩展预训练。ReAlign+ReVision证明未配对数据在统计对齐后可替代昂贵的配对数据，为高效扩展MLLM提供可行路径。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [13] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: 提出Fair Context Learning（FCL），一种面向VLM的测试时自适应（TTA）方法，摒弃熵最小化，通过公平性约束校准文本上下文以缓解共享证据偏置，在多种域移与细粒度基准上取得与SOTA相当的鲁棒提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLM（如CLIP）在分布偏移下性能显著下降。主流基于提示的TTA依赖熵最小化，易被类别间共享的视觉特征误导，放大虚假相关并产生过度自信错误。需要一种不依赖熵最小化、能处理共享证据偏置的自适应策略。

Method: 提出“加性证据分解”假设，将适配分为两步：1）基于增强的探索，识别可能的类别候选；2）公平性驱动的校准，对文本上下文进行调整，使模型对共享视觉证据的敏感度在候选类别间被平衡，避免对部分特征的“痴迷”，从而在不降低熵的前提下校准文本嵌入。方法以情景式（episodic）TTA范式运行。

Result: 在多种域移与细粒度识别基准上进行广泛实验和理论动机验证，FCL在无需熵最小化的情况下实现与当前SOTA TTA方法具有竞争力的适配性能与鲁棒性。

Conclusion: 通过显式建模并校准共享证据偏置，FCL有效提升VLM在分布移位下的稳健性，提供了脱离熵最小化的可行TTA路径，并在多基准上展现出竞争性表现。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [14] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: 比较CNN与ANFIS增强模型在多个数据集与攻击下的鲁棒性：ResNet18-ANFIS更抗扰动，VGG-ANFIS反而更弱，说明神经模糊增强并非通用灵丹。


<details>
  <summary>Details</summary>
Motivation: CNN虽性能强，但可解释性差且易受对抗攻击；神经模糊混合模型（如以ANFIS替换全连接分类头）有望提升可解释性，但其鲁棒性影响尚不清楚。

Method: 以MNIST、Fashion-MNIST、CIFAR-10、CIFAR-100为基准，对比三类基础CNN（ConvNet、VGG、ResNet18）与其将全连接分类器替换为ANFIS后的变体。在干净样本与两类攻击（基于梯度的PGD与无梯度的Square）下评测精度与鲁棒性。

Result: ANFIS集成并不稳定提升干净精度；其鲁棒性效果依赖网络结构：ResNet18-ANFIS在对抗鲁棒性上优于基线，而VGG-ANFIS常劣于原模型，ConvNet表现不一。

Conclusion: 神经模糊增强可在某些架构（如ResNet18）提升鲁棒性，但并非普适；是否采用需结合具体架构与任务权衡精度、可解释性与鲁棒性。

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [15] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: UNIKIE-BENCH 提出一个统一基准，系统评估大规模多模态模型在真实文档关键字段抽取（KIE）上的能力，涵盖受约束与开放两类任务；实验显示现有15个模型在多样版式、长尾字段与复杂场景下显著掉点，暴露版面感知与定位推理不足。


<details>
  <summary>Details</summary>
Motivation: 现有文档KIE受版面多样、图像质量参差与任务需求差异制约；尽管LMM能端到端从图像抽取信息，但缺乏覆盖真实多场景、可比性强的系统化评测基准。

Method: 构建统一评测基准UNIKIE-BENCH，包含两条赛道：1）受约束类别KIE（给定场景化schema）；2）开放类别KIE（提取文档中显式存在的任意关键信息）。对15个SOTA LMM进行大规模实验，对比不同文档类型、布局复杂度、字段长尾与多样schema设定下的表现。

Result: 在多样schema、长尾关键字段与复杂布局下，模型性能显著下降；不同文档类型与应用场景之间存在明显性能差异。

Conclusion: 当前LMM在KIE上仍面临定位与版面感知推理的核心瓶颈；UNIKIE-BENCH为后续模型改进与更贴近应用的评测提供了标准化平台与数据。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [16] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: OMNI-Dent提出一种结合临床推理的VLM牙科诊断框架，用多视角手机照片在无专科微调下进行逐牙评估，数据高效、可解释，旨在早期辅助识别异常并提示就医需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法多把牙科诊断当作纯视觉识别，缺乏与牙医一致的结构化临床推理，依赖大量专家标注，且在真实多样成像条件下泛化差；许多人难以及时获得专业口腔评估。

Method: 构建OMNI-Dent：以多视角智能手机照片为输入，将牙科专家的诊断启发式/规则嵌入到流程中，利用通用VLM的视觉-语言能力，通过提示工程和引导策略在无需牙科专向微调的前提下实现逐牙级别评估，并提供可解释的推理输出。

Result: 在不依赖大规模牙科特定标注和专门微调的情况下，框架能在复杂、非标准化拍摄条件下完成逐牙级别的可解释评估，支持发现潜在异常并给出就医建议（摘要未给出具体数值）。

Conclusion: OMNI-Dent作为早期辅助工具，面向缺乏临床影像条件与线下资源的人群，利用VLM与专家启发式结合，实现数据高效、可解释、可迁移的牙科初筛与决策支持，帮助决定是否需要专业就诊。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [17] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: 提出COMBOOD：一种将最近邻与马氏距离信号融合的半参数无监督OOD检测框架，兼顾近OOD与远OOD，在线性可扩展前提下在OpenOOD v1/v1.5及文档数据集上优于SOTA且多处显著。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在不同情景下各有短板：非参数最近邻对近OOD较敏感但对远OOD可能不足；马氏距离在远OOD有效但在近OOD表现欠佳。实际应用需要在推理时可靠识别各类OOD并具备可扩展性。

Method: 在图像识别的嵌入空间中，同时计算样本到训练集的最近邻距离与基于类别（或整体）分布估计的马氏距离，设计半参数融合策略生成单一置信分数，用于判定是否为OOD。方法可配合多种特征提取器，融合权重或规则以提升近/远OOD的鲁棒性，并保证计算随嵌入维度线性扩展。

Result: 在OpenOOD v1与v1.5（含近OOD和远OOD）以及文档数据集上，COMBOOD在准确率上整体超越当前SOTA，且多数数据集上的提升具有统计显著性。

Conclusion: 将非参数与参数距离信号融合的半参数框架能同时覆盖近/远OOD，效果稳定且高效，具备实际落地潜力。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [18] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: 该论文发布并基准化了首个大规模管道磁漏(MFL)伪彩图像目标检测数据集PipeMFL-240K（24万图像/19万标注），揭示现有检测器在极端长尾、小目标与类内差异大的场景下仍有明显性能短板。


<details>
  <summary>Details</summary>
Motivation: MFL是油气管道完整性评估的核心无损检测手段，但缺乏公开、规模化的数据集与统一基准，导致方法难以公平对比、复现与落地。作者希望通过构建真实工业复杂度的数据集，推动算法创新与可复现研究。

Method: - 构建PipeMFL-240K：从11条、约1480 km管道采集MFL伪彩图像，整理12类缺陷/目标，提供191,530个高质量BBox标注。
- 设计基准评测：选取多种SOTA目标检测器进行广泛实验，分析长尾分布、小目标占比高、类内变化大的挑战对性能的影响。

Result: 基线实验显示：即便是当前SOTA检测器，在MFL数据的长尾分布、极小目标与显著类内差异条件下仍表现欠佳，存在较大改进空间；数据集稳定、可复现实验对比。

Conclusion: PipeMFL-240K作为首个公开且同等规模与范围的MFL管道检测数据集/基准，为高效诊断与运维规划奠定基础，可促进针对长尾与小目标的算法研究，推动MFL管道完整性评估的标准化与可复现性发展。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [19] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 作者提出VLRS-Bench，一个专注于复杂遥感多模态推理的基准，覆盖认知、决策与预测三维度的14类任务与最多8期时序，共2000个长篇问答；基于RS先验与专家知识构建。实验显示现有最先进MLLM在该基准上存在明显瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准偏重感知（目标识别、场景分类），缺乏对多模态大模型复杂推理能力的系统评测，制约了面向高认知需求RS应用的发展。

Method: 构建Vision Language ReaSoning Benchmark（VLRS-Bench）：围绕认知、决策、预测三大维度，覆盖14项任务、最长8个时间相位的时序情景；形成2000个平均71词的问答对。采用含遥感特定先验与专家知识的标注与生成流程，确保地理空间真实性与推理复杂度；随后用多种SOTA MLLM进行评测。

Result: 在VLRS-Bench上，多数现有SOTA多模态大模型在复杂推理、时序理解与任务迁移等方面表现不佳，暴露出显著性能瓶颈。

Conclusion: VLRS-Bench填补了RS复杂推理评测空白，为推动遥感领域多模态推理研究提供了标准化基准与诊断工具，未来需针对时序、多步与决策型推理进行方法改进。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [20] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: 提出ShapBPT：在图像的二叉分割树（BPT）上计算层级Shapley归因，以获得更快、更符合图像形态学结构的像素级可解释性。实验与用户研究显示优于现有XCV方法。


<details>
  <summary>Details</summary>
Motivation: 现有层级Shapley用于视觉任务未利用图像的多尺度结构，收敛慢、与真实形态学特征对齐差；缺少数据感知（data-aware）的层级来提升视觉可解释性与效率。

Method: 构建以图像形态为导向的二叉分割树（BPT）作为层级，基于Owen层级Shapley公式在该多尺度树上分配Shapley系数，实现对区域/像素的归因；通过数据感知的分割使重要区域优先、减少无关组合评估，降低计算量。

Result: 在多项实验中，ShapBPT产生的归因更符合图像结构、效率更高；并通过20人用户研究，显示人类更偏好ShapBPT给出的解释。

Conclusion: 将层级Shapley与图像数据的BPT相结合，实现语义一致、形态对齐且高效的视觉可解释性；弥补了XCV中对数据感知层级的空白。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [21] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: 提出ECHWR训练框架，通过临时辅助分支与双重对比学习，在不增加推理开销的前提下提升基于IMU的在线手写识别精度；在OnHW-Words500上显著降低CER（独立/依赖写手分别降至+7.4%/+10.4%）。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的IMU手写识别可带来隐私与低时延优势，但受限于内存与算力，难以采用大模型或复杂推理结构；需要在不增加推理成本的情况下提升特征表征与识别精度，且需适应不同写手风格的分布偏移。

Method: 在训练阶段引入临时辅助分支，使传感器序列特征与文本语义嵌入对齐；采用双重对比目标：1) 批内对比损失实现总体跨模态对齐；2) 新的“基于错误”的对比损失，构造合成的困难负样本以区分正确信号与近似错误样本；训练结束后丢弃辅助分支，推理仍使用原高效架构。

Result: 在OnHW-Words500数据集上，相比SOTA，字符错误率在写手独立划分上最多下降7.4%，在写手依赖划分上最多下降10.4%；消融显示不同挑战需要特定架构/目标组合，其中基于错误的对比损失对未见写风具有显著效果。

Conclusion: ECHWR在不增加推理成本的前提下，通过跨模态对齐与错误增强对比学习，显著提升IMU手写识别，尤其提升对未见写手风格的鲁棒性；方法适合边缘部署，兼顾效率与精度。

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [22] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 论文对大型视频Transformer编码器内部是否显式/因子化表示物理变量进行首个可解释性研究，发现在中间层出现“物理涌现区”，物理信息在此后短暂峰值后衰减；速度/加速度等标量较早可读出，方向在涌现区出现且以高维环形分布的人群编码表示。结论：模型采用分布式而非因子化表示，但足以支撑物理预测。


<details>
  <summary>Details</summary>
Motivation: 直觉物理任务上视频世界模型表现强，但尚不清楚其内部是否像经典物理引擎一样因子化（显式变量）表示物理量，或以任务特定、分布式方式隐式表示。缺乏直接探查大规模视频编码器物理表示的可解释性研究。

Method: 对多种基于编码器的视频Transformer进行系统可解释性分析：1) 分层探针（layerwise probing）评估各层可读出的物理变量；2) 子空间几何分析刻画物理信息的组织结构；3) patch 级解码定位物理信息的空间分布；4) 有针对性的注意力消融测试依赖性与因果作用。并将运动分解为速度、加速度、方向等显式变量以比较其可获取性。

Result: 跨架构发现清晰的中间深度转折点“物理涌现区”，物理变量在此处变得可获取，之后短暂达到峰值并向输出层衰减。标量（速度、加速度）从早期层即可读出；方向信息仅在涌现区变得可获取，且以高维、具有圆周几何的人群（population）编码表示，需要协同多特征干预才能控制。

Conclusion: 现代视频模型内部并非采用类似经典引擎的因子化物理变量表示，而是以分布式、高维结构编码物理信息；尽管不因子化，但该表示仍足以实现物理预测。提出“物理涌现区”作为理解视频模型物理表征形成与衰减的关键阶段。

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [23] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: 提出Neural Sentinel：用单一VLM前向一次同时完成车牌识别、州/省分类与车辆属性抽取，较传统多阶段ALPR显著更准、更快、更简。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR采用检测+OCR多阶段管线，误差累积、延迟高、工程复杂；希望用统一模型降低复杂度并获得多任务能力与更好泛化。

Method: 以PaliGemma-3B为基座，使用LoRA微调为视觉问答式VLM：对车辆图像一次前向回答多问题（车牌、州/省、属性等）。引入HITL持续学习：收集用户纠正并用经验回放防灾难遗忘，回放配比原训练数据:纠正样本=70:30。评估延迟、校准度，并测试零样本辅助任务能力。

Result: 车牌识别准确率92.3%，较EasyOCR提升14.1个百分点、较PaddleOCR提升9.9个百分点；平均推理延迟152ms；ECE=0.048（置信度良好校准）；零样本完成颜色(89%)、安全带(82%)、乘员计数(78%)等任务。

Conclusion: 统一VLM架构在真实收费站数据上优于传统流水线：更高准确率、更低架构复杂度，并具备多任务与零样本泛化潜力；表明VLM-first是ALPR的新范式。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [24] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 提出无标记神经导航：用低成本可见光/红外立体与深度相机融合面部几何建模，替代昂贵硬件与头部标记；在50名受试者上较传统有标记系统实现中位误差2.32 mm与2.01°，达经颅磁刺激所需精度，优于既往无标记方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经导航依赖头部标记，需人工配准、易位移、影响舒适度且硬件昂贵，限制了经颅磁刺激等应用的普及；需要一种低成本、舒适且稳定的定位方案。

Method: 采用可见光与红外相机（含立体与深度）采集多模态人脸数据，结合面部几何算法建模与传感器融合实现实时头部/目标位姿追踪；与传统有标记系统对照评估误差。

Result: 在50名受试者实验中，最佳无标记算法相对有标记基线的中位位置误差2.32 mm、角度误差2.01°；较以往无标记方法显著改进；多传感器数据融合有望进一步提升精度。

Conclusion: 所提无标记神经导航可在保证TMS所需精度下，降低成本与部署复杂度，提升受试者舒适度并扩大临床与科研可及性；未来可通过进一步的传感器融合继续提升精度。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [25] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: 提出RECITYGEN：基于潜在扩散模型与交互式语义分割的参与式街景生成工具，支持文本提示与用户交互，用于城市更新场景共创；在北京试点显示可更好贴合公众偏好，尽管仍有局限。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计忽视公众参与，导致设计愿景与现实脱节；新兴数字工具虽提升参与度，但仍缺少让普通人低门槛直观表达空间设想的方式。

Method: 将最先进的潜在扩散模型与交互式语义分割结合，开发RECITYGEN：用户通过文本提示与分割编辑交互，生成多样化的城市街景变体；在北京的城市更新项目中进行试点应用与反馈收集。

Result: 试点中，公众利用该工具提出改造建议；生成结果在偏好上与公众意向较一致，展现出较好的可用性与参与促进效果，但作者也指出存在一定限制。

Conclusion: RECITYGEN能降低设计生成门槛，促进更动态、包容的城市规划流程；尽管有局限，但其潜力表明参与式城市设计正向数据驱动与人机共创方向转变；源码已开源于GitHub。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [26] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: FADE提出一种面向文本到图像扩散模型的高效“机器遗忘”方法，通过两阶段流程在保持总体生成能力的同时有效擦除特定数据/概念影响，达到SOTA的遗忘-保留权衡。


<details>
  <summary>Details</summary>
Motivation: 法规与负责任AI要求能从已训练模型中删除特定数据或概念的影响；扩散模型遗忘难在计算开销大、且易伤及无关概念。作者希望以低成本、可控的方式实现选择性遗忘，同时维持保留集性能与可部署性。

Method: 两阶段FADE：1）参数定位+稀疏LoRA适配——用基于梯度的显著性识别与“遗忘集”最相关的参数，只通过稀疏LoRA进行局部更新，确保轻量与可逆；2）自蒸馏——用用户定义的替代（surrogate）概念覆盖被忘概念，同时在保留数据上保持行为，通过蒸馏损失平衡遗忘与保留。生成的adapter可在运行时合并/移除。

Result: 在UnlearnCanvas基准上取得SOTA的概念擦除与保留性能；在Imagenette、LFW、AtharvaTaras狗品种、SUN Attributes等进行消融，显示对遗忘-保留权衡的细粒度控制与跨域稳健性。

Conclusion: FADE能在扩散式图像生成中实现高效、可逆、内存友好的选择性遗忘，兼具强概念擦除与高保留度，适合生产部署与多域应用。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [27] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出一个在钢铁废钢接收环节用计算机视觉评估污染含量并分类废钢类型的管线：以车厢级回归估计污染百分比，结合MIL与多任务学习，达到MAE≈0.27、R2≈0.83；多任务设置同时实现分类F1≈0.79，并在近实时工作流中联动检测、推理、人工复核与主动学习闭环，提升安全与一致性。


<details>
  <summary>Details</summary>
Motivation: 人工目检废钢非金属夹杂既主观又危险，且影响能耗、排放与安全；需要可量化、可集成到接收与熔炼计划中的客观自动化评估方法。

Method: 将污染评估建模为车厢级回归问题，利用序列图像并采用多实例学习（MIL）聚合帧/段信息；同时用多任务学习（MTL）在共享特征上联合进行污染回归与废钢类型分类。系统层面：磁铁/车厢检测切分时间层，版本化推理服务输出车厢级估计与置信度；操作员带结构化覆写，纠正与不确定样本进入主动学习持续改进。

Result: MIL取得最佳污染估计性能：MAE 0.27、R2 0.83；MTL方案在回归MAE 0.36的同时，分类达到F1 0.79。实现近实时在线部署并融入接收流程。

Conclusion: 该管线降低主观性与安全风险，提供可置信的车厢级污染估计与废钢分类，易于融入验收与熔炼计划，并通过主动学习实现持续性能提升。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [28] [Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine](https://arxiv.org/abs/2602.07064)
*Minghao Han,Dingkang Yang,Yue Jiang,Yizhou Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: OmniFysics提出一个紧凑的全模态（图像/音频/视频/文本）理解与生成模型，通过专门的物理知识数据引擎与分阶段训练，将显式物理属性与跨模态线索注入模型，显著提升物理相关任务表现，同时在通用多模态基准上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有全模态大模型对物理世界的理解脆弱：关键物理属性（质量、材质、受力、约束等）在视觉上易混淆，且在网络数据中稀疏、标注不可靠。因此需要构建能显式注入物理知识并强调跨模态物理线索的数据与训练机制，以提升模型的物理推理与泛化能力。

Method: 1) 物理数据引擎：
- FysicsAny：从精选原型库分层检索，将显著物体映射到已验证的物理属性；再用物理定律约束进行验证与改写生成指令-图像监督。
- FysicsOmniCap：对网路视频进行音视一致性过滤，蒸馏得到突出跨模态物理线索的高保真视频-指令对。
2) 模型训练：分阶段多模态对齐与指令微调；文本到图像生成采用潜空间flow matching；引入“意图路由器”仅在需要时激活语音/图像生成能力。

Result: 在通用多模态基准上达到具有竞争力的成绩；在面向物理的评测任务上显著优于现有方法，证明显式物理知识注入与跨模态物理蒸馏的有效性。

Conclusion: 通过构建以显式物理属性与物理一致性为核心的数据与训练流程，OmniFysics增强了全模态模型的物理理解与生成能力；该框架可作为提升多模态物理推理的通用范式，并兼顾生成质量与触发可控性。

Abstract: Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.

</details>


### [29] [Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework](https://arxiv.org/abs/2602.07065)
*A. N. Maria Antony,T. Richter,E. Gladilin*

Main category: cs.CV

TL;DR: 提出一种端到端深度学习框架，从图像序列同时估计连续介质位移场与材料可压缩性，较传统迭代配准+FEM/FDM更快更准，并能在配准局部偏差下仍准确推断可压缩性，因为模型学会了如涡量等高阶场特征。


<details>
  <summary>Details</summary>
Motivation: 工程与生医场景常需从光学观测无创估计材料力学性质；传统非接触方法依赖非刚性配准与本构-数值求解（FEM/FDM），计算开销大、难以高通量处理。

Method: 构建两个深度神经网络：一用于图像配准（估计位移场），一用于材料可压缩性估计；端到端训练于参考数据，使第二网络可在配准存在局部误差时仍鲁棒；模型利用更高阶向量场特征（如涡量）而非仅局部位移。

Result: 与传统迭代方法相比，所提出框架在效率与精度上均有提升；实验表明即便配准预测与参考位移场存在显著局部偏差，模型仍能准确推断材料可压缩性。

Conclusion: 端到端深度学习可在非接触材料探测中替代耗时的配准+数值求解流程，通过利用高阶场特征提升鲁棒性与准确性，适合高通量应用。

Abstract: Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.

</details>


### [30] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Bird-SR 是一种双向奖励引导的扩散式超分辨率框架，通过基于偏好优化的奖励反馈学习（ReFL），结合合成对和真实低清图，在扩散轨迹的不同阶段分别优化结构保真与感知质量，并用相对优势奖励与语义对齐正则抑制奖励黑客，配合动态保真-感知权重，实现在真实场景中既保结构又高感知的超分。


<details>
  <summary>Details</summary>
Motivation: 扩散超分能生成细节但在真实低清图上易失效，根源是用合成配对数据训练带来的分布偏移；同时，直接用奖励驱动的感知优化会破坏结构并引发奖励黑客。需要一种方法：既利用合成对保障结构，又能利用真实低清图提升真实感知质量，并在训练中抑制奖励滥用。

Method: 提出 Bird-SR：将超分表述为扩散轨迹级的偏好优化（ReFL）。- 早期扩散步：在合成 LR-HR 对上进行直接监督，强化结构保真，并因结构层面分布差距较小而可迁移到真实图。- 后期扩散步：对合成与真实 LR 的采样结果施加质量导向奖励进行感知优化。- 防奖励黑客：对合成样本在“相对优势”空间计算奖励（相对其干净真值有界），对真实样本加入语义对齐约束。- 动态权重：随扩散步数从保真侧逐步转向感知侧，实现结构-感知的平衡。

Result: 在多个真实场景超分基准上，Bird-SR 在感知质量上持续优于现有 SOTA，同时保持更好的结构一致性。

Conclusion: 通过双阶段（早保真、后感知）的奖励引导与防黑客机制，并联合合成与真实数据，Bird-SR 能有效缓解分布偏移，在真实超分中实现兼顾结构与感知的提升。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [31] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: 提出MosaicThinker：在设备端通过把多帧视频的碎片化空间信息整合为全局语义地图，并用视觉提示引导小型VLM在该地图上进行推理，从而显著提升跨帧空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM缺乏三维空间知识与跨帧整合能力，导致在机器人操作/动作规划等需要空间关系理解的任务上表现薄弱；需要在资源受限设备上提升跨帧空间推理。

Method: 推理时计算（无需再训练）：从多帧视频提取并融合物体与空间线索，构建统一的全局语义地图；将该地图以视觉提示形式喂给小型VLM，引导其在地图上进行空间关系推理，适配嵌入式/设备端场景。

Result: 在多种类型与复杂度的跨帧空间推理任务上，小型VLM在资源受限设备上的准确率大幅提升。

Conclusion: 通过语义地图聚合与视觉提示驱动的推理时计算，可显著增强设备端小型VLM的跨帧空间推理能力，为具身AI的操控与规划提供有效支持。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [32] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: 提出WorldEdit数据集与评测集WorldEdit-Test，面向“因果/世界知识驱动”的隐式图像编辑，并以两阶段训练+因果验证奖励微调模型（如Bagel），显著提升隐式指令遵循与知识合理性，逼近GPT-4o与Nano-Banana表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型擅长显式指令（如属性、风格、姿态），但对隐式指令（描述变化原因、不直接给出结果）表现不佳，因其采用统一编辑策略、缺少世界知识与因果推理能力；缺乏相应训练与评测资源。

Method: 构建WorldEdit数据集：含高质量编辑样本与与真实因果逻辑对齐的复述（paraphrased）隐式指令；提供WorldEdit-Test评测。训练上采用两阶段框架微调开源编辑模型（如Bagel），并引入因果验证奖励（causal verification reward）以强化因果一致性与知识合理性。

Result: 在隐式因果编辑任务上，所提出的数据与训练策略显著缩小与GPT-4o、Nano-Banana的差距；在指令遵循与知识可信度两方面取得竞争性结果，优于多数开源系统的常见短板。

Conclusion: 面向世界知识与因果推理的隐式编辑可通过专门的数据集与带因果奖励的训练框架有效提升；WorldEdit与WorldEdit-Test为该方向提供了训练与评测基准，推动开源模型在隐式指令与知识合理性上的进展。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [33] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: 提出TLC-Plan：直接在矢量空间生成户型图的分层生成模型，结合两级VQ-VAE与自回归Transformer，通过CodeTree统一全局布局与局部几何，实现多样、拓扑有效且受边界约束的设计，在RPLAN/LIFULL上达SOTA（如FID 1.84、MSE 2.06）。


<details>
  <summary>Details</summary>
Motivation: 现有户型自动生成多在栅格空间操作，需后处理矢量化，易引入结构不一致，且难以端到端学习；而人类设计具有模块化、可复用与层次化的空间推理特征，亟需与之对齐的矢量生成范式。

Method: 提出TLC-Plan：以楼层边界为条件，在矢量域直接生成。核心是两级VQ-VAE层次编码——上层将全局布局表示为带语义标签的房间包围框（bounding boxes），下层用多边形级别的code细化局部几何；用CodeTree统一两层结构，并通过自回归Transformer按边界条件采样codes，无需显式房间拓扑或尺寸先验即可得到多样且拓扑正确的方案。

Result: 在RPLAN数据集取得SOTA（FID 1.84，MSE 2.06），在LIFULL也领先；可生成多样、拓扑有效、受约束的矢量户型；支持可扩展到实际场景。

Conclusion: 分层、组合式的矢量生成框架能更好对齐建筑设计流程，避免栅格到矢量的失真与不一致，实现约束感知、可扩展的端到端户型生成；公开代码与模型，具备落地潜力。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [34] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 提出一种面向无结构户外环境的端到端强化学习导航框架，结合“可重光照的3D高斯点云”以在仿真中系统地改变光照，实现从单目RGB到连续控制的零样本迁移，并在真实森林中以最高10 m/s实现稳健避障。


<details>
  <summary>Details</summary>
Motivation: 仿真到真实存在显著视觉域差，特别是光照条件在现有3D Gaussian Splatting中与几何强耦合，导致在动态光照下策略泛化差，限制单目无人机在真实户外的可靠导航。

Method: 1) 在由真实数据构建的高保真仿真中进行端到端RL，从单目RGB直接输出连续控制；2) 提出Relightable 3D Gaussian Splatting，将场景分解以实现基于物理的环境光照可编辑；3) 通过合成多样光照（强定向日光到漫射阴天）进行数据增广，促使策略学习对光照不变的视觉特征。

Result: 在真实复杂森林环境中，小型四旋翼无需微调即可实现稳健、无碰撞导航，速度可达10 m/s；在剧烈光照变化下表现出显著鲁棒性。

Conclusion: 通过可重光照的神经表示与端到端RL结合，缓解了仿真-现实光照域差，使得单目视觉导航可零样本迁移至无结构户外场景，并在高速度下保持安全与鲁棒性。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [35] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

TL;DR: PI3D提出在真实3D环境中通过摆放带文本的物体来对多模态大模型进行提示注入，优化其三维位姿以诱导模型执行攻击者任务；实验证明在多模型与多相机轨迹下有效，现有防御不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于文本域或经数字编辑的2D图像提示注入，缺乏对现实3D物理环境中相机观察下的攻击理解与评估；随着MLLM在机器人与具身交互中广泛应用，物理可实施的攻击面变得关键且迫切。

Method: 提出PI3D：将带有攻击文本的物理物体放置在环境中，通过优化其3D位姿（位置、朝向）在保证物理可实现性的约束下最大化对MLLM的注入效力；在多种相机运动/视角下评估，并对比现有防御策略。

Result: PI3D在多种MLLM与多样相机轨迹设置中均能有效诱导模型执行注入任务；对现有防御（文本过滤、多视图一致性等）进行评测，发现仍无法充分抵御该物理3D提示注入。

Conclusion: 现实世界中的文本物体可作为有效的3D提示注入媒介，对具身与视觉语义推理MLLM构成实质威胁；现有防御不足，需面向3D物理环境特性的专门防护与鲁棒性提升方案。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [36] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: Ex-Omni提出将语音与3D面部动画融入OLLM，通过解耦语义推理与时序生成，并以语音单元作时序支架、TQGF机制控注语义，实现稳定对齐的语音-面部动画生成，表现与现有开源OLLM相当。


<details>
  <summary>Details</summary>
Motivation: 现有OLLM多聚焦图像/文本，多模态生成难覆盖“语音-3D表情”这一自然交互关键模态。LLM以离散token进行语义推理，而3D面部动画需要高密度、细粒度的时序信号，表示失配导致端到端训练难、数据依赖强，亟需降低优化难度并实现稳定对齐。

Method: 提出Ex-Omni框架：1) 解耦策略——将语义推理与时序生成分离；2) 以语音单元（speech units）作为时序脚手架，承载细粒度时序结构；3) 统一的“token-as-query gated fusion (TQGF)”机制，将语义以可控门控方式注入时序生成；4) 构建InstructEx数据集，专用于增强语音-3D面部动画指令学习与训练。

Result: 在多项实验中，Ex-Omni在通用OLLM基准上达到与现有开源方法相当的性能，同时能够稳定地产生与语音对齐的3D面部动画；展示了在语音-表情同步与生成稳定性方面的优势。

Conclusion: 通过以语音单元为时序骨架并用TQGF进行受控语义注入，Ex-Omni有效缓解了LLM离散语义与3D动画连续时序的表示鸿沟，实现在开源条件下的可竞争OLLM表现与稳定的语音-面部动画生成；InstructEx数据集为该方向提供训练支撑。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [37] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 作者在LAION-400M中用CLIP相似度检索“孕期超声图”，发现包含大量可识别的私密信息（如姓名、地点），部分样本存在高再识别/冒充风险，并提出数据集整理与隐私合规建议。


<details>
  <summary>Details</summary>
Motivation: 生成模型依赖大规模网络抓取数据，往往缺少整理与过滤，可能无意中包含敏感/私人信息。孕期超声图常被在社交媒体公开分享，天然携带身份线索与医疗隐私，故成为一个具代表性的风险切片以评估公共图像数据集的隐私暴露问题。

Method: 对LAION-400M进行系统性检索：以CLIP文本-图像嵌入相似度为检索信号，定位“孕期超声”相关图像；随后对检索结果进行自动与人工审查，识别并统计图像中出现的可识别实体（如姓名、地理位置等），评估其潜在隐私风险。

Result: 在LAION-400M中检索到大量孕期超声图，并检测到成千上万条私人实体信息（姓名、地点等）；其中多张图像含高风险信息，可能使个体被再识别或被冒充。

Conclusion: 公共大规模图像数据集中确有敏感医疗影像与个人信息泄露的现象。作者建议：加强数据集策划与过滤流程；在抓取与发布前执行隐私安全审查与去标识化；提供申诉与删除机制；在模型训练与共享时遵循数据最小化与合规伦理实践。

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [38] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: 提出DuMeta++：无需配对纵向数据的双重元学习框架，实现跨年龄MRI脑组织分割的一致性与高性能。


<details>
  <summary>Details</summary>
Motivation: 跨生命周期的脑MRI外观和形态随年龄显著变化，导致分割模型跨年龄泛化差；现有依赖配对纵向数据的自监督正则在实际中难以获取。

Method: 双元学习框架：1) 元特征学习，提取对年龄不敏感的语义表示，捕捉时空形变的稳定结构；2) 元初始化学习，提升少样本条件下的快速适配能力；并引入基于记忆库的类别感知正则，在无显式纵向监督下强化纵向一致性；给出收敛性理论证明。

Result: 在iSeg-2019、IBIS、OASIS、ADNI等多数据集的few-shot设置下，跨年龄泛化优于现有方法。

Conclusion: DuMeta++在无配对纵向数据条件下，实现稳定收敛、数据高效适配与更强跨年龄泛化，为脑MRI分割在全生命周期应用提供可行方案。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [39] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 提出用视角不变的语义特征作为条件，解决全头部3D GAN因视角条件造成的偏置与崩溃，显著提升生成的一致性、多样性与泛化，并加速训练。


<details>
  <summary>Details</summary>
Motivation: 以往全头部3D GAN依赖视角作为条件以避免无条件训练的模式崩溃，但由此在条件视角方向上形成偏置：条件视角生成质量和多样性高，非条件视角差，整体各区域不一致，影响可用性。

Method: 构造视角不变的语义条件：利用FLUX.1 Kontext将高质量正脸人像数据集扩展到多视角；对同一身份的正脸图像提取CLIP图像特征，作为该身份在所有扩展视角下共享的语义条件输入GAN。训练时来自不同视角的监督在同一语义条件下对齐，去除方向偏置并稳定对抗训练。

Result: 在全头部生成与单视图GAN反演任务上，方法在保真度、多样性与泛化上显著优于现有方法，并展现更好的全局一致性与训练收敛速度。

Conclusion: 用视角不变的共享语义条件取代视角条件可解除方向偏置、缓解模式崩溃、提升多样性和一致性，并加速训练，为稳定高质量的全头部3D生成提供了有效范式。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [40] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: RoadSafe365 是一个面向交通安全的超大规模视觉-语言基准，以系统化、符合官方安全标准的层级分类为核心，提供3.6万+带多选问答与场景描述的真实视频剪辑，覆盖多样事件、环境与交互场景，并给出强基线与跨域验证，旨在推动可复现的交通安全多模态研究与评测。


<details>
  <summary>Details</summary>
Motivation: 现有多模态交通基准多聚焦粗粒度的事故检测，缺乏与官方交通安全标准对齐的系统化评估框架，难以支持细粒度安全分析与可复现研究。

Method: 独立构建并按层级分类法组织数据，细化并扩展“事故/事件/违章”等核心概念；从行车记录仪与监控视频中筛选多源多域片段，进行丰富属性标注；为每段视频配套多选问答、标准化候选项与详细场景描述；提供基线模型与跨域实验设置，用于训练与评测。

Result: 发布36,196段标注视频，覆盖多类型交通事件、环境与交互情景；配套86.4万候选项、8.4千唯一答案与3.6万场景描述；在该基准上微调可带来一致性能提升，并在真实与合成数据的跨域实验中验证有效性。

Conclusion: RoadSafe365 构建了与官方标准对齐、可大规模训练和标准化评测的视觉-语言基准，为交通安全的细粒度理解与推理提供数据与评价体系，促进可复现的真实世界交通安全研究。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [41] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: 提出AdvSR：在超分辨率模型权重中植入对下游任务的对抗行为，无需推理时访问输入或触发器，兼顾重建质量与定向误导，几乎不降画质却造成显著误分类风险。


<details>
  <summary>Details</summary>
Motivation: 超分辨率常作为预处理提升分类/检测，但其模型层面可能成为新的攻击面。现有攻击多依赖输入扰动或后门触发，尚缺少无需推理时干预、仅通过模型权重实现的对抗手段。作者旨在揭示并验证这种模型级威胁。

Method: 提出AdvSR训练框架：在训练阶段将重建损失与针对下游分类器（如YOLOv11）的定向对抗目标联合优化，使得到的SR模型在常规图像质量指标上表现良好，但其输出系统性诱导下游误分类。适配多种SR架构（SRCNN、EDSR、SwinIR），无需在部署时修改输入或加入触发器。

Result: 在与YOLOv11分类器组合的实验中，AdvSR在三种SR架构上均取得高攻击成功率，同时图像质量指标仅有最小幅度下降，表面看似“正常”的SR模型会显著误导下游。

Conclusion: SR预处理环节存在被模型级对抗训练滥用的安全隐患。仅凭常规重建质量评估无法发现此类后门，应在模型获取与验证中引入安全感知评测与来源审计，特别是在安全关键场景。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [42] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 3D-TBM 提供基于最优传输嵌入的三维医学影像形态学分析工具，实现从影像到传输域的可逆映射，用于分类/回归与可解释可视化，并配套文档与教程，代码在 PyTransKit 开源。


<details>
  <summary>Details</summary>
Motivation: 推动 TBM 在三维医学影像中的广泛、标准化应用，使得模型结果可回投到原始空间以获得空间可解释的临床形态特征，同时提供一体化流程与易用工具降低研究门槛。

Method: 构建完整流水线：数据预处理；计算3D最优传输(OT)嵌入与可逆映射；在传输域进行分类、回归与主方向可视化；识别判别方向等分析方法；并提供文档与教程。

Result: 形成名为 3D-TBM 的工具包与工作流，能够在传输域提取特征、进行下游任务，并将结果逆映射到原图像以进行空间解释；源代码通过 PyTransKit 发布。

Conclusion: 3D-TBM 使 TBM 在3D医学影像中的应用更加可及和可解释，为临床研究提供可视化与判别分析能力，促进形态学研究与模型解释。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [43] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: 提出TwistNet-2D：在保持局部空间位置信息的同时建模二阶通道相关，用极小开销显著提升纹理与细粒度识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有二阶统计方法（如双线性池化、Gram矩阵）能捕获全局通道相关但丢失空间结构；自注意力虽考虑空间上下文，却主要做加权汇聚而非显式成对交互，难以表达具有方向性与周期性的纹理共现模式。需要一种既保留空间位置信息又建模通道间二阶互动的高效机制。

Method: 提出Spiral-Twisted Channel Interaction（STCI）：将一个特征图按给定方向做空间位移后，与另一特征图做逐元素通道乘积，显式编码跨位置的通道共现；设置四个方向头并用可学习通道重加权聚合；通过sigmoid门控的残差路径注入主干。构成的TwistNet-2D作为轻量模块，可插入ResNet等CNN。

Result: 在保持仅约3.5%参数与2% FLOPs开销（相对ResNet-18）的情况下，于四个纹理与细粒度基准上，一致优于参数相当乃至更大的基线模型（含ConvNeXt、Swin Transformer及CNN-Transformer混合架构）。

Conclusion: 局部方向位移下的通道对乘能同时编码“在哪里共现”和“如何交互”，在极低计算与参数增量下提升纹理/细粒度识别，优于现有全局二阶或自注意力范式。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [44] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: 提出VideoNeuMat：用视频扩散模型提取可复用神经材质资产的两阶段方法。阶段一微调大规模视频模型，在受控相机/光照轨迹下生成“材质采样”视频；阶段二用从小模型微调的LRM，从17帧单次推理重建可泛化的神经材质参数。结果表明能将互联网规模视频中的材质知识转移为独立3D资产，真实感与多样性大幅超越有限的合成训练集。


<details>
  <summary>Details</summary>
Motivation: 现有材质生成受限于缺乏高质量标注数据；而视频生成模型虽能产生逼真的材质外观，却与几何与光照纠缠，难以直接复用为材质资产。需要一种方法把视频模型中的材质知识解耦并转化为可重用的神经材质参数。

Method: 两阶段：1) 微调大型视频扩散模型（Wan 2.1 14B），在受控相机与光照轨迹下生成材质样本视频，等效为虚拟测角反射计，学习结构化采样模式且保留材质真实感；2) 从较小的视频骨干（Wan 1.3B）微调的大型重建模型（LRM），对17帧输入视频单次前向，预测可在新视角/新光照下泛化的紧凑神经材质参数。

Result: 从仅17帧视频即可一次性恢复材质；重建的神经材质在新视角与光照下保持真实感与多样性，显著优于受限的合成训练数据；证明了从互联网规模视频模型向可复用3D材质资产迁移知识的可行性。

Conclusion: VideoNeuMat有效将视频扩散模型中的材质知识解耦并转化为可复用神经材质，低帧数单次推理即可泛化渲染，显著提升材质真实感与多样性，为数据受限的材质生成提供了新范式。

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [45] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: 提出跨视角世界模型（XVWM），通过跨视角预测学习视角不变的3D表示，使智能体能在最适合任务的参考系中想象与规划，同时从自我视角执行。多视角一致性在Aimlabs多机位数据上显著增强空间表征与规划能力，并为多智能体“换位思考”奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型多基于单一视角（多为自我视角），在需要全局空间推理的任务（如导航）时受限；若能利用鸟瞰等其他视角，规划会更高效。因此需要一种能跨视角理解与预测环境的模型，以获得更稳健、空间对齐的内部表示。

Method: 提出Cross-View World Models：给定某一视角的帧序列与动作，预测同一或不同视角下行动后的未来状态。以多机位同步游戏数据（Aimlabs）训练，通过跨视角预测目标强制几何一致性，从而学习视角不变、3D结构感知的潜表示。模型为智能体提供并行的多视角“想象流”，规划时可选最优参考系，执行仍在自我视角。

Result: 实验证明：引入多视角一致性提供强学习信号，得到空间扎根的表示，提升基于想象的规划效果；模型能在多视角间一致地预测未来状态。

Conclusion: 跨视角预测作为几何正则化，可让世界模型学习到视角不变的3D表征，支持在不同参考系下的计划与执行，并为多智能体场景中的换位视角预测提供潜在基础。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [46] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 提出一种将注意力机制集成到DeepLab‑V3+中的病灶分割模型，用DDR 757张眼底图像对4类DR病灶实现像素级分割，显著提升mAP、mIoU，尤其是微动脉瘤检测提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有DR自动化筛查方法在临床可用性有限，尤其缺乏可直接辅助医生的病灶级（像素级）标注与分割，且早期关键征象如微动脉瘤检测效果不足，需要更精细、可解释的分割模型。

Method: 基于DeepLab‑V3+，引入注意力机制构建Attention‑DeepLab，对DDR数据集（757张）中四类病灶（微动脉瘤、软性渗出、硬性渗出、出血）进行像素级分割训练与评估；以mAP与mIoU作为主要指标，并单独报告微动脉瘤检测性能。

Result: 与基线DeepLab‑V3+相比，mAP由0.3010提升至0.3326，mIoU由0.1791提升至0.1928；微动脉瘤检测从0.0205显著提升到0.0763。

Conclusion: 注意力增强的DeepLab‑V3+在DR病灶分割上较基线有稳定改进，特别是对早期征象微动脉瘤的检测提升具临床意义，表明该方法更适合用于辅助临床筛查与解释性分析。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [47] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 利用线性遗传规划自动优化图像处理流程，在FIB截面显微图中稳健分割析出相，达成人工接近的精度（XOR像素误差1.8%），单张3.6MP图约2秒处理，加速铜基铌合金的开发迭代。


<details>
  <summary>Details</summary>
Motivation: 手工标注因对比度波动、噪声与伪影而耗时且不稳定，限制了增材制造铜基铌合金微观组织分析与合金迭代速度，需要自动化、可解释且适应伪影的分割方法。

Method: 构建以图像处理领域特定语言（DSL）为核心的优化环境：程序由若干可调参数的滤波模块顺序组成；采用线性遗传规划搜索滤波与分割流水线；以与人工基线的逐像素XOR误差为适应度；在理想设置（种群60、最大5个模块）下进化出最优流程；自动导出可读的MATLAB代码。

Result: 在FIB截面图的析出相检测任务中，得到接近人工的分割效果：平均评估误差1.8%；对3.6MP图像平均约2秒处理时间；在存在多种伪影和噪声条件下仍具稳健性。

Conclusion: 该自动化、可解释的LGP优化图像流水线显著提升微观组织分割效率，缩短材料成分与工艺探索的迭代周期，助力获得强度高、低活化、经析出强化的增材制造聚变反应堆铜合金部件。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [48] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 提出LUCID：一种统一视觉-语言稀疏自编码器，同时为图像patch与文本token学习共享稀疏字典并保留各自私有容量，通过无监督最优传输对齐实现可解释、可对应的跨模态特征，支持细粒度定位与自动化字典解读。


<details>
  <summary>Details</summary>
Motivation: 现有SAE通常按模态分别训练，字典特征不直观、跨域不可迁移，难以建立跨模态的一致解释与神经元对应，且相似度评估中易受概念聚类问题影响。

Method: 设计统一的视觉-语言SAE：在编码器-解码器中引入共享潜在稀疏字典与模态私有容量；通过学习的最优传输（OT）匹配目标将共享稀疏代码在两种表示（图像patch、文本token）之间对齐，无需标注；基于对齐性质构建自动化词条聚类管线，对字典特征进行解释。

Result: LUCID学得可解释的共享特征：能进行patch级语义落地、建立跨模态神经元对应；在基于相似度的评估中对“概念聚类”问题更稳健；自动化聚类管线可在无需人工观察下为字典赋义。

Conclusion: 统一训练与OT对齐带来可解释、可迁移的多模态稀疏表征。共享特征不仅涵盖物体，还覆盖动作、属性与抽象概念，促进多模态可解释性与下游分析。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [49] [Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation](https://arxiv.org/abs/2602.07343)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.CV

TL;DR: 提出CLARITY：一种在不同光照/阴影条件下自适应RGB-热成像融合的语义分割方法，在MFNet上达成SOTA（mIoU 62.3%、mAcc 77.5%）。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T融合通常采用固定融合策略，无法针对恶劣光照、照明变化与阴影自适应，导致某一模态的噪声在网络中传播，尤其在暗光或强光条件下影响道路场景分割的稳健性。

Method: 引入以VLM先验引导的条件自适应融合框架CLARITY：1）首先识别场景光照状态，并根据状态动态调节RGB与热成像模态的权重；2）使用面向对象的嵌入作为分割表征，替代固定融合策略；3）提出两项机制：a) 保护暗色目标的有效语义，避免传统去噪错误抑制；b) 分层解码器在多尺度上强化结构一致性，提升细长目标边界锐利度。

Result: 在MFNet数据集上取得新的SOTA：mIoU 62.3%、mAcc 77.5%。

Conclusion: 动态、条件感知的RGB-T融合结合VLM先验与结构化解码可显著提升恶劣光照条件下的道路场景语义分割精度，并优于静态融合基线。

Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.

</details>


### [50] [Optimizing Few-Step Generation with Adaptive Matching Distillation](https://arxiv.org/abs/2602.07345)
*Lichen Bai,Zikai Zhou,Shitong Shao,Wenliang Zhong,Shuo Yang,Shuo Chen,Bojun Chen,Zeke Xie*

Main category: cs.CV

TL;DR: 论文提出自适应匹配蒸馏（AMD），通过检测并逃离“禁区”来提升分布匹配蒸馏的稳定性与性能，在图像/视频生成任务上显著提升保真度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配蒸馏（DMD）在优化过程中会落入“禁区”：真教师信号不可靠、假教师排斥力不足，导致训练不稳与模式坍塌。以往改进多为隐式规避，缺少统一理解与显式纠偏机制。

Method: 提出统一优化视角，将既有方法解释为隐式避开禁区；基于此设计AMD：1）用奖励代理显式检测禁区；2）结构化信号分解，动态优先强化纠偏梯度；3）引入“排斥景观锐化”，在失败模式周围构建陡峭能量屏障，增强排斥力与稳定性。

Result: 在SDXL、Wan2.1等模型与VBench、GenEval等基准上，AMD显著提升样本质量与训练稳健性；如SDXL的HPSv2由30.64提升到31.25，优于SOTA。

Conclusion: 显式识别并矫正优化轨迹中的禁区对推动少步生成模型上限至关重要；AMD提供通用、稳定的加速蒸馏方案并优于现有基线。

Abstract: Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.

</details>


### [51] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 提出在改进U-Net后加入行列分离注意力（RCSA），以极少参数利用全局信息提升低照度图像/视频增强，并配合时间一致性损失在多数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net做低照度增强缺乏全局信息指导，易产生局部噪声放大与细节丢失。直接用全局注意力虽可缓解，但参数与计算代价过高，尤其在高分辨率与视频场景下不切实际。

Method: 在改进U-Net主干后插入RCSA模块：对特征图按行与列分别做均值与最大池化，形成紧凑的全局描述，再用于引导局部特征（行/列方向注意力），以极少参数实现全局-局部融合。为视频场景设计两种时间损失以约束跨帧一致性。

Result: 在LOL与MIT-Adobe FiveK图像集及SDSD视频集上进行大量实验，证明该方法能在保持或提升增强质量的同时，显著降低参数/计算量并改善时间一致性。代码已开源。

Conclusion: RCSA以行列方向的统计量高效建模全局信息，缓解低照度增强中的噪声与细节流失问题；结合时间损失可稳定视频一致性，具备良好的精度-效率权衡并经多数据集验证。

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [52] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 提出一种透视感知的对数深度融合方法，将深度与法线在透视投影下统一融合，实现度量准确的3D重建，并用法线信息补齐缺失深度；在DiLiGenT-MV数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于正交投影的深度-法线融合忽视透视效应，导致度量失真；同时深度图常有缺失，需要利用法线来弥补以获得更完整、更准确的重建。

Method: 在单目透视相机模型下，将深度表示为对数深度，推导透视投影约束下的梯度关系，把传感器获得的深度和光度立体得到的法线统一到一个优化框架中；在有缺失的深度区域通过法线约束进行内插/修复，实现联合重建。

Result: 在DiLiGenT-MV数据集上的实验显示，该方法较正交假设的梯度融合方法取得更高的几何精度与完整性，尤其在透视效应明显与深度缺失区域表现更佳。

Conclusion: 显式建模透视投影并采用对数深度的深度-法线融合能显著提升单目系统的3D表面重建精度与鲁棒性，同时可利用法线信息有效填补深度缺口。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [53] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: 提出PTB-XL-Image-17K：从PTB-XL信号合成的1.7万张高质量12导联心电图图像及全套标注，覆盖从导联检测、波形分割到信号重建的完整评测管线。


<details>
  <summary>Details</summary>
Motivation: ECG数字化可将纸质/扫描心电图恢复为可用时间序列，以释放大量历史临床数据供深度学习利用。但缺乏同时包含图像、真值信号与全面标注的大规模数据集，限制了方法发展与公平评测。

Method: 基于PTB-XL原始信号，构建开源的可控合成框架：设定纸速(25/50 mm/s)、增益(5/10 mm/mV)、采样率(500 Hz)、网格外观(4色)与波形特征，自动渲染逼真的12导联纸面心电图；同时生成像素级分割掩膜、导联与标签名的YOLO格式框、对应时间序列真值以及视觉与患者元数据。

Result: 生成17,271个样本，50%带网格、50%不带网格；每样本含5类互补数据；平均1.35秒/样本，生成成功率100%。

Conclusion: PTB-XL-Image-17K填补了ECG数字化研究的关键缺口，是首个支持导联检测、波形分割与信号提取全流程、具备完整真值的大规模资源；数据与生成框架公开，可用于严谨基准与方法开发。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [54] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: 提出SoulX-FlashHead：1.3B统一框架，实现实时、无限时长、高保真音频驱动人像直播合成；通过流式时空预训练与音频上下文缓存稳定短音频特征，并用双向蒸馏抑制长序列漂移；配套VividHead数据集；在HDTF/VFHQ达SOTA，Lite版在4090上96 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有大模型算力开销大、不适合实时；轻量模型又牺牲整体面部一致性与时间稳定性。需要同时兼顾高保真、低时延、稳定、可无限时长的流式音驱人像生成方案。

Method: 1) 统一1.3B参数端到端框架用于流式视频生成；2) Streaming-Aware Spatiotemporal Pre-training：引入Temporal Audio Context Cache，从连续短音频片段中稳健抽取特征；3) Oracle-Guided Bidirectional Distillation：利用真实运动先验进行双向蒸馏，提供精确的物理指导，缓解自回归长序列误差累积与身份漂移；4) 构建VividHead数据集（782小时严格对齐视频音频）以支撑鲁棒训练。

Result: 在HDTF与VFHQ基准上达SOTA；Lite变体在单张RTX 4090上以96 FPS推理，保持视觉一致性与超快交互。

Conclusion: SoulX-FlashHead在保证高保真与时间稳定的同时，实现低时延和无限时长的流式生成；配套的预训练策略与蒸馏机制有效提升鲁棒性和长程一致性，具有实际部署潜力。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [55] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: 提出SpatialReward空间推理奖励模型，解决图像编辑评估中的“注意力塌缩”，在多基准集上SOTA，并显著提升在线RL效果。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习用于复杂图像编辑需要可靠、细粒度的奖励信号；现有评估器忽视跨图对比与细节定位，产生误判与分数失准（注意力塌缩）。

Method: 构建SpatialReward：以预测的编辑区域为锚，执行显式空间/像素级推理与验证；在26万条带空间标注的数据上训练；将空间证据与语义判断绑定以校准评分。

Result: 在MMRB2与EditReward-Bench上达SOTA；在自建MultiEditReward-Bench上优于专有评估器；作为在线RL奖励信号，使OmniGen2在GEdit-Bench上提升+0.90，超过最佳判别模型，且是GPT-4.1奖励增益（+0.45）的两倍。

Conclusion: 空间推理与区域对齐对图像编辑评估至关重要；SpatialReward提供更可靠的细粒度奖励，显著改善在线RL对齐与编辑质量。

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [56] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 他们提出并发布了一个名为 GlobalWasteData (GWD) 的大规模垃圾图像数据集：89,807 张图片、14 个主类、68 个子类，统一标注、去重与质检，提升多域多场景的泛化能力，旨在支撑更稳健的自动垃圾分类模型与后续研究复现。


<details>
  <summary>Details</summary>
Motivation: 现有公开垃圾分类数据集分散、命名不一致、标注格式各异、场景偏置明显、类别分布失衡，难以直接合并使用，训练出的模型泛化性差，难以适应真实世界的复杂环境。

Method: 将多个公开垃圾分类数据集整合为统一档案：统一类别体系（14 主类、68 子类）与标注格式；进行质量过滤、重复样本去除、元数据生成；在合并过程中关注域多样性与类别平衡，以减少偏差并提升可用性。

Result: 得到覆盖更广域的 89,807 张图像数据集，标注一致、类别更均衡，包含丰富元数据，质量与多样性优于单一来源数据集，适合作为训练与评估稳健模型的基础。

Conclusion: GWD 为环境监测、回收自动化和垃圾识别等 ML/AI 应用提供了可靠、可复现的基础数据资源；其一致标注与更均衡分布有助于训练在真实世界中更具泛化能力的模型，并鼓励社区共享与后续研究。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [57] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: 提出TOM-GS：结合学习型里程计与高斯点绘(Gaussian Splatting)的热红外SLAM，实现鲁棒热成像位姿估计与稠密重建，并在运动估计与新视角渲染上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 热红外成像在黑暗、粉尘、烟雾等恶劣环境下具备稳定性，适合机器人感知与运动估计。但现有热成像里程计/建图方法多为几何型，跨数据集易失效且难以生成稠密地图。近期GS在高效高质量重建方面表现突出，激发将其引入热红外SLAM以提升鲁棒性与重建质量。

Method: 提出TOM-GS框架：1) 学习型里程计用于热红外图像的鲁棒位姿估计；2) 基于Gaussian Splatting的稠密建图，实现高质量新视角渲染；3) 针对热图像的专用增强模块；4) 融合单目深度以改进尺度与几何一致性。

Result: 在多数据集上进行运动估计与新视角渲染评测，TOM-GS优于现有学习型方法；展示在恶劣环境下的鲁棒性与更高质量的稠密重建效果。

Conclusion: 学习型流水线结合GS适用于热红外相机的SLAM，能在挑战条件下实现稳定里程计与高保真重建；TOM-GS验证了该方向的有效性并为热红外稠密SLAM提供新基线。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [58] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 提出一种将脑信号与图像表示对齐的新方法，结合多种预训练视觉编码器与对比学习，并引入“融合先验”，在检索准确率与重建保真度间取得较好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视觉解码多偏重高层语义、忽视像素级细节，难以全面揭示人类视觉系统是否以及如何在脑信号中编码多层次视觉信息。因此需要一种能捕获层级与多尺度表征、并稳健对齐跨模态分布的方法。

Method: 1) 多编码器对齐：利用具不同归纳偏置的多种预训练视觉编码器，提取层级与多尺度图像表征；2) 脑-图像对比学习：采用对比学习目标，使脑信号嵌入与视觉嵌入在共同空间中有效对齐；3) 融合先验（Fusion Prior）：先在大规模视觉数据上学习稳定的映射作为先验，再将脑特征匹配到该先验，从而提升跨模态分布一致性；4) 进行定量与定性评估，兼顾检索与重建。

Result: 在多项实验中，相比现有方法，同时提升了图像检索的准确性与重建的图像细节保真度，呈现更均衡的性能。

Conclusion: 多编码器+对比学习的层级对齐配合融合先验，可更充分利用脑信号中的视觉信息，在跨模态检索与重建之间取得良好权衡，表明脑信号确含可用于像素级与语义级解码的多层信息。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [59] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 提出IM-Animation：用紧凑1D隐式运动token与时序一致的掩码token重定向模块，结合三阶段训练，在角色动画生成上优于/媲美SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有显式方法（骨架、DWPose等）受2D空间约束，难以处理空间错配与尺度变化；隐式方法虽抓高层运动语义，但易身份泄漏、运动与外观纠缠。需要一种既能弱化空间约束、又能避免身份泄漏并提升重定向一致性的运动表征。

Method: 1) 设计隐式运动表征：将每帧运动压缩为紧凑的一维motion token，弱化2D位置约束并隔离外观信息；2) 提出基于mask token的时序一致重定向模块：通过时序训练瓶颈抑制源图像运动干扰，提升跨帧一致性；3) 三阶段训练策略，以提高训练效率与保真度；整体在视频扩散模型框架下进行角色动画合成。

Result: 大量实验显示：所提隐式运动token与IM-Animation生成能力在画质、身份保持与重定向一致性等方面达到优于或可比于最新方法的表现。

Conclusion: 隐式一维运动token有效避免身份泄漏并缓解空间错配问题，结合时序一致的掩码重定向与三阶段训练，可在角色动画生成中实现高保真且稳定的跨视频运动重定向，达到SOTA级别表现。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [60] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: 提出ZoomDet：一种对UAV图像进行自适应非均匀放大的通用框架，通过预测偏移与框级缩放目标学习变形，并配合角点对齐的框变换，在训练/推理中于缩放空间检测、再映射回原图；在SeaDronesSee上以Faster R-CNN提升mAP超8.4且仅约+3ms延迟。


<details>
  <summary>Details</summary>
Motivation: UAV图像中的目标普遍更小且更稀疏，常规检测器难以充分学习有效特征；期望通过对前景区域自适应放大以提升小目标可见性与特征分辨率，同时保持高效性与与架构无关的通用性。

Method: 1) 设计轻量的偏移（offset）预测网络，结合新颖的基于框的缩放（zooming）损失，学习对输入图像进行非均匀自适应放大；2) 提出角点对齐的边界框变换：将真实框按照学习到的变换扭曲映射到缩放空间用于训练，推理时再把预测框逆变换回原图；3) 将该流程无缝挂接到任意检测器（如Faster R-CNN）。

Result: 在VisDrone、UAVDT、SeaDronesSee三个UAV检测数据集上全面验证；在SeaDronesSee上与Faster R-CNN结合，mAP提升>8.4个点，额外时延约+3毫秒；框架独立于检测器架构，具备通用增益与低开销。

Conclusion: 自适应非均匀放大能显著改善UAV小目标检测；所提ZoomDet通过轻量偏移预测与角点对齐框变换，在不改变主干检测器的前提下，以极低计算开销带来显著mAP提升，具备通用可移植性。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [61] [CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization](https://arxiv.org/abs/2602.07523)
*Zhen Zhang,Qing Zhao,Xiuhe Li,Cheng Wang,Guoqiang Zhu,Yu Zhang,Yining Huo,Hongyi Yu,Yi Zhang*

Main category: cs.CV

TL;DR: 提出基于CA-YOLO的仿生稳像定位系统，通过引入小目标检测头与特征融合注意力机制（CFAM）提升小目标识别与整体定位精度，并结合受VOR启发的云台跟踪控制，实现更稳更准的时敏定位；在COCO与VisDrone上mAP分别提升约3.94%与4.90%，实验验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有目标定位系统在复杂环境下存在精度不足与小目标识别能力弱的问题，需要一种兼顾检测与跟踪稳定性的方案以满足时敏应用（如无人机、监控、机器人）的高要求。

Method: 在YOLO骨干中嵌入仿生视觉机制：1) 新增小目标检测头以加强多尺度、尤其是小尺度特征感知；2) 提出特征融合注意力机制（CFAM），在跨层特征融合中强化关键信息、抑制冗余；3) 设计受人类前庭—眼反射（VOR）启发的云台跟踪控制策略，含中心定位、稳定性优化、自适应控制系数调整与智能再捕获，提高动态场景下的稳像与跟踪鲁棒性。

Result: 在COCO与VisDrone基准上较原模型平均精度分别提升3.94%与4.90%；在时敏目标定位实验中表现优于基线，证明检测与稳像一体化方案的有效性与实时性。

Conclusion: CA-YOLO仿生稳像定位系统同时提升了检测（尤其小目标）与动态稳像跟踪能力，具有较强的实用价值；未来可在更多场景与硬件上扩展验证，并探索与自适应感知、轻量化部署的结合。

Abstract: In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the "brain" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.

</details>


### [62] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 提出用指令微调的多模态大模型作为评测器，并以统一任务/指标同时评估“在哪里”和“是什么”，用于系统评估面向对象的表示学习在复杂推理与OOD鲁棒性上的效用；并给出一个多特征重建的简单基线。


<details>
  <summary>Details</summary>
Motivation: 现有OCL评测偏重目标发现与简单分类探针，难以反映在复杂推理与OOD情景下的表征效用；且定位与表征质量用割裂指标评估，导致结论不一致、不可比较。

Method: 1) 采用指令微调的视觉语言模型作为评测器，将OCL产生的对象级表示输入VLM，在多种VQA数据集上衡量其用于复杂推理的价值；2) 设计统一的评测任务与单一指标，同时度量对象定位准确性（where）与表征可用性（what），避免分离评测带来的偏差；3) 提供一个多特征重建（multi-feature reconstruction）的简单基线作对照。

Result: 在多样化VQA基准上，所提评测框架能够区分不同OCL方法在复杂推理与表征可用性上的差异，统一指标减少了与定位/语义割裂评估的不一致性；基线为结果解读提供参考点。

Conclusion: 使用指令微调VLM作为统一评测器并以单一任务/指标联合评估定位与表征，可更全面、可扩展地衡量OCL表示对复杂推理与OOD鲁棒性的实际价值；所提框架与基线为未来OCL研究提供标准化评测路径。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [63] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 论文使用GCViT-Tiny对Oxford-IIIT Pet数据集中猫品种进行图像分类，通过旋转、水平翻转、亮度调整等增强，验证集94.54%准确率、测试集92.00%，展示Transformer在细粒度识别中的有效性，并给出Hugging Face在线演示。


<details>
  <summary>Details</summary>
Motivation: 猫品种区分细微，传统或简单CNN方法在毛色、花纹、脸型等细差异上鲁棒性不足；需要验证基于Transformer的全局上下文建模是否能提升细粒度图像分类的表现并支持实际应用（兽医、收容所、移动端识别）。

Method: 选用Oxford-IIIT Pet数据子集的高分辨率猫图像；采用Global Context Vision Transformer (GCViT) 的Tiny变体；进行广泛数据增强（旋转、水平翻转、亮度调整）以提升泛化；训练并在验证集与测试集评估性能；提供Hugging Face在线demo。

Result: GCViT-Tiny在验证集达到94.54%准确率，测试集达到92.00%准确率，证明该模型在猫品种细粒度分类上具有较高精度。

Conclusion: Transformer架构（尤其具备全局上下文建模的GCViT）对细粒度图像分类有效；所提方法在实际场景中具潜在应用价值（诊断、收容所管理、移动端识别）；未来可在更大数据规模、更丰富增强与模型变体上进一步提升表现。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [64] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 提出一个双时相分析框架，用入院CTP与随访DWI的空间对齐，基于手工分区构建六类ROI，并在T1上提取统计特征、纹理放射组学与深度特征（mJ‑Net、nnU‑Net编码器）。在18例成功再灌注患者中，区域级表征在特征空间形成有意义聚类：最终恢复的“假性病变/可救组织”更接近健康脑组织，而注定梗死区域分群明显。深度特征（尤以mJ‑Net）对可救与不可救半暗带的分离更强，Wilcoxon符号秩检验证明分离指数显著异于0，提示编码器特征流形能刻画组织表型及其状态转变。


<details>
  <summary>Details</summary>
Motivation: 单一时间点的病灶分割忽视了卒中病灶的异质性与时间演化；临床上CTP用于估计核心/半暗带，而最终结局需DWI确认。需要一种能同时利用入院与随访信息、在特征空间中表征组织命运差异的方法，以改进对半暗带可救性的量化与理解。

Method: 提出“双时相”（入院T1与随访T2）框架：1) 将T2 DWI配准到T1 CTP以保证空间对应；2) 使用T1与T2的人工分割交集构建6个ROI，编码初始状态与最终结局（如核心/半暗带/健康×恢复或梗死等）；3) 在T1 CTP上为各ROI提取统计描述、GLCM等放射组学纹理，以及来自mJ‑Net与nnU‑Net编码器的深度嵌入；4) 区域级聚合与特征空间分析，包括聚类与组间分离度评估；5) 通过Wilcoxon符号秩检验评估半暗带分离指数显著性。

Result: 在18例成功再灌注患者中：- 区域级表示在特征空间中形成与组织命运一致的聚类。- 最终恢复的半暗带/疑似病变区域与健康组织特征相近；注定梗死区域形成独立分群。- 基线GLCM与深度嵌入均显示：半暗带特征随最终结局显著不同，而核心区域差异不显著。- 深度特征（尤其mJ‑Net）对可救与不可救半暗带的区分最强，半暗带分离指数显著大于0（Wilcoxon检验）。

Conclusion: 编码器导出的深度特征流形能够反映缺血组织表型与状态演变，优于传统纹理在区分可救与不可救半暗带方面，支持用双时相、区域级表示来量化卒中演化并潜在改进个体化治疗判断。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [65] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: 他们提出LGDEA：用大语言模型从放射报告中抽取诊断证据，构建共享证据空间，让图像与文本在“证据层面”对齐，从而利用大量非配对数据，在多任务上显著优于现有方法并接近大量配对数据的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP式医学多模态预训练依赖大量成对数据，并采用全局或局部对齐：全局对齐易被非诊断信息主导，局部对齐难整合关键证据，导致诊断表征不可靠，限制在小配对数据场景中的效果。

Method: 提出LLM-Guided Diagnostic Evidence Alignment（LGDEA）。用LLM从放射学报告中提取关键诊断证据，构建共享的诊断证据空间；在该空间中进行“证据感知”的跨模态对齐，使模型能够有效利用大量非配对的医学图像与报告，减轻对成对数据的依赖。

Result: 在短语定位、图文检索和零样本分类等任务上取得稳定且显著提升，性能可与依赖大量配对数据的预训练方法相媲美。

Conclusion: 面向医学诊断流程的证据级对齐可提升表征质量与下游任务性能，并大幅缓解对成对图文数据的依赖，适用于配对数据稀缺的医疗场景。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [66] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 提出MUFA SA：在ViT多层特征上执行slot attention并融合多层slot以提升无监督对象分割效果，训练更快、推理开销小，SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有OCL常仅用预训练ViT最后一层提取slot，忽略中间层蕴含的丰富语义与多尺度信息，限制了分割质量与收敛效率。

Method: 在ViT编码器的多层特征上并行/分阶段运行slot attention，得到各层的slot；设计融合策略将多层slot聚合为统一的对象中心表示；该框架轻量、可插拔，可集成到现有slot-attention式OCL方法中；仅带来少量推理开销。

Result: 将MUFA SA集成到多种OCL基线后，在多个数据集上提升无监督对象分割性能，达到新的SOTA，并改进训练收敛速度/稳定性。

Conclusion: 充分利用ViT多层语义的slot融合可显著提升无监督对象分割；MUFA SA作为轻量可插拔模块，通用于现有方法，带来更好精度与更快收敛且推理开销很小。

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [67] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: 论文提出FSSDINO：在冻结的DINOv3特征上，零训练地用类原型与Gram矩阵细化即可做小样本语义分割；在多种基准上接近甚至匹配复杂方法。作者通过“Oracle引导层分析”发现：中间层潜力更优但难以用常规无监督或支持集度量选出来，暴露了“语义选择鸿沟”。最后层虽然非最优，却是当前最稳健的强基线。


<details>
  <summary>Details</summary>
Motivation: 自监督ViT（如DINOv3）在密集任务中表现强，但少样本分割往往依赖复杂解码器或测试时自适应。作者想回答：在完全冻结、无需训练的前提下，仅凭特征本身能做到多好？哪些层的表示真正决定FSS上限？为何实际选择总落后于潜在最优？

Method: 提出FSSDINO：1) 用支持集生成类特定原型；2) 计算查询像素与原型相似度进行初分割；3) 通过Gram矩阵（通道相关性）进行预测细化；4) 仅使用DINOv3最后一层特征，零训练、零适配。另设计Oracle引导层分析：用理想化上界选择各层，以比较最后层与中间层的“可达最优”。同时评估既有无监督或支持引导的层选择指标。

Result: 在二类、多类与跨域CDFSS基准上，FSSDINO（仅最后层）已与专门方法（含复杂解码器/测试时适配）相当或具竞争力。Oracle分析显示：存在显著的层选择性能差距，中间层可达更高mIoU，能匹配计算开销大的自适应方法；但现有无监督/支持引导的层选择度量在实践中均劣于直接用最后层。

Conclusion: 冻结DINOv3的“最后层”是一个被低估的强基线；然而模型内部存在更优的中间语义表征。由于现有层选择度量无法稳定定位这些高保真特征，形成“语义选择鸿沟”。工作提供训练自由的强基线与系统诊断，并呼吁研究更可靠的层/特征选择机制以释放基础模型的潜在语义能力。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [68] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: FlexID是一个无需训练的个性化文生图框架，通过意图感知的调制机制在保持身份一致性与文本可编辑性之间取得新SOTA平衡。其核心是在语言与视觉潜空间分别注入身份先验与结构锚点，并用时序与意图自适应的门控在两者间动态权衡。


<details>
  <summary>Details</summary>
Motivation: 现有零训练个性化方法多采用刚性视觉特征注入，导致身份保真与文本顺从度（可编辑性）相互牵制，难以在强编辑意图下兼顾人物一致性和语义变化。作者欲在不额外训练的前提下缓解这一冲突。

Method: 提出FlexID，将身份信息正交解耦为两路：1) 语义身份投射器（SIP）将高层身份先验注入语言空间；2) 视觉特征锚（VFA）在扩散潜空间保证结构/外观保真。关键引入情境感知自适应门控（CAG），依据编辑意图强度与扩散时间步，动态调整两路权重：在强编辑意图或早/中特定时刻放松视觉刚性约束，允许更大语义变化；在需要保真时增强VFA权重。全流程训练自由。

Result: 在IBench基准上，FlexID在身份一致性与文本贴合度的折中上达到SOTA，能在复杂叙事生成中保持高身份保真同时提升语义可控性与多样性。

Conclusion: 意图感知的双流解耦与自适应门控可在无需训练的设定下有效调和身份与语义的冲突，为复杂个性化文生图与编辑提供高效、兼容性强的解决方案。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [69] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

TL;DR: 提出一个3B参数的视觉-语言-行动（VLA）体态智能体，通过显式、可解释的图像落地推理完成目标识别与动作选择，避免多模型拼接管线，提升泛化、效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么端到端用视觉-语言嵌入，难以超出训练分布且动作层面不可解释；要么用LLM+开放集检测的零样本模块化管线，存在误差传播、计算开销大、且难把推理反馈至导航策略的问题。需要一种既紧凑又能进行类人可解释推理、并可直接驱动行动的统一模型。

Method: 设计一个约30亿参数的VLA代理，以“想—想的摘要—行动”三阶段推理。与基于相似度的匹配不同，模型显式回答“这是否是目标物体？”以及“为何采取该动作？”，将图像落地的推理直接整合到动作选择中，从而在单一模型内完成目标识别与导航决策。

Result: 该三阶段显式推理带来更强的可解释性、更好的跨分布泛化，以及更高效的导航性能；无需昂贵的多模型零样本管线。代码与数据集将随接收后发布。

Conclusion: 一个紧凑统一的VLA通过显式、阶段化的图像落地推理，解决了以往方法的可解释性、泛化与效率问题，为语言驱动的目标导航提供了更实用的端到端方案。

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [70] [SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens](https://arxiv.org/abs/2602.07564)
*Xiaoyan Zhang,Zechen Bai,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: SIGMA 是在统一扩散 Transformer（以 Bagel 为骨干）上进行后训练的框架，引入可选择的多属性 token（风格、内容、主体、身份），支持在交错的图文序列中进行多条件组合与编辑，显著提升可控性、一致性与画质，尤其在组合任务上优于 Bagel。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型（如 Bagel）虽能对齐多视觉任务，但基本局限于单一条件输入，难以从多种异构条件（文字、图像、身份、风格等）同时综合生成或编辑，导致可控性与跨条件一致性不足。

Method: 在 Bagel 统一骨干上进行后训练，构造交错的文本-图像序列；设计可选择的多属性 token（style、content、subject、identity），作为条件入口与选择开关；用约 70 万条交错示例进行训练，使模型学会在扩散 Transformer 中进行多条件解析、选择与组合，支持选择性属性迁移、细粒度多模态对齐与组合式编辑。

Result: 在多种编辑与生成任务上，SIGMA 提升了：1) 可控性（更精确地应用/屏蔽特定属性）；2) 跨条件一致性（风格/身份/主体与文本描述更一致）；3) 视觉质量。尤其在需要多条件组合的任务上，相比 Bagel 有显著增益。

Conclusion: 通过引入可选择的多属性 token 与交错训练，SIGMA 将多源条件高效对齐到单一扩散 Transformer 内，实现更强的组合式与选择式控制，补足了 Bagel 的单条件限制，并在广泛任务上带来显著性能提升。

Abstract: Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.

</details>


### [71] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: 这篇摘要介绍了HID竞赛在无专用训练数据、服饰/携带物/视角强变化的SUSTech-Competition数据上评测步态识别，2025年赛季在更严苛设定下仍将准确率推至94.2%，并总结技术趋势与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统人脸、指纹在远距离和复杂场景下难以获取与鲁棒识别，步态作为可远距离采集的生物特征更实用；需要一个公平、可泛化评测平台来推动跨域步态识别研究进展。

Method: 组织年度HID竞赛：自2023年采用SUSTech-Competition数据（服饰、携物、视角大变化），不提供专用训练数据，要求参赛队用外部数据训练；每年用不同随机种子划分评测集以抑制过拟合、检验跨域泛化；2025年重点检验是否能突破既有精度上限，并对参赛方案的技术趋势进行分析。

Result: 在更高难度设定下，参赛队仍取得提升；最佳方法在该数据集上达到94.2%识别准确率，刷新基准；竞赛对技术路线进行了系统性分析。

Conclusion: HID竞赛验证了步态识别在远距离身份识别中的潜力与进步空间；随机种子分割和无专用训练数据的设定有效促进了公平评测与跨域泛化；未来可沿技术趋势继续提升鲁棒性与泛化性能。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [72] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 提出一种基于可分离子空间表示学习的跨相机奶牛身份识别框架，在5个摄像节点数据上，跨相机平均准确率86%，优于源域仅训练(51.9%)和最强基线(79.8%)。


<details>
  <summary>Details</summary>
Motivation: 现有动物识别方法在单相机或受控环境下表现好，但在跨相机部署时因光照、背景、视角与成像差异导致识别性能骤降，限制了大规模、非接触式智慧牧场应用。需要一种能提取对相机变化不敏感、可泛化的身份表征方法。

Method: 基于Subspace Identifiability Guarantee(SIG)理论，构建物理生成过程驱动的特征解耦模块，将观测图像分解为相互正交的潜在子空间，隔离与身份稳定相关的生物特征，使其对相机变化保持不变。配套构建覆盖5个摄像节点、不同设备与复杂光照/角度变化的数据集，并在多种跨相机任务上评测。

Result: 在7个跨相机任务中，方法平均准确率86.0%，显著高于源域仅训练基线51.9%和最强跨相机对比方法79.8%。

Conclusion: 子空间理论支持的特征解耦为跨相机协同的奶牛识别提供了有效范式，能够稳健提取相机无关的身份特征，推动在非受控智慧养殖场景中的精准个体监测与大规模落地。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [73] [Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding](https://arxiv.org/abs/2602.07568)
*Hui Ye,Shilong Yang,Yexuan Xing,Juan Yu,Yaoqin Xie,Wei Zhang,Chulong Zhang*

Main category: cs.CV

TL;DR: 提出MammoColor：用任务驱动的色彩编码（TDCE）把单通道乳腺X线片转为彩色视图，增强感知；在多数据集与MRMC读片实验中提升AUC与特异性，尤其在致密乳腺。


<details>
  <summary>Details</summary>
Motivation: 致密乳腺中灰度钼靶片对病灶对比度低、组织重叠多，放射科医师与算法均易漏检或误报；需要一种能突出与分诊任务相关特征、降低感知难度与假阳性的表示方式。

Method: 设计端到端框架MammoColor：轻量级任务驱动色彩编码（TDCE）模块与BI-RADS分诊分类器联合训练。TDCE将单通道乳腺片映射为任务优化的三通道彩色视图，整个系统在VinDr-Mammo上端到端训练；在内部测试集、CBIS-DDSM、INBreast与三个人群外部临床队列上评估；并进行带清洗期的MRMC读片实验，比较灰度、仅TDCE、灰度+TDCE并列显示三种条件。

Result: 在VinDr-Mammo上，AUC由0.7669提升至0.8461（P=0.004），致密乳腺中由0.749升至0.835；MRMC实验中，TDCE编码图像在保持相当敏感度的同时将特异性由0.90提升到0.96（P=0.052）。

Conclusion: 任务驱动的色彩编码可增强与分诊相关的感知显著性，在乳腺钼靶分诊中有望减少假阳性召回，尤其对致密乳腺更有益。

Abstract: Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.

</details>


### [74] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

TL;DR: ViCA 通过让视觉token绕过大部分Transformer自注意与FFN，仅在少数层用稀疏跨注意与文本交互，把视觉侧算力降到约4%，在多基座与多基准上保持≈98%性能，并带来显著推理加速（单batch>3.5x，多batch>10x）。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在每层对视觉与文本统一自注意，视觉token在全层密集参与，带来高昂计算且未必必要。作者观察：投影后的视觉嵌入已与语言空间对齐，且有效的视-文交互只发生在少数层，因此有动机去除冗余的视觉密集处理。

Method: 提出ViCA：视觉token完全绕过各层自注意与前馈，仅在选定的少数层通过视觉到文本的稀疏跨注意进行交互；选择这些层以维持对齐与信息注入；架构规则、硬件友好，可与token剪枝方法正交结合。

Result: 在3个MLLM骨干、9个多模态基准、对比26个剪枝基线中，ViCA以仅约4%的视觉侧计算维持≈98%的基线精度；推理端实现显著加速：单batch超过3.5倍，多batch超过10倍；将视觉定位开销降至接近纯文本LLM水平。

Conclusion: 稀疏的跨注意足以支撑有效的视-文融合，无需在所有层对视觉进行密集处理。ViCA在保持性能的同时显著提升效率，并可与剪枝等方法叠加，提供更好的性能-效率折中与工程友好推理流水线。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [75] [Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling](https://arxiv.org/abs/2602.07590)
*Jessica Ka Yi Chiu,Tom Frode Hansen,Eivind Magnus Paulsen,Ole Jakob Mengshoel*

Main category: cs.CV

TL;DR: 提出一种以地质为驱动的机器学习方法，将离散裂隙网络（DFN）参数化建模与合成数据生成相结合，并通过监督图像分割实现自动化岩体节理迹线提取；在数据稀缺与类别不平衡条件下，混合训练与微调分别适用于标签一致与噪声较大的情形。


<details>
  <summary>Details</summary>
Motivation: 现场真实标注数据稀缺、质量参差且类别极不均衡，导致传统监督分割难以鲁棒识别岩体节理迹线；同时纯视觉方法缺乏地质先验，难以保持节理的延续性、连通性与节点类型等几何-拓扑特征。

Method: 1) 以DFN为核心的参数化地质建模，生成在尺度与统计特性上贴近实测的合成节理图像（保持延续性、连通性、节点类型分布）；2) 采用“合成+真实”的混合训练与“先预训练后微调”的两种策略训练分割模型；3) 在箱体与边坡两个域、多个真实数据集上评测，包括零样本、混合训练与小样本微调。

Result: - 合成数据可在真实数据稀缺时有效支撑监督式迹线检测；- 标签一致性高（箱体域）时，混合训练表现佳；- 标签噪声大（边坡域）时，预训练后的小样本微调更稳健；- 纯零样本从合成到真实的可迁移性有限，但用少量真实样本微调可实现有用泛化；- 质性结果显示得到的迹线更清晰且地质意义更强，超出定量指标单独反映的效果。

Conclusion: 将地质先验注入机器学习流程，能在数据受限和标签噪声场景下提升岩体节理映射的可靠性；建议未来在领域自适应与更合理的评测指标上继续深化，以更全面反映地质意义与工程可用性。

Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.

</details>


### [76] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出一个面向视频生成器的系统化后训练框架，将监督策略塑形、奖励驱动的强化学习与偏好优化整合到带稳定性约束的同一优化栈中，以提升感知质量、时序一致性与指令遵循，同时保持可控性与部署稳定性。


<details>
  <summary>Details</summary>
Motivation: 预训练视频生成模型在生产场景中需要具备指令跟随、可控性和长时间尺度的稳健性，但现实中存在高代价的长序列回放、误差随时间累积、以及异质且噪声较大的反馈信号等难点。缺乏系统化、可扩展且稳定的后训练流程会导致模型不可靠、不可部署。

Method: 将后训练视为分阶段、以诊断为导向的优化过程：1) 监督策略塑形（SFT/监督微调）用于建立基础的指令遵循与可控性；2) 基于奖励的强化学习在稳定性约束下优化感知质量与时序一致性；3) 偏好驱动的精炼（如基于人/模型偏好的比较信号）进一步对齐弱辨别的反馈；同时在优化中显式纳入稳定性约束与实用限制（高rollout成本、时序复合失败）以维持训练可扩展与稳健。

Result: 得到一套统一、可扩展且可部署的后训练“配方”，在不牺牲可控性的情况下提升了视频的感知保真度、时间一致性与对提示的遵循；并提供了清晰的工程蓝图以应对现实部署中的高成本与不确定反馈。

Conclusion: 通过将监督、强化和偏好优化整合到稳定性约束的单一优化栈，并以阶段化与诊断驱动方式执行，可构建稳定、可扩展、适用于真实世界的视频生成后训练流水线，兼顾质量、时序与可控性。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [77] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出Fine-R1：一种面向细粒度视觉识别（FGVR）的MLLM训练框架，通过CoT监督微调与三元组增强的策略优化，在仅4-shot下优于通用/推理型MLLM与CLIP，对已见与未见子类均有效。


<details>
  <summary>Details</summary>
Motivation: 通用MLLM在粗粒度任务表现强，但在FGVR上依赖大量标注、泛化到未见子类差，且相比CLIP等专用判别模型有明显性能差距；需要一种能以极少样本获取判别力与开放世界泛化的方案。

Method: R1风格两阶段：1) CoT监督微调：构建含“视觉分析-候选子类-对比-预测”链式推理的数据集，引导模型开放世界分类与可解释推理；2) 三元组增强策略优化（TAPO）：- 类内增强：将同类的anchor与positive轨迹混合，提升对类内变化的鲁棒性；- 类间增强：利用不同子类图像，最大化条件响应差异，强化判别性。整体采用policy optimization对生成式解码进行训练。

Result: 在仅4-shot训练下，Fine-R1在已见与未见子类识别上均超过现有通用与推理型MLLM，并且超越对比学习的CLIP基线，显示出更强的细粒度泛化与判别能力。

Conclusion: 以最小监督通过CoT与三元组增强优化，可将MLLM有效适配到FGVR；方法在低标注、知识密集领域具实用潜力，并改善对未见子类的开放世界泛化。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [78] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出HistoMet：一种决策感知、概念对齐的多实例学习框架，从原发肿瘤WSI预测转移风险并在高风险病例上条件性预测转移部位，在多机构6504例泛癌队列上实现高灵敏度筛查下显著减负且保持高召回，并在转移病例上达到较高宏平均F1与AUC。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理多将“是否转移”和“转移部位”视为彼此独立任务，未显式模拟临床先评估转移风险、后判定转移部位的顺序决策流程，导致可部署性与可解释性不足。

Method: 构建决策感知的两阶段MIL管线：第一阶段从原发肿瘤WSI估计转移发生概率；第二阶段仅对高风险样本进行条件性转移部位分类。为提升表征学习与临床可解释性，融合由预训练病理视觉-语言模型提供的语言定义与数据自适应的“转移概念”进行概念对齐监督。

Result: 在包含6504名患者、具随访与部位标注的多机构泛癌数据上评估：在95%灵敏度的筛查设定下，HistoMet显著减少下游工作量且保持高转移风险召回；在已确定转移的病例上，宏平均F1=74.6±1.3，宏平均一对其余AUC=92.1。

Conclusion: 将临床顺序决策结构显式纳入并用概念对齐引导学习，可在原发肿瘤病理切片上实现稳健、可部署的转移发生与部位预测，兼顾性能、工作量减负与可解释性。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [79] [AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning](https://arxiv.org/abs/2602.07625)
*Binxiao Xu,Junyu Feng,Xiaopeng Lin,Haodong Li,Zhiyuan Feng,Bohan Zeng,Shaolin Lu,Ming Lu,Qi She,Wentao Zhang*

Main category: cs.CV

TL;DR: AD-MIR提出面向广告视频的两阶段多模态理解框架：先将视频构造成结构化记忆库，后用结构化推理代理基于证据迭代分析说服策略；在AdsQA上达SOTA，优于通用代理DVD。


<details>
  <summary>Details</summary>
Motivation: 通用多模态代理在广告理解上难以从像素层面感知过渡到高层营销逻辑，特别是识别品牌细节、过滤噪声并推断隐含劝服策略的能力不足。

Method: 两阶段：1) 结构感知记忆构建：将原始视频转为结构化数据库，结合语义检索与精确关键词匹配，突出品牌细节（logo、字幕/屏幕文字），过滤无关背景并锁定关键角色；2) 结构化推理代理：模仿营销专家，通过迭代提问分解叙事，采用基于证据的自校正机制，将推断与具体帧对齐，若缺乏视觉支持则回溯修正。

Result: 在AdsQA基准上达SOTA，相比最强通用代理DVD，严格准确率提升1.8%，宽松准确率提升9.5%。

Conclusion: 有效的广告理解需要将抽象营销策略显式锚定到像素级证据。AD-MIR通过结构化记忆与证据驱动推理实现更优表现，并开源代码以促进复现与扩展。

Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.

</details>


### [80] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 论文构建UMD全身分子影像数据集，并用其系统评估3D医学分割基础模型在不同成像模态上的稳健性，发现从结构到功能模态时性能显著下滑，呼吁多模态训练与评测以实现真正的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学基础模型被期望具备通用能力，但验证多集中在结构/区域影像（如CT/MRI结构），缺乏对功能模态（如PET）以及跨模态泛化的严格评估；文献基准与真实应用之间可能存在显著落差，需要客观数据与方法揭示并量化这一差距。

Method: - 构建UMD数据集：包含490例全身PET/CT、464例全身PET/MRI（约67.5万张2D图与约1.2万个3D器官标注）。
- 选择具有代表性的3D分割基础模型进行系统评测。
- 基于同一受试者成对扫描（PET/CT与PET/MRI）的受试者内对照，隔离成像模态为主要自变量，评估模型在结构到功能以及跨模态场景的稳健性与泛化。

Result: 评估显示：文献报告的高分数与真实跨模态应用表现存在巨大差距；当从结构域（CT/MRI结构）转向功能域（PET相关）时，模型性能显著劣化，出现系统性失效，表明当前3D基础模型远未达到真正的通用状态。

Conclusion: 需要从单一结构模态的理想化基准转向覆盖结构与功能的多模态训练与评测范式；UMD数据集与本研究的分析为构建模态无关的医学基础模型提供了关键基石与客观参照。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [81] [From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding](https://arxiv.org/abs/2602.07645)
*Leonardo Gonzalez*

Main category: cs.CV

TL;DR: 提出Images2Slides：将静态信息图（PNG/JPG）转为可编辑Google Slides页面的API流水线，借助VLM提取区域规范、映射几何并用Slides API重建；在合成基准上元素找回率高（~0.989），文本CER低（~0.033），布局IoU中等；总结工程难点与失效模式。


<details>
  <summary>Details</summary>
Motivation: 信息图一旦导出为位图即难以更新、本地化与复用；需要从像素级图像中恢复成可编辑、结构化的演示文稿元素以降低后续编辑成本。

Method: 构建模型无关的流水线：1) 用视觉-语言模型从输入图片提取区域级JSON规范（文本/图像区域、坐标等）；2) 将像素坐标映射到Slides坐标；3) 通过Google Slides批量更新API创建文本框、图片等；4) 采用确定性后处理，支持多种VLM后端。

Result: 在29个可控、程序生成的带真值区域的基准上：整体元素找回率0.989±0.057（文本0.985±0.083，图像1.000±0.000）；文本转录误差CER=0.033±0.149；布局一致性IoU：文本0.364±0.161、图像0.644±0.131。

Conclusion: 该系统能高精度恢复元素与文本，但布局几何仍有改进空间；工程挑战包括文本字号标定、非均匀背景处理等；给出失效模式为未来改进方向。

Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.

</details>


### [82] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 研究评估从医学影像到3D模型重建管线中各环节误差，比较不同分割算法与几何类型下体素与表面度量的差异，发现Otsu总体最稳健，薄壁与类不平衡及配准敏感性显著影响准确性；Jaccard较Dice更严格，需确保体素与点云配准一致。


<details>
  <summary>Details</summary>
Motivation: 医学3D重建精度受硬件、分割与网格处理等多因素影响，但几何形状类型、类别不平衡、体素/点云配准对精度的系统性效应尚缺乏深入量化与对比。需要一套贯穿体素与表面层面的评估框架来识别误差来源并指导方法选择。

Method: 打印三种代表性几何（球体、面罩、AAA动脉瘤薄壁）并以微CT扫描；用GMM、Otsu、区域生长(RG)分割；体素级用KU算法配准，计算Dice、Jaccard、Precision、Specificity等；表面级以ICP配准，评估Chamfer距离与平均Hausdorff距离；比较不同几何与算法在两类度量间的一致性与分歧。

Result: Otsu在整体上对各几何表现最稳；AAA因壁薄与配准误差导致重叠指标低、类不平衡使Specificity最受影响；表面度量与体素度量趋势不一致；RG在球体最好，GMM与Otsu在AAA更佳；面罩表面误差最高，推测源于ICP配准问题；Jaccard较Dice更严格，薄壁结构评估更合适。

Conclusion: 分割精度是重建流水线多阶段误差的叠加；高体素级指标在类不平衡与配准敏感场景可能误导；评估薄壁结构宜采用Jaccard；进行可靠评估须确保体素与点云配准一致，并针对几何类型选择合适的分割与度量组合。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [83] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

TL;DR: 该论文在原有“看内看外”(LILO)方案基础上，引入音频模态，形成“看听内外”(L‑LIO)多模态框架，通过车内外音频与视觉融合提升对驾驶员/乘客/外部行人的理解与安全干预能力，并在三类用例中给出初步有效性证据，同时讨论噪声、隐私与跨人群鲁棒性等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有智能车多依赖视觉感知，对驾驶员状态与复杂路侧互动的把握有限；许多关键安全线索（醉酒语音特征、乘客口头导航、交警口令/哨声等）存在于声音中，单视觉难以覆盖。因而需将音频纳入统一框架，提升在真实复杂环境下的安全与可靠性。

Method: 提出L‑LIO框架：将车内外音频与视觉传感进行多模态融合，用于驾驶员状态评估与环境理解。通过三项案例评估：1) 监督学习利用驾驶员语音分类受损状态（如酒醉）；2) 采集与对齐乘客自然语言指令（如“在红色建筑后转弯”）以对接规划系统；3) 在视觉歧义场景中利用音频消歧外部交通参与者的引导/手势。使用实地采集的车内与车外音频数据集进行试验。

Result: 试点结果表明：在语义细腻或上下文丰富的情境中，音频能提供与安全高度相关的洞见，尤其当纯视觉不足以决策时表现更佳。

Conclusion: 将音频纳入LILO形成L‑LIO能增强驾驶员与场景理解，开辟新的安全干预路径；但仍面临环境噪声、隐私与跨主体鲁棒性等挑战，需在动态真实环境中进一步验证与提升可靠性。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [84] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 论文探讨如何将视觉-语言模型用于自动驾驶中的安全评估与决策：用CLIP做类目无关的快速风险筛查；把场景级VL嵌入接入Transformer规划器但发现“直接喂全局嵌入”无效；用自然语言作为规划的显式行为约束在含歧义场景提升安全性。结论：VL表示对表达语义风险、意图与行为约束有潜力，但需任务对齐与工程化落地。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在开放世界中会遇到长尾与分布外风险，传统检测/规划难以覆盖全部语义与罕见情形。VLM能以自然语言桥接视觉与高层语义，有望提升安全相关的感知与决策能力；作者想检验其在系统层面三个环节（风险筛查、规划条件、语言约束）中的实际效用与局限。

Method: 1) 使用CLIP图文相似度，构造轻量、类目无关的语义风险信号，实现无需显式检测/QA的低时延隐患筛查。2) 将场景级VL嵌入作为条件接入基于Transformer的轨迹规划器（Waymo数据）以评估对轨迹精度影响。3) 在doScenes数据上，用与场景元素绑定的“乘客式”自然语言指令作为规划的显式约束，观察对罕见严重失败与含糊场景行为的影响。

Result: - CLIP相似度可稳健发现多样及分布外道路隐患，延迟低且无需类别定义。- 直接用全局VL嵌入条件化规划并未提升轨迹精度，显示表示与任务不匹配的问题。- 语言行为约束可抑制罕见但高危的规划失败，并在模糊场景中更贴合安全偏好。

Conclusion: VL表示在自动驾驶中最有价值的角色是表达语义风险、意图与行为约束；要释放其潜力，需任务知情的表示抽取、结构化落地与系统工程设计，而非把全局特征生硬注入规划器。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [85] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: 提出用于视频推理的PoT框架，将长视频的多步推理分解为可验证的轻量步骤（证据选择→状态更新→受约束答案生成），可与现有视觉-语言模型结合，显著提升事实正确性与时间对齐，并生成可解释的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 视频理解不仅要识别内容，还要在长且嘈杂的序列中进行时间定位与多步推理；现有方法易受干扰、产生幻觉解释、缺乏可追溯性与可诊断性。

Method: 提出Process-of-Thought (PoT) 推理：1) 时间证据选择，定位与问题相关的片段；2) 逐步状态更新，基于已选证据迭代完善假设；3) 受约束答案综合，保证与证据一致。提供统一PoT轨迹表示，将中间决策与时间段对齐；框架与模型无关，可插入现有视觉-语言骨干，支持闭卷与借助外部工具的证据增强推理。

Result: 在标准视频推理任务上，PoT相较基线持续提升事实正确率与时间落地性，并减少幻觉与干扰影响；实验展示了可解释的推理轨迹，有利于诊断与下游使用。

Conclusion: 显式化、可验证的分步视频推理能提高鲁棒性与可解释性；PoT作为模型无关的插件式框架，为长视频多步推理提供有效范式并改善时间对齐与答案可靠性。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [86] [Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes](https://arxiv.org/abs/2602.07694)
*Wenping Jin,Yuyang Tang,Li Zhu*

Main category: cs.CV

TL;DR: 提出CoalAD基准与一种用于输送带煤流场景的无监督异物异常检测与像素级定位方法，在复杂无结构环境下实现稳健图像级判别与精确像素级分割，并显著优于主流基线。


<details>
  <summary>Details</summary>
Motivation: 煤矿输送带场景中煤与矸石堆叠无序、背景多变、异物低对比且易形变/遮挡，导致传统依赖稳定结构假设的工业异常检测在此退化。缺乏针对该场景的公开基准也限制了方法评估与比较。

Method: 构建CoalAD无监督基准；提出“互补线索协同感知”框架，从三类互补证据联合检测与定位：1) 目标级语义组成建模，学习正常语义结构；2) 基于语义归因的全局偏差分析，量化图像整体与正常分布的偏离；3) 细粒度纹理匹配，捕捉局部外观异常；多分支结果在融合后输出稳健图像级分数与精确像素热图。

Result: 在CoalAD上，该方法在图像级与像素级指标上均超过常用基线；消融实验显示三类互补分支均有显著贡献，融合最优。

Conclusion: 面向煤流无结构场景，互补线索协同感知能有效提升无监督异物检测与定位的鲁棒性与精度；CoalAD为该领域提供标准化评测平台与数据支撑，代码已开源。

Abstract: Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.

</details>


### [87] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

TL;DR: 提出U-KABS：把U-Net式编码器-解码器与KAN（Kolmogorov‑Arnold Networks）中的Bernstein与B样条激活融合，用于更好刻画医学图像中的非线性与多尺度结构，跨多数据集分割精度优于强基线，尤其在复杂解剖结构上。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割受限于图像复杂性与高度变异性，传统卷积网络难以充分建模非线性关系与兼顾全局平滑趋势与局部细节，需要一种能同时处理宽域上下文与细粒度边界的模型。

Method: 在U形编码器-解码器中，编码阶段结合卷积与SE注意力增强通道表征；在关键阶段引入KAN的KABS模块，使用基于Bernstein多项式与B样条的可学习激活函数：Bernstein提供全局平滑性，B样条提供局部自适应性；通过跳跃连接实现多尺度特征融合与空间细节保留。

Result: 在多个医学影像基准上评估，U-KABS整体优于强基线方法，尤其在复杂解剖结构分割任务中表现更好（摘要未给出具体数值）。

Conclusion: 将KAN的可学习多项式/样条激活与U-Net架构融合，能更有效捕捉非线性与多尺度信息，提升医学图像分割性能；方法对复杂结构具有优势。

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [88] [All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving](https://arxiv.org/abs/2602.07717)
*Yingjie Li,Daniel Robinson,Cunxi Yu*

Main category: cs.CV

TL;DR: 提出一种用于自动驾驶中RGB语义分割与车道线检测的全光学计算框架，基于衍射光学神经网络（DONN），在Cityscapes与自建/仿真数据上验证有效与一定泛化性，同时具备更高能效与更低ADC开销。


<details>
  <summary>Details</summary>
Motivation: 传统DNN在自动驾驶的实时语义分割与车道检测任务中，需要大规模像素级计算与频繁模数转换，导致高能耗与时延瓶颈。全光学计算可在光速下完成卷积/传播类运算，并减少ADC/数据搬移开销，但缺乏在真实自动驾驶场景中的系统化验证。作者旨在用DONN验证全光学端到端图像理解（分割、车道检测）的可行性与优势。

Method: 构建基于衍射光学神经网络的端到端全光学推理系统：将RGB图像进行全光学编码，利用多层衍射元件实现前向传播与特征变换，在光场中直接输出表征分割/车道线的结果或中间强度分布；在数字域进行必要的解码与评估。以Cityscapes进行语义分割训练/测试，并在自定义室内赛道与CARLA仿真中进行车道线检测案例研究，考察不同环境条件下的泛化。

Result: 在Cityscapes上取得有效的语义分割性能；在自建室内赛道和CARLA仿真中成功完成车道线检测，显示出在多样环境条件下的可迁移性。相较传统DNN，系统在推理阶段具备能效优势与更低的ADC开销。

Conclusion: DONN可用于自动驾驶感知中的语义分割与车道检测，兼具能效与实时性潜力，并在标准与仿真数据集上表现出有效性与一定泛化。为全光学计算在实际自动驾驶场景落地提供了可行路径与实证基础。

Abstract: Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.

</details>


### [89] [PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification](https://arxiv.org/abs/2602.07768)
*Qiuming Luo,Yuebing Li,Feng Li,Chang Kong*

Main category: cs.CV

TL;DR: 提出PAND，用提示感知的语义校准与邻域结构蒸馏两阶段框架，把大规模视觉-语言模型的知识有效蒸馏到轻量学生网络，在四个细粒度分类基准上均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: FGVC依赖细粒度差异，现有将VLM知识迁移到小模型的方法多用固定提示与全局对齐，导致语义错配与忽视局部判别结构，蒸馏效果受限。

Method: 两阶段解耦：1) 提示感知语义校准（Prompt-Aware Semantic Calibration），结合类名/属性与VLM生成自适应语义锚点，从而为每类构建更贴合图像分布的文本表示；2) 邻域感知的结构蒸馏，约束学生在特征空间/预测空间中的局部决策结构，使相邻样本保持与教师一致的相对关系与边界。整体不依赖固定prompt和纯全局对齐。

Result: 在四个FGVC基准上持续优于SOTA；以ResNet-18学生在CUB-200上达76.09%准确率，较强基线VL2Lite提升约3.4%。

Conclusion: 将语义校准与结构迁移解耦的PAND，有效缓解固定提示与全局对齐的局限，提升轻量模型在FGVC中的判别力与泛化能力，并具备稳定的跨数据集优势。

Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.

</details>


### [90] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: 提出一种训练免调参的方法Rolling Sink，解决自回归视频扩散模型在测试长时域时的劣化问题，将仅在5秒片段上训练的模型扩展到分钟级（5–30分钟，16FPS）生成，显著提升长时一致性与保真度。


<details>
  <summary>Details</summary>
Motivation: AR视频扩散模型在短时训练下，长时测试会出现快速视觉崩塌（颜色漂移、结构破坏、主体不一致），存在训练-测试时域不匹配。长视频训练代价极高，需在不增加训练成本的前提下弥合超出训练窗口的时域鸿沟。

Method: 系统分析AR推理中的缓存（cache）维护策略，并在Self Forcing框架上提出Rolling Sink：一种训练免调参的缓存滚动/清洗机制，稳定长期依赖与信息流，抑制误差积累与分布漂移，从而在开放式长时生成中保持主体、颜色、结构与运动的稳定与连贯。

Result: 在仅使用5秒训练片段的前提下，测试时可稳定生成5–30分钟、16FPS的长视频；在多项实验中，相比SOTA基线，Rolling Sink在长时视觉保真度与时序一致性上更优，减少了退化现象。

Conclusion: 训练-测试时域鸿沟可通过训练免调参的缓存维护策略显著缓解。Rolling Sink将短训AR视频扩散模型扩展到超长时生成，实证优于现有方法，展示了高效可扩展的长视频生成路径。

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [91] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: 提出UCATSC：在部分可观测与不确定感知下，以约束随机决策过程建模路口信号控制，离线在信念空间反事实推演并强制执行安全与饥饿防止等硬约束，兼顾可解释性、延误与排放改进，避免RL基于奖励塑形的隐式安全。


<details>
  <summary>Details</summary>
Motivation: 现实部署受限：视觉感知不确定导致状态噪声；安全性多依赖仿真与奖励塑形无法保证硬约束；策略不可解释；需要在真实噪声与部分可观测条件下提供可解释且可验证的安全控制。

Method: 将单路口信号控制建模为带约束、部分可观测的随机决策过程（POMDP/Constrained MDP）。在信念空间进行反事实滚动推演（counterfactual rollouts），显式预测并施加安全与“防饥饿”（防长时间不放行）硬约束；利用视觉感知不确定性构建概率式观测模型；输出基于显式模型的可解释控制策略。

Result: 系统在考虑感知不确定性的前提下，降低交通延误与排放，并避免安全关键错误（如冲突），同时给出可解释的控制输出；对比基于RL的奖励塑形方法，实现更严格的安全保障。

Conclusion: UCATSC通过约束化、模型驱动与信念空间规划，将安全与可解释性内化为硬约束，适用于现实世界部署，能在不牺牲效率的前提下提升安全与环保表现。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [92] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

TL;DR: 提出VideoTemp-o3，一个将视频定位与问答联合建模的“agentic thinking-with-videos”框架，支持按需裁剪、强定位与纠偏，并通过统一掩码机制与强化学习奖励设计，结合高质量长视频标注数据与基准，显著提升长视频理解与定位性能，减少幻觉与无效采样。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解多用均匀采样，难以抓住关键证据，性能差且幻觉增多；新兴的本地化-裁剪-作答范式虽有改进，但仍存在效率低、定位弱、流程僵化等问题。需要一个高效、可自我纠偏、定位强、端到端联合的视频理解方案。

Method: 提出VideoTemp-o3统一框架：1) 将视频定位与QA联合建模，支持按需裁剪并可对不准的定位进行迭代修正；2) 监督微调阶段引入统一掩码机制，在探索相关片段的同时抑制噪声；3) 强化学习阶段设计专用奖励以缓解reward hacking；4) 构建高质量长视频定界QA数据与覆盖多时长的评测基准，用于系统训练与评估。

Result: 在多个长视频理解与视频定位任务上取得显著性能提升，较现有方法在准确率与定位质量方面都有明显优势，同时降低了幻觉与无效帧采样带来的开销。

Conclusion: 联合建模与可纠偏的agent式视频思考框架能更好地解决长视频中的关键信息捕获问题；配合合理的监督掩码与RL奖励以及高质量数据与基准，VideoTemp-o3在长视频理解与 grounding 上都达到先进水平。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [93] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 本文对16种最先进AI生成图像检测方法（23个预训练变体）在12个数据集上的零样本表现做首个系统评估，涵盖291个生成器与260万图像。结果显示：无通吃检测器、排名极不稳定；最佳与最差相差37个百分点；训练数据对齐主导泛化；商业生成器显著攻破多数检测器；并揭示三类跨数据集系统性失效。统计检验证实检测器间差异显著，提示部署需按威胁场景精挑而非迷信榜单。


<details>
  <summary>Details</summary>
Motivation: 现有深假图检测研究多在微调后评测，忽视“开箱即用”情境，而实际部署往往依赖零样本能力。为弥补该基准空白，作者系统刻画预训练检测器在多源、多生成器、多分布下的真实鲁棒性与可迁移性。

Method: 构建覆盖广的评测基准：16种SOTA检测方法（23预训练版本），12个多样化数据集，约260万张图像，涉及291个生成器（含最新扩散与商业模型）。在严格零样本设定下评估并进行跨数据集排名一致性、性能方差、训练数据对齐影响、对现代商业生成器鲁棒性与系统性失败模式的分析；使用Friedman检验与Kendall一致性评价统计显著性。

Result: - 无单一最优：跨数据集Spearman ρ在0.01–0.87，排名高度不稳。
- 性能跨度大：最佳均准75.0%，最差37.5%，相差37个百分点。
- 训练数据对齐显著影响泛化：同架构不同训练数据可致20–60%性能差异。
- 面对Flux Dev、Firefly v4、Midjourney v7等商业生成器，平均准确率仅18–30%。
- 识别出三类跨数据集系统性失效模式。统计上差异显著（χ²=121.01, p<1e-16, Kendall W=0.524）。

Conclusion: “一招鲜吃遍天”的检测器不存在；零样本部署需基于具体威胁模型与数据分布择优，并关注训练数据与目标域的对齐。已发布榜单不足以指导实际落地，应依据场景做组合、集成与持续校准，同时警惕商业生成器带来的显著挑战。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [94] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 首次系统性比较VLM与专用年龄估计模型：零样本VLM在8数据集上显著胜出（平均MAE 5.65 vs 9.88），最佳VLM优于最佳非LLM 15%。非LLM在18岁阈值下误判未成年为成年高达60–100%，VLM仅13–25%。粗粒度分箱显著恶化误差；极端年龄最难。建议将VLM能力蒸馏到高效专用模型。


<details>
  <summary>Details</summary>
Motivation: 年龄估计对内容审核、年龄核验、深伪检测关键，但缺乏跨范式基准系统比较现代VLM与专用架构的性能与行为，导致研究方向与实际部署缺少客观指导。

Method: 构建跨范式大规模基准：评测34个模型（22个专用、12个通用VLM）在8个标准数据集上，共每模型1100张测试图像。度量连续年龄MAE、18岁阈值验证（误判率），以及不同年龄段与年龄分箱设置下的分层分析。

Result: 零样本VLM整体优于大多数专用模型：平均MAE 5.65年 vs 9.88年；最佳VLM（Gemini 3 Flash Preview，MAE 4.32）优于最佳非LLM（MiVOLO，MAE 5.10）约15%。非LLM在18岁阈值下未成年误判为成年率为60–100%，VLM为13–25%。将年龄离散为8–9类使MAE劣化至>13年。所有模型在<5岁与65+段表现最差。

Conclusion: 任务特定架构并非年龄估计必要条件；VLM具显著零样本优势。建议将VLM能力蒸馏/迁移到高效专用模型，并重点攻克极端年龄段与更细粒度回归而非粗分箱。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [95] [Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction](https://arxiv.org/abs/2602.07820)
*Zhibo Chen,Yu Guan,Yajuan Huang,Chaoqi Chen,XiangJi,Qiuyun Fan,Dong Liang,Qiegen Liu*

Main category: cs.CV

TL;DR: 该论文提出面向SMS（同时多切片）+ 面内欠采样的高加速MRI重建的新框架，通过显式建模采集算子与确定性更新实现“算子对齐”的反演，并以OCDI-Net解耦目标切片与切片间串扰，先做切片分离再做面内补全，较传统与学习方法提升保真度并降低串扰泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有扩散/先验驱动的重建多以高斯噪声退化为核心假设，对SMS物理（确定性的切片串扰、k-space缺失）处理往往依赖额外一致性步骤，导致与实际由采集算子主导的退化不匹配，性能受限。需要一种直接由已知采集算子指导、与退化轨迹同构的重建方法。

Method: 提出“算子引导”的重建框架：使用已知采集算子显式建模SMS与面内欠采样的退化路径，并通过确定性（算子对齐）的更新进行反演。设计OCDI-Net（算子条件的双流交互网络）：显式分离目标切片内容与跨切片干扰，预测具结构性的退化量以配合算子一致的反演。重建以两阶段串联推理实现：1）SMS切片分离；2）面内数据补全。

Result: 在fastMRI脑数据与前瞻性体内扩散MRI数据上，相比传统与学习型SMS重建，取得更高的重建保真度并显著降低切片泄漏（跨切片串扰）。

Conclusion: 通过将采集算子纳入模型并以算子对齐的确定性反演驱动学习网络，能够更好契合SMS物理退化，OCDI-Net与两阶段链式推理在高加速SMS MRI中提升了成像质量、减少了串扰，展示了算子引导重建的有效性。

Abstract: Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.

</details>


### [96] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 提出OTA-Det，将开放词汇航拍检测（OVAD）与遥感视觉指代（RSVG）统一到单一实时框架，在六个基准上达SOTA并以34 FPS运行。


<details>
  <summary>Details</summary>
Motivation: 现有OVAD仅支持类别级粗语义，RSVG又受限于单目标定位，二者各自难以同时实现细粒度语义理解与多目标检测，限制了航拍场景的全面理解与落地效率。

Method: 1）任务重构：统一OVAD与RSVG的目标与监督形式，使来自两种范式的数据可在同一模型中联合训练，获得稠密监督。2）稠密语义对齐：在整体描述、局部短语到属性多粒度上建立显式对齐，提高细粒度语义理解与定位能力。3）高效架构：以RT-DETR为骨干，将闭集检测扩展为开文本检测，加入若干高效模块以保持实时性。

Result: 在覆盖OVAD与RSVG的六个基准数据集上取得SOTA，同时在推理阶段保持34 FPS的实时速度。

Conclusion: OTA-Det首次将OVAD与RSVG统一到单一高效架构，实现细粒度语义与多目标检测的兼顾，并在多基准上验证了其准确性与实时性，可作为开放词汇遥感理解的通用方案。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [97] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 提出SPD-Faith Bench评测MLLM推理“忠实度”，并给出无训练的SAGE框架以缓解视觉-推理不一致，揭示感知失明与感知-推理解耦两类系统性失败。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型常用CoT来提升可解释性，但其生成的推理链是否真实反映视觉证据并不清楚。以往研究多关注感知层面的幻觉，忽视了推理层面的不忠实。为剥离语言先验影响，需要一个强制显式视觉比较、能诊断推理-证据一致性的基准。

Method: 1) 构建SPD-Faith Bench：基于细粒度图像差异推理（显式视觉比对）以隔离语言先验并检验推理链的证据一致性；2) 对SOTA MLLMs诊断，发现两种失败模式，并通过注意力与残差流表示分析定位成因（视觉注意衰减与表示偏移）；3) 提出SAGE：无需训练的视觉证据校准框架，改进视觉信息路由并将推理对齐到感知。

Result: 在SPD-Faith Bench上，多种最先进MLLM出现显著的“感知失明”和“感知-推理解耦”，其根源与视觉注意力衰减、残差流表示偏移相关。引入SAGE后，可在无需训练的条件下提升视觉证据利用与推理-感知一致性。

Conclusion: 仅以答案正确性评估不足以衡量模型可靠性，需专门评估“忠实度”。SPD-Faith Bench为诊断提供工具，SAGE作为简单无训练方法可缓解问题，强调加强视觉证据对齐与路径路由的重要性。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [98] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: VFace是一种无需训练、可即插即用的视频人脸替换方法，基于扩散模型的图像换脸框架之上，通过频谱注意力插值、目标结构注意力注入和光流引导的注意力时序平滑三招，同时提升身份保真与时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的逐帧图像换脸在视频场景中常出现身份特征丢失与时序不一致（闪烁、抖动），且往往需要额外训练或微调，工程成本高、泛化性差。作者希望在不改动、不过训练底层扩散模型的前提下，为视频换脸提供可插拔的模块以增强时空一致性与视觉质量。

Method: 在扩散模型推理阶段插入三种注意力层面的模块化操作：1) 频谱注意力插值（Frequency Spectrum Attention Interpolation），在频域融合以保留关键身份特征；2) 目标结构引导（Target Structure Guidance），以即插即用的注意力注入，将目标帧的结构特征对齐到生成过程；3) 光流引导的注意力时序平滑（Flow-Guided Attention Temporal Smoothening），利用光流在相邻帧间约束注意力映射，提升时空一致性，且无需任何再训练或特定视频微调。

Result: 在多组视频换脸实验中，VFace显著降低逐帧生成的时序不一致与闪烁，提升身份保真与视觉真实感；可与多种扩散式图像换脸方法无缝结合，表现出更高的时空一致性与画面质量。

Conclusion: VFace提供了一个无需训练、模块化、可插拔的解决方案，可在不修改底层扩散模型的前提下，为视频人脸替换显著提升身份保持与时空一致性，具有实际落地价值与良好兼容性。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [99] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 提出ViewRope，一种将相机光线方向注入视频Transformer自注意力的几何感知编码，并配合几何感知的稀疏帧注意与评测基准ViewBench，以缓解长序列中的几何漂移与回环失败，显著提升长期一致性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 交互式AI需要可在显式相机控制下预测未来观测的世界模型；现有方法在长轨迹中缺乏空间持久性，重访位置时结构不稳、细节幻觉。作者将根因归结为依赖屏幕坐标位置编码，与投影几何不匹配，导致3D一致性破坏与几何漂移。

Method: 1) ViewRope：在视频Transformer的自注意力中，以相对相机光线（ray）几何替代屏幕空间位置编码，直接用光线方向参数化注意力，形成对3D一致性的先验偏置；2) Geometry-Aware Frame-Sparse Attention：利用上述几何线索选择性回溯历史帧，实现高效且一致的记忆检索；3) ViewBench：用于诊断回环一致性与几何漂移的评测套件。

Result: 在作者提出的ViewBench等评测上，模型在长时一致性（回环闭合、几何漂移控制）显著优于基线，同时计算成本降低。

Conclusion: 将注意力从像素局部性转向相对光线几何，为视频世界模型提供与3D投影相容的归纳偏置，可在不牺牲效率的情况下提升长期空间一致性；配套基准验证其有效性。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [100] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 提出一种用于从超高速运动模糊图像中恢复3D形状的可微分逆渲染方法，核心是快速重心坐标求解器，缓解渲染模糊时的计算瓶颈，实现最高4.57倍加速，并在快速平移与旋转场景中验证了前向模拟真实性与从2D到3D的重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统MVS等方法在极端运动模糊下失效，但这类场景在体育与工业中常见；需要既能真实模拟高速运动模糊、又能把梯度回传到几何上的渲染框架，实现从模糊图像中恢复3D形状。

Method: 提出可微分的逆渲染框架：将运动模糊视作在曝光时间内对几何投影的积分；识别常规做法对多帧平均时重复计算三角形重心权重是瓶颈；设计快速重心坐标求解器以减少重复计算、提升渲染效率；端到端可微，将图像梯度回传到形状参数；在两类运动（快速平移、快速旋转）上实现前向渲染与形状优化。

Result: 在前向模拟中实现真实且高效的超高速运动模糊渲染，速度最高提升4.57倍；在极端平移与旋转下，能从单/少张2D模糊图像中恢复物体3D形状，优于传统基线（间接）。

Conclusion: 快速重心坐标求解器使得可微分的运动模糊渲染高效可用，从而推动了在超高速运动场景中通过逆渲染恢复3D几何的能力；为自然与工业应用提供了有效的3D重建方案。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [101] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: SSI-Bench 是一个专注受限流形上的空间推理 VQA 基准，利用真实复杂3D结构、严格的几何/拓扑/物理约束来避免2D捷径，包含1000道排序题，评测显示当前VLM与人类存在巨大差距。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多在不受限场景上评测，容易依赖二维像素线索而非真正的三维空间理解；因此需要一个能强制模型遵循几何、拓扑与物理约束的基准，检验模型在受限结构中的空间智能。

Method: 构建 SSI-Bench：基于真实世界复杂3D结构与受限流形，人工严选图像、标注结构组件并设计问题以最小化像素级线索；问题覆盖几何与拓扑推理，包含心智旋转、截面推断、遮挡与力路径等组合空间操作；采用排名式问答形式，规模为1000题。

Result: 评测31个主流VLM：最佳开源模型准确率22.2%，最强闭源33.6%，远低于人类的91.6%；鼓励“思考链”等策略仅带来微弱提升；误差分析显示模型在结构落地与满足约束的一致性3D推理上失败。

Conclusion: SSI-Bench有效揭示VLM在真实受限3D推理中的短板，当前方法难以进行结构化、约束一致的空间理解；未来需加强三维结构表征、物理/拓扑约束建模与更稳健的空间推理机制。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [102] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: WristMIR 提出一种针对儿科腕部X线的区域感知检索框架，通过报告挖掘与骨骼定位联合训练全局与局部对比编码器，以两阶段（全局粗检索+按解剖区域重排序）提升相似病例检索与骨折判别性能。


<details>
  <summary>Details</summary>
Motivation: 腕部骨折模式细粒度、局部且易被解剖结构重叠与视角变化掩盖，导致相似病例检索困难；同时缺乏大规模、精标注的医学影像检索数据集，限制了基于案例的临床决策支持。

Method: 利用MedGemma对稠密放射学报告进行结构化挖掘，生成全局与区域级文字描述；对腕部X线进行预处理，并按解剖结构裁剪出桡骨远端、尺骨远端与茎突等骨特异区域；联合训练全局与局部对比学习编码器；推理时先以全局表征进行粗匹配，再基于指定骨区进行区域条件重排序，实现区域感知的两阶段检索。

Result: 相较强基线，图像到文本Recall@5由0.82%提升到9.35%；其嵌入用于骨折分类达到AUROC 0.949、AUPRC 0.953；在区域感知评估中，两阶段设计使基于检索的骨折诊断平均F1由0.568升至0.753；放射科医师对检索病例的临床相关性评分由3.36升至4.35。

Conclusion: 解剖学引导的区域级多模态对比学习与两阶段检索显著提升儿科腕部影像的相似病例检索与诊断支持效果，表明在小标注、细粒度病灶场景中具备较强的临床应用潜力；代码已开源（WristMIR）。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [103] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: SAGE 利用海量互联网视频，在缺乏三维标注与存在噪声的情况下，通过分层挖掘与混合监督，适配并提升几何基础模型的3D重建能力，在多数据集零样本评测中显著降低Chamfer距离。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型受限于稀缺、多样性不足的三维标注数据；互联网视频规模巨大却无GT几何且含噪，如何将其转化为可用监督以扩展模型能力成为瓶颈。

Method: 提出SAGE框架：1) 从原始视频中筛选“信息量高”的训练轨迹；2) 用SfM点云做稀疏几何锚点提供全局结构引导；3) 结合可微分的3D高斯渲染实现多视图稠密一致性监督；并以锚数据正则化避免灾难性遗忘。

Result: 在7Scenes、TUM-RGBD、Matterport3D等未见数据上零样本测试，相较SOTA基线将Chamfer Distance降低约20–42%，泛化显著提升。

Conclusion: SAGE首次系统性地将互联网视频用于几何基础模型适配，形成可扩展的通用3D学习范式，并在无GT条件下实现有效监督与稳健泛化。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [104] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: 提出TLQ：在VLM的PTQ中用“token级重要性”指导、并按层逐步在GPU上暴露量化路径进行校准，实现更稳健的量化与更低硬件要求。


<details>
  <summary>Details</summary>
Motivation: VLM中视觉与文本token的激活分布与对量化误差的敏感性差异大，传统PTQ用少量未加权样本做全局/层级校准常失效；同时实际推理路径与校准不一致、且需要大显存（如A100）。需一种既能细粒度反映token重要性、又能与真实量化路径一致且可在多张消费级GPU上运行的校准方案。

Method: 1) 基于梯度信息估计token级重要性，将其整合到量化误差度量中；2) 由此构建“token级校准集”，对不同token（视觉/文本）进行细粒度、加权的校准；3) 设计量化暴露的按层校准流程（layer-wise），确保校准时各层输入来自已量化的上游输出，贴近真实推理；4) 多GPU并行，将复杂的分层校准任务拆分到多张RTX3090，降低对A100大显存的依赖；5) 在两种量化设置、多个模型与规模上评测。

Result: 在两种模型、三种规模、两种量化设置下均带来稳定性能提升，显示更强的量化稳定性；同时在多张RTX3090上完成层级校准，缓解显存瓶颈。

Conclusion: 面向VLM的PTQ，应以“token级重要性”对齐校准目标，并保证校准路径与真实量化推理一致。TLQ在无需微调的前提下显著提升量化鲁棒性，并将高质量校准从A100迁移到多GPU消费级平台。

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [105] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 评估开源视觉语言模型在图像隐私属性零样本识别中的表现：VLM普遍更倾向标注为“有隐私属性”，在模型间高一致时能补充人类遗漏，显示其用于大规模隐私标注的潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模图像数据集需要隐私相关属性的标注，但人工标注成本高且易遗漏；VLM具备零样本能力，可能在无需专门训练的情况下辅助隐私属性识别，需系统评估其可靠性与与人类的一致性。

Method: 对开源VLM进行零样本评估：定义一组隐私相关属性；收集人类与VLM的标注；度量VLM间一致性与人类-模型一致性；分析VLM过度预测（偏向阳性）与人类遗漏的情况，并讨论分歧样例。

Result: 与人类标注相比，VLM更频繁预测存在隐私属性；在VLM间高一致的属性上，模型能够发现人类忽视的隐私线索；识别出若干属性类别具有较高的模型间一致性，并总结典型分歧场景。

Conclusion: VLM在隐私属性零样本识别上虽存在偏阳性倾向，但其高一致性时的补充作用可用于提高大规模数据集的隐私标注覆盖率；结合人类审核与不同比例阈值可提升实用性。

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [106] [Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps](https://arxiv.org/abs/2602.07938)
*Rabbia Asghar,Lukas Rummelhard,Wenqian Liu,Anne Spalanzani,Christian Laugier*

Main category: cs.CV

TL;DR: 提出一个统一预测框架，基于动态占用栅格与轻量时空主干，同时解码未来占用、车辆与场景流三种网格，通过相互依赖的损失联合建模，既预测具体车辆行为也覆盖未识别动态体，在nuScenes与Woven Planet上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有用占用栅格的预测多为“无体代理”的场景级预测，难刻画动态体复杂行为；“有体代理”的预测依赖语义与检测，难泛化到感知不全或未识别体。需要兼顾两者：既具行为细粒度，又对感知不确定与多未来鲁棒。

Method: 以动态占用栅格（DOGMa）为核心输入，构建轻量级时空骨干与简化的时间解码器，同时输出三类网格：未来占用状态网格、车辆专属网格、场景流（速度/位移）网格。设计互相依赖的损失函数，利用占用状态引导流一致性与转移（flow-guided transitions），作为正则处理障碍物与遮挡，鼓励多样未来。

Result: 在nuScenes与Woven Planet真实数据集上，对动态车辆与通用动态要素的预测精度超过基线方法；证明联合预测与互依损失带来更好行为刻画与泛化。

Conclusion: 统一的多网格联合预测与互依损失能同时覆盖代理特定与无体代理两类预测，提升对复杂交通场景中多主体、多未来的不确定性建模与鲁棒性，带来更安全的运动预测。

Abstract: Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.

</details>


### [107] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 提出一种面向未知监控场景的少样本人群计数方法，通过局部与全局密度特征共同引导模型，使其在跨场景泛化上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人群计数模型在不同摄像头/场景间分布差异大，现有模型对从未见过的监控场景泛化较差。将“场景”视为“类别”，借助少样本学习，让模型仅凭少量范例即可适配新场景，解决跨场景密度分布变化带来的性能退化。

Method: - 将每个监控场景视为一个类别的少样本任务（support + query）。
- 多重局部密度学习器：从support图像中学习多个原型，分别表征不同密度分布；构建多种局部密度相似度矩阵并编码，用于局部方式引导query的计数预测。
- 全局密度引导：从support图像抽取全局密度特征，作为全局条件对query预测进行调制。
- 组合局部与全局引导，实现对目标场景局部密度变化和整体密度水平的双重自适应。

Result: 在三个监控数据集上进行评测，在少样本人群计数设定下，对未见过的目标场景适配效果显著，整体性能优于最新SOTA方法。

Conclusion: 利用局部多原型密度与全局密度的联合引导，可有效提升人群计数模型的跨场景泛化与少样本适配能力，适用于实际多摄像头监控场景。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [108] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: D-ORCA 是一个面向多方对话的视频理解“全模态”大模型，用强化学习优化说话人归属、语义准确和时间边界对齐，在新构建的大规模双语对话数据集 DVD 上训练与评测，显著优于现有开源模型，并以 8B 参数在多项多模态基准上接近 Qwen3-Omni。


<details>
  <summary>Details</summary>
Motivation: 视频中口语对话是关键信息源，但现有模型在“谁在何时说了什么”的精确标注上薄弱，开源社区缺乏大规模高质量的多方对话音视频数据与专门的优化目标。

Method: 提出 D-ORCA：对话中心的全模态 LLM；构建 DVD 数据集（近 4 万训练视频、2000 测试视频，英/中双语，多方对话）。在策略优化中引入分组相对策略优化（GRPO），设计三类新奖励：说话人归属准确度、全局语音内容准确度、句级时间边界对齐；这些奖励来自语音处理常用评价指标，首次作为音视频字幕生成的强化学习目标。

Result: 在说话人识别、语音识别与时间定位上显著优于开源同类模型；在多项通用音视频理解基准上，8B 参数的 D-ORCA 表现与 Qwen3-Omni 具竞争力。提供演示、代码、数据与权重。

Conclusion: 以对话为中心的强化学习优化（结合语音评测指标）能显著提升音视频字幕的精细度与鲁棒性；新的双语多方对话数据集与方法为开源音视频理解提供强基线和资源。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [109] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: 提出EasyTune：逐步微调扩散模型以高效细粒度地与偏好对齐，并配合自精炼偏好学习（SPL）缓解偏好数据稀缺；在对齐度、显存与速度上显著优于DRaFT-50。


<details>
  <summary>Details</summary>
Motivation: 现有将可微奖励用于扩散模型对齐的方法虽有效，但存在优化粗糙且低效、显存开销大。作者分析其根因在于扩散去噪轨迹中各步之间的递归依赖，导致端到端优化难以精细、训练成本高。同时，动作（motion）偏好数据稀缺，限制了奖励/偏好模型训练。

Method: 1）提出EasyTune：不再对整条去噪轨迹端到端微调，而是对扩散过程的每个去噪步单独微调，将递归依赖解耦，从而实现密集、细粒度且显存友好的优化。2）提出自精炼偏好学习（SPL）：在训练过程中动态挖掘与构造偏好成对样本并进行偏好学习，缓解偏好数据不足。

Result: 在动作生成与偏好对齐任务上，EasyTune相较DRaFT-50在MM-Dist对齐指标上提升8.2%，仅需其31.16%的额外显存，并带来7.3倍训练加速。

Conclusion: 逐步解耦微调能有效破解扩散端到端对齐的优化与资源瓶颈；配合SPL可在偏好数据有限的情况下进一步提升对齐性能与训练效率。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [110] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出FSP-Diff：一种融合全谱先验、投影/图像双域信息并在潜空间进行扩散的超低剂量光子计数谱CT重建框架，显著提升成像质量与效率。


<details>
  <summary>Details</summary>
Motivation: 超低剂量谱CT因各能段投影SNR极低而易产生伪影与细节丢失，现有方法难以在抑噪、结构保真与计算开销间兼顾，需要利用跨能段信息与高效生成式先验提升重建质量。

Method: 三大策略：1) 互补特征构建：将直接重建图像（保留纹理）与投影域去噪结果（提供稳健结构）融合；2) 全谱先验：把多能段投影融合为高SNR的全谱参考，用于引导各能段重建；3) 高效潜空间扩散：将多路径特征嵌入紧凑潜空间，在低维流形中进行扩散与交互式特征融合，加速同时保持细节。

Result: 在仿真与真实数据上，FSP-Diff在图像质量与计算效率上均优于最新方法，减少伪影、提升细节与稳定性。

Conclusion: 全谱先验+双域融合+潜空间扩散的协同可在超低剂量谱CT中实现更佳的去噪与结构保真，具备临床可行性潜力。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [111] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 该论文提出CSDN框架，用神经先验与双路径扩散协同（投影域与DR影像域）并通过双投影融合，实现超稀疏角度CBCT在低剂量下抑制伪影、提升纹理与体积一致性，优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: CBCT在临床受限于剂量-质量权衡。超稀疏角度采样可降剂量但带来严重欠采样伪影与层间不一致，现有重建方法难以同时兼顾角度连续性与空间细节保真。

Method: 1) 引入神经先验，学习连续三维衰减场，使得从超稀疏测量合成物理一致的致密投影（作为初始化/结构基座）。2) 在此基础上设计协同扩散：a) Sinogram Refinement Diffusion（Sino-RD）在正弦域恢复角度连续性；b) Digital Radiography Refinement Diffusion（DR-RD）从投影图像角度强化层间一致性。3) 通过Dual-Projection Reconstruction Fusion（DPRF）自适应融合两条扩散路径的输出，生成连贯的体数据。

Result: 在多组超稀疏视角实验中，CSDN显著抑制伪影、恢复精细纹理与体积一致性，定量指标和视觉效果均优于现有SOTA方法。

Conclusion: 将神经先验的连续体建模与双域协同扩散相结合，可在极低视角下实现高质量CBCT重建，有效缓解剂量-质量矛盾，并提供通用的投影/影像协同重建范式。

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [112] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 本文实证评估最先进深伪检测方法，发现其对基于扩散模型与NeRF等新一代高质量深伪的识别显著失效，甚至人类也难以分辨，呼吁加速改进检测技术。


<details>
  <summary>Details</summary>
Motivation: 深伪生成技术（扩散模型、NeRF、改进GAN）快速提升逼真度与易用性，潜在滥用风险攀升；现有检测方法虽在架构与训练策略上有进展，但其对最新生成技术的真实鲁棒性与泛化性仍不明确，需要系统性验证。

Method: 对多种最新深伪检测技术开展全面实证评测，引入Transformer、对比学习等代表性检测器；同时使用前沿生成方法（扩散、NeRF、先进GAN）合成视频建立对抗评测，并包含人类受试者的辨别实验；跨方法、跨数据与跨质量等级进行系统对比。

Result: 多数SOTA检测器在面对由现代生成技术产生的高质量深伪时表现显著下降；在人类实验中，参与者对顶级质量深伪的辨识率也明显偏低；整体体现检测侧相对生成侧的性能落后与弱泛化。

Conclusion: 当前检测方法与新一代深伪生成技术之间存在显著能力鸿沟，亟需在模型鲁棒性、跨分布泛化、与人机协同等方向持续改进与加速研究，以跟上深伪技术演进。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [113] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

TL;DR: 提出MCIE-E1，通过空间感知与背景一致性跨注意力模块，配合复杂指令数据管线与新基准CIE-Bench，显著提升复杂与组合式图像编辑中的指令遵从与背景保真（合规提升23.96%）。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑多停留在简单操作，难以处理真实场景的复杂、组合式编辑；常见问题是指令遵从不足与未编辑区域背景被破坏。

Method: 提出MCIE-E1：1) 空间感知跨注意力（在扩散去噪中将语义指令与空间区域显式对齐，提升定位与遵从）；2) 背景一致性跨注意力（保护未编辑区域特征，保持背景稳定）；3) 构建复杂指令数据管线，先用强MLLM进行细粒度自动筛选，再经人工严格验证；4) 提出CIE-Bench基准，引入两项新评测指标。

Result: 在CIE-Bench上，MCIE-E1在定量与定性上均优于SOTA，指令遵从性提升23.96%，并在背景一致性方面表现更佳。

Conclusion: 通过架构改进与数据/评测闭环，MCIE-E1有效解决复杂指令编辑中的遵从与背景破坏难题，树立更全面的评测标准，推动实际应用落地。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [114] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出ForecastOcc：首个仅依赖摄像头、联合预测未来体素占据与语义的框架，端到端从历史图像输出多时域语义占据；在Occ3D-nuScenes与SemanticKITTI上建立设置/基准并显著超越改造的2D预测基线。


<details>
  <summary>Details</summary>
Motivation: 现有占据预测多聚焦运动/静态对象，缺少细粒度语义；少数语义占据工作依赖独立网络的过去占据结果，导致误差累积，且无法直接从图像端到端学习时空特征。自动驾驶需要兼顾几何与语义的未来推断，推动无地图、纯视觉的语义占据预测。

Method: 提出ForecastOcc：从多/单目历史图像直接预测未来多时域语义占据。核心包括：1) 时间跨注意力（temporal cross-attention）预测模块以建模时序依赖；2) 2D→3D视图变换器将图像特征投影到体素空间；3) 3D编码器进行占据预测；4) 语义占据头在多时间地平线上输出体素级类别。并在框架内改造两种2D预测模块作为基线。

Result: 在Occ3D-nuScenes的多视角和SemanticKITTI的单目设置上进行大量实验，ForecastOcc在多项指标上稳定优于基线，生成兼具动态与语义的未来体素预测。

Conclusion: ForecastOcc实现端到端、纯视觉、多时域的语义占据预测，避免外部地图与先验占据依赖，缓解误差累积，显著提升对场景动态与语义的前瞻理解，利于自动驾驶决策。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [115] [PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping](https://arxiv.org/abs/2602.08020)
*Minghai Chen,Mingyuan Liu,Yuxiang Huan*

Main category: cs.CV

TL;DR: 提出PhysDrape：将物理约束显式求解与神经网络融合的可微混合解算器，实现无穿插、低应变能、实时的服装披挂。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习披挂多用软惩罚处理碰撞，导致在几何可行（无相交）与物理合理（保持形变/能量）间两难：强惩罚致网格形变失真，弱惩罚又出现穿插，且训练难以对齐物理定律。

Method: 1) 物理信息图神经网络：以含材料参数、人体近邻等物理增强图为条件，预测残差位移；2) 可微两阶段显式约束求解：a) 可学习的力求解器，基于StVK弹性模型迭代平衡非平衡力以达准静态；b) 可微投影，对服装-身体表面实施严格几何碰撞约束；3) 端到端训练，将神经预测与物理解算闭环耦合以保持物理一致性。

Result: 在大量实验中，相比SOTA基线，几乎零穿插、显著更低应变能，物理保真度与鲁棒性更佳，并支持实时推理。

Conclusion: 通过将显式力学与可微约束融入学习框架，PhysDrape兼顾几何可行与物理合理，缓解软惩罚固有权衡，达成高保真、稳定、实时的服装披挂。

Abstract: Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.

</details>


### [116] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: FlashVID 是一个无需训练的推理加速框架，通过联合选择与合并视觉token，在仅保留约10%视觉token的情况下，维持接近原模型性能（如 LLaVA-OneVision 保留 99.1% 性能），并在相同算力下把可输入帧数提升至 10 倍（Qwen2.5-VL 上相对提升 8.6%）。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型（VLLMs）处理大量视觉token，计算开销大；多数加速方法将空间与时间冗余分开处理，忽视跨时空关联，导致压缩次优。视频中同一语义内容会随时间在位置、尺度、方向等属性上变化，需要能捕捉并对齐这种时空对应关系的压缩策略。

Method: 提出 FlashVID：1) 注意力与多样性驱动的Token选择（ADTS），在全局注意力与特征多样性约束下挑选最具代表性的基础token；2) 树状时空Token合并（TSTM），以层级树结构在时空维度精细合并高相关token，消除冗余。整个流程训练免疫、可插拔，适配多种VLLM。

Result: 在三个代表性VLLM、五个视频理解基准上验证有效性与泛化性；仅保留10%视觉token即可在 LLaVA-OneVision 上保持 99.1% 性能；在 Qwen2.5-VL 中，作为扩展长视频帧的模块，实现输入帧数×10，在相同计算预算下取得 8.6% 的相对性能提升。

Conclusion: 利用关注度与多样性选择+层级时空合并，可在不训练的前提下显著压缩视觉token且保持性能，通用、即插即用，并有效扩展长视频处理能力。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [117] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 提出MIND基准与MIND-World基线，系统评测世界模型在开放域视频中的记忆一致性与动作控制能力，展示长时记忆与跨动作空间泛化仍是关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型缺乏统一、闭环、开放域的评测来衡量其核心能力，尤其是随时间稳定的记忆一致性与在交互中的动作控制与泛化。

Method: 构建MIND基准：250段1080p@24FPS视频，含第一/第三人称视角与8类场景；设计共享与多样动作空间（移动速度、相机旋转等）；提出高效评估框架衡量记忆一致性（时序稳定、上下文连贯、跨视角一致）与动作控制；给出交互式Video-to-World基线MIND-World用于基准上的闭环测试。

Result: 在MIND上的广泛实验表明该基准完整且具挑战性；现有世界模型在长时记忆一致性和跨动作空间的泛化方面表现薄弱。

Conclusion: MIND为世界模型提供首个开放域闭环回访评测，覆盖记忆与控制两大核心维度，并通过多样动作空间检验泛化；MIND-World作为基线促进后续对比与进展，同时暴露当前模型在长期一致性与跨空间泛化的主要短板。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [118] [Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects](https://arxiv.org/abs/2602.08046)
*Yahia Hamdi,Nicolas Andrialovanirina,Kélig Mahé,Emilie Poisson Caillault*

Main category: cs.CV

TL;DR: 提出将MoE与深度3D卷积GAN结合，以提升3D物体生成与补全质量与效率，并引入无辅助损失的动态容量约束以稳定训练与专家选择，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统GAN在3D对象生成与缺失补全中难以覆盖复杂多样分布，且3D体素处理计算昂贵、结构异质性强，导致现实应用受限。需要一种既能适应多模态结构、又能提高效率与稳定性的框架。

Method: 构建MoE-DCGAN框架：多个专门化生成器作为“专家”捕获数据集中不同模态/结构；通过门控机制动态选择与激活合适的生成器；提出无辅助损失的动态容量约束（DCC）以平衡专家负载、稳定训练并控制计算；用于3D体素数据的生成与缺失区域重建。

Result: 在不同缺失尺度下的3D形状生成与补全任务上进行定量与定性评估；与SOTA方法对比显示本方法在质量与鲁棒性上更优，且在计算效率上具备优势。

Conclusion: MoE与3D CGAN的融合能有效刻画复杂3D数据分布并提升不完整对象的重建效果；DCC机制在不引入额外辅助损失的前提下改善专家选择与训练稳定性，实现性能-效率兼顾。

Abstract: The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>


### [119] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 提出一个将ViT关键模块（补丁嵌入、Self-Attention、位置编码、下/上采样）系统性地改造成群等变的通用框架，保证等变性并提升多任务性能与数据效率，可即插即用并扩展到Swin。


<details>
  <summary>Details</summary>
Motivation: 现有等变ViT难以在“性能-等变性”间取得平衡，症结在于很难对ViT内多样化模块进行整体、协调的等变改造，尤其是让自注意力与补丁嵌入在群作用下保持一致的等变性。

Method: 构建一个统一的等变化设计：对补丁嵌入、自注意力、位置编码、以及下/上采样进行群等变（例如旋转/平移等）改造；提供具有理论等变性保证的模块化替换件，可直接嵌入现有ViT，包括Swin架构。

Result: 在广泛视觉任务上的大量实验显示，该等变ViT在保证等变性的同时，一致性地提高了性能与数据效率，且可扩展到更大/分层Transformer。

Conclusion: 通过对ViT全栈关键组件施加系统的等变约束，可在理论上保证等变性并在实践中带来稳定的性能与样本效率增益，是一种可推广、可扩展的即插即用方案。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [120] [Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks](https://arxiv.org/abs/2602.08057)
*Yufei Wang,Haixu Liu,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 提出一种多模态弱监督框架识别视频中的隐蔽情绪，在iMiGUE网球采访数据集上把准确率从<0.6提升到>0.69，创SOTA。流程：人像检测+视觉特征；大模型(CoT+Reflection)生成伪标签与推理文本；OpenPose关键点+帧间偏移，使用精简MLP骨干；超长序列Transformer编码图像与关键点，再与BERT文本融合；先单模态预训练再多模态微调，并混合伪标注样本。验证MLP化关键点骨干可匹敌/超越GCN。


<details>
  <summary>Details</summary>
Motivation: 隐蔽情绪在现实访谈中短促细微且受遮挡、失衡样本分布严重，现有方法精度<0.6；GCN关键点建模复杂、计算重；缺乏高质量标签。需要一种能利用弱监督、多模态长序列信息、且更高效的关键点建模方法。

Method: - 视觉：YOLO11x逐帧人像裁剪，DINOv2-Base提取视觉特征。
- 伪监督：用Gemini 2.5 Pro结合CoT+Reflection自动生成伪标签与推理文本，作为弱监督信号。
- 姿态：OpenPose提取137维关键点序列，加入帧间偏移特征；将传统GCN骨干简化为MLP以建模三路关键点的时空关系。
- 编码：超长序列Transformer分别编码图像与关键点序列；BERT编码采访文本；多模态表示拼接融合。
- 训练：先各模态独立预训练，再联合微调；将伪标注样本并入训练集提升鲁棒性。

Result: 在iMiGUE网球采访数据集上，尽管类别极度不均衡，方法将准确率从先前<0.6提升到>0.69，刷新公开基准；关键点分支中MLP骨干达到或超过GCN性能。

Conclusion: 多模态弱监督与超长序列编码能有效识别隐蔽情绪；利用大模型生成伪标签与解释文本可缓解标注稀缺；简化的MLP关键点骨干在本任务中是高效且具竞争力的替代方案。

Abstract: To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>


### [121] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: 论文提出Picasso：一个物理约束的多物体场景重建管线，通过同时考虑几何一致性、非穿透和动力学可行性，显著提升接触密集场景中的重建准确性与物理合理性，并在新建数据集与YCB-V上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实场景中存在遮挡与噪声，纯几何拟合常导致物理不合理（如物体互穿、不稳定平衡），削弱数字孪生对动态行为预测与下游控制/规划的价值。因此需要在位姿与形状估计中全局建模物体交互与物理可行性。

Method: 1) 提出Picasso管线：以物理约束（非穿透、稳定性）与几何一致性联合优化多物体重建；利用快速拒绝采样并结合推断的物体接触图来高效探索多物体交互假设空间。2) 提出Picasso数据集：10个接触密集的真实场景，含GT标注；并给出物理可行性评价指标与基准。

Result: 在Picasso数据集与YCB-V上进行广泛实验，Picasso在重建精度与物理可行性上均显著优于现有方法，生成更符合人类直觉的多物体场景解。

Conclusion: 将物理约束与接触关系纳入多物体重建，可显著缓解遮挡与噪声引发的物理错误；Picasso验证了整体化、物理感知的重建策略的有效性，并为后续基于仿真的规划与控制提供更可靠的数字孪生初态。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [122] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: 提出DICE：一个无需再训练、在线消除特定艺术家风格的框架，通过对比子空间分解在潜空间中分离风格与内容，并基于自适应注意力解耦对QKV进行差异化抑制与增强，达到强风格擦除与内容保真之间的更优权衡，仅约3秒额外开销。


<details>
  <summary>Details</summary>
Motivation: 扩散模型使未授权风格拟合变得容易，带来版权/IP风险；现有方法要么需代价高昂的权重编辑、要么依赖显式替换风格，不适合部署端即时安全防护。

Method: 训练免（training-free）的对比子空间分解：构造对比三元组，迫使模型区分风格与非风格特征；将风格与内容解耦形式化为可解的一般化特征值问题，精确估计风格子空间；再以自适应注意力解耦编辑（AAD）：对每个token估计风格浓度，对注意力Q/K/V向量实施差异化风格抑制与内容增强，实现“净化式”而非“替换式”编辑。

Result: 广泛实验显示在风格擦除强度与内容完整性上优于现有方法；可在部署端按需在线运行，额外开销约3秒完成风格子空间解耦。

Conclusion: DICE能在不指定替换风格且无需再训练的条件下，有效、快速地擦除特定艺术家风格并保持内容，适合实际平台部署以抑制风格模仿与降低版权风险。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [123] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: 提出ReRoPE：在不改架构/大规模训练的前提下，将相对相机位姿信息无缝注入预训练视频扩散模型，实现可控、多视角稳定的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有可控相机视频生成多以“相对首帧”的位姿编码为基准，缺乏平移不变性，易累积漂移；而任意视对的相对位姿虽更鲁棒，却难以在不高成本或改模型的情况下融入现有预训练扩散模型。

Method: 观察到预训练模型中的RoPE在低频段存在频谱“冗余/未充分利用”。提出ReRoPE：将相对相机位姿信息编码并注入到这些低频带宽中，作为即插即用模块，无需改网络结构、最小化训练；同时保持原有高频/主体RoPE以维持强生成先验。

Result: 在I2V与V2V任务上，实现更精确的相机控制与较高视觉保真度；减少漂移并优于基线的可控性与稳定性，且训练效率高。

Conclusion: 利用RoPE低频带宽承载相对相机信息，可在不破坏预训练能力的情况下实现精确、稳健的相机可控视频生成；为低成本控制式视频扩散提供有效路径。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [124] [ViT-5: Vision Transformers for The Mid-2020s](https://arxiv.org/abs/2602.08071)
*Feng Wang,Sucheng Ren,Tiezheng Zhang,Predrag Neskovic,Anand Bhattad,Cihang Xie,Alan Yuille*

Main category: cs.CV

TL;DR: ViT-5 在保持标准注意力-FFN 框架下，系统引入近五年架构改进（归一化、激活、位置编码、门控、可学习token），在理解与生成任务上全面优于同级ViT，并作为扩散模型骨干也更强，是面向当代实践的可直接替换型ViT升级版。


<details>
  <summary>Details</summary>
Motivation: 经典ViT虽简单通用，但在规范化、激活、位置表示、信息路由与可学习先验等细节上已落后于近年大模型/视觉骨干演化；需要在不破坏ViT简洁结构的前提下，系统性吸收这些进步以提升性能、可迁移性与生成质量。

Method: 保留Attention+FFN基本单元，对若干组件做逐项升级：改进归一化策略、替换/优化激活函数、采用更优位置编码、引入门控机制、加入可学习token等；以“同等计算量”设定下对比评测，并在分类、扩散生成与分析任务上验证。

Result: 在ImageNet-1k上，ViT-5-Base以相当算力达84.2% top-1，优于DeiT-III-Base的83.8%；作为SiT扩散骨干，FID从2.06降至1.84；表现出更好的表示学习、空间推理与跨任务迁移稳定性。

Conclusion: ViT-5将近年架构实践系统注入到ViT中，作为中期（2020s）视觉骨干的简洁、可直接替换方案，在理解与生成上均带来稳健收益，适合作为基础模型范式的现代化升级。

Abstract: This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>


### [125] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 作者发现多模态大模型（MLLM）中间层已蕴含对视频检索有用的信息；将中间层嵌入与校准的头部结合可零样本取得强性能，并提出仅用文本的轻量对齐策略，将密集视频字幕映射为简短摘要，实现无需视觉监督的跨模态对齐，在多项视频检索基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有把生成式MLLM改造成通用表示器的方法在图像有效，但在视频上落后于专门的VFM。作者想弄清：MLLM在视频文本检索中哪些层最有用？是否可不经耗费昂贵的视觉微调就获得强表示？能否用纯文本信号来学习视频-文本对齐？

Method: 1) 分层分析：评估预训练MLLM各层在视频-文本检索中的信息量；2) 零训练组合：提取并融合中间层嵌入，配合一个经过校准的MLLM头部实现零样本视频检索；3) 文本对齐策略：把密集视频字幕压缩为短摘要，作为监督，进行仅基于文本的嵌入对齐学习，无需任何视觉微调。

Result: 无需任何视觉微调，仅通过中间层嵌入+头部校准即可取得强零样本检索；进一步加入文本对齐后，在常见视频检索基准上超过现有方法，且优势显著。

Conclusion: MLLM的中间层已包含丰富、可利用的跨模态信息；通过简单的中间层融合与文本对齐即可在视频-文本检索上达到甚至超过专门视频模型的性能，减少或消除视觉端微调的需求。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [126] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: MMLSv2 是一个用于火星表面滑坡分割的多模态数据集，含7个通道（RGB、DEM、坡度、热惯量、灰度），共664张图并含地理异域的276张独立测试集。多模型实验显示可稳定训练且表现具竞争力，但在破碎、细长、小尺度滑坡上仍具挑战；在独立测试集上性能显著下降，凸显评估跨区域泛化与鲁棒性的价值。


<details>
  <summary>Details</summary>
Motivation: 现有行星地貌（特别是火星）滑坡分割缺乏高质量、带多模态信息的数据集，限制了模型在复杂地形与跨区域场景中的鲁棒性与泛化评估。作者希望提供标准化基准，推动在异构传感与地理转移条件下的研究。

Method: 构建包含7模态通道（RGB、DEM、坡度、热惯量、灰度）的数据集MMLSv2，提供训练/验证/测试划分（共664张）与来自地理不相交区域的独立测试集（276张）。基于多种分割模型进行基准实验，比较常规测试与独立测试上的表现，分析对不同形态（破碎、细长、小尺度）滑坡目标的难点。

Result: 在主测试集上，多种分割模型均能稳定收敛并取得有竞争力的分割性能；但对破碎、细长与小尺度目标的识别较差。在地理异域的独立测试集上性能显著下滑，表明跨区域泛化更具难度。

Conclusion: MMLSv2为火星滑坡分割提供了多模态、标准化基准，既支持稳定训练又能暴露在复杂目标形态与跨区域泛化上的挑战；独立测试集对于评估模型鲁棒性与分布外泛化具有重要价值。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [127] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 研究评估小型ViT在xBD灾损数据集上的多类别建筑损伤分类，结合补丁化预处理与冻结头微调，在噪声与类不平衡条件下取得与CNN基线相当的宏平均F1。


<details>
  <summary>Details</summary>
Motivation: 灾后快速评估需要可扩展的遥感算法，但卫星数据存在标签噪声与严重类不平衡；xBD提供标准基准，需检验Transformer在此困难设定下的判别能力。

Method: 选用DINOv2-small与DeiT进行多类别损伤分类；提出面向结构特征的目标化patch级预处理以削弱背景干扰；采用冻结head的微调策略以降低计算成本；用准确率、精确率、召回率与宏平均F1评估。

Result: 小型ViT结合所提训练与预处理，在xBD上实现与既有CNN基线相竞争的宏平均F1（具体数值未给出），表明在噪声与不平衡数据下仍具鲁棒性。

Conclusion: 在xBD灾损任务中，小型ViT配合针对性patch预处理与冻结头微调可在资源可控下达到与CNN相当的性能，验证了ViT在噪声、类不平衡遥感场景的可行性。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [128] [MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection](https://arxiv.org/abs/2602.08126)
*Venkatraman Narayanan,Bala Sai,Rahul Ahuja,Pratik Likhar,Varun Ravi Kumar,Senthil Yogamani*

Main category: cs.CV

TL;DR: MambaFusion提出一种将相机与激光雷达融合的BEV 3D检测框架，结合选择性状态空间模型与窗口化Transformer实现线性时间的全局-局部建模，并通过多模态对齐与置信门控进行自适应融合，最终用结构约束的扩散头进行不确定性感知推理，在nuScenes上达SOTA与更稳健、可解释的感知。


<details>
  <summary>Details</summary>
Motivation: 现有BEV多模态融合在三方面受限：全局上下文建模低效、融合对空间与置信不敏感、难以在不确定性下进行物理一致的推理。需要一种既高效又能可靠处理多源不确定性的统一框架。

Method: 1) 用选择性状态空间模型(SSM)与窗口化Transformer交错：SSM线性时间传播全局上下文，窗口Transformer保留局部几何；2) 多模态Token对齐(MTA)与可靠性感知门控：依据空间置信与标定一致性自适应重权相机/雷达特征；3) 结构条件扩散头：融合图结构先验与不确定性感知去噪，约束物理可行并输出校准置信。

Result: 在nuScenes基准上达到了新的SOTA，同时保持线性时间复杂度，并在鲁棒性、时序稳定性与可解释性方面表现优异。

Conclusion: 将SSM的效率与以可靠性为核心的融合策略相结合，可实现高效、稳健且物理一致的3D多模态感知，适用于真实自动驾驶系统。

Abstract: Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>


### [129] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: 论文介绍了“Fields of The World (FTW)”生态系统：包含160万个覆盖24个国家的地块多边形基准数据、预训练分割模型与命令行推理工具，并提供两个示例笔记本支持本地与国家尺度的地块边界提取、作物分类与森林损失归因；同时利用MOSAIKS随机卷积特征与FTW边界进行小样本的地块级作物分类（宏平均F1=0.65–0.75），并展示了5个国家范围内的预计算预测与地块面积分布（中位数0.06–0.28公顷）。


<details>
  <summary>Details</summary>
Motivation: 农业数据产品需要高质量地块边界，用于作物监测、产量与病害估计，但缺乏大规模、跨国的标准化基准与易用工具链，且在标签有限的情况下实现可扩展的作物类型制图仍具挑战。

Method: 构建FTW生态系统：1) 1.6M跨24国的地块多边形基准；2) 预训练地块边界分割模型与命令行推理工具；3) 两个教学笔记本：本地尺度（地块边界提取+作物分类+森林损失归因）与国家尺度（基于云优化数据的推理）。在作物分类中，使用MOSAIKS随机卷积特征与FTW派生的地块边界，在有限标签条件下进行地块级分类评估。

Result: 地块级作物类型分类在有限标签下达到了宏平均F1 0.65–0.75；提供了五个国家（覆盖约476万平方公里）的预计算预测结果，并报告各国中位数地块面积从0.06公顷（卢旺达）到0.28公顷（瑞士）。

Conclusion: FTW提供了从数据基准、模型到工具的端到端生态，显著降低了跨尺度地块边界提取与作物分类的门槛，并在有限标注条件下取得可靠性能；其可扩展的预测与公开探索界面为大范围农业监测和下游应用奠定了基础。

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [130] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 论文揭示了当图像被拆分为多片并联合表达有害语义时，主流视觉-语言模型（VLM）在安全对齐上存在盲区，提出一套分割图像越狱攻击（SIVA）与基于对抗蒸馏的黑箱可迁移方案，显著提升跨模型攻击成功率，并给出缓解对策。


<details>
  <summary>Details</summary>
Motivation: 现有视觉越狱多基于单幅整体图像，但现代VLM因RLHF等偏好对齐已对这类攻击更稳健。作者发现训练与指令微调对“拼接/分割输入”泛化良好，而安全对齐通常仅在整体图像上进行，忽视了有害语义可分布于多个图像碎片、只有组合后才显性，从而造成新的安全缺口。

Method: 提出SIVA框架：从朴素分割逐步演化到自适应白盒攻击，再到黑箱迁移攻击。核心技术包括一种新颖的对抗知识蒸馏（Adv-KD），从强攻击模型向目标黑箱模型迁移有害表征，提升无梯度场景的可迁移性；并在多阶段设计中按模型响应迭代调优分割与组合策略。

Result: 在三款最先进VLM与三套越狱数据集上评测，最强策略相较现有基线在黑箱迁移成功率上最高提升约60%，整体证明SIVA对当前VLM形成实质威胁且具良好跨模型泛化。

Conclusion: VLM安全对齐对“分割图像-组合语义”的攻击面防护不足。SIVA证明该缺口可被系统性利用。作者建议在安全对齐中纳入多片段/组合语义分布场景，并提出高效缓解方案，以弥补现有对齐流程的结构性盲点。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [131] [DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation](https://arxiv.org/abs/2602.08168)
*Mei Ling Chee,Thangarajah Akilan,Aparna Ravindra Phalke,Kanchan Keisham*

Main category: cs.CV

TL;DR: 提出DAS-SK：在轻量化DeepLabV3框架上，将选择性卷积(SK-Conv)融入双空洞可分离卷积(DAS-Conv)，并强化ASPP，实现高分辨率农业影像的高效多尺度分割；在LandCover.ai、VDD、PhenoBench上达SOTA，参数与GFLOPs显著低于主流Transformer。


<details>
  <summary>Details</summary>
Motivation: 农业高分辨率遥感/机器人场景要求在边缘设备(UAV等)上实时分割，现有强模型常受限于参数/算力开销大、需大数据训练、光谱泛化差，难以落地。需要一种在准确率与效率间取得更佳权衡、可扩展且对小样本/域移更稳健的结构。

Method: 基于改进的DeepLabV3，采用双主干(MobileNetV3-Large与EfficientNet-B3)以互补表征；在DAS-Conv中引入SK-Conv以自适应选择不同感受野通道，强化多尺度特征融合；增强ASPP以同时捕获细粒度局部结构与全局上下文；整体轻量化设计，适配边缘部署。

Result: 在LandCover.ai、VDD、PhenoBench三基准上稳定取得SOTA；相较顶尖Transformer模型，参数量最多减少至1/21，计算量(GFLOPs)降至1/19，同时优于CNN、Transformer与混合模型对手的精度-效率权衡。

Conclusion: DAS-SK在保证精度的同时显著降低参数与算力，适合UAV与其他边缘设备的实时高分辨率农业分割，具备良好鲁棒性、效率与可扩展性，并有潜力推广到更广泛视觉任务。

Abstract: Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>


### [132] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus提出一种可个性化生成3D形状的框架，通过从参考形状中学习可复用的几何与外观概念，并与文本组合来合成新形状；采用逐步优化与区域级概念学习，实现细粒度、跨类别且可控的生成，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法在“个性化”与“可控性”上受限：难以从少量参考中提炼可迁移的几何/外观概念，跨类别复用与局部可控编辑更难。作者希望构建一种能从参考中抽取类别无关、可重组的形状概念，并与文本条件灵活组合，提升细粒度控制与个性化能力。

Method: 1) 任务定义：将3D个性化表述为从参考形状中提炼类别无关的几何与外观属性，并与文本提示进行组合生成。2) 概念学习：提出逐步（progressive）优化，解耦几何与外观的概念学习，分别对形状结构与材质/纹理表征进行训练。3) 区域级扩展：进行region-wise概念学习，结合上下文感知与上下文无关损失，支持在局部区域抽取与应用概念，从而提升可控性与复用性。

Result: 在广泛参考形状集上验证，PEGAsus能有效抽取并重组概念，与文本组合生成多样、个性化且细粒度可控的新3D形状，包括困难的跨类别场景。定量与定性评估均优于SOTA。

Conclusion: 通过逐步解耦的几何/外观概念学习与区域级概念抽取，PEGAsus实现了灵活、细粒度、跨类别的3D个性化生成，显著优于现有方法，为可控三维内容创作提供了通用框架。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [133] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 提出一种面向回归问题的条件扩散模型（MCSDR），从超声心动图视频和人口学先验中生成LVEF的后验分布，优于MSE回归并在多数据集上SOTA，同时生成轨迹提供可解释性。


<details>
  <summary>Details</summary>
Motivation: LVEF从超声视频估计是病态逆问题：噪声、伪影、视角限制导致同一视频对应多种合理生理值。传统以MSE为目标的点估计回归学习条件期望，在多峰或重尾后验下会给出误导性预测，尤其在病理场景中。需要能表达不确定性与多模态性的概率方法。

Method: 将回归转为生成式回归：提出多模态条件得分扩散模型MCSDR。以超声视频与患者人口学属性为条件，学习LVEF连续后验分布的得分场；通过扩散/反演过程采样得到预测分布与轨迹，可输出均值、分位数等；相较传统回归，能够显式建模多模态与不确定性。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric、CAMUS三数据集上取得SOTA。定量指标优于现有深度回归方法；定性上，在高噪声或生理变异大样本中，生成轨迹呈现差异化收敛行为，反映不确定性结构。

Conclusion: 面向LVEF估计的MCSDR有效捕获条件后验的多模态与重尾特性，较传统MSE回归更稳健且可解释；其采样轨迹为AI辅助诊断提供新的解释维度。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [134] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 提出GR-CoT，通过链式地理空间推理增强MLLM，引导开放词汇遥感语义分割，实现更精准地物映射；在LoveDA、GID5上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇分割多是“基于外观”的视觉-文本对齐，缺乏地理空间上下文推理，遇到光谱相似但语义不同的地物（如道路vs屋顶、裸地vs沙地）易混淆与误判。需要融入地理常识与场景语境的推理机制，缓解语义歧义并提升像素级对齐的正确性。

Method: 提出Geospatial Reasoning Chain-of-Thought (GR-CoT) 框架，面向MLLM：1）离线知识蒸馏流，构建细粒度类别解释标准，消解相似类别间的语义冲突；2）在线实例推理流，按顺序执行宏场景锚定、视觉特征解耦、知识驱动决策综合，生成与图像自适应的词汇列表，引导下游开放词汇分割模型实现正确地理语义对齐。

Result: 在LoveDA与GID5基准上做了大量实验，方法整体优于现有SOTA（摘要未给出具体数值，但宣称显著提升）。

Conclusion: 将地理空间链式推理注入开放词汇遥感分割，可有效缓解外观相似类别的歧义，提升像素级语义对齐与映射精度；GR-CoT为MLLM指导下的开放词汇遥感解译提供了有效范式。

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [135] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 提出“Chain-of-Caption”无训练框架，通过为多模态大模型（MLLM）注入额外文本与视觉上下文（含工具使用）提升指代表达理解（REC）中的目标定位准确率，在RefCOCO/RefCOCOg/RefCOCO+与Ref-L4上于多IoU阈值下较基线提高5%–30%。


<details>
  <summary>Details</summary>
Motivation: MLLM在REC上已因模型与数据规模、CoT与工具使用而进步，但尚缺对“如何以额外文本/视觉上下文（尤其工具产生的上下文）系统性增强REC”的分析与无需微调即可普适增益的方法。

Method: 系统分析多种工具产生的上下文（文本描述、辅助视觉线索等）对REC的影响；提出训练免调的Chain-of-Caption：先利用工具生成多阶段/多粒度的图像字幕或区域描述，作为额外上下文喂给MLLM，再让MLLM执行定位，亦可融合多种上下文以互补。

Result: 在RefCOCO/RefCOCOg/RefCOCO+与Ref-L4数据集上，无需微调即可通过单一文本或视觉上下文带来性能提升；当组合多种上下文时，相较基线模型在不同IoU阈值的准确率上获得约5%–30%的增益。

Conclusion: 为REC提供外部文本与视觉上下文（含工具生成）能显著强化MLLM定位能力；所提Chain-of-Caption在无需训练的前提下稳定提升多基准表现，显示上下文组合与分阶段描述是增强REC的有效路径。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [136] [Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval](https://arxiv.org/abs/2602.08224)
*Jing Zhang,Zhikai Li,Xuewen Liu,Qingyi Gu*

Main category: cs.CV

TL;DR: 提出Efficient-SAM2，通过稀疏感知加速SAM2视频分割：用SWR将背景窗口走轻支路、用SMR只取显著记忆token，几乎零额外参数，SAM2.1-L推理提速1.68倍，SA-V上精度仅降1.0%。


<details>
  <summary>Details</summary>
Motivation: SAM2视频分割性能强但计算量大、难以实时。既有加速多依赖重新训练轻量主干，后训练阶段的结构化加速探索不足。作者观察到SAM2存在类似生物视觉的稀疏感知：解码器注意力集中前景、早期编码器关注范围过广；记忆库中只有少量token对注意力显著且显著区域时序一致。这为去除冗余计算提供契机。

Method: 基于两大稀疏性提出Efficient-SAM2：1) 对图像编码器提出对象感知的稀疏窗口路由SWR：利用前一帧解码器输出的显著性与一致性线索，按窗口级分配计算，将背景窗口路由到轻量捷径分支，仅对前景/显著窗口执行重计算。2) 对记忆注意提出对象感知的稀疏记忆检索SMR：仅让每帧的显著记忆token参与注意力计算，并复用其首次被召回时的显著性模式，避免对全token的重复计算。整体仅引入可忽略的额外参数与极少训练开销。

Result: 在SAM2.1-L上实现1.68倍推理加速；在SA-V测试集上精度仅下降约1.0%。

Conclusion: 利用模型内在稀疏感知对编码与记忆模块做对象感知的稀疏化与路由，可在几乎不牺牲精度的前提下显著加速SAM2推理，显示后训练阶段结构化稀疏加速的有效性与可推广性。

Abstract: Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>


### [137] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

TL;DR: 提出MA-ADV：首个基于点云表示的事件相机对抗样本生成框架，利用运动感知与扩散平滑实现高效、低代价、鲁棒的攻击，实验达成100%攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 事件相机在自动驾驶等安全关键场景应用广泛，但深度模型易受对抗攻击。主流事件表示不可微，使梯度攻击难以直接应用，导致针对事件数据的对抗研究稀缺，安全风险评估不足。

Method: 1) 将事件数据表示为时空点云以获得可微表示并显式建模时空关系；2) 引入运动感知机制，考虑事件的高频噪声特性；3) 采用扩散式平滑以抑制扰动噪声并保持结构一致性；4) 通过样本级Adam优化、迭代细化与二分搜索联合，寻找最小代价扰动。

Result: 在多项实验中，MA-ADV以最小扰动代价实现100%攻击成功率，并在多种防御下表现出更强鲁棒性。

Conclusion: 面向事件相机的MA-ADV证明了事件感知系统在对抗攻击下存在严重安全隐患；点云表示与运动感知+扩散平滑+高效搜索的组合可高效生成低可见度且鲁棒的对抗事件。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [138] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 论文分析了在空间推理任务中，测试时“视觉想象（基于世界模型的多视角生成/仿真）”应何时启用与如何控制，以避免不必要的计算与误导性证据；提出AVIC自适应框架，在调用想象前先判断现有证据是否充足，并按需选择与缩放想象量，在多基准上以更少模型调用与更少token达到匹配或更优性能。


<details>
  <summary>Details</summary>
Motivation: MLLM在需要跨视角/未见视角的空间推理上仍不稳定。现有方法常无差别地使用世界模型进行想象，带来高计算成本，且可能因虚假或噪声想象而降低准确率。缺乏系统理解：何时需要想象、需要多少、何时有害。

Method: 提出AVIC（Adaptive Visual Imagination Control）测试时框架：1）先评估当前静态视觉证据是否“充分”；2）若不足，再“选择性地”调用世界模型进行视觉想象，并“可伸缩地”控制想象强度/次数；3）在推理过程中持续权衡证据充足性与想象收益与成本。并在空间推理基准（SAT、MMSI）与具身导航（R2R）上系统分析不同想象策略（必要、边际、有害）的情景。

Result: 在SAT、MMSI与R2R上，AVIC通过有选择的想象控制，较固定/一刀切的想象策略，以显著更少的世界模型调用与语言token，达到相当或更优的准确率/成功率。实验揭示想象在不同任务情景下分别呈现关键、边际或负面作用的清晰边界条件。

Conclusion: 测试时视觉想象应被视为可控资源：先判断证据充足性，再按需选择与调节想象量，可提升效率与可靠性。对MLLM的空间推理而言，分析与控制想象比盲目增加想象更重要。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [139] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: 论文针对fMRI分类常用的静态FC方法忽略时间动态与非线性关系的问题，基准测试多种时间序列模型在原始BOLD信号上，表现优于FC；据此提出DeCI，通过“周期-漂移分解”和“通道独立”两原则，显著提升跨数据集泛化与准确率，倡导端到端时序建模。


<details>
  <summary>Details</summary>
Motivation: 现有以Pearson相关为核心的FC将4D BOLD压缩为静态2D矩阵，丢失时间动态信息且只捕捉线性相关，限制了对复杂脑动态与疾病表型的识别能力。需要能直接建模BOLD时序、刻画周期性振荡与缓慢基线漂移的方案，并验证其对分类与泛化的收益。

Method: 1) 系统基准：在五个公开数据集上，用多种SOTA时序模型（PatchTST、TimesNet、TimeMixer等）直接处理原始BOLD序列；2) 提出DeCI框架：- Cycle & Drift Decomposition：在每个ROI内将BOLD分解为周期成分与漂移成分；- Channel-Independence：对各ROI独立建模，减少通道间过拟合与噪声耦合；3) 与传统FC方法及时序基线对比评测。

Result: 时序模型在所有数据集上稳定优于传统FC基线；DeCI在准确率与泛化上进一步超过两类基线（FC与通用时序模型），对周期振荡与慢速漂移的显式解耦与ROI独立建模带来稳健提升。

Conclusion: 直接对BOLD进行端到端时间建模能更好捕捉复杂脑动态；DeCI以周期-漂移分解与通道独立为核心，达到更高准确率和更强泛化。应从静态FC范式转向时序建模；代码已开源以促进复现与扩展。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [140] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: PISCO 提出一种可在现有视频中精确插入指定实例的扩散式生成方法，支持单帧/首末帧/稀疏多关键帧控制，并在保证时空一致与物理交互的前提下，自动传播外观与运动；通过可变信息引导、分布保持的时间掩膜与几何感知条件，缓解稀疏条件导致的分布偏移与时间不稳；并构建带成对干净背景与标注的评测集 PISCO-Bench，实验显示在稀疏控制下显著优于强基线且随控制增多单调提升。


<details>
  <summary>Details</summary>
Motivation: 专业级 AI 辅助影视制作需要对现有镜头进行细粒度、可控且物理一致的定向修改，传统“拼提示+挑结果”范式无法满足；“视频实例插入”是关键能力，但需在最少人工操作下实现精确时空定位、与场景物理一致交互，并保持原有动态与外观。

Method: 提出 PISCO：基于视频扩散模型，支持任意时间戳的稀疏关键帧条件（单帧、起止帧或多帧），自动传播对象外观、运动与交互；为应对预训练模型在稀疏条件下的分布偏移，引入（1）Variable-Information Guidance（可变信息引导）以稳健利用条件信息，（2）Distribution-Preserving Temporal Masking（分布保持的时间掩膜）以稳定时序生成，（3）几何感知条件以实现与场景的真实适配。另构建含实例标注与成对干净背景的视频基准 PISCO-Bench，并采用参考/非参考感知指标评测。

Result: 在稀疏控制设定下，PISCO 在感知质量与一致性上持续超过强视频修复与编辑基线；随着增加更多关键帧控制，性能呈现清晰、单调提升趋势。

Conclusion: PISCO 证明了在最小用户成本下实现精确的视频实例插入的可行性，通过稳健的条件引导与时间稳定策略有效应对稀疏条件挑战，并借助新基准全面评测，显示出相较现有方法的稳定优势与可扩展的控制收益。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [141] [Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning](https://arxiv.org/abs/2602.08282)
*Haixu Liu,Yufei Wang,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 提出融合PA与PO数据的多模态框架，利用卫星影像覆盖度进行PO伪标签聚合，与Swin/TabM/Temporal Swin与三模态串行交叉注意力融合；并用基于空间邻近的专家混合式推理以缓解分布移位与PO噪声，在GeoLifeCLEF 2025上取得更优表现。


<details>
  <summary>Details</summary>
Motivation: 大规模跨物种植物分布预测受观测稀疏与偏倚制约：PA数据干净却稀缺昂贵，PO数据覆盖广但负样本噪声重，且训练-测试间存在显著地理分布漂移。需要一种方法同时利用两类数据优势并应对分布移位与噪声。

Method: 1) 为PO数据提出基于卫星影像地理覆盖度的伪标签聚合，使标签空间与遥感特征空间地理对齐。2) 模型采用多模态骨干：Swin Transformer Base处理影像，TabM处理表格特征，Temporal Swin处理时序；通过可堆叠串行三模态交叉注意力实现异构模态融合。3) 经验分析发现直接混合PO/PA训练会因PO噪声致退化，且PA训练与测试地理分布偏移明显；据此设计专家混合范式：按测试样本与PA样本的空间邻近进行分区，为各分区调用在不同数据集上训练的模型并做后处理。

Result: 在GeoLifeCLEF 2025数据集上，所提方法在PA覆盖受限且分布移位显著的场景下取得优于基线的预测性能。

Conclusion: 融合PA与PO的多模态框架配合地理对齐伪标签与空间分区的Mixture-of-Experts推理，可有效缓解PO噪声与分布移位，提升大规模跨物种植物分布预测表现。

Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>


### [142] [CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment](https://arxiv.org/abs/2602.08309)
*Yunzuo Hu,Wen Li,Jing Zhang*

Main category: cs.CV

TL;DR: 提出CAE-AV框架，通过CASTE与CASE两模块缓解音视频失配，并配合轻量目标函数，在冻结骨干下于多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有音视频学习常受离屏声源与背景杂讯造成的模态失配影响，易放大无关区域/时刻，导致训练不稳定与表示退化。需要一种能在空间与时间上更精准对齐并利用高层语义指导的机制。

Method: 提出CAE-AV：1) CASTE（跨模态一致性引导的时空强化）按帧评估音视频一致性，动态平衡空间与时间关系，从前后帧提取关键信息以对抗失配；2) CASE（字幕对齐的显著性引导强化）在选定的时空位置注入跨模态语义线索（由字幕/文本指导），进一步缓解失配。另设计三类轻量训练目标：caption-to-modality InfoNCE、视觉-音频一致性约束、熵正则，用于令牌选择与跨模态语义对齐。

Result: 在冻结视觉与音频骨干的设定下，于AVE、AVVP、AVS、AVQA基准取得SOTA；定性可视化显示对失配更鲁棒，能聚焦关键区域/时刻。

Conclusion: 通过一致性评估与字幕语义双重引导，CAE-AV在不微调骨干的前提下有效缓解音视频失配，提升多任务性能与鲁棒性。

Abstract: Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>


### [143] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 提出语言引导的运动离散化方法LG-Tok，用语言在分词阶段对动作进行对齐，得到更紧凑且具语义的离散表示；基于Transformer的分词器与language-drop训练策略，使生成更易学、可带或不带语言引导；在HumanML3D与Motion-X上取得优于SOTA的Top-1与FID，并有半token版本保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有动作离散分词通常通过增加token数提升重建质量，但会加大学习难度与生成复杂度；卷积式分词器难以进行全局语言对齐，限制了语义层次的紧凑表示与生成效率。

Method: 提出Language-Guided Tokenization (LG-Tok)：在分词阶段引入文本-动作对齐，产生高层语义、紧凑的离散token；采用Transformer分词器以注意力实现全局对齐；设计language-drop策略，训练时随机移除语言条件，使解码器在有/无语言条件下都稳健；并给出LG-Tok-mini以更少token维持性能。

Result: 在HumanML3D与Motion-X生成基准上，Top-1分别达0.542/0.582（优于MARDM的0.500/0.528），FID分别为0.057/0.088（优于0.114/0.147）；LG-Tok-mini使用一半token，仍达Top-1 0.521/0.588，FID 0.085/0.071。

Conclusion: 语言引导的分词在保持或提升重建与生成质量的同时，显著降低生成学习难度；Transformer分词器与language-drop带来更强的语义对齐与无语言鲁棒性；语义紧凑token化提高效率且可在更少token下维持SOTA级表现。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [144] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: 提出UGData数据集、UGE两阶段训练策略和UGBench基准，显式把街景图像与城市空间图结构对齐，通过指令引导对比学习与图空间编码学习可迁移多模态空间嵌入，在多种VLM骨干上用LoRA训练，显著提升图像检索与地理定位等空间密集型任务表现，含跨城市泛化。


<details>
  <summary>Details</summary>
Motivation: 现有多模态城市理解需要空间感知，但公开数据和评测缺少街景图像与城市空间结构的显式对齐，无法捕捉距离、方向、连通性与邻里上下文等空间关系，限制了嵌入在地理定位、检索与空间指向任务的能力与可迁移性。

Method: 1) 构建UGData：将街景图锚定到结构化空间图，提供空间推理路径与上下文描述作为图对齐监督；2) 提出UGE：两阶段训练——阶段一用指令引导的对比学习对齐图像-文本-空间结构；阶段二引入基于图的空间编码（包含距离、方向、连通性与邻域上下文）以稳定、逐步对齐；3) 推出UGBench评测多任务（地理定位排序、图像检索、城市感知、空间定位）；4) 在多种VLM骨干（Qwen2/2.5-VL、Phi-3-Vision、LLaVA1.6-Mistral）上以LoRA微调学习定长空间嵌入。

Result: 基于Qwen2.5-VL-7B的UGE在训练城市上图像检索提升至多44%、地理定位排序提升30%；在留出城市上分别提升30%+与22%+，显示显式空间对齐带来显著收益与跨城泛化。

Conclusion: 显式将街景图像与城市空间图结构对齐，并用两阶段指令对比+图空间编码训练，可稳定学习具空间感知与可迁移性的多模态嵌入，显著提升多种城市理解任务并具有跨城市泛化潜力。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [145] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 论文提出首个面向“思维-图像（thinking with images）”范式下的PRM评测基准，定义7类细粒度错误，构建1206条人工标注推理轨迹，并实证表明现有LVLM作为PRM效果不佳。


<details>
  <summary>Details</summary>
Motivation: “思维-图像”范式允许模型在多步推理中动态编辑与重编码图像信息，但引入了多样化过程性错误。现有PRM基准多为文本中心，无法全面评估视觉推理过程，因此需要一个专门面向该范式的PRM评测基准。

Method: 1) 基于对推理轨迹与PRM引导搜索的广泛分析，归纳7种细粒度错误类型；2) 构建涵盖4大类、16个子类、共1206条人工标注的“思维-图像”推理轨迹数据集；3) 用该基准系统评测现有LVLM充当PRM的能力，分析不同错误类型、正负样本、以及推理步位置对评估的影响。

Result: 实验显示：现有LVLM作为PRM的表现有限；对不同错误类型的识别能力差异显著；存在对正向步骤的偏置；对推理步骤位置敏感。上述结果验证了所建基准能够区分模型能力并揭示不足。

Conclusion: 该基准为评估与改进“思维-图像”范式下的PRM提供了系统工具与数据依据，强调需要专门训练与设计的PRM来提升视觉推理过程评估质量。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [146] [E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs](https://arxiv.org/abs/2602.08355)
*Xianjie Liu,Yiman Hu,Liang Wu,Ping Hu,Yixiong Zou,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 提出电商短视频理解基准与强化学习推理模型，验证该领域多模态信息密度高，并显著提升商业意图推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准偏通用场景，忽视电商短视频中以转化为目标的商业意图推理；该领域多模态信号密集、任务更难，导致现有模型表现不佳。

Method: 1) 构建多模态信息密度评估框架，量化视觉/音频/文本复杂度；2) 发布E-VAds数据集：从淘宝筛选3,961条高质短视频，利用多智能体生成19,785条开放式问答，按“感知”“认知与推理”两大维度、五类任务组织；3) 提出E-VAds-R1：基于强化学习的推理模型，设计多粒度奖励MG-GRPO，既平滑引导早期探索，又用非线性激励促成专家级精度。

Result: 评估显示电商短视频在多模态维度的信息密度显著高于主流数据集；E-VAds-R1在少量训练样本条件下，商业意图推理性能提升109.2%。

Conclusion: 电商短视频理解需要面向高信息密度与商业意图的专门基准与方法；E-VAds与E-VAds-R1有效弥补这一空白，并以MG-GRPO显著提升推理效果，证明针对性强化学习与奖励设计的价值。

Abstract: E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>


### [147] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出GeoEdit框架，利用扩散Transformer的情境生成与几何变换融合，实现物体平移/旋转/缩放的精确编辑；并引入对光效敏感的注意力机制以提升光照与阴影真实感；构建12万+配对数据集RS-Objects。实验显示在视觉质量、几何精度与真实感上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有扩散编辑方法在复杂场景中难以准确执行几何变换（平移、旋转、缩放），且对复杂光照与阴影建模不足，导致不真实的编辑结果。

Method: 1) GeoEdit：在扩散Transformer中融入几何变换算子，通过in-context generation对目标对象进行精确几何编辑；2) Effects-Sensitive Attention：在注意力中显式建模与光照/阴影相关的细节与依赖；3) RS-Objects数据集：构建12万+高质量成对图像，监督学习几何精度同时提升光照与阴影的逼真度。

Result: 在公共基准上进行广泛实验，GeoEdit在视觉质量、几何准确性和真实感方面持续超越当前SOTA方法。

Conclusion: 通过结合几何变换感知的扩散Transformer与对光效敏感的注意力，并配合大规模几何编辑数据集训练，可显著提升复杂场景下的精确几何编辑与光照/阴影逼真度，取得全面领先表现。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [148] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 提出D^2-VR：基于单帧扩散先验、低步采样的视频复原框架，通过稳健光流对齐+对抗蒸馏+协同优化，实现高感知质量与时间一致性，并将采样加速约12倍、达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有将扩散先验与时间对齐结合的方法在真实复杂退化下推理延迟高、时间不稳定，难以落地。需要在保持感知质量的同时显著降低推理步数，并获得鲁棒的时间引导。

Method: 1) 退化鲁棒流对齐（DRFA）：利用置信感知注意力，过滤不可靠运动线索，获得更精确的时间对齐；2) 对抗蒸馏：把扩散采样轨迹压缩到少步推理，实现快速生成；3) 协同优化：共同优化感知质量与时间一致性损失，权衡视觉细节与时序稳定。

Result: 在多项实验中达成SOTA，同时将扩散采样过程加速约12×，在复杂退化场景下仍保持较强的时间一致性与高感知质量。

Conclusion: D^2-VR通过DRFA、少步对抗蒸馏与协同优化，实现低时延且时序稳定的视频复原，在真实复杂退化中兼顾速度与质量，具备实际部署潜力。

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [149] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: 提出RealSynCol：高度逼真的结肠镜合成数据集（28,130帧，含深度、光流、3D网格、相机轨迹），显著提升在临床图像上的深度与位姿泛化表现。


<details>
  <summary>Details</summary>
Motivation: 临床结肠镜3D重建可改进病灶可视化与未探查区域识别，但缺乏大规模带真实标注的数据限制了鲁棒深度学习方法的发展。

Method: 从10例CT提取结肠几何体，导入高仿真的虚拟内镜环境，加入逼真的血管纹理渲染；生成包含28,130帧的视频数据，并配套真值深度、光流、3D网格、相机位姿；并对现有合成结肠数据集进行基准评测（深度与位姿估计任务）。

Result: RealSynCol在逼真度与多样性上优于现有合成数据集；使用其训练的模型在临床图像上的深度与位姿估计泛化性能显著提升。

Conclusion: 高逼真、可标注的RealSynCol是推进结肠镜3D感知/重建与导航算法的重要数据资源，可有效弥补真实标注数据稀缺，促进临床应用泛化。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [150] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 论文重新审视基于注意力的稀疏图像匹配训练，发现一个关键但被忽视的设计选择显著影响LightGlue表现；实证表明性能差异多由检测器而非描述子引起；提出用多种检测器关键点微调得到与检测器无关的通用匹配器，在零样本场景对新检测器也能达到或超越专门训练模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer式稀疏图像匹配（如LightGlue）在不同局部特征设置下性能不稳定，原因与训练设计和特征组件（检测器vs描述子）作用尚不清晰；实际部署需要一个对不同检测器稳健、可泛化的匹配器。

Method: 1）系统分析并调整LightGlue训练中的一个被忽略的关键设计选择；2）分离评估检测器与描述子在Transformer匹配框架内的贡献，定位性能瓶颈主要在检测器；3）提出使用来自多种检测器的关键点对现有匹配器进行微调，学习到对检测器分布不敏感的匹配策略，从而得到通用、检测器不可知的模型；4）在零样本设置上测试对未见过的检测器。

Result: 微调后的通用模型在面对新检测器时，无需再训练即可达到或超过针对该检测器专门训练的模型精度；同时验证了关键训练设计对LightGlue性能的显著影响与检测器主导的性能差异来源。

Conclusion: 在Transformer式稀疏匹配中，检测器往往是性能差异主因；通过以多检测器关键点进行微调，可获得检测器无关、具备强零样本泛化能力的匹配器。这为匹配器部署与未来局部特征/训练策略设计提供了实用指引。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [151] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出“演示驱动的视频类比学习（Demo-driven Video In-Context Learning）”任务与评测基准Demo-ICL-Bench，并给出两阶段训练的MLLM模型Demo-ICL，在该基准上显著提升少样本、上下文演示条件下的视频问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准多评估模型基于静态内部知识的理解，而非其从少量新颖、动态上下文中快速学习与迁移的能力；因此需要一种评测与方法，检验模型是否能通过上下文演示即时习得任务与知识。

Method: 1) 任务与基准：构建Demo-ICL-Bench，源自1200个教学类YouTube视频及配套问题；提供两类“演示”：a) 文本演示：基于字幕的摘要；b) 视频演示：相关教学视频片段。2) 模型：提出Demo-ICL，采用两阶段训练：i) 视频监督微调（video-supervised fine-tuning）；ii) 信息辅助的DPO（information-assisted direct preference optimization），以增强模型从演示中抽取与迁移的能力。

Result: 在Demo-ICL-Bench上，现有SOTA MLLMs整体表现较差，显示该基准具有挑战性；Demo-ICL在多项指标上优于对比模型，验证两阶段训练与演示利用策略的有效性。

Conclusion: Demo-ICL-Bench填补了“演示驱动视频类比学习”的评测空白；Demo-ICL证明在此设定下可显著提升性能。工作揭示了未来方向：更高效的视频-文本演示对齐、跨视频迁移与更强的上下文适应能力。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [152] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: 提出Vista框架，通过“场景感知的分割-压缩-召回”三步，实现流式视频QA中的高效、可扩展长期记忆与检索，兼顾时效、内存与准确度，并在StreamingBench上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 流式视频QA中，视频帧连续到达且提问时刻任意，固定大小记忆或简单压缩容易造成上下文丢失或内存溢出，难以在长时、实时场景下稳定表现。需要一种既能保持关键信息完整性、又具可扩展与低延迟的记忆与检索机制。

Method: Vista框架包含三大核心：1) 场景感知分割：在线将输入帧按时间与视觉相似性动态聚类为场景单元；2) 场景感知压缩：将每个场景编码为紧凑token并存入GPU以便索引检索，同时把全分辨率帧下放到CPU内存；3) 场景感知召回：收到问题时基于相关性选择性召回场景，将其紧凑表示与必要的高分辨率帧重组入模型输入，实现效率与信息完备性的平衡。框架与多种视觉-语言骨干解耦，可无缝集成。

Result: 在StreamingBench上取得SOTA，展现出在长上下文推理、低延迟与内存效率上的综合优势，建立强有力基线。

Conclusion: 通过场景级的在线分割、紧凑表示与按需召回，Vista实现了对连续视频流的高效与可扩展推理，缓解了传统固定内存或粗糙压缩带来的上下文丢失与溢出问题，并具备良好的模型无关性与实用性。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [153] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: 提出TriC-Motion：一个将时域、空域与频域统一建模并结合因果干预的文本到动作扩散生成框架，在HumanML3D上R@1达0.612，生成更逼真、一致且文本对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法多仅关注时空建模或独立的频域分析，缺少三域联合优化，难以同时利用多域信息，导致生成质量受限；同时噪声引入的与动作无关线索与有益特征纠缠，造成动作失真。

Method: 基于扩散模型，构建三域专用模块：Temporal Motion Encoding（时序编码保证时间一致性）、Spatial Topology Modeling（空间拓扑建模保证骨架结构与姿态合理）、Hybrid Frequency Analysis（混合频域分析刻画运动趋势与动态）。随后以Score-guided Tri-domain Fusion融合三域信息；并引入Causality-based Counterfactual Motion Disentangler，通过因果干预/反事实机制暴露并剥离与动作无关的噪声线索，分离各域的真实贡献。

Result: 在多个基准上优于SOTA；在人类动作文本数据集HumanML3D上取得R@1=0.612，并展现高保真、连贯、多样且与文本高度对齐的生成效果。

Conclusion: 统一的时-空-频三域建模与因果反事实去噪能协同提升文本到动作扩散生成的质量与鲁棒性，兼顾时间一致性、空间拓扑正确性及运动动态表达，达成SOTA性能；代码已开源。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [154] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 基于2D姿态估计，从真实交通视频中提取关键点并构建76项静/动态特征，将行人手势分为“停、行、致谢/致意、无手势”，以手部位置与速度为核心判别，分类准确率达87%。


<details>
  <summary>Details</summary>
Motivation: 行人-驾驶员（含自动驾驶）之间大量依赖手势等非语言沟通，但规则不足或含糊时，AV难以正确解读。需要系统化方法识别常见行人手势，提升AV感知与决策的可靠性。

Method: 在WIVW真实道路视频上使用2D姿态估计，归一化关键点后构建76个静态与动态特征（尤其手部位置与运动速度相关），将手势划分为4类：Stop、Go、Thank & Greet、No Gesture；训练并评估分类器对手势进行识别。

Result: 手部位置与运动速度是最具判别力的特征；整体分类准确率达到87%。

Conclusion: 所提框架能有效区分常见行人手势，为自动驾驶系统在与行人互动时提供更可靠的感知能力，并为理解交通情境下的行人行为提供实证支持。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [155] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 研究多类食品识别在不同光照条件下的域偏移问题，利用Food-101与Fruits-360进行跨数据集评估，发现光照差异显著降低准确率；通过合成光照增强（温度与强度可控）提升鲁棒性，并验证迁移学习/域泛化在苹果等对光照敏感类别上的改进，且仍保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景（如传送带自动检验）中光照变化导致视觉域偏移，现有研究多受限于单一品类或受控环境，且公开数据缺少光照标注，急需系统性方法评估与提升多类食品识别在光照变化下的稳健性。

Method: 1) 在Food-101与Fruits-360之间进行跨数据集评估以量化光照引起的性能退化；2) 构建合成光照增强数据集，通过系统改变色温与光强而无需额外标注，实现可控鲁棒性分析；3) 研究跨数据集迁移学习与域泛化策略，聚焦对光照敏感的目标类别（如苹果相关类）。

Result: 跨数据集存在显著精度下降；引入光照感知的数据增强后，识别在域偏移场景下显著提升且不牺牲实时性；对苹果等高敏感类别的鲁棒性提升尤为明显。

Conclusion: 光照鲁棒性是部署可靠食品识别系统的关键。通过可控的光照增强与合适的迁移/域泛化策略，可有效缓解因光照变化导致的域偏移，并为真实检验场景提供可行的工程实践。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [156] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: 提出Octopus：通过“纠错特定rollout重组”与“响应掩码”提升VLM自我纠错学习信号密度与稳定性；据此训练的Octopus-8B在7项基准上达开源SoTA，较最佳RLVR基线+1.0分、每步训练时间0.72×。


<details>
  <summary>Details</summary>
Motivation: VLM在复杂推理中需要自我纠错，但RL很难学到：有效的自我纠错行为在rollout中极其稀有，导致回报/监督信号稀疏与不稳定，样本效率低、优化困难。

Method: 1) Correction-specific rollouts：将已有rollout分解并重组，合成包含“发现错误-提出修正-得到更优答案”的密集纠错样例，既复用数据提升样本效率，又通过平衡的正负监督稳定优化；2) Response-masking：在训练中对输出进行掩码，解耦“直接推理”与“自我纠错”两种行为，避免梯度/信号冲突，使二者可分别有效学习；3) 据此训练一个8B参数推理型VLM，支持可控启用纠错能力（Octopus-8B）。

Result: 在7个基准上，Octopus-8B作为开源VLM取得SoTA，相比最强RLVR基线平均提升1.0分；由于rollout重用与掩码设计，每步训练时间仅为其0.72×，训练更高效与稳定。

Conclusion: 通过面向纠错的rollout增强与响应掩码，显著缓解RL中自我纠错信号稀疏与冲突问题，实现更高效稳定的训练；Octopus-8B验证了方法有效性，并提供可控的自我纠错能力及开源领先性能。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [157] [Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?](https://arxiv.org/abs/2602.08505)
*Caterina Fuster-Barceló,Virginie Uhlmann*

Main category: cs.CV

TL;DR: 研究评估了三种视觉基础模型（DINOv2、DINOv3、OpenCLIP）在两种电子显微镜数据集（Lucchi++、VNC）上的线粒体分割迁移能力。单域训练效果好，LoRA能进一步提升；多域联合训练显著退化，PEFT收效甚微。潜表示分析显示两个数据集存在显著、持久的域差异，阻碍单一鲁棒模型的获得。


<details>
  <summary>Details</summary>
Motivation: VFMs在生物医学影像被广泛复用，但其潜表示是否足够通用以跨不同显微数据集迁移未明。电子显微线粒体分割是重要任务，借此检验VFMs跨域复用的有效性，并评估轻量适配与参数高效微调对跨域泛化的帮助。

Method: 在Lucchi++与VNC两套EM数据上，分别使用DINOv2、DINOv3、OpenCLIP作为冻结骨干配轻量分割头，或采用LoRA进行PEFT微调。评估单域训练与跨域/多域联合训练，对比Foreground IoU表现。用PCA、Fréchet DINOv2距离与线性探针分析潜表示的域差异。

Result: 单一数据集上，三类骨干均取得良好分割性能，且LoRA稳定提升域内表现。联合两个数据集训练时，所有模型性能均明显下降，PEFT仅带来边际改进。潜空间分析显示两数据集间存在显著域不匹配，且在多种分析方法下一致。

Conclusion: 当前VFMs在EM线粒体分割的单域适配有效，但面对异构EM数据集，现有PEFT（如LoRA）不足以得到单一鲁棒模型；需要额外的域对齐机制（如显式域适配或不变特征学习）以缓解跨域性能退化。

Abstract: Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>


### [158] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: GeoFocus提出面向几何题的双模块框架：用“关键局部感知器”抓取角度/平行/距离等13类理论模板的局部结构，并用紧凑的拓扑语言VertexLang以顶点与连通关系编码全局图形。相比以往，局部关键特征覆盖+61%，全局训练耗时-20%、拓扑识别更准；在Geo3K/GeoQA/FormalGeo7K上精度+4.7%，在MATHVERSE下更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在几何题上易只看“整体形状”，难以精准捕捉与几何理论相关的关键局部关系（角、平行、相等、对比长度等），且全局结构常用笨重代码式表示，训练代价高、泛化差。需要一种既能突出关键局部证据、又能高效刻画全局拓扑的表示与学习方法。

Method: 提出GeoFocus，包括：1) Critical Local Perceptor：基于十三种几何理论感知模板，自动检测并强化角度关系、平行/垂直、相等等关键局部结构，提升局部证据覆盖；2) VertexLang：以顶点坐标与连通关系描述全局图形的紧凑拓扑语言，替代臃肿代码表示，配合全局感知学习提升拓扑识别与训练效率。

Result: 与最强专用模型相比，在Geo3K、GeoQA、FormalGeo7K平均精度提升4.7%；关键局部特征覆盖率提升61%；使用VertexLang后全局感知训练时间降低约20%，拓扑识别精度提高；在MATHVERSE多样视觉条件下表现更稳健。

Conclusion: 面向几何题，兼顾“局部关键关系强化+全局拓扑紧凑编码”是有效路径。GeoFocus通过模板化局部感知与VertexLang全局表示，显著提升准确率、效率与鲁棒性，为LMM几何推理提供了可扩展的表示与学习框架。

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [159] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 提出一种在X射线断层重建中自动选择正则化参数的方法：通过两种不同离散化的重建结果相似性作为准则，用反馈控制迭代地减小正则强度，直到两网格重建足够一致，并在真实数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 有限投影/欠采样使X射线层析重建高度病态，重建质量强依赖正则化，但手工或启发式选参不稳健、代价高或需额外先验，需一种无需真值、可自动、稳健的参数选择策略。

Method: 构建同一成像问题的两种计算离散化（如粗/细网格或不同像素化），在给定正则参数下分别求解重建；用相似性度量（例如差异范数）比较两网格解；设计反馈控制器根据相似性与阈值的偏差自适应调整正则强度，迭代寻找到使两解“足够接近”的最小正则参数；以此作为最终参数并输出对应重建。

Result: 在真实X射线断层数据上，所提方法能稳定找到较小且有效的正则化参数，使重建在保真与先验约束间取得良好平衡，并优于固定或人工调参方案；展示了重建质量提升与鲁棒性。

Conclusion: 利用双离散化一致性作为无真值的准则，并配合反馈控制选参，可在病态层析重建中自动、稳健地确定正则化强度；该策略实用、数据驱动，适合有限角度或欠采样场景。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [160] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 提出一种基于稀疏、单目、图优化的热成像SLAM，使用可迁移的可见光训练特征（SuperPoint+LightGlue），通过预处理与模块改造适配热像特性，并将关键点置信度融入因子图以提升鲁棒性；在公开热成像数据集上无需再训练也取得可靠表现。


<details>
  <summary>Details</summary>
Motivation: 热成像在低光、烟雾、恶劣天气下更稳健，但其低纹理、低对比度与高噪声使传统特征法SLAM失效；同时，缺乏高质量热数据难以支撑在热域专门训练或微调深度特征。需要一种能直接在热数据上稳定工作的SLAM方案，且尽量复用可见光域的大规模学习能力。

Method: - 采用通用学习特征：SuperPoint检测 + LightGlue匹配（原在可见光上训练）以增强跨域泛化。
- 预处理管线：对热图像进行适配性增强（如对比度/归一化/去噪等，摘要未详述）。
- SLAM模块改造：针对稀疏、离群较多的匹配，调整前端和后端（鲁棒估计、关键帧/地图点管理等）。
- 置信度加权因子图：将SuperPoint关键点置信度引入图优化的因子权重，提升鲁棒性与估计质量。

Result: 在公开热成像数据集上，无需针对数据集的再训练或微调即可获得可靠定位/建图性能，优于或匹配需要专门训练的方法（摘要未给出具体数值）。

Conclusion: 通用可见光训练的特征在经适配后可有效支持热成像稀疏单目图优化SLAM；置信度加权与预处理显著提升鲁棒性。该方法缓解了热域数据稀缺问题，具有实用价值；代码将在发表后公开。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [161] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 提出一个无监督/无学习的4D Gaussian场景目标级分割框架TIBR4D，通过将视频分割掩码提升到4D，高效获得清晰边界的对象高斯点云。核心为两阶段迭代边界细化：时间段级的迭代实例追踪（IGIT）与逐帧渲染范围控制（RCC），并配合时间分段合并策略以兼顾身份一致性与动态变化。实验在HyperNeRF与Neu3D上优于现有方法，边界更清晰、效率更高。


<details>
  <summary>Details</summary>
Motivation: 动态场景的4D Gaussian表示中进行对象级分割很难，受复杂运动、遮挡与边界模糊影响。现有多为一次性阈值或需学习的方法，容易在遮挡处丢失结构、边界不准且效率受限。作者希望在无需训练的前提下，稳定、准确且高效地获得对象级高斯点云并改善边界质量与遮挡鲁棒性。

Method: 提出TIBR4D，两阶段迭代细化：1) IGIT（时间段级）：基于视频掩码初始化高斯-实例概率，沿时间段迭代“追踪—更新—重提取”，逐步提高实例一致性并生成更完整的对象高斯点云；并引入“时间分段合并”策略：长段增强身份稳定，短段敏感于动态变化，权衡一致性与及时更新。2) RCC（逐帧）：在渲染时对靠近边界且不确定性高的高斯进行抑制，仅保留其核心贡献，从而锐化边界、减少越界渲染。整体为学习无关，直接从视频掩码提升至4D高斯场。

Result: 在HyperNeRF与Neu3D基准上，生成的对象级高斯点云边界更清晰、遮挡下结构更完整，且推理效率高于SOTA一次性阈值或学习型方法；定量与可视化均显示更优的分割精度与效率。

Conclusion: 学习无关的TIBR4D通过时间段级迭代追踪与逐帧渲染控制，有效解决动态4D高斯场景中目标级分割的遮挡与边界难题；在多个基准上实现更清晰边界与更高效率，证明该策略在实际应用中的实用性与泛化潜力。

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [162] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 提出GOT-Edit：在2D视频上进行在线跨模态模型编辑，把3D几何线索融入通用目标跟踪器，以提升遮挡、干扰与外观/几何变化下的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有通用目标跟踪器主要依赖2D外观与局部上下文，忽略3D几何先验，因而在部分遮挡、相似干扰物与显著姿态/形变下易失败。人类感知依赖3D先验与语义推理，启发将几何线索融入2D跟踪以提升鲁棒性。

Method: 利用预训练的视觉几何对齐Transformer提取可从少量2D图像推断的几何感知特征；在在线跟踪过程中进行“空域约束(null-space)的模型编辑”，将几何信息注入跟踪模型参数，同时约束更新落在不破坏原有语义判别能力的子空间，做到几何与语义的无缝融合。

Result: 在多种GOT基准上实现更高的鲁棒性与精度，尤其在遮挡与杂乱场景下显著优于现有方法，表现稳定一致。

Conclusion: GOT-Edit证明了在2D语义表征中在线注入3D几何推理的有效性，为通用目标跟踪提供了一种新范式：通过跨模态在线编辑在保持语义判别的同时引入几何先验，显著提升复杂场景下的跟踪表现。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [163] [FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction](https://arxiv.org/abs/2602.08558)
*Guan Yuan Tan,Ngoc Tuan Vu,Arghya Pal,Sailaja Rajanala,Raphael Phan C. -W.,Mettu Srinivas,Chee-Ming Ting*

Main category: cs.CV

TL;DR: FLAG-4D提出通过时空演化的3D高斯原语进行动态场景新视角合成，采用双重形变网络（局部瞬时+全局运动）并融合光流特征，较现有方法在稀疏视角下获得更高保真与更强时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有4D/动态新视角方法多用单一MLP建模时间形变，难以在稀疏视角下稳定捕获复杂点运动与细粒度动态细节，导致时序抖动与细节丢失。

Method: 以一组规范域3D高斯为基础，提出双重形变网络：Instantaneous Deformation Network（IDN）建模细粒度局部形变，Global Motion Network（GMN）捕获长程全局动力学；二者通过互学习共同优化。引入预训练光流主干产生致密运动特征，融合相邻帧信息，并用“形变引导注意力”将光流与当前高斯状态对齐，从而得到时间平滑且精确的形变（位置与各向异性尺度）。

Result: 在多项实验中，相比SOTA方法，FLAG-4D重建的细节更丰富、时序一致性更强，且在稀疏视角条件下表现更稳健。

Conclusion: 双重形变+光流融合的3D高斯时空建模能有效提升动态场景新视角合成的保真度与时间稳定性，为稀疏多视角下的4D重建提供了更可靠方案。

Abstract: We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>


### [164] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: 提出SemiNFT：一种基于DiT的参考图像驱动调色与风格迁移框架，先监督学配对三元组学结构与颜色映射，再用无配对数据的强化学习提升审美与语义理解，并通过在线-离线混合奖励避免遗忘；在预设迁移基准上SOTA，并在零样本如黑白上色与跨域（动漫→照片）迁移中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有参考式调色多依赖像素统计做全局映射，缺乏对语义与人类审美的理解，效果“像新手”，且手工调色门槛高。需要一种既能保持结构、实现可控色彩迁移，又能具备更高层次审美与语义感知的自动化方法。

Method: 提出SemiNFT框架：1) 基于Diffusion Transformer。2) 训练分两阶段：监督阶段用成对三元组（源、参考、目标）学习结构保持与颜色映射；强化学习阶段在无配对数据上，通过混合在线-离线奖励进行美学探索，同时用结构回顾信号锚定以防灾难性遗忘。3) 奖励体系同时考虑审美质量与结构一致性。

Result: 在标准预设迁移基准数据集上超越SOTA；展现强零样本能力，包括黑白照片上色与跨域（动漫到照片）的预设迁移；结果表明其超越简单统计匹配，具备更成熟的美学理解。

Conclusion: 分阶段“先模仿后创造”的训练范式结合DiT与RL，可在保持结构的同时学习到更高层次的审美与语义，显著提升参考式调色的真实感与泛化能力，并能零样本应对新任务。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [165] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 论文综述中国AVS工作组制定的首代点云压缩标准（AVS PCC），从技术工具与性能对比两方面回顾其与MPEG的G-PCC/V-PCC差异与优势。


<details>
  <summary>Details</summary>
Motivation: 点云数据体量庞大，传输与存储成本高，制约在沉浸式媒体、自动驾驶、数字文保等场景的部署；需面向人/机感知优化的高效压缩标准。MPEG已有G-PCC与V-PCC，中国需要自主且技术差异化的标准体系，因此开展并完成AVS PCC标准化。

Method: 对AVS PCC标准进行系统性回顾：梳理其采用的新型编码工具与技术，并与MPEG G-PCC/V-PCC进行横向性能与技术对比评估。

Result: AVS PCC引入多项不同于MPEG的编码工具与技术，实验/对比显示其在若干指标或场景下具备竞争力（摘要未给出具体数值，但强调技术差异与性能比较结果）。

Conclusion: AVS PCC作为中国首代点云压缩标准，在工具链与性能上形成与G-PCC/V-PCC的差异化与补充，为面向人机感知优化的点云应用提供新的标准选择。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [166] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: 提出“Inspiration Seeds”框架：输入两张参考图，无需文本提示，前向生成多样且视觉一致的合成图，揭示两者的潜在关系，用于早期创意发散。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型偏向执行精确文本提示，不利于设计师在创意前期进行开放式视觉探索；设计实践常依赖松散关联的视觉参考以激发新联想，缺乏无语言依赖、快速重组的工具。

Method: 训练基于纯视觉构造的合成三元组：利用CLIP稀疏自动编码器在CLIP潜空间提取可编辑方向、分离概念对；建立前向生成模型，给定两张图，将其分解为视觉要素并重组，生成多样、连贯的合成图，无需文本提示。

Result: 模型可从两张输入图产生多样且视觉一致的合成结果，显式揭示并重组两者的潜在概念与关系，实现快速、直观的视觉构思。

Conclusion: 通过去语言化与前向高效生成，框架支持创意早期的模糊探索与灵感联想，为设计流程提供从参考到新构图的直观途径。

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [167] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 提出LV-RAE，一种在语义特征上补充低层信息的表征自编码器，同时通过解码器鲁棒化与受控噪声平滑缓解高维潜变量解码敏感性，从而兼顾高保真重建与强生成质量。


<details>
  <summary>Details</summary>
Motivation: 以Vision Foundation Models作为图像编码器的LDM能更易学习语义分布，但语义特征缺乏颜色、纹理等低层信息，造成重建细节差，成为进一步扩展LDM的瓶颈；此外，高维信息丰富的潜空间在生成时对微小扰动敏感，导致解码伪影与生成质量下降。

Method: 1) 设计LV-RAE：在VFM语义特征上补充缺失的低层信息，得到同时语义对齐且信息充足的潜变量；2) 诊断解码器对“离数据流形方向”的过度响应造成的敏感与伪影；3) 两种缓解：a) 微调解码器以提升鲁棒性；b) 在生成阶段对潜变量施加受控噪声以平滑潜空间分布。

Result: 在多项实验中，LV-RAE显著提升图像重建保真度，同时保持语义抽象能力；配合解码器鲁棒化和潜变量平滑后，生成质量得到明显提升，较现有方法在重建与生成两方面均取得更优表现。

Conclusion: 通过在语义潜表示中补足低层信息并抑制解码器对流形外方向的过响应，LV-RAE实现了高保真重建与强生成质量的兼顾，为基于VFM的LDM提供了可扩展的表征与解码策略。

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [168] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 论文提出在ViT中对[CLS]与patch token进行“专门化处理”，尤其在归一化层和早期QKV投影上分离通路，从而缓解全局/局部特征学习的摩擦；在分割任务上+2 mIoU、参数仅增8%、算力不增，分类精度保持。


<details>
  <summary>Details</summary>
Motivation: 现有ViT用可学习的[CLS]汇聚全局信息、patch token承载局部信息，但两者在网络各层被同质化处理；标准归一化层会隐式地差异化这两类token，导致全局/局部特征学习存在冲突，影响密集预测任务的patch表示质量。

Method: 分析类/patch token交互与归一化影响，提出对两类token进行选择性“解耦”的专用通路：在归一化层和早期QKV投影中为[CLS]与patch token采用不同的处理路径（参数/统计），从而在保持跨token交互的同时减少不必要的耦合。改动轻量：参数+8%，无额外计算开销。

Result: 在标准分割基准上带来>+2 mIoU的提升，同时保持强分类精度；通过全面消融，定位哪些组件最受益，并验证跨模型规模与学习框架的泛化。

Conclusion: 为ViT引入对类/patch token的定制化归一化与QKV专用化可有效提升密集预测的patch表示质量，几乎不增加计算成本且维持分类性能；这种“按token类型专门化”的设计是一条通用、可扩展的改造策略。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [169] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 提出一种用低分辨率缩略图预测病理切片固定类型（FFPE/FS）的深度学习模型，在TCGA上AUROC 0.88，跨扫描仪泛化受域移位影响（Regensburg与Augsburg AUROC 0.72），处理速度每张21毫秒，比高分辨率方法快约400倍，可用于高通量质控与标签错误检测。


<details>
  <summary>Details</summary>
Motivation: 手工标注固定类型易错，影响诊断与后续分析；现有验证方法多依赖全分辨率WSI，计算昂贵且不利于规模化高通量质控。

Method: 使用预扫描低分辨率缩略图训练深度学习分类器区分FFPE与FS（以及FFPE/FS/可能多类），训练集为TUM 1,200张（Leica GT450DX），评估于TCGA 8,800张（Leica AT2）和两外部中心Augsburg 695张（Philips UFS）与Regensburg 202张（3DHISTECH P1000），类均衡抽样；与其他预扫描方法及高放大全分辨率方法比较。

Result: 在TCGA上AUROC 0.88，较可比预扫描方法提升4.8%；在Augsburg与Regensburg上AUROC均为0.72，显示跨扫描仪域移位带来性能下降；单张推理21 ms，约比高分辨率方法快400倍。

Conclusion: 低分辨率缩略图也能可靠预测固定类型，显著提升速度，适合高通量病理质控与标签错误检测；需进一步提升对不同扫描仪的泛化并扩展到更多低分辨率标注任务。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [170] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: WiFlow提出一种基于WiFi CSI的连续人体姿态估计框架，采用时序与不对称卷积的编码器提取时空特征，结合轴向注意力建模关键点依赖，解码器映射到关键点坐标。在自采36万对CSI-姿态样本上取得PCK@20=97.00%、PCK@50=99.48%、MPJPE=0.008m，仅4.82M参数，精度高且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有WiFi姿态估计方法在连续动作场景下表现不稳且计算开销大；部分方法将CSI当作图像用2D残差网络处理，破坏序列结构，难以高效捕捉时空关联。需要一种既保留CSI序列特性、又能高效建模人体关键点依赖、适合IoT低成本部署的方法。

Method: 设计WiFlow的编码-解码架构：编码器用时间卷积与不对称卷积提取并解耦CSI的时空特征；通过轴向注意力细化关键点特征并捕获人体结构依赖；解码器将高维特征回归为关键点坐标。用包含5名受试者、8类日常连续活动、36万对齐的CSI-姿态样本训练；模型仅4.82M参数以降低复杂度。

Result: 在自建数据集上，达到PCK@20=97.00%、PCK@50=99.48%，平均关节位置误差MPJPE=0.008米；相较视觉式2D残差网络类方法，显著降低参数量与计算成本，同时在连续动作中保持稳定高精度。

Conclusion: WiFlow证明利用保持序列结构的编码器与轴向注意力能高效从WiFi CSI中恢复连续人体姿态，在精度与效率上建立新基线，适合IoT实际部署；公开代码与数据集促进复现与后续研究。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [171] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

TL;DR: 论文探讨以数据驱动离散化为核心的机器学习求解PDE方法：在结构化网格上学习准线性模板系数以替代/增强传统差分/有限体积离散，从而在低分辨率下提升精度与稳定性；实现并对比浅水方程与欧拉方程的经典求解器与ML求解器，部分深度网络方案给出满意解。


<details>
  <summary>Details</summary>
Motivation: 传统ML在图像/文本等领域成功，但对具有强数学约束（如PDE）的应用仍不明确；现有低分辨率数值模拟受限于离散误差与稳定性；希望用ML学习更优离散模板，同时继承数值方法的物理约束（如守恒）。

Method: 在结构化网格上采用数据驱动离散化：训练深度神经网络预测用于计算函数值/导数的准线性模板系数，并可在有限体积框架中嵌入以实现守恒。实现浅水方程与欧拉方程的经典（非ML）求解器作为基线并与Pyclaw对比；提出四种深度网络架构作为ML求解器并评估其解的质量。

Result: 自研的经典求解器在基准实验中显著优于Pyclaw；四种ML架构中有两种能够产生“满意”的解，显示数据驱动离散化在低分辨率下可提升精度与稳定性并具备可行性。

Conclusion: 数据驱动离散化是加速并改进结构化网格PDE求解的有前景途径，可在保持（通过有限体积适配）守恒等物理约束的同时提升低分辨率模拟质量；初步实验表明该方法可行，但不同网络架构性能差异明显，需进一步研究与完善。

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


### [172] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: ALIVE 将现有文本生成视频模型适配为可同时生成并对齐音频与视频的统一生成框架，支持文本到音视(T2VA)与参考动画；在大规模高质数据上继续预训练与微调后，效果优于开源并可比肩商用SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型缺乏音画同步与基于参考的动画能力，社区也缺少系统的数据流程与基准来评测统一视听生成；需要一种能在不从零开始训练的前提下，把成熟的T2V能力扩展到音视频联合生成与精确对齐的方案。

Method: 以MMDiT为骨干，新增联合音视频分支：引入TA-CrossAttn实现时间对齐的跨模态融合，引入UniTemp-RoPE提升音画时序对齐精度；配套构建包含音视频字幕生成与质量控制的完整数据流水线；在百万级高质量数据上继续预训练与微调；提出新的综合基准用于系统评测与对比。

Result: ALIVE 获得T2VA与基于参考的动画能力，在多项基准上稳定优于开源模型，并达到或超越商业SOTA的水平；展示了更好的音画同步、参考驱动的动作一致性与整体生成质量。

Conclusion: 通过对T2V模型的结构扩展与数据/评测体系建设，ALIVE 实现了高质量统一音视频生成，验证了在现有基础上适配与扩展为视听联合生成是可行且高效的路径，并为社区提供可复现的配方与基准。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [173] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: 论文提出OneVision-Encoder（OV-Encoder），以“视频即压缩/编解码问题”为核心，将计算聚焦于高信息熵的稀疏区域，通过Codec式patch稀疏化与共享3D RoPE统一时空建模，并以大规模聚类判别目标学习语义与运动；在更少token与数据下，显著超越现有强大视觉骨干，视频任务平均+4.1%。


<details>
  <summary>Details</summary>
Motivation: AGI本质是压缩：高效压缩需与数据结构共振。视觉信号冗余大、真正判别信息稀疏，但主流模型对致密像素均匀计算，浪费算力于静态背景，未对“可预测残差/惊奇”聚焦。作者主张以信息论与视频编解码原则重构视觉架构，使计算对齐高熵区域与预测残差。

Method: 提出OneVision-Encoder：1) Codec Patchification：放弃均匀patch，动态选择仅约3.1%-25%高信号熵区域进行计算，实现patch级稀疏化；2) 共享3D RoPE：在不规则token布局下统一时空位置编码，兼容空间与时间推理；3) 大规模聚类判别训练：在百万级语义概念上进行cluster discrimination，联合学习物体持久性与运动动态；4) 与LLM集成，作为视觉前端生成稀疏而语义密集的视觉token。

Result: 在16个图像/视频/文档理解基准上，OV-Encoder以更少视觉token和更少预训练数据，稳定优于强力基线（如Qwen3-ViT、SigLIP2）；尤其在视频理解上，较Qwen3-ViT平均提升4.1%。显示效率与准确率正相关，而非权衡。

Conclusion: 将视觉理解架构与编解码/信息论对齐、实施patch级稀疏是可扩展的基础原则。OV-Encoder证明：聚焦高熵区域与预测残差能同时提升精度与效率，为下一代通用视觉模型提供可扩展引擎。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [174] [Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm](https://arxiv.org/abs/2602.08699)
*Xiaogang Xu,Kun Zhou,Tao Hu,Jiafei Wu,Ruixing Wang,Hao Peng,Bei Yu*

Main category: cs.CV

TL;DR: 提出VLLVE/VLLVE++：将低光视频分解为视图无关(本征外观)与视图相关(光照/遮蔽)两部分，并通过跨帧对应与场景连续性约束实现一致增强；再加入残差项与双向学习，兼顾增强与对应 refinement，在真实高动态场景上效果强。


<details>
  <summary>Details</summary>
Motivation: 传统LLVE难以同时处理跨帧一致性与真实场景退化（动态、噪声、复杂光照）；单帧方法忽视时序与视角因素，易产生闪烁和误增强；现有分解模型难以覆盖复杂退化。

Method: 1) 视图感知分解：图像被拆为视图无关(内在外观)与视图相关(光照/阴影)。无关项用动态跨帧对应对齐并共享，相关项施加场景级连续性约束。2) 双结构增强网络：具备跨帧交互机制，同时监督多帧，使分解特征匹配；可无缝嵌入编解码单帧网络，参数增量小。3) VLLVE++：在分解上叠加可加性残差项，模拟难建模的场景自适应退化；引入增强与退化感知的双向学习，端到端地细化与筛除跨帧对应。

Result: 在多个主流LLVE基准上取得优异表现，尤其在真实世界与高动态视频上表现稳健；对应估计更可靠、闪烁更少、增强更一致。

Conclusion: 视图感知分解结合跨帧交互与残差建模，有效提升低光视频的时序一致性与真实鲁棒性；VLLVE++通过残差与双向学习进一步扩大适用性并改进对应质量。

Abstract: Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>


### [175] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: 提出Omni Dense Captioning任务：以时间戳为轴，生成连续、细粒度、结构化的视听叙事；并构建基准、度量、数据与模型，SOTA并促进下游时序与多模态理解。


<details>
  <summary>Details</summary>
Motivation: 现有视频/音频描述多为片段级、粗粒度、缺少统一结构与明确时间边界，难以覆盖场景细节、对齐多模态线索，且评测易受场景切换模糊干扰。作者希望提供“剧本式”高密度叙事能力与更公平可靠的评测框架。

Method: 1) 定义六维结构化模式，按时间戳输出“剧本式”多维描述；2) 构建人工标注基准OmniDCBench；3) 提出统一度量SodaM，时间感知并减轻场景边界歧义；4) 汇集训练集TimeChatCap-42K；5) 训练TimeChat-Captioner-7B，采用SFT+GRPO并引入任务特定奖励。

Result: TimeChat-Captioner-7B在OmniDCBench上达SOTA，优于Gemini-2.5-Pro；其生成的高密度描述显著提升下游音视频推理（DailyOmni、WorldSense）与时间定位（Charades-STA）表现。

Conclusion: 结构化、时间对齐的稠密多模态叙事是提升视听理解与时序推理的关键。所提出的任务、基准、度量、数据与模型形成完整生态，具备强基线并推动相关研究；代码与数据将开源。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [176] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 论文通过“分阶段模型差分”方法，首次从机制层面解析多模态微调如何让语言模型获得“看见”的能力：在微调过程中出现并重定向了偏好视觉的特征，其中一部分稳定编码空间关系，这些特征的因果激活可追溯到少量注意力头。方法提高了多模态训练的可解释性，并为优化语言模型的视觉对齐提供基础。


<details>
  <summary>Details</summary>
Motivation: VLM 取得强性能，但尚不清楚：语言骨干在多模态微调中如何适配？视觉能力何时、何处出现？现有工作缺乏对表征改变的机制性解析和可定位证据。

Method: 提出“分阶段模型差分（stage-wise model diffing）”：逐阶段比较预训练语言模型到多模态微调各阶段的表示，识别微调期间新生或被重定向为视觉偏好的特征；利用受控空间提示（spatial prompts）扰动，检测哪些特征稳定编码空间关系；进一步进行因果溯源，将关键特征的激活归因到少量注意力头。

Result: 发现：1）微调中出现并重定向出一组偏好视觉的特征；2）其中一小撮特征对空间关系具有稳定、可泛化的编码；3）这些空间特征的因果来源可追溯到少量特定注意力头；4）视觉对齐会重塑原本纯文本特征，实现模态融合。

Conclusion: 分阶段模型差分能揭示多模态特征何时何处产生并如何由注意力头实现，对理解视觉落地与模态融合提供了清晰视角，为解释与改进预训练语言模型获取视觉能力奠定方法学基础。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [177] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 提出三种无需训练的零样本体部位识别管线，用于CT/MR体积数据；基于预训练分割的规则法最稳健，CT/MR加权F1分别0.947/0.914；MLLM在视觉明显区域尚可，分割感知型MLLM暴露出局限。


<details>
  <summary>Details</summary>
Motivation: 现有体部位识别高度依赖不可靠的DICOM元数据，多为有监督学习，在真实场景泛化与标注成本受限。作者探究仅利用大规模预训练基础模型蕴含的知识，能否实现完全零样本、无需再训练的体部位检测。

Method: 设计并系统评估三条训练-free流程：1) 以预训练多器官分割模型为基础的分割驱动规则系统；2) 由放射科规则指引的多模态大语言模型（MLLM）；3) 将视觉输入与显式分割证据结合的分割感知型MLLM。于887例异质CT/MR扫描（具人工核验标签）上评测，关注加权F1与跨模态、非典型覆盖的稳健性。

Result: 分割驱动规则法表现最强且最稳健：CT加权F1=0.947，MR加权F1=0.914，能适应不同模态与异常覆盖范围。纯MLLM在视觉特征明显的部位有竞争力；分割感知型MLLM表现不佳，暴露基础瓶颈。

Conclusion: 在零样本体部位识别任务中，依托预训练分割并配合规则的传统方法目前优于直接或分割感知的MLLM；基础模型的通用知识尚不足以稳定涵盖多模态与非典型扫描，实际部署仍以可解释、稳健的分割驱动策略为宜。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [178] [Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering](https://arxiv.org/abs/2602.08724)
*Geng Lin,Matthias Zwicker*

Main category: cs.CV

TL;DR: 提出RotLight：通过在采集时让物体进行少量旋转（最少两次）来缓解逆向渲染中材质与光照分解的歧义，并结合2D高斯+代理网格以改进入射光追踪与全局光照，从而高效获得更准确、无烘焙阴影的反照率。


<details>
  <summary>Details</summary>
Motivation: 基于NeRF/高斯的逆向渲染虽能恢复几何与辐射，但材质/光照分解高度不适定，易产生颜色偏差与反照率中“烘焙阴影”。现有正则难以彻底消歧，且全局光照与入射光估计不准。

Method: 1) 采集装置RotLight：在常规多视采集基础上，仅需让物体在拍摄过程中进行少量旋转（如两次），引入可辨别的照明-视角变化以打破材质/光照耦合歧义。2) 以2D Gaussian Splatting为核心的逆向渲染框架，加入代理网格（proxy mesh）用于精确的入射光追踪，并施加残差约束以校正辐射与着色误差；同时改进全局光照处理。

Result: 在合成和真实数据上，相比现有方法，获得更优的反照率（颜色更准确、阴影未被烘焙）、同时保持计算高效；少量旋转（最少两次）已显著降低伪影。

Conclusion: 通过极小的采集改动（物体旋转）与代理网格增强的2DGS逆向渲染，可有效缓解材质-光照歧义，提升反照率质量与全局光照建模，并在效率上保持竞争力。

Abstract: Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.

</details>


### [179] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: FusionEdit是一个无需训练的文本引导图像编辑框架，通过自动识别需编辑/保留区域、边界距离感知潜变量融合与TV平滑、以及在DiT注意力中结合AdaIN进行统计注意力融合，实现更精细、自然且可控的编辑，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖显式二值掩码限定编辑区域，但硬边界导致伪影、过渡生硬，且限制编辑性；需要一种兼顾精确编辑与自然过渡、同时保持源图像身份与全局一致性的方案。

Method: 1) 语义差异驱动的区域定位：根据源/目标文本提示之间的语义差异自动划分编辑与保留区域。2) 距离感知潜变量融合：沿区域边界进行基于距离的latent融合生成软掩码，并加入总变分（TV）损失以平滑过渡，减少边界伪影。3) 统计注意力融合：在DiT注意力层中引入AdaIN调制，对编辑区域进行统计特征与注意力的融合，既增强可编辑性又保持与源图的全局一致性。4) 全流程训练免：直接在扩散变换器框架中运行。

Result: 在广泛实验中，FusionEdit在编辑精度、自然过渡与身份保持方面显著优于SOTA方法，视觉与定量指标均有提升；提供开源代码以复现。

Conclusion: 通过软掩码的距离感知潜融合与AdaIN统计注意力融合，FusionEdit在无需训练的前提下实现精细且自然的文本引导编辑，缓解硬边界伪影并提升可编辑性与一致性，具有实用价值与可复现性。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [180] [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](https://arxiv.org/abs/2602.08726)
*Khadija Iddrisu,Waseem Shariff,Suzanne Little,Noel OConnor*

Main category: cs.CV

TL;DR: 该论文利用事件相机与脉冲神经网络（SNN）对眼动（扫视与凝视）进行分类，在合成数据集上训练并在真实数据上微调，取得最高0.83准确率，并在不同时间分辨率下保持稳定，同时较ANN具显著计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机存在运动模糊与时间分辨率受限，难以精确捕捉快速眼动；且真实标注数据稀缺、成本高。作者希望用事件相机的高时域与无模糊特性，加上可控的合成数据，提升眼动分类的准确性、稳定性与效率。

Method: 1) 使用Blender构建可控的合成事件流数据，模拟扫视与凝视；2) 设计并训练两种SNN架构，在合成数据上预训练；3) 在真实事件相机数据上进行微调；4) 评估不同时间分辨率设置下的分类性能与对比ANN的计算开销。

Result: 模型在任务上最高达到0.83准确率；在不同时间分辨率下性能基本稳定；与ANN对比，SNN在推理计算上更高效。

Conclusion: 利用合成事件数据与SNN可实现稳定且高效的眼动分类，证明事件相机在快速动态视觉任务中的优势，并表明合成数据增强对事件视觉有实际价值。

Abstract: The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>


### [181] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 提出一种混合2D-3D深度学习框架，先用2D U-Net提取切片特征，再用3D解码器融合体素上下文以去除稀疏采样CT伪影，实现高质量、计算高效的3D重建，跨切面一致性显著提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 稀疏采样（欠采样）CT可减少时间和辐射，但带来严重伪影，降低诊断价值；需在有限计算预算下提升三维一致性与图像质量。

Method: 两阶段混合模型：1) 对每个切片用2D U-Net提取特征图；2) 将切片级特征沿体轴堆叠，输入3D解码器，利用跨切片上下文重建无伪影的3D体数据。兼顾2D高效性与3D体一致性。

Result: 在冠状和矢状方向的层间一致性显著改善，同时保持低计算开销；总体图像质量和伪影抑制优于纯2D或纯3D基线（摘要未给出具体数值）。

Conclusion: 该混合框架在保持效率的同时提升三维一致性与去伪影效果，适用于高质量3D CT后处理；实现代码已开源，具备复现与扩展潜力。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [182] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: 提出CGA：在源不可用的域自适应中，利用CLIP引导显式建模并缓解类别混淆，显著提升细粒度与易混场景性能。


<details>
  <summary>Details</summary>
Motivation: SFDA常用伪标签在细粒度任务中受细微类间相似度影响，出现不对称且动态的类别混淆，导致噪声标签与差的判别性；现有方法忽视该混淆结构。

Method: CGA包含三部分：1) MCA：在目标域基于源模型预测统计，检测定向（不对称）的混淆类对；2) MCC：借助CLIP构造“易混上下文”文本提示（如“像公交车的卡车”），进行更具语境感知的伪标签生成；3) FAM：为CLIP与源模型分别建立混淆引导的特征库，用对比学习对齐以消解表示空间歧义。

Result: 在多数据集上优于现有SOTA SFDA方法，尤其在细粒度和高混淆场景有显著提升。

Conclusion: 显式建模并利用跨模态先验处理类间混淆，是提升源不可用域自适应效果的关键；CGA验证了这一点并给出通用框架。

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [183] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: 提出HATCH框架，通过显式“跨视角对应”与“逐步视角变换”两机制训练MLLM，显著提升多图空间推理，同时不损伤单图能力。


<details>
  <summary>Details</summary>
Motivation: 多图空间推理需整合多视角信息，但现有MLLM多只部分、隐式利用跨视角对应或视点变换，缺少显式监督，导致性能受限。认知研究表明人类依赖两机制：跨视角对应与逐步视角变换。

Method: 提出HATCH训练框架，包含两互补目标：1）Patch-Level Spatial Alignment：对跨视角中空间对应区域的patch表征进行对齐，显式监督跨视角对应；2）Action-then-Answer Reasoning：先生成显式的视角转换动作序列，再输出最终答案，实现可解释的逐步视角变换。

Result: 在三个基准上，相较同规模基线有显著提升，并与更大模型达到具竞争力表现，同时保持单图推理能力不下降。

Conclusion: 显式建模并监督跨视角对应与逐步视角变换能有效提升MLLM多图空间推理；HATCH在效率与性能间取得良好平衡，并兼容单图任务。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [184] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 提出一种用于流匹配（flow matching）图像编辑的“实例解耦注意力”，可在单次前向中实现多实例、区域级别的独立编辑，减少语义干扰并保持全局一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于流/扩散的文本引导图像编辑方法多依赖全局条件与联合注意力，适合全局或单指令编辑，但在多实例（多个部位独立编辑）时容易相互干扰，导致编辑纠缠与越界。需要一种在连续时间动力学（flow）框架下实现指令—空间区域绑定、提升局部可控性且保持效率的方法。

Method: 在流匹配模型的速度场估计中引入“实例解耦注意力”：将联合注意力按实例进行分区，将每条实例级文本指令与对应空间区域进行强绑定，从而在注意力和条件化上解耦不同实例；整体仍为单次、连续时间的推理流程，适配自然图像与文本密集信息图编辑。

Result: 在自然图像编辑与新构建的文本密集信息图（包含区域级编辑指令）基准上，方法实现更好的编辑解耦性与局部性，同时维持全局连贯；支持单次前向的实例级编辑，较现有流式编辑器在多实例场景下更稳健。

Conclusion: 通过实例解耦注意力对条件化与注意力进行分区绑定，可在流匹配编辑中实现高效的多实例独立编辑，减少语义干扰并保持全局一致性，拓展了流式图像编辑在复杂、多指令场景下的实用性。

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [185] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: MVAnimate提出利用多视角先验融合2D与3D信息生成角色动画，提升时空一致性与视频质量，并能优化目标角色的多视角视频；在多数据集上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有仅基于2D或3D的人体姿态建模方法存在输出质量不足与训练数据匮乏的问题，难以生成高质量动画视频；需要一种能有效结合多源信息并改善时空一致性的方案。

Method: 提出MVAnimate框架：以多视角先验为核心，联合建模动态人物的2D与3D信息；通过多视角一致性约束与时空一致性机制，生成空间连贯、时间稳定的动画；并对目标角色的多视角视频进行联合优化以进一步提升质量。

Result: 在多样化数据集上实验显示，相较现有动画方法，MVAnimate在时序一致性、空间连贯性与多视角视频质量上都有提升，对不同动作模式与外观具有较强鲁棒性。

Conclusion: 多视角先验驱动的2D+3D融合是提升角色动画生成质量的有效途径；MVAnimate能稳定生成高质量、多视角一致的动画并具备良好泛化能力。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [186] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 提出一种面向CPU、确定性的说话人头像（THG）生成框架，通过语音→音素→视觉音素（viseme）映射与符号化协同发音，实现低算力条件下的实时口型合成，效果接近可用并显著降低延迟与负载。


<details>
  <summary>Details</summary>
Motivation: 现有THG多依赖GPU的神经渲染、大规模训练或扩散模型，不适合离线或资源受限的教育场景。需要一种在低端硬件上仍可提供可接受口型同步与稳定性的方案。

Method: 构建“Symbolic Vedic Computation”框架：1) 将语音强制对齐为音素序列；2) 将音素映射到紧凑viseme集合；3) 受吠陀经Urdhva Tiryakbhyam启发的符号化协同发音，生成平滑的viseme轨迹；4) 轻量2D渲染器在CPU上进行ROI形变与口部合成，并配合稳像；5) 全流程CPU可实时运行。

Result: 在仅CPU执行条件下评测了同步准确性、时间稳定性与身份一致性，并与可在CPU运行的代表性基线对比；结果显示在显著降低计算负载与时延的同时，口型同步质量达到可接受水平。

Conclusion: 符号计算与轻量2D渲染可在低端硬件上实现实用的教育头像THG，减少对GPU/大模型的依赖，适合离线与资源受限应用。

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [187] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 提出多模态异常检测框架MultiDeepSAD，将高分辨率图像与受电弓接触力数据融合，并配合伪异常生成与两套同步数据集，在跨域与小样本条件下显著提升电弧事件检测效果。


<details>
  <summary>Details</summary>
Motivation: 受电弓-接触网界面电弧会加剧磨损、降级性能并致服务中断，但电弧短暂、环境噪声大、真实标注稀缺、且与其他瞬态现象相似，现有方法难以稳定检测。

Method: 1) 构建两套多模态数据集：SBB实测（同步图像+力）与公开电弧视频配合合成力数据；2) 设计MultiDeepSAD：将DeepSAD扩展到多模态，并提出新的多模态损失；3) 面向不同模态的伪异常生成：图像中注入类电弧伪迹，力信号中模拟不规则波动；4) 通过多实验与消融验证。

Result: 在多种基线之上取得显著性能提升，尤其在真实电弧稀缺与跨域（domain shift）条件下对电弧更敏感、鲁棒性更强。

Conclusion: 多模态融合+伪异常增强+改进DeepSAD可有效解决受电弓-接触网电弧检测中的数据稀缺与跨域鲁棒难题，为实际铁路运维提供更可靠的在线检测方案。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [188] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: MOVA 是一个开源的多模态生成模型，直接从图像+文本生成同步的视频与音频（口型对齐语音、环境音效、配乐），采用32B参数的MoE架构（推理激活18B），提供高效推理、LoRA微调与提示增强工具，旨在降低级联管线的成本与误差并推动社区研究。


<details>
  <summary>Details</summary>
Motivation: 现有音视频生成多依赖级联流程，导致成本高、误差累积、质量下降；同时，强调“同步生成”的闭源系统（如Veo 3、Sora 2）限制了研究复现与改进。需要一个同时建模音频与视频、开放可复现、并能在真实场景中产生高质量对齐内容的模型。

Method: 提出MOVA（MOSS Video and Audio），支持IT2VA任务（图像+文本到视频+音频），采用Mixture-of-Experts（MoE）架构：总参数32B、推理时激活约18B。提供完整代码与权重，并在工程上支持高效推理、LoRA微调和提示工程/增强，以实现端到端联合生成和可扩展训练。

Result: MOVA 能生成高质量、时序同步的音视频内容，包括真实口型对齐的语音、环境自适应音效和与内容匹配的音乐，效果对比级联系统更一致、更鲁棒（摘要中未给出具体定量指标）。

Conclusion: 通过开源、MoE架构与端到端联合建模，MOVA 展示了同步音视频生成在质量与一致性上的优势，并为研究与创作社区提供了可复现的基础设施与可扩展的微调能力。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [189] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 提出一种半监督Teacher-Student脑肿瘤MRI分割框架：基于不确定性的伪标签+按置信度递进的课程学习；学生用双重损失在高置信区域学习、在低置信区域“反学习”，并配合一致性精炼。以少量标注即可获得接近全监督的性能，学生在子区域上部分超越教师。


<details>
  <summary>Details</summary>
Motivation: MRI脑肿瘤分割受限于昂贵标注和跨设备/站点异质性；现有半监督方法容易被噪声伪标签误导，且对样本与区域难度一视同仁，数据效率低。需一种能量化不确定性、分阶段引入难样本、并抑制错误监督的策略。

Method: 采用Teacher-Student框架：教师输出概率掩膜与像素级不确定性；对未标注扫描按图像级置信度排序，分阶段引入（课程学习）。学生以双重损失学习：对高置信区域强化监督，对低置信区域施加“去学习/反学习”以降低其影响；同时使用基于师生一致性的伪标签精炼以提升质量。

Result: 在BraTS 2021上：仅用10%标注时DSC为0.393，随数据增至100%达0.872，早期阶段收益最大；教师验证DSC 0.922。学生在肿瘤子区上超过教师：如NCR/NET 0.797、Edema 0.980；对教师失败的Enhancing类学生仍达0.620。

Conclusion: 基于置信度的课程学习与选择性反学习能在有限监督与噪声伪标签环境下实现稳健的脑肿瘤分割；该框架具备数据效率高、对难样本与噪声有韧性，学生可在关键子区域超越教师。

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [190] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2 将多模态大语言模型与视频扩散模型耦合，通过显式目标字幕与轻量适配器，实现统一的视频生成与编辑，在复杂组合编辑上显著提升，并在FiVE与VBench上表现领先/具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成/编辑模型难以准确理解复杂、组合式用户指令；直接用扩散模型条件往往语义不足、难以复用大模型的高层理解。需要一种方法把MLLM的理解与推理优势直接转化为可用于扩散生成的强条件，同时以参数高效方式复用强大的预训练文本到视频先验。

Method: 1) 用预训练MLLM解析用户指令并生成显式目标caption（含细粒度与多约束信息），作为生成/编辑的强语义条件；2) 设计轻量级适配器，将多模态条件token注入到预训练文本到视频扩散模型中，实现参数高效的条件控制与最大化复用生成先验；3) 将模型扩展至14B规模，在高质量精选数据上训练，覆盖文本生成与多类编辑（移除/添加/换背景/复杂运动编辑等）。

Result: 在FiVE细粒度视频编辑基准上展现更强的复杂组合指令跟随能力；在VBench文本到视频生成上达到有竞争力或更优的生成质量。

Conclusion: 通过把MLLM的理解与视频扩散的生成有效耦合，并以轻量适配器进行条件注入，Omni-Video 2 实现统一的视频生成与编辑框架，显著提升复杂编辑与总体生成质量与效率，展示了可扩展至大规模的潜力。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [191] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 提出一个统一的任意到全模态MRI合成基础模型，用对比表征与视觉-语言对齐实现跨模态稳健重建，在多中心数据上取得高保真与强泛化，并提升下游放疗相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 临床NPC放疗依赖MRI多模态信息，但受患者耐受性、时间与成本限制，常出现模态缺失，影响计划精度；现有方法模态特定、泛化差、缺乏可解释性，难以满足临床需求。

Method: 构建统一基础模型：1) 对比学习编码器学习模态不变的视觉表征；2) 基于CLIP的文本引导解码器进行语义一致的图像合成；支持任意输入模态到目标全模态的合成；在多机构大规模数据上训练与评估，并测试噪声与域移鲁棒性；检验统一表征对下游任务（如分割）的增益。

Result: 在13家机构40,825幅影像上训练，在26个内外部验证站点（15,748幅）上取得平均SSIM约0.90、PSNR约27的稳定表现，合成保真度优于对照方法，对噪声与域移更鲁棒；同时提升放疗相关下游任务性能。

Conclusion: 统一的对比学习与视觉-语言对齐驱动的基础模型可实现NPC任意到全模态MRI合成，兼具高保真与强泛化，并对临床下游放疗任务有实际助益，推动数字医疗在NPC中的应用。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [192] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 提出VideoVeritas框架，将细粒度感知与基于事实的推理结合，用PPRL强化感知能力，并发布MintVid数据集；在多基准上较现有方法更均衡、有效地检测生成视频与含事实错误的视频。


<details>
  <summary>Details</summary>
Motivation: 视频生成能力激增带来安全风险，现有MLLM虽善于推理但细粒度感知薄弱，现有检测方法容易偏向表层推理或机械分析，缺乏兼顾感知与推理的可靠检测方案与高质量评测数据。

Method: 提出VideoVeritas：将细粒度感知与事实推理融合。训练上引入“联合偏好对齐+感知前文本强化学习(PPRL)”，在RL阶段不直接优化检测目标，而是用通用时空指向(spatiotemporal grounding)与自监督目标计数两种简单感知前任务来增强感知；并构建MintVid数据集，含来自9个SOTA生成器的约3K视频及一部分真实采集但含事实错误的子集，用于稳健评测。

Result: 实验显示现有方法在推理或机械分析上各自偏颇，VideoVeritas在多样化基准上取得更均衡且更优的检测表现；MintVid支持更全面的评估。

Conclusion: 通过在RL阶段引入与检测相关的感知前任务，VideoVeritas显著提升MLLM的细粒度感知并与事实推理协同，达成对生成视频与含事实错误视频的更可靠检测；MintVid为该方向提供轻量且高质的评测基准。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [193] [FlattenGPT: Depth Compression for Transformer with Layer Flattening](https://arxiv.org/abs/2602.08858)
*Ruihan Xu,Qingpei Guo,Yao Zhu,Xiangyang Ji,Ming Yang,Shiliang Zhang*

Main category: cs.CV

TL;DR: 提出FlattenGPT，通过将相邻两层Transformer块“扁平化”为一层来进行深度压缩，同时在合并后进行更有效的参数冗余检测与裁剪，既减少深度又尽量保留各块知识，在多模型上以约20%压缩保留90–96%零样本性能并改进推理加速与困惑度。


<details>
  <summary>Details</summary>
Motivation: 整块剪枝易丢失被剪块中的关键信息导致性能大幅下滑；通道剪枝虽能较好保性能但无法减少深度且各层剪枝比例难以一致，限制了压缩与加速收益。需要一种既能减深度又能稳定保留知识与性能的压缩方法。

Method: 提出FlattenGPT：将相邻两个Transformer块结构性合并为一个“扁平化”块，在这一合并表示上进行参数冗余检测与移除。方法保持与原始Transformer架构一致（保留注意力与FFN语义），但通过合并使跨块冗余更显性，便于裁剪；由此实现深度压缩与更一致的冗余剔除。

Result: 在多种LLM（LLaMA-2/3、Qwen-1.5等）上，FlattenGPT在约20%压缩率下保留90–96%的零样本性能；在WikiText-2上困惑度更优；相较现有剪枝方法在零样本准确率与推理加速上均有优势。

Conclusion: 通过“块扁平化”同时解决深度压缩与冗余检测难题，较好地兼顾性能与效率，优于现有整块或通道剪枝方案，具有提升Transformer推理效率的潜力。

Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.

</details>


### [194] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出TiFRe：用文本引导的视频帧压减与融合框架，在显著降低算力开销的同时维持/提升视频语言任务表现。


<details>
  <summary>Details</summary>
Motivation: 视频MLLM计算代价高，尤其是注意力在大量帧上的开销；简单按固定FPS取关键帧会丢失非关键帧中的语义，导致性能下降，需要一种既能降帧又尽量保留语义的信息保留式采样与融合方法。

Method: 1) 文本引导帧采样（TFS）：将用户输入交给LLM生成CLIP风格提示词；用预训练CLIP编码器计算该提示与各帧的语义相似度，选取最相关帧为关键帧。2) 帧匹配与融合（FMM）：把非关键帧与所选关键帧进行匹配并融合，将非关键帧信息注入关键帧特征，从而在减少帧数的同时保留语义。

Result: 在多种视频语言任务上，TiFRe在更少输入帧与更低计算量条件下，较基线获得更好或不降的性能；显示其能有效缓解注意力计算开销并保持/提升准确率。

Conclusion: 文本引导的采样结合非关键帧信息的匹配融合可在不牺牲、甚至提升视频理解与问答性能的前提下降低视频MLLM的计算成本；为高效视频-语言建模提供通用前端。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [195] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 论文分析3D Gaussian Splatting在标准多视图优化下形成的“渲染最优参考”(ROR)结构，发现其尺度呈混合分布、辐射呈双峰，并揭示密度驱动的参数可学习性分化：稠密区可由点云无渲染重建，稀疏区需多视图约束。


<details>
  <summary>Details</summary>
Motivation: 3DGS在实践中效果好但其解空间与参数结构缺乏系统性理解。作者想回答：多视图优化自然会收敛到什么样的统计结构？哪些参数能在无渲染监督下由几何直接预测，哪些必须依赖渲染与可见性约束？

Method: 1) 定义并提取多场景的ROR解；2) 统计分析其尺度与辐射分布特性；3) 设计“可学习性探针”：用仅以点云为输入、无渲染监督的预测器重构ROR参数，比较稠密/稀疏区域表现；4) 方差分解与协方差分析，量化可见性异质性对几何-外观参数耦合的影响；5) 基于密度提出训练与架构策略。

Result: 发现稳健规律：尺度呈混合结构、辐射双峰。稠密区域参数与几何强相关，可被前馈模型准确预测；稀疏区域各架构系统性失败。方差分解显示在稀疏区，由于可见性异质性，几何与外观参数出现以协方差主导的强耦合，必须依赖多视图渲染约束。

Conclusion: ROR具有二元属性：在稠密区像“几何基元”，点云足够；在稀疏区像“视图合成基元”，需多视图优化。基于密度的训练与架构（自适应地在前馈预测与渲染精炼间切换）可提升鲁棒性，并对未来系统设计提出启示。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [196] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 提出一种用于植物生长时变外观建模的3D高斯流场表示，通过对高斯参数的时间导数建模，实现非线性、连续时间的生长与外观变化；先重建成熟植株并学习“逆向生长”以初始化足够的高斯原语，在多视角延时数据上优于现有方法，图像质量与几何精度更好。


<details>
  <summary>Details</summary>
Motivation: 植物在生长过程中会产生全新几何（扩展、分枝、分化），与仅有形变的常见动态场景不同。现有方法如形变场无法引入新几何，4D高斯splatting将运动限制为时空线性轨迹且无法跨时间跟踪同一组高斯，因而难以精准表示植物的时变结构与外观。

Method: 引入3D高斯流场：将植物生长表述为对高斯原语（位置、尺度、朝向、颜色、不透明度）的时间导数，从而得到可非线性、连续时间的动态。为获得足量原语，先重建成熟阶段的植物，再学习“逆向生长”过程以回溯生成早期结构，实现对发展历史的反演与初始化。

Result: 在多视角植物生长延时数据集上，本方法在重建图像质量与几何精度方面优于现有基线（如形变场与4D高斯splatting），能更稳定地追踪结构随时间的生成与变化。

Conclusion: 通过对高斯参数导数的时变建模与逆向生长初始化，可有效表示和渲染随时间产生新几何的植物生长过程，为可生长3D结构的外观建模提供了新的通用思路。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [197] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: MotionCrafter 是一种基于视频扩散模型的框架，从单目视频联合重建4D几何并估计稠密运动。其核心是将稠密3D点图与3D场景流在共享坐标系中进行联合表示，并通过新颖的4D VAE学习。通过新的数据归一化与VAE训练策略，无需将3D表示/潜变量与RGB VAE潜变量强行对齐，显著提升重建与运动估计效果，且无需事后优化，SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法常将3D几何与运动估计依赖于与RGB潜空间的强制对齐，但RGB与3D/流场的分布差异大，导致表示不匹配、重建质量和运动估计受限；同时在单目视频场景下，实现稳定的4D重建与稠密场景流估计仍具挑战，需要更有效地利用扩散先验并提升训练稳定性与泛化。

Method: 提出联合表示：在同一坐标系下同时表达稠密3D点图与3D场景流；设计4D VAE对该联合表示进行编码/解码；引入新数据归一化与VAE训练策略，以更好地迁移视频扩散先验，避免与RGB潜变量的硬对齐；整体框架基于视频扩散模型进行端到端学习，从单目视频直接预测4D几何与稠密流场；无需后处理/后优化。

Result: 在多个数据集上达到SOTA：几何重建提升约38.64%，稠密场景流（运动重建）提升约25.0%；同时具备更高的重建质量与运动一致性，推理阶段无需后期优化。

Conclusion: 强制与RGB潜空间对齐并非必要且有害；通过联合3D点图与场景流的共享坐标表示、配合4D VAE与归一化/训练策略，可有效利用扩散先验，显著提升单目视频的4D几何与稠密运动估计性能，并简化流程（无后优化）。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [198] [Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting](https://arxiv.org/abs/2602.08962)
*Guangxun Zhu,Xuan Liu,Nicolas Pugeault,Chongfeng Wei,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 提出一种基于车辆条件的3D行人姿态预测框架，利用增强的Waymo-3DSkelMo（加入对齐的3D车辆框）与新采样策略，改造TBIFormer并加入车辆编码器与跨注意力融合，实现行人-车辆交互建模，显著提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有行人运动/姿态预测多忽略或弱化周围车辆影响，而城市交通中车辆—行人的时空互动对行人未来运动至关重要；缺少含高质量车辆标注与可变交互复杂度的数据支持这一研究。

Method: 1) 扩展Waymo-3DSkelMo，加入时间对齐的3D车辆框；2) 设计按行人与车辆数量分层的场景采样策略，覆盖不同交互复杂度；3) 基于TBIFormer改造网络：新增车辆编码器；构建行人-车辆跨注意力模块，将历史行人特征与邻近车辆特征融合，实现车辆条件的3D姿态预测。

Result: 在扩展数据集上进行大量实验，车辆感知的融合带来显著的预测精度提升；比较多种交互建模方式，证明所提跨注意力与车辆编码器方案更优。

Conclusion: 面向自动驾驶，车辆感知的3D行人姿态预测至关重要。所提框架与数据扩展有效建模多主体行人-车辆交互，显著提升预测性能，并为后续研究提供可复现代码与基准。

Abstract: Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>


### [199] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: WorldArena提出一个统一基准，系统评估具身世界模型的“看得好”与“用得好”，并用新指标EWMScore把多维表现汇总为单一可解释分数；实验证明感知质量高并不等于任务功能强。


<details>
  <summary>Details</summary>
Motivation: 当前对具身世界模型的评估割裂，偏重视频/感知保真度，忽视其在下游决策与任务中的功能效用；缺乏统一、可比较、兼顾感知与功能的评测框架与聚合指标。

Method: 构建WorldArena基准，覆盖三大维度：1) 视频感知质量：16个指标、6个子维度；2) 具身任务功能性：把世界模型作为数据引擎、策略评估器、动作规划器，并引入主观人类评估；3) 提出EWMScore，将多维结果融合为单一可解释指数。对14个代表性模型做系统实验与对比，并提供公共排行榜。

Result: 在14个模型上的广泛实验显示显著的“感知—功能”鸿沟：高视觉质量的模型并非在具身任务上表现更强。WorldArena及其排行榜能清晰区分与量化各模型在感知与功能的表现差异。

Conclusion: 世界模型的评估应同时关注感知与功能；EWMScore与WorldArena提供了统一、可追踪的评测框架，有助于推动从“会看”走向“会用”的具身AI世界模型发展。

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [200] [Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study](https://arxiv.org/abs/2602.08996)
*Arushi Rai,Adriana Kovashka*

Main category: cs.CV

TL;DR: 论文针对视频LLM在体育动作反馈生成上的泛化差与评测失真两大痛点：用攀岩为目标域，结合可获取的网络视频与教练手册等辅助数据，以及来自其他运动的现有反馈数据，提升目标域反馈生成；并提出“具体性”“可执行性”两项新评测指标，促使反馈更有用。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM需要为每个运动收集昂贵且稀缺的微调反馈数据，且对未见运动泛化差；同时，通用文本评测指标无法衡量体育反馈的有效性与实用性。

Method: 以攀岩为案例：在目标域引入可自由获取的网络资源（比赛视频、教案/手册等）作为辅助知识，并与来源域（其他运动）的已存在反馈数据联合利用，以迁移/适配方式改善目标域反馈生成。同时，设计两项新评测指标：具体性（specificity）与可执行性（actionability），用于衡量反馈内容的细粒度与可操作程度。

Result: 在有限标注条件下，通过引入目标域网络数据与跨域反馈，模型在目标域（攀岩）反馈生成上取得显著提升；传统指标外，新指标能更好区分反馈质量与实用性。

Conclusion: 利用目标域免费网络数据进行跨域适配，可在标注稀缺时有效提升体育反馈生成并改善对未见运动的泛化；同时，针对性评测指标（具体性、可执行性）更能反映反馈的实际价值。

Abstract: While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.

</details>


### [201] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: ArcFlow 提出一种用于扩散模型少步蒸馏的非线性流动轨迹方法，通过将推理速度场参数化为连续动量过程的混合，并可解析积分，每步在少量函数评估下高精度逼近教师轨迹，实现2 NFE约40倍加速且质量几乎不降，只需微调<5%参数。


<details>
  <summary>Details</summary>
Motivation: 多步扩散推理代价高，现有少步蒸馏多用线性近似，难以匹配教师轨迹随时间变化的切向方向，导致质量下降；需要能刻画非线性速度演化、减少离散化误差的蒸馏框架。

Method: 提出ArcFlow：将推理轨迹的速度场参数化为“连续动量过程”的混合（可理解为具备惯性的速度演化），在每个去噪步内外推出连续、非线性的速度轨迹；该参数化允许对轨迹进行解析积分，避免数值离散误差；通过在预训练教师（如Qwen-Image-20B、FLUX.1-dev）上做轨迹蒸馏并以轻量适配器实现，仅微调少量参数以稳定、快速收敛并保持多样性。

Result: 在大模型上仅微调<5%参数即可用2次函数评估（2 NFE）达到约40×速度提升；在定性与定量基准上与多步教师相比几乎无显著质量损失，保持生成多样性与稳定性。

Conclusion: 用非线性、可解析的速度轨迹替代线性捷径能更好拟合教师扩散轨迹，显著减少推理步数与计算成本，同时维持生成质量；ArcFlow为扩散模型少步蒸馏提供了高精度、可扩展的解决方案。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [202] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出Raster2Seq，将平面图矢量化重建表述为序列到序列问题，用自回归解码器逐角预测多边形（房间、窗、门）并引入可学习锚点引导注意力，在多数据集上达SOTA并具强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从复杂、房间多、角点数变化大的栅格化平面图中忠实恢复结构与语义，限制了自动理解与CAD等下游应用。需要一种既能表达几何又能表达语义、且能灵活处理多样多边形结构的方案。

Method: 将重建转化为seq2seq：每个平面元素表示为带标签的多边形角点序列；采用自回归解码器，根据图像特征与已生成角点预测下一个角；引入可学习锚点（图像坐标）作为注意力引导，聚焦关键信息区域；整体输出灵活，能处理多房间与不定长多边形。

Result: 在Structure3D、CubiCasa5K、Raster2Graph等标准基准上取得SOTA，并在更具挑战的WAFFLE等数据集上表现出强泛化能力，对多样房型与复杂几何有鲁棒性。

Conclusion: 自回归、锚点引导的序列化表示有效统一几何与语义、提升复杂平面图重建精度与灵活性，为面向多样房型的自动理解与CAD流程提供更稳健的基础。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [203] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: WorldCompass 是面向长时序、交互式视频世界模型的RL后训练框架，通过在自回归视频生成中引入片段级多样化展开、互补奖励（交互跟随与视觉质量）以及高效的负样本感知微调策略，显著提升与指令/交互一致性和画面保真度，并在WorldPlay上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型在长时序交互场景中易出现探索偏差、奖励作弊与视觉退化，缺乏能稳定“驾驶”模型探索并兼顾交互正确性与视觉质量的后训练方法。

Method: 围绕自回归视频生成提出三点：1) 片段级展开：在目标clip处并行采样多条轨迹并局部评估，提升效率与细粒度监督；2) 互补奖励：同时对交互跟随准确性与视觉质量打分，抑制reward hacking；3) 高效RL：采用negative-aware微调与多项效率优化，稳定且高效提升容量。

Result: 在开源SOTA世界模型WorldPlay上，多个场景下的交互准确率与视觉保真度均显著提升。

Conclusion: 通过片段级展开、互补奖励与高效RL，WorldCompass能更准确、稳定地引导长时序交互式视频世界模型进行探索，兼顾行为一致性与生成质量，并在实际基座模型上证明有效。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [204] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: 论文指出视觉生成中离散方法与连续方法的差距主要源自潜空间比特预算（压缩率），而非离散分词器本质缺陷；通过增大码本可弥补差距。为解决大码本训练/采样困难，提出掩码比特自回归（BAR），以位为粒度逐步生成离散token，支持任意码本规模，在ImageNet-256上以gFID 0.99达SOTA，并降低采样成本、收敛更快。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉生成多依赖连续潜空间；社区普遍认为离散分词器先天不如连续表示。作者动机是：厘清性能差距真正成因，并使离散生成在扩大码本后既能稳定训练又能高效采样，从而挑战连续管线的统治地位。

Method: 提出BAR：在自回归Transformer上加入“掩码比特建模头”，将每个离散token分解为若干比特，采用掩码与逐步预测策略按位生成；该设计可线性扩展到任意码本规模，避免直接在超大词表上softmax导致的训练不稳与计算开销飙升。

Result: 实验显示：当增大码本（提升潜空间比特数）后，离散分词器与连续方法性能持平或更优；BAR在ImageNet-256上取得gFID 0.99的新SOTA，超越现有离散与连续范式，同时显著降低采样成本并较连续方法更快收敛。

Conclusion: 离散方法的主要瓶颈是潜空间比特预算不足而非表示劣势；通过以位为单位的可扩展自回归建模，离散生成可以在大码本设定下实现SOTA性能与更高效率，为视觉生成提供了可行、可扩展的离散替代方案。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>
