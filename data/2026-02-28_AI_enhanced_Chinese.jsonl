{"id": "2602.22819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22819", "abs": "https://arxiv.org/abs/2602.22819", "authors": ["Purbayan Kar", "Ayush Ghadiya", "Vishal Chudasama", "Pankaj Wasnik", "C. V. Jawahar"], "title": "Face Time Traveller : Travel Through Ages Without Losing Identity", "comment": "Accepted at CVPR 2026 (Findings Track)", "summary": "Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51faFaceTT\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5c5e\u6027\u611f\u77e5\u63d0\u793a\u3001\u65e0\u8c03\u53c2\u4e0e\u89d2\u53cd\u6f14\u3001\u81ea\u9002\u5e94\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u4e14\u4fdd\u8eab\u4efd\u7684\u4eba\u8138\u5e74\u9f84\u53d8\u6362\uff0c\u4f18\u4e8eSOTA\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u7528\u6570\u503c\u5e74\u9f84\u63a7\u5236\uff0c\u5ffd\u7565\u751f\u7269\u4e0e\u73af\u5883\u7ebf\u7d22\u7684\u4ea4\u4e92\uff1b\u5728\u5927\u8de8\u5ea6\u53d8\u9f84\u65f6\u8eab\u4efd\u6613\u6f02\u79fb\uff0c\u6ce8\u610f\u529b\u673a\u5236\u50f5\u5316\u3001\u6269\u6563\u53cd\u6f14\u8017\u65f6\u4e14\u96be\u4ee5\u9002\u914d\uff0c\u4e14\u96be\u4ee5\u7cbe\u7ec6\u63a7\u5236\u5e76\u4fdd\u6301\u80cc\u666f\u4e00\u81f4\u3002", "method": "1) Face-Attribute-Aware Prompt Refinement\uff1a\u5c06\u5185\u5728\uff08\u751f\u7269\uff09\u4e0e\u5916\u5728\uff08\u73af\u5883\uff09\u8001\u5316\u7ebf\u7d22\u7f16\u7801\u8fdb\u6587\u672c/\u6761\u4ef6\u63d0\u793a\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u63a7\u5236\uff1b2) Tuning-free Angular Inversion\uff1a\u65e0\u9700\u5fae\u8c03\uff0c\u57fa\u4e8e\u89d2\u5ea6\u5ea6\u91cf\u5c06\u771f\u5b9e\u4eba\u8138\u9ad8\u6548\u6620\u5c04\u5230\u6269\u6563\u6f5c\u7a7a\u95f4\uff0c\u5feb\u901f\u51c6\u786e\u91cd\u5efa\uff1b3) Adaptive Attention Control\uff1a\u52a8\u6001\u5e73\u8861\u8de8\u6ce8\u610f\u529b\uff08\u8bed\u4e49\u8001\u5316\u7ebf\u7d22\uff09\u4e0e\u81ea\u6ce8\u610f\u529b\uff08\u7ed3\u6784\u4e0e\u8eab\u4efd\u4fdd\u6301\uff09\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u4e0e\u80cc\u666f\u4e00\u81f4\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0ein-the-wild\u6d4b\u8bd5\u4e2d\uff0cFaceTT\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u80cc\u666f\u4fdd\u771f\u4e0e\u8001\u5316\u771f\u5b9e\u611f\u4e0a\u5747\u4f18\u4e8eSOTA\uff0c\u5c55\u73b0\u66f4\u9ad8\u7684\u89c6\u89c9\u8d28\u91cf\u4e0e\u7a33\u5b9a\u6027\u3002", "conclusion": "\u878d\u5408\u5c5e\u6027\u611f\u77e5\u6761\u4ef6\u3001\u65e0\u8c03\u53c2\u4e0e\u89d2\u53cd\u6f14\u53ca\u81ea\u9002\u5e94\u6ce8\u610f\u7684\u6269\u6563\u5f0fFaceTT\uff0c\u80fd\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u8eab\u4efd\u4e00\u81f4\u7684\u5e74\u9f84\u53d8\u6362\u5e76\u63d0\u5347\u9002\u914d\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u4e3a\u5b9e\u9645\u573a\u666f\uff08\u5a31\u4e50\u3001\u53d6\u8bc1\u3001\u6570\u5b57\u6863\u6848\uff09\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22932", "abs": "https://arxiv.org/abs/2602.22932", "authors": ["Wenhui Tan", "Xiaoyi Yu", "Jiaze Li", "Yijing Chen", "Jianzhong Ju", "Zhenbo Luo", "Ruihua Song", "Jian Luan"], "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding", "comment": "Accepted by CVPR2026", "summary": "Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\\% accuracy gain upon the base MLLM, and 1.1\\% higher accuracy than strongest baseline method.", "AI": {"tldr": "\u63d0\u51faMSJoE\u6846\u67b6\uff0c\u901a\u8fc7\u4e0e\u8f7b\u91cf\u5173\u952e\u5e27\u91c7\u6837\u5668\u8054\u5408\u8fdb\u5316\uff0c\u63d0\u5347MLLM\u5bf9\u957f\u89c6\u9891\u95ee\u7b54\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff1a\u7528\u591a\u89c6\u89d2\u6587\u672c\u67e5\u8be2\u4e0eCLIP\u76f8\u4e92\u4f5c\u7528\u751f\u6210\u76f8\u4f3c\u5ea6\u77e9\u9635\uff0c\u8f7b\u91cf\u91c7\u6837\u5668\u636e\u6b64\u9009\u53d6\u5c11\u91cf\u5173\u952e\u5e27\uff0c\u9001\u5165MLLM\u4f5c\u7b54\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\u67e5\u8be2\u63a8\u7406\u3001\u5e27\u91c7\u6837\u4e0e\u7406\u89e3\uff1b\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u957f\u89c6\u9891\u5305\u542b\u5927\u91cf\u5197\u4f59\u4fe1\u606f\uff0c\u73b0\u6709MLLM\u5904\u7406\u6210\u672c\u9ad8\u3001\u5173\u952e\u8bc1\u636e\u7a00\u758f\u4e14\u6613\u88ab\u5e73\u5747\u6c60\u5316\u6df9\u6ca1\uff1b\u9700\u8981\u5728\u4fdd\u8bc1\u8bed\u4e49\u8986\u76d6\u7684\u540c\u65f6\u6781\u5927\u964d\u4f4e\u8f93\u5165\u5e27\u6570\uff0c\u5e76\u8ba9\u6a21\u578b\u628a\u6ce8\u610f\u529b\u96c6\u4e2d\u5230\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u5173\u952e\u65f6\u523b\u3002", "method": "1) \u5047\u8bbe\u201c\u6bcf\u4e2a\u95ee\u9898\u4ec5\u9700\u5c11\u91cf\u5173\u952e\u5e27\u201d\u30022) \u7531MLLM\u5148\u751f\u6210\u591a\u6761\u591a\u6837\u5316\u201c\u67e5\u8be2\u201d\u4ee5\u8986\u76d6\u4e0d\u540c\u89c6\u89c9\u89c6\u89d2\u30023) \u8fd9\u4e9b\u67e5\u8be2\u4e0e\u51bb\u7ed3\u7684CLIP\u4ea4\u4e92\uff0c\u8ba1\u7b97\u67e5\u8be2-\u5e27\u76f8\u4f3c\u5ea6\u77e9\u9635\u30024) \u8f7b\u91cf\u91c7\u6837\u5668\u4ece\u76f8\u4f3c\u5ea6\u77e9\u9635\u9884\u6d4b\u6bcf\u5e27\u91c7\u6837\u6743\u91cd\uff0c\u6311\u9009\u7d27\u51d1\u5e27\u96c6\u30025) \u5c06\u9009\u4e2d\u5e27\u8f93\u5165MLLM\u751f\u6210\u7b54\u6848\u30026) \u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316MLLM\u4e0e\u91c7\u6837\u5668\uff0c\u4f7f\u67e5\u8be2\u751f\u6210\u3001\u5e27\u91c7\u6837\u4e0e\u5173\u952e\u5e27\u7406\u89e3\u534f\u540c\u8fdb\u5316\u30027) \u6784\u5efa\u5305\u542b2.8K\u89c6\u9891\u4e0e7K\u95ee\u7b54\u7684\u65b0\u957f\u89c6\u9891QA\u6570\u636e\u96c6\u4ee5\u652f\u6491\u8bad\u7ec3\u3002", "result": "\u5728VideoMME\u3001LongVideoBench\u3001LVBench\u3001MLVU\u7b49\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u57fa\u5e95MLLM\u51c6\u786e\u7387\u63d0\u53478.0%\uff1b\u8f83\u6700\u5f3a\u5bf9\u6bd4\u65b9\u6cd5\u518d\u63d0\u53471.1%\u3002", "conclusion": "\u8054\u5408\u8fdb\u5316\u7684\u201c\u67e5\u8be2-\u91c7\u6837-\u7406\u89e3\u201d\u95ed\u73af\u6709\u6548\u805a\u7126\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u5c11\u91cf\u5173\u952e\u5e27\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u7684\u540c\u65f6\u63d0\u5347\u957f\u89c6\u9891\u95ee\u7b54\u8868\u73b0\uff1bRL\u8bad\u7ec3\u4f7f\u91c7\u6837\u7b56\u7565\u4e0eMLLM\u80fd\u529b\u534f\u540c\u589e\u5f3a\uff0c\u4f18\u4e8e\u73b0\u6709\u957f\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2602.22938", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22938", "abs": "https://arxiv.org/abs/2602.22938", "authors": ["Shentong Mo", "Xufang Luo", "Dongsheng Li"], "title": "pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation", "comment": null, "summary": "Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.", "AI": {"tldr": "\u63d0\u51fapMoE\uff1a\u5c06\u591a\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\u901a\u8fc7Mixture\u2011of\u2011Experts\u5f0f\u63d0\u793a\u8c03\u4f18\u6574\u5408\uff0c\u501f\u52a9\u4e13\u5bb6\u4e13\u5c5eprompt\u4e0e\u53ef\u5b66\u4e60\u7684\u5206\u53d1\u5668\uff0c\u5728\u591a\u4efb\u52a1\uff08\u901a\u7528\u4e0e\u533b\u5b66\u3001\u5206\u7c7b\u4e0e\u5206\u5272\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u6548\u7387\u4e0e\u6548\u679c\u95f4\u53d6\u5f97\u66f4\u4f18\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u591a\u7528\u5355\u4e00\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u901a\u7528\u6216\u533b\u5b66\uff09\uff0c\u5ffd\u89c6\u591a\u9886\u57df\u77e5\u8bc6\u7684\u4e92\u8865\u6027\uff0c\u9650\u5236\u4e86\u5728\u8de8\u9886\u57df\u3001\u8de8\u4efb\u52a1\u9002\u914d\u65f6\u7684\u8868\u73b0\u4e0e\u6cdb\u5316\u3002", "method": "\u8bbe\u8ba1pMoE\uff1a1\uff09\u4e3a\u6bcf\u4e2a\u9886\u57df\u4e13\u5bb6\u5efa\u7acb\u4e13\u5bb6\u7279\u5b9a\u7684prompt token\u96c6\u5408\uff1b2\uff09\u5728\u591a\u4e2aprompt\u5c42\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u52a8\u6001token\u5206\u53d1\uff08dispatcher\uff09\uff0c\u6309\u8f93\u5165\u4e0e\u4efb\u52a1\u81ea\u9002\u5e94\u52a0\u6743\u8def\u7531\u5404\u4e13\u5bb6\u63d0\u793a\uff1b3\uff09\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u4e0d\u540c\u9886\u57df\u77e5\u8bc6\u5728\u9002\u914d\u8fc7\u7a0b\u4e2d\u6309\u9700\u8d21\u732e\u3002", "result": "\u572847\u4e2a\u9002\u914d\u4efb\u52a1\uff08\u8986\u76d6\u901a\u7528\u4e0e\u533b\u5b66\u3001\u5206\u7c7b\u4e0e\u5206\u5272\uff09\u4e0a\uff0cpMoE\u53d6\u5f97\u5927\u5e45\u6027\u80fd\u63d0\u5347\uff1b\u540c\u65f6\u5728\u8ba1\u7b97\u5f00\u9500\u4e0e\u9002\u914d\u6548\u679c\u4e4b\u95f4\u5b9e\u73b0\u66f4\u4f18\u6298\u4e2d\uff0c\u8d85\u8fc7\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u4e0e\u5355\u57dfprompt\u8c03\u4f18\u3002", "conclusion": "\u878d\u5408\u591a\u4e13\u5bb6\u9886\u57df\u63d0\u793a\u5e76\u8fdb\u884c\u52a8\u6001\u5206\u53d1\uff0c\u53ef\u663e\u8457\u63d0\u5347\u8de8\u57df\u591a\u4efb\u52a1\u9002\u914d\u7684\u6027\u80fd\u4e0e\u6548\u7387\uff1bpMoE\u4e3a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u8303\u5f0f\u3002"}}
{"id": "2602.22941", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22941", "abs": "https://arxiv.org/abs/2602.22941", "authors": ["Julian Ziegler", "Daniel Matthes", "Finn Gerdts", "Patrick Frenzel", "Torsten Warnke", "Matthias Englert", "Tina Koevari", "Mirco Fuchs"], "title": "Velocity and stroke rate reconstruction of canoe sprint team boats based on panned and zoomed video recordings", "comment": null, "summary": "Pacing strategies, defined by velocity and stroke rate profiles, are essential for peak performance in canoe sprint. While GPS is the gold standard for analysis, its limited availability necessitates automated video-based solutions. This paper presents an extended framework for reconstructing performance metrics from panned and zoomed video recordings across all sprint disciplines (K1-K4, C1-C2) and distances (200m-500m). Our method utilizes YOLOv8 for buoy and athlete detection, leveraging the known buoy grid to estimate homographies. We generalized the estimation of the boat position by means of learning a boat-specific athlete offset using a U-net based boat tip calibration. Further, we implement a robust tracking scheme using optical flow to adapt to multi-athlete boat types. Finally, we introduce methods to extract stroke rate information from either pose estimations or the athlete bounding boxes themselves. Evaluation against GPS data from elite competitions yields a velocity RRMSE of 0.020 +- 0.011 (rho = 0.956) and a stroke rate RRMSE of 0.022 +- 0.024 (rho = 0.932). The methods provide coaches with highly accurate, automated feedback without requiring on-boat sensors or manual annotation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u5e38\u89c4\u8f6c\u62cd/\u53d8\u7126\u6bd4\u8d5b\u89c6\u9891\u7684\u81ea\u52a8\u5316\u901f\u5ea6\u4e0e\u5212\u9891\u91cd\u5efa\u6846\u67b6\uff0c\u5728\u65e0GPS\u4e0e\u4eba\u5de5\u6807\u6ce8\u6761\u4ef6\u4e0b\u4e3a\u72ec\u6728/\u76ae\u8247\u5404\u7ec4\u522b\u4e0e\u8ddd\u79bb\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u8868\u73b0\u5206\u6790\u3002\u6838\u5fc3\u5305\u62ec\u6d6e\u6807\u4e0e\u8fd0\u52a8\u5458\u68c0\u6d4b\u3001\u57fa\u4e8e\u6d6e\u6807\u7f51\u683c\u7684\u5355\u5e94\u4f30\u8ba1\u3001U\u2011Net\u5b66\u4e60\u8239\u578b\u7279\u5f02\u7684\u8239\u5c16/\u8fd0\u52a8\u5458\u504f\u79fb\u6821\u51c6\u3001\u5149\u6d41\u9c81\u68d2\u591a\u8247\u8ddf\u8e2a\uff0c\u4ee5\u53ca\u4ece\u59ff\u6001\u6216\u5305\u56f4\u6846\u63d0\u53d6\u5212\u9891\u3002\u4e0e\u7cbe\u82f1\u8d5bGPS\u5bf9\u6bd4\uff1a\u901f\u5ea6RRMSE\u22480.020\u00b10.011\uff08\u03c1=0.956\uff09\uff0c\u5212\u9891RRMSE\u22480.022\u00b10.024\uff08\u03c1=0.932\uff09\u3002", "motivation": "GPS\u662f\u5212\u8247\u7ade\u901f\u8282\u594f\u5206\u6790\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u8bbe\u5907\u7a00\u7f3a\u3001\u88c5\u8f7d\u53d7\u9650\u4e14\u591a\u8247\u9879\u76ee\u90e8\u7f72\u56f0\u96be\uff1b\u73b0\u6709\u89c6\u9891\u65b9\u6cd5\u591a\u5c40\u9650\u5355\u8247/\u5b9a\u673a\u4f4d\uff0c\u96be\u4ee5\u5e94\u5bf9\u8f6c\u62cd\u4e0e\u53d8\u7126\u3002\u9700\u8981\u4e00\u79cd\u901a\u7528\u4e8e\u4e0d\u540c\u8247\u578b\u548c\u8d5b\u7a0b\u3001\u53ef\u4ece\u8d5b\u4e8b\u5b9e\u51b5\u89c6\u9891\u81ea\u52a8\u6062\u590d\u901f\u5ea6\u4e0e\u5212\u9891\u7684\u65b9\u6848\uff0c\u4e3a\u6559\u7ec3\u63d0\u4f9b\u65e0\u4f20\u611f\u5668\u7684\u8d5b\u540e/\u8d5b\u4e2d\u53cd\u9988\u3002", "method": "1) \u76ee\u6807\u68c0\u6d4b\uff1a\u7528YOLOv8\u68c0\u6d4b\u8d5b\u9053\u6d6e\u6807\u4e0e\u8fd0\u52a8\u5458/\u8247\u4f53\uff1b2) \u51e0\u4f55\u6807\u5b9a\uff1a\u5229\u7528\u5df2\u77e5\u6d6e\u6807\u7f51\u683c\u6c42\u53d6\u573a\u666f\u5355\u5e94\u77e9\u9635\uff0c\u7edf\u4e00\u5230\u8d5b\u9053\u5e73\u9762\u5750\u6807\uff1b3) \u8239\u4f4d\u4f30\u8ba1\uff1a\u63d0\u51faU\u2011Net\u8239\u5c16\u6821\u51c6\uff0c\u5b66\u4e60\u201c\u8fd0\u52a8\u5458\u68c0\u6d4b\u4e2d\u5fc3\u2192\u8239\u5c16\u201d\u7684\u8239\u578b\u7279\u5f02\u504f\u79fb\uff0c\u5f97\u5230\u66f4\u7cbe\u786e\u7684\u8247\u4f4d\uff1b4) \u8ddf\u8e2a\uff1a\u7ed3\u5408\u5149\u6d41\u7684\u9c81\u68d2\u8ddf\u8e2a\u7b56\u7565\uff0c\u9002\u914dK2/K4\u3001C2\u7b49\u591a\u8fd0\u52a8\u5458\u8247\u578b\uff1b5) \u5212\u9891\u63d0\u53d6\uff1a\u57fa\u4e8e\u59ff\u6001\u4f30\u8ba1\u7684\u5468\u671f\u4fe1\u53f7\u6216\u76f4\u63a5\u4ece\u5305\u56f4\u6846\u65f6\u5e8f\u632f\u8361\u63d0\u53d6\u5212\u9891\uff1b6) \u6307\u6807\u91cd\u5efa\uff1a\u7531\u6821\u51c6\u540e\u8f68\u8ff9\u6c42\u901f\u5ea6\u66f2\u7ebf\u4e0e\u914d\u901f\uff1b7) \u8bc4\u4f30\uff1a\u4e0e\u7cbe\u82f1\u8d5b\u4e8bGPS\u5bf9\u9f50\uff0c\u8ba1\u7b97RRMSE\u4e0eSpearman\u76f8\u5173\u3002", "result": "\u5728\u591a\u5b66\u79d1\u522b\u4e0e\u8ddd\u79bb\u4e0a\uff0c\u901f\u5ea6\u91cd\u5efa\u8fbe\u5230RRMSE 0.020\u00b10.011\u3001\u03c1=0.956\uff1b\u5212\u9891\u91cd\u5efaRRMSE 0.022\u00b10.024\u3001\u03c1=0.932\uff0c\u663e\u793a\u4e0eGPS\u9ad8\u5ea6\u4e00\u81f4\u3002\u65b9\u6cd5\u5728\u65e0\u8239\u8f7d\u4f20\u611f\u5668\u3001\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u4e0b\u7a33\u5b9a\u8fd0\u884c\u4e8e\u8f6c\u62cd/\u53d8\u7126\u5b9e\u51b5\u89c6\u9891\u3002", "conclusion": "\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u89c6\u9891\u91cd\u5efa\u6846\u67b6\u5728\u591a\u8247\u578b\u3001\u591a\u8ddd\u79bb\u4e0e\u590d\u6742\u6444\u5236\u6761\u4ef6\u4e0b\u5747\u80fd\u9ad8\u7cbe\u5ea6\u6062\u590d\u901f\u5ea6\u4e0e\u5212\u9891\uff0c\u53ef\u4e3a\u6559\u7ec3\u4e0e\u8fd0\u52a8\u5458\u63d0\u4f9b\u81ea\u52a8\u5316\u3001\u4f4e\u6210\u672c\u7684\u914d\u901f\u4e0e\u6280\u672f\u53cd\u9988\uff1b\u8239\u578b\u7279\u5f02\u504f\u79fb\u5b66\u4e60\u4e0e\u5149\u6d41\u8ddf\u8e2a\u662f\u9002\u914d\u591a\u8fd0\u52a8\u5458\u8247\u578b\u7684\u5173\u952e\u3002\u672a\u6765\u53ef\u6269\u5c55\u5230\u5b9e\u65f6\u5316\u3001\u8de8\u573a\u5730\u81ea\u9002\u5e94\u4e0e\u66f4\u5f3a\u7684\u906e\u6321\u5904\u7406\u3002"}}
{"id": "2602.22945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22945", "abs": "https://arxiv.org/abs/2602.22945", "authors": ["Kamal Sherawat", "Vikrant Bhati"], "title": "Cross-Task Benchmarking of CNN Architectures", "comment": null, "summary": "This project provides a comparative study of dynamic convolutional neural networks (CNNs) for various tasks, including image classification, segmentation, and time series analysis. Based on the ResNet-18 architecture, we compare five variants of CNNs: the vanilla CNN, the hard attention-based CNN, the soft attention-based CNN with local (pixel-wise) and global (image-wise) feature attention, and the omni-directional CNN (ODConv). Experiments on Tiny ImageNet, Pascal VOC, and the UCR Time Series Classification Archive illustrate that attention mechanisms and dynamic convolution methods consistently exceed conventional CNNs in accuracy, efficiency, and computational performance. ODConv was especially effective on morphologically complex images by being able to dynamically adjust to varying spatial patterns. Dynamic CNNs enhanced feature representation and cross-task generalization through adaptive kernel modulation. This project provides perspectives on advanced CNN design architecture for multiplexed data modalities and indicates promising directions in neural network engineering.", "AI": {"tldr": "\u6bd4\u8f83\u57fa\u4e8eResNet\u201118\u7684\u591a\u79cd\u52a8\u6001CNN\uff08\u786c/\u8f6f\u6ce8\u610f\u529b\u4e0eODConv\uff09\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u5206\u5272\u4e0e\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1b\u52a8\u6001\u673a\u5236\u6574\u4f53\u4f18\u4e8e\u4f20\u7edfCNN\uff0cODConv\u5728\u5f62\u6001\u590d\u6742\u56fe\u50cf\u4e0a\u5c24\u4f73\u3002", "motivation": "\u8bc4\u4f30\u5e76\u7edf\u4e00\u6bd4\u8f83\u4e0d\u540c\u52a8\u6001\u5377\u79ef\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u5728\u8de8\u6a21\u6001\u3001\u8de8\u4efb\u52a1\uff08\u56fe\u50cf\u4e0e\u65f6\u95f4\u5e8f\u5217\uff09\u4e2d\u7684\u6548\u679c\u4e0e\u6548\u7387\uff0c\u5bfb\u627e\u66f4\u901a\u7528\u3001\u66f4\u9ad8\u6548\u7684CNN\u8bbe\u8ba1\u3002", "method": "\u4ee5ResNet\u201118\u4e3a\u9aa8\u5e72\uff0c\u6784\u5efa\u4e94\u79cd\u53d8\u4f53\uff1a1) \u666e\u901aCNN\uff1b2) \u786c\u6ce8\u610f\u529bCNN\uff1b3) \u8f6f\u6ce8\u610f\u529bCNN\uff08\u5c40\u90e8\u50cf\u7d20\u7ea7\u4e0e\u5168\u5c40\u56fe\u50cf\u7ea7\u7279\u5f81\u6ce8\u610f\uff09\uff1b4) \u5168\u5411\u52a8\u6001\u5377\u79ef\uff08ODConv\uff09\u3002\u5728Tiny ImageNet\u3001Pascal VOC\u3001UCR\u65f6\u95f4\u5e8f\u5217\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u7cbe\u5ea6\u3001\u6548\u7387\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5206\u6790\u52a8\u6001\u6838\u8c03\u5236\u4e0e\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "result": "\u6ce8\u610f\u529b\u4e0e\u52a8\u6001\u5377\u79ef\u6574\u4f53\u5728\u51c6\u786e\u7387\u3001\u6548\u7387\u4e0e\u8ba1\u7b97\u6027\u80fd\u4e0a\u4f18\u4e8e\u666e\u901aCNN\uff1bODConv\u5bf9\u5f62\u6001\u590d\u6742\u56fe\u50cf\u9002\u5e94\u6027\u6700\u5f3a\uff1b\u52a8\u6001CNN\u901a\u8fc7\u81ea\u9002\u5e94\u6838\u8c03\u5236\u63d0\u5347\u7279\u5f81\u8868\u5f81\u4e0e\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "conclusion": "\u52a8\u6001\u4e0e\u6ce8\u610f\u529b\u9a71\u52a8\u7684CNN\u662f\u4f18\u4e8e\u4f20\u7edfCNN\u7684\u901a\u7528\u65b9\u6848\uff0c\u5c24\u5176ODConv\u5bf9\u7a7a\u95f4\u5f62\u6001\u53d8\u5316\u5177\u6709\u4f18\u52bf\uff1b\u4e3a\u591a\u6a21\u6001/\u591a\u4efb\u52a1CNN\u67b6\u6784\u63d0\u4f9b\u8bbe\u8ba1\u53c2\u8003\u5e76\u6307\u5411\u540e\u7eed\u795e\u7ecf\u7f51\u7edc\u5de5\u7a0b\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2602.23013", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23013", "abs": "https://arxiv.org/abs/2602.23013", "authors": ["Camile Lendering", "Erkut Akdag", "Egor Bondarev"], "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling", "comment": "Accepted to CVPR 2026", "summary": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.", "AI": {"tldr": "SubspaceAD\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5c11\u6837\u672c\u5de5\u4e1a\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff1a\u7528DINOv2\u63d0\u53d6\u8865\u4e01\u7279\u5f81\uff0cPCA\u62df\u5408\u6b63\u5e38\u5b50\u7a7a\u95f4\uff0c\u63a8\u7406\u65f6\u7528\u91cd\u5efa\u6b8b\u5dee\u4f5c\u4e3a\u5f02\u5e38\u5206\u6570\uff1b\u5728MVTec-AD\u4e0eVisA\u7684\u4e00\u6b21/\u5c11\u6837\u672c\u8bbe\u5b9a\u4e0b\u8fbe\u6210\u65b0\u7684SOTA\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u867d\u5229\u7528\u5927\u6a21\u578b\u7279\u5f81\u4f46\u5e38\u4f9d\u8d56\u590d\u6742\u7ec4\u4ef6\uff08\u8bb0\u5fc6\u5e93\u3001\u5916\u90e8\u8f85\u52a9\u6570\u636e\u3001\u591a\u6a21\u6001\u5fae\u8c03\uff09\uff0c\u5b9e\u73b0\u548c\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u4f5c\u8005\u8d28\u7591\u5728\u5f3a\u8868\u5f81\u4e0b\u8fd9\u4e9b\u590d\u6742\u6027\u662f\u5426\u5fc5\u8981\u3002", "method": "\u4e24\u9636\u6bb5\u3001\u514d\u8bad\u7ec3\uff1a1\uff09\u7528\u51bb\u7ed3\u7684DINOv2\u63d0\u53d6\u5c11\u91cf\u6b63\u5e38\u56fe\u50cf\u7684patch\u7ea7\u7279\u5f81\uff1b2\uff09\u5bf9\u8fd9\u4e9b\u7279\u5f81\u505aPCA\u4ee5\u4f30\u8ba1\u6b63\u5e38\u53d8\u5316\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u3002\u63a8\u7406\u65f6\u5c06\u6d4b\u8bd5\u7279\u5f81\u6295\u5f71\u5e76\u8ba1\u7b97\u91cd\u5efa\u6b8b\u5dee\uff0c\u5f97\u5230\u53ef\u89e3\u91ca\u3001\u5177\u7edf\u8ba1\u4f9d\u636e\u7684\u5f02\u5e38\u5206\u6570\uff1b\u540c\u65f6\u4ea7\u751f\u50cf\u7d20\u7ea7\u70ed\u529b\u56fe\u3002", "result": "\u5728\u4e00\u6b21\u6837\u672c\u8bbe\u5b9a\uff1aMVTec-AD\u56fe\u50cf\u7ea7/\u50cf\u7d20\u7ea7AUROC\u5206\u522b\u4e3a98.0%/97.6%\uff1bVisA\u4e3a93.3%/98.3%\uff0c\u5747\u8d85\u8fc7\u5148\u524dSOTA\u3002\u5c11\u6837\u672c\u573a\u666f\u4e5f\u4fdd\u6301\u9886\u5148\u3002", "conclusion": "\u7b80\u5355\u7684\u5b50\u7a7a\u95f4\u5efa\u6a21\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7279\u5f81\u5373\u53ef\u5728\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fbe\u5230SOTA\uff0c\u65e0\u9700\u8bad\u7ec3\u3001\u63d0\u793a\u8c03\u4f18\u6216\u8bb0\u5fc6\u5e93\uff0c\u65b9\u6cd5\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u90e8\u7f72\u3002"}}
{"id": "2602.23022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23022", "abs": "https://arxiv.org/abs/2602.23022", "authors": ["Xinglong Luo", "Ao Luo", "Zhengning Wang", "Yueqi Yang", "Chaoyu Feng", "Lei Lei", "Bing Zeng", "Shuaicheng Liu"], "title": "DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis", "comment": "Accepted by CVPR 2026", "summary": "Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.", "AI": {"tldr": "\u63d0\u51faDMAligner\uff1a\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9762\u5411\u5bf9\u9f50\u7684\u89c6\u56fe\u5408\u6210\u6765\u66ff\u4ee3\u5149\u6d41\u626d\u66f2\uff0c\u4ece\u800c\u66f4\u7a33\u5065\u5730\u5b8c\u6210\u56fe\u50cf\u5bf9\u9f50\uff0c\u7279\u522b\u5e94\u5bf9\u906e\u6321\u4e0e\u5149\u7167\u53d8\u5316\uff1b\u5e76\u6784\u5efaDSIA\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5bf9\u9f50\u591a\u4f9d\u8d56\u5149\u6d41\u4e0e\u56fe\u50cf\u626d\u66f2\uff0c\u6613\u53d7\u906e\u6321\u3001\u52a8\u6001\u524d\u666f\u4e0e\u5149\u7167\u53d8\u5316\u5f71\u54cd\uff0c\u5bfc\u81f4\u53ef\u89c6\u8d28\u91cf\u5dee\u5e76\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u6269\u6563\u5f0f\u5bf9\u9f50\u6846\u67b6DMAligner\uff1a\u901a\u8fc7\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u8fdb\u884c\u201c\u5bf9\u9f50\u5bfc\u5411\u7684\u89c6\u56fe\u5408\u6210\u201d\uff0c\u907f\u514d\u663e\u5f0f\u5149\u6d41\u626d\u66f2\u3002\u6838\u5fc3\u662f\u201c\u52a8\u6001\u611f\u77e5\u7684\u6269\u6563\u8bad\u7ec3\u201d\uff08Dynamics-aware Diffusion Training\uff09\uff0c\u5305\u542b\u52a8\u6001\u611f\u77e5\u63a9\u7801\u751f\u6210\u6a21\u5757DMP\uff0c\u4ee5\u81ea\u9002\u5e94\u533a\u5206\u52a8\u6001\u524d\u666f\u4e0e\u9759\u6001\u80cc\u666f\uff0c\u4f7f\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3/\u751f\u6210\u65f6\u5bf9\u52a8\u6001\u533a\u57df\u7279\u6b8a\u5904\u7406\uff0c\u4ece\u800c\u63d0\u5347\u5bf9\u9f50\u3002\u53e6\u6784\u5efa\u57fa\u4e8eBlender\u7684DSIA\u6570\u636e\u96c6\uff081033\u4e2a\u573a\u666f\uff0c3\u4e07+\u5bf9\u56fe\u50cf\uff09\u3002", "result": "\u5728\u81ea\u5efaDSIA\u57fa\u51c6\u53ca\u591a\u79cd\u5e38\u7528\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0cDMAligner\u53d6\u5f97\u66f4\u4f18\u7684\u53ef\u89c6\u5bf9\u9f50\u8d28\u91cf\u4e0e\u5b9a\u91cf\u6307\u6807\uff0c\u76f8\u6bd4\u5149\u6d41\u7c7b\u65b9\u6cd5\u5728\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\u4e0e\u52a8\u6001\u573a\u666f\u4e0b\u8868\u73b0\u663e\u8457\u66f4\u7a33\u5065\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u7684\u5bf9\u9f50\u5bfc\u5411\u89c6\u56fe\u5408\u6210\u53ef\u6709\u6548\u66ff\u4ee3\u4f20\u7edf\u6d41\u573a\u626d\u66f2\uff0c\u501f\u52a9\u52a8\u6001\u611f\u77e5\u8bad\u7ec3\u4e0e\u63a9\u7801\u673a\u5236\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u56fe\u50cf\u5bf9\u9f50\u6027\u80fd\uff1b\u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\u652f\u6301\u590d\u73b0\u4e0e\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.23029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23029", "abs": "https://arxiv.org/abs/2602.23029", "authors": ["Tianyue Wang", "Leigang Qu", "Tianyu Yang", "Xiangzhao Hao", "Yifan Xu", "Haiyun Guo", "Jinqiao Wang"], "title": "WISER: Wider Search, Deeper Thinking, and Adaptive Fusion for Training-Free Zero-Shot Composed Image Retrieval", "comment": null, "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a multimodal query (comprising a reference image and a modification text), without training on annotated triplets. Existing methods typically convert the multimodal query into a single modality-either as an edited caption for Text-to-Image retrieval (T2I) or as an edited image for Image-to-Image retrieval (I2I). However, each paradigm has inherent limitations: T2I often loses fine-grained visual details, while I2I struggles with complex semantic modifications. To effectively leverage their complementary strengths under diverse query intents, we propose WISER, a training-free framework that unifies T2I and I2I via a \"retrieve-verify-refine\" pipeline, explicitly modeling intent awareness and uncertainty awareness. Specifically, WISER first performs Wider Search by generating both edited captions and images for parallel retrieval to broaden the candidate pool. Then, it conducts Adaptive Fusion with a verifier to assess retrieval confidence, triggering refinement for uncertain retrievals, and dynamically fusing the dual-path for reliable ones. For uncertain retrievals, WISER generates refinement suggestions through structured self-reflection to guide the next retrieval round toward Deeper Thinking. Extensive experiments demonstrate that WISER significantly outperforms previous methods across multiple benchmarks, achieving relative improvements of 45% on CIRCO (mAP@5) and 57% on CIRR (Recall@1) over existing training-free methods. Notably, it even surpasses many training-dependent methods, highlighting its superiority and generalization under diverse scenarios. Code will be released at https://github.com/Physicsmile/WISER.", "AI": {"tldr": "\u63d0\u51faWISER\uff1a\u4e00\u4e2a\u96f6\u6837\u672c\u590d\u5408\u56fe\u50cf\u68c0\u7d22\u7684\u8bad\u7ec3-free\u6846\u67b6\uff0c\u7edf\u4e00T2I\u4e0eI2I\uff0c\u901a\u8fc7\u201c\u68c0\u7d22-\u9a8c\u8bc1-\u7cbe\u70bc\u201d\u5faa\u73af\u4e0e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u8d85\u8d8a\u591a\u6b3e\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "motivation": "ZS-CIR\u9700\u5728\u65e0\u6807\u6ce8\u4e09\u5143\u7ec4\u8bad\u7ec3\u4e0b\uff0c\u7528\uff08\u53c2\u8003\u56fe\u50cf+\u6587\u672c\u4fee\u6539\uff09\u68c0\u7d22\u76ee\u6807\u56fe\u50cf\u3002\u5355\u4e00\u8def\u5f84\u8f6c\u6362\u4e3aT2I\u6216I2I\u5404\u6709\u74f6\u9888\uff1aT2I\u6613\u4e22\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\uff0cI2I\u96be\u5904\u7406\u590d\u6742\u8bed\u4e49\u4fee\u6539\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6309\u67e5\u8be2\u610f\u56fe\u81ea\u9002\u5e94\u878d\u5408\u4e8c\u8005\u5e76\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51faWISER\u8bad\u7ec3-free\u6846\u67b6\uff1a1\uff09Wider Search\u5e76\u884c\u751f\u6210\u201c\u7f16\u8f91 caption\u201d\uff08T2I\uff09\u4e0e\u201c\u7f16\u8f91\u56fe\u50cf\u201d\uff08I2I\uff09\u4ee5\u6269\u5927\u5019\u9009\uff1b2\uff09Adaptive Fusion\u5229\u7528\u9a8c\u8bc1\u5668\u8bc4\u4f30\u68c0\u7d22\u7f6e\u4fe1\u5ea6\uff0c\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u7ed3\u679c\u8fdb\u884c\u52a8\u6001\u878d\u5408\uff0c\u5bf9\u4e0d\u786e\u5b9a\u7ed3\u679c\u89e6\u53d1\u7cbe\u70bc\uff1b3\uff09Deeper Thinking\u901a\u8fc7\u7ed3\u6784\u5316\u81ea\u53cd\u601d\u751f\u6210\u7cbe\u70bc\u5efa\u8bae\uff0c\u5f15\u5bfc\u4e0b\u4e00\u8f6e\u68c0\u7d22\u8fed\u4ee3\u3002\u6574\u4f53\u5b9e\u73b0\u201c\u68c0\u7d22-\u9a8c\u8bc1-\u7cbe\u70bc\u201d\u7684\u95ed\u73af\uff0c\u5e76\u663e\u5f0f\u5efa\u6a21\u201c\u610f\u56fe\u611f\u77e5+\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u201d\u3002", "result": "\u5728\u591a\u57fa\u51c6\u4e0a\u663e\u8457\u9886\u5148\uff1a\u76f8\u5bf9\u63d0\u5347CIRCO mAP@5\u8fbe45%\uff0cCIRR Recall@1\u8fbe57%\uff0c\u5e76\u8d85\u8fc7\u591a\u79cd\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f53\u73b0\u5f3a\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "WISER\u5728\u96f6\u6837\u672c\u590d\u5408\u68c0\u7d22\u4e2d\u6709\u6548\u8054\u5408T2I\u4e0eI2I\uff0c\u501f\u52a9\u5e76\u884c\u68c0\u7d22\u3001\u9a8c\u8bc1\u878d\u5408\u4e0e\u81ea\u53cd\u601d\u7cbe\u70bc\uff0c\u5f25\u8865\u4e24\u8def\u5f84\u4e92\u8865\u6027\u5e76\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u8fbe\u5230SOTA\u4e43\u81f3\u8d85\u8d8a\u8bad\u7ec3\u4f9d\u8d56\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2602.23103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23103", "abs": "https://arxiv.org/abs/2602.23103", "authors": ["Fuhao Zhang", "Lei Liu", "Jialin Zhang", "Ya-Nan Zhang", "Nan Mu"], "title": "SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation", "comment": null, "summary": "Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.", "AI": {"tldr": "SpectralMamba-UNet \u901a\u8fc7\u5728\u9891\u57df\u89e3\u8026\u4f4e/\u9ad8\u9891\u4fe1\u606f\uff0c\u628a\u5168\u5c40\u7ed3\u6784\u4ea4\u4e92\u4ea4\u7ed9\u9891\u57dfMamba\u3001\u628a\u8fb9\u754c\u7ec6\u8282\u4fdd\u7559\u5728\u9ad8\u9891\u652f\u8def\uff0c\u5e76\u7528\u901a\u9053\u9891\u7387\u91cd\u52a0\u6743\u4e0e\u9891\u8c31\u5f15\u5bfc\u89e3\u7801\u878d\u5408\uff0c\u63d0\u5347\u533b\u5b66\u5206\u5272\u7684\u7cbe\u5ea6\u4e0e\u6cdb\u5316\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Vision Mamba\uff09\u5584\u4e8e\u9ad8\u6548\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\uff0c\u4f46\u4e00\u7ef4\u5e8f\u5217\u5316\u4f1a\u524a\u5f31\u5c40\u90e8\u7a7a\u95f4\u8fde\u7eed\u6027\u4e0e\u9ad8\u9891\u7ec6\u8282\u8868\u8fbe\uff1b\u533b\u5b66\u56fe\u50cf\u5206\u5272\u540c\u65f6\u9700\u8981\u5168\u5c40\u89e3\u5256\u7ed3\u6784\u4e0e\u7ec6\u7c92\u5ea6\u8fb9\u754c\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e2\u4fdd\u5168\u9ad8\u9891\u3001\u53c8\u80fd\u505a\u5168\u5c40\u5efa\u6a21\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faSpectralMamba-UNet\uff1a\u5728\u7f16\u7801\u9636\u6bb5\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u5c06\u7279\u5f81\u5206\u89e3\u4e3a\u4f4e\u9891\u4e0e\u9ad8\u9891\uff1b\u4f4e\u9891\u652f\u8def\u8fdb\u5165\u9891\u57dfMamba\u8fdb\u884c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u9ad8\u9891\u652f\u8def\u7528\u4e8e\u8fb9\u754c\u654f\u611f\u7ec6\u8282\u4fdd\u7559\u3002\u8bbe\u8ba1\u5149\u8c31\u901a\u9053\u91cd\u52a0\u6743\uff08SCR\uff09\u5b9e\u73b0\u901a\u9053\u7ea7\u9891\u7387\u611f\u77e5\u6ce8\u610f\uff1b\u5728\u89e3\u7801\u7aef\u7528\u5149\u8c31\u5f15\u5bfc\u878d\u5408\uff08SGF\uff09\u81ea\u9002\u5e94\u5730\u8fdb\u884c\u591a\u5c3a\u5ea6\u878d\u5408\uff0c\u5e73\u8861\u4e0d\u540c\u9891\u6bb5\u8d21\u732e\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u533b\u5b66\u5206\u5272\u57fa\u51c6\uff08\u8de8\u6a21\u6001\u3001\u8de8\u76ee\u6807\uff09\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u663e\u793a\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u53ef\u6cdb\u5316\u6027\u3002", "conclusion": "\u9891\u57df\u89e3\u8026\u7ed3\u5408Mamba\u7684\u5168\u5c40\u5efa\u6a21\u4e0e\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u91cd\u52a0\u6743\u4e0e\u9891\u8c31\u5f15\u5bfc\u89e3\u7801\u878d\u5408\uff0c\u6709\u6548\u7f13\u89e3\u4e00\u7ef4\u5e8f\u5217\u5316\u5e26\u6765\u7684\u5c40\u90e8\u8fde\u7eed\u6027\u4e0e\u9ad8\u9891\u635f\u5931\u95ee\u9898\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u8868\u73b0\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.23114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23114", "abs": "https://arxiv.org/abs/2602.23114", "authors": ["Xudong Yan", "Songhe Feng", "Jiaxin Wang", "Xin Su", "Yi Jin"], "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "comment": null, "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u66f4\u65b0\u591a\u6a21\u6001\u539f\u578b\u7684\u65b9\u6cd5\uff08WARM-CAT\uff09\uff0c\u7f13\u89e3CZSL\u56e0\u6807\u7b7e\u7a7a\u95f4\u5206\u5e03\u8fc1\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316\uff1b\u901a\u8fc7\u65e0\u76d1\u7763\u6570\u636e\u7d2f\u79ef\u6587\u672c\u4e0e\u89c6\u89c9\u77e5\u8bc6\u3001\u52a8\u6001\u4f18\u5148\u961f\u5217\u4e0e\u81ea\u9002\u5e94\u66f4\u65b0\u6743\u91cd\u3001\u4ee5\u53ca\u591a\u6a21\u6001\u534f\u540c\u8868\u793a\u5bf9\u9f50\uff0c\u5b9e\u73b0\u95ed/\u5f00\u4e16\u754cSOTA\uff1b\u5e76\u53d1\u5e03\u65b0\u6570\u636e\u96c6C-Fashion\u4e0e\u7cbe\u70bc\u7248MIT-States\u3002", "motivation": "CZSL\u9700\u8981\u8bc6\u522b\u672a\u89c1\u8fc7\u7684\u5c5e\u6027-\u5bf9\u8c61\u7ec4\u5408\uff0c\u4f46\u6d4b\u8bd5\u65f6\u6807\u7b7e\u7a7a\u95f4\u5305\u542b\u7531\u5c5e\u6027\u4e0e\u5bf9\u8c61\u91cd\u7ec4\u800c\u6210\u7684\u5927\u91cf\u672a\u89c1\u7ec4\u5408\uff0c\u4ea7\u751f\u5206\u5e03\u8fc1\u79fb\uff0c\u4f20\u7edf\u9759\u6001\u539f\u578b\u6216\u56fa\u5b9a\u8bad\u7ec3\u5206\u5e03\u5047\u8bbe\u7684\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e0b\u6027\u80fd\u660e\u663e\u4e0b\u964d\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u4e0d\u4f9d\u8d56\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u4e8e\u6d4b\u8bd5\u9636\u6bb5\u5229\u7528\u65e0\u76d1\u7763\u6570\u636e\u6301\u7eed\u7d2f\u79ef\u77e5\u8bc6\u5e76\u8c03\u6574\u6a21\u578b\u8868\u5f81\uff0c\u4ece\u800c\u9002\u5e94\u6807\u7b7e\u5206\u5e03\u53d8\u5316\u5e76\u63d0\u5347\u6cdb\u5316\u3002", "method": "1) \u5728\u6d4b\u8bd5\u65f6\u4ece\u65e0\u76d1\u7763\u6570\u636e\u540c\u65f6\u6316\u6398\u6587\u672c\u4e0e\u89c6\u89c9\u4fe1\u606f\uff0c\u66f4\u65b0\u591a\u6a21\u6001\u539f\u578b\uff08textual/visual prototypes\uff09\u30022) \u8bbe\u8ba1\u81ea\u9002\u5e94\u66f4\u65b0\u6743\u91cd\uff0c\u6309\u6837\u672c\u7f6e\u4fe1\u4e0e\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u63a7\u5236\u539f\u578b\u8c03\u6574\u5e45\u5ea6\u30023) \u5f15\u5165\u52a8\u6001\u4f18\u5148\u961f\u5217\uff0c\u5b58\u50a8\u9ad8\u7f6e\u4fe1\u5ea6\u56fe\u50cf\u4ee5\u5f62\u6210\u5386\u53f2\u89c6\u89c9\u539f\u578b\u7528\u4e8e\u63a8\u7406\uff1b\u901a\u8fc7\u8bad\u7ec3\u96c6\u56fe\u50cf\u5bf9\u5df2\u89c1\u7ec4\u5408\u8fdb\u884c\u201cwarm-start\u201d\uff0c\u5e76\u5229\u7528\u5df2\u89c1\u4e0e\u672a\u89c1\u6587\u672c\u539f\u578b\u95f4\u7684\u6620\u5c04\u751f\u6210\u672a\u89c1\u89c6\u89c9\u539f\u578b\u30024) \u91c7\u7528\u591a\u6a21\u6001\u534f\u540c\u8868\u793a\u5b66\u4e60\u5bf9\u9f50\u6587\u672c\u4e0e\u89c6\u89c9\u539f\u578b\uff0c\u4fdd\u8bc1\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u8de8\u6a21\u6001\u53ef\u4ea4\u6362\u6027\u30025) \u63d0\u51fa\u65b0\u6570\u636e\u96c6C-Fashion\uff0c\u5e76\u6e05\u6d17MIT-States\u4ee5\u51cf\u5c11\u566a\u58f0\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\uff08\u542b\u95ed\u4e16\u754c\u4e0e\u5f00\u4e16\u754c\u8bbe\u7f6e\uff09\u4e0a\u53d6\u5f97SOTA\u8868\u73b0\uff1b\u52a8\u6001\u961f\u5217\u4e0e\u81ea\u9002\u5e94\u6743\u91cd\u5e26\u6765\u663e\u8457\u589e\u76ca\uff1b\u65b0\u6570\u636e\u96c6\u4e0e\u7cbe\u70bc\u6570\u636e\u96c6\u63d0\u5347\u4e86CZSL\u8bc4\u6d4b\u53ef\u9760\u6027\u3002\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u5f00\u6e90\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u591a\u6a21\u6001\u539f\u578b\u7684\u81ea\u9002\u5e94\u66f4\u65b0\u4e0e\u5386\u53f2\u8bb0\u5fc6\u673a\u5236\u53ef\u6709\u6548\u7f13\u89e3CZSL\u7684\u5206\u5e03\u8fc1\u79fb\u95ee\u9898\uff1b\u591a\u6a21\u6001\u5bf9\u9f50\u8fdb\u4e00\u6b65\u63d0\u5347\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002\u65b9\u6cd5\u901a\u7528\u3001\u65e0\u76d1\u7763\u53cb\u597d\uff0c\u5e76\u5728\u591a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u4f34\u968f\u66f4\u53ef\u9760\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002"}}
{"id": "2602.23117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23117", "abs": "https://arxiv.org/abs/2602.23117", "authors": ["Xiaosen Wang", "Zhijin Ge", "Bohan Liu", "Zheng Fang", "Fengfan Zhou", "Ruixuan Zhang", "Shaokang Wang", "Yuyang Luo"], "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation", "comment": "Code is available at https://github.com/Trustworthy-AI-Group/TransferAttack", "summary": "Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u8bc4\u6d4b\u6846\u67b6\u4e0e\u6807\u51c6\u4ee5\u516c\u6b63\u6bd4\u8f83\u5404\u7c7b\u8fc1\u79fb\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u8fc1\u79fb\u653b\u51fb\u7814\u7a76\u7e41\u591a\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u6d4b\u6846\u67b6\u4e0e\u6807\u51c6\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u6bd4\u3001\u7ed3\u8bba\u504f\u5dee\uff1b\u9700\u8981\u7cfb\u7edf\u5316\u68b3\u7406\u65b9\u6cd5\u7c7b\u522b\u3001\u5171\u6027\u7b56\u7565\u4e0e\u8bc4\u6d4b\u6ce8\u610f\u4e8b\u9879\u3002", "method": "1) \u7cfb\u7edf\u56de\u987e\u6570\u767e\u7bc7\u76f8\u5173\u5de5\u4f5c\uff1b2) \u5c06\u8fc1\u79fb\u5f0f\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5f52\u7eb3\u4e3a\u516d\u5927\u7c7b\uff1b3) \u63d0\u51fa\u4e00\u5957\u7528\u4e8e\u8bc4\u6d4b\u8fc1\u79fb\u653b\u51fb\u7684\u5168\u9762\u57fa\u51c6\u4e0e\u6d41\u7a0b\uff1b4) \u603b\u7ed3\u53ef\u63d0\u5347\u8fc1\u79fb\u6027\u7684\u901a\u7528\u7b56\u7565\uff1b5) \u6307\u51fa\u5e38\u89c1\u5bfc\u81f4\u4e0d\u516c\u5e73\u6bd4\u8f83\u7684\u95ee\u9898\uff1b6) \u7b80\u8ff0\u5728\u56fe\u50cf\u5206\u7c7b\u4ee5\u5916\u4efb\u52a1\u4e0a\u7684\u8fc1\u79fb\u653b\u51fb\u8fdb\u5c55\u3002", "result": "\u5f62\u6210\u4e86\u516d\u7c7b\u65b9\u6cd5\u7684\u6e05\u6670\u5206\u7c7b\u4f53\u7cfb\u4e0e\u672f\u8bed\uff0c\u7ed9\u51fa\u4e86\u53ef\u590d\u73b0\u5b9e\u9a8c\u4e0e\u8bc4\u6d4b\u57fa\u51c6\u6846\u67b6\uff0c\u63d0\u70bc\u51fa\u63d0\u5347\u8fc1\u79fb\u6027\u7684\u5b9e\u7528\u7b56\u7565\uff0c\u5e76\u63ed\u793a\u5f53\u524d\u6587\u732e\u4e2d\u7684\u8bc4\u6d4b\u504f\u5dee\u4e0e\u9677\u9631\uff1b\u540c\u65f6\u8865\u5145\u8de8\u4efb\u52a1\uff08\u8d85\u8d8a\u56fe\u50cf\u5206\u7c7b\uff09\u7684\u7b80\u8981\u7efc\u8ff0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8fc1\u79fb\u5f0f\u653b\u51fb\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u6d4b\u4e0e\u5206\u7c7b\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u516c\u5e73\u6bd4\u8f83\u4e0e\u590d\u73b0\u5b9e\u9a8c\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u5728\u66f4\u5e7f\u4efb\u52a1\u4e0a\u7684\u7814\u7a76\u4e0e\u65b9\u6cd5\u6539\u8fdb\u3002"}}
{"id": "2602.23120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23120", "abs": "https://arxiv.org/abs/2602.23120", "authors": ["Arian Sabaghi", "Jos\u00e9 Oramas"], "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement", "comment": "This paper consists of 8 pages including 6 figures. Accepted at CVPR 2026", "summary": "Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.", "AI": {"tldr": "TriLite \u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u3001\u5c11\u53c2\u6570\uff08<80\u4e07\uff09\u3001\u57fa\u4e8e\u51bb\u7ed3 Dinov2 \u81ea\u76d1\u7763 ViT \u7684\u5f31\u76d1\u7763\u76ee\u6807\u5b9a\u4f4d\uff08WSOL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u63d0\u51fa\u7684 TriHead \u5c06 patch \u8868\u793a\u5206\u89e3\u4e3a\u524d\u666f/\u80cc\u666f/\u6a21\u7cca\u4e09\u7c7b\uff0c\u4ece\u800c\u63d0\u5347\u76ee\u6807\u8986\u76d6\u5e76\u6291\u5236\u865a\u5047\u6fc0\u6d3b\uff0c\u5728 CUB-200-2011\u3001ImageNet-1K\u3001OpenImages \u4e0a\u8fbe\u5230\u65b0\u7684 SOTA\uff0c\u4e14\u8bad\u7ec3\u66f4\u7b80\u5355\u3001\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709 WSOL \u65b9\u6cd5\u5e38\u9700\u591a\u9636\u6bb5\u6d41\u7a0b\u6216\u5bf9\u5927\u89c4\u6a21\u9aa8\u5e72\u8fdb\u884c\u5168\u91cf\u5fae\u8c03\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u540c\u65f6\u6613\u51fa\u73b0\u53ea\u8986\u76d6\u76ee\u6807\u663e\u8457\u90e8\u4f4d\u3001\u5bf9\u8c61\u8986\u76d6\u4e0d\u5168\u7684\u95ee\u9898\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u76ee\u6807\u8986\u76d6\u5e76\u51cf\u5c11\u8bef\u68c0\u3002", "method": "\u51bb\u7ed3\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08Dinov2\uff09ViT \u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\uff0c\u65b0\u589e\u6781\u5c11\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u6784\u6210 TriLite\uff1a\u6838\u5fc3 TriHead \u6a21\u5757\u5c06 patch \u7279\u5f81\u4e09\u5206\u89e3\u4e3a\u524d\u666f\u3001\u80cc\u666f\u3001\u6a21\u7cca\u533a\u57df\uff0c\u901a\u8fc7\u89e3\u8026\u5206\u7c7b\u4e0e\u5b9a\u4f4d\u76ee\u6807\uff0c\u5206\u522b\u8fdb\u884c\u4f18\u5316\uff1b\u5229\u7528\u901a\u7528\u8868\u5f81\uff0c\u65e0\u9700\u7aef\u5230\u7aef\u5927\u89c4\u6a21\u5fae\u8c03\uff0c\u5b9e\u73b0\u5355\u9636\u6bb5\u8bad\u7ec3\u4e0e\u63a8\u7406\u3002", "result": "\u5728 CUB-200-2011\u3001ImageNet-1K\u3001OpenImages \u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cTriLite \u5728\u5206\u7c7b\u4e0e\u5b9a\u4f4d\u6307\u6807\u4e0a\u53d6\u5f97\u65b0\u7684 SOTA\uff0c\u540c\u65f6\u53c2\u6570\u6548\u7387\u663e\u8457\uff08<80\u4e07\uff09\u4e14\u8bad\u7ec3\u66f4\u6613\u6536\u655b\u3001\u6d41\u7a0b\u66f4\u7b80\u5355\u3002", "conclusion": "\u5229\u7528\u51bb\u7ed3\u7684\u81ea\u76d1\u7763 ViT \u4e0e\u5c11\u91cf\u53ef\u8bad\u7ec3\u5934\u90e8\u5e76\u89e3\u8026\u5206\u7c7b/\u5b9a\u4f4d\uff0c\u53ef\u4ee5\u5728\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347 WSOL \u7684\u76ee\u6807\u8986\u76d6\u4e0e\u9c81\u68d2\u6027\u3002TriLite \u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5355\u9636\u6bb5 WSOL \u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u5b9e\u7528\u524d\u666f\uff1b\u4ee3\u7801\u5373\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.23133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23133", "abs": "https://arxiv.org/abs/2602.23133", "authors": ["Xin Yuan", "Zhiyong Zhang", "Xin Xu", "Zheng Wang", "Chia-Wen Lin"], "title": "From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification", "comment": "Accepted by IEEE TMM 2026", "summary": "With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.", "AI": {"tldr": "\u63d0\u51faCARE\uff0c\u4e24\u9636\u6bb5\uff08\u6821\u51c6->\u7cbe\u70bc\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u6821\u51c6\u4e0e\u4f20\u64ad\u63d0\u5347\u5728\u542b\u566a\u4e14\u6bcf\u7c7b\u6837\u672c\u7a00\u758f\u7684\u4eba\u91cd\u8bc6\u522b\u9c81\u68d2\u6027\u3002\u7528PEC\u6253\u7834softmax\u5e73\u79fb\u4e0d\u53d8\u5e76\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\uff1b\u7528EPR\u7684CAM+COSW\u7cbe\u786e\u533a\u5206\u5e76\u52a0\u6743\u6837\u672c\uff0c\u4fdd\u7559\u96be\u6b63\u6837\u672c\u3001\u6291\u5236\u566a\u58f0\uff0c\u4e09\u5927\u6570\u636e\u96c6\u5728\u591a\u79cd\u566a\u58f0\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9eRe-ID\u6570\u636e\u5e38\u542b\u9519\u8bef\u6807\u6ce8\u4e0e\u6bcf\u4e2a\u8eab\u4efd\u6837\u672c\u5c11\uff0c\u73b0\u6709\u57fa\u4e8esoftmax\u7684\u635f\u5931\u6821\u6b63/\u5c0f\u635f\u6837\u672c\u9009\u62e9\u65b9\u6cd5\u4f1a\uff1a1) \u56e0softmax\u5e73\u79fb\u4e0d\u53d8\u5bfc\u81f4\u5bf9\u566a\u58f0\u6807\u7b7e\u8fc7\u5ea6\u81ea\u4fe1\uff1b2) \u5c0f\u635f\u51c6\u5219\u8bef\u5220\u5bf9\u5224\u522b\u6027\u5b66\u4e60\u5173\u952e\u7684\u201c\u96be\u6b63\u6837\u672c\u201d\u3002\u9700\u8981\u80fd\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\u5e76\u66f4\u51c6\u786e\u533a\u5206\u5e72\u51c0/\u566a\u58f0\u3001\u540c\u65f6\u4fdd\u7559\u96be\u6b63\u6837\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u4e24\u9636\u6bb5CARE\uff1a\n- \u6821\u51c6\uff08PEC\uff09\uff1a\u5728\u76f8\u4f3c\u5ea6\u51fd\u6570\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u6253\u7834softmax\u5e73\u79fb\u4e0d\u53d8\uff1b\u914d\u5408\u201c\u8bc1\u636e\u5f0f\u6821\u51c6\u635f\u5931\u201d\u964d\u4f4e\u5bf9\u9519\u6807\u6837\u672c\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3002\n- \u7cbe\u70bc\uff08EPR\uff09\uff1a\u57fa\u4e8e\u8bc1\u636e\u4f20\u64ad\u66f4\u51c6\u5730\u533a\u5206\u5e72\u51c0\u4e0e\u566a\u58f0\u6837\u672c\u3002\u5305\u542b\u4e24\u6b65\uff1a1) \u590d\u5408\u89d2\u5ea6\u95f4\u9694\uff08CAM\uff09\u5ea6\u91cf\uff0c\u5728\u8d85\u7403\u7a7a\u95f4\u533a\u5206\u5e72\u51c0\u4f46\u96be\u5b66\u7684\u6b63\u6837\u672c\u4e0e\u8bef\u6807\uff1b2) \u4ee5\u786e\u5b9a\u6027\u4e3a\u5bfc\u5411\u7684\u7403\u9762\u52a0\u6743\uff08COSW\uff09\uff0c\u4f9d\u636eCAM\u52a8\u6001\u5206\u914d\u6837\u672c\u6743\u91cd\uff0c\u8ba9\u5e72\u51c0\u6837\u672c\u4e3b\u5bfc\u66f4\u65b0\u3002", "result": "\u5728Market1501\u3001DukeMTMC-ReID\u3001CUHK03\u4e09\u5927\u6570\u636e\u96c6\u3001\u968f\u673a\u53ca\u6a21\u5f0f\u5316\u566a\u58f0\u8bbe\u5b9a\u4e0b\u53d6\u5f97\u6709\u7ade\u4e89\u529b\uff08SOTA\u7ea7\u522b\u6216\u63a5\u8fd1\uff09\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u5728\u566a\u58f0\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\u3002", "conclusion": "\u901a\u8fc7\u4ece\u6821\u51c6\u5230\u7cbe\u70bc\u7684\u8bc1\u636e\u4f20\u64ad\u8303\u5f0f\uff0cCARE\u7f13\u89e3softmax\u7684\u7ed3\u6784\u6027\u7f3a\u9677\u4e0e\u5c0f\u635f\u7b5b\u9009\u7684\u504f\u5dee\uff0c\u65e2\u6291\u5236\u9519\u8bef\u6807\u6ce8\u5e26\u6765\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u53c8\u4fdd\u7559\u96be\u6b63\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u542b\u566aRe-ID\u7684\u9c81\u68d2\u6027\u4e0e\u6027\u80fd\u3002"}}
