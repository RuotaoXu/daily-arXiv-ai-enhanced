<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility](https://arxiv.org/abs/2512.19711)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.CV

TL;DR: PHANTOM提出一种基于“变形（anamorphic）”物理对抗样本的黑盒攻击，可在特定视角下欺骗CAV目标检测器，并通过V2X放大为系统级通信与交通扰动。


<details>
  <summary>Details</summary>
Motivation: CAV依赖视觉DNN和低时延V2X，但物理对抗攻击仍是薄弱环节；现有攻击多需白盒或对模型特定，难在真实道路与多模型间稳定迁移，且对车联网级影响缺乏系统评估。

Method: 利用“变形艺术”在地面/立面绘制几何畸变图案，使其从特定距离和视角（6–10m）在车载摄像头成像域呈现对检测器不友好的特征；黑盒优化生成图案，目标是跨模型迁移并在多环境下稳健误检/漏检；在CARLA进行多速度、天气、光照测试，并在SUMO-OMNeT++联仿真评估V2X层面连锁影响。

Result: 在YOLOv5、SSD、Faster R-CNN、RetinaNet上表现出强迁移性；最佳条件下攻击成功率>90%，退化环境下仍有60–80%；触发距离6–10m，留给规避时间不足；在联仿真中诱发错误紧急消息，经V2X传播使峰值信息时效（Peak AoI）升高68–89%，削弱安全关键通信。

Conclusion: PHANTOM揭示了CAV从感知到通信的系统性脆弱性：视角依赖的物理对抗样本可在黑盒、跨模型与复杂环境下起效，并通过V2X造成网络级退化，提示需要在感知稳健性、攻击检测与V2X协议抗操纵方面的协同防御。

Abstract: Connected autonomous vehicles (CAVs) rely on vision-based deep neural networks (DNNs) and low-latency (Vehicle-to-Everything) V2X communication to navigate safely and efficiently. Despite their advances, these systems remain vulnerable to physical adversarial attacks. In this paper, we introduce PHANTOM (PHysical ANamorphic Threats Obstructing connected vehicle Mobility), a novel framework for crafting and deploying perspective-dependent adversarial examples using \textit{anamorphic art}. PHANTOM exploits geometric distortions that appear natural to humans but are misclassified with high confidence by state-of-the-art object detectors. Unlike conventional attacks, PHANTOM operates in black-box settings without model access and demonstrates strong transferability across four diverse detector architectures (YOLOv5, SSD, Faster R-CNN, and RetinaNet). Comprehensive evaluation in CARLA across varying speeds, weather conditions, and lighting scenarios shows that PHANTOM achieves over 90\% attack success rate under optimal conditions and maintains 60-80\% effectiveness even in degraded environments. The attack activates within 6-10 meters of the target, providing insufficient time for safe maneuvering. Beyond individual vehicle deception, PHANTOM triggers network-wide disruption in CAV systems: SUMO-OMNeT++ co-simulation demonstrates that false emergency messages propagate through V2X links, increasing Peak Age of Information by 68-89\% and degrading safety-critical communication. These findings expose critical vulnerabilities in both perception and communication layers of CAV ecosystems.

</details>


### [2] [Generating the Past, Present and Future from a Motion-Blurred Image](https://arxiv.org/abs/2512.19817)
*SaiKiran Tedla,Kelly Zhu,Trevor Canham,Felix Taubner,Michael S. Brown,Kiriakos N. Kutulakos,David B. Lindell*

Main category: cs.CV

TL;DR: 论文提出利用预训练视频扩散模型，从单张运动模糊图像恢复“过去-现在-未来”的短视频，并重建复杂场景动态与相机/物体运动。方法在网络规模先验加持下，优于传统基于手工先验或专用网络的去模糊与视频预测，且可用于相机轨迹、动态3D等下游任务。


<details>
  <summary>Details</summary>
Motivation: 运动模糊虽降低清晰度，但携带曝光期间相机与场景运动信息。现有方法多依赖手工先验或小规模训练，难以处理复杂动态，且通常仅复原捕获瞬间，不涉及前后时刻。作者希望利用互联网规模视频先验，消解模糊反演歧义，恢复更丰富的时间轴信息。

Method: 将大规模预训练视频扩散模型“再利用/重定向”到单帧运动模糊输入：将模糊图像作为条件，引导扩散生成与之相一致的时间序列。通过设计条件化与约束，使生成视频既解释模糊产生的轨迹与强度，又能向前/向后外推短时动态。并提供从生成结果估计相机运动、物体运动与动态3D结构的流程。

Result: 方法在合成与真实“in-the-wild”模糊图像上优于现有技术；可稳定生成反映曝光期动态及其前后片段的视频。展示了鲁棒泛化能力，并能从生成视频中恢复相机轨迹、对象运动与动态3D结构。

Conclusion: 借助互联网规模视频扩散先验，可将单张运动模糊图像转化为描述过去-现在-未来的短视频，显著提升对复杂场景动态的恢复能力，且支持多种下游几何与运动估计任务。

Abstract: We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io

</details>


### [3] [Learning to Refocus with Video Diffusion Models](https://arxiv.org/abs/2512.19823)
*SaiKiran Tedla,Zhoutong Zhang,Xuaner Zhang,Shumian Xin*

Main category: cs.CV

TL;DR: 提出一种基于视频扩散模型的单张失焦图像重聚焦方法，可生成感知准确的焦点序列（焦平面视频），支持交互式后期对焦，性能与鲁棒性优于现有方法，并公开大规模智能手机实拍焦栈数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 自动对焦常无法准确抓取意图主体，用户也希望拍后调整对焦；现有后期对焦方法在真实感与鲁棒性方面不足，缺少多样的真实数据支撑。

Method: 将“焦栈”建模为视频序列，使用视频扩散模型从单张失焦图像生成多帧不同焦平面的图像，形成感知一致的焦栈；依此实现交互式重聚焦，并在多样化真实智能手机条件下训练/评估；同时发布大规模焦栈数据集。

Result: 在多种具有挑战性的场景中，相比现有方法在感知质量与鲁棒性上均取得一致领先，能够生成可信的焦栈并支持多种下游应用。

Conclusion: 视频扩散驱动的焦栈生成使得从单张失焦图像实现高质量后期对焦成为可能，配套数据集推动该方向发展，为日常摄影提供更强大的焦点编辑能力。

Abstract: Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io

</details>


### [4] [RANSAC Scoring Functions: Analysis and Reality Check](https://arxiv.org/abs/2512.19850)
*A. Shekhovtsov*

Main category: cs.CV

TL;DR: 论文重新审视RANSAC中候选几何模型的打分函数，统一了高斯/球面噪声下的极大似然与鲁棒M估计视角，澄清并质疑MAGSAC++的理论推导，提出公平评测方法并发现各类打分在实践中表现几乎无差异。


<details>
  <summary>Details</summary>
Motivation: RANSAC的核心是对模型好坏进行打分。传统“几何误差”基于高斯噪声的金标准，但现实中存在球面噪声与离群点。现有最优基准（MAGSAC++）取得最好结果却采用不同假设，其理论是否严谨和值得推广需要系统检验。

Method: 1) 将高斯噪声下的几何误差扩展到球面噪声；2) 在鲁棒情形中构建高斯/球面与均匀离群的混合模型，基于阈值参数化统一极大似然、鲁棒M估计及其局部优化；3) 解析MAGSAC++的假设与推导，显示其数值等价于简单高斯-均匀似然；4) 提出两种评测方案：大规模验证集，或期望意义上的小随机验证集。

Result: 理论上给出统一的概率模型与阈值化视角，将多种打分方法纳入同一框架；证明MAGSAC++推导不符合严谨原则且其打分与基础高斯-均匀模型数值等价；实验上在不同评测方案下发现所有打分（包括学习到的内点分布）表现几乎一致，对阈值超参同样敏感。

Conclusion: 现有最先进打分（如MAGSAC++）并不优于简单的高斯-均匀似然基线；统一框架与评测方法澄清了理论与实践差异，为未来改进鲁棒拟合打分或迁移到其他问题提供可靠基础。

Abstract: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.

</details>


### [5] [HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction](https://arxiv.org/abs/2512.19871)
*Jong Wook Kim,Wonseok Roh,Ha Dam Baek,Pilhyeon Lee,Jonghyun Choi,Sangpil Kim*

Main category: cs.CV

TL;DR: 提出HyGE-Occ，用混合视角变换与3D高斯+边缘先验，提升3D全景占据预测的几何一致性与边界分割能力，在Occ3D-nuScenes上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D全景占据方法难以同时保持精确几何和实例空间范围，导致全景分割不稳、边界模糊，亟需既具几何一致性又具边界敏感的表征。

Method: 设计HyGE-Occ框架：1) 混合视角变换分支，将连续的高斯深度表示与离散深度bin融合，生成几何一致、结构连贯的BEV特征；2) 并行从BEV特征中提取边缘图，作为辅助监督/先验以学习边界线索；3) 利用3D高斯与边缘先验共同提升占据的语义与实例预测。

Result: 在Occ3D-nuScenes数据集上超越现有方法，展示更强的3D几何推理与全景占据性能（具体数值未给出）。

Conclusion: 混合高斯-离散深度表示结合边缘先验能显著提升3D全景占据的几何一致性与边界感知，带来更优的实例分离与整体性能。

Abstract: 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.

</details>


### [6] [Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs](https://arxiv.org/abs/2512.19918)
*Houston H. Zhang,Tao Zhang,Baoze Lin,Yuanqi Xue,Yincheng Zhu,Huan Liu,Li Gu,Linfeng Ye,Ziqiang Wang,Xinxin Zuo,Yang Wang,Yuanhao Yu,Zhixiang Chi*

Main category: cs.CV

TL;DR: 提出Widget2Code任务与基准，发现通用MLLM虽强但在小部件重建上不稳定；提供WidgetDSL+编译/渲染基础设施与感知-生成联合方法，大幅提升视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有UI2Code主要面向网页/移动端整页，忽视“微界面”组件（小部件）。小部件缺少层级上下文、空间极度紧凑、图标密集且没有公开标注或标记数据，导致现有方法与数据设定不适用，亟需新的问题定义、评测与方法。

Method: 1) 定义Widget2Code设定并构建仅图像输入的基准，提供细粒度、多维度评测指标。2) 基线模型：遵循小部件设计原则的“感知→组装”流程，将原子组件组合为完整布局，集成图标检索和可复用可视化模块。3) 系统层：提出WidgetFactory，包括与框架无关的WidgetDSL，以及可编译到多种前端（React、HTML/CSS）的编译器；并加入自适应渲染模块以精调空间尺寸满足紧凑性约束。

Result: 基准实验显示通用MLLM优于传统UI2Code但代码不稳定、视觉不一致；所提基线与WidgetFactory在视觉保真度上显著提升，成为强有力基准。

Conclusion: 小部件需要有别于整页UI的专门建模与基础设施。通过WidgetDSL+编译/渲染流水线与感知-结构化生成联合方法，可显著提升Widget2Code的可靠性与视觉一致性，为后续研究提供统一框架与强基线。

Abstract: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.

</details>


### [7] [Unified Brain Surface and Volume Registration](https://arxiv.org/abs/2512.19928)
*S. Mazdak Abulnaga,Andrew Hoopes,Malte Hoffmann,Robin Magnet,Maks Ovsjanikov,Lilla Zöllei,John Guttag,Bruce Fischl,Adrian Dalca*

Main category: cs.CV

TL;DR: NeurAlign提出一个统一体-表联合注册的深度学习框架，通过中间球坐标空间实现皮层表面与体积（皮质/皮下）的一致对齐，较传统与现有学习法在多数据集上显著提升Dice（最高+7），变形更规则、推理更快且仅需MRI输入。


<details>
  <summary>Details</summary>
Motivation: 现有脑MRI配准往往将体积与皮层表面注册分离，导致体-表不一致，影响跨被试分析的准确性与可重复性；亟需一种在同一框架下实现几何一致、解剖学准确且高效的联合注册方法。

Method: 构建端到端深度网络，在统一的体-表表示中同时对齐皮层与皮下；关键是引入中间球坐标空间，将皮层拓扑的球面映射与体积解剖联系起来，并将球面注册嵌入学习过程，保证体-表几何一致与变形规则性；无需额外输入，直接从3D MRI学习变形场。

Result: 在域内与跨域数据集上均优于经典与学习基线：Dice最高提升约7个百分点，同时保持规则（平滑、可逆性更好）的形变场；推理速度较标准方法快数量级；使用更简便（仅需MRI）。

Conclusion: NeurAlign通过球坐标桥接体积与表面，在准确性、几何一致性、速度与易用性上树立联合皮层-皮下注册的新基准，适用于更广泛的神经影像跨被试分析场景。

Abstract: Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.

</details>


### [8] [Vehicle-centric Perception via Multimodal Structured Pre-training](https://arxiv.org/abs/2512.19934)
*Wentao Wu,Xiao Wang,Chenglong Li,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 提出VehicleMAE‑V2：在掩码重建中引入车辆对称、轮廓与语义先验（SMM/CRM/SRM），并以Autobot4M大规模车辆数据预训练，显著提升车辆中心感知在多任务上的泛化与性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自监督/MAE类预训练对车辆知识学习不足，导致对车辆中心任务（监控、交通、自动驾驶）表征能力弱、泛化差，需要在预训练阶段显式注入与车辆相关的结构化先验。

Method: 构建车辆多模态结构化先验并嵌入掩码重建流程：1) SMM基于车辆对称性约束，避免保留对称冗余补丁，选更高价值掩码；2) CRM通过最小化轮廓特征与重建特征的分布差异，在像素级重建中保持整体车体结构；3) SRM以对比学习与跨模态蒸馏对齐图像-文本特征，缓解语义理解不足导致的特征混淆。并构建Autobot4M（约400万车辆图像+12,693文本）支撑预训练。

Result: 在五个下游车辆中心任务上取得优于现有方法的性能（文中称“显著/优越”），显示更强的可迁移与泛化能力。

Conclusion: 在MAE式预训练中显式引入车辆对称、轮廓与语义先验，并配合大规模车辆数据，可有效学习通用车辆中心表征；VehicleMAE‑V2在多任务上验证了其优势。

Abstract: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.

</details>


### [9] [Block-Recurrent Dynamics in Vision Transformers](https://arxiv.org/abs/2512.19941)
*Mozes Jacobs,Thomas Fel,Richard Hakim,Alessandra Brondetta,Demba Ba,T. Andy Keller*

Main category: cs.CV

TL;DR: 论文提出“块递归假设”（BRH）：训练好的ViT深度可分为少量可复用的计算阶段，原本L层的计算可被k≪L个共享块反复应用近似重写；并以Raptor模型给出强有力的实证支持。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT层堆叠常被直觉地联想到动态系统，但缺乏将Transformer深度解释为明确“流/相位”的统一框架。作者希望建立一种机制化解释，说明为何和如何深度中的计算具有可复用的阶段结构，从而降低解释复杂度并启发更高效的模型。

Method: 1) 用层间表征相似度矩阵（RSM）观察ViT沿深度的“相位”分段；2) 构造“块递归替身”Raptor：仅用k个共享块在深度上递归展开，拟合预训练ViT；3) 实证分析随机深度与训练对递归结构出现的促进作用；4) 大规模验证：以仅2块的Raptor在等计算预算下复现DINOv2在ImageNet-1k线性探针的96%性能；5) 基于该假设开展“动力学可解释性”研究，分析轨迹收敛、token特定动力学与低秩收敛等现象。

Result: - 多种ViT显示少量连贯的阶段；- Raptor能高精度近似原模型，尤其在有随机深度与恰当训练时；- 在DINOv2上，2块Raptor达成96%线性探针精度；- 动力学上观测到：i) 类依赖的角度盆地及小扰动下自纠轨迹；ii) cls在后期急剧重定向，而patch token在后期向其均值方向强一致；iii) 深层出现低秩更新坍缩，指向低维吸引子。

Conclusion: ViT深度中涌现出紧凑的块递归程序，说明其有效计算维度与复杂度远低于表面层数；这为以动力系统方法研究、压缩与解释ViT提供了统一视角，并可指导以少量共享块实现高效推理与分析。

Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.

</details>


### [10] [SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction](https://arxiv.org/abs/2512.19943)
*Haoyi Zhong,Fang-Lue Zhang,Andrew Chalmers,Taehyun Rhee*

Main category: cs.CV

TL;DR: SE360提出一个用于360度全景多条件引导的对象编辑框架，包含无人工参与的由粗到细数据生成与两阶段数据精炼，并训练Transformer扩散模型支持文本/掩码/参考图引导，实现在ERP与透视视图中更高质量与语义一致的编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑在360度全景中表现不佳，易产生几何失真、语义错配与透视/ERP视图中的不真实结果，缺乏高质量带语义与几何一致性的训练数据。

Method: 1) 提出自动化的粗到细数据生成流水线：利用VLM进行层级分析与语义理解，结合自适应投影调整，实现对象与物理上下文的整体化分割；从无标注全景中构造语义与几何一致的数据对。2) 两阶段低成本数据精炼：提升逼真度并缓解模型对擦除伪影的过拟合。3) 基于上述数据训练Transformer架构的扩散模型，支持文本、掩码或参考图多条件引导的对象编辑。

Result: 在定性与定量评估中，相比现有方法，在视觉质量与语义准确性上均有显著提升，且在ERP与透视视图中表现稳定。

Conclusion: 自动化、几何感知的数据构建与精炼结合Transformer扩散模型，可有效解决360度全景对象编辑中的语义与几何一致性问题，提供灵活多模态引导并优于现有方法。

Abstract: While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.

</details>


### [11] [How Much 3D Do Video Foundation Models Encode?](https://arxiv.org/abs/2512.19949)
*Zixuan Huang,Xiang Li,Zhaoyang Lv,James M. Rehg*

Main category: cs.CV

TL;DR: 作者提出首个与模型无关的评估框架，用浅层读出器从视频基础模型的特征中预测多种3D属性，以量化其3D理解能力。结果显示，先进的视频生成模型在未用3D数据训练的情况下也具备强烈的3D物体与场景理解，甚至可超过针对3D任务训练的专家模型，为规模化3D模型构建提供了参考。


<details>
  <summary>Details</summary>
Motivation: 视频是3D世界的2D连续投影，理论上大规模视频预训练可能蕴含3D线索。作者想回答：在不显式使用3D数据的前提下，VidFM是否会自然涌现全局3D理解？现有缺少统一、可比的量化手段。

Method: 提出模型无关的评测框架：固定预训练的VidFM，仅在其特征上训练浅层读出器，预测多种3D属性（如物体/场景几何、位姿、深度等），以此衡量模型的3D意识；并在多种主流VidFM（尤其视频生成模型）上进行系统基准测试与对比。

Result: 发现SOTA视频生成模型表现出强3D结构理解能力，即便未用3D标注或显式3D数据训练；在若干3D任务上，其读出性能可超过专为3D任务训练的大型专家模型。

Conclusion: 大规模视频预训练可自发学习到丰富的3D先验。所提评估框架与基准为后续VidFM与可扩展3D模型的设计与训练提供了实证依据与方向指引。

Abstract: Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.

</details>


### [12] [HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes](https://arxiv.org/abs/2512.19954)
*Yuechen Yang,Junlin Guo,Yanfan Zhu,Jialin Yue,Junchao Zhu,Yu Wang,Shilin Zhao,Haichun Yang,Xingyi Guo,Jovan Tanevski,Laura Barisoni,Avi Z. Rosenberg,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出HistoWAS：将GIS点格/点模式分析融入病理WSI，以拓展空间/拓扑特征，并用类PheWAS的大规模单变量回归连接组织空间结构与临床结局；在KPMP肾脏数据上验证与开源。


<details>
  <summary>Details</summary>
Motivation: 现有WSI分析多聚焦于单个对象或非空间汇总特征，难以量化微观—宏观层面的组织空间交互并系统评估其与临床参数的关联，限制生物标志物发现与临床转化。

Method: 构建HistoWAS框架：1）定义包含传统对象级特征与30个自GIS点模式分析改造的空间/拓扑特征的综合特征空间，用于刻画组织微结构；2）设计类PheWAS的关联引擎，对每个特征与临床表型进行批量单变量回归并做多重比较校正；在KPMP的385张PAS染色WSI（206例）上同时评估102个特征（72传统+30空间）。

Result: 在给定数据集上可同时量化并检验多种组织空间组织特征与临床结局/参数的统计关联，证明该框架的可行性与通用性；代码与数据公开以复现与扩展。

Conclusion: HistoWAS能系统化地连接组织空间结构与临床结果，弥补现有WSI分析的空间维度不足，为生物标志物发现与病理学定量化提供通用工具。

Abstract: High-throughput "pathomic" analysis of Whole Slide Images (WSIs) offers new opportunities to study tissue characteristics and for biomarker discovery. However, the clinical relevance of the tissue characteristics at the micro- and macro-environment level is limited by the lack of tools that facilitate the measurement of the spatial interaction of individual structure characteristics and their association with clinical parameters. To address these challenges, we introduce HistoWAS (Histology-Wide Association Study), a computational framework designed to link tissue spatial organization to clinical outcomes. Specifically, HistoWAS implements (1) a feature space that augments conventional metrics with 30 topological and spatial features, adapted from Geographic Information Systems (GIS) point pattern analysis, to quantify tissue micro-architecture; and (2) an association study engine, inspired by Phenome-Wide Association Studies (PheWAS), that performs mass univariate regression for each feature with statistical correction. As a proof of concept, we applied HistoWAS to analyze a total of 102 features (72 conventional object-level features and our 30 spatial features) using 385 PAS-stained WSIs from 206 participants in the Kidney Precision Medicine Project (KPMP). The code and data have been released to https://github.com/hrlblab/histoWAS.

</details>


### [13] [WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2512.19982)
*Le Feng,Li Xiao*

Main category: cs.CV

TL;DR: 提出WSD-MIL，将“窗口尺度递减注意力+SE区域门控”用于WSI中的MIL，兼顾大尺度肿瘤异质性与计算效率，在CAMELYON16与TCGA-BRCA上达SOTA且显著降内存（约62%）。


<details>
  <summary>Details</summary>
Motivation: 现有MIL多优化特征与聚合，忽视WSI实例间复杂语义关系；Transformer-MIL虽建模依赖，但二次复杂度难以扩展到超大WSI；且固定尺度注意力难适配不同WSI中肿瘤区域尺度差异与基于距离的相关性衰减。

Method: 提出WSD-MIL包含两模块：1）窗口尺度递减注意力模块：先基于聚类的采样策略降实例密度和计算，再逐步缩小注意力窗口尺度，引入距离衰减以在多尺度下捕捉局部相关性；2）基于SE的区域门控模块：对不同窗口/区域动态加权，以增强全局信息的整合与建模。

Result: 在CAMELYON16与TCGA-BRCA上取得SOTA性能，同时内存开销降低约62%。

Conclusion: 多尺度、距离衰减的局部依赖建模结合全局门控，有效提升WSI级MIL的准确率与可扩展性，适合大规模病理切片分析；代码将开源。

Abstract: In recent years, the integration of pre-trained foundational models with multiple instance learning (MIL) has improved diagnostic accuracy in computational pathology. However, existing MIL methods focus on optimizing feature extractors and aggregation strategies while overlooking the complex semantic relationships among instances within whole slide image (WSI). Although Transformer-based MIL approaches aiming to model instance dependencies, the quadratic computational complexity limits their scalability to large-scale WSIs. Moreover, due to the pronounced variations in tumor region scales across different WSIs, existing Transformer-based methods employing fixed-scale attention mechanisms face significant challenges in precisely capturing local instance correlations and fail to account for the distance-based decay effect of patch relevance. To address these challenges, we propose window scale decay MIL (WSD-MIL), designed to enhance the capacity to model tumor regions of varying scales while improving computational efficiency. WSD-MIL comprises: 1) a window scale decay based attention module, which employs a cluster-based sampling strategy to reduce computational costs while progressively decaying attention window-scale to capture local instance relationships at varying scales; and 2) a squeeze-and-excitation based region gate module, which dynamically adjusts window weights to enhance global information modeling. Experimental results demonstrate that WSD-MIL achieves state-of-the-art performance on the CAMELYON16 and TCGA-BRCA datasets while reducing 62% of the computational memory. The code will be publicly available.

</details>


### [14] [A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection](https://arxiv.org/abs/2512.19989)
*Tamim Ahasan Rijon,Yeasin Arafath*

Main category: cs.CV

TL;DR: 提出基于CNN与传统机器学习级联并与梯度提升集成的模型，在孟加拉国本地种植番石榴病害（健康/果蝇/炭疽）识别上据称达约99.99%准确度，适用于实时农业监测。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国番石榴是重要经济作物，果蝇与炭疽病显著影响产量与品质。需要早期、自动化的病害识别系统以减少损失、提升农业效益，尤其针对本地品种与真实田间环境。

Method: 构建GFDD24数据集（来自Rajshahi与Pabna的番石榴图像，三类：健康/果蝇/炭疽）。采用卷积神经网络提取特征，并与传统机器学习分类器进行级联（CNN-ML），再与梯度提升（GBM）进行集成，形成CNNML+GBM的集成模型，实现分类。强调可用于实时监测的高效推理。

Result: 在GFDD24数据上，所提集成模型报告约99.99%的最高分类准确率，表明其在三分类任务上的极高性能。

Conclusion: CNN-ML级联并与GBM集成的方法在本地番石榴病害识别上表现出色，适合部署到实时农业监测系统中，以保护收成并促进孟加拉农业发展。

Abstract: As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.

</details>


### [15] [A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping](https://arxiv.org/abs/2512.19990)
*Peng Gao,Ke Li,Di Wang,Yongshan Zhu,Yiming Zhang,Xuemei Luo,Yifeng Wang*

Main category: cs.CV

TL;DR: 提出DDTM，一个用于跨分辨率地物覆盖制图的弱监督框架，通过双分支（扩散精化+Transformer上下文）缓解粗标注与细粒度结构的不匹配，并配合伪标签置信评估，最终在Chesapeake Bay基准上以66.52% mIoU达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 跨分辨率监督下，高分辨率预测需要从低/粗分辨率标签学习，存在显著分辨率鸿沟：细粒度空间结构难与粗标签对齐，导致监督噪声高、精度受损。需要一种既能细化局部语义又能保持全局一致性，并能抑制伪标签噪声的方法。

Method: 设计双分支DDTM：1) 扩散分支（diffusion-based）在粗监督下逐步递进地细化局部高频语义；2) Transformer分支建模长程依赖，强制大范围上下文一致性。另提出伪标签置信度评估模块，过滤跨分辨率带来的不可靠监督，只利用高置信样本训练。

Result: 在Chesapeake Bay基准上取得66.52% mIoU，显著优于既有弱监督方法，刷新SOTA；实验广泛，验证了各组件（双分支与置信评估）的有效性。

Conclusion: 通过将局部细化与全局推理显式解耦，并引入置信控制的伪标签利用机制，DDTM有效克服跨分辨率监督的噪声与对齐难题，实现更准确的高分辨率地物制图；代码已开源。

Abstract: Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.

</details>


### [16] [Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models](https://arxiv.org/abs/2512.20000)
*Zhenhao Li,Shaohan Yi,Zheng Liu,Leonartinus Gao,Minh Ngoc Le,Ambrose Ling,Zhuoran Wang,Md Amirul Islam,Zhixiang Chi,Yuanhao Yu*

Main category: cs.CV

TL;DR: 提出MIVA：为预训练扩散模型外挂的轻量“动作适配器”，每个适配器学习一种运动模式，可并联组合，实现无需提示工程的图像到视频动画控制，用极少样本与消费级GPU即可训练，效果与大规模数据训练模型相当或更优。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像/视频生成逼真，但在图像动画任务上：1) 视频维度高、有效标注稀缺，模型易记忆训练集而不听从指令；2) 难以泛化到训练集中未见的新运动模式，用少量数据微调学习新动作尚缺探索。因此需要一种可数据高效、可扩展、可精确控制运动的方案。

Method: 提出模块化Image-to-Video Adapter（MIVA）：在预训练扩散模型上外挂一个极轻的子网络，每个MIVA专注学习单一运动模式；通过并联多个MIVA实现可组合的动作控制。训练上，每个MIVA可用约10个样本、单张消费级GPU高效训练；推理时用户直接选择一个或多个MIVA指定运动，无需复杂提示工程。

Result: 大量实验显示：MIVA在保持甚至提升生成质量的同时，显著提高运动控制的精确度；在极少样本训练条件下，达到或超过使用大规模数据训练的基线模型。

Conclusion: 模块化、可并联的轻量动作适配器使扩散模型在图像动画任务上实现数据高效学习与精确可控生成，为少样本学习新运动模式与用户友好的动作选择提供有效途径。

Abstract: Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.

</details>


### [17] [PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification](https://arxiv.org/abs/2512.20011)
*Blessing Agyei Kyem,Joshua Kofi Asamoah,Anthony Dontoh,Andrews Danyo,Eugene Denteh,Armstrong Aboah*

Main category: cs.CV

TL;DR: 该论文构建并发布了一个标准化、跨国家的路面病害检测基准数据集（5.27万张图、13类病害、13.5万框），统一标注与类别定义，并在多种SOTA检测器上验证其有效性，支持公平评测与零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 现有路面病害数据集在标注风格、病害类别定义与数据格式上不一致，导致难以合并训练、泛化能力差、跨场景评测不公平。需要一个标准化、规模大、跨地域与条件多样的基准数据集。

Method: 整合七个国家的多源公开数据，统一类别体系（13类），规范标注格式为边界框，清洗与对齐数据质量，并发布包含52747张图与135277个标注的标准化集合；随后用多种主流检测器（YOLOv8–YOLOv12、Faster R-CNN、DETR）进行基准测试，评估在不同成像条件、视角、分辨率、天气下的性能与迁移能力。

Result: 在该标准化数据集上，多种SOTA检测器均取得具有竞争力的性能；数据集展现良好的跨场景适用性，支持对多种模型进行统一、可对比的评测，并观察到对新环境的零样本迁移能力。

Conclusion: 该工作提供首个具有全球代表性的路面病害检测标准基准，通过统一类别与标注格式，促进公平比较与模型泛化研究，为实际应用中的跨域部署和后续方法改进提供基础。

Abstract: Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.

</details>


### [18] [SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images](https://arxiv.org/abs/2512.20013)
*Zepeng Xin,Kaiyu Li,Luodi Chen,Wanchen Li,Yuchen Xiao,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 提出LaSeRS数据集与SegEarth-R2模型，面向遥感场景的复杂语言引导分割，在层级粒度、多目标、推理、语言多样性四维度上系统评测与训练；通过空间注意力监督与灵活查询机制显著提升小目标与多目标分割性能，在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导遥感分割多为单目标、简单指令，无法应对层级粒度、多目标与隐式意图等复杂地理场景，导致真实应用敏感与脆弱；缺乏覆盖关键维度的大规模数据集与相匹配的模型架构。

Method: 1) 构建LaSeRS大规模数据集，从四个维度刻画复杂性：层级粒度、目标多重性、推理需求、语言变体；2) 提出SegEarth-R2多模态大模型架构：a) 空间注意力监督模块，强化定位小目标及其部件；b) 灵活高效的分割查询机制，统一处理单/多目标；在LaSeRS与其他基准上训练与评测。

Result: SegEarth-R2在LaSeRS及其他基准上取得优异（SOTA级）表现，成为复杂地理语义分割的强基线；能更好处理小目标、组件级别、以及多目标与复杂指令。

Conclusion: 面向复杂语言-像素对齐的遥感分割，LaSeRS提供系统化数据基准，SegEarth-R2通过空间注意力监督与灵活查询机制有效解决小目标与多目标场景，推动下一代地理分割研究；代码数据将开源。

Abstract: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.

</details>


### [19] [A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments](https://arxiv.org/abs/2512.20025)
*Anthony Dontoh,Stephanie Ivey,Armstrong Aboah*

Main category: cs.CV

TL;DR: 论文探讨在分心驾驶检测中将道路前向视角与驾驶员面部视角联合输入的价值，并在真实自然驾驶数据上对三种主流时空动作识别网络进行对比，发现多视角并非总是带来收益，收益取决于架构的融合设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法多仅使用驾驶员朝向视频，忽视道路环境上下文对驾驶行为的影响；作者想验证加入道路前向视角是否在自然驾驶条件下能提高分心检测性能，并量化不同架构在单视角与双视角输入下的表现差异。

Method: 采集同步双摄（驾驶员面部+道路前向）的真实驾驶数据；选取三种代表性时空动作识别模型（SlowFast-R50、X3D-M、SlowOnly-R50）；为每个模型设置两种输入配置：仅驾驶员视角与堆叠的双视角；在相同评测协议下对精度进行基准对比。

Result: 融合道路前向视角对不同架构影响显著：SlowOnly-R50 在双视角下精度提升约9.8%；X3D-M未给出具体数值（推断变化有限或依模型而定）；SlowFast-R50 在双视角下精度下降约7.2%，显示多路径特征流出现表征冲突。

Conclusion: 简单叠加视觉上下文并不保证收益，甚至可能因表征冲突导致性能下降；需要针对多视角/多模态的融合感知与架构设计（fusion-aware design）。该工作提供了在自然驾驶数据上的首批系统性单/双视角对比基线，为未来多模态驾驶员监测系统的架构设计提供参考。

Abstract: Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.

</details>


### [20] [MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis](https://arxiv.org/abs/2512.20026)
*Ziwei Qin,Xuhui Song,Deqing Huang,Na Qin,Jun Li*

Main category: cs.CV

TL;DR: 提出MAPI-GNN，通过从语义解耦子空间学习多激活平面图谱，并以关系融合引擎聚合，实现针对患者的动态关系建模，在两项任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医学诊断中的GNN多基于单一静态图，且用不加区分的特征构图，难以刻画患者特异的病理关系，限制诊断效果。

Method: 设计MAPI-GNN：1) 用多维判别器从多模态特征中挖掘潜在的“图感知”模式并实现语义解耦；2) 在这些模式指导下，为每个样本动态构建一组激活图（多激活平面）；3) 通过关系融合引擎对多图谱进行聚合与上下文化，输出鲁棒诊断。

Result: 在两个多样化任务、总计1300+患者样本上进行大量实验，MAPI-GNN显著优于当前最先进方法。

Conclusion: 多激活平面与语义解耦驱动的动态图构建，加上关系融合，可有效捕捉患者特异病理关系，提升多模态医学诊断性能。

Abstract: Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.

</details>


### [21] [$\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.20029)
*Lin Li,Jiahui Li,Jiaming Lei,Jun Xiao,Feifei Shao,Long Chen*

Main category: cs.CV

TL;DR: 提出H2em：在超曲空间学习层级嵌入的组合零样本学习方法，通过层级蕴含与判别对齐损失、以及超曲跨模态注意力，显著提升闭/开集CZSL表现。


<details>
  <summary>Details</summary>
Motivation: 现有CZSL忽视两类层级结构：原语（状态/对象）的语义层级与原语-组合的概念层级；用欧式空间正则化层级在大规模树状结构下失真严重，限制泛化。

Method: 在超曲几何中学习层级嵌入：1）Dual-Hierarchical Entailment Loss，基于超曲蕴含锥同时约束原语语义层级与原语-组合层级；2）Discriminative Alignment Loss + 硬负样本挖掘，拉大语义相近组合的测地距离以提高细粒度判别；3）Hyperbolic Cross-Modal Attention，实现实例级跨模态信息在超曲空间的融合与对齐。

Result: 在三个基准上进行大量消融与实验，在闭集与开集CZSL均达成新的SOTA；方法在大规模层级上保持低失真与更强泛化。

Conclusion: 超曲几何更适合表示CZSL所需的指数型层级结构；结合层级蕴含和判别对齐的H2em有效避免层级塌缩、提升细粒度区分与跨模态对齐，显著优于欧式基线。

Abstract: Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.

</details>


### [22] [VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement](https://arxiv.org/abs/2512.20032)
*Chang Sun,Dongliang Xie,Bo Qin,Hong Yang*

Main category: cs.CV

TL;DR: VALLR-Pin提出一个面向中文的两阶段视觉语音识别框架：多任务预测汉字与拼音，并用LLM在推理时结合拼音与候选文本消歧、再经合成噪声数据微调LLM以适配模型错误模式，从而提升中文读唇转写。


<details>
  <summary>Details</summary>
Motivation: 中文读唇比英文更难：唇形可区分度低（同形/近形音素多）且同音字/同音词普遍，易引发歧义与错写，需要结合显式的语音（拼音）与语言上下文来消歧。

Method: 1) 视觉编码器+双解码器：共享视频编码器同时输出汉字序列与标准拼音序列，多任务学习以增强视觉-语义表示；2) 推理阶段，文本解码器生成多条中文候选转写，将拼音与候选中文拼接构造提示词，输入LLM进行消歧与润色；3) 使用训练集通过中间检查点生成带错误的拼音-文本对，构造成“指令-回应”样本微调LLM，使其对特定错误模式更鲁棒。

Result: 在中文读唇任务上较基线（含原VALLR英语架构迁移）取得更佳的转写准确率，尤其在同音消歧与噪声场景下表现提升。（具体数值未在摘要中给出）

Conclusion: 通过结合视觉特征、拼音作为显式声韵信息与LLM的语言知识，并用合成噪声对齐模型误差进行特化微调，VALLR-Pin有效缓解中文读唇中的同音/同形歧义，提升转写质量。

Abstract: Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.

</details>


### [23] [FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs](https://arxiv.org/abs/2512.20033)
*Andreas Zinonos,Michał Stypułkowski,Antoni Bigata,Stavros Petridis,Maja Pantic,Nikita Drobyshev*

Main category: cs.CV

TL;DR: FlashLips 是一个两阶段、无需显式口部掩膜的实时对口型系统，100+ FPS，画质可比大型 SOTA。第一阶段用紧凑的一步潜空间编辑器进行图像重建；第二阶段用音频到唇部姿态的变换器预测控制向量。整体稳定、确定性强、推理快。


<details>
  <summary>Details</summary>
Motivation: 现有高质量唇同步多依赖GAN/扩散、显式口部掩膜与较大模型，难以兼顾高画质、稳定性与实时性。作者希望在保持或接近SOTA视觉质量的同时，将系统做得更小、更快、更稳定，并去除推理时对掩膜的依赖。

Method: 两阶段解耦：
1) 潜空间编辑重建器：输入参考身份图像、被遮挡/目标帧、低维唇部姿态向量，纯重建损失训练，无GAN/扩散。通过自监督生成“口部被修改”的伪真值图像进行微调，使模型学会仅在嘴部局部编辑、保持全局一致，从而在推理时不需显式掩膜。
2) 音频→姿态变换器：以flow-matching目标训练，从语音直接预测唇部姿态向量。
推理：先由阶段2得到姿态，再由阶段1进行确定性重建。

Result: 单GPU超过100 FPS的实时性能；视觉质量与更大SOTA方法相当；在无显式掩膜的设定下仍能将编辑局限在唇部并保持面部其余区域一致；管线稳定、训练与推理简洁。

Conclusion: 将“控制”（低维唇姿态）与“渲染”（潜空间重建）解耦，配合自监督无掩膜学习与flow-matching的音频驱动，可在小模型与确定性框架下实现高画质、鲁棒且超实时的唇同步。

Abstract: We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.

</details>


### [24] [Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva](https://arxiv.org/abs/2512.20042)
*Nguyen Lam Phu Quy,Pham Phu Hoa,Tran Chi Nguyen,Dao Sy Duy Minh,Nguyen Hoang Minh Ngoc,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 提出一个多模态管线，用外部文本知识增强图像字幕，通过相似图像检索、几何重排与语义检索获取上下文，再由微调的Qwen3融合，生成富含事件与背景信息的描述；在 OpenEvents v1 上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图像字幕往往缺乏事件背景、时间线、结果与不可见命名实体等上下文信息，限制了在新闻、教育与档案等需要深度理解的场景中的实用性。

Method: 1) 用 BEIT-3（Flickr30k-384/COCO-384）与 SigLIP So-384 进行相似图像检索；2) 用 ORB/SIFT 做几何一致性重排序；3) 对相关文章进行语义检索以抽取上下文；4) 以 InstructBLIP(Vicuna-7B) 生成基础字幕；5) 用 QLoRA 微调的 Qwen3 融合外部上下文与基础字幕，产出事件增强的描述。

Result: 在 OpenEvents v1 数据集上，生成的字幕显著更为信息丰富，相较传统方法表现更好（摘要未给出具体指标，但强调显著提升）。

Conclusion: 外部知识增强的多模态管线能有效提升图像字幕的事件与上下文丰富度，具备在需要深层视觉-文本理解的实际应用中的潜力。

Abstract: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding

</details>


### [25] [Progressive Learned Image Compression for Machine Perception](https://arxiv.org/abs/2512.20070)
*Jungwoo Kim,Jun-Hyuk Kim,Jong-Seok Lee*

Main category: cs.CV

TL;DR: 提出PICM-Net：面向机器感知的渐进式（FGS）学习图像压缩，通过trit-plane编码与自适应解码控制器，实现单比特流多质量级解码并在分类任务上高效传输与高性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习型图像编解码器多面向人类感知，且面向机器的编解码很少支持细粒度可伸缩的渐进式（FGS）压缩；需要兼顾机器任务在不同码率/质量下的鲁棒性能与传输效率，并能在实用场景中按需求动态解码。

Method: 1) 以trit-plane（3进制位平面）为核心的渐进式编码框架，产生单一比特流可在多质量级逐步解码；2) 分析人类与机器导向的率失真优先级差异，设计面向机器任务的潜变量优先传输/保留策略；3) 提出自适应解码控制器，在推理时根据下游模型的置信度动态决定解码层级，达到目标置信度即停止解码。

Result: 在下游图像分类任务上，实现更高的码率效率与任务性能平衡；在渐进式传输场景中可根据需要提前停止解码，显著降低带宽与延迟，同时保持高分类精度。

Conclusion: PICM-Net为机器感知导向的渐进式图像压缩提供了新的范式：单比特流、多质量级、任务驱动的自适应解码，在真实应用中实现高效与鲁棒的机器感知传输。

Abstract: Recent advances in learned image codecs have been extended from human perception toward machine perception. However, progressive image compression with fine granular scalability (FGS)-which enables decoding a single bitstream at multiple quality levels-remains unexplored for machine-oriented codecs. In this work, we propose a novel progressive learned image compression codec for machine perception, PICM-Net, based on trit-plane coding. By analyzing the difference between human- and machine-oriented rate-distortion priorities, we systematically examine the latent prioritization strategies in terms of machine-oriented codecs. To further enhance real-world adaptability, we design an adaptive decoding controller, which dynamically determines the necessary decoding level during inference time to maintain the desired confidence of downstream machine prediction. Extensive experiments demonstrate that our approach enables efficient and adaptive progressive transmission while maintaining high performance in the downstream classification task, establishing a new paradigm for machine-aware progressive image compression.

</details>


### [26] [Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts](https://arxiv.org/abs/2512.20088)
*Jinyoung Choi,Youngchae Kwon,Injung Kim*

Main category: cs.CV

TL;DR: 提出IRSN，通过物品区域池化与门控特征融合，并用双骨干改进特征提取，在两个数据集上将风格分类准确率平均提升约7%。


<details>
  <summary>Details</summary>
Motivation: 时尚风格分类受同类风格内在差异大与不同风格间视觉相似度高的双重挑战；风格不仅体现在整体外观，还体现在细粒度服饰部件及其组合关系，需要同时利用全局与局部（按服饰项）信息。

Method: 构建基于物品区域的风格分类网络IRSN：1）提出Item Region Pooling（IRP）在检测/分割到的服饰项区域上提取特征；2）对各项特征单独建模并通过Gated Feature Fusion（GFF）进行自适应加权组合，同时与全局特征融合；3）采用双骨干（domain-specific + 大规模图文预训练的一般骨干）以互补特征增强表示；4）可无缝适配多种主流骨干（EfficientNet、ConvNeXt、Swin等）。

Result: 在FashionStyle14上平均+6.9%、最高+14.5%；在ShowniqV3上平均+7.6%、最高+15.1%的准确率提升；可视化显示IRSN更能区分相似风格类。

Conclusion: 按服饰项的区域化特征建模并通过门控融合、双骨干互补，能显著提升风格分类，对相似风格区分尤为有效，且方法对多种骨干具有通用可移植性。

Abstract: Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.
  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.
  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.
  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).
  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.
  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.

</details>


### [27] [Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models](https://arxiv.org/abs/2512.20104)
*Subrata Kumer Paula,Dewan Nafiul Islam Noora,Rakhi Rani Paula,Md. Ekramul Hamidb,Fahmid Al Faridc,Hezerul Abdul Karimd,Md. Maruf Al Hossain Princee,Abu Saleh Musa Miahb*

Main category: cs.CV

TL;DR: 评估三种激活函数与四种优化器在BiLSTM与ConvLSTM上的组合对医疗相关HAR任务的影响；ConvLSTM+Adam/RMSprop在HMDB51与UCF101子集上达最高约99%准确率，BiLSTM跨数据集鲁棒性不足。


<details>
  <summary>Details</summary>
Motivation: 现有HAR多关注网络结构，较少系统分析激活函数与优化器及其交互对性能与鲁棒性的影响，尤其在医疗情境需要高准确与稳定的场景下。

Method: 选择六类与医疗相关的动作，从HMDB51与UCF101中抽取；采用两种时空递归架构（BiLSTM、ConvLSTM），分别组合ReLU/Sigmoid/Tanh与SGD/Adam/RMSprop/Adagrad；比较不同组合在两个数据集上的分类准确率与稳定性。

Result: ConvLSTM在两数据集上均优于BiLSTM；ConvLSTM+Adam或RMSprop可达约99.00%准确率且表现稳定；BiLSTM在UCF101接近98.00%，但在HMDB51仅约60.00%，对激活与优化器变化不敏感，跨数据集鲁棒性差。

Conclusion: 在医疗导向的HAR中，应优先采用具备时空卷积能力的ConvLSTM，并配合Adam或RMSprop；BiLSTM不适合作为跨数据集的稳健基线。研究为真实场景下快速、精准的动作识别提供了实践指引。

Abstract: Human Activity Recognition (HAR) plays a vital role in healthcare, surveillance, and innovative environments, where reliable action recognition supports timely decision-making and automation. Although deep learning-based HAR systems are widely adopted, the impact of Activation Functions (AFs) and Model Optimizers (MOs) on performance has not been sufficiently analyzed, particularly regarding how their combinations influence model behavior in practical scenarios. Most existing studies focus on architecture design, while the interaction between AF and MO choices remains relatively unexplored. In this work, we investigate the effect of three commonly used activation functions (ReLU, Sigmoid, and Tanh) combined with four optimization algorithms (SGD, Adam, RMSprop, and Adagrad) using two recurrent deep learning architectures, namely BiLSTM and ConvLSTM. Experiments are conducted on six medically relevant activity classes selected from the HMDB51 and UCF101 datasets, considering their suitability for healthcare-oriented HAR applications. Our experimental results show that ConvLSTM consistently outperforms BiLSTM across both datasets. ConvLSTM, combined with Adam or RMSprop, achieves an accuracy of up to 99.00%, demonstrating strong spatio-temporal learning capabilities and stable performance. While BiLSTM performs reasonably well on UCF101, with accuracy approaching 98.00%, its performance drops to approximately 60.00% on HMDB51, indicating limited robustness across datasets and weaker sensitivity to AF and MO variations. This study provides practical insights for optimizing HAR systems, particularly for real-world healthcare environments where fast and precise activity detection is critical.

</details>


### [28] [LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs](https://arxiv.org/abs/2512.20105)
*Haiyun Wei,Fan Lu,Yunwei Zhu,Zehan Zheng,Weiyi Xue,Lin Shao,Xudong Zhang,Ya Wu,Rong Fu,Guang Chen*

Main category: cs.CV

TL;DR: LiDARDraft 通过将文本/图像/点云统一到3D布局，再转为语义与深度控制，用rangemap版ControlNet像素级对齐，引导生成高质量、可控的激光雷达点云，实现从文本/图像/草图“从零到仿真”的生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法从简单控制信号（文本、草图等）生成复杂分布的LiDAR点云时质量与可控性受限，缺少在控制粒度与点云复杂性间的有效桥梁。

Method: 提出以3D布局为中介：将文本、图像、点云统一映射为3D布局，再转化为语义与深度控制信号；采用基于rangemap表示的ControlNet对点云生成进行像素级对齐与条件引导，从而支持多模态条件输入与细粒度控制。

Result: 在可控点云生成任务上取得显著效果，能从多种输入（文本、图像、草图）稳定生成高质量、多样化且与条件一致的LiDAR点云。

Conclusion: 3D布局作为通用中介有效平衡了控制信号与点云复杂度，配合rangemap-ControlNet实现强可控的LiDAR点云生成，为自动驾驶仿真“从零搭建”环境提供了实用方案。

Abstract: Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.

</details>


### [29] [UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis](https://arxiv.org/abs/2512.20107)
*Thanh-Tung Le,Tuan Pham,Tung Nguyen,Deying Kong,Xiaohui Xie,Stephan Mandt*

Main category: cs.CV

TL;DR: 提出一种混合式NVS框架：用双向Transformer融合多视图图像token与Plücker射线嵌入，生成共享潜表示；再用回归头渲染几何确定区域、用掩码自回归扩散头补全遮挡/未观测区域。端到端联合光度与扩散损失训练，在无手工3D先验下实现SOTA画质，并较纯生成式基线推理提速约10倍。


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法在观测区域快但对未观测区域易模糊；扩散式方法能合理臆测但训练与推理代价高。需要一种同时具备快速、清晰与对未观测区域的合理补全能力的统一框架。

Method: - 以双向Transformer为骨干，编码多视图图像token与Plücker射线嵌入，得到共享潜在表示。
- 在该表示上并行接两种轻量头：
  (i) 前馈回归头：对几何约束充分的像素直接预测颜色/密度，实现快速确定性渲染；
  (ii) 掩码自回归扩散头：对遮挡或未观测区域进行条件扩散补全。
- 端到端训练，联合光度重建损失与扩散损失，无需手工3D归纳偏置，适配多样场景。

Result: 在标准NVS基准上达到SOTA图像质量；与完全生成式扩散/自回归模型相比，渲染速度提升约一个数量级，同时保持对未观测区域的逼真补全能力。

Conclusion: 混合确定性回归与条件扩散的统一Transformer框架兼顾速度与质量，无需显式3D先验即可在多场景中扩展，成为高效高质的新视图合成方案。

Abstract: Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.

</details>


### [30] [Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection](https://arxiv.org/abs/2512.20113)
*Alireza Moayedikia,Sattar Dorafshan*

Main category: cs.CV

TL;DR: 提出一种融合地质雷达与红外热成像的多模态注意力网络用于桥面层间脱空检测，结合时间/空间注意力与跨模态融合，并量化不确定性；在多数据集上显著优于单模态与简单拼接，在极端类别不平衡下注意力易退化。


<details>
  <summary>Details</summary>
Motivation: 传统人工目视检查主观且低效；单一传感器（GPR或IRT）各有致命短板：GPR受含水率与浅层缺陷影响，IRT受天气与探测深度限制。需要一种能互补优势、适用于实际桥检、且能给出不确定性以支撑安全决策的自动化方法。

Method: 设计多模态注意力架构：对雷达序列引入时间注意力，对热像特征施加空间注意力；通过可学习嵌入实现跨模态注意力融合以挖掘互补缺陷模式。引入蒙特卡洛dropout与学习方差估计实现不确定性量化，并将不确定性分解为认知（epistemic）与数据内在（aleatoric）两类；包含多头机制与消融实验。

Result: 在五个桥梁数据集上，针对类别平衡到中度不平衡场景，方法在准确率和AUC上显著优于单模态与简单拼接的融合基线；跨模态注意力带来关键增益，多头机制提升校准度；不确定性估计降低校准误差，支持通过拒识高不确定样本实现选择性预测；但在极端类别不平衡时，注意力出现多数类坍塌的脆弱性。

Conclusion: 注意力驱动的多模态融合适用于常见桥面脱空检测场景，具备实时部署效率与较好校准；实际落地应结合不确定性阈值进行选择性预测，并在极端不平衡数据下配合专门的抗不平衡策略。

Abstract: Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.

</details>


### [31] [DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation](https://arxiv.org/abs/2512.20117)
*Jingqi Tian,Yiheng Du,Haoji Zhang,Yuji Wang,Isaac Ning Lee,Xulong Bai,Tianrui Zhu,Jingxuan Niu,Yansong Tang*

Main category: cs.CV

TL;DR: 提出DDAVS框架，通过“音频语义解耦+延迟式双向对齐”解决AVS中的多源缠绕与视听失配，基于原型记忆与对比学习提升判别性，并用延迟交互的双重交叉注意力稳健对齐，多基准全面领先。


<details>
  <summary>Details</summary>
Motivation: 现有AVS方法易被更响/更大目标主导，忽视弱小或共现声源，根因在于多源语义混杂与音视特征时间/语义对不齐，导致定位偏差与泛化差。

Method: 1) 音频语义解耦：用可学习query从音频中提取语义，并以“音频原型记忆库”构建结构化语义空间；结合对比学习拉近同类、拉远异类，增强可分性与鲁棒性。2) 延迟式双向对齐：设计双重交叉注意力并延迟模态交互，先各自稳固语义再对齐，缓解噪声/不同步带来的错配。

Result: 在AVS-Objects与VPO上，单源、多源、多实例场景均优于现有方法，表现稳定且泛化性强。

Conclusion: 通过语义解耦与延迟式对齐，DDAVS有效缓解多源缠绕与视听失配，在复杂真实场景下实现更精确、稳健的像素级声源定位。

Abstract: Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/

</details>


### [32] [HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer](https://arxiv.org/abs/2512.20120)
*Mohammad Helal Uddin,Liam Seymour,Sabur Baidya*

Main category: cs.CV

TL;DR: HEART-ViT提出一种基于Hessian的联合动态裁剪框架，同时对ViT的token与注意力头进行输入自适应剪枝，在保持精度的同时大幅降低FLOPs、时延与能耗，并在边缘设备上验证了实际加速。


<details>
  <summary>Details</summary>
Motivation: 现有ViT加速方法多独立地剪token或head，依赖启发式或一阶信号，泛化与精度受限；ViT的二次方注意力开销和冗余阻碍在资源受限设备上的部署。需要一个统一、可泛化且能在不牺牲精度下显著降计算的方案。

Method: 利用高效的Hessian-向量积估计token与注意力头的曲率加权敏感度，在给定损失预算下做出有原则的动态（输入自适应）剪枝决策；同时探索token与head的互补性：token剪枝主导算力节省，head剪枝做细粒度冗余移除；之后进行微调以恢复/提升精度。

Result: 在ImageNet-100与ImageNet-1K、ViT-B/16与DeiT-B/16上，实现最高49.4% FLOPs降低、36%时延下降、46%吞吐提升；在40% token剪枝下精度可恢复+4.7%；在AGX Orin等边缘设备上，FLOPs/时延降低直接转化为推理速度与能效提升。

Conclusion: HEART-ViT提供首个统一的、二阶曲率驱动且输入自适应的ViT剪枝框架，能在保持甚至提升精度的同时显著加速并节能，缩小理论方法与实际部署之间的鸿沟。

Abstract: Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.

</details>


### [33] [milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion](https://arxiv.org/abs/2512.20128)
*Niraj Prakash Kini,Shiau-Rung Tsai,Guan-Hsun Lin,Wen-Hsiao Peng,Ching-Wen Ma,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出 milliMamba：一种基于毫米波雷达的2D人体姿态估计框架，通过时空联合建模在稀疏反射下恢复关键点，较基线在TransHuPR与HuPR上分别提升11.0和14.6 AP。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在隐私与光照鲁棒性上优于RGB，但因镜面反射导致信号稀疏，传统方法难以提取稳健特征与跨时空上下文，从而影响关键点恢复与时序平滑。

Method: 1) 使用Cross-View Fusion Mamba编码器，以线性复杂度从长序列高维雷达输入中提取时空特征；2) 采用Spatio-Temporal-Cross Attention解码器，在多帧上联合预测关节点坐标，利用邻近帧与关节间上下文补全缺失；3) 训练时在关键点损失外加入速度损失，以约束运动平滑性。

Result: 在TransHuPR与HuPR数据集上显著优于基线，AP分别提升11.0与14.6，同时保持合理计算复杂度。

Conclusion: 通过在编码与解码阶段的一体化时空建模并引入速度约束，milliMamba有效缓解雷达信号稀疏带来的关键点缺失问题，兼顾精度与效率。

Abstract: Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba

</details>


### [34] [Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)](https://arxiv.org/abs/2512.20148)
*Robert van de Ven,Trim Bresilla,Bram Nelissen,Ard Nieuwenhuizen,Eldert J. van Henten,Gert Kootstra*

Main category: cs.CV

TL;DR: 论文提出利用3D高斯喷溅重建果园场景，并将少量简化3D标注自动投影到多视图图像，极大降低苹果姿态估计的数据标注成本，同时分析遮挡与数据规模对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 果园环境变化大且遮挡严重，苹果关键点（如萼部）常被遮挡，传统姿态估计需依赖且标注这些关键点，导致标注矛盾、缺失与高成本。需要一种能在遮挡下稳健、并显著减少人工标注负担的标注与训练流程。

Method: 提出端到端数据生成与训练管线：1) 以3D Gaussian Splatting对果园场景进行多视图重建；2) 在3D中进行少量、简化的苹果标注；3) 将3D标注自动投影回各视角，生成大规模2D训练标签；4) 用这些标签训练并评估苹果姿态估计模型；并系统评测不同遮挡阈值与数据规模对性能的影响。

Result: 仅需105次人工3D标注即可自动生成28,191个训练标签，标注量减少99.6%。当训练样本选择遮挡≤95%的果实时性能最佳：在原始图像上的中性F1为0.927，在渲染图像上为0.970。扩大训练集对F1与姿态精度影响较小；遮挡越少，位置估计越准；所测模型未能有效学习苹果的朝向估计。

Conclusion: 利用3D重建与投影可极大降低果园姿态估计的标注成本并提升可扩展性；适度过滤重度遮挡样本（≤95%遮挡）可获得最佳综合表现。当前方法对位置估计稳健但对朝向学习不足，未来应改进朝向表征/损失或传感器融合以提升朝向估计。

Abstract: Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\leq95\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.

</details>


### [35] [CoDi -- an exemplar-conditioned diffusion model for low-shot counting](https://arxiv.org/abs/2512.20153)
*Grega Šuštar,Jer Pelhan,Alan Lukežič,Matej Kristan*

Main category: cs.CV

TL;DR: 提出CoDi：基于潜在扩散的低样本计数器，生成高质量密度图并通过NMS定位，显著提升FSC与MCAC基准的MAE。


<details>
  <summary>Details</summary>
Motivation: 低样本目标计数在小目标密集场景困难：密度图方法总数准但定位差；基于查询的点检测定位好但预训练查询数有限，面对海量目标需拼接/上采样等权宜之计，性能受限。

Method: 引入CoDi——首个用于低样本计数的潜在扩散模型。核心是新的基于示例的条件模块：从少量示例中提取对象原型，并在去噪网络的多层中自适应注入/调整这些原型以引导生成高质量密度图；随后用非极大值抑制在密度图上确定目标位置。

Result: 在FSC基准上分别在few-shot、one-shot、无参考三种设置下将SOTA的MAE降低15%、13%、10%；在MCAC基准上相较最佳方法将MAE降低44%，取得新的SOTA。

Conclusion: 通过潜在扩散与示例条件模块，CoDi兼顾总数与定位，优于密度法与查询检测法的缺点，在多种低样本场景显著提升计数与定位精度，并具有良好可扩展性。

Abstract: Low-shot object counting addresses estimating the number of previously unobserved objects in an image using only few or no annotated test-time exemplars. A considerable challenge for modern low-shot counters are dense regions with small objects. While total counts in such situations are typically well addressed by density-based counters, their usefulness is limited by poor localization capabilities. This is better addressed by point-detection-based counters, which are based on query-based detectors. However, due to limited number of pre-trained queries, they underperform on images with very large numbers of objects, and resort to ad-hoc techniques like upsampling and tiling. We propose CoDi, the first latent diffusion-based low-shot counter that produces high-quality density maps on which object locations can be determined by non-maxima suppression. Our core contribution is the new exemplar-based conditioning module that extracts and adjusts the object prototypes to the intermediate layers of the denoising network, leading to accurate object location estimation. On FSC benchmark, CoDi outperforms state-of-the-art by 15% MAE, 13% MAE and 10% MAE in the few-shot, one-shot, and reference-less scenarios, respectively, and sets a new state-of-the-art on MCAC benchmark by outperforming the top method by 44% MAE. The code is available at https://github.com/gsustar/CoDi.

</details>


### [36] [AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model](https://arxiv.org/abs/2512.20157)
*Sofian Chaybouti,Sanath Narayan,Yasser Dahou,Phúc H. Lê Khac,Ankit Singh,Ngoc Dung Huynh,Wamiq Reyaz Para,Hilde Kuehne,Hakim Hacid*

Main category: cs.CV

TL;DR: 提出AMoE：同时蒸馏SigLIP2与DINOv3到专家混合学生模型，通过关系蒸馏、token均衡批处理与分层聚类采样提升训练稳定性与数据效率，构建OpenLVD200M并发布模型与数据。


<details>
  <summary>Details</summary>
Motivation: 多教师蒸馏能统一视觉表示，但其学习动力学与数据效率研究不足；希望在更低算力下训练强大的视觉基础模型。

Method: 提出Agglomerative Mixture-of-Experts（AMoE）框架：1）非对称关系知识蒸馏（保几何结构并促进知识迁移）；2）token-balanced batching（跨分辨率统一token预算以稳定训练）；3）分层聚类与采样策略用于数据选择；并据此构建200M图像的OpenLVD200M数据集。

Result: 在Mixture-of-Experts学生上，联合SigLIP2与DINOv3蒸馏，验证三项关键技术分别提升稳定性、性能与样本效率；OpenLVD200M在多教师蒸馏场景下展现更高数据效率。

Conclusion: 多教师蒸馏可在较低计算成本下训练强表征的视觉基础模型；关系蒸馏、token均衡批处理与分层数据采样是关键；公开了OpenLVD200M与蒸馏模型以促进研究。

Abstract: Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.

</details>


### [37] [Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark](https://arxiv.org/abs/2512.20174)
*Hao Guo,Xugong Qin,Jun Jie Ou Yang,Peng Zhang,Gangyan Zeng,Yubo Li,Hailun Lin*

Main category: cs.CV

TL;DR: 提出NL-DIR基准：用自然语言细粒度描述来检索文档图像，构建41K文档、每图5条高质量描述；评测/微调主流跨模态与OCR-free VDU模型，并探索高效两阶段检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有文档图像检索多依赖图像查询且只区分粗粒度类别（如报纸、收据），难以应对真实场景中以自然语言、细粒度语义检索文档的需求。

Method: 1) 构建NL-DIR数据集：41K真实文档图像；结合大模型生成与人工校验，每张图5条细粒度自然语言描述；给出评价指标。2) 评测零样本与微调：对比主流对比式视觉-语言模型与无需OCR的VDU模型。3) 提出两阶段检索：先粗排后精排，以提升效果并兼顾时间与空间效率。

Result: 在NL-DIR上系统评测显示：现有跨模态与OCR-free VDU模型在细粒度自然语言检索上存在明显差距；两阶段检索在保持或降低计算与存储成本的同时带来性能提升。

Conclusion: NL-DIR为自然语言驱动的文档图像检索提供了新的标准与挑战，促进VDU研究；数据与代码将开源，期望推动更有效的细粒度跨模态检索方法。

Abstract: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.

</details>


### [38] [Generative Latent Coding for Ultra-Low Bitrate Image Compression](https://arxiv.org/abs/2512.20194)
*Zhaoyang Jia,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出GLC：在生成式VQ‑VAE的潜在空间而非像素空间进行变换编码，结合类别化超模块与代码预测监督，在极低码率下兼顾逼真度与保真度，并支持图像复原与风格迁移。


<details>
  <summary>Details</summary>
Motivation: 像素空间压缩在低码率下难以同时保持高真实感与高保真，因为像素级失真与人类感知不一致。希望利用与感知更一致、语义更稀疏的生成式潜在空间来提升低码率主观质量并维持客观一致性。

Method: 1) 将变换编码移至生成式VQ‑VAE的潜在空间，利用其稀疏、语义丰富、与感知对齐的特性；2) 设计“类别化超模块”以压缩超信息、降低比特开销；3) 引入基于码预测的监督，提升潜在码与语义一致性；4) 在自然/人脸数据上训练与评估。

Result: 在自然图像<0.04 bpp、人脸图像<0.01 bpp下保持高视觉质量；在CLIC2020测试集上，以少45%比特达到与MS‑ILLM相同的FID；展示了图像复原、风格迁移等下游应用能力。

Conclusion: 在生成式潜在空间进行压缩能在超低码率下兼顾真实感与保真度；类别化超模块与码预测监督进一步降低码率并增强语义一致性。该框架通用且可扩展，并能支撑多种图像生成/编辑应用。

Abstract: Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.

</details>


### [39] [JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement](https://arxiv.org/abs/2512.20213)
*Tao Ye,Hongbin Ren,Chongbing Zhang,Haoran Chen,Xiaosong Li*

Main category: cs.CV

TL;DR: 提出JDPNet，在统一框架下挖掘与处理水下图像中的耦合退化，实现SOTA性能与更优的性能-参数-计算权衡。


<details>
  <summary>Details</summary>
Motivation: 水下图像受多种退化（颜色偏移、模糊、低对比等）影响，这些退化呈非线性耦合而非简单叠加。现有方法多为针对单一退化设计专门分支或模块，忽略了退化之间耦合蕴含的信息，难以自底向上捕获和处理多退化的非线性交互。

Method: 提出联合退化处理网络JDPNet：1）联合特征挖掘模块，用于从耦合退化中提取潜在共享与交互特征；2）概率自举分布策略（probabilistic bootstrap），用于对耦合退化特征进行有效挖掘与统一调节；3）AquaBalanceLoss，多目标平衡损失，在颜色、清晰度与对比度之间进行权衡，指导网络从多种耦合退化损失中学习。

Result: 在6个公开水下数据集及作者新构建的2个数据集上，JDPNet获得SOTA表现；同时在准确性、参数规模与计算量之间实现更优权衡。

Conclusion: 统一建模与挖掘多退化耦合信息是提升水下图像复原/增强效果的关键。JDPNet通过联合特征挖掘、概率自举策略与AquaBalanceLoss有效处理非线性耦合退化，兼顾性能与效率。

Abstract: Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.

</details>


### [40] [LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation](https://arxiv.org/abs/2512.20217)
*Xiangxuan Ren,Zhongdao Wang,Pin Tang,Guoqing Wang,Jilai Zheng,Chao Ma*

Main category: cs.CV

TL;DR: LiteFusion是一种无需3D稀疏卷积骨干、以相机为主并用LiDAR作几何补充的轻量多模态3D检测器，在nuScenes上显著提升视觉基线的mAP与NDS，同时对缺失LiDAR具有鲁棒性并便于在多种硬件上部署。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D检测器依赖复杂结构与训练，且对LiDAR依赖强：一旦LiDAR缺失性能骤降；同时大量使用3D稀疏卷积，主要为NVIDIA GPU优化，难以在NPU/FPGA等平台部署。需要一种对LiDAR不敏感、部署友好的融合方式。

Method: 重新定位LiDAR在相机-激光融合中的角色：不再为其单独建3D骨干，而是把LiDAR点作为几何先验，直接注入到图像特征中。通过在四元数空间中融合，利用正交约束保持跨模态关系，得到紧凑的跨模态嵌入，从而完全去除3D稀疏卷积与专用LiDAR编码器。

Result: 在nuScenes上，相比纯视觉基线，LiteFusion以仅约1.1%的参数增量带来+20.4% mAP与+19.7% NDS提升；在无LiDAR输入时仍保持较强效果，显示出良好鲁棒性。

Conclusion: 将LiDAR作为几何补充而非独立模态，可在不引入3D骨干的前提下显著增强相机3D检测，既提升精度又提升跨硬件可部署性，并在缺失LiDAR时保持稳健。

Abstract: 3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.

</details>


### [41] [IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing](https://arxiv.org/abs/2512.20236)
*Oikantik Nath,Sahithi Kukkala,Mitesh Khapra,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: 提出IndicDLP，一个覆盖11种印度语言加英语、12类文档域的大规模版面分析数据集，并配套UED-mini以增强预训练；在Indic与非Indic场景均显著提升版面分析模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有大规模数据集（PubLayNet、DocBank）缺乏细粒度区域标注与多语种多域多脚本覆盖；人工精标数据集（M6Doc、D4LA）虽丰富但规模小、语种不足，尤其对多脚本的印度语系文档支持薄弱，限制了版面理解与数字化应用。

Method: 构建IndicDLP：覆盖11种印度语言+英语、12个常见文档域，提供更细粒度的区域标注；并从DocLayNet与M6Doc蒸馏/整合出UED-mini用于预训练；在现有英文模型上进行微调与评测，检验跨语言与跨域泛化。

Result: 在IndicDLP上微调的现有英文模型性能显著提升；基于IndicDLP训练的模型对非Indic文档布局也具备良好泛化能力。

Conclusion: IndicDLP与UED-mini弥补了规模、标注粒度与多语言多域覆盖的缺口，为包容高效的文档版面理解与数字化提供了有力基础。

Abstract: Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.

</details>


### [42] [Degradation-Aware Metric Prompting for Hyperspectral Image Restoration](https://arxiv.org/abs/2512.20251)
*Binfeng Wang,Di Wang,Haonan Guo,Ying Fu,Jing Zhang*

Main category: cs.CV

TL;DR: 提出DAMP框架，通过可学习的退化度量作为提示，统一解决多类型/混合/未知退化的高光谱图像恢复，并在多数据集上达SOTA且具强泛化。


<details>
  <summary>Details</summary>
Motivation: 现实HSI常遭受多种且混合的退化，现有统一恢复方法往往依赖明确的退化先验或标签作为提示，但真实场景难以获取、且退化复杂多变，限制了模型泛化与实用性。

Method: 1) 设计空间-光谱退化度量，连续量化多维退化并作为Degradation Prompts (DP)，用于建模跨任务退化分布相似性并指导共享特征学习；2) 提出空间-光谱自适应模块（SSAM），以可学习参数动态调制空间与光谱特征提取；3) 将SSAM作为专家融入Mixture-of-Experts（MoE）结构，以DP作为门控路由，实现对多样、混合或未见退化的自适应高效恢复。

Result: 在自然场景与遥感HSI数据集上取得SOTA性能，并展示出卓越的泛化能力（对多样、混合及未见退化的稳健恢复）。

Conclusion: 用连续退化度量替代离散退化先验作为提示，并结合SSAM+MoE的自适应架构，可在统一HSI恢复中实现高效、鲁棒且可泛化的表现。

Abstract: Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.

</details>


### [43] [BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation](https://arxiv.org/abs/2512.20255)
*Jinghao Shi,Jianing Song*

Main category: cs.CV

TL;DR: 提出BiCoR-Seg：以热力图驱动的双向信息协同模块HBIS，在特征图与类别嵌入间建立双向流；配合层级监督与Fisher判别损失，强化判别性与可解释性；在LoveDA、Vaihingen、Potsdam上取得优异分割效果。


<details>
  <summary>Details</summary>
Motivation: HRSS面临类间相似高、类内差异大，像素级学习难以注入抽象且强判别的语义知识，导致边界模糊与类别混淆，亟需在像素特征与语义先验间建立有效联系并提升表示判别性与可解释性。

Method: 1) 设计HBIS模块：生成类别级热力图，在特征图与类嵌入间建立双向信息流，实现语义先验对像素特征的引导与像素证据对类嵌入的反哺。2) 层级监督：各层HBIS输出的可解释热力图直接作为低分辨率分割预测参与监督，增强浅层特征的判别性。3) 交叉层类嵌入Fisher判别损失：促使类内紧致、类间可分，并在跨层一致地优化嵌入表示。

Result: 在LoveDA、Vaihingen、Potsdam数据集上取得领先/出色的语义分割性能，并展示更强的可解释性（基于热力图）。

Conclusion: BiCoR-Seg通过热力图驱动的双向协同、层级监督与Fisher判别正则，显著缓解HRSS中的类混淆与边界模糊，兼顾精度与可解释性，并具有通用可扩展性。

Abstract: High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.

</details>


### [44] [LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation](https://arxiv.org/abs/2512.20257)
*Daniele Cardullo,Simone Teglia,Irene Amerini*

Main category: cs.CV

TL;DR: 提出LADLE-MM，一种在标注受限与训练资源受限下的多模态谣言/虚假信息检测模型，参数更少但在DGM4与VERITE上表现与/优于SOTA，并具备开放集泛化与抗单模态偏置能力。


<details>
  <summary>Details</summary>
Motivation: 多媒体生成与篡改工具普及导致跨模态（图文）虚假信息激增，社交媒体传播严重；现有检测方法往往需大量标注或计算昂贵，难以在资源有限场景落地。

Method: 采用“模型汤”初始化策略的多模态检测器LADLE-MM：包含两个单模态分支（图像、文本）与一个多模态分支；多模态分支利用BLIP提取的多模态嵌入作为固定参考空间来增强图文表示；在有限标注设定下进行训练，整体参数量较小（较SOTA少60.3%可训练参数）。

Result: 在DGM4基准上，二分类与多标签任务取得与SOTA竞争的结果，尤其在无grounding标注训练时优于现有方法；在VERITE数据集上优于使用大型视觉语言模型的复杂架构，表现出良好的开放集泛化与对单模态偏置的鲁棒性。

Conclusion: 在资源与标注受限条件下，LADLE-MM以更轻量的参数规模实现强竞争力的跨模态虚假信息检测，并在不同数据集与开放集场景中表现出良好的泛化与鲁棒性。

Abstract: With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.

</details>


### [45] [${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations](https://arxiv.org/abs/2512.20260)
*Jiawei Ge,Jiuxin Cao,Xinyi Li,Xuelin Zhu,Chang Liu,Bo Liu,Chen Feng,Ioannis Patras*

Main category: cs.CV

TL;DR: 提出D^3ETOR，一个两阶段弱监督伪装目标检测框架：用“争辩增强”改进SAM伪标注，并用频域感知的渐进去偏网络缓解涂鸦标注偏差，显著逼近全监督性能。


<details>
  <summary>Details</summary>
Motivation: 弱监督COD依赖稀疏涂鸦标注，现有方法受限于：通用分割模型生成的伪掩码不可靠，且忽视涂鸦固有的局部偏置，导致难以获得全局结构与细节的兼顾。

Method: 两阶段：1) Debate-Enhanced Pseudo Labeling：自适应熵驱动点采样+多智能体辩论机制，提升SAM对COD的语义判别力与伪掩码精度与可解释性。2) Frequency-Aware Progressive Debiasing（FADeNet）：多层频域特征渐进融合，兼顾全局语义与局部细节；区域动态重加权以缓解涂鸦监督偏差；联合利用伪掩码与涂鸦语义进行训练。

Result: 在多个基准上取得SOTA，弱监督与全监督COD性能差距显著缩小（定量指标未给出，但宣称全面领先）。

Conclusion: 通过“辩论增强伪标注”与“频域渐进去偏”，D^3ETOR有效提升WSCOD伪标签质量并克服涂鸦偏置，实现对伪装目标的更精准定位与分割，逼近全监督表现。

Abstract: Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.

</details>


### [46] [UbiQVision: Quantifying Uncertainty in XAI for Image Recognition](https://arxiv.org/abs/2512.20288)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.CV

TL;DR: 提出一个框架，用Dirichlet后验采样与Dempster–Shafer证据理论来量化SHAP解释在医学影像中的不确定性，并用信念/似然/融合图与统计分析给出可量化的不确定性度量，在三类医学影像数据上验证。


<details>
  <summary>Details</summary>
Motivation: 深度模型（ResNet、ViT、混合CNN）性能提升但复杂度上升，导致可解释性不足；SHAP虽常用但在认知（epistemic）与内在随机（aleatoric）不确定性下解释不稳定、可能不可靠。需要一种能对SHAP解释自身的不确定性进行量化的方法，特别是在噪声多、分布异质的医学影像环境中。

Method: 结合Dirichlet后验采样与Dempster–Shafer（DS）理论：对模型输出进行Dirichlet建模以表达预测分布不确定性；基于DS理论构建信念（belief）、似然/可然（plausibility）与融合（fusion）图来表示与融合来自SHAP解释的不确定性证据；并辅以统计定量分析对不确定性进行量化与比较。

Result: 在三种具有不同类别分布、图像质量与成像模态（病理、眼科、放射科）的数据集上进行评测，表明该框架能稳定量化并揭示SHAP解释在存在噪声与显著认知不确定性条件下的不确定性特征。

Conclusion: 该框架提升了医学影像场景中SHAP解释的可靠性与可量化性，为在高不确定条件下进行更可信的模型解释与决策提供支持。

Abstract: Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.

</details>


### [47] [TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation](https://arxiv.org/abs/2512.20296)
*Ji-Hoon Kim,Junseok Ahn,Doyeop Kwak,Joon Son Chung,Shinji Watanabe*

Main category: cs.CV

TL;DR: 提出TAVID：从文本与参考图像同步生成互动视频与对话语音的一体化框架，通过跨模态映射实现音视频双向耦合，显著提升说话脸真实度、聆听头回应性、双人互动流畅度与语音质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将“会说头像/会听头像”与“对话语音生成”割裂处理，忽视真实人类对话中音频与视觉的紧密耦合，难以实现类人、同步、流畅的多模态互动。

Method: 构建统一管线TAVID：同时生成交互人脸视频与对话语音；引入两个跨模态映射器——运动映射器（将音频或文本线索映射到面部运动/表情/唇形动态）与说话人映射器（在语音与人脸身份之间传递声纹/身份特征），支持音视频间的双向信息交换与对齐，实现同步驱动与协同优化。

Result: 在四个维度上系统评测：说话脸真实度、聆听头响应性、二人互动流畅度与语音质量；大量实验显示TAVID在上述各项指标上均优于或显著提升于现有方法。

Conclusion: 跨模态双向耦合的一体化生成能更好模拟人类对话的多模态本质；TAVID验证了联合生成策略在真实度、响应性、交互流畅度与语音质量上的有效性，为构建类人多模态对话系统奠定基础。

Abstract: The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.

</details>


### [48] [The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection](https://arxiv.org/abs/2512.20340)
*Qingdong He,Xueqin Chen,Yanjie Pan,Peng Tang,Pengcheng Xu,Zhenye Gan,Chengjie Wang,Xiaobin Hu,Jiangning Zhang,Yabiao Wang*

Main category: cs.CV

TL;DR: 提出KeyTailor框架与ViT-HD数据集，通过关键帧驱动的细节注入，在不改动DiT结构的前提下提升服饰动态细节与背景一致性，并以更低复杂度实现高质量视频试穿。


<details>
  <summary>Details</summary>
Motivation: 现有DiT式视频试穿难以同时捕捉细粒度服装动态与跨帧背景完整性，且常需额外交互模块带来高计算开销；公共数据集规模与质量有限，限制泛化与训练效果。

Method: 1) 指令引导的关键帧采样，从输入视频筛选信息量高的关键帧；2) 两个关键帧驱动模块：服装细节增强模块将关键帧中的服装动态蒸馏为服装相关潜变量；协同背景优化模块优化背景潜变量的完整性；3) 将增强后的潜变量与姿态、遮罩、噪声潜变量一起注入标准DiT块，无需改动架构、避免额外复杂度；4) 构建ViT-HD，含15,070条810×1080高质量视频，覆盖多样服饰。

Result: 在动态与静态场景下，相比SOTA基线，KeyTailor在服装逼真度与背景完整性上均更优；推理效率更高（未显式改动DiT且避免额外模块）。

Conclusion: 关键帧驱动的细节注入可在不增加架构复杂度的情况下显著提升VVT的服装动态与背景一致性；配套的ViT-HD数据集为训练与泛化提供支持。

Abstract: Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.

</details>


### [49] [CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation](https://arxiv.org/abs/2512.20362)
*V. Kovalev,A. Kuvshinov,A. Buzovkin,D. Pokidov,D. Timonin*

Main category: cs.CV

TL;DR: 提出CRAFT：一种无需训练、与模型无关的结构化推理框架，通过将文本提示分解为约束性视觉问题、用VLM验证、并仅在失败处用LLM进行定向改写，迭代直至满足停止条件，从而在多模型与基准上显著提升合成图像的可组合性、文字渲染与偏好评分，且推理开销可忽略，轻量生成器收益尤大。


<details>
  <summary>Details</summary>
Motivation: 现有推理时改写与反思方法多是整体、隐式的批评或不受约束的提示重写，难以解释与控制，也难以可靠早停；而LLM领域显示出基于验证、定向纠错和早停的显式结构化思维优势。作者希望把这种可解释、可控的结构化推理引入多模态图像生成。

Method: 提出CRAFT（Continuous Reasoning and Agentic Feedback Tuning）：1）将原始提示分解为具有依赖结构的视觉问题/约束；2）用视觉-语言模型对生成图像逐约束验证；3）仅对未满足的约束由LLM代理进行有针对性的提示编辑；4）迭代执行并设定显式停止准则（所有约束满足或达到上限），形成可解释、可控的推理时精炼闭环。框架训练无关、模型无关。

Result: 在多种模型家族与困难基准上，CRAFT提升了组合性准确率、文本渲染质量与人类偏好评估分数；轻量级生成器提升尤为显著；改进带来的推理时间开销可忽略，使小模型接近更昂贵系统的质量。

Conclusion: 将显式、基于约束的推理引入推理时图像生成能显著提升可靠性与可控性，且具高性价比与可解释性，是改进多模态生成的重要要素。

Abstract: Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.

</details>


### [50] [Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge](https://arxiv.org/abs/2512.20376)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Mahmood Malik,Markus Schedl*

Main category: cs.CV

TL;DR: FAME 2026 挑战旨在评测跨语言条件下的人脸-声音关联方法，摘要为赛事简报。


<details>
  <summary>Details</summary>
Motivation: 现实中双语/多语场景普遍，训练与测试语言不匹配时，现有人脸-声音关联模型鲁棒性不足，需要推动能跨语言泛化的方法。

Method: 组织ICASS P 2026 FAME挑战，设定训练语言与测试语言不同的评测协议与数据集，比较参赛系统在跨语言场景下的人脸-声音匹配能力。

Result: 本文仅为挑战简报，未提供具体成绩；核心产出是任务定义、基准与评测框架。

Conclusion: FAME 2026 为跨语言人脸-声音关联提供标准化测试平台，期望激励开发对语言变化更稳健的多模态关联方法。

Abstract: Over half of the world's population is bilingual and people often communicate under multilingual scenarios. The Face-Voice Association in Multilingual Environments (FAME) 2026 Challenge, held at ICASSP 2026, focuses on developing methods for face-voice association that are effective when the language at test-time is different than the training one. This report provides a brief summary of the challenge.

</details>


### [51] [SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images](https://arxiv.org/abs/2512.20377)
*Linfei Li,Lin Zhang,Zhong Wang,Ying Shen*

Main category: cs.CV

TL;DR: SmartSplat 是一种面向超高分辨率图像的高自适应、高特征感知的2D高斯压缩框架，通过梯度/颜色引导的采样与尺度自适应颜色初始化，在极强压缩下仍保持高还原度，并在DIV8K与16K数据上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 生成式AI推动超高分辨率内容激增，终端侧需要高压缩比与实时解码；现有2D高斯表示虽高效但在超高分辨率下难以兼顾压缩率与重建质量。

Method: 提出SmartSplat：1) 梯度-颜色引导的变分采样（根据梯度与颜色方差自适应分配与采样高斯）；2) 基于排除的均匀采样（减少像素空间重叠，提升覆盖效率）；3) 尺度自适应的高斯颜色采样（跨尺度改进颜色初始化）；4) 对高斯的空间布局、尺度与颜色联合优化，使有限高斯同时刻画局部结构与全局纹理；支持任意分辨率与压缩率。

Result: 在DIV8K与新建16K数据集上，SmartSplat在相当压缩率下稳定优于SOTA，并突破其压缩上限，实现更强的可扩展性与实用性；重建质量在强压缩下仍保持领先。

Conclusion: 特征感知与尺度自适应策略显著提升2D高斯压缩在超高分辨率场景的效率与保真，SmartSplat兼具高压缩比与高重建质量，适合终端实时应用并具备良好扩展性。

Abstract: Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.

</details>


### [52] [DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning](https://arxiv.org/abs/2512.20409)
*Junho Yoon,Jaemo Jung,Hyunju Kim,Dongman Lee*

Main category: cs.CV

TL;DR: 提出DETACH：将视频-传感器对齐分解为空间与时间两部分，通过在线聚类获得语义化传感器空间特征，并用两阶段（空间互监督+时序加权对比）对齐，显著优于改造的自我视角-可穿戴基线。


<details>
  <summary>Details</summary>
Motivation: 现有将第一人称视频与可穿戴传感器对齐的方法依赖全局序列表征（Global Alignment），但在第三人称视频与环境传感器（非侵入、可扩展）场景中失效：捕捉不到局部细节（细微动作），且过度依赖模态不变的时间模式，导致具有相似节奏但空间语义不同的动作被误对齐。

Method: 提出DETACH分解式时空框架：1）显式时空分解以保留局部细节；2）通过在线聚类从环境传感器发现“传感器-空间”语义特征以进行语义落地；3）两阶段对齐：先用跨模态互监督建立空间对应关系，再用空间-时间加权的对比损失进行时间对齐，自适应处理易负样本、难负样本和伪负样本。

Result: 在Opportunity++与HWU-USP数据集上，面向多种下游任务，相比改造的自我视角-可穿戴基线取得了显著性能提升。

Conclusion: 将对齐问题分解并引入语义化的传感器空间特征与分阶段对齐策略，可有效解决第三人称视频-环境传感器场景中的局部细节缺失和时间模式歧义问题，具有非侵入、可扩展的优势。

Abstract: Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.

</details>


### [53] [Chain-of-Anomaly Thoughts with Large Vision-Language Models](https://arxiv.org/abs/2512.20417)
*Pedro Domingos,João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: 提出CoAT：在多智能体推理框架中加入“异常思维链”，以克服大规模视听语言模型在监控场景中过度正常化的偏见；通过末端的异常聚焦分类层注入犯罪（异常）归纳偏置，显著提升异常检测与分类性能。


<details>
  <summary>Details</summary>
Motivation: LVLM 在视频监控中常把含犯罪片段解释为正常，源于对“常态”的先验偏置；即便引入传统链式思维（CoT），其缺乏面向异常的归纳偏置，依旧趋向正常解释。因此需要一种能在推理过程中显式偏向异常假设的机制。

Method: 提出 Chain-of-Anomaly-Thoughts (CoAT)：一种多智能体推理框架。在标准推理链后附加一个“异常聚焦”的最终分类层，显式注入犯罪/异常归纳偏置；该层对上游推理结果进行再评估与重加权，以面向异常的视角输出判断，从而纠正常态偏置。

Result: 在低清监控视频上异常检测 F1 提升 11.8 个百分点；在高清视频上异常分类提升 3.78 个百分点，较基线显著改善。

Conclusion: 通过在推理链末端加入异常导向的分类层并采用多智能体协作推理，CoAT 能有效缓解 LVLM 的正常性偏置，提升监控异常检测与分类性能。

Abstract: Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.

</details>


### [54] [Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks](https://arxiv.org/abs/2512.20431)
*Abdullah Al Shafi,Abdul Muntakim,Pintu Chandra Shill,Rowzatul Zannat,Abdullah Al-Amin*

Main category: cs.CV

TL;DR: 提出一种基于软投票的CNN集成用于早期皮肤癌分类，先通过迁移学习的双编码器实现病灶分割以聚焦关键特征，再将MobileNetV2、VGG19、InceptionV3进行集成，三大数据集上达到约96.3%、90.9%、93.9%的准确率。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期发现能显著提高生存率，但人工皮镜/目诊受经验与背景伪影干扰。利用AI和带标注图像的CNN可提升诊断准确性，但单模型易受背景、类别不均衡与泛化能力限制，因此需要通过更鲁棒的分割与集成策略改进分类性能与部署可行性。

Method: • 数据：HAM10000、ISIC 2016、ISIC 2019三基准数据集。
• 预处理：类别重平衡、图像增强与过滤。
• 分割：基于迁移学习的混合双编码器（hybrid dual encoder）实现病灶精确分割，以去除背景伪影并突出临床相关特征。
• 分类：对分割后的病灶图像，采用MobileNetV2、VGG19、InceptionV3三模型，并通过软投票（概率平均/加权）进行集成，权衡精度与速度以利于真实场景部署。

Result: 在三数据集上的病灶识别准确率分别为96.32%、90.86%、93.92%。并使用常见皮肤病灶检测评估指标进行验证，结果显示性能优异（摘要未给出具体灵敏度、特异度、AUC等数值）。

Conclusion: 通过“先分割后分类”的流程与软投票集成，可有效聚焦临床显著区域、降低背景干扰，实现高精度且具部署潜力的皮肤癌早期分类。

Abstract: Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\%, 90.86\%, and 93.92\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.

</details>


### [55] [High Dimensional Data Decomposition for Anomaly Detection of Textured Images](https://arxiv.org/abs/2512.20432)
*Ji Song,Xing Wang,Jianguo Wu,Xiaowei Yue*

Main category: cs.CV

TL;DR: 提出TBSD方法：先学习纹理基表示准周期纹理，再用其作为先验进行光滑-稀疏分解，减少将纹理误判为缺陷，在小数据下也能高精度检测异常，优于基线。


<details>
  <summary>Details</summary>
Motivation: 制造业中纹理缺陷图像的异常检测常因准周期纹理干扰而出现误检、对大规模结构化数据依赖强、鲁棒性不足。需要能在小数据下准确区分背景纹理与稀疏异常的方法。

Method: 定义并分析图像准周期性的数学表述与性质；两阶段TBSD：1) 学习纹理基函数，提取准周期纹理模式；2) 利用学得的纹理基作为先验，进行平滑背景与稀疏异常的分解，实现异常定位与抑制纹理误识别。

Result: 在模拟与真实数据集上，相较基线方法，TBSD达到更少误识别、更强鲁棒性与更小训练集需求，整体异常检测性能更优。

Conclusion: 利用纹理基先验的平滑-稀疏分解能有效处理带有准周期纹理的缺陷图像，显著降低误检并提升小样本场景下的异常检测效果，适合制造业图像检测应用。

Abstract: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.

</details>


### [56] [Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding](https://arxiv.org/abs/2512.20451)
*Anh Dao,Manh Tran,Yufei Zhang,Xiaoming Liu,Zijun Cui*

Main category: cs.CV

TL;DR: 将物理推断的关节驱动力（力线索）融入现有视觉管线，在步态识别、动作识别与细粒度视频描述三大任务上、8个基准中带来稳定提升，尤其在遮挡、服饰变化与剧烈动作等困难场景中增益更显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的人体运动理解忽视了生物力学中的关键物理线索（如关节驱动力），导致在动态、遮挡或外观变化条件下表现受限。作者探究：何时以及在多大程度上，物理推断的力信息能提升运动理解？

Method: 在不改动任务主干框架的前提下，将从视频中物理推断得到的关节/肢体受力作为补充模态，注入到已有的步态识别、动作识别与视频描述模型中，并在多个数据集上与标准视觉/运动学特征做系统性对比评估。

Result: 8个基准均有提升：步态识别CASIA-B Rank-1由89.52%至90.39%（+0.87），在穿外套与侧视条件分别+2.7%与+3.0%；Gait3D由46.0%至47.3%（+1.3）。动作识别中CTR-GCN在Penn Action整体+2.00%，高用力类如击打/扇耳光+6.96%。视频描述中Qwen2.5-VL的ROUGE-L由0.310至0.339（+0.029），显示时间定位与语义丰富度改善。

Conclusion: 物理推断的力线索与视觉、运动学特征互补，尤其在动态、遮挡或外观变化强的场景中显著提升表现。将力作为额外模态是通用、可迁移的增强策略。

Abstract: Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.

</details>


### [57] [UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images](https://arxiv.org/abs/2512.20479)
*Yiming Zhao,Yuanpeng Gao,Yuxuan Luo,Jiwei Duan,Shisong Lin,Longfei Xiong,Zhouhui Lian*

Main category: cs.CV

TL;DR: UTDesign提出统一框架，实现高精度风格化文字编辑与条件文字生成，支持中英文；基于DiT风格迁移生成透明文字前景，并通过多模态条件编码器实现与背景、提示和布局一致的文本合成，整合T2I与MLLM布局形成自动化T2D流程，开源并在风格一致性与文本准确性上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视觉生成上强，但在小字号与非拉丁文字渲染上表现欠佳，导致设计场景（海报、广告等）中文字风格一致性与准确性不足，缺乏统一、可控且支持多语言的高精度文本编辑/生成方案。

Method: 1) 训练自研的DiT文本风格迁移模型，从合成数据学习，输出保留参考字形风格的透明RGBA文本前景；2) 通过在含细粒度文本标注的数据上训练多模态条件编码器（结合背景图、文本提示、布局规范），将模型扩展为条件文本生成框架，实现风格一致且位置/语义可控的合成；3) 与预训练T2I模型和基于MLLM的布局规划器集成，组成自动化Text-to-Design流水线。

Result: 在开源方法中于风格一致性和文本准确性上达SOTA；相较商用闭源方案也具有独特优势。实验展示中英文脚本的高精度渲染与复杂版式下的稳定输出。

Conclusion: UTDesign提供统一、可控、跨语言的设计文本生成/编辑框架，通过DiT风格迁移+多模态条件对齐+T2I与MLLM集成，实现从风格提取到版式落地的全流程自动化，效果与准确性领先且已开源。

Abstract: AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.

</details>


### [58] [Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems](https://arxiv.org/abs/2512.20487)
*James E. Gallagher,Edward J. Oughton,Jana Kosecka*

Main category: cs.CV

TL;DR: 研究评估将RGB与LWIR热红外自适应融合用于无人机识别地表布设地雷；YOLOv11在5–10米高度、10–30%热融合时最佳（mAP 86.8%），并在效率上优于更准但更慢的RF-DETR。


<details>
  <summary>Details</summary>
Motivation: 地雷仍造成严重人道主义危机，传统探测在效率、覆盖与环境适应性受限。无人机平台结合可见光与热红外有望利用地雷与土壤的热对比提升检测，但最佳融合比例、飞行高度、模型选择、季节/时间多样性对性能的影响尚不清楚。

Method: 在UAS平台获取RGB与LWIR影像，对两模态进行自适应比例融合，系统评测YOLO系列（v8/v10/v11）在114张测试图上的表现（共35,640种模型-条件组合），并与RF-DETR、Faster R-CNN、RetinaNet等架构比较训练效率与精度；同时比较多时相聚合训练与季节特定训练，并区分反坦克与杀伤人员地雷的检测差异。

Result: YOLOv11在5–10m高度、热红外占10–30%融合时达到最佳检测（mAP 86.8%）；架构比较中，RF-DETR精度最高（69.2% mAP），其次为Faster R-CNN（67.6%）、YOLOv11（64.2%）、RetinaNet（50.2%），但YOLOv11训练速度快17.7倍（41分钟 vs 12小时）；多时相数据训练较季节特定提升1.8–9.6%；反坦克地雷检测率61.9%，显著高于杀伤人员地雷19.2%。

Conclusion: RGB-LWIR融合在地表地雷的无人机检测中有效，存在明确的最佳热融合比例与飞行高度；选择模型需在准确性与训练/部署效率间权衡。多时相数据可提升鲁棒性。由于研究对象为地表地雷，未来应量化不同埋深与异质土壤下的热对比与检测可行性。

Abstract: Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.

</details>


### [59] [Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition](https://arxiv.org/abs/2512.20501)
*Gorjan Radevski*

Main category: cs.CV

TL;DR: 论文系统研究多模态对齐、翻译、融合与迁移，覆盖空间语言到视觉、医学文本到3D解剖、文本到知识图谱事实、视频-检测融合的动作识别，以及以蒸馏实现RGB单模态复现多模态性能，全面提升复杂多模态输入的理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态应用（场景生成、医学导航、知识抽取、动作识别）中，语言、图像/视频与结构化知识之间存在语义与空间鸿沟，现有方法在可解释性、鲁棒性、标注成本与推理效率方面受限，亟需统一方法学来实现跨模态对齐、可解释映射与高效迁移。

Method: 分章提出针对性方法：1）Spatial-Reasoning BERT，将文本空间关系翻译为2D剪贴画布局；2）基于空间共现损失的医学文本到解剖3D定位；3）构建文本到知识图谱实体/谓词链接基准与解析框架；4）视频帧与目标检测表示的融合用于组合式动作识别；5）多模态知识蒸馏，将融合模型知识迁移至仅RGB模型。

Result: 实现从语言到视觉布局的有效译码；显著提升医学文本的可导航性与可解释定位；改进文本到KG事实抽取的准确性与消歧；在组合式动作识别上取得更鲁棒且更高的准确率；通过蒸馏在保持性能的同时显著降低推理开销。

Conclusion: 论文在多模态翻译、融合与知识迁移上提出一系列可解释且实用的方法，贯穿空间语言理解、医学导航、知识图谱增强与动作识别，证明了跨模态对齐与蒸馏可同时提升性能与效率，具备广泛应用潜力。

Abstract: This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.
  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.
  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.
  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.
  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.
  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.

</details>


### [60] [SirenPose: Dynamic Scene Reconstruction via Geometric Supervision](https://arxiv.org/abs/2512.20531)
*Kaitong Cai,Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.CV

TL;DR: SirenPose 将正弦网络的高频建模与关键点几何监督结合，通过几何与物理一致性约束，实现从单目视频高保真、时序连贯的动态3D重建，在快速运动与遮挡等复杂场景显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目视频动态3D重建在快速运动、多目标交互、遮挡与剧烈场景变化下易出现运动失真与时空不一致。需要一种既能捕获细粒度几何细节，又能在时空维度保持关键点与姿态一致性的框架。

Method: 提出几何感知的损失，将SIREN（正弦表示网络）的周期激活用于高频细节建模；引入基于物理启发的约束，确保关键点在空间与时间上的连贯一致；扩展UniKPT至60万标注样本；融合图神经网络以建模关键点间关系与结构相关性；整体用于从单目视频重建动态3D场景与姿态。

Result: 在Sintel、Bonn、DAVIS等基准上超越SOTA：在DAVIS上相对MoSCA的FVD降低17.8%、FID降低28.7%、LPIPS提高6.0%，并显著提升时间一致性、几何精度、用户评分和运动平滑度；在位姿估计上相较Monst3R获得更低的ATE和更小的平移/旋转相对误差。

Conclusion: SirenPose有效融合高频信号表示与关键点几何、物理一致性约束，实现对快速运动与复杂动态的物理合理重建，兼顾时序稳定与几何精度，全面优于现有方法。

Abstract: We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.

</details>


### [61] [AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment](https://arxiv.org/abs/2512.20538)
*Anna Šárová Mikeštíková,Médéric Fourmy,Martin Cífka,Josef Sivic,Vladimir Petrik*

Main category: cs.CV

TL;DR: 提出AlignPose：一种无需对象特定训练与对称标注的多视角RGB 6D姿态估计方法，通过跨视角特征度量优化统一物体位姿，显著优于现有方法，尤其在工业数据集上。


<details>
  <summary>Details</summary>
Motivation: 单目基于RGB的模型匹配方法泛化强但受深度歧义、遮挡与杂乱场景限制；多视角有潜力缓解这些问题，但现有方法要么依赖精确的单目初值，要么对未见物体泛化差。需要一种能在多视角下稳健、通用且无需对象特定训练的姿态估计方案。

Method: 提出AlignPose：1) 输入多路外参已标定的RGB图像与物体CAD模型；2) 在所有视角间进行“多视角特征度量优化”（feature-metric refinement），在线渲染物体特征，与观测图像特征在各视角对齐；3) 直接在世界坐标系优化单一一致的6D物体位姿，联合最小化跨视角特征差异；4) 全流程不需要对象特定训练或对称性标注。

Result: 在四个数据集（YCB-V、T-LESS、ITODD-MV、HouseCat6D）上按BOP基准评测，AlignPose优于已发表方法，尤其在多视角充足、遮挡和工业场景（如T-LESS/ITODD-MV）中提升显著。

Conclusion: 多视角特征度量联合优化可在无需对象特定先验的情况下，稳健解决单目下的深度与遮挡问题，实现对未见物体的强泛化，并在多基准上取得SOTA水平。

Abstract: Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.

</details>


### [62] [Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios](https://arxiv.org/abs/2512.20556)
*Mingwei Tang,Jiahao Nie,Guang Yang,Ziqing Cui,Jie Li*

Main category: cs.CV

TL;DR: 提出MTIF：用多粒度文本指导图像融合，通过层级跨模态调制与显著性驱动的数据增强，在多曝光与多焦点任务上领先。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本辅助的图像融合多用粗粒度描述，难以捕获细节，跨模态对齐不精确，且训练数据语义密度不足，限制了融合质量。

Method: 1) 多粒度文本（细节、结构、语义）分层引导；2) 在每个粒度上施加监督，促进视觉-文本特征对齐与利用；3) 显著性驱动的语义富集模块，扩充含密集语义的训练样本；整体通过层级跨模态调制模块实现融合。

Result: 在多曝光和多焦点图像融合基准上，MTIF在多项指标上稳定超越最新方法，显示更好的细节保留、结构一致性与语义一致性。

Conclusion: 多粒度文本引导与显著性增强可显著提升跨模态对齐与融合质量，MTIF为文本引导图像融合提供了有效范式，具有通用性并可扩展至多种融合场景。

Abstract: Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.

</details>


### [63] [Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models](https://arxiv.org/abs/2512.20557)
*Shengchao Zhou,Yuxin Chen,Yuying Ge,Wei Huang,Jiehong Lin,Ying Shan,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出DSR Suite，构建可扩展4D动态空间推理数据与评测，并用轻量GSM将几何先验融入VLM，显著提升Qwen2.5-VL-7B在DSR上的表现且不损通用视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有多模态VLM虽擅长一般理解，但在随时间变化的3D几何与关系推理（DSR）上薄弱，核心原因是缺少规模化、4D感知的训练资源与有效利用几何先验的机制。需要在数据、基准和模型侧共同补齐。

Method: 1) 数据管线：从野外视频自动生成多选QA，借助基础视觉模型提取相机位姿、局部点云、目标掩码、朝向与3D轨迹等几何/运动线索；据此构建训练集DSR-Train与经人工精修的评测集DSR-Bench，强调野外来源、物体/场景级3D约束、视角变化、多物体交互与步骤化细粒度答案。2) 模型：提出Geometry Selection Module (GSM)，在不大改VLM的前提下，将预训练4D重建先验中的与问题相关的几何知识压缩为少量“几何token”，通过语义对齐实现有针对性的几何检索与融合，避免无关信息干扰。3) 训练：将DSR-Train与GSM集成至Qwen2.5-VL-7B进行微调与评测。

Result: 在DSR相关任务上显著优于基线；集成DSR-Train+GSM的Qwen2.5-VL-7B动态空间推理能力明显提升，同时在通用视频理解基准上保持原有精度，无明显退化。

Conclusion: 通过可扩展的4D感知数据与轻量几何选择模块，能够系统性提升VLM的动态空间推理而不牺牲通用性；面向真实世界视频、强调多物体与视角变化的DSR Suite为该方向提供了数据与方法基础。

Abstract: Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.

</details>


### [64] [FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models](https://arxiv.org/abs/2512.20561)
*Kaitong Cai,Jusheng Zhang,Jing Yang,Yijia Fan,Pengtao Xie,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 提出FlashVLM：在相同或更低视觉token预算下，通过文本引导的token选择，实现高压缩率同时保持甚至小幅提升性能。


<details>
  <summary>Details</summary>
Motivation: VLM对每帧需要数百/上千视觉token，注意力成本随token数二次增长且存在大量冗余。现有压缩/剪枝多不考虑文本查询或依赖深层注意力图，后者在激进剪枝时不稳定，破坏语义对齐。

Method: 提出文本引导的视觉token选择框架FlashVLM：1) 在语言模型空间将图像token投影后与归一化的文本嵌入做显式跨模态相似度，作为外在相关性；2) 与视觉显著性（内在）在对数域加权融合，并用温度控制锐化；3) 多样性保持的分区策略，保留少量代表性背景token以维持全局上下文；整体动态适配查询进行可变速率剪枝。

Result: 在相同token预算与评测协议下，基于LLaVA-1.5可剪除最多77.8%的视觉token仍“超越无损”：略优于不剪枝基线；在94.4%压缩下仍保持92.8%的基线精度。14个图像与视频基准上展现SOTA的效率-性能折中，并对主流VLM具备鲁棒性与泛化。

Conclusion: 跨模态显式相似度+显著性融合、并保留多样背景的选择策略，可在大幅减少视觉token的同时保持或提升VLM性能，兼顾效率、鲁棒性与泛化，优于依赖注意力权重的既有方法。

Abstract: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.

</details>


### [65] [LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving](https://arxiv.org/abs/2512.20563)
*Long Nguyen,Micha Fauth,Bernhard Jaeger,Daniel Dauner,Maximilian Igl,Andreas Geiger,Kashyap Chitta*

Main category: cs.CV

TL;DR: 论文研究模拟驾驶中专家与学生感知失配对模仿学习鲁棒性的影响，并通过缩小失配（可见性/不确定性/导航意图）提出TFv6，在CARLA等基准上创SOTA，并带来稳健的sim-to-real收益。


<details>
  <summary>Details</summary>
Motivation: 虽然模拟器能产生海量数据，但基于传感器的学生策略在闭环驾驶仍表现欠佳。作者怀疑核心原因之一是训练时“特权专家演示”与测试时“学生传感器观测”之间的失配：专家拥有更高可见性（无遮挡、全知）和更低不确定性（知晓他车动作），而学生仅依赖部分可见、噪声较大的感知；此外，学生在测试时的导航意图仅由单点目标弱指定，导致可模仿性差与决策歧义。

Method: 进行实证分析，量化专家-学生在可见性与不确定性上的不对称及其对闭环性能的影响；据此设计干预以缩小失配，包括改进导航意图表达与感知对齐、在学生端引入更贴近专家的监督与结构；提出并实现TransFuser v6（TFv6）学生策略；将感知监督融入共享的sim-to-real管线以提升现实数据集端到端表现。

Result: 在CARLA闭环公开基准上达成SOTA：Bench2Drive得分95，在Longest6 v2与Town13上将此前成绩提升超过一倍；在NAVSIM与Waymo基于视觉的端到端驾驶基准上亦获得一致增益。

Conclusion: 专家与学生之间的可见性、不确定性与导航意图失配显著限制了模仿学习驾驶性能。通过针对性地缩小这些差距并采用TFv6，可显著提升模拟和现实闭环驾驶表现，验证了对齐设计与感知监督的有效性；代码与数据已开源。

Abstract: Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.

</details>


### [66] [Repurposing Video Diffusion Transformers for Robust Point Tracking](https://arxiv.org/abs/2512.20606)
*Soowon Son,Honggyu An,Chaehyun Kim,Hyunah Ko,Jisu Nam,Dahyun Chung,Siyoon Jin,Jung Yi,Jaewon Min,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出DiTracker：利用视频扩散Transformer(视频DiT)特征进行点跟踪，结合查询-键匹配、轻量LoRA微调与与ResNet代价融合，在ITTO与TAP-Vid上达SOTA或更优表现。


<details>
  <summary>Details</summary>
Motivation: 现有点跟踪多用浅层CNN（如ResNet）逐帧处理，时间一致性弱，遇到大运动与遮挡时匹配代价不可靠。作者观察到预训练视频DiT具备强时空表征与内在跟踪能力，想将其转化为高鲁棒点跟踪基础。

Method: 以视频DiT为骨干：1) 采用query-key注意力进行匹配，直接利用DiT的时空注意力；2) 通过轻量LoRA进行高效适配；3) 与ResNet生成的匹配代价进行融合，兼顾稳定性与效率。训练批量较小（1/8常规模型）。

Result: 在ITTO基准上达到SOTA，在TAP-Vid系列基准上匹配或超越当前最优方法；对动态运动与频繁遮挡具有更强鲁棒性。

Conclusion: 视频DiT的时空特征是有效、效率高的点跟踪基础；通过简洁适配与代价融合即可显著提升在复杂场景下的跟踪性能。

Abstract: Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.

</details>


### [67] [FedPOD: the deployable units of training for federated learning](https://arxiv.org/abs/2512.20610)
*Daewoon Kim,Si Young Yie,Jae Sung Lee*

Main category: cs.CV

TL;DR: FedPOD 是一种面向联邦学习的“按轮次编排的导数”方法，在不依赖历史状态的前提下，用轮次级任务设计与验证损失加权来提升训练效率，并在保留被视为异常的客户端参与的同时，降低通信开销，性能与FedPIDAvg相当。


<details>
  <summary>Details</summary>
Motivation: 现有FedPIDAvg虽通过引入与熵相关的损失下降权重和PID控制实现了在非独立同分布数据上的性能与通信效率提升，但存在两大限制：1）基于泊松分布识别的异常客户端被排除，数据利用不足；2）PID 控制依赖跨轮次的历史信息，需要固定参与者集合，削弱了灵活性与可扩展性。

Method: 提出FedPOD：1）定义“按轮次”的任务与基于轮次的验证损失计算，作为权重或信号；2）不排除被判为异常的客户端，全部纳入；3）消除对历次训练信息（PID状态）的依赖，使客户端参与可动态变化；4）设计与Kubernetes POD概念对齐，可扩展至POD级伸缩以支持自动扩容/缩容。

Result: 在分割指标上与FedPIDAvg相当：Dice（WT/ET/TC）分别约0.78/0.71/0.72；投影收敛分数约0.74。并在通信代价与灵活性上具优势（可动态伸缩、无需固定参与者）。

Conclusion: FedPOD在不牺牲性能的前提下，提升了联邦学习的效率、灵活性与可扩展性，适配Kubernetes的自动伸缩场景，克服了FedPIDAvg对历史状态和异常客户端排除的局限。

Abstract: This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.

</details>


### [68] [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/abs/2512.20615)
*Xuanhua He,Tianyu Yang,Ke Cao,Ruiqi Wu,Cheng Meng,Yong Zhang,Zhuoliang Kang,Xiaoming Wei,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出L-IVA基准与ORCA框架，使视频化身具备长期目标与主动交互能力，通过闭环“观-思-行-省”与分层双系统实现IWM式状态预测与验证，显著提升任务成功率与行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频化身虽能保持身份与动作一致，但缺乏“能动性”，不能在生成不确定环境中自主规划、达成长期目标。需要一种评测与方法，使化身具备内在世界模型、可在开放域中多步推理与自适应交互。

Method: 1) 定义L-IVA任务/基准，评估随机生成环境中的目标导向规划。2) 提出ORCA：内部世界模型驱动的在线推理与认知架构。关键创新：a) 闭环OTAR（Observe-Think-Act-Reflect），将预测与实际生成对比以稳健跟踪状态并更新信念；b) 分层双系统：System 2做策略推理与状态预测，System 1把抽象计划映射为具体、模型适配的动作字幕。3) 将化身控制建模为POMDP，持续信念更新与结果核验，实现自主多步任务执行。

Result: 在多项实验中，ORCA在任务成功率与行为连贯性方面显著优于开环与“无反思”基线，显示其IWM范式的有效性。

Conclusion: 通过IWM启发的闭环推理与分层控制，视频化身从被动动画迈向主动、目标导向智能；L-IVA提供了评估该能力的标准化任务与基准，ORCA验证了该方向的可行性与优势。

Abstract: Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.

</details>


### [69] [SpatialTree: How Spatial Abilities Branch Out in MLLMs](https://arxiv.org/abs/2512.20617)
*Yuxi Xiao,Longfei Li,Shen Yan,Xinhang Liu,Sida Peng,Yunchao Wei,Xiaowei Zhou,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出SpatialTree，一个受认知科学启发的空间能力分层框架（L1感知、L2心智地图、L3模拟、L4代理能力），并据此构建首个以能力为中心的层级化基准，评测主流多模态大模型在27项子能力上的表现；发现低阶能力彼此独立而高阶能力强相关；微调显示L1内部存在负迁移但从低到高有正迁移；朴素鼓励长思考的RL对直觉感知有副作用，提出auto-think策略抑制不必要推理，从而在各层面稳定提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM空间能力研究碎片化、任务覆盖狭窄，缺乏一套与人类认知发展相呼应的系统框架与评测，以致难以理解不同空间技能之间的结构关系与迁移规律，也缺乏可指导训练与对齐的改进路径。

Method: 1) 定义认知学启发的四层空间能力层级（L1感知、L2心智映射、L3物理/时空模拟、L4具身/代理能力），细化为27个子能力；2) 构建对应的层级化评测基准，系统测试多种主流MLLM；3) 分析能力相关性与结构；4) 进行针对性的有监督微调以研究层内/跨层迁移效应；5) 评估RL对“思考长度”的影响，提出抑制无谓思考的auto-think策略以提升全层性能。

Result: - 结构性发现：L1子技能基本正交，高层（L2-L4）技能强相关，显示随层级上升而互依性增强；- 迁移规律：L1内部存在负迁移，但低→高层存在显著正迁移与协同增益；- 训练策略：朴素鼓励长推理的RL提升复杂推理却损伤直觉感知；auto-think能抑制不必要思考，使RL在各层均有稳定增益。

Conclusion: SpatialTree提供了理解与扩展MLLM空间能力的分层框架与首个能力中心基准；空间能力呈层级相关结构且存在跨层正迁移；优化时需平衡思考深度与直觉感知，auto-think为统一提升各层能力的简洁有效策略。

Abstract: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.

</details>


### [70] [SemanticGen: Video Generation in Semantic Space](https://arxiv.org/abs/2512.20619)
*Jianhong Bai,Xiaoshi Wu,Xintao Wang,Fu Xiao,Yuanxing Zhang,Qinghe Wang,Xiaoyu Shi,Menghan Xia,Zuozhu Liu,Haoji Hu,Pengfei Wan,Kun Gai*

Main category: cs.CV

TL;DR: 提出SemanticGen：先在语义空间扩散生成全局语义特征，再条件生成VAE潜变量到像素，从而更快收敛、算力更省，并在长视频上更有效，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成多在VAE潜空间直接建模，需在低层次视频token上做双向注意力，冗余大、收敛慢、长视频计算成本高。作者希望用更紧凑的表示先做全局规划，减少冗余和训练/采样成本。

Method: 两阶段扩散：1) 语义阶段：训练扩散模型在紧凑高层语义空间生成视频语义特征（编码全局布局与运动）。2) 细节阶段：另一个扩散模型以语义特征为条件生成VAE潜变量，再经VAE解码得到视频。训练与采样中通过条件建模实现从全局到细节。

Result: 在标准与长视频基准上实现更快收敛、更低计算开销，同时生成高质量视频；在客观指标与主观评测上优于现有SOTA与强基线。

Conclusion: 在语义空间进行全局规划再细化到像素的两阶段扩散框架能有效缓解冗余与计算瓶颈，特别适合长视频生成，具有更快收敛与更高质量，优于主流VAE潜空间直接建模方法。

Abstract: State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.

</details>
