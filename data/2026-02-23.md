<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 提出KPM-Bench数据集与MoPE算法，结合运动学解析与语言解析，提供细粒度肢体运动标注与评测，并通过无模型依赖的幻觉度量与GRPO后训练显著减少视频描述中的运动幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕模型难以精确描述细粒度的肢体与动作细节，且在以运动为核心的视频上出现严重幻觉（捏造不存在的动作/属性）。缺乏用于细粒度运动理解与幻觉评测的专门数据与方法。

Method: 1) 构建自动标注流水线：将基于运动学的运动计算与语言解析结合，分解并生成复杂人体运动的细粒度文字描述；2) 基于该流水线构建KPM-Bench：含细粒度视频-字幕对、聚焦运动理解的多样化QA对、以及专门评测运动描述幻觉的集合；3) 提出MoPE算法：从文本中精确解析并抽取运动相关属性（动作、肢体、动力学要素等），据此定义与实现不依赖大规模多模态/纯文本模型的幻觉评估指标；4) 将MoPE融入GRPO后训练流程，指导模型减少幻觉。

Result: KPM-Bench为细粒度运动理解提供了开放数据与评测基准；MoPE带来精确、可复现的运动幻觉度量；把MoPE用于GRPO后训练可显著降低视频字幕模型的运动幻觉并提升可靠性。

Conclusion: 通过运动学+语言学的解析、专用数据基准与无模型依赖的幻觉评测，再配合MoPE引导的GRPO训练，能有效缓解运动类视频字幕的细粒度缺失与幻觉问题，提升模型对肢体级动态的描述能力与可信度。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 提出3D-HIW数据集与CLUTCH系统（含SHIFT VQ-VAE与几何细化），在野外手部动作的文本↔动作双向任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本到手部动作/手动画描述方法依赖室内采集、小规模且情景受限的数据，难以扩展到“野外”环境；同时模型在动作保真度与文本—动作对齐上表现不足。

Method: 1）构建3D-HIW：用VLM自动生成文本描述并结合先进3D手追踪器，从大规模第一人称动作视频中提取并配对32K条3D手部动作与文本。2）提出CLUTCH系统：包含SHIFT——一种部件/模态解耦的VQ-VAE对手部动作进行离散化标记以提升泛化与重建；以及几何细化阶段——在LLM生成后对解码的手部参数直接施加重建损失进行联合监督微调，提高动画质量与对齐。

Result: 在文本到动作与动作到文本两项任务上取得SOTA；首次建立可扩展“野外”手部动作建模基准。

Conclusion: 数据与方法共同推动了野外手部动作建模：通过自动化标注和SHIFT离散化提升泛化与重建，再配合几何细化提升保真与对齐，实现端到端、更可扩展的手部动作生成与理解；代码数据模型将开源。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: 提出PRISM自监督框架，通过边缘图与内在分解的明暗信息引导几何学习，在结肠镜单目深度与位姿估计上达SOTA，并给出关于数据选择和帧率的实践洞见。


<details>
  <summary>Details</summary>
Motivation: 结肠镜导航需要准确的单目深度与位姿估计以减少盲区与漏检，但受纹理缺失、复杂光照、器官形变及缺乏可靠体内真值数据的限制，现有方法效果受限。

Method: 提出PRISM：1) 结构引导——利用学习型边缘检测器（DexiNed/HED）生成细薄高频边界的边缘图；2) 光照先验——采用内在图像分解模块将图像分离为反射率与阴影/照明（luminance decoupling），从阴影线索中学习深度；3) 自监督几何学习框架，将上述先验融入重投影/光度一致性与几何约束中以联合估计深度与相机位姿；并进行消融与跨数据评测。

Result: 在多个真实与合成数据集上取得SOTA性能；系统性消融显示：真实视频的自监督训练优于在高拟真体模数据上的有监督训练；视频帧率对性能极其关键，需要针对数据集进行帧采样策略优化。

Conclusion: 将边缘与内在明暗先验融入自监督单目深度/位姿学习能显著提升结肠镜场景的几何估计效果。实践上应优先使用真实数据的自监督训练，并精心选择视频帧率/采样以获得高质量训练对。

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net，通过从H&E直接映射到IHC潜在表征并结合临床先验的轻量正则，实现 HER2 评分的高效准确预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: IHC 多步染色耗时昂贵且并非随处可得；像素级虚拟IHC虽然可行但计算重、易产生伪影并传播诊断错误，亟需能直接从H&E高效可靠预测HER2的方案。

Method: 构建潜在引导的双流网络（LGD-Net）：以IHC编码器为教师，训练学生网络将H&E形态学特征映射到IHC分子潜在空间（跨模态特征“幻化”），并通过两个辅助正则任务注入领域知识（细胞核分布、细胞膜染色强度）以保证临床相关性；最终用H&E单模态高效推理完成HER2评分。

Result: 在公开BCI数据集上取得SOTA，显著优于基线，同时推理高效，仅需H&E输入。

Conclusion: 跨模态潜在空间对齐+领域知识正则可避免像素级生成伪影与高算耗，从而更准确、高效地从H&E预测HER2水平，具备临床可用潜力。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一个无需额外训练或仅需轻量LoRA微调的文本引导遥感分割方案：用CLIP选择SAM的掩码提案以做零样本开放词汇分割，并用大模型（GPT-5 或 LoRA调优的 Qwen-VL）生成点击提示驱动SAM实现指代与推理分割；在19个遥感基准上达到SOTA或强竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本文本引导遥感分割多依赖可训练模块，降低泛化和实用性。作者想验证：是否可仅依赖现成的视觉/多模态基础模型，在完全不训练或极少训练下实现高质量的遥感分割（开放词汇、指代、推理）。

Method: 两条管线：1) 对比式：SAM生成网格提案→CLIP计算图文相似度筛选掩码，实现完全零样本开放词汇语义分割。2) 生成式：利用大语言/多模态模型（零样本GPT-5，或进行LoRA微调的Qwen-VL）从文本生成点击提示（prompt points），喂给SAM以完成指代与推理分割；可选轻量LoRA调优提升效果。

Result: 在19个遥感基准上（涵盖开放词汇、指代、推理任务）进行广泛实验：对比式管线在零样本OVSS上达到SOTA；生成式管线支持复杂指代与推理分割，LoRA调优的Qwen-VL表现最佳。

Conclusion: 仅用现成基础模型即可实现强大的文本引导遥感分割；CLIP+SAM在零样本开放词汇上达SOTA，生成式点击提示进一步支持指代与推理；轻量LoRA可带来额外收益，具备良好泛化与实用价值。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: 提出VidEoMT：纯编码器的ViT视频分割模型，无需专用跟踪模块，依靠“查询传播+查询融合”，在保持精度的同时显著加速（最高160 FPS，5–10倍更快）。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频分割依赖逐帧分割器+复杂跟踪模块，带来架构复杂与计算开销；而大规模预训练的纯ViT在图像分割上已足够强，启发作者探索去除专用跟踪模块的可能。

Method: 构建仅含ViT编码器的Video Encoder-only Mask Transformer（VidEoMT）。核心两点：1）轻量级查询传播：将前一帧的查询复用到当前帧，实现时序信息传递；2）查询融合：将传播的时序查询与一组时间不变的可学习查询融合，兼顾记忆与对新内容的适应性。无需显式跟踪头或复杂时序模块。

Result: 在保持具有竞争力的分割精度下，显著提升速度：相较含跟踪模块的方法快5–10倍；在ViT-L骨干上最高可达160 FPS。

Conclusion: 通过“编码器-only + 查询传播/融合”的简单设计，VidEoMT在不引入跟踪模块复杂性的前提下，获得了类似跟踪器的优势，实现高效且准确的在线视频分割；代码已开源。

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 该工作提出首个视频检索领域的查询性能预测（VQPP）基准，涵盖两套文本到视频数据集与两种CBVR系统，共56K查询与51K视频，提供可复现实验划分，并系统评测了多种检索前/后预测器；结果显示检索前预测器已具竞争力，并可作为奖励模型结合DPO训练LLM进行查询重写。


<details>
  <summary>Details</summary>
Motivation: QPP在文本与图像检索中应用广泛（如查询重写、扩展、系统选择），但在内容为本的视频检索（CBVR）上研究稀缺。缺乏统一可复现的基准阻碍了方法比较与进展，且无法评估在检索前即可启用的应用场景。

Method: 构建VQPP基准：包含两套text-to-video数据集与两种CBVR系统，提供训练/验证/测试划分；实现与评测多种预检索（pre-retrieval）与后检索（post-retrieval）的性能预测器；进一步将最佳预检索预测器作为奖励模型，通过直接偏好优化（DPO）训练LLM用于查询重写。

Result: 在VQPP上，预检索预测器表现与后检索方法具有竞争性，能在不运行检索的情况下提供有用的性能预估；将其作为奖励模型的DPO训练能推动LLM在查询重写任务中的效果（摘要暗示成功应用）。

Conclusion: VQPP为视频域QPP提供首个标准化评测平台，验证了预检索预测器的实用性，并展示其在下游（如LLM驱动的查询重写）中的价值；发布数据与代码以促进复现与后续研究。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 论文质疑Liu与Szirányi手势识别方法的评估有效性，指出其使用逐帧随机划分导致同一受试者样本泄漏到训练与测试集，从而产生近乎完美但失真的准确率。作者通过混淆矩阵、学习曲线与数据集构造分析，证明该评估并未检验对未见个体的泛化能力，呼吁采用受试者独立划分，尤其在UAV-人机交互等应用中。


<details>
  <summary>Details</summary>
Motivation: 现有报告的“高准确率”可能因评估协议缺陷而高估性能；实际应用（如无人机与人互动）需要对未见个体的稳健泛化，因此有必要审视并纠正评估方法。

Method: 对原方法的评测流程进行方法学审查：分析其逐帧随机训练/测试划分；对照已发表的混淆矩阵与学习曲线；梳理数据集构造方式，判断是否存在跨主体样本混入从而导致数据泄漏与偏乐观评估。

Result: 发现近乎完美的精度源于帧级随机划分引发的严重数据泄漏；当前评估并未真正衡量对未见个体的泛化能力。

Conclusion: 必须采用受试者独立的数据划分与更严格的协议来评估基于视觉的手势识别方法，特别是在需识别未见操作者的实际应用场景（如UAV-人机交互）。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出一种端到端长视频理解框架，包含基于信息密度的自适应采样器与自编码器式时空压缩器，并与多模态大模型集成，在长视频与常规视频基准上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 长视频包含大量冗余帧，现有方法受限于内存难以纳入足够帧，且难从海量输入中提炼判别信息，导致效率与效果受限。

Method: 设计两大组件并端到端训练/集成：1) 自适应视频采样器（AVS），依据信息密度动态选取关键帧/片段以适配不同时长与内容密度；2) 时空视频压缩器（SVC），基于自编码器对时空特征进行高比例压缩，尽量保留判别信息；最终与多模态大语言模型（MLLM）融合进行理解与推理。

Result: 在多个基准上表现出色，尤其在长视频理解任务上，同时在常规视频基准也具竞争力，显示出良好的泛化与效率。

Conclusion: 通过信息自适应采样与时空压缩，实现高效且有效的长视频理解，在内存受限条件下依旧能抓取关键信息，具备通用性与实用价值。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 论文评估多种最新视觉-语言模型（VLM）在细粒度图像分类任务上的表现，发现与在通用VQA等任务上的强势形成对比，并通过消融分析找出影响细粒度能力的关键因素：视觉编码器与预训练设置。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在VQA、文档理解、多模态对话等基准上显著进步，但在需要精细视觉知识的传统图像分类上表现落后。作者想解释这种“能力断层”的原因，并给出改进细粒度视觉理解的方向。

Method: 对大量近期VLM在多种细粒度分类基准上系统评测；开展消融实验，分别替换/提升LLM与视觉编码器，并比较冻结/解冻语言模型权重的预训练策略对结果的影响。

Result: 更强的LLM会均匀提升各类基准分数；更强的视觉编码器会不成比例地更大幅提高细粒度分类表现；预训练阶段对细粒度能力至关重要，尤其当预训练中解冻语言模型权重时，细粒度表现显著改善。

Conclusion: 细粒度视觉理解的瓶颈主要在视觉表征与预训练策略而非LLM推理能力。要提升VLM的细粒度与视觉中心能力，应优先强化视觉编码器、并在预训练中解冻语言模型以促进跨模态对齐与细节学习。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [11] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该论文提出利用极稀疏多模态测距（雷达/激光雷达）重建稠密深度，用作扩散式单图新视角合成的几何条件，从而显著提升几何一致性与画质，尤其在低纹理、遮挡与恶劣天气下。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的单图新视角合成通常依赖单目深度，但单目深度在低纹理、恶劣天气、强遮挡等真实场景下不稳定，限制了合成质量与时空一致性。作者希望引入更可靠的几何先验来克服单目深度的脆弱性，同时保持与现有扩散管线的兼容。

Method: 提出多模态深度重建框架：将深度在角域建模，并采用局部高斯过程（Localized GP）从极稀疏的测距（汽车雷达或LiDAR）推断稠密深度，同时显式输出不确定度。在计算上高效，并能在观测稀缺区域量化不确定性。将重建的深度与不确定度作为现有扩散渲染管线的几何条件，直接替换单目深度，无需修改生成模型。

Result: 在真实多模态自动驾驶场景上实验，使用稀疏测距重建的深度替代视觉深度，显著提升单图新视角视频生成的几何一致性与视觉质量，优于仅视觉基线，且在极端稀疏传感条件下仍有效。

Conclusion: 可靠的几何先验对扩散式视角合成至关重要。即便在极稀疏水平下，多模态测距结合局部GP的深度重建能带来实用收益，并可无缝集成到现有扩散管线。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [12] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: 提出ZACH‑ViT：去除位置嵌入与[CLS]标记，通过对补丁表示做全局平均实现置换不变；在少样本MedMNIST上，小模型（0.25M参数、从零训练）在空间先验弱的数据集上更有优势，推理快、适合资源受限临床部署。


<details>
  <summary>Details</summary>
Motivation: 传统ViT依赖固定空间先验（位置嵌入与[CLS]标记），在空间布局弱或不一致（医学影像与边缘设备场景常见）时可能妨碍泛化。希望以更匹配数据结构的归纳偏置替代通用先验，并满足临床资源受限与小样本训练约束。

Method: 设计ZACH‑ViT：1) 移除位置嵌入与[CLS]标记（“Zero‑token”指移除聚合标记与位置编码，补丁token保留）；2) 通过对patch表示的全局平均池化获得置换不变聚合；3) 采用自适应残差投影，保证紧凑设置下的训练稳定与严格参数预算。

Result: 在七个MedMNIST数据集、严格少样本协议（每类50样本、固定超参、5个随机种子）下评估。ZACH‑ViT（0.25M参数、从零训练）在BloodMNIST表现最优，并在PathMNIST上与TransMIL具竞争力；在具有强解剖先验的数据集（OCTMNIST、OrganAMNIST）优势减弱。总体在无预训练条件下取得有竞争力表现，且推理亚秒级。

Conclusion: 当数据的空间先验弱或不稳定时，去除固定空间先验并采用置换不变聚合可带来更好泛化；与其追求通用基准最优，不如让模型的归纳偏置与数据结构对齐。ZACH‑ViT体量极小、无需预训练、推理快速，适合资源受限的临床部署。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [13] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: 提出ROCKET：一种残差导向的多层表示对齐框架，用共享投影器在多层对齐2D VLA与强3D视觉模型，配合套娃式稀疏激活与无训练层选择，大幅降低算力（≈4%）仍达接近SOTA（98.5% on LIBERO），并在多数据集与多VLA上泛化良好。


<details>
  <summary>Details</summary>
Motivation: 现有VLA多依赖2D预训练，缺乏3D空间理解；常见的表示对齐只在单层监督，未充分利用深度信息；而朴素的多层对齐会产生梯度干扰，影响收敛与性能。需要一种既能充分利用多层信息、又能避免梯度冲突、并具备高效算力开销的方法。

Method: 提出ROCKET：将多层对齐视为“残差流对齐”。以强大的3D视觉基础模型作为教师，用一个“共享投影器”实现对VLA骨干多层与3D模型多层的层不变映射，减轻跨层梯度冲突；理论与实证表明共享投影器足够且优于以往设计。进一步引入“套娃（Matryoshka）式稀疏激活”来在共享投影器中平衡多重对齐损失，并配合训练自由（training-free）的层选择策略以降低计算。

Result: 在LIBERO上仅用约4%的计算预算即可达到98.5%的SOTA成功率；在LIBERO-Plus与RoboTwin等基准上同样表现优异；对多种VLA模型均有效，展现出良好的泛化与效率。

Conclusion: 多层残差导向对齐+共享投影器是有效且高效的VLA-3D对齐范式；通过稀疏激活与无训练层选择，可在极低算力下接近或达到SOTA，并具备跨数据集与模型的稳健泛化。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [14] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出记忆驱动的图像质量评估框架MQAF，构建失真模式记忆库，并在有/无参考两种模式间自适应切换，降低对高质量参考图的依赖，在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有全参考IQA虽精度高，但高度依赖参考图质量，现实中常无理想参考；人类视觉具长期记忆并可基于记忆评估质量，启发引入“记忆库”以缓解对参考的依赖并统一FR/NR场景。

Method: 构建包含多类失真模式的记忆库；设计双模式评估：1) 有参考时，自适应加权参考信息，并将失真图与记忆库失真模式比对，生成参考引导的质量分；2) 无参考时，仅依赖记忆库从失真模式相似性推断质量；核心在于记忆检索与动态权重切换策略。

Result: 在多个数据集上，MQAF在FR与NR任务均优于现有SOTA，呈现更强的泛化与鲁棒性（文摘层面未给出具体数值）。

Conclusion: 利用记忆库和双模式切换，MQAF降低对高质量参考的依赖，实现统一的FR/NR评估并取得领先性能，表明记忆驱动机制在IQA中的有效性。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [15] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: 提出MUOT_3M多模态海洋目标跟踪基准（300万帧、3030视频、27.8小时，含多属性与语言标签），并基于此提出MUTrack：将多模态知识蒸馏到单模态的高效跟踪器，在五个基准上显著超越SOTA并可24FPS实时运行。


<details>
  <summary>Details</summary>
Motivation: 海下环境受颜色畸变、浑浊与低能见度影响严重，现有数据集规模小且仅RGB，难以支持鲁棒的水下目标跟踪研究与实用部署，亟需大规模、多模态且多样化的数据与能落地的算法。

Method: 1) 构建MUOT_3M：含约300万帧/3030段视频/27.8小时，提供同步的RGB、估计增强RGB、估计深度与语言描述，标注32种跟踪属性与677细粒度类别，并由海洋生物学家验证。2) 提出MUTrack：以SAM为基础的“多模态训练→单模态推理”追踪框架，包含视觉几何对齐、视觉-语言融合，以及四层次知识蒸馏，将多模态信息迁移到仅RGB（或单模态）学生模型，实现实际部署效率。

Result: 在五个水下跟踪基准上，MUTrack相较最强SOTA，AUC最高提升8.40%，精度最高提升7.80%，推理速度达24 FPS。

Conclusion: MUOT_3M为水下跟踪提供了前所未有的大规模多模态数据基础；MUTrack通过多模态到单模态的知识迁移，在保持实时性的同时显著提升鲁棒性与精度，为可扩展训练与可部署应用奠定新范式。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [16] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 提出L-AVC任务：用多模态LLM编辑图像的“主观情绪”，并通过高效情绪转换(EIC)与精确内容保留(PER)模块，实现高效且精确的情绪操控；在自建数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制主要关注语言/布局/边缘等客观对齐，忽视图像的主观情绪，且缺乏面向情感的通用基础模型。需要一套能在不破坏与情绪无关内容的前提下，可靠地编辑图像情绪的范式与方法。

Method: 定义LLM-centric情感视觉定制(L-AVC)：利用多模态LLM在图像编辑中修改情绪。提出EPEM框架，包含两模块：1) EIC高效情绪转换，对编辑前后语义进行情绪层面对齐；2) PER精确外部情绪保持，最大化保留与情绪无关的图像内容。

Result: 在自构L-AVC数据集上进行全面实验，所提EPEM在效率与精度上均显著优于多种SOTA基线，验证了方法在情绪操控与内容保留方面的优势。

Conclusion: 情绪信息对视觉定制至关重要。EPEM能够高效且精确地操控图像主观情绪，同时保持与情绪无关的语义与外观，为基于多模态LLM的情感图像编辑提供了通用范式与有效技术路径。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [17] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 本文提出面向安防视频的“深层安全视频理解”（DeepSVU）新任务，不仅检测/定位威胁，还要归因并评估威胁成因；为此设计统一物理世界正则化MoE（UPRM），含UPE模块与PTR正则，在两套指令数据集上优于多种视频LLM与非VLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有安防视频理解主要停留在威胁事件的检测与定位，缺乏对威胁成因的生成与评估能力；同时，尚未系统建模并权衡粗到细的物理世界信息（人类行为、物体交互、场景背景）以支持更深入的安防理解。

Method: 提出DeepSVU任务，并针对两大挑战提出UPRM方法：1) 统一物理世界增强MoE（UPE）块，用多专家机制融合/刻画从粗到细的人-物-景信息；2) 物理世界权衡正则（PTR），自适应调节各信息因子的权重以取得最优权衡。在自建指令数据集（UCF-C instructions与CUVA instructions）上训练/评测。

Result: 在两套DeepSVU指令数据集上，UPRM在识别、定位、归因与成因评估等指标上均优于多种先进Video-LLMs及非VLM基线，显示其对物理世界信息的有效捕获与利用。

Conclusion: 粗到细的物理世界信息对深层安防视频理解至关重要；UPRM通过UPE与PTR有效建模并自适应权衡这些因素，显著提升DeepSVU性能，验证了新任务设定与方法的有效性。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [18] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出UAOR：在VLA推理中利用“动作熵”检测不确定性，按需将观测信息通过注意力检索注入下一层FFN，零训练、即插即用地提升动作生成的可靠性与泛化。


<details>
  <summary>Details</summary>
Motivation: VLA常用VLM作为骨干，但为提升精度常依赖额外深度/点云或检测器等辅助模块，带来昂贵数据与训练成本。受LM中FFN可作键值记忆的启发，作者希望不加训练与外部传感的前提下，让模型在不确定时更好“看”观测，从而更稳健地执行任务。

Method: 提出不确定性感知的观测再注入（UAOR）：在推理时逐层监控语言模型层的动作分布熵（Action Entropy）；当熵高于阈值，说明不确定，则用注意力检索关键观测表征，并把其作为键值写入下一层FFN（将观测信息重插入“记忆”）；无需额外训练或数据，作为外接模块接入多种VLA。

Result: 在多种仿真与真实机器人操作任务上，UAOR以很小开销一致性提升多款VLA模型的表现；无需深度/点云或外部检测器等额外线索与模块。

Conclusion: UAOR是一个训练-free、可插拔的推理时增强机制，通过基于不确定性的观测再注入，让VLA更关注视觉观测，生成更自信且忠实的动作，具有通用性与实用性。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [19] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出DCAG，在DiT多模态注意力中同时操控Key与Value通道，实现训练免的编辑强度控制，显著优于仅Key操控，特别在局部编辑任务上（LPIPS显著下降）。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式图像编辑（基于DiT）只能在注意力的Key空间操控路由，忽视了决定特征聚合的Value空间，导致编辑强度控制粗糙、难以在保真与编辑幅度间精细权衡。作者发现Key与Value投影存在“bias-delta”结构，为联合操控提供了结构性依据。

Method: 提出Dual-Channel Attention Guidance（DCAG），训练免：同时在Key通道（决定关注位置）与Value通道（决定聚合内容）施加引导。理论分析：Key通道通过softmax形成非线性、粗粒度控制；Value通道通过线性加权求和提供细粒度补充。将两者参数化为二维(δ_k, δ_v)，实现可调的编辑-保真权衡。

Result: 在PIE-Bench（700图、10类编辑）中，DCAG在所有保真指标上统一优于仅Key方法。局部编辑任务收益最大：如目标删除LPIPS降低4.9%，目标添加LPIPS降低3.2%。

Conclusion: 利用DiT注意力中Key/Value的bias-delta结构，DCAG以训练免、双通道联合引导，实现更精细的编辑强度与保真度控制，优于单通道方案，尤其在局部编辑场景中表现突出。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [20] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST通过将动作名称分解为空间与时间属性，并在视频特征学习中分别注入这两类知识，构建多粒度原型，从而在少样本动作识别上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统FSAR通常仅使用动作类别名作为语义上下文，信息过于粗糙，难以为新颖类别提供足够的空间与时间常识，从而限制对细粒度时空概念的捕获。

Method: 提出DiST的“分解-注入”两阶段框架：1) 分解阶段：利用大语言模型将原始动作名解耦为多样的空间（对象/部位/场景等）与时间（阶段/顺序/动态关系等）属性描述，形成动作相关常识库；2) 注入阶段：设计空间知识补偿器（SKC）与时间知识补偿器（TKC）。SKC在空间知识引导下自适应聚合关键patch token，学习可解释的对象级原型；TKC借助时间属性辅助建模帧间关系，学习帧级原型与多样的时序模式；最终形成多粒度原型用于匹配与分类。

Result: 在五个标准FSAR数据集上取得最新最优（SOTA）结果，显示出对细粒度空间细节与多样时序模式的更强捕获能力与可解释性。

Conclusion: 将LLM解耦得到的空间与时间常识分别注入视觉模型，可有效缓解语义上下文不足的问题，学习到更具表达力与可解释性的多粒度原型，从而显著提升少样本动作识别性能。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [21] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard提出一种面向城市级、去中心化相机网络的人重识别框架，结合拓扑感知Transformer、分散自适应度量学习、空间条件注意力和差分隐私嵌入，提升跨视角/遮挡/跨域鲁棒性，并在严格隐私预算下实现可调隐私-效用权衡，实验在Market-1501等基准与数据库级检索场景中优于强基线且吞吐更高。


<details>
  <summary>Details</summary>
Motivation: 城市级分布式监控需要在不共享原始图像的前提下进行身份检索，面临视角变化、遮挡、域偏移带来的外观剧变，以及合规要求带来的隐私约束。现有ReID方法要么依赖精确标定与集中式数据，要么隐私保护不足或影响性能，缺少同时兼顾拓扑先验、鲁棒性与差分隐私的可部署方案。

Method: 提出CityGuard：1) 分散自适应度量学习（dispersion-adaptive metric learner），依据类内特征离散度自适应调节实例级margin，压缩类内方差、拉开类间距离；2) 空间条件注意力（spatially conditioned attention），在图式自注意力中注入粗几何（GPS/楼层平面/部署拓扑），实现仅用粗先验的投影一致跨视图对齐，无需高精度标定；3) 差分隐私嵌入映射结合紧凑近似索引（如PQ/HNSW变体），在严格DP会计下提供可调隐私预算与高效检索部署。整体为拓扑感知Transformer骨干，端到端学习稳健描述子并支持去中心化部署。

Result: 在Market-1501及其他公开基准上，检索精度较强基线显著提升；在城市级数据库规模的检索实验中，达到更高的查询吞吐与稳定的延迟；在不同隐私预算下展示可调的隐私-效用曲线，保持跨视角、遮挡与跨域条件下的鲁棒性。

Conclusion: CityGuard在不共享原始图像的前提下，实现了面向城市级分布式相机的人重识别，兼顾拓扑先验对齐、鲁棒描述子学习与差分隐私保护，实证表明其在精度与效率上优于强基线，具备隐私关键场景的实际部署潜力。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [22] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: 提出TCA-T2M：通过引入时间一致性感知的空间VQ-VAE与遮罩式运动Transformer，并辅以运动学约束，提升文本到人体动作生成的跨序列时间一致性、语义对齐与物理合理性；在HumanML3D与KIT-ML上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段离散化表示的T2M方法忽视同一动作在不同样本间共享的时间结构（跨序列时间一致性），导致语义错配与不合理运动（抖动、关节穿模等）。需要一种能对齐跨样本时间结构并保持物理可行性的生成框架。

Method: 1) 提出TCaS-VQ-VAE：在空间VQ-VAE离散编码基础上引入时间一致性感知，对不同序列间的时间结构进行对齐；2) 文本条件的遮罩式运动Transformer进行生成，利用掩码预测提升稳健性；3) 运动学约束模块，在生成阶段缓解离散化伪影、保证物理合理性。

Result: 在HumanML3D与KIT-ML基准上取得SOTA表现（更好的文本-动作对齐与连贯性、物理可行性指标）。

Conclusion: 跨序列时间一致性是T2M生成的关键。通过时间一致性感知的离散表示与约束化生成，可显著提升鲁棒性与连贯性，优于既有两阶段方法。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [23] [3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis](https://arxiv.org/abs/2602.18064)
*Ziyue Wang,Linghan Cai,Chang Han Low,Haofeng Liu,Junde Wu,Jingyu Wang,Rui Wang,Lei Song,Jiang Bian,Jingjing Fu,Yueming Jin*

Main category: cs.CV

TL;DR: 提出3DMedAgent：用2D多模态大模型作为中枢、调用异构视觉/文本工具并维护结构化长期记忆，实现无需3D特定微调的通用3D CT分析；并发布DeepChestVQA基准。跨40+任务优于通用/医疗/3D专用MLLM。


<details>
  <summary>Details</summary>
Motivation: 当前3D CT分析要么各自为政的任务特化模型，要么端到端一次性输出，难以累积多步证据；而现有MLLM主要面向2D，难以有效处理体数据与从感知到临床理解的链式推理。需要一种把2D MLLM的通用推理/工具编排能力与3D体数据感知结合的统一框架。

Method: 提出3DMedAgent：以MLLM为智能代理，分层把复杂3D任务分解为可处理子任务——从全局到局部、从3D体到关键信息2D切片、从视觉证据到结构化文本；通过协调异构视觉/文本工具获得中间结果，并以长期结构化记忆聚合这些结果，支持查询自适应、证据驱动的多步推理；同时构建DeepChestVQA用于3D胸部成像感知到理解一体化评测。

Result: 在40余项任务上，3DMedAgent稳定优于通用、医疗和3D专向的MLLM基线，显示其无需3D专用微调即可实现强大的3D CT分析能力。

Conclusion: 通过代理式工具编排与结构化记忆，2D MLLM可被扩展为通用3D CT助手；DeepChestVQA为统一评测提供基准，工作揭示了通向通用3D临床助手的可扩展路径。

Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.

</details>


### [24] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出一种两阶段训练策略：先用自监督阶段将BEV预测可微重投影到图像，用Mask2Former伪标签与时间一致性训练；再用仅50%有标注数据进行监督微调。在nuScenes上较完全监督基线mIoU提升至多+2.5pp，同时标注与训练时间显著减少。


<details>
  <summary>Details</summary>
Motivation: BEV密集语义图对自动驾驶关键，但多相机方法依赖昂贵且不一致的BEV标注。需要一种减少真实BEV标注依赖、可扩展又能保持/提升性能的方案。

Method: 两阶段：1) 自监督预训练：用BEVFormer产生BEV预测，将其可微重投影回各视角图像，与Mask2Former生成的多视角语义伪标签对齐；加入时间一致性损失约束跨帧稳定。2) 监督微调：只用50%的带标注数据和较少训练轮数进行微调。

Result: 在nuScenes上，相比完全监督基线，mIoU最高提升+2.5个百分点；标注用量减半，整体训练时间最多减少约2/3。展示了伪标签+可微重投影能学到可迁移的BEV特征。

Conclusion: 通过可微重投影结合相机视角伪标签与时间一致性，预训练可学到强BEV先验，使得在少标注微调下仍优于全监督基线，为自动驾驶BEV感知提供更可扩展、低标注成本的路径。

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [25] [Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation](https://arxiv.org/abs/2602.18083)
*Ioannis Kontogiorgakis,Athanasios Askitopoulos,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Fotios Balampanis,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 该文提出在欧洲植被区以10米分辨率估算土壤水分的机器学习框架，融合Sentinel‑1/2与ERA‑5，并以ISMN站点做空间交叉验证；最佳方案R^2≈0.518，基础模型嵌入未优于传统特征；树模型+谱指数是高效可用的方案。


<details>
  <summary>Details</summary>
Motivation: 现有卫星土壤水分产品分辨率>1 km，不足以满足农田级精细管理与监测需求；需在植被覆盖区实现更高分辨率、更具可推广性的SM反演，同时检验大模型表征是否优于手工特征。

Method: 构建10 m SM估算流程：融合Sentinel‑1 SAR、Sentinel‑2光学影像与ERA‑5再分析；以113个ISMN站点（不同植被类型、欧洲范围）进行训练与评估；比较多模态组合与时间匹配策略（含ERA‑5回溯窗口）；采用空间交叉验证以测试地理泛化；对比IBM‑NASA Prithvi基础模型嵌入与传统光谱指数/手工特征；使用树型集成模型进行回归。

Result: 最优的“混合时间匹配”策略（当日S2 + S1下降轨）达到R^2=0.514；加入10天ERA‑5回溯窗口提升至R^2=0.518。Prithvi嵌入与手工特征表现近似（R^2=0.515 vs 0.514），改进可忽略。

Conclusion: 在稀疏标注回归场景中，领域特定光谱指数结合树型集成方法具备竞争力与计算效率，可用于泛欧洲田块尺度的业务化SM监测；当前基础模型嵌入对该任务增益有限，时间匹配与适度气象回溯有助于提升精度。

Abstract: Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.

</details>


### [26] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: 提出DohaScript：一个包含531位书写者、六首恒定dohas的印地语天城文手写大规模并行风格数据集，用于识别、写者鉴别与生成等任务，并提供质量与版面难度标注与基线验证。


<details>
  <summary>Details</summary>
Motivation: 现有天城文字手写数据集规模小、以字符/短词为主、缺乏受控词汇与写者多样性，不能反映实际连续、连写与连笔（shirorekha与合字）特性，限制了现代数据驱动方法。

Method: 收集531名写者以相同6首传统印地语dohas抄写，形成“平行语料”，附带非可识别的人口学元数据；基于客观清晰度与分辨率进行质量筛选；提供页面级版面难度标注；构建基线实验评估质量分层与跨写者泛化。

Result: 基线实验显示质量分层明显，模型对未见写者具有良好泛化，证明数据集可靠且具实用价值。

Conclusion: DohaScript作为标准化、可复现实验基准，填补连续天城文手写低资源场景空白，支持识别、写者鉴别、风格分析与生成建模等研究。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [27] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT 提出一种无需再训练的加速框架，用线性多步预测+纠正、并自适应步长调制来预测DiT在扩散过程中的特征演化，在高动态区间触发校正，显著减少迭代开销，在多种DiT图像与视频生成上将时延最高降至原来的约1/5，且几乎无质量损失。


<details>
  <summary>Details</summary>
Motivation: DiT 的逐步去噪迭代计算代价高；现有训练-free加速多靠跨步复用特征，假设时序稳定，但多步复用会导致潜变量漂移与画质劣化。作者观察到多数扩散轨迹上模型输出随时间平滑变化，因此希望用“预测”替代“复用”。

Method: 将特征演化建模为线性多步（LMS）预测：利用过去若干步的模型输出线性组合外推下一步/多步；在高动态区域加入纠正器以抑制误差累积；通过监控特征变化率进行动态步长调制，自适应调整预测视野（预测步数/跨度），在稳定段大胆外推，在剧变段保守并触发校正。整体为训练-free、即插即用。

Result: 在多种基于DiT的图像与视频生成模型上，PrediT 实现最高约5.54×的时延降低，同时几乎不损失生成质量；与基于缓存复用的方法相比，能更好地避免潜在漂移与可见退化。

Conclusion: 平滑轨迹上的特征可被多步数值方法可靠预测，结合动态调制与纠正机制，可在不改动或再训练模型的情况下显著加速DiT推理并保持保真度。

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [28] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: 论文提出OODBench，一个主要自动化、少量人工验证的基准，用于评估视觉-语言模型在分布外数据上的稳健性，并配套分级提问的自动评估指标。结果显示主流VLM在常见类别的OOD实例上仍明显退化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据很难满足IID假设，VLM对分布外对象处理不当将带来安全风险，但目前缺少系统、有效的OOD评测基准与指标。因此需要可扩展、低人工成本的基准来全面评估VLM应对OOD的能力。

Method: 构建OODBench：自动化流程生成并筛选40K条实例级“OOD实例-类别”对，辅以少量人工核验；设计“从基础到高级”的分级提示问题体系，形成可靠的自动化评估指标，以多难度问题衡量OOD对模型问答表现的影响。

Result: 在OODBench上，现有多种主流VLM在面对OOD实例（即使图像类别常见）时出现显著性能下降，表明其OOD稳健性不足；分级评测揭示不同难度任务受OOD影响的差异。

Conclusion: OODBench为VLM的分布外鲁棒性提供了规模化、可复现的评测基准与指标，暴露了当前模型在常见类别的OOD实例上仍不稳健。作者总结了关键观察与建议，为未来OOD数据获取与评估研究提供方向。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [29] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: 论文比较ViT与CNN及人类在图形感知任务上的表现，发现ViT虽在通用视觉强，但在人类式图形感知一致性上有限，存在感知差距。


<details>
  <summary>Details</summary>
Motivation: 以往CNN在图形感知（如长度、角度、面积等编码的判别）方面已有评估，但ViT在该方向基本空白。随着ViT在视觉任务中崛起，需了解其在可视化解读中的“人类一致性”与可靠性，以指导其在可视化系统与感知建模中的应用。

Method: 复刻并改编Cleveland & McGill的基础视觉判断任务，构造受控图形感知基准；以同一任务对比评测ViT、CNN与人类参与者的准确性/误差，并分析不同视觉编码与任务条件下的表现差异与模式。

Result: ViT在通用视觉表现强，但在图形感知基准上与人类的感知排序与误差结构不一致；相较CNN，ViT并未系统性更接近人类的感知特性，呈现特定编码与任务下的误差模式偏差。

Conclusion: ViT在人类式图形感知上的对齐有限，存在关键感知鸿沟。应谨慎将ViT直接用于可视化与感知建模，并考虑专门训练、归纳偏置或人机混合策略来缩小差距。

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [30] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: 提出BLM-Guard，用于短视频广告的多模态违规审查；结合规则驱动的链式推理与强化学习，面向细粒度、政策导向的审核；在真实广告上优于强基线。


<details>
  <summary>Details</summary>
Motivation: 短视频平台广告包含视觉、语音与字幕等多模态要素，存在夸大、误导、跨模态不一致等欺骗性内容。传统社区安全过滤器过于粗糙，难以满足商业广告合规与政策审查的细粒度需求，因此需要一个能对多模态操控和跨模态偏差进行策略化、可解释审核的系统。

Method: 1) 规则驱动的ICoT数据合成：基于政策规则自动生成结构化场景描述、推理链与标签，降低人工标注成本；2) 监督预训练结合链式推理；3) 强化学习微调：使用由因果一致性与政策遵循构成的复合奖励，并引入critic指导；4) 多任务多模态架构：同时建模单模态操控（如夸张图像）与跨模态失配（如字幕与语音漂移），提升鲁棒性与泛化。

Result: 在真实短视频广告数据上，相比强基线取得更高的准确性、一致性与泛化能力；表现出更稳健的多模态对齐与策略遵从性。

Conclusion: 融合规则与学习的BLM-Guard能以可解释、策略导向的方式识别多模态广告中的欺骗与不合规行为；ICoT数据合成与复合奖励的RL显著降低标注成本并提升审核质量，可为商业广告审核提供更可靠、可扩展的解决方案。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [31] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: 提出DMC后处理模块，利用自监督的数据驱动方式，从文本与“有意失真”的动作中校正生成动作的物理不合理性，同时保持语义一致；在多模型上显著降低FID、提高R-Precision，并减少穿插与悬浮等伪影。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成虽能对齐语义，但常出现物理不合理（脚飘、穿插等），且引入复杂物理引擎或约束代价高、泛化弱。需要一个既能增强物理真实性、又不损害语义且可泛化到任意生成器的轻量后处理方案。

Method: 提出Distortion-aware Motion Calibrator (DMC)：后处理校准器。核心思路是自监督、数据驱动训练：以文本描述与“刻意扭曲”的动作为输入，学习还原为物理合理的动作，同时保持语义。模型学习识别并修复如脚部悬浮、肢体穿插等失真，无需显式物理建模。DMC可无缝接入不同文本到动作生成器作为后置模块。

Result: 将DMC接到多种T2M模型后，物理与语义指标均提升：在T2M与T2M-GPT上FID分别下降42.74%与13.20%，R-Precision达到最高；在高质量模型如MoMask上，穿插减少33.0%，脚部悬浮更接近GT参考，整体物理真实性提升。

Conclusion: DMC作为与模型无关的后处理框架，能够在不引入复杂物理模拟的前提下，显著改进动作的物理合理性并增强语义一致性，适用于多种文本到动作系统，具有通用性与实用价值。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [32] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 论文首次系统研究离散图像分词器（tokenizer）的对抗鲁棒性：提出高效、任务无关的攻击以改变提取的token，并用无监督对抗微调仅训练分词器来显著提升在多任务上的鲁棒性与泛化。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态模型（encoder-only/encoder-decoder/decoder-only）中日益关键，但其相较广泛研究的CLIP编码器，鲁棒性与对抗脆弱性几乎未被探索。若分词器不稳，所有下游任务都会受影响，因此需要系统评估并提升其安全性与稳健性。

Method: 1）攻击：将目标定义为扰动分词器特征从而改变离散token分配；设计计算高效、任务与模型头无关的无监督攻击，可跨分类、检索、描述任务迁移。2）防御：仅微调分词器，保持其他组件冻结，采用无监督对抗训练（无需标签），借鉴鲁棒CLIP的思路，以对抗样本与原样本一致性为训练信号。

Result: 所提攻击在多任务上有效破坏性能，展示了分词器层面的系统性脆弱性。所提无监督对抗微调显著提升对无监督与端到端监督攻击的鲁棒性，并能泛化到未见任务与数据。

Conclusion: 分词器的鲁棒性是多模态系统安全与性能的关键瓶颈。通过仅对分词器进行无监督对抗微调，可在不依赖标签与下游任务的前提下大幅增强泛化鲁棒性，为构建安全的多模态基础模型提供了可扩展路径。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [33] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: 提出DEIG框架，通过实例细粒度表征与实例掩码注意力，实现对复杂文本的可控多实例生成，在空间一致性、语义准确性与组合泛化上优于现有方法，并可无缝接入扩散模型流程。


<details>
  <summary>Details</summary>
Motivation: 现有多实例生成虽能进行空间布局与属性绑定，但在面对复杂、细粒度文本时容易出现语义理解不足与属性泄漏，难以精准对齐局部描述与实例。

Method: 1) Instance Detail Extractor(IDE)：将文本编码器输出转为紧凑、实例感知的表征，增强每个实例的细粒度语义。2) Detail Fusion Module(DFM)：基于实例的masked attention，限制跨实例的信息流以避免属性泄漏，并融合细节以提高局部一致性。3) 数据与评价：构建带有细致、组合型实例描述的数据集（由VLM生成），并提出含区域级标注与多属性提示的DEIG-Bench。4) 作为即插即用模块接入标准扩散式生成管线。

Result: 在多基准上，DEIG在空间一致性、语义准确性与组合泛化方面持续超越现有方法；可与主流扩散模型兼容，显示良好可移植性。

Conclusion: 细粒度实例表征与实例级掩码注意力是提升多实例生成可控性与语义对齐的关键。DEIG提供了通用、可插拔的方案，并配套高质量数据与新基准以推动该方向的发展。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [34] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: 提出LOTS框架：在扩散模型中结合全局草图与多组局部“草图-文本”对，通过多级条件编码与注意力引导，提升时尚图像生成的结构遵从与局部语义细节；并发布含多对文本-草图标注的Sketchy数据集（含专业与野外子集），方法优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 早期时装设计依赖草图表达结构与轮廓，但难以细化材质与风格；文本能补足细节但易破坏草图结构。现有多模态生成方法难以同时保证全局结构一致与局部语义精确控制，缺乏能对单图提供多对局部草图-文本标注的数据集。

Method: 提出LOTS：1）多级条件阶段（Multi-level Conditioning）在共享潜空间中独立编码局部特征，同时维持全局结构协调；2）扩散对引导阶段（Diffusion Pair Guidance）在扩散去噪多步过程中，以注意力机制融合全局与局部条件，实现局部语义与全局结构的联合控制。并构建Sketchy数据集：每张图配多对文本-草图（专业高质量与“野外”非专家子集）。

Result: 在专业与野外设置上，LOTS在结构遵从与局部语义丰富度方面优于SOTA；方法对非专家草图的变异与缺陷更具鲁棒性。

Conclusion: 将全局草图与多组局部草图-文本通过多级条件与注意力引导融合，可在扩散生成中兼顾结构与细节；数据集与代码开源，推动时装多模态生成研究。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [35] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: 提出Diff2DGS，两阶段方法：先用基于扩散的视频修复补全被器械遮挡的组织，再用带可学习形变模型的2D Gaussian Splatting重建动态组织，兼顾外观与几何；并在SCARED做深度定量。性能在EndoNeRF与StereoMIS上SOTA，且强调仅优化图像质量不足以保证3D准确，加入深度优化以提升几何保真。


<details>
  <summary>Details</summary>
Motivation: 现有内镜/机器人手术重建在遮挡区域质量差，深度/几何准确性评估不足（缺少3D GT），尽管GS类方法具备实时性但对形变与遮挡处理不佳，且研究多停留在图像指标，无法保证真实几何。

Method: 两阶段框架Diff2DGS：1) 扩散式视频修复模块利用时间先验，对被器械遮挡的组织进行时空一致的补全；2) 改进2D Gaussian Splatting，引入Learnable Deformation Model以拟合组织动态形变与解剖几何；此外，设计/采用深度质量优化与在SCARED上的深度定量评估，超越仅用图像指标的训练与评价。

Result: 在EndoNeRF和StereoMIS上外观重建达到SOTA（PSNR分别38.02 dB与34.40 dB）；在SCARED上进行深度准确性量化并优于现有方法；证明单纯优化图像指标并不等价于最优3D几何。

Conclusion: Diff2DGS通过遮挡补全+可学习形变的2DGS稳健重建可变形手术场景，实现实时且更可靠的外观与几何；引入深度导向优化与评测，提升几何保真并为手术机器人提供更可信3D信息。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [36] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: 提出 Luminance-GS++，在多视图、复杂光照下实现稳健的新视角合成；通过全局视角自适应亮度校正与局部像素残差细化，并配合无监督一致性损失，在低光、过曝与色度变化场景达 SOTA，同时保持3DGS的实时渲染与显式表示。


<details>
  <summary>Details</summary>
Motivation: NeRF/3DGS 等 NVS 方法依赖光度一致性假设，但真实多视图采集中光照变化、传感器与 ISP 差异导致光度与色度不一致，破坏假设，造成重建与渲染质量下降。需要一种在不更改底层显式表示的前提下，鲁棒应对复杂照明并校正跨视角/相机的光度偏差的方法。

Method: 基于 3D Gaussian Splatting 的框架：1) 全局层面引入视角自适应的亮度（lightness）调整模块，统一不同视角下的整体亮度；2) 局部层面采用像素级残差细化实现精确颜色校正；3) 设计无监督训练目标，联合约束亮度校正与多视图几何、光度一致性；4) 不修改 3DGS 的显式表示与管线，从而保留其实时渲染效率。

Result: 在多种具有挑战的场景（低光、过曝、复杂亮度与色度变化）上取得 SOTA 表现，相比现有方法提升重建保真度与渲染质量，并保持实时性。

Conclusion: Luminance-GS++ 能在多样照明条件下显著缓解多视图光度/色度不一致问题，通过全局+局部颜色校正与无监督一致性约束，提升 NVS 的稳定性与质量，同时保留 3DGS 的显式表示与实时优势。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [37] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 提出一种基于高斯-拉普拉斯（LoG）运算子的双参数G-LoG双滤波（bi-filtration），在医学体数据上生成更适配多参数持续同调的拓扑特征；理论上证明对应模块在最大范数下的稳定性；在MedMNIST实验证明其优于单参数滤波，并用简单MLP即可达到与复杂深度模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 单参数滤波在复杂医学影像中难以同时捕获多尺度与边界敏感的几何/拓扑信息；LoG在增强边界方面有效，但尚未系统用于多参数持续同调。作者希望构建对医学体图像友好的双参数滤波，并提供稳定性保证，验证其在真实数据上的判别力。

Method: 将体数据建模为有界函数；利用高斯核和平滑尺度与LoG响应构成双参数（G-LoG）滤波，生成多参数持续同调特征；从范畴视角定义对应的多参数持续模；证明由G-LoG得到的模块的交错距离对输入函数的最大范数扰动稳定；在MedMNIST上提取拓扑特征，用MLP分类，并与单参数滤波及AutoML/ResNet/AutoKeras/auto-sklearn比较。

Result: G-LoG双滤波生成的拓扑特征在MedMNIST多个任务上显著优于单参数滤波；基于这些特征的简单MLP分类器达到与强深度学习基线相当的性能，显示出更高的数据效率与可解释性。

Conclusion: G-LoG双滤波为医学影像提供了稳定且有效的多参数拓扑表示，在无需复杂网络的情况下即可取得竞争性性能；为将LoG边界增强与多参数持续同调结合提供了理论与实践路径，并可推广到更多体数据场景。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [38] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 提出一种“退化感知”的自我觉察目标检测框架：通过在特征空间中学习退化流形，使模型在不依赖置信度与退化标签的情况下，识别输入是否偏离清洁分布，并在多种分布移位下稳健泛化。


<details>
  <summary>Details</summary>
Motivation: 现有检测器在模糊、噪声、压缩、恶劣天气、分辨率变化等退化下会静默失效；安全关键场景需要在给出预测的同时判断输入是否仍处于模型的名义工作域，传统依赖置信度的方法不足。

Method: 在常规模型主干上加入轻量嵌入头，进行多层对比学习：相同退化组合的图像在表示空间被拉近，不同组合被拉远，从而按退化类型和强度形成几何有序的“退化流形”。以干净训练样本的嵌入估计“清洁原型”作为名义工作点，输入的自我觉察信号由其与原型的几何偏离度给出；无需显式退化标签或密度建模，且独立于检测置信度。

Result: 在合成腐蚀基准、跨数据集零样本迁移、自然天气分布移位上，表现出强的清洁-退化可分性、跨多种检测器的一致性，以及在语义移位下的稳健泛化。

Conclusion: 基于退化感知的表示几何为自我觉察检测提供了实用且与检测器无关的基础，可在不依赖置信度与退化标签的前提下，稳定识别退化诱发的分布偏移。

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [39] [Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges](https://arxiv.org/abs/2602.18406)
*Minh Dinh,Stéphane Deny*

Main category: cs.CV

TL;DR: 论文探讨在缺乏先验群对称知识的情况下，通过在潜在空间学习等变算子以提升对稀见姿态/尺度/位置组合的泛化能力，在旋转和平移噪声MNIST上实现了OOD分类改进，并指出扩展到复杂数据集的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在遇到训练中少见的群对称变换（非常规姿态、尺度、位置与其组合）时泛化不佳；经典等变网络虽能处理对称变换，但需事先知道变换群，适用性受限。作者希望在不依赖明确先验群结构的前提下，学习能跨对称变换泛化的模型。

Method: 采用一类“在潜在空间学习等变算子”的体系结构：从样例对中学习对称变换在潜在表示上的作用规则，而非在输入空间显式编码固定群；以旋转与平移的噪声MNIST为基准，训练并评估该架构在分布外（未见姿态/位移组合）场景中的分类性能。

Result: 在旋转与平移的噪声MNIST上，该潜在等变方法实现了对OOD样本更稳健的分类，相比传统CNN和需要先验群知识的等变网络，展现更强的跨变换泛化能力。

Conclusion: 在无需显式先验群知识的情况下，学习潜在等变算子可有效提升对对称变换导致的分布外样本的识别；然而将其扩展到更复杂数据集仍面临挑战，如可扩展性、稳定训练与对更丰富变换群的建模。

Abstract: Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.

</details>


### [40] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出一种以人类动作为条件的生成视频世界模型，可根据头部与手部3D姿态生成可交互的第一视角虚拟环境，并通过教师-学生蒸馏实现实时因果交互；用户实验显示相比基线有更好的任务表现与控制感。


<details>
  <summary>Details</summary>
Motivation: XR需要能响应用户真实动作的生成模型，但现有视频世界模型多仅支持文本或键盘等粗粒度控制，难以支撑体现化交互与精细手—物操作。因此需要一种能以头部与手部精细姿态为条件的生成机制，以提升沉浸感、可控性与任务完成效率。

Method: 1) 系统性评估扩散Transformer的条件注入策略；2) 提出适用于3D头姿与逐关节手部姿态的有效条件控制机制，支持精细手—物交互；3) 训练一个双向视频扩散教师模型；4) 将教师蒸馏为因果、可交互的学生模型，实现在线生成第一视角的虚拟环境；5) 人体实验评估。

Result: 用户研究表明：与相关基线相比，所生成的“生成现实”系统带来更高的任务完成表现，并显著提升用户对动作控制量的主观感知。

Conclusion: 以头部与手部3D姿态为条件的视频世界模型能显著提升XR中的交互可控性与任务表现；通过双向教师模型与因果学生模型的蒸馏，可在保持质量的同时实现实时交互生成，优于仅用文本/键盘控制的基线。

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [41] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: 提出CapNav基准，评估VLM在具备特定机体/操作能力约束下的室内导航推理；含5类人机体、45场景、473任务、2365问答；测评13个VLM，发现约束越强性能越差，空间尺度相关障碍尤为困难。


<details>
  <summary>Details</summary>
Motivation: 现有VLN多忽视真实世界中不同代理（人/机器人）在尺寸、机动与交互能力上的差异，导致在现实部署时决策不可靠。需要一个能显式编码与考核“能力条件”的评测框架，以推动VLM具备面向具体机体的可行路径与交互推理。

Method: 构建CapNav基准：定义5类代表性人/机器人代理（含物理尺寸、机动能力、环境交互能力）；收集45个真实室内场景，标注473个能力条件导航任务与2365个QA；以13个主流VLM在该基准上进行系统评测，分析在不同能力约束与障碍类型下的性能变化，重点检验空间尺寸与机动可行性推理。

Result: 随着机动/能力约束收紧，VLM导航表现显著下降；对需空间尺度推理（如通道宽度、台阶高度、转弯半径等）的障碍最为薄弱；即使SOTA模型在这些情形中也频繁失败，体现出对可达性与可通行性的推理不足。

Conclusion: 能力感知对导航至关重要，当前VLM缺乏稳健的机体条件化空间推理。CapNav为社区提供可重复评测平台，呼吁将能力建模、几何/尺度推理与环境交互纳入训练与推理管线，以推动面向具体代理的可部署导航。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [42] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 提出首个可在VR头显上实时流式部署、具备空间感知的对话动作生成方法：基于用户位置与语音，生成与语音同步且面向用户的全身动作与凝视，并可在推理时调节眼神交流强度，达成SOTA质量与>300 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有对话动作生成多仅对齐语音手势，缺乏对用户位置/移动与自然凝视的空间感知，难以在VR与远程临场等场景中建立沉浸式交互与社会信号一致性。

Method: 构建完全因果、可流式推理的体系：将因果Transformer-VAE与交错潜变量token用于在线生成；以条件化用户轨迹和音频的flow matching模型生成空间对齐动作；提出基于评分与无分类器引导的凝视控制机制，训练学得自然空间对齐，推理时可调眼神接触强度。

Result: 在Embody 3D数据集上实现SOTA动作质量，速度>300 FPS，比非因果基线快约3倍；能捕捉对话中的细微空间动态（转身、面向、凝视）。

Conclusion: 方法将空间感知的对话动作首次带到实时VR部署，兼具高质量与高速度，并提供用户可控的凝视偏好，适用于VR、远程临场和数字人应用。

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [43] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过扩大视频流VQA的token预算并引入自适应选择与检索MoE，缓解密集流中的后时刻偏置与冗余问题，在多基准上显著优于ReKV。


<details>
  <summary>Details</summary>
Motivation: 流式视频理解需在连续长时序中稳健编码、存储与检索细粒度信息。现有方法用每帧少量token做KV缓存，导致细节流失；且在密集流中，因特征编码造成查询-帧相似度随时间上升，检索偏向后帧，影响准确性。

Method: 1) 扩大每帧/整体token预算以捕获更细粒度的时空信息；2) 发现并分析相似度随时间上升的偏置问题；3) 提出自适应token选择策略，降低冗余、保留局部时空信息；4) 设计免训练的检索Mixture-of-Experts，结合外部模型改进相关帧识别；5) 将上述机制集成为MemStream并用于流式VQA。

Result: 在与Qwen2.5-VL-7B搭配、对比ReKV的设置下，MemStream在CG-Bench提升+8.0%，在LVBench提升+8.5%，在VideoMME(Long)提升+2.4%。

Conclusion: 扩大token预算并配合自适应选择与检索MoE能在不额外训练的前提下显著改善流式视频VQA的检索与推理，缓解后时刻偏置并保留细粒度时空细节。

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>
