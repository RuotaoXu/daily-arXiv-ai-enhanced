<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 320]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks](https://arxiv.org/abs/2511.17576)
*Rayan Aldajani*

Main category: cs.CV

TL;DR: 研究探索用AI基于正面人体图像与基本人体测量估算体脂率，以替代昂贵难得的DEXA；图像模型在小样本自建数据上取得RMSE≈4.44%、R^2≈0.807，显示低成本可行性。


<details>
  <summary>Details</summary>
Motivation: 体脂率是体重管理关键指标，但DEXA等金标准昂贵且不易获得；缺乏公开的视觉体脂估计数据集，促使作者评估AI是否能在低成本场景提供可用的体脂估计。

Method: 自建数据集共535例：253例有人体测量（体重、身高、颈围、踝围、腕围），282例为从Reddit抓取的带自报体脂（部分声称为DEXA）的正面身体图像。开发两类模型：1）基于ResNet的图像回归模型；2）基于人体测量的回归模型；并提出未来可扩展的多模态融合框架（待有配对数据时）。

Result: 图像模型达到RMSE 4.44%、R^2 0.807；表明在该数据集上，AI能较好拟合自报或测定的体脂率。

Conclusion: AI辅助的图像/测量模型有望以低成本提供体脂估计，支持面向消费者的健康健身应用；但依赖小型、异源且含自报标签的数据，需未来在更大、标准化、带金标准标签的配对数据上验证与融合。

Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.

</details>


### [2] [Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding](https://arxiv.org/abs/2511.17596)
*Yassir Benhammou,Suman Kalyan,Sujay Kumar*

Main category: cs.CV

TL;DR: 论文提出一种多模态自编码器（MMAE），在文本、音频、视觉三模态上学习统一表示，通过最小化联合重构损失实现无需大规模配对/对比数据的元数据自动生成与语义聚类，并在LUMA数据集上显著提升聚类与对齐指标。


<details>
  <summary>Details</summary>
Motivation: 现有广播媒体中的AI多依赖单一模态，难以捕捉跨模态语义关系，影响内容索引、标注和检索的自动化与准确性。需要一种能统一建模多模态并可规模化生成元数据的方案。

Method: 提出MMAE：一个在文本、音频、视觉三模态上的多模态自编码器。使用对齐的LUMA三元组数据训练，通过联合重构损失（跨模态/多模态重构）学习模态不变的语义嵌入，无需大规模配对或对比学习数据。

Result: 在LUMA上，相比线性基线，在聚类与对齐指标（Silhouette、ARI、NMI）上取得显著提升，表明重构驱动的多模态嵌入更适合作为可扩展元数据生成与跨模态检索的基础。

Conclusion: 重构式多模态学习能有效提升广播工作流程中的自动化、可检索性与内容管理效率；MMAE为跨模态元数据抽取与语义聚类提供了统一且可扩展的表示学习框架。

Abstract: Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.

</details>


### [3] [BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction](https://arxiv.org/abs/2511.17597)
*Zhengsen Xu,Sibo Cheng,Hongjie He,Lanying Wang,Wentao Sun,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出一个覆盖加拿大不列颠哥伦比亚及周边、25年逐日、含38个多模态变量的超大规模野火基准数据集，并用多类时序模型系统评测与因素重要性分析。


<details>
  <summary>Details</summary>
Motivation: 现有野火预测受燃料、气象、地形与人类活动等复杂交互影响，缺少同时具备长时间跨度、大空间尺度与多模态驱动因子的公开基准数据集，限制了方法比较与进展。

Method: 构建240百万公顷范围、1999–2023（隐含25年）逐日分辨率的数据集，涵盖38个协变量（主动火点、天气、燃料、地形、人为因素等）；基于该基准比较CNN、线性、Transformer、Mamba等多种时序预测架构；研究位置编码策略与不同驱动因子的相对重要性。

Result: 数据集与代码公开；在统一协议下得到各类模型的性能基线；发现位置编码与特征选择对预测效果具有显著影响，并量化不同驱动因子的贡献。

Conclusion: 该工作为野火风险长期时空预测提供标准化大规模基准与初始基线，促进模型与特征设计研究，并表明结合合适的位置编码与关键驱动因子可提升预测性能。

Abstract: Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire

</details>


### [4] [Robustness of Structured Data Extraction from Perspectively Distorted Documents](https://arxiv.org/abs/2511.17607)
*Hyakka Nakada,Yoshiyasu Tanaka*

Main category: cs.CV

TL;DR: 研究评估了多模态大模型在文档透视畸变与旋转下的数据抽取能力，发现结构识别（阅读顺序）对畸变更敏感，但简单的旋转校正可显著缓解。


<details>
  <summary>Details</summary>
Motivation: 现实文档图像不仅存在平面内旋转，还常伴随透视畸变；已有工作多关注旋转，对高自由度的透视影响与可控评测不足。需量化这类畸变对最先进多模态LLM（Gemini-1.5-pro）OCR式数据抽取的影响，以指导实际应用与预处理策略。

Method: 观察真实文档畸变形态，提出用近似等腰梯形变换建模，将原本八个自由度降至两个：旋转角与畸变比；基于合成样本文档系统地扫参，使用Gemini-1.5-pro抽取特定字段；同时评估字符识别准确率与结构（阅读顺序）识别准确率，以区分文字识别与版面顺序理解的鲁棒性。

Result: 在旋转与透视共同作用下，结构识别准确率显著下降，远较字符识别更敏感；揭示常见文档透视畸变对阅读顺序造成突出破坏。简单的旋转校正可明显提升结构识别表现，尽管仍受畸变影响。

Conclusion: 多模态LLM用于OCR时，对透视畸变尤其在结构理解上较脆弱；通过低维参数化的畸变模型可高效评测鲁棒性。实践中应采用至少旋转校正等几何预处理，并考虑进一步的透视校正，以提升数据抽取的可靠性。

Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.

</details>


### [5] [3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF](https://arxiv.org/abs/2511.17609)
*Linh Van Ma,Unse Fatima,Tepy Sokun Chriv,Haroon Imran,Moongu Jeon*

Main category: cs.CV

TL;DR: 提出用UKF融合多相机2D标注，自动生成高精度3D真值，兼顾位置与完整三维形状，并优于基线数据集对照。


<details>
  <summary>Details</summary>
Motivation: 许多视觉应用需精确3D真值，但现有方法常依赖昂贵的3D标注或只给地平面信息，且多相机情境中遮挡与视角差异导致3D估计困难。

Method: 将多台已标定相机的2D边界框或关键点标注，通过单应投影将2D坐标映射到世界坐标假设，再用无迹卡尔曼滤波（UKF）进行时序与跨视角融合，得到目标的3D位置与形状；算法为多相机单目标跟踪框架，可自动处理多视角、遮挡等问题，并输出完整3D形状。

Result: 在CMC、Wildtrack、Panoptic数据集上评估，相比可用的3D真值，3D定位精度高；相较仅提供地平面信息的方法，本方法能恢复完整三维形状。

Conclusion: 仅依赖2D图像标注即可在多相机系统中自动、可扩展地产生高精度3D真值，同时输出目标完整3D形状，为导航、安防、机器人等提供实用的数据生成方案。

Abstract: Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.

</details>


### [6] [Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression](https://arxiv.org/abs/2511.17612)
*Siddiqua Namrah*

Main category: cs.CV

TL;DR: 提出一个全无监督、多阶段深度网络用于低照度交通图像增强，通过分解光照与反射并逐步校正亮度、去噪复细节、补偿过曝，在多种自监督与正则损失下训练，实测在通用与交通数据集上优于SOTA并提升下游感知可靠性。


<details>
  <summary>Details</summary>
Motivation: 低照度夜间交通场景可见性差，受低照明、噪声、运动模糊、不均匀光照与眩光影响，严重妨碍目标检测与场景理解；现有方法要么需成对标注、泛化差，要么难兼顾亮度提升、细节保真与过曝区域修复。

Method: 构建全无监督、多阶段网络，先进行Retinex式分解为光照与反射；三模块级联：1) 光照自适应，进行全局与局部亮度校正；2) 反射恢复，利用空间-通道注意力抑噪并恢复结构细节；3) 过曝补偿，重建饱和区域、平衡场景亮度。训练采用自监督重建损失、反射平滑、感知一致性、领域自适应正则等，无需成对GT。

Result: 在通用与交通特定数据集上，定量指标（PSNR、SSIM、LPIPS、NIQE）和主观视觉质量均优于现有方法，增强后的图像在结构保真与可见性上更佳。

Conclusion: 该无监督多阶段框架能有效提升低照度交通图像的可见性与结构保真，稳健处理噪声与过曝，并提升下游感知任务的可靠性，具有实际应用价值。

Abstract: Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.

</details>


### [7] [HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2511.17614)
*Danyang Sun,Fadi Dornaika,Nagore Barrena*

Main category: cs.CV

TL;DR: 提出HSMix：结合“硬混合”超像素拼接与“软混合”基于局部显著性的亮度混合，用于医学图像语义分割的数据增强，简单即插即用、模型无关，在多任务与多模态上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 医学分割常受标注昂贵/疾病稀有导致的数据稀缺与过拟合所困。自监督、半监督虽可缓解，但需要复杂的预任务或伪标签。相比之下，数据增强更简单，但面向分割的局部编辑增强有效性研究不足，亟需一种保留语义边界且提升多样性的局部增强方法。

Method: 提出HSMix：1) 硬混合（Hard-mix）——在两张源图像间以同质区域（超像素）为单位重组，保留清晰结构边界；2) 软混合（Soft-mix）——基于局部聚合的像素级显著性系数进行亮度混合，细调由硬混合得到的局部区域；3) 将两张源图的真值掩膜同步执行相同的混合操作，生成一致的增强标签。方法可插拔、与模型无关，适用于多种医学影像模态。

Result: 在多种医学图像分割数据集与任务上进行大量实验，HSMix显著优于基线与现有增强策略，显示更强的泛化与鲁棒性。

Conclusion: 通过结合边界先验（超像素）与显著性引导的亮度混合，HSMix在不依赖复杂自/半监督框架的前提下有效缓解数据稀缺与过拟合问题，作为简单通用的增强策略可广泛用于医学图像分割；源码已开源。

Abstract: Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.

</details>


### [8] [Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2511.17615)
*Young-Beom Woo*

Main category: cs.CV

TL;DR: 提出PnP-MIX：一种免调参、可即插即用的多概念个性化文生图方法，通过注意力引导外观、掩码引导噪声混合与Background Dilution++，在复杂多目标场景中提高组合保真度并减少概念泄漏，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多概念个性化T2I在复杂场景中易篡改个性化与非个性化区域，破坏提示结构与区域交互，产生语义不一致与“概念泄漏”。需要一种能在不调参的情况下稳健整合多个个性化对象、并保持非个性化区域完整性的方案。

Method: 提出PnP-MIX，核心包括：1）Guided Appearance Attention：在生成过程中引导注意力以忠实复现每个个性化概念的外观；2）Mask-Guided Noise Mixing：依据区域掩码混合噪声，保护背景/无关物体等非个性化区域，同时精确注入个性化对象；3）Background Dilution++：降低个性化特征向其他区域的泄漏，提升特征在目标区域的定位；全流程免额外模型微调，作为即插即用模块集成。

Result: 在单概念与多概念个性化任务中，定量与定性实验均显示PnP-MIX在组合保真度、语义一致性与泄漏控制方面持续优于现有方法，并在复杂多目标场景中表现稳健。

Conclusion: PnP-MIX通过注意力引导、掩码噪声混合与Background Dilution++实现高保真多概念个性化文生图，在无需额外调参的条件下显著降低概念泄漏并提升构图与区域一致性，具备通用、稳健与可即插即用的优势。

Abstract: Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.

</details>


### [9] [Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach](https://arxiv.org/abs/2511.17618)
*Ju-Young Oh*

Main category: cs.CV

TL;DR: 提出FIQ框架，从视频中自动生成基础性描述类问答对，并配合VQ-CAlign对齐问题嵌入与视觉特征，在SUTD-TrafficQA上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答多依赖事件中心的标注，缺少对象类别、空间关系、属性等基础语义，导致模型对场景理解不完整，泛化与推理能力受限。

Method: 1) FIQ：从视频直接抽取描述性信息（对象类别、空间配置、视觉属性等），自动生成补充性的基础问答对以扩充训练数据；2) 通过嵌入整合的方式将这些基础知识融入VQA训练流程；3) 提出VQ-CAlign模块，将任务相关的问题嵌入与对应视觉特征对齐，保留上下文线索并增强下游适配性。

Result: 在SUTD-TrafficQA数据集上取得优于现有基线的方法，达到SOTA表现。

Conclusion: 补充基础场景层面的问答监督并进行问题-视觉对齐，可显著提升视频问答的整体理解、泛化与推理能力。

Abstract: Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.

</details>


### [10] [Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds](https://arxiv.org/abs/2511.17619)
*Qinghao Meng,Junbo Yin,Jianbing Shen,Yunde Jia*

Main category: cs.CV

TL;DR: 论文提出将LiDAR 3D检测从“中心对齐回归”改为“角点对齐回归”，以缓解BEV中心落在稀疏区导致的不稳定；并利用角点与2D框的几何约束实现弱监督训练，在KITTI上显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 中心对齐回归依赖于目标中心位置，但LiDAR点云前表面偏置使得BEV中心常位于稀疏/空洞区域，导致框回归噪声大、稳定性差。需要一种更稳定、信息更丰富的回归目标与训练监督形式。

Method: 重新定义3D框表示，采用“角点对齐回归”，将预测目标从中心转为处于可观测密集区域的几何角点；设计可插拔的角点感知检测头；利用角点间几何约束及与图像2D框的关系，从角点标注恢复部分3D框参数，实现弱监督训练（仅需BEV角点点击）。

Result: 在KITTI数据集上，相比中心式基线平均精度提升3.5% AP；仅用BEV角点弱监督即可达到全监督精度的83%。

Conclusion: 角点对齐回归在点云稀疏和中心不稳定场景下更稳健，结合几何约束可减少3D标注需求，兼容现有检测器并提升精度，证明角点感知策略的有效性。

Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.

</details>


### [11] [BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?](https://arxiv.org/abs/2511.17633)
*DoYoung Kim,Jin-Seop Lee,Noo-ri Kim,SungJoon Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 提出一种改进BNN的方法：用1.58位卷积增强表达力，并在残差中引入pre-BN以稳定训练；首次将深度可分离中的深度卷积成功二值化，在ImageNet等上以极低OPs达新SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统BNN极端量化带来表示能力不足与训练不稳定，尤其在轻量网络（含depth-wise卷积）中更严重，限制在移动端/边缘高效推理的实际应用。

Method: 1）提出1.58-bit卷积，在二值与多比特之间取得折中，提升表达力与精度；2）在残差结构中使用pre-BN（BatchNorm在残差加和前），改善Hessian条件数以稳定优化；3）据称首次使深度卷积在BNN中可行；在MobileNet V1等架构上应用并进行多数据集评测。

Result: 在ImageNet上基于MobileNet V1仅33M OPs即达新SOTA，优于同等OPs的以往方法；在CIFAR-10/100、STL-10、Tiny ImageNet、Oxford Flowers 102等上均明显优于现有方法，最高提升达9.3个百分点。

Conclusion: 适度提高位宽至1.58位并采用pre-BN残差可同时提升表达与稳定性，从而实现深度卷积的首次成功二值化与全面SOTA，验证了低比特BNN在极低算力预算下的可行性与优越性。

Abstract: Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.

</details>


### [12] [Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection](https://arxiv.org/abs/2511.17634)
*Kaikwan Lau,Andrew S. Na,Justin W. L. Wan*

Main category: cs.CV

TL;DR: 提出一种加速分数式扩散模型的框架：将稳定扩散转为Fokker-Planck形式，配合跨矩阵Krylov投影，用“种子”矩阵构建共享子空间以快速求解后续“目标”矩阵的大型线性系统，训练与推理显著提速。


<details>
  <summary>Details</summary>
Motivation: 标准稳定扩散在Fokker-Planck表述下需要为每张图像多次求解大型稀疏线性系统，训练/推理在多图像场景下计算代价高；现有稀疏求解器未充分利用不同线性系统之间的结构相似性。

Method: 将扩散过程重写为Fokker-Planck方程离散化后的线性系统序列；提出跨矩阵Krylov投影（cross-matrix Krylov projection）：先用少量“种子”矩阵构建共享Krylov子空间，再将后续“目标”矩阵的求解投影到该子空间中以快速收敛；与标准稀疏求解器和DDPM进行对比实验。

Result: 相较标准稀疏求解器，时间开销降低15.8%–43.7%；在去噪任务上较DDPM最高加速达115×；在相同计算预算下，本方法能生成高质量图像，而DDPM生成内容不可辨识。

Conclusion: 跨矩阵Krylov投影能复用矩阵间的结构相似性，高效求解Fokker-Planck形式下的线性系统，从而显著加速分数式扩散模型；在资源受限场景具备实际可用性。

Abstract: This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.

</details>


### [13] [Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification](https://arxiv.org/abs/2511.17635)
*Max A. Nelson,Elif Keles,Eminenur Sen Tasci,Merve Yazol,Halil Ertugrul Aktas,Ziliang Hong,Andrea Mia Bejar,Gorkem Durak,Oznur Leman Boyunaga,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出UPMI：在低维meta特征空间进行上游概率化“插补/扩增”，以提升儿科胰腺炎（急/慢性）MRI多模态小样本诊断；使用模态特异逻辑回归→7维概率meta特征→类条件GMM合成→与真实特征一起训练RF；在67例T1W/T2W上AUC 0.908±0.072，较仅用真实特征的0.864±0.061提升约5%。


<details>
  <summary>Details</summary>
Motivation: 儿科胰腺炎诊断困难且影像多模态、样本稀缺，传统影像增广在图像空间复杂、成本高且难以对齐多模态；需要一种在小样本、异质多模态下提升泛化与鲁棒性的轻量级方法。

Method: 1) 分模态训练逻辑回归（基于T1W和T2W MRI放射组学），输出每模态的类别概率/分数；2) 将这些概率转换为7维meta特征向量；3) 在交叉验证每折内，对每一类别分别拟合高斯混合模型（GMM），从中采样合成meta特征（上游概率化meta插补/扩增）；4) 将合成与真实meta特征合并，训练随机森林（RF）作为meta分类器；5) 在67例成对T1W/T2W儿科受试者上评估，指标为AUC。

Result: UPMI平均AUC 0.908±0.072，相比仅用真实meta特征的基线AUC 0.864±0.061，相对提升约5%。

Conclusion: 在小样本多模态儿科胰腺炎诊断中，在低维meta特征空间进行类条件概率化增广可提升性能；UPMI轻量、与下游元学习器解耦，减少图像级增广与配准复杂度，显示出改进泛化的潜力。

Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).

</details>


### [14] [TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection](https://arxiv.org/abs/2511.17636)
*Weijun Gao,Rundong He,Jinyang Dong,Yongshun Gong*

Main category: cs.CV

TL;DR: 提出一种面向通道的典型集(typical set)精炼方法与偏度校正，用于改进激活基础的OOD检测，在ImageNet-1K与CIFAR-100上达SOTA，并可泛化到不同主干与打分函数。


<details>
  <summary>Details</summary>
Motivation: 激活法是OOD检测的重要方向，但现有方法在对激活做“整流”时忽略了通道的固有差异与分布偏斜，导致典型集估计不准，把异常激活误纳入，从而影响ID/OOD分离与可靠性。

Method: 1) 通道感知的典型集精炼：依据“判别性(discriminability)”与“活跃度(activity)”两指标对每个通道的激活进行筛选与整流，形成更准确的通道级典型集；2) 偏度(skewness)精炼：利用分布偏度对典型集估计进行校正，减少长尾/偏斜导致的阈值偏差；3) 用精炼后的激活计算能量分数(energy score)进行OOD判别。

Result: 在ImageNet-1K与CIFAR-100基准上取得SOTA的OOD检测表现，并展示对不同网络主干与不同打分函数的良好可迁移与泛化能力。

Conclusion: 考虑通道特性与分布偏度的典型集精炼能显著提升激活法OOD检测的稳健性与准确性，且方法通用、可与多种骨干和评分策略集成。

Abstract: Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.

</details>


### [15] [SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios](https://arxiv.org/abs/2511.17649)
*Jieru Lin,Zhiwei Yu,Börje F. Karlsson*

Main category: cs.CV

TL;DR: SWITCH 提出面向现实设备控制的具身智能基准，覆盖从理解界面到执行与验证的全链路评测，揭示现有多模态模型在视频证据利用与因果验证上的明显短板。


<details>
  <summary>Details</summary>
Motivation: 现实世界的自主智能需要与现有物理与数字基础设施交互。家庭与日常场景充满可操作的实体控制界面（TCIs），例如开关、家电面板、嵌入式GUI，这些任务要求常识与物理推理、因果预测、时空上的结果验证，并具有安全风险。然而现有基准很少在具身、视频、部分可观测与事后验证等维度进行系统评测。

Method: 提出 SWITCH 基准框架，采用迭代式发布路线。首版 SWITCH-Basic 在以自我视角（egocentric）RGB视频作为输入、跨多样真实设备条件下，评测五种互补能力：任务感知VQA、语义UI定位/对齐、动作序列生成、状态转移预测、结果验证。共包含351个任务、覆盖98种真实设备与家电，并提供数据、代码与保留划分以便可复现评测与社区扩展。

Result: 在 SWITCH-Basic 上，商用与开源大规模多模态模型即便在单步交互上也表现不稳定，倾向过度依赖文本线索，未充分利用视觉/视频证据；总体分数可能掩盖关键失败模式。

Conclusion: SWITCH 揭示当前LMM在具身控制、因果与时空验证方面的不足，提供标准化数据与评测以推动更具挑战的后续迭代与面向训练的数据集构建。

Abstract: Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.

</details>


### [16] [Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment](https://arxiv.org/abs/2511.17655)
*Md. Mohaiminul Islam,Md. Mofazzal Hossen,Maher Ali Rusho,Nahiyan Nazah Ridita,Zarin Tasnia Shanta,Md. Simanto Haider,Ahmed Faizul Haque Dhrubo,Md. Khurshid Jahan,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: 提出一个端到端深度学习系统，用统一流程评估6种CNN（5个ImageNet预训练+1个轻量自研），在脑肿瘤MRI分类上达SOTA；既追求高准确，也重视可解释与可部署。


<details>
  <summary>Details</summary>
Motivation: 当前脑肿瘤自动分类研究常存在评估不统一、仅报准确率、可解释性不足、难以在资源受限设备部署等问题。作者希望用标准化流程横向公平比较多模型，同时提供可解释、可落地的轻量方案。

Method: - 数据与预处理统一；同一训练协议（AdamW、CosineAnnealingLR、早停耐心值7）。- 比较六个架构：VGG-16、Inception V3、ResNet-50、Inception-ResNet V2、Xception、以及1.31M参数自研紧凑CNN。- 用Grad-CAM和GradientShap生成可解释热区，验证注意力与解剖学一致性。- 评估除准确率外，还包括IoU、Hausdorff距离、PR曲线、混淆矩阵等。- 报告推理延迟以衡量可部署性。

Result: Inception-ResNet V2在测试集达99.53%准确率，Precision/Recall/F1均≥99.50%。自研轻量CNN参数1.31M，测试准确率96.49%，在边缘设备实时推理约375ms，模型体积比Inception-ResNet V2小约100倍。

Conclusion: 标准化评测显示SOTA性能与稳定可解释性；提出的轻量模型适合资源受限场景部署。整体框架在准确性、可解释性与可部署性间取得平衡，可用于临床筛查与分诊级别的应用。

Abstract: Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.

</details>


### [17] [MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation](https://arxiv.org/abs/2511.17668)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: 提出MedPEFT-CL，一个面向医疗视觉-语言分割的参数高效持续学习框架，通过双阶段架构（自适应学习+知识巩固）在适配新任务的同时减轻遗忘，显著减少可训练参数并保持跨模态性能。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉-语言分割模型在适应新解剖结构时易灾难性遗忘，常需完全重训，限制临床部署。现有持续学习方法少有专为医疗视觉-语言任务设计的研究，因此需要既能高效学习新任务又能保留旧知识的参数高效方案。

Method: 基于CLIPSeg构建双阶段架构：1) 自适应学习阶段：通过语义相似度驱动的适配器分配与提示（prompt）相似度分析，采用参数高效微调（如LoRA）实现医疗任务适配；2) 知识巩固阶段：引入双向Fisher-记忆协调（双向Fisher信息与记忆回放的互导）以确定回放优先级并巩固旧任务。核心技术包括语义驱动的适配器分配、双模态LoRA以维持跨模态对齐、以及双向Fisher-记忆协调。

Result: 在多种医疗数据集上进行广泛实验，相比基线在遗忘抑制与性能保持上更优，同时显著降低可训练参数开销，实现小参数开销的持续学习效果。

Conclusion: MedPEFT-CL通过语义驱动的适配器与双模态LoRA实现高效任务适配，并以双向Fisher-记忆协调缓解灾难性遗忘，在医疗视觉-语言持续学习场景中兼顾效率与性能，适合实际临床迭代部署。

Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.

</details>


### [18] [Person Recognition in Aerial Surveillance: A Decade Survey](https://arxiv.org/abs/2511.17674)
*Kien Nguyen,Feng Liu,Clinton Fookes,Sridha Sridharan,Xiaoming Liu,Arun Ross*

Main category: cs.CV

TL;DR: 综述过去10年150+篇关于“以人为中心”的空中监控研究（无人机/航拍），系统梳理检测、身份识别与重识别任务的挑战、数据集与方法，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 空中平台（无人机/UAV等）与成像传感器迅速发展，具备尺度大、机动性强、易部署与隐蔽性等优势，但较地面场景在分辨率、视角、尺度变化、遮挡、跟踪稳定性与隐私等方面存在独特难题，亟需系统性总结现状、挑战与方法以指导研究。

Method: 对近十年150+篇文献进行系统性调研：按人类目标检测、身份识别、重识别三大任务分类；对比空中与地面场景的特有挑战；汇编并分析公开空中数据集；深入梳理方法如何应对空中挑战与提升技术；最后总结研究缺口与开放问题。

Result: 形成针对空中人类监控任务的系统框架与技术谱系：明确关键挑战清单、整理任务对应的公开数据集、归纳代表性方法及其应对策略与改进技巧，揭示当前方法的有效性与局限。

Conclusion: 空中人类监控虽具应用潜力但仍面临小目标、极端视角/尺度变化、运动模糊、域偏移、数据与标注匮乏、隐私与伦理等问题；需在高分辨率成像、多模态/多视角融合、域自适应与自监督、轻量与在线学习、隐私保护与可解释性等方向开展深入研究。

Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.

</details>


### [19] [Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models](https://arxiv.org/abs/2511.17681)
*Weiyi Lv,Ning Zhang,Hanyang Sun,Haoran Jiang,Kai Zhao,Jing Xiao,Dan Zeng*

Main category: cs.CV

TL;DR: 提出VMRMOT框架，在RMOT中引入“运动模态”，通过MLLM进行视觉-运动-语言三方对齐，并用MGPH提升预测，实验SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RMOT仅用静态语言描述（外观、相对位置、初始运动），无法反映速度变化与方向转变等动态行为，造成语言与视觉时序不一致，限制多模态跟踪性能。

Method: 1) 从目标动态行为生成“运动感知描述”，利用具备时序推理能力的多模态大模型（MLLM）提取运动特征作为运动模态；2) 设计VMRA模块，分层将视觉查询与运动、语言参考进行对齐，强化跨模态一致性；3) 设计MGPH（Motion-Guided Prediction Head），利用运动模态引导预测头以提升定位与关联。

Result: 在多个RMOT基准上全面超过现有SOTA，显示引入运动模态与MLLM对齐机制的有效性。

Conclusion: 通过引入运动模态并用MLLM进行视觉-运动-语言对齐，VMRMOT缓解了静态语言与动态视觉的时序错配问题，显著提升RMOT性能，开辟了在RMOT中使用MLLM的新方向。

Abstract: Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.

</details>


### [20] [Understanding Counting Mechanisms in Large Language and Vision-Language Models](https://arxiv.org/abs/2511.17699)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 研究通过受控计数任务与因果中介/激活修补，揭示LLMs与LVLMs在层内逐步形成可转移的数值表示：低层编码小数目，高层编码更大计数；计数器状态多存于末尾token/区域并可跨上下文转移；LVLM的数值信号也出现在视觉嵌入中，受空间与编码器性质影响；文本分隔符等结构线索充当“捷径”。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型在计数上表现出一定能力，但其内部如何表示与逐步更新数目仍不明。缺乏针对数值内容的机械可解释工具，尤其跨模态（文本—视觉）的统一视角。

Method: 设计受控重复刺激（文本token与视觉重复元素）的计数任务；使用因果中介分析与activation patching定位贡献通路；提出CountScope工具解析层级与位置的数值特征；在LLM与LVLM上进行对比与迁移实验。

Result: 发现单个token/视觉特征包含潜在位置计数信息，可被提取并迁移；层级分析显示从低到高逐步形成数值表征（低层小计数，高层大计数）；识别到随项目更新的内部计数器，主要存于末尾token/区域，可跨上下文转移；LVLM中数值信号在视觉嵌入里出现，并因版式将注意力在前景/背景间转移；模型依赖结构线索（如分隔符）作为跟踪捷径，影响最终数值预测。

Conclusion: 计数在LLMs中呈现结构化、分层涌现机制，在LVLMs中亦遵循相同范式但受视觉编码器属性与空间结构影响。对可解释性与鲁棒计数基准设计、以及减少对“捷径”依赖具有启示。

Abstract: This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.

</details>


### [21] [Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions](https://arxiv.org/abs/2511.17722)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: 论文构建合成计数评测基准与框架，分析VLM在不同视觉/语言条件下的注意力分配与计数表现，并通过注意力干预取得小幅提升，但总体计数仍困难。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明VLM在回答视觉属性（尤其是计数）时依赖训练偏置，且在高特异性提问下更严重，缺乏系统化、可控变量的评测来揭示影响因素并验证干预手段。

Method: 1) 构建合成数据集与评测框架，系统操控图像与提示词变量（物体数目、物体/背景颜色与纹理、提示特异性等）；2) 使用开源VLM，分析在不同输入条件下注意力随层与视觉token的分配变化；3) 设计并注入基于注意力的干预，在不同层面增强/重分配对关键视觉token的关注；4) 评估干预对计数性能的影响。

Result: 在视觉或语言复杂度高时，VLM计数性能明显下降；注意力分配随输入参数变化而显著波动。某些注意力干预在多种视觉条件下带来小幅但稳定的计数精度提升。

Conclusion: VLM计数仍具挑战，偏置与注意力错配在高复杂度条件下加剧；基于注意力的干预可一定程度改善，但提升有限，提示需更强的归因与显式计数机制及更稳健的训练/评测范式。

Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.

</details>


### [22] [AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography](https://arxiv.org/abs/2511.17724)
*Mohammad Atwany,Mojtaba Lashgari,Robin P. Choudhury,Vicente Grau,Abhirup Banerjee*

Main category: cs.CV

TL;DR: 提出“AngioDG”，通过通道正则与重加权提升X射线冠脉造影分割的单源域泛化，在6个数据集上实现最佳OOD性能且不损及域内表现。


<details>
  <summary>Details</summary>
Motivation: XCA是介入中的金标准，但跨设备/协议/人群的域移导致分割模型在未见域表现不佳；标注稀缺难以做多源/自适应，单源域泛化成为现实需求。现有方法多依赖数据增广或合成域，容易过拟合增强分布，缺少可解释的通道级泛化机制。

Method: 提出AngioDG：在早期特征层评估各通道对任务相关度（与DG指标关联），据此进行通道重加权与正则，放大域不变特征通道、抑制域特异通道；以通道级约束提升可解释性与稳健性，同时保持主干网络和训练流程简洁。

Result: 在6个XCA冠脉分割数据集进行跨域评测，AngioDG在OOD测试上优于对比SDG/增广基线，并保持与基线相当的域内测试性能。

Conclusion: 通道层面的可解释重加权与正则能有效缓解单源域泛化中的过拟合与域移问题，提升XCA冠脉分割的跨域鲁棒性，且不牺牲域内精度。

Abstract: Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.

</details>


### [23] [The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation](https://arxiv.org/abs/2511.17727)
*Victor Li,Naveenraj Kamalakannan,Avinash Parnandi,Heidi Schambra,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 评估愿景语言模型在视频化脑卒中康复剂量与障碍量化上的可行性，发现现有模型细粒度动作理解不足，但在优化提示与后处理下对高层活动与粗粒度计数有一定潜力。


<details>
  <summary>Details</summary>
Motivation: 康复研究需要从视频中自动量化训练“剂量”（动作次数/时长等）与功能障碍程度，但现有方法往往依赖任务特定训练、标注昂贵。VLM在多任务视觉理解上表现突出，可能提供零样本或少样本的通用方案，因而探索其在临床视频分析中的适用性与局限。

Method: 将两项任务（康复剂量与障碍量化）统一为“动作识别/检测”问题，利用现成VLM以提示工程与后处理策略在不进行特定任务训练或微调的前提下，从视频帧抽样进行分类与计数；在29名健康对照与51名卒中幸存者数据集上评估。

Result: 现有VLM对细粒度动作理解不足：剂量估计与不使用视觉信息的基线相当；障碍评分预测不可靠。但通过优化提示与后处理，可在少量帧上进行高层活动分类、以中等准确度检测运动与抓握，并在轻度障碍及健康个体上将剂量计数误差控制在约25%以内。

Conclusion: 当前VLM尚不适合精确的临床级量化，尤其是细粒度动作与障碍评估；但其零训练条件下对高层活动与粗粒度计数的能力显示出应用前景。需要更强的细粒度运动表征、领域适配与评估框架，以推动在卒中康复及更广泛临床视频分析中的落地。

Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.

</details>


### [24] [VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.17731)
*Lingxiao Li,Yifan Wang,Xinyan Gao,Chen Tang,Xiangyu Yue,Chenyu You*

Main category: cs.CV

TL;DR: 提出VisReason/VisReason-Pro两大规模视觉CoT数据集（489K/165K），含多轮、人类式推理与深度辅助3D空间标注；在Qwen2.5-VL上微调显著提升逐步视觉推理、可解释性与跨基准泛化，推动MLLM系统化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的视觉推理缺乏大规模、人类式链式思维数据支撑；现有资源规模小、领域窄，或缺少逐步、可组合的空间推理结构，限制MLLM的复杂视觉理解。

Method: 构建VisReason（489K，四域、多轮人类式理由）与VisReason-Pro（165K，专家级GPT标注，含更细致推理轨迹与基于深度的3D空间锚定）；用其对SOTA的Qwen2.5-VL进行微调，并在多基准上评估逐步推理准确性、可解释性与泛化。

Result: 在多个基准上取得显著提升，尤其是逐步视觉推理与解释性；训练后的模型表现出更强的跨数据集泛化能力。

Conclusion: 大规模、结构化的视觉CoT数据能系统性提升MLLM的可解释与泛化的视觉推理能力；VisReason/VisReason-Pro可作为构建下一代多模态智能的重要基石。

Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.

</details>


### [25] [Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders](https://arxiv.org/abs/2511.17735)
*Samuel Stevens,Jacob Beattie,Tanya Berger-Wolf,Yu Su*

Main category: cs.CV

TL;DR: 论文探讨能否用稀疏自编码器（SAE）在基础模型表示上进行开放式特征发现，并在受控“再发现”实验与生态图像案例中验证其可与语义概念对齐，从而作为科学发现的实用工具。


<details>
  <summary>Details</summary>
Motivation: 科学数据规模巨大且跨域，基础模型在弱监督大规模数据上学得的内部表示包含超越训练目标的结构，但现有方法多为面向预设目标的确认式分析，缺乏对未知模式的开放式发现能力。作者希望找到一种能从基础模型表示中自动提出并对齐语义特征的方法，以推动从“确认”走向“发现”。

Method: 采用稀疏自编码器对基础模型的中间表示进行稀疏分解，得到可解释的稀疏特征。通过受控再发现实验评估这些特征与语义概念的对齐度：包括在标准分割基准上对齐测试，并与强力的无标签替代方案进行概念对齐指标比较。随后将同一流程应用于生态图像，无需分割或部件标签，观察是否能浮现细粒度解剖结构，并用具备真值的科学数据集进行验证。

Result: SAE在受控再发现实验中学得的特征与语义概念良好对齐，在概念对齐指标上与强无标签方法竞争或更优；在生态图像上无需额外标注就能涌现细粒度解剖结构，并得到真值验证支持。

Conclusion: 对基础模型表示进行稀疏分解可作为探索其已学知识的实用手段，能支持开放式特征发现，具有跨领域适用性（视觉、蛋白、基因组、天气等），为从确认式分析迈向真正的科学发现奠定前提。

Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.

</details>


### [26] [AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations](https://arxiv.org/abs/2511.17747)
*Dawid Wolkiewicz,Anastasiya Pechko,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: 提出AEGIS：对3D高斯头像进行对抗式身份遮蔽，在不改动几何的前提下跨视角一致地隐藏可识别特征，同时保持逼真度与功能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点渲染的人脸头像在生物识别场景中易被滥用，现有2D对抗遮蔽无法在动态3D、多视角中稳定工作，缺少能保持外观与属性而去标识的方案。

Method: 在不改变头像几何的情况下，对高斯颜色系数施加对抗扰动；以预训练的人脸验证网络为目标引导，多视角一致优化，使人脸相似度下降，同时加入感知质量与属性保持约束，确保逼真度与年龄/性别/种族/情绪等属性不变；无需重训练或模型改动。

Result: 在多视角下实现“完全去身份化”，人脸检索与验证准确率降至0%；视觉质量高（SSIM 0.9555、PSNR 35.52 dB），同时保留年龄、种族、性别、情绪等关键属性。

Conclusion: AEGIS首次为3D高斯头像提供跨视角一致的隐私保护：在保持感知真实与语义属性的同时强力抑制身份可识别性，且部署简便、无需改几何或重训，适用于需要隐私与真实感兼顾的3D头像场景。

Abstract: The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.

</details>


### [27] [SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration](https://arxiv.org/abs/2511.17750)
*Zhimin Shao,Abhay Yadav,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: 提出SPIDER：结合共享特征骨干与双头网络（2D与3D）进行从粗到细匹配，并构建大基线评测基准，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域（航拍、室内、室外）匹配受外观、尺度、视角剧变影响；3D基础模型虽具空间一致性，但匹配集中于主平面、难以捕捉细粒度几何，尤其大视角变化下。需要兼顾2D与3D优势的通用匹配方案与公正评测基准。

Method: 线性探针比较多种视觉基础模型的匹配能力；据此设计SPIDER：共享特征提取骨干 + 两个专门头部，分别预测2D与3D对应关系；采用从粗到细的多阶段匹配策略，融合2D/3D几何线索以提升跨域与大基线鲁棒性；并发布针对非受限场景的大基线图像匹配评测基准。

Result: 在新提出的大基线评测基准及若干现实场景中，SPIDER在匹配质量与下游（结构恢复/位姿估计）指标上显著超越当前SOTA。

Conclusion: 2D与3D互补的双头匹配框架能缓解3D基础模型偏向平面的局限，并在跨域、大视角条件下实现更稳健、通用的图像匹配。

Abstract: Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.

</details>


### [28] [CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation](https://arxiv.org/abs/2511.17755)
*Prantik Howlader,Hoang Nguyen-Canh,Srijan Das,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: CORA 是一个半监督的“推理分割”框架，利用少量标注与大量无标注图像，通过条件化视觉指令、跨同义查询一致性筛选伪标签、以及token级对比对齐，显著提升在复杂指令下的像素级分割泛化与鲁棒性，在少样本标注设置下于Cityscapes与PanNuke取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 推理分割需要依据复杂、常含隐含语义的指令在场景中进行上下文依赖的推理与定位，但收集配对的高质量像素级标注与丰富语言监督成本高昂，导致模型在分布外场景下泛化差。作者希望减少标注依赖并提升鲁棒性。

Method: 提出半监督框架 CORA，包含三点：1) 条件化视觉指令：编码目标与其他物体的空间与语境关系，强化指令可解性；2) 伪标签噪声过滤：对语义等价的多种查询，若多模态LLM输出一致则保留，否则丢弃，以提升伪标签质量；3) token级对比对齐：在有标注与伪标注样本间进行特征级对比学习，增强表示一致性与区分性。

Result: 在极少标注下优于现有方法：Cityscapes 仅用约100张标注图像即达SOTA，较基线提升约+2.3%；在PanNuke 仅180张标注图像下提升约+2.4%。

Conclusion: 通过条件化指令、稳健的伪标签筛选与token级对比对齐，CORA 在低标注成本下实现更强的推理分割泛化与鲁棒性，优于现有半监督与指令分割基线，适用于分布转移场景。

Abstract: Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\%$. Similarly, CORA improves performance by $+2.4\%$ with only 180 labeled images on PanNuke, a histopathology dataset.

</details>


### [29] [Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers](https://arxiv.org/abs/2511.17757)
*Giancarlo Giannetti,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: 提出一种结合Transformer与Dirichlet先验的变分自编码器（LDVAE-T）用于高光谱解混，通过对端元束与丰度的联合建模提升解混性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱像素常为多种材料混合，传统方法依赖固定端元谱，难以处理谱变异并同时满足丰度的物理约束（非负、和为1）。需要能捕捉全局上下文、表征材料内在变异且物理可解释的模型。

Method: 构建LDVAE-T：以Transformer编码器输出Dirichlet分布参数，采样得到满足非负与和为1的丰度；解码器将每个端元建模为“端元束”，对每个补丁预测端元的均值谱与分段协方差以刻画相关的谱变异；用采样丰度对端元束进行线性混合重建，并以VAE框架训练（隐空间Dirichlet先验、重建损失+正则项）。

Result: 在Samson、Jasper Ridge、HYDICE Urban三基准数据集上，LDVAE-T在丰度估计RMSE与端元提取SAD指标上均优于现有SOTA模型，表现稳定一致。

Conclusion: 将Dirichlet先验与Transformer全局建模融合，并以端元束刻画谱变异，可在保持物理可解释性的同时显著提升高光谱解混的丰度与端元估计质量。

Abstract: Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.

</details>


### [30] [Deepfake Geography: Detecting AI-Generated Satellite Images](https://arxiv.org/abs/2511.17766)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: 比较CNN与ViT用于检测AI生成卫星图像，ViT以更高准确率与鲁棒性胜出，并用可解释性方法验证其判别依据。


<details>
  <summary>Details</summary>
Motivation: 生成式模型威胁卫星图像真实性，而现有深伪检测多聚焦人脸场景，不适用于具有地形结构复杂性的遥感影像；需要系统评估何种模型更能捕捉合成痕迹。

Method: 构建超过13万张RGB标注数据集（来源DM-AER与FSI），训练并比较多种CNN与ViT架构；评估准确率与鲁棒性；采用Grad-CAM（CNN）与Chefer注意力归因（ViT）进行可解释分析。

Result: ViT显著优于CNN（准确率95.11% vs 87.02%），在对结构不一致与重复纹理等合成特征的捕捉上更强，表现出更好的鲁棒性与全局语义建模能力。

Conclusion: ViT是检测合成卫星图像的更优选择；解释性分析支持其可信度。未来将扩展至多光谱和SAR，并引入频域分析以进一步提升检测能力，保障高风险应用中的图像完整性。

Abstract: The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.

</details>


### [31] [Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?](https://arxiv.org/abs/2511.17792)
*Dingrui Wang,Hongyuan Ye,Zhihao Liang,Zhexiao Sun,Zhaowei Lu,Yuchen Zhang,Yuyu Zhao,Yuan Gao,Marvin Seegert,Finn Schäfer,Haotong Qin,Wei Li,Luigi Palmieri,Felix Jahncke,Mattia Piccinini,Johannes Betz*

Main category: cs.CV

TL;DR: 提出Target-Bench，用于评估世界模型在无地图语义目标路径规划上的能力；构建含450段实拍机器人视频与SLAM轨迹的数据集与评测流程；现有最强模型仅得0.299，总体表现不足；对开源5B模型小规模微调可显著提升至0.345；将开源代码与数据。


<details>
  <summary>Details</summary>
Motivation: 当前生成式世界模型可合成逼真视频，但其是否具备可用于机器人路径规划的时空一致性与可控性仍不清楚且缺乏量化评估。需要一个标准化基准与指标体系，直接衡量模型在真实环境中面向语义目标的规划能力，而非仅靠感知或视觉逼真度指标。

Method: 构建Target-Bench：- 数据集：450段机器人实采视频，覆盖45个语义目标类别，并配套SLAM生成的真实相机/轨迹真值。- 评测流程：从模型生成的视频中恢复相机运动，定义5个互补指标，衡量到达目标能力、轨迹精度与方向一致性。- 模型评测：比较Sora 2、Veo 3.1、Wan系列等SOTA；并在仅325个场景上对一个5B开源模型进行微调以测试可提升性。

Result: 最佳即用模型（Wan2.2-Flash）整体分数仅0.299，显示当前世界模型在机器人规划上的显著不足。开源5B模型经仅325场景微调后达到0.345，相比其基线0.066提升>400%，并超过最佳商用即用模型约15%。

Conclusion: 世界模型的视频逼真度不等于可用于机器人规划的可控可用性；当前方法在无地图语义导航上仍较弱。Target-Bench提供统一数据与指标，揭示差距并表明通过小规模任务定向微调可显著提升表现。代码与数据将开源，利于后续研究与对比。

Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.

</details>


### [32] [Attention Guided Alignment in Efficient Vision-Language Models](https://arxiv.org/abs/2511.17793)
*Shweta Mahajan,Hoang Le,Hyojin Park,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出AGE-VLM，通过在高效VLM中引入交错式跨模态注意力并利用SAM蒸馏的空间先验，显著缓解拼接式架构导致的图文错配与幻觉问题，在多项视觉中心基准上优于或可比现有高效VLM。


<details>
  <summary>Details</summary>
Motivation: 高效VLM常采用视觉特征与文本拼接的简单对齐策略，但分析发现此类架构难以区分语义匹配与不匹配的图文对，造成注意力错配，从而引发对象幻觉。需要一种既高效又具更强视觉落地能力的架构来弥补这一缺陷。

Method: 提出Attention-Guided Efficient VLM（AGE-VLM）：在小参数LLM中交错插入多层跨模态注意力，以“注入”视觉能力；以SAM提供的空间分割/区域先验进行知识蒸馏，指导跨注意力将关注集中于与文本语义相关的图像区域；整体框架仍保持效率导向，并对注意力模式进行系统分析与约束。

Result: 在多种视觉中心任务/基准上进行验证，AGE-VLM在减少对象幻觉和提升视觉落地方面优于或与现有高效VLM相当。实验表明交错式跨注意力与SAM空间蒸馏能显著改善注意力对齐和匹配判别能力。

Conclusion: 简单拼接式对齐是高效VLM产生幻觉的关键诱因；通过交错式跨注意力并借助SAM空间知识对注意力进行引导，可显著增强小型LLM的视觉 grounding 能力，降低幻觉。该框架为未来提升VLM视觉-语言理解提供了有效方向。

Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.

</details>


### [33] [Pillar-0: A New Frontier for Radiology Foundation Models](https://arxiv.org/abs/2511.17803)
*Kumar Krishna Agrawal,Longchao Liu,Long Lian,Michael Nercessian,Natalia Harguindeguy,Yufu Wu,Peter Mikhael,Gigin Lin,Lecia V. Sequist,Florian Fintelmann,Trevor Darrell,Yutong Bai,Maggie Chung,Adam Yala*

Main category: cs.CV

TL;DR: 提出Pillar-0放射学基础模型与RATE标注框架，在多部位CT与乳腺MRI上以体素级输入与保留灰度对比实现SOTA，AUROC显著超越多家大模型基线，并在外部验证与下游任务（肺癌风险、脑出血）保持领先与高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像大模型多将三维CT/MRI拆成低保真2D切片、丢失关键灰度对比，且缺少贴近临床的评估体系，难以覆盖放射科全流程与真实表现。

Method: 1) 预训练Pillar-0：在单中心大规模多模态数据（腹盆/胸/头CT与乳腺MRI）上，以三维体数据与灰度对比保留的输入进行自监督/监督学习（文中隐含）并面向366项放射学发现进行建模；2) RATE：利用LLMs从报告中可扩展地抽取结构化标签，达近乎完美的标注准确度；3) 以临床贴近的内部大样本测试与外部数据集进行系统评测，并扩展至未预训练任务。

Result: 内部测试在四类数据上分别达平均AUROC 86.4/88.0/90.1/82.9，较MedGemma、MedImageInsight、灵枢、Merlin高7.8–15.8点，366任务中319项最佳（87.2%）；外部Stanford腹部CT上优于Merlin（82.2 vs 80.6 AUROC）；在长期肺癌风险预测上优于Sybil（NLST提升3.0 C-index，MGH+5.9，CGMH+1.9）；脑出血检测以1/20数据即达>95 AUROC，样本效率显著。

Conclusion: Pillar-0与RATE共同构成开放、临床严谨的放射学基础设施，突破三维处理与标注/评测瓶颈，提供跨任务高性能与强泛化、样本效率，推动可落地的放射学AI系统开发。

Abstract: Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.

</details>


### [34] [A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking](https://arxiv.org/abs/2511.17805)
*Chengan Che,Chao Wang,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: 提出PL-Stitch，一种面向流程性视频的自监督学习方法，通过基于Plackett-Luce排序的时序排序任务与时空拼图损失，强制模型学习时间顺序与跨帧关联，在外科与烹饪数据集上显著提升下游识别与分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督视频表示多从短时或静态片段出发，缺乏对“步骤顺序”这种流程性结构的建模；实验证明：对正向与时间反转序列预训练后特征近似，说明表示对时序方向不敏感，无法捕获程序化活动的内在顺序。

Method: 提出PL-Stitch框架：1) 以Plackett-Luce(PL)概率模型为基础的主目标，要求模型对随机采样帧进行时间排序，学习全局流程推进；2) 辅助的时空拼图(jigsaw)损失，建模跨帧细粒度物体关联；两者共同作为自监督信号提升对程序性结构的感知。

Result: 在5个外科与烹饪基准上均取得领先：如Cholec80外科阶段识别k-NN准确率提升11.4个百分点，Breakfast烹饪动作分割线性探测准确率提升5.7个百分点，表现稳定且显著优于现有方法。

Conclusion: 将时间顺序作为监督信号并结合PL排序与时空拼图，可有效注入流程意识，学得对步骤顺序敏感的表示，从而在程序性视频理解任务上带来显著增益。

Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.

</details>


### [35] [REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion](https://arxiv.org/abs/2511.17806)
*Ryoma Yataka,Pu Perry Wang,Petros Boufounos,Ryuhei Takahashi*

Main category: cs.CV

TL;DR: 提出REXO：将DiffusionDet的2D框扩散提升到3D雷达空间，并用噪声3D框显式对齐多视角雷达特征，结合“人接触地面”的先验以减少扩散参数；在HIBER与MMVR上分别提升+4.22和+11.02 AP。


<details>
  <summary>Details</summary>
Motivation: 多视角室内雷达感知成本低、隐私友好，但现有方法依赖隐式跨视角特征关联（如提案配对、跨注意力），在复杂室内场景中易产生歧义，导致检测退化。需要一种稳健、可解释的显式跨视角关联机制。

Method: 构建REXO：以DiffusionDet为基础，将2D边界框扩散过程“抬升”到3D雷达空间；利用含噪3D BBox作为锚，显式引导多视角雷达特征对齐，从而在跨视角条件下进行去噪；引入“人体接地”的先验以约束/确定部分扩散参数，减少参数量并稳定训练。

Result: 在两个公开室内雷达数据集上验证：相较SOTA，HIBER上AP提升+4.22，MMVR上AP提升+11.02。

Conclusion: 显式的、基于3D框扩散的跨视角雷达特征关联可显著提升多视角室内雷达目标检测性能；结合物理先验可减少参数并提升稳定性与准确率。

Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.

</details>


### [36] [Importance-Weighted Non-IID Sampling for Flow Matching Models](https://arxiv.org/abs/2511.17812)
*Xinshuang Liu,Runfa Blark Li,Shaoxiu Wei,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出一种针对flow-matching模型的非独立同分布采样与重要性加权框架，在有限采样预算下以低方差、无偏方式估计期望，并通过基于score的正则化提升样本多样性与质量。


<details>
  <summary>Details</summary>
Motivation: 独立采样在稀有但高影响事件主导期望时方差很大，导致对flow-matching模型输出的期望估计不稳定；需要一种既能覆盖分布多样显著区域又能保持无偏估计的方法。

Method: 1) 设计非IID的联合采样策略，使同时抽取的样本在分布中相互分散以覆盖多样、高显著性区域；2) 提出基于score（log密度梯度）的多样性正则，将样本在高密度流形内相互推远，抑制偏离数据流形的漂移；3) 首次给出非IID流样本的的重要性加权方法：学习一个残差速度场，使由非IID采样得到的边缘分布可被复现，从而估计样本的重要性权重；4) 在此基础上进行无偏、低方差的期望估计。

Result: 实验显示该方法能生成多样且高质量的样本，能够准确估计重要性权重与目标期望，相比独立采样具有更可靠的表征能力。

Conclusion: 通过非IID联合采样+score正则+残差速度场的重要性加权，实现在有限预算下对flow-matching模型输出的稳健、无偏、低方差期望估计，并提升样本覆盖与质量。

Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.

</details>


### [37] [QAL: A Loss for Recall Precision Balance in 3D Reconstruction](https://arxiv.org/abs/2511.17824)
*Pranay Meshram,Yash Turkar,Kartikeya Singh,Praveen Raj Masilamani,Charuvahan Adhivarahan,Karthik Dantu*

Main category: cs.CV

TL;DR: 提出用于3D体素/点云学习的质量感知损失QAL，解耦召回与精度，通过覆盖加权最近邻与未覆盖真值吸引两项，替代CD/EMD，显著提升覆盖，恢复细薄结构，并在抓取评估中带来实际收益。


<details>
  <summary>Details</summary>
Motivation: CD/EMD在3D重建/补全等任务中难以同时兼顾召回与精度：易忽略薄结构与稀疏区域，导致覆盖不足；且其目标不可调节两者权衡。需要一种可解释、可调、稳健的训练目标。

Method: 设计QAL：1) 覆盖加权最近邻项（衡量预测到真值的贴近度，并根据真值局部覆盖度加权，抑制过度密集、鼓励均匀覆盖）；2) 未覆盖真值吸引项（对未被预测点覆盖的GT区域施加吸引力，提升召回）；两项分别控制precision与recall，可调权重解耦两者。与现有管线无缝替换CD/EMD；进行广泛消融、跨分辨率与超参稳定性测试。

Result: 在多种流程中，QAL带来平均覆盖提升：相对CD +4.3点、相对最佳替代 +2.8点；能稳定恢复薄结构和欠代表区域。对PCN与ShapeNet进行完整重训，跨数据集与骨干泛化良好。用于GraspNet抓取评估时获得更高抓取分数，表明覆盖改进转化为更可靠的机器人抓取。

Conclusion: QAL是一个可解释、可调且实用的训练目标，可替代CD/EMD，为鲁棒3D视觉与安全关键机器人应用提供更好的覆盖与下游性能。

Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.
  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.
  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines

</details>


### [38] [Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations](https://arxiv.org/abs/2511.17828)
*Guilherme J. Cavalcante,José Gabriel A. Moreira,Gabriel A. B. do Nascimento,Vincent Dong,Alex Nguyen,Thaís G. do Rêgo,Yuri Malheiros,Telmo M. Silva Filho,Carla R. Zeballos Torrez,James C. Gee,Anne Marie McCarthy,Andrew D. A. Maidment,Bruno Barufaldi*

Main category: cs.CV

TL;DR: 使用BiomedCLIP作基座，针对乳腺影像BI-RADS致密度分级，比较单一(s2D)与多模态（s2D/DM/DBT）训练；两者准确率相近（0.74 vs 0.73），多模态在多成像类型上适用性更强且AUC≥0.84，并在外部数据集RSNA与EMBED上实现AUC 0.80–0.93；Grad-CAM显示关注区域与临床一致，模型具可解释性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺影像模型在不同机构/模态上的泛化与可扩展性不足；基础模型有望缓解但在乳腺领域验证有限。

Method: 采用医学多模态对比学习基础模型BiomedCLIP，进行适配微调以自动完成BI-RADS致密度分级；训练使用三种模态的乳腺影像（s2D、DM、DBT）共96,995张，比对单模态与多模态训练；通过加权对比学习应对类别不平衡；评估准确率、AUC，并在RSNA与EMBED进行外部验证；使用Grad-CAM评估注意力与可解释性。

Result: 多模态与单模态总体准确率相近（0.74 vs 0.73），但多模态模型在不同模态上适用性更强，且对各BI-RADS类别AUC均≥0.84；外部验证AUC 0.80–0.93；可视化显示关注区域与临床相关。

Conclusion: 以BiomedCLIP为基座的多模态训练在乳腺致密度分级上具良好泛化、稳健与可解释性，优于仅单模态方案的跨模态适用性，显示基础模型在乳腺影像中的应用潜力并为后续诊断任务奠定基础。

Abstract: Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.

</details>


### [39] [Show Me: Unifying Instructional Image and Video Generation with Diffusion Models](https://arxiv.org/abs/2511.17839)
*Yujiang Pu,Zhanbo Huang,Vishnu Boddeti,Yu Kong*

Main category: cs.CV

TL;DR: ShowMe 统一图像指令编辑与视频预测，通过激活视频扩散模型的空间/时间分量并配合结构与运动一致性奖励，实现更高结构保真与时序连贯，跨任务互相增益，效果优于专家模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法割裂：图像编辑忽视动作随时间展开，视频预测忽视目标意图与结果，导致交互式世界模拟器难以在同一框架下进行指令驱动的状态变化建模。

Method: 提出 ShowMe：在单一视频扩散模型中可选择性地启用空间（图像编辑）或时间（视频预测）组件；并引入结构一致性奖励（提升形体/背景结构保真）与运动一致性奖励（提升时序与运动连贯）；利用视频预训练获取空间知识用于非刚性编辑，同时在指令驱动编辑阶段强化面向目标的推理以反哺视频预测。

Result: 在多样基准上，ShowMe 在指令驱动图像与视频生成两方面均超过各自领域的专家模型，表现出更好的上下文一致性、真实感和时序连贯性。

Conclusion: 视频扩散模型可作为统一的“动作-物体状态转换器”。通过空间/时间组件的选择性激活与一致性奖励，ShowMe 实现了跨任务统一并带来双向增益，推动交互式世界模拟器的发展。

Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.

</details>


### [40] [JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception](https://arxiv.org/abs/2511.17843)
*Chenyi Wang,Zhaowei Li,Ming F. Li,Wujie Wen*

Main category: cs.CV

TL;DR: 提出JigsawComm：在带宽受限下，通过“语义稀疏编码+效用评估+最优传输策略”实现多智能体协同感知的高效通信，数据量可降至原来的1/500且精度不降反升。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知多依赖压缩或启发式选择，忽略了特征的语义重要性与跨车冗余，难以在严格带宽下发挥作用。需要一种方法使每一比特都直接服务于最终感知任务。

Method: 将“语义特征编码与传输”建模为在带宽约束下最大化协同感知精度的联合优化问题。提出JigsawComm：1) 正则化编码器提取语义相关、空间稀疏的特征；2) 轻量级特征效用估计器预测每个智能体在每个位置对任务的贡献，产生元效用图；3) 在代理间交换元效用图，计算可证最优的传输策略，按位置从效用最高的代理选择特征，天然去冗余；4) 整体端到端训练。通信复杂度随代理数增长仍维持O(1)。

Result: 在OPV2V与DAIR-V2X基准上，在保持或超过SOTA精度的同时，总数据量最多减少>500×。

Conclusion: 通过语义感知的特征稀疏化与效用驱动的选择，JigsawComm在带宽受限场景实现可扩展、冗余最小化的协同感知，为实际部署提供更高通信效率与任务性能。

Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.

</details>


### [41] [Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation](https://arxiv.org/abs/2511.17844)
*Shihan Cheng,Nilesh Kulkarni,David Hyde,Dmitriy Smirnov*

Main category: cs.CV

TL;DR: 提出一种数据高效的微调策略，用稀疏、低质的合成数据就能给大规模文生视频扩散模型新增物理相机参数等控制，并且效果优于用真实高保真数据微调。并给出直观与定量的解释框架。


<details>
  <summary>Details</summary>
Motivation: 获取覆盖物理相机控制（如快门、光圈）的高保真视频数据昂贵且难，限制了在现有文生视频扩散模型中加入新控制维度的能力。需要一种在数据稀缺、质量不高的情况下仍能学到有效控制的方法。

Method: 提出数据高效微调：使用稀疏、低质量的合成数据对已有大规模文生视频扩散模型进行有针对性的微调，使模型学会新的生成控制（相机物理参数）。同时构建并分析一个理论/经验框架，解释为何低质合成数据也能有效甚至更优。

Result: 微调后模型不仅成功习得所需控制，还在相关任务上优于使用“真实”高保真数据微调的模型。实验与定量分析支持该现象。

Conclusion: 低质合成数据在为大模型注入新控制维度时更具数据效率与效果；所提框架为这一反直觉现象提供直观与定量依据，提示未来可用简单合成数据扩展生成模型能力。

Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.

</details>


### [42] [MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use](https://arxiv.org/abs/2511.17881)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: 提出MGA-VQA：结合token级编码、空间图推理、记忆增强推理和问题引导压缩，用于文档VQA；在6个数据集上提升准确率与效率，并提供可解释的决策路径与定位。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA方法难以显式建模空间关系、处理高分辨率文档效率低、缺乏多跳推理能力且可解释性有限，影响准确性与实用性。

Method: 构建多模态框架MGA-VQA：1) token级文本/视觉/布局编码；2) 基于空间关系的图结构推理以显式建模对象间布局；3) 记忆增强推理模块，使用结构化记忆进行跨步骤信息整合与读取；4) 基于问题的压缩机制以降低高分辨率冗余并加速推理；5) 输出可解释的图决策路径与空间定位。

Result: 在FUNSD、CORD、SROIE、DocVQA、STE-VQA、RICO六个基准上，相比现有方法在答案预测与空间定位上均有一致提升，同时推理效率更高。

Conclusion: MGA-VQA通过图推理与记忆增强实现对文本语义、空间布局与视觉特征的联合理解，兼顾性能与效率，并提供可解释的决策与定位，适用于多种文档场景。

Abstract: Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.

</details>


### [43] [ArticFlow: Generative Simulation of Articulated Mechanisms](https://arxiv.org/abs/2511.17883)
*Jiong Lin,Jinchen Ruan,Hod Lipson*

Main category: cs.CV

TL;DR: 提出ArticFlow：动作可控的两阶段flow matching，用于可动三维点云的生成与仿真；在MuJoCo Menagerie上优于对象专用模拟器与动作条件的静态生成器，在运动学准确性与形状质量上更好。


<details>
  <summary>Details</summary>
Motivation: 静态三维生成已显著进步，但可动（关节/机构）三维生成仍难：形变依赖动作、可用数据有限、模型难以跨动作与类别泛化。需要一种既能受动作控制又可高质量生成/预测运动学的通用方法。

Method: 提出两阶段flow matching框架ArticFlow：1）潜变量流（latent flow）将噪声运输到形状先验编码；2）点流（point flow）在动作与形状先验条件下将点集从噪声运输到目标点集。通过显式动作控制学习从噪声到目标点集的速度场，实现单模型覆盖多类可动物体，并可跨动作泛化；支持潜空间插值生成新形态。

Result: 在MuJoCo Menagerie上，ArticFlow既可作生成模型也可作神经模拟器：从紧凑先验预测动作条件的运动学，并通过潜插值合成新形态。相较对象专用模拟器与动作条件的静态点云生成器，取得更高的运动学精度与更好的形状质量。

Conclusion: 动作条件的flow matching为可控且高质量的可动机制生成提供了可行路径；ArticFlow实现跨类别、跨动作的统一建模，兼具生成与仿真的能力。

Abstract: Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.

</details>


### [44] [FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning](https://arxiv.org/abs/2511.17885)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Wei Chen,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CV

TL;DR: 提出FastMMoE：在MoE型多模态大模型上，训练免调、通过减少视觉token的专家激活与基于路由相似度的token裁剪，实现最高55% FLOPs削减且保持约95.5%性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像导致视觉token冗余与推理时延，现有裁剪方法多依赖注意力冗余分析并面向致密模型，不适用于或未优化于MoE结构；需要一种利用MoE路由特性的高效、低损耗加速方案。

Method: 从MoE路由分析出发的训练免加速框架FastMMoE，包含两策略：1) 专家激活缩减：针对视觉token降低不必要的专家计算（减少被激活专家/门控开销）；2) 路由感知token裁剪：计算视觉token在专家上的路由概率分布，相似度高的判为冗余并删除，以保留关键信息同时降低序列长度与专家开销。

Result: 在DeepSeek-VL2、InternVL3.5等大型MoE-MLLM上，FLOPs最高降低55.0%，整体性能保持约95.5%，在多种保留率设定下均优于面向致密模型的FastV、SparseVLM等基线。

Conclusion: 基于MoE路由特性的训练免加速在多模态模型上有效：可显著减少视觉计算与token冗余、兼顾速度与精度，并优于传统致密模型的注意力导向裁剪方法。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.

</details>


### [45] [When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA](https://arxiv.org/abs/2511.17886)
*Pume Tuchinda,Parinthapat Pengpun,Romrawin Chumpu,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CV

TL;DR: 研究系统性评估了在CLIP风格的视觉-语言模型上进行知识蒸馏的可扩展性，发现更强教师并不总能带来更好学生，现有蒸馏框架在放大到大教师与多任务时会失效。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态任务上效果显著但计算代价高，业界常用知识蒸馏构建轻量模型；然而在CLIP类VLM中的蒸馏研究仍局限：教师规模偏小、评测任务单一（多为分类/检索），缺乏对大规模教师与更广泛下游任务的系统性理解。

Method: 搭建首个覆盖多种CLIP风格教师（从标准基线到大规模SOTA）的蒸馏评测基准与实验框架；在统一设置下将学生从不同强度教师蒸馏，并在多模态下游任务（含VQA等）上评估；比较不同蒸馏策略与损失设计，观察教师规模对学生性能与可扩展性的影响。

Result: 与NLP与视觉领域常见结论相反：教师越强并不必然带来更优学生；当教师规模增大、任务超出分类/检索时，现有CLIP蒸馏方法常出现不可扩展或性能下降，尤其在VQA等复杂多模态推理任务上更明显。

Conclusion: 现有VLM蒸馏范式对大规模教师与多任务不鲁棒，挑战了“更强教师=更好学生”的假设；需要重新设计面向多模态、参数高效且可扩展的蒸馏目标与训练策略，以在广泛下游任务上稳定受益。

Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.

</details>


### [46] [MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization](https://arxiv.org/abs/2511.17888)
*Seulgi Jeong,Jaeil Kim*

Main category: cs.CV

TL;DR: 提出MINDiff：在推理阶段通过“负注意力+掩膜”抑制主体在无关区域的影响，缓解DreamBooth个性化过拟合，无需重训、可调节主体忠实度与文本对齐。


<details>
  <summary>Details</summary>
Motivation: 现有个性化（如DreamBooth）用类先验保持损失缓解少样本过拟合，但需额外训练开销，并在推理时限制用户控制；需要一种在推理期即可抑制过度主体主导、提升文本对齐且无需改模型的方法。

Method: 提出Mask-Integrated Negative Attention Diffusion（MINDiff）。在推理中修改跨注意力：对由掩膜标注的“与主体无关区域”施加负注意力，降低主体相关token在这些区域的注意力权重；引入可调参数lambda平衡主体保真与文本对齐，过程不改变模型结构、不参与训练，直接应用于已训练的DreamBooth模型。

Result: 在质化与量化实验中，相较类先验保持损失，MINDiff更有效缓解过拟合：更少的主体泄漏到无关区域，更好的文本对齐，同时保持较高主体保真；且因仅在推理期操作，计算成本更低、交互控制更灵活。

Conclusion: MINDiff通过掩膜引导的负注意力在推理期抑制主体在无关区域的影响，提升文本对齐并缓解过拟合，兼具零训练改动与可控性，能直接作用于现有DreamBooth模型。

Abstract: In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.

</details>


### [47] [Decoupled Audio-Visual Dataset Distillation](https://arxiv.org/abs/2511.17890)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出DAVDD：一种基于预训练与解耦的音视数据集蒸馏框架，通过公共/私有特征分离与双层对齐策略，在所有IPC设置上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配蒸馏难以捕捉跨模态对齐；直接跨模态交互易破坏各自模态的私有信息；且独立随机初始化的编码器造成模态映射空间不一致，训练不稳定、难收敛。

Method: 1) 预训练多样化编码器库获取稳定音频/视频特征；2) 轻量解耦器将特征分成公共与私有表示；3) 仅在公共表示上进行“公共跨模态匹配”（Common Intermodal Matching），并提出“样本—分布联合对齐”，同时在样本级与全局分布级对齐公共表示；4) 私有表示完全隔离跨模态交互以保留模态特异线索。

Result: 在多个基准与不同IPC（每类图像/样本数）设置下均达SOTA表现，优于以往DM及跨模态匹配方法，证明所提解耦表示与联合对齐策略有效。

Conclusion: 预训练支撑的稳定特征 + 公私解耦 + 仅对公共部分进行跨模态对齐 + 样本与分布双层对齐，是实现高质量音视数据集蒸馏的有效途径。代码将开源。

Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.

</details>


### [48] [CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation](https://arxiv.org/abs/2511.17904)
*Yuhang Ming,Chenxin Fang,Xingyuan Yu,Fan Zhang,Weichen Dai,Wanzeng Kong,Guofeng Zhang*

Main category: cs.CV

TL;DR: 提出CUS-GS：一种把多模态语义与结构化3D几何统一起来的紧凑高效高斯点渲表示，参数仅约600万但性能接近/优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点渲方法分为偏语义（缺显式几何）与偏结构（缺高层语义），缺乏同时兼顾语义抽象与精确几何的统一表示，且模型规模较大、效率欠佳。

Method: 1) 设计体素化锚点（voxelized anchor）作为空间脚手架，承载几何结构；2) 从CLIP、DINOv2、SEEM等基础模型提取多模态语义特征；3) 提出多模态潜特征分配机制，统一外观、几何与语义到一致潜空间，跨异构特征对齐；4) 提出特征感知的重要性评估，动态生长/剪枝锚点，去冗余同时保持语义完整性。

Result: 在多项实验中，以约600万参数取得与SOTA相当或具竞争力的结果；相较最近的35M参数对手，模型规模缩小约一个数量级，显示优异性能-效率权衡。

Conclusion: CUS-GS将结构化几何与多模态语义统一到紧凑的高斯表示中，并通过特征感知的锚点调度实现高效且语义保持的重建/理解，达成更小模型规模下的竞争性表现。

Abstract: Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.

</details>


### [49] [Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation](https://arxiv.org/abs/2511.17914)
*Chenyang Jiang,Hang Zhao,Xinyu Zhang,Zhengcen Li,Qiben Shan,Shaocong Wu,Jingyong Su*

Main category: cs.CV

TL;DR: 论文提出在长尾分布下进行数据集蒸馏的新模块ADSA，用于自适应校准软标签偏差，显著提升尾类与总体性能（ImageNet-1k-LT上尾类+11.8%，总准确率41.4%）。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏大多假设数据均衡，面对现实中的长尾类别分布表现不佳。作者发现软标签在长尾蒸馏中至关重要，但会受到数据不均衡引发的系统性偏差影响，从而导致泛化退化。需要一种能在蒸馏流程内处理软标签偏差的通用方法。

Method: 1) 推导不均衡感知的泛化上界，揭示长尾下由软标签误差导致的风险放大机制；2) 通过系统性扰动不均衡程度，定位两类软标签偏差来源：来自蒸馏模型本身与来自蒸馏生成图像；3) 设计ADSA（Adaptive Soft-label Alignment），在蒸馏过程中对软标签进行自适应对齐与校准，解耦并校正两类偏差；模块轻量、可无缝集成多种蒸馏管线。

Result: 在ImageNet-1k-LT上，结合EDC、IPC=50时，ADSA使尾类准确率提升最高11.8%，总体准确率达41.4%；在有限标签预算与多种蒸馏技术上均有稳健增益。

Conclusion: 软标签偏差是长尾数据集蒸馏性能下降的关键因素。ADSA能自适应对齐并校准偏差，普适、轻量、可泛化，显著提升长尾与整体性能，适用于多种蒸馏框架。

Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.

</details>


### [50] [Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization](https://arxiv.org/abs/2511.17918)
*Youngsik Yun,Dongjun Gu,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出FASR，在3D Gaussian Splatting的少样本视角下通过频率自适应的锐度正则改善泛化，抑制floater并保留高频细节，优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 3DGS在稀疏观测/少视角时容易对训练视角过拟合，导致新视角泛化差、出现漂浮伪影与细节缺失。现有如SAM的通用锐度最小化方法不适配重建任务：要么过强导致细节过平滑，要么过弱难以抑制尖锐损失地形。需要一种面向重建、能兼顾高频与稳定性的泛化导向优化。

Method: 将新视角合成视为泛化问题，重构3DGS的训练目标为频率自适应的“损失地形锐度”正则：根据图像局部频率动态设定正则权重与在参数空间估计局部锐度时的邻域半径。高频区域减小正则或邻域、低频区域增强，从而在不牺牲细节的前提下降低锐度，避免过拟合和floater。

Result: 在多数据集与多配置下，相比SAM和多种3DGS基线，FASR稳定提升新视角质量：减少floater/伪影，恢复细节，整体指标与视觉效果更好。

Conclusion: 频率自适应的锐度正则可有效改善3DGS在少样本下的泛化，兼顾高频细节与稳定渲染；相较将SAM直接应用于重建任务更合适、鲁棒且通用。代码将开源。

Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.

</details>


### [51] [PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning](https://arxiv.org/abs/2511.17927)
*Yingjie Ma,Xun Lin,Yong Xu,Weicheng Xie,Zitong Yu*

Main category: cs.CV

TL;DR: 提出PA-FAS：通过扩展高质量推理序列与答案打乱监督，提升多模态人脸活体检测的推理、泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态FAS在融合、跨域与可解释性方面虽有进展，但SFT+RL在多模态场景下受限：推理路径稀缺、探索空间受限，以及单一任务监督与多样化推理路径不匹配，导致模型走捷径、忽视真正跨模态推理。

Method: 1) 路径增强：基于有限标注构建高质量、扩展的多模态推理序列，丰富推理路径并放松RL探索约束；2) 答案打乱（answer-shuffling）机制：在SFT阶段随机化答案与表述，以迫使模型进行全面的跨模态分析，减少依赖表层线索与捷径；3) 将上述与RL结合，进行策略驱动的多模态推理学习。

Result: PA-FAS在多模态推理准确率、跨域泛化能力上显著优于SFT+RL等基线，并在融合、泛化与可解释性三者之间取得更好的统一。

Conclusion: 通过扩展推理路径与答案打乱的策略化训练，可有效缓解多模态FAS中路径不足与监督不匹配问题，促进更深层的跨模态推理与可信活体检测。

Abstract: Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.

</details>


### [52] [MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection](https://arxiv.org/abs/2511.17929)
*Hui Lu,Yi Yu,Shijian Lu,Deepu Rajan,Boon Poh Ng,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 提出MambaTAD：基于结构化状态空间模型（SSM）的端到端一阶段TAD方法，解决长程建模衰减与全局建模冲突，借助DMBSS与全局特征融合检测头，在多基准上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有TAD方法在处理长时跨度动作时常受限于：1) 传统卷积/自注意缺乏高效全局上下文或计算昂贵；2) SSM虽线性高效，但递归处理导致时间上下文衰减、全局建模时元素间冲突；3) 检测头缺乏全局感知与多粒度融合，难以准确定位长动作边界。

Method: 提出MambaTAD，包括：1) 斜对角掩蔽双向状态空间模块（DMBSS），通过双向与对角掩蔽机制缓解递归衰减、避免自元素冲突，实现全局特征融合与长程依赖建模；2) 全局特征融合检测头，逐级利用多粒度特征进行全局感知与边界细化；3) 状态空间时间适配器（SSTA），以线性复杂度进行端到端一阶段训练/推理，降低参数与计算。

Result: 在多个公开TAD基准上取得一致且优于现有方法的性能（摘要未给出具体mAP/框架对比，但强调跨数据集一致领先）。

Conclusion: 通过引入DMBSS与全局融合检测头，并以SSTA实现端到端一阶段、线性复杂度的TAD，MambaTAD有效解决SSM在TAD中的长程衰减与全局冲突问题，显著提升长跨度动作检测与整体TAD性能。

Abstract: Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.

</details>


### [53] [UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection](https://arxiv.org/abs/2511.17930)
*Yuan Qu,Zhipeng Zhang,Chaojun Xu,Qiao Wan,Mengying Xie,Yuzeng Chen,Zhenqi Liu,Yanfei Zhong*

Main category: cs.CV

TL;DR: 提出UniRSCD：基于状态空间模型主干与频域变化提示生成器的统一编码器，配合统一解码器与任务自适应预测头，实现BCD/SCD/BDA等多粒度变化检测的一体化架构，并在五个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测针对不同任务（如二值变化、语义变化、损伤评估）常需设计特定解码器来弥补编码信息损失，依赖专家经验，导致模型选择不确定性与泛化受限，尤其在突发灾害场景中难以快速部署统一方案。

Method: 以状态空间模型（SSM）为骨干，引入“频率变化提示生成器”作为统一编码器：动态扫描双时相全局上下文，将高频细节与低频全局信息融合，减少对专用解码器的依赖。随后通过统一解码器与预测头，利用层级特征交互建立共享表示空间，并通过任务自适应的输出映射同时适配BCD、SCD、BDA等不同输出粒度任务。

Result: 在五个数据集上验证，涵盖LEVIR-CD（二值变化）、SECOND（语义变化）与xBD（建筑损伤评估）等，取得领先性能，显示在多任务、多粒度场景下的适配性与效果。

Conclusion: UniRSCD提供了一个无需专用解码器的通用变化检测框架，通过频域提示与统一的编码-解码/预测设计，兼顾高频细节与全局上下文，实现跨任务的一体化与SOTA表现，提升了在突发事件中的可用性与部署效率。

Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.

</details>


### [54] [Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion](https://arxiv.org/abs/2511.17932)
*Yan Xu,Yixing Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 利用预训练视频扩散模型，在测试时对稀疏多视角数据进行“自然视频补全”，生成不在原始相机轨迹中的中间视角帧，并与3D Gaussian Splatting迭代交互，从而在无场景特定训练下显著提升极稀疏条件下的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏输入下的新视角合成常因未观测区域监督不足而失败；传统方法难以在极少视角间保持空间一致性与细节。作者希望借助强大的生成先验，在不进行场景特定训练的情况下填补视角/时空空缺，实现更连贯、更高保真度的重建与渲染。

Method: 将问题表述为“测试时自然视频补全”。利用预训练视频扩散模型零样本生成在新相机位姿处的伪视图，并通过不确定性感知的调制机制保证空间一致性；这些合成帧用于稠密化对3D Gaussian Splatting的监督。采用迭代反馈：3D几何重建与2D生成彼此校正，逐步提升几何与视图质量。

Result: 在LLFF、DTU、DL3DV、MipNeRF-360等基准上，在极端稀疏视角条件下，相较强力3D-GS基线显著提升重建与渲染质量，生成结果连贯且高保真。

Conclusion: 零样本、生成引导的测试时视频补全能有效弥补稀疏多视角间的空缺，并通过与3D-GS的迭代协同提升重建与合成效果，无需对特定场景进行训练或微调。

Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \emph{completing a natural video} unfolding through space.
  We recast the task as \emph{test-time natural video completion}, using powerful priors from \emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
  The result is coherent, high-fidelity renderings from sparse inputs \emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.

</details>


### [55] [V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction](https://arxiv.org/abs/2511.17941)
*Xiangyan Kong,Xuecheng Wu,Xiongwei Zhao,Xiaodong Li,Yunyun Shi,Gang Wang,Dingkang Yang,Yang Liu,Hong Chen,Yulong Gao*

Main category: cs.CV

TL;DR: V2X-RECT提出面向高密度交通的V2X协同轨迹预测框架，通过稳定跨视角目标关联、信号引导的有效交互筛选与可复用的局部时空编码，实现更准更快的预测，在V2X-Seq与V2X-Traj上显著优于SOTA并具更强鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 密集交通下视线受限导致车辆/路侧感知不全；多源目标ID频繁切换破坏跨视角特征融合；多源交互易冗余；车辆为中心的重复历史编码拖累实时推理。需要一个在高密度场景下既稳健关联又高效编码与预测的框架。

Method: 1) 多源身份匹配与纠正规模：利用多视角时空关系做稳定一致的目标关联，缓解误匹配对编码与融合的影响。2) 交通信号引导的交互模块：将红绿灯相位变化趋势编码为特征，基于通行权约束筛选关键交互体，捕捉信号动态对交互模式的影响。3) 局部时空坐标编码：让历史轨迹与地图特征可复用，支持并行解码，减少重复编码并提升推理效率。

Result: 在V2X-Seq与V2X-Traj数据集上，相比SOTA取得显著性能提升；在不同交通密度下保持更强鲁棒性；推理效率明显提升。

Conclusion: V2X-RECT通过稳定关联、信号引导交互与可复用时空编码，缓解高密度场景的ID切换与冗余问题，实现高效、准确且鲁棒的V2X协同轨迹预测，具备实际部署潜力。

Abstract: V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.

</details>


### [56] [SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System](https://arxiv.org/abs/2511.17943)
*Zhiyu Xu,Weilong Yan,Yufei Shi,Xin Meng,Tao He,Huiping Zhuang,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 提出SciEducator：一个基于德明环PDSA的自进化多智能体系统，用于科学视频理解与教学，并配套SciVBench基准；在多模态教育内容生成与科学问答上显著优于主流MLLM与视频代理。


<details>
  <summary>Details</summary>
Motivation: 通用MLLM与视频代理在科学视频场景下常因缺少专业知识整合与严谨的逐步推理而表现欠佳，影响科学过程解读与教学应用，需要一种能持续迭代优化、可解释并能输出教育材料的系统。

Method: 将管理学中的德明环（Plan-Do-Study-Act）映射为自进化推理与反馈框架，构建多代理协同：规划理解目标与信息需求（Plan），执行视频解析与外部知识检索/融合（Do），对推理与生成结果进行评估与误差分析（Study），基于反馈调整策略与提示再迭代（Act）。系统可生成多模态教学产物（文本步骤、可视化指引、音频讲解、交互参考）。同时构建包含500个专家校验、文献支撑的跨物理/化学/日常现象的SciVBench用于评测。

Result: 在SciVBench上，SciEducator显著超越领先闭源MLLM（如Gemini、GPT-4o）与最新视频代理，在科学视频理解与教育任务上取得SOTA表现。

Conclusion: 基于PDSA的自进化多代理框架能有效解决科学视频中的知识整合与严谨推理难题，并能产出高质量多模态教学内容；SciEducator与SciVBench为该方向提供了新范式与标准化评测基础。

Abstract: Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.

</details>


### [57] [Test-Time Temporal Sampling for Efficient MLLM Video Understanding](https://arxiv.org/abs/2511.17945)
*Kaibin Wang,Mingbao Lin*

Main category: cs.CV

TL;DR: 提出一种无需训练、可即插即用的测试时时间采样方法T3S，通过在推理时生成多个短且多样的子序列并在一次前向中打包处理、汇聚预测，降低长视频在MLLM中的二次方注意力开销，同时提升准确率并减少首token延迟。


<details>
  <summary>Details</summary>
Motivation: 长视频输入会导致MLLM自注意力计算量随视频token数量二次增长，推理慢且资源消耗大。现有子采样、帧选择器或记忆式摘要方案要么牺牲准确率，要么需额外训练或导致推理减速，难以兼顾效率与效果。

Method: 在不改模型且无需训练的前提下，T3S在推理阶段从长视频token流中采样多个短、彼此多样的时间子序列（利用时空冗余），将其在一次前向传播中并行打包输入MLLM，并对各子序列的输出进行聚合（投票/加权等）。该多子序列策略使注意力复杂度从O(L^2)降为O(∑α_i^2 L^2)，其中∑α_i^2<1，从而节省计算同时扩大视觉覆盖。

Result: 在长视频理解基准上，T3S最高提升准确率3.1%，将首token时延降低约2.04倍，并保持最小集成成本；适配多种预训练MLLM且仅在推理阶段工作。

Conclusion: T3S把视频的时空冗余转化为计算优势，为长视频理解提供可扩展、通用、训练无关的推理加速与性能提升方案，兼容广泛MLLM而无需改动或微调。

Abstract: Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\sum_{i=1}^m α_i^2L^2)$, where $\sum_{i=1}^m α_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.

</details>


### [58] [Multi-speaker Attention Alignment for Multimodal Social Interaction](https://arxiv.org/abs/2511.17952)
*Liangyang Ouyang,Yifei Huang,Mingfang Zhang,Caixin Kang,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 论文提出一种在多说话者视频场景中增强跨模态对齐的无参注意力偏置方法，使现有MLLM在社交理解任务上显著提升并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在社交互动视频中仅加入视觉输入效果不稳，尤其多说话者场景下，视觉与文本token缺乏与说话者一致的对齐，跨模态注意力弱于物体中心图像任务，导致对“谁在说、说给谁、伴随何种目光与手势”的推理不可靠。

Method: 提出“多模态多说话者注意力对齐”框架：1) 动态跨模态头选择，自动识别负责视觉-文本对齐的注意力头；2) 社交感知自适应注意力偏置，基于已有注意力模式与说话者位置计算，对注意力进行注入，强化说话者的视觉表征与其语句的对齐；该方法无需新增可训练参数或改动架构。集成到LLaVA-NeXT-Video、Qwen2.5-VL、InternVL3。

Result: 在TVQA+、MMSI、OnlineMMSI三基准、四类社交任务上全面提升，达到SOTA；注意力可视化显示模型更聚焦与说话者相关的区域，提升多方社交推理鲁棒性。

Conclusion: 通过选择关键跨模态头并注入社交感知偏置，可在不改变模型结构或增加参数的前提下，显著增强MLLM在多说话者视频社交理解中的对齐与推理能力，具备普适可集成性。

Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.

</details>


### [59] [HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation](https://arxiv.org/abs/2511.17958)
*Yulong Shi,Jiapeng Li,Lin Qi*

Main category: cs.CV

TL;DR: 提出HEAL：一种面向源数据不可用的无监督领域自适应（SFUDA）框架，通过分层去噪、边缘引导样本选择、尺寸感知融合与免训练特性，提升跨模态迁移性能并取得SOTA。


<details>
  <summary>Details</summary>
Motivation: SFUDA需在无法访问源域数据且目标域无标注的条件下解决域偏移，但缺乏源数据与监督会导致伪标签噪声高、样本选择不稳与多尺度/多模态差异难以对齐。

Method: HEAL框架包含四个核心模块：1) Hierarchical denoising：逐层/多阶段去噪以抑制伪标签与特征噪声；2) Edge-guided selection：利用边缘与结构线索筛选可靠目标样本或区域，降低错误传播；3) size-Aware fusion：考虑目标物体/器官尺度与多尺度特征进行融合与一致性约束；4) Learning-free characteristic：引入部分无需额外训练的算子/规则，降低过拟合与计算开销，稳定适配流程。

Result: 在大规模跨模态实验（如不同成像模态/机构/设备）上显著优于现有SFUDA方法，达到SOTA；代码已开源。

Conclusion: 通过分层去噪、边缘引导与尺寸感知融合并结合免训练组件，HEAL在源不可用、目标无标注的严苛设定下有效缓解域偏移，提升泛化与鲁棒性，具有实践落地价值。

Abstract: Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.

</details>


### [60] [VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment](https://arxiv.org/abs/2511.17962)
*Ziheng Jia,Linhan Cao,Jinliang Han,Zicheng Zhang,Jiaying Qian,Jiarui Wang,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出VITAL系列用于视觉质量评估（VQualA）的多模态大模型，通过以视觉编码器为中心的生成式预训练、4.5M规模VL数据与多任务训练，实现跨图像/视频的评分与解释，具备强零样本、低成本迁移与高可扩展的模型库能力。


<details>
  <summary>Details</summary>
Motivation: 现有VQualA类LMM多聚焦单一任务，并依赖全参数微调，容易过拟合特定模态或任务，导致泛化与可迁移性不足。需要一种既通用又强大的训练与架构范式，兼顾评分精度与可解释性，并能高效扩展到不同解码器/任务。

Method: 1) 机器执行的标注-审校流程，构建>4.5M的视觉-语言训练对；2) 多任务联合训练，同时优化质量“打分”精度与“解释”能力，覆盖图像与视频；3) 以视觉编码器为中心的生成式预训练与模型库设计：固定/复用强视觉编码器，配对不同解码器，解码器仅需极少量数据快速升温即可达全训练水平。

Result: VITAL系列在零样本设置下表现强劲；每个新配对解码器只需不到1/1000预训练数据的快速热身即可达到与完全训练模型相当的效果；在多任务（评分+解释）与多模态（图像/视频）上获得稳健表现。

Conclusion: 以视觉编码器为核心的生成式预训练与多任务策略，有效缓解过拟合并提升泛化与迁移，构建出易扩展、低成本适配、强零样本能力的VQualA基础型LMM，为视觉质量评估领域的基础模型奠定基石。

Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.
  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.

</details>


### [61] [X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.17964)
*Chenyang Yu,Xuehu Liu,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出X-ReID框架，结合跨模态原型协作与多粒度时空交互，实现视频可见-红外行人重识别的跨模态对齐与时序建模，SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型在检索任务强，但用于视频可见-红外行人重识别（VVI-ReID）研究不足。该任务面临两大难点：可见与红外的模态鸿沟，以及如何充分利用视频序列的时空信息。

Method: 1) 跨模态原型协作（CPC）：构建并对齐不同模态的特征原型，进行协同学习以缩小模态差异并促进融合。2) 多粒度信息交互（MII）：包含相邻帧的短期交互、跨多帧的长期信息融合，以及在多粒度层面进行跨模态对齐，强化时序建模并进一步减小模态差距。3) 融合上述多粒度信息，得到鲁棒的序列级表示。

Result: 在两个大规模VVI-ReID基准（HITSZ-VCM与BUPTCampus）上取得优于现有方法的性能（具体指标未在摘要中给出）。

Conclusion: X-ReID通过CPC对齐模态、MII增强时序与跨模态交互，获得稳健的序列表示，在VVI-ReID任务上达到SOTA；代码已开源。

Abstract: Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.

</details>


### [62] [Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification](https://arxiv.org/abs/2511.17965)
*Yangyang Liu,Yuhao Wang,Pingping Zhang*

Main category: cs.CV

TL;DR: 提出Signal框架，通过选择性交互和全局-局部对齐，提升多模态目标ReID的判别性并抑制背景干扰，在RGBNT201/100与MSVR310上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态ReID多强调模态特征融合，但忽视背景干扰；且常做两两模态对齐，缺少整体多模态一致性约束，导致对齐不足与判别力下降。

Method: 提出Signal框架包含三大组件：1) 选择性交互模块SIM：在各模态内外选择重要patch token，并与class token交互，强化判别信息、抑制无关背景；2) 全局对齐模块GAM：在Gram矩阵空间以最小化三维多面体体积的方式同时对齐多模态全局特征，实现多模态一致性；3) 局部对齐模块LAM：以shift-aware方式对齐局部特征，缓解视角/位移引起的错配。

Result: 在RGBNT201、RGBNT100、MSVR310三大多模态ReID数据集上做了大量实验，方法取得显著性能提升（摘要未给出具体数值），验证框架有效性。

Conclusion: 选择性交互+全局/局部一致性对齐能有效抑制背景干扰并增强多模态特征判别力；Signal在多个基准上表现优异，代码已开源。

Abstract: Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.

</details>


### [63] [CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking](https://arxiv.org/abs/2511.17967)
*Hao Li,Yuhao Wang,Xiantao Hu,Wenning Hao,Pingping Zhang,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出CADTrack：通过Mamba特征交互、基于MoE的上下文聚合、以及可变形对齐与时序传播，缓解RGB与热红外模态差异与空间错位，实现高效稳健的全天气跟踪，在五个基准上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有RGBT跟踪器难以充分应对可见光与热红外模态分布差异与空间错位，导致跨模态信息传播/融合受阻、鲁棒性与精度下降；同时计算开销与特征判别性难以兼顾。

Method: 整体框架CADTrack包含三大组件：1) Mamba-based Feature Interaction（MFI）：基于状态空间模型（SSM/Mamba）实现线性复杂度的跨模态高效交互，提升判别性并降低计算量；2) Contextual Aggregation Module（CAM）：以Mixture-of-Experts为基础，利用稀疏门控动态激活骨干的不同层专家，将跨层上下文互补信息编码融合；3) Deformable Alignment Module（DAM）：结合可变形采样与时序传播，缓解空间错位与定位漂移，增强时空一致性。

Result: 在五个RGBT跟踪基准上进行大量实验，CADTrack取得更高的精度与鲁棒性（摘要未给出具体数值），显示组件设计带来显著性能提升与较优的计算效率。

Conclusion: 通过MFI、CAM与DAM的协同，CADTrack有效缩小模态差异、提升跨模态信息融合与时空对齐能力，从而在复杂场景下实现稳健、准确且高效的RGBT跟踪，并在多基准上验证了方法有效性；代码已开源。

Abstract: RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.

</details>


### [64] [Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2511.17973)
*Hiroto Honda*

Main category: cs.CV

TL;DR: 提出APR：通过对新任务图像进行对抗扰动来在线合成旧类伪回放，无需存储样本，并配合协方差校准以缓解遗忘，达SOTA。


<details>
  <summary>Details</summary>
Motivation: EFCIL无法保存旧任务图像，导致稳定-可塑性矛盾与灾难性遗忘。需要在无样本回放的条件下保留旧知识并吸收新类。

Method: 1) 对抗伪回放：对新任务图像施加针对“扩增后的旧类均值原型”的目标导向对抗攻击，生成伪回放图像；2) 用这些伪回放进行蒸馏，抑制语义漂移；3) 任务结束后基于伪回放样本学习迁移矩阵以校准类条件协方差，从而补偿漂移。

Result: 在标准EFCIL基准的冷启动设置上达到SOTA表现，兼顾稳定与可塑性。

Conclusion: APR可在无存储旧样本的前提下，通过对抗伪回放与协方差校准有效缓解遗忘，实现更好的稳定-可塑性权衡，取得领先性能。

Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.

</details>


### [65] [FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning](https://arxiv.org/abs/2511.17979)
*Bo Yin,Xiaobin Hu,Xingyu Zhou,Peng-Tao Jiang,Yue Liao,Junwei Zhu,Jiangning Zhang,Ying Tai,Chengjie Wang,Shuicheng Yan*

Main category: cs.CV

TL;DR: 提出FeRA：一种频率驱动的扩散模型微调框架，通过对齐参数更新与去噪过程中的频率能量演化来提升适配效果。核心包括频率能量指示器、软频率路由（多频专家适配器）、以及频率能量一致性正则；训练与推理均进行动态路由，兼容多种Adapter方案与扩散骨干/分辨率，带来稳定、简单且鲁棒的适配。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练扩散模型在新任务上的高效适配仍具挑战。作者观察到扩散去噪过程中存在可描述的频率能量重建机制，推断若微调过程能与这种频率能量随时间（步）演化的内在规律对齐，可能更稳定、更高效，从而激发提出一个频率域视角的适配框架。

Method: 1) 频率能量指示器：在潜空间按频带刻画能量分布，形成紧凑表征以感知当前步的频域状态。
2) 软频率路由：基于指示器对多个“频率特定”Adapter专家进行加权融合，训练与推理期均动态选择/融合专家，实现频带自适应参数更新。
3) 频率能量一致性正则：约束不同频带的优化过程与能量演化保持一致，稳定训练并促进跨频带协同。
框架与现有Adapter微调方案无缝结合，适配多种扩散骨干与分辨率。

Result: FeRA在多种扩散模型与分辨率、以及不同下游任务中表现稳健，较现有Adapter式微调更稳定、更高效，推理期动态路由可直接应用并提升鲁棒性与泛化。

Conclusion: 将微调与扩散去噪的频率能量机制对齐是有效途径。FeRA通过频率能量指示、软路由与一致性正则构成简洁通用的频域适配范式，能稳定提升扩散模型在新任务上的适配性能并具良好兼容性与推广性。

Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.

</details>


### [66] [Plan-X: Instruct Video Generation via Semantic Planning](https://arxiv.org/abs/2511.17986)
*Lun Huang,You Xie,Hongyi Xu,Tianpei Gu,Chenxu Zhang,Guoxian Song,Zenan Li,Xiaochen Zhao,Linjie Luo,Guillermo Sapiro*

Main category: cs.CV

TL;DR: Plan-X通过引入“语义规划器”在视频扩散生成前先做高层语义规划，以文本化的时空语义token指导扩散模型，从而显著减少幻觉、提升与指令对齐并实现细粒度、具上下文一致性的视频生成。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer虽擅长高保真视觉合成，但在高层语义推理、长程规划上薄弱，导致在人物-物体交互、多阶段动作、复杂场景理解与结合上下文的运动推理中出现视觉幻觉与指令偏离。需要一种能把语言模型的规划推理能力与扩散模型的细节合成能力结合的机制。

Method: 提出Plan-X框架：核心为可学习的多模态语言模型“语义规划器”，从文本与视觉上下文推理用户意图，自回归生成一串文本可对齐的时空语义token；这些token作为结构化“语义草图”随时间引导视频扩散模型合成具体画面，实现规划-到-生成的协同。

Result: 在多种评测与场景下，Plan-X相较纯扩散Transformer显著降低视觉幻觉，提升对用户指令与多模态上下文的一致性，实现更细粒度的动作与交互控制。

Conclusion: 通过显式高层语义规划并以语义token指导扩散生成，Plan-X有效融合语言模型的推理规划优势与扩散模型的高保真合成能力，达到更可靠、可控、对齐性更强的视频生成。

Abstract: Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.

</details>


### [67] [HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.17988)
*Haodong Chen,Xianfei Han,Qwen*

Main category: cs.CV

TL;DR: 提出HyM-UNet：浅层用CNN保纹理，深层用Visual Mamba捕捉全局依赖，并以MGF-Skip用深层语义门控浅层特征；在ISIC 2018上以更少参数与更低延迟取得优于SOTA的Dice/IoU。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割需要同时掌握局部细节与全局结构。CNN因局部感受野难以建模长程依赖，而纯Transformer类方法代价高、推理慢。需要一种兼具局部纹理保留与高效全局建模的架构。

Method: 提出混合架构HyM-UNet：1) 分层编码器——浅层卷积模块保留高频纹理，深层引入线性复杂度的Visual Mamba以建模长距离语义；2) Mamba-Guided Fusion Skip (MGF-Skip)——以深层语义特征为门控信号，对浅层跳连进行动态抑噪，强化模糊边界感知；3) 与解码器配合完成端到端分割。

Result: 在ISIC 2018皮肤病变基准上广泛实验，HyM-UNet在Dice和IoU上显著优于现有SOTA，同时参数量与推理延迟更低。

Conclusion: 混合CNN+Mamba并辅以语义门控跳连，可在复杂形状与尺度变化的医学分割任务中实现更优精度与效率，方法有效且鲁棒。

Abstract: Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.

</details>


### [68] [SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining](https://arxiv.org/abs/2511.17993)
*Jiayu Wang,Haoyu Bian,Haoran Sun,Shaoning Zeng*

Main category: cs.CV

TL;DR: 提出SD-PSFNet：在多阶段复原框架中显式引入可学习PSF物理机制，动态建模雨丝退化并跨阶段自适应融合，实现从粗到细的去雨与细节恢复，指标达SOTA。


<details>
  <summary>Details</summary>
Motivation: 雨丝具有多尺度、与场景强耦合的光学成因（散射、点扩散等），传统学习或纯物理模型难以准确刻画与逐步校正退化过程；需要一种能在网络中显式嵌入物理先验、动态估计并反复细化退化的框架。

Method: 构建三阶段级联的顺序复原架构；在各阶段引入带可学习参数的PSF组件，动态模拟雨丝点扩散与成像过程，实现雨与背景的可分离建模；每一阶段用新的PSF模块对估计与输出进行再评估与细化；通过自适应门控融合跨阶段特征，完成从粗去雨到细节重建的级联增强。

Result: 在Rain100H获得33.12dB/0.9371，在RealRain-1k-L达42.28dB/0.9872，在RealRain-1k-H达41.08dB/0.9838，均达SOTA水平；在复杂场景与高密度降雨条件下表现优异。

Conclusion: 将PSF物理机制与多阶段顺序复原、动态建模和自适应融合结合，可有效刻画并校正雨致退化，实现高质量去雨与细节恢复，为物理感知的图像复原提供可推广范式。

Abstract: Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.

</details>


### [69] [RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale](https://arxiv.org/abs/2511.18005)
*Shengyuan Wang,Zhiheng Zheng,Yu Shang,Lixuan He,Yangcheng Yu,Fan Hangyu,Jie Feng,Qingmin Liao,Yong Li*

Main category: cs.CV

TL;DR: RAISECity提出一个面向城市级3D世界生成的“现实对齐”智能合成引擎，通过多模态代理式框架整合外部工具与自反迭代，显著提升形状精度、纹理真实感与规模化构建能力，相较基线在感知质量上>90%胜率。


<details>
  <summary>Details</summary>
Motivation: 现有城市级3D生成在质量、真实对齐与可扩展性上存在明显不足：形状与纹理失真、误差累积、跨大范围场景的稳定构建困难，以及与传统图形学管线兼容性不足，限制了沉浸式媒体、具身智能与世界模型的发展。

Method: 提出RAISECity，一个“代理式”多模态框架：- 利用多模态基础工具（检索/解析/视觉-语言/几何与纹理工具）获取真实世界知识；- 维护稳健的中间表示以组织大型场景；- 采用动态数据处理、迭代自反与自我修正；- 按需调用高级多模态工具完成形状重建、纹理生成、布局与细节完善；- 与计算机图形学管线无缝对接以支持大规模城市级构建。

Result: 在大量定量与定性实验中，RAISECity在现实对齐、形状精度、纹理保真与美学评分方面优于现有方法，对总体感知质量达到>90%胜率；展示了在城市尺度上的可扩展与稳健性。

Conclusion: 代理式多模态合成与自反迭代能够显著降低误差累积，提升城市级3D生成的质量与现实对齐度；RAISECity具备与传统图形学流程的兼容与规模化能力，是沉浸式媒体、具身智能与世界模型的有力基础。

Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \textbf{R}eality-\textbf{A}ligned \textbf{I}ntelligent \textbf{S}ynthesis \textbf{E}ngine that creates detailed, \textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.

</details>


### [70] [Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging](https://arxiv.org/abs/2511.18007)
*Siteng Ma,Honghui Du,Prateek Mathur,Brendan S. Kelly,Ronan P. Killeen,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TL;DR: 提出一套针对纵向医学影像变化检测的深度主动学习框架LMI-AL，通过构建基线与随访3D影像的2D切片配对与差分，迭代选择最具信息量的样本进行标注，仅用<8%标注即可接近全标注模型表现。


<details>
  <summary>Details</summary>
Motivation: 纵向医学影像的变化检测需要跨时点标注，代价高且易漏检细微变化；现有主动学习多聚焦于静态任务（分类/分割），难以直接用于多时相、微小差异的变化检测，迫切需要面向该场景的DAL方法以降低标注成本。

Method: 将基线与随访3D影像全部2D切片两两配对并做差分，构造成变化检测样本池；在主动学习循环中，根据不确定性/信息量准则选择最具价值的切片对进行人工标注，用被选样本增量训练深度模型，直至性能收敛或标注预算耗尽。

Result: 在实验中，标注不到8%的数据即可达到与全量标注训练相当的变化检测性能；同时给出消融与误差分析，验证配对-差分策略与主动选择机制的有效性。

Conclusion: LMI-AL能在极低标注成本下实现接近全监督的纵向变化检测效果，为纵向医学影像主动学习提供可行方案与实践指南，代码已开源以促进复现与后续研究。

Abstract: Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.

</details>


### [71] [RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios](https://arxiv.org/abs/2511.18011)
*Jun Zhang,Jie Feng,Long Chen,Junhui Wang,Zhicheng Liu,Depeng Jin,Yong Li*

Main category: cs.CV

TL;DR: 提出RoadBench，一个评估MLLM在城市场景细粒度空间理解与推理能力的基准，聚焦道路标线，包含6类任务与9121个严格人工校验样例，使用BEV与FPV图像，系统测试识别、联合理解、全局推理及与领域知识融合能力；对14个模型评测显示任务具有挑战性，部分模型甚至不如规则或随机基线。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽具一般空间理解/推理能力，但在复杂城市场景中的细粒度空间理解（如道路标线）研究不足；道路标线作为城市交通网络关键元素，迫切需要一个系统化基准来量化并推进模型能力。

Method: 构建RoadBench：围绕道路标线与城市交通系统设计六类任务，覆盖从局部识别到全局推理的层次化评估框架；采用俯视图（BEV）与第一人称视角（FPV）图像作为输入；收集并严格人工校验9121个测试样例；对14个主流MLLM进行系统评测，并与规则/随机基线比较。

Result: RoadBench揭示当前主流MLLM在城市细粒度空间理解与推理方面存在显著短板；在某些任务上表现低于简单规则或随机选择基线，显示任务难度高、模型泛化与知识整合不足。

Conclusion: RoadBench为评估和提升MLLM在城市细粒度空间理解与推理方面提供了系统基准与实证证据；其发现将推动模型在整合视觉细节与交通领域知识方面的改进。

Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.

</details>


### [72] [State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection](https://arxiv.org/abs/2511.18012)
*Jiaying Zhou,Qingchao Chen*

Main category: cs.CV

TL;DR: 提出两种原型增强：状态增强语义原型（SESP）+场景增强伪原型（SAPP），分别解决类内状态多样性与区域-文本语义错配，提升WS-OVOD的识别与对齐效果。


<details>
  <summary>Details</summary>
Motivation: WS-OVOD在开放类识别中面临两难：1) 静态/稀疏的语义原型无法覆盖同一类别因姿态、动作、状态带来的巨大外观变化；2) 伪框生成把含上下文的视觉候选与“对象中心”的文本嵌入强行对齐，产生语义错配，影响伪标签质量与跨模态对齐。

Method: 提出两个互补模块：1) SESP：利用LLM生成“状态感知”的类别描述（如“睡觉的猫”“跳跃的猫”），形成更细粒度、覆盖多状态的文本原型，用于训练和匹配；2) SAPP：在伪标签阶段将上下文语义（如“猫躺在沙发上”）纳入，构造场景化伪原型，并采用软对齐机制，使视觉区域与包含上下文的文本表示更一致，缓解错配。

Result: 集成SESP与SAPP后，语义原型更丰富、视觉-文本对齐更稳健，伪框质量与识别泛化能力提升，在WS-OVOD基准上取得显著改进。

Conclusion: 通过状态增强和场景增强的双重原型设计，既解决类内状态多样性，又缓解视觉候选与文本原型的语义缺口，从而全面提升WS-OVOD的检测表现。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.

</details>


### [73] [Modeling Retinal Ganglion Cells with Neural Differential Equations](https://arxiv.org/abs/2511.18014)
*Kacper Dobek,Daniel Jankowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: 研究比较了LTC与CfC在虎斑蝾螈视网膜神经节细胞建模中的表现，优于CNN与LSTM：更低MAE、更快收敛、更小模型与良好查询时间，但皮尔逊相关略低；适合小数据与需频繁再训练的边缘场景（如视觉假体）。


<details>
  <summary>Details</summary>
Motivation: 视网膜神经节细胞时序放电具有连续时间、非平稳与资源受限的建模需求。传统离散时间深度网络（CNN/LSTM）在少样本、低延迟、低功耗环境下往往效率不足。LTC与CfC作为连续时间神经ODE风格模型，可能在参数效率、训练稳定性与线上推理上更有优势，值得系统评估。

Method: 在三组虎斑蝾螈数据集上，以LTC与CfC建模RGC活动，并与卷积基线与LSTM对比。指标包括MAE、皮尔逊相关、收敛速度、模型大小与查询时间。

Result: LTC与CfC相较于CNN/LSTM：MAE更低、收敛更快、参数量/模型体积更小、推理查询时间有优势；但皮尔逊相关略低。

Conclusion: LTC与CfC在资源受限与需频繁再训练的设置中具备综合优势，适合边缘部署（如视觉假体）。尽管相关度略降，但整体精度-效率权衡更优。

Abstract: This work explores Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) for modeling retinal ganglion cell activity in tiger salamanders across three datasets. Compared to a convolutional baseline and an LSTM, both architectures achieved lower MAE, faster convergence, smaller model sizes, and favorable query times, though with slightly lower Pearson correlation. Their efficiency and adaptability make them well suited for scenarios with limited data and frequent retraining, such as edge deployments in vision prosthetics.

</details>


### [74] [MambaX: Image Super-Resolution with State Predictive Control](https://arxiv.org/abs/2511.18028)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Naoto Yokoya,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 提出MambaX：用非线性状态预测控制来做光谱/多模态超分辨，能在重建过程的中间状态介入，控制误差传播，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SR多直接追求最终像素提升，忽视中间阶段误差传播与累积的可控性；Mamba虽可将重建过程建模为状态序列便于干预，但采用固定线性映射，感受野窄、灵活性不足，难以处理细粒度图像与多模态光谱数据。

Method: 构建非线性状态预测控制模型MambaX：将连续光谱带映射到潜在状态空间，动态学习控制方程的非线性状态参数；通过动态状态预测控制近似非线性微分系数；提出“状态交叉控制”范式实现多模态SR融合；采用渐进过渡式学习以缓解域/模态差异带来的异质性。

Result: 在单幅图像SR和多模态融合SR任务上，MambaX优于现有序列模型与SR方法，显示动态光谱-状态表征在不同维度与模态上的泛化优势。

Conclusion: 通过将SR过程视为可控的状态空间演化，MambaX实现对中间状态与误差传播的有效干预，凭借非线性动态控制、状态交叉融合及渐进迁移，提升了细粒度和多模态SR性能，并具备跨维度、跨模态的泛化潜力。

Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.

</details>


### [75] [Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation](https://arxiv.org/abs/2511.18037)
*Yunfan Lu,Nico Messikommer,Xiaogang Xu,Liming Chen,Yuhan Chen,Nikola Zubic,Davide Scaramuzza,Hui Xiong*

Main category: cs.CV

TL;DR: 提出首个同时覆盖APS与EVS的统一统计噪声模型与标定-仿真框架（HESIM），可从真实数据标定并在多任务上实现逼真模拟与良好迁移。


<details>
  <summary>Details</summary>
Motivation: 混合事件-帧传感器同时输出强度帧与事件，具备高动态范围与低延迟，但其复杂电路导致噪声机理复杂、缺乏统一模型，限制了算法设计、标定与仿真评测的可靠性与可复现性。

Method: 建立统一的统计成像噪声模型，显式建模光子散粒噪声、暗电流噪声、固定模式噪声与量化噪声，并将EVS噪声与照度和暗电流关联；提出从真实数据估计噪声参数的标定流程；据此实现HESIM模拟器，可在统一、校准的噪声统计下生成RAW帧与事件。

Result: 在两款混合传感器上完成标定与分析，模型与模拟在视频插帧、去模糊等任务中与真实数据一致性高，模拟到真实的迁移性能强，验证了模型的准确性与实用性。

Conclusion: 统一噪声模型+标定流程+仿真器为混合事件-帧成像提供了可量化、可复现实验基础，能推动下游视觉任务的算法开发与评测，并提升从仿真到真实的可靠迁移。

Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.

</details>


### [76] [UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios](https://arxiv.org/abs/2511.18050)
*Tian Ye,Song Fei,Lei Zhu*

Main category: cs.CV

TL;DR: UltraFlux 通过数据-模型协同设计，把 DiT 原生扩展到多纵横比 4K：重新设计位置编码、改进VAE重建、引入SNR感知小波Huber损失与分阶段审美课程，从而在稳定性、细节与跨AR泛化上显著优于开源基线并逼近/超越商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer在~1K分辨率表现强，但直接扩展到原生4K（且涵盖多种纵横比）会同时触发位置编码失效、VAE压缩失真与优化不稳定等耦合问题。单点修补无法充分提升质量，需要系统性共同设计的数据与模型。

Method: 提出UltraFlux：1) 数据侧：MultiAspect-4K-1M，含100万张原生4K、多纵横比、双语caption与VLM/IQA元数据，用于分辨率/AR感知采样；2) 模型侧：i) Resonance 2D RoPE+YaRN，实现对训练窗口、频率与AR敏感的4K位置编码；ii) 非对抗式VAE后训，提升4K重建保真；iii) SNR感知Huber小波目标，按时间步与频带再平衡梯度；iv) 分阶段审美课程学习，将高审美监督集中于由模型先验主导的高噪声阶段。

Result: 得到在4K与多AR下稳定、细节保真的DiT；在Aesthetic-Eval@4096与多AR 4K基准上，在保真度、审美与对齐等指标上持续优于强开源基线；结合LLM提示词优化后，可匹敌或超过专有Seedream 4.0。

Conclusion: 4K多AR生成的关键在于数据-模型协同：合适的4K数据分布与元数据驱动采样，配合AR/频率感知的位置编码、提升VAE重建、以及SNR/频带重加权与课程学习，可共同消除高分辨率下的耦合失败模式，带来稳健的跨AR 4K生成性能。

Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.

</details>


### [77] [IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment](https://arxiv.org/abs/2511.18055)
*Bowen Qu,Shangkun Sun,Xiaoyu Liang,Wei Gao*

Main category: cs.CV

TL;DR: 该论文提出用于文本驱动图像编辑质量评估的基准套件IE-Bench，并提出基于可验证奖励强化学习的评估模型IE-Critic-R1，显著提升与人主观一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像编辑评估困难：同时受文本与源图像条件约束，编辑结果需兼顾文本对齐与源图结构/语义的保留；以往方法多仅关注文图对齐或与人类感知不一致，缺乏系统性数据与可靠指标。

Method: 1) 构建IE-Bench：收集多样源图、编辑提示、来自多种编辑方法的结果，汇总近4000个样本；组织15名受试者标注MOS分数。2) 提出IE-Critic-R1：基于RLVR（可验证奖励的强化学习）训练评估器，使其在多维度（文本对齐、保真度、视觉质量、编辑程度等）上给出更全面、可解释的打分，并与人类主观更一致。

Result: IE-Critic-R1在IE-Bench上的主观一致性（与MOS的相关性等指标）优于以往评估度量，实验显示在文本驱动编辑任务上具有更强的判别与解释能力；相关数据与代码公开。

Conclusion: IE-Bench为该领域提供标准化评测数据与主观标注，IE-Critic-R1通过RLVR实现更贴近人类感知的编辑质量评估，为未来方法与指标研究提供坚实基础。

Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.

</details>


### [78] [Hierarchical Semi-Supervised Active Learning for Remote Sensing](https://arxiv.org/abs/2511.18058)
*Wei Huang,Zhitong Xiong,Chenying Liu,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出HSSAL框架，把半监督学习与分层主动学习在闭环中迭代结合，通过弱到强自训练提升表示与不确定性估计，并用分层聚类的查询策略同时兼顾规模性、多样性和不确定性，实现高效选样。实验在UCM、AID、NWPU-RESISC45上，以极少标注（8%/4%/2%）达到>95%全监督精度。


<details>
  <summary>Details</summary>
Motivation: 遥感场景分类深度模型依赖大量高质量标注，但标注昂贵且耗时，而海量无标注影像未被充分利用；现有仅SSL或仅AL的方法难以同时高效利用无标注数据并选择最具信息量的样本。

Method: 提出层次化半监督主动学习（HSSAL）：在每轮迭代中，(1) 半监督部分用监督学习+弱到强自训练联合优化，改进特征表示与不确定性估计；(2) 层次化主动学习（HAL）基于改进后的表示与不确定性，对无标注样本做渐进式聚类，按可扩展性、多样性与不确定性三准则联合选取最具信息量的实例；形成闭环迭代。

Result: 在UCM、AID、NWPU-RESISC45三套基准上，HSSAL稳定优于仅SSL或仅AL基线；以仅8%/4%/2%的标注即可达到超过全监督精度95%的水平，表现出显著的标注效率。

Conclusion: 将SSL与分层AL在闭环内融合，可通过改进表示与不确定性来更好地指导选样，以较少标注逼近全监督性能；方法高效、可扩展，适合遥感场景分类的低标注场景。

Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.

</details>


### [79] [A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)](https://arxiv.org/abs/2511.18063)
*Gabriela Fernandes*

Main category: cs.CV

TL;DR: 提出一个基于EfficientNet-B3的虚拟病理助手，在CAISHI数据集上区分宫颈原位腺癌（AIS）与正常腺体，整体准确率0.7323，Abnormal类F1=0.75，Normal类F1=0.71，并用Grad-CAM提供可解释性与轻量部署（Gradio）。


<details>
  <summary>Details</summary>
Motivation: AIS是宫颈腺癌的关键癌前病变，形态学诊断难、漏诊风险高；在资源有限环境中需要可解释、轻量的AI工具辅助早筛与培训。

Method: - 数据：CAISHI，2240张H&E图（1010正常、1230 AIS），专家标注。
- 预处理：Macenko染色归一化；基于patch的切块以强化形态特征。
- 模型：EfficientNet-B3 CNN；类均衡采样+Focal Loss缓解不均衡并聚焦困难样本。
- 解释性：Grad-CAM显示激活集中于核异型、腺体拥挤等AIS特征。
- 部署：Gradio界面为虚拟诊断助手。

Result: 测试表现：总体准确率0.7323；F1分数Abnormal=0.75、Normal=0.71；可解释热力图与病理形态相符。

Conclusion: 在宫颈腺体病理上，轻量、可解释的DL模型具备可行性，可用于筛查流程、教学与低资源场景；仍需提升准确性并在更大、更异质的数据上验证以支持临床转化。

Abstract: Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.

</details>


### [80] [VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection](https://arxiv.org/abs/2511.18075)
*Jianhang Yao,Yongbin Zheng,Siqi Lu,Wanying Xu,Peng Sun*

Main category: cs.CV

TL;DR: 提出VK-Det：一种无需额外监督的视觉知识引导开放词汇航拍目标检测方法，通过视觉编码器的内在区域感知与原型感知伪标注实现更好地发现未知类别，SOTA：DIOR上mAP^N 30.1，DOTA上23.3。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇航拍检测多依赖文本弱监督和自学习，用VLM对齐检测器。但文本依赖带来语义偏置，模型更关注文本指定概念，限制对未见类别的泛化，且伪标签常较粗糙。需要一种脱离额外文本监督、能更细粒度定位并更好刻画类间边界的方法。

Method: 1) 视觉知识利用：发掘视觉编码器对信息区域的内在感知，进行细粒度定位与自适应蒸馏，将检测器的区域表征与视觉编码器对齐。2) 原型感知伪标注：对特征进行聚类以建模类间决策边界，得到潜在类别原型；将检测区域与原型进行匹配，生成原型一致的伪标签，提升对新颖目标关注并弥补监督缺失。全流程不引入额外监督。

Result: 在DIOR与DOTA数据集上取得SOTA开放词汇新类检测性能：DIOR上mAP^N 30.1，DOTA上mAP^N 23.3，超过使用额外监督的方法。

Conclusion: 通过摆脱文本依赖，利用视觉编码器的区域感知与原型匹配的伪标注，VK-Det有效缓解语义偏置并提升对未知类别的检测能力，体现了视觉知识在开放词汇检测中的价值。

Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.

</details>


### [81] [ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models](https://arxiv.org/abs/2511.18082)
*Wencheng Ye,Tianshi Wang,Lei Zhu,Fengling Li,Guoli Yang*

Main category: cs.CV

TL;DR: 提出ActDistill：一种动作引导的自蒸馏框架，将大型VLA模型的动作预测能力迁移到轻量学生模型，通过图结构封装与动态路由，在保持或提升性能的同时，将计算开销降至一半以上、推理加速至1.67×。


<details>
  <summary>Details</summary>
Motivation: 现有VLA在机器人操作上泛化强但推理开销大、延迟高，限制实际部署。既有高效化方法多关注视觉-语言对齐，忽视“动作预测”这一终端任务需求，缺乏面向动作的高效压缩与迁移方法。

Method: 以强VLA为教师，构建“图结构封装”来显式刻画动作预测的层级演化；基于该图导出学生模型，并在学生中加入动态路由模块，按动作需求自适应选择计算路径；以层级图知情的监督信号训练学生。推理时移除图相关辅助部件，仅保留动态路由的高效前向，实现低延迟高精度动作预测。

Result: 在多种具身基准上，学生模型在保持或优于全规模VLA性能的同时，计算量下降>50%，推理加速最高1.67×。

Conclusion: 面向动作的自蒸馏与动态路由结合，提供了通用、可迁移的高效VLA范式，在不牺牲（甚至提升）效果的前提下显著降低机器人操作中的计算与延迟。

Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.

</details>


### [82] [Less Is More: An Explainable AI Framework for Lightweight Malaria Classification](https://arxiv.org/abs/2511.18083)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CV

TL;DR: 提出EMFE流水线，用简单形态学特征与轻量传统模型在CPU上实现对疟疾细胞的高准确二分类，接近/优于小型深度网络，同时更小更快更可解释。


<details>
  <summary>Details</summary>
Motivation: 医学影像常用深度学习但计算开销大、难解释。对于形态简单的二分类任务（如疟疾感染与否），是否必须使用复杂神经网络存疑。作者希望用低算力、透明可复现的方法达到临床可用的性能，便于资源受限环境部署。

Method: 使用NIH疟疾细胞图像数据集，从每张细胞图像提取两个可解释的形态学特征：非背景像素数与细胞内孔洞数。以这些特征训练逻辑回归与随机森林，并与ResNet18、DenseNet121、MobileNetV2、EfficientNet在准确率、模型大小、CPU推理时间上比较；进一步将逻辑回归与随机森林做两阶段集成以提升精度。

Result: 单变量逻辑回归在测试集达到94.80%准确率，模型仅1.2 kB，CPU推理约2.3 ms；两阶段集成提升到97.15%。相比之下，深度模型需13.6–44.7 MB存储，推理约68 ms。

Conclusion: 针对简单细胞形态任务，少量可解释特征配合轻量传统模型可实现临床上有意义的性能，并在透明性、可复现性、速度与可部署性方面优于深度网络；在算力有限场景下是实用诊断方案。

Abstract: Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.
  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.
  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).
  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.

</details>


### [83] [Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective](https://arxiv.org/abs/2511.18089)
*Wenjing Liu,Qin Ren,Wen Zhang,Yuewei Lin,Chenyu You*

Main category: cs.CV

TL;DR: 论文提出TTA框架，通过“先聚合后分离”的最小-最大优化，实现多模态（病理图像+基因组）生存分析中对齐与区分的平衡，避免表示坍塌并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生存分析多依赖注意力式融合追求跨模态对齐，但忽视模态特异信息，导致表示多样性降低与坍塌，从而影响预测与可解释性。需要一个同时兼顾语义对齐与模态独特性的统一方法。

Method: 提出Together-Then-Apart（TTA）框架：1）Together阶段：通过共享原型（prototypes）对齐模态嵌入，并用不平衡最优传输自适应强调信息量大的token，最小化语义差异；2）Apart阶段：引入模态锚点与对比式正则，最大化表示多样性，保留模态特异结构，防止特征坍塌；整体为一个联合的min-max优化。

Result: 在五个TCGA基准上，TTA持续优于现有最先进方法，获得更好生存预测表现；同时保留更具可解释且生物学有意义的表示。

Conclusion: 对齐与区分应被同时建模。TTA通过共享原型对齐与对比正则分离实现鲁棒、可解释且具生物学意义的多模态生存分析，提供了新的理论视角并带来显著经验提升。

Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.

</details>


### [84] [Versatile Recompression-Aware Perceptual Image Super-Resolution](https://arxiv.org/abs/2511.18090)
*Mingwei He,Tongda Xu,Xingtong Ge,Ming Sun,Chao Zhou,Yan Wang*

Main category: cs.CV

TL;DR: 提出VRPSR：让感知型超分辨在训练中显式考虑后续的多种视频图像编解码重压缩影响，通过扩散模型模拟多编解码器，实现更省码率且画质稳健。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，感知型SR输出还要再被H.264/H.265/H.266等编解码器重压缩；若训练时忽略该过程，会在下游编码中引入额外伪影与码率浪费。因真实编解码器不可导、配置多变，难以端到端联合优化，故需一个通用、可训练且可泛化的重压缩感知框架。

Method: 1) 将“压缩”表述为条件图像生成任务，利用预训练扩散模型构建可泛化的编解码器模拟器（输入包含SR结果与条件/文本表征以刻画编解码设置）。2) 为感知型SR设计训练策略：用感知指标优化模拟器；以轻度压缩图像作为训练目标，使SR在被真实压缩后更接近目标分布；从而实现对多种编解码器与配置的适配。3) 进一步支持与重压缩后的后处理模型联合优化。

Result: 在Real-ESRGAN与S3Diff框架下，面对H.264/H.265/H.266，VRPSR在保持感知质量的同时可节省10%以上码率，并可无缝联合优化SR与重压缩后处理。

Conclusion: 通过扩散式通用编解码模拟与针对性训练，VRPSR使感知型SR对多种重压缩更稳健、码率更经济，并提供可扩展的联合优化途径。

Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.

</details>


### [85] [Spotlight: Identifying and Localizing Video Generation Errors Using VLMs](https://arxiv.org/abs/2511.18102)
*Aditya Chinchure,Sahithya Ravi,Pushkar Shukla,Vered Shwartz,Leonid Sigal*

Main category: cs.CV

TL;DR: 提出Spotlight任务：在生成视频中定位并解释细粒度错误；构建包含600个视频和1600+标注的基准，发现“提示遵从”和“物理”错误最常见且持续时间更长；评测现有VLM显著落后于人类，并通过推理时策略将其性能近翻倍提升。


<details>
  <summary>Details</summary>
Motivation: 现有T2V评测多为整体打分，无法指出错误发生的时间位置与类型；新一代T2V错误更细粒度、局部化，亟需面向错误定位与解释的基准与任务。

Method: - 设计Spotlight任务：对视频生成错误进行时间段级定位与类型解释。
- 数据构建：用200个多样化文本提示分别在Veo 3、Seedance、LTX-2上生成600个视频；针对6类错误（含运动、物理、提示遵从、外观消失-出现、身体姿态等）标注1600+细粒度错误与其时间范围。
- 评测：在Spotlight上评估多种VLM的人类对齐程度（识别与定位）。
- 改进：提出推理时策略（如分段分析、逐帧比较、投票/聚合等）来逼近上限并提升VLM表现。

Result: 发现“提示遵从”和“物理”错误占比最高且往往跨越更长片段；“出现-消失”和“身体姿态”错误更短暂。现有VLM在错误识别与定位上显著落后于人类；采用推理时策略后性能接近翻倍提升。

Conclusion: Spotlight为视频生成细粒度评测提供了新任务与基准，揭示了主流T2V的主要错误形态与时长特征，并指出当前VLM在视频错误定位上的不足；所提推理时策略为未来构建细粒度评测工具与更强的奖励模型铺路。

Abstract: Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.

</details>


### [86] [Consolidating Diffusion-Generated Video Detection with Unified Multimodal Forgery Learning](https://arxiv.org/abs/2511.18104)
*Xiaohong Liu,Xiufeng Song,Huayu Zheng,Lei Bai,Xiaoming Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出MM-Det++多模态视频伪造检测框架与DVF数据集，通过时空分支与多模态推理分支结合，配合统一多模态学习模块，在扩散模型生成视频检测上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成视频的激增带来信息安全风险，而现有方法多停留在图像级检测，缺乏针对通用视频级伪造的有效方案与大规模数据集。

Method: 提出MM-Det++，包含两条创新分支与UML统一多模态学习模块：1) 时空(ST)分支采用帧中心FC-ViT，利用FC-token在每帧抽取整体伪造线索并进行时空聚合；2) 多模态(MM)分支基于MLLM可学习推理，获取多模态伪造表示(MFR)，从语义层面辨识伪迹；3) UML模块将两分支表征对齐到统一空间，增强泛化能力。同时构建大规模DVF扩散视频取证数据集。

Result: 在广泛实验中，MM-Det++在扩散生成视频检测任务上优于现有方法，验证了统一多模态伪造学习的有效性。

Conclusion: 结合时空视觉特征与多模态语义推理，并通过UML统一对齐，可显著提升扩散视频伪造检测的鲁棒性与泛化；所建DVF数据集为该领域研究提供重要支撑。

Abstract: The proliferation of videos generated by diffusion models has raised increasing concerns about information security, highlighting the urgent need for reliable detection of synthetic media. Existing methods primarily focus on image-level forgery detection, leaving generic video-level forgery detection largely underexplored. To advance video forensics, we propose a consolidated multimodal detection algorithm, named MM-Det++, specifically designed for detecting diffusion-generated videos. Our approach consists of two innovative branches and a Unified Multimodal Learning (UML) module. Specifically, the Spatio-Temporal (ST) branch employs a novel Frame-Centric Vision Transformer (FC-ViT) to aggregate spatio-temporal information for detecting diffusion-generated videos, where the FC-tokens enable the capture of holistic forgery traces from each video frame. In parallel, the Multimodal (MM) branch adopts a learnable reasoning paradigm to acquire Multimodal Forgery Representation (MFR) by harnessing the powerful comprehension and reasoning capabilities of Multimodal Large Language Models (MLLMs), which discerns the forgery traces from a flexible semantic perspective. To integrate multimodal representations into a coherent space, a UML module is introduced to consolidate the generalization ability of MM-Det++. In addition, we also establish a large-scale and comprehensive Diffusion Video Forensics (DVF) dataset to advance research in video forgery detection. Extensive experiments demonstrate the superiority of MM-Det++ and highlight the effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.

</details>


### [87] [AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens](https://arxiv.org/abs/2511.18105)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,Yung-Hsiang Lu,James C. Davis*

Main category: cs.CV

TL;DR: AdaPerceiver提出统一可自适应的Transformer，在单一模型内同时按深度、宽度与token数动态调度算力，兼顾多硬件/延迟需求，并在分类与密集预测上刷新准确率-吞吐/算力的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有动态计算多聚焦于单一维度（如token裁剪），难以在真实部署中按不同设备与时延约束灵活分配计算；需要一个能在多轴上联合自适应、且不牺牲性能的通用架构与训练方法。

Method: 设计支持三轴（深度、宽度、tokens）统一可调的Transformer架构，并配套高效的联合训练范式，使各子配置共享参数且在训练中被覆盖、校准；另配策略模块/策略以在推理时根据约束选择配置。

Result: 在图像分类上扩展准确率-吞吐帕累托前沿：在相近精度下吞吐提升（例如85.4%准确率、较FlexiViT-L吞吐+36%）。在语义分割与深度估计等密集预测任务上，以约26倍更少的编码器FLOPs达到与ViT-H/14相当的性能。配合策略后，在ImageNet1K上保持精度±0.1个百分点的同时减少FLOPs 24–33%。

Conclusion: AdaPerceiver实现单模型在深度/宽度/token三轴的统一动态推理，兼顾不同算力与时延场景，并在分类与密集预测上实现更优的精度-效率权衡，展示了多轴联合自适应的有效性与可部署潜力。

Abstract: Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.

</details>


### [88] [Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training](https://arxiv.org/abs/2511.18115)
*Wenyu Li,Sidun Liu,Peng Qiao,Yong Dou,Tongrui Hu*

Main category: cs.CV

TL;DR: Muskie 是一个原生多视角视觉骨干网络，用多视图一致性预训练，通过跨视角对应来重建被大幅遮挡的目标，从而学到视角不变与几何感知，在无3D监督下优于帧级模型（如 DINO），并在位姿估计与点图重建等下游3D任务上带来一致增益。


<details>
  <summary>Details</summary>
Motivation: 现有视觉骨干多为逐帧处理，缺乏多视角一致性，导致在3D任务中几何理解不足、跨视角对应弱。作者希望在预训练阶段就显式引入多视角信息，学到对下游3D任务更友好的几何与视角不变表征。

Method: 设计原生多视角骨干 Muskie，同时输入多张视图；采用“跨视角重建”的自监督任务：对某一视图进行重遮挡（aggressive masking），强制模型从其他视图中寻找几何对应并重建被遮挡内容；通过该预训练目标在无3D标注下诱导视角不变与几何感知。

Result: 相较于帧级骨干（如 DINO），Muskie 提升多视角对应精度；作为通用骨干用于下游3D任务时，在相机位姿估计、pointmap（点图）重建等任务上稳定取得更优表现。

Conclusion: 在预训练阶段引入原生多视角处理与跨视角重建，可在无3D监督下学到强几何与视角不变表征，显著改善多视角一致性与3D下游任务表现。

Abstract: We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric correspondences from other views. Through this pretext task and our proposed aggressive masking strategy, the model implicitly to learn view-invariant features and develop strong geometric understanding without any 3D supervision. Compared with state-of-the-art frame-wise backbones such as DINO, Muskie achieves higher multi-view correspondence accuracy. Furthermore, we demonstrate that using Muskie as a backbone consistently enhances performance on downstream 3D tasks, including camera pose estimation and pointmap reconstruction. Codes are publicly available at https://leo-frank.github.io/Muskie/

</details>


### [89] [PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures](https://arxiv.org/abs/2511.18116)
*Yuheng Shao,Lizhang Wang,Changhao Li,Peixian Chen,Qinyuan Liu*

Main category: cs.CV

TL;DR: 提出PromptMoE：用视觉引导的提示混合（VGMoP）以稀疏MoE组合多个专家提示，解决ZSAD中单一/密集提示的过拟合与表达瓶颈，在15个工业与医疗数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 基于CLIP的零样本异常检测受限于提示工程：单一固定、可学习或稠密动态提示都存在表达受限、依赖辅助数据、易过拟合，难以覆盖未知异常的复杂多样性。需要一种可组合、泛化强的提示表征方式。

Method: 学习一个由“正常/异常状态”专家提示构成的提示池；使用视觉引导的稀疏混合专家（VGMoP）门控机制，根据输入图像动态选择并加权组合多个专家提示，生成语义丰富的文本表示以与图像对齐，实现定位与判别。

Result: 在工业与医疗场景共15个数据集上进行大量实验，整体性能优于现有方法，达到最新SOTA（具体指标未在摘要给出）。

Conclusion: 通过将提示学习从单一提示转为可组合的专家混合，并用图像门控进行实例自适应组合，可显著提升ZSAD的鲁棒性与泛化能力。

Abstract: Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\mathtt{PromptMoE}$.

</details>


### [90] [MVS-TTA: Test-Time Adaptation for Multi-View Stereo via Meta-Auxiliary Learning](https://arxiv.org/abs/2511.18120)
*Hannuo Zhang,Zhixiang Chi,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: 提出MVS-TTA：一种在推理阶段进行自监督测试时自适应的框架，通过跨视角一致性和元辅助学习提升学习式MVS在跨数据集的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 学习式MVS依赖固定参数与训练分布，跨场景/跨数据集泛化弱；优化式MVS可做场景特定适配但代价高、不可扩展。需要一种在不重训练与不昂贵逐场景优化的前提下，提高学习式MVS在新场景的适应能力的方法。

Method: 在推理时引入自监督的跨视角一致性损失作为辅助任务，对模型进行少步梯度更新（TTA）。为使模型能从辅助任务更新中稳定获益，训练阶段采用“元辅助学习”策略：通过元学习目标显式地优化主任务在经历辅助任务更新后的性能。框架对架构无关，可最小改动地接入多种MVS模型。

Result: 在DTU、BlendedMVS及跨数据集设置上，MVS-TTA对多种（含SOTA）学习式MVS持续带来性能提升，显示更强的泛化能力与适配性。

Conclusion: 将优化式的测试时自适应以元学习方式融入学习式MVS，实现高效、可扩展的跨场景适配，开创该方向的首次尝试。

Abstract: Recent learning-based multi-view stereo (MVS) methods are data-driven and have achieved remarkable progress due to large-scale training data and advanced architectures. However, their generalization remains sub-optimal due to fixed model parameters trained on limited training data distributions. In contrast, optimization-based methods enable scene-specific adaptation but lack scalability and require costly per-scene optimization. In this paper, we propose MVS-TTA, an efficient test-time adaptation (TTA) framework that enhances the adaptability of learning-based MVS methods by bridging these two paradigms. Specifically, MVS-TTA employs a self-supervised, cross-view consistency loss as an auxiliary task to guide inference-time adaptation. We introduce a meta-auxiliary learning strategy to train the model to benefit from auxiliary-task-based updates explicitly. Our framework is model-agnostic and can be applied to a wide range of MVS methods with minimal architectural changes. Extensive experiments on standard datasets (DTU, BlendedMVS) and a challenging cross-dataset generalization setting demonstrate that MVS-TTA consistently improves performance, even when applied to state-of-the-art MVS models. To our knowledge, this is the first attempt to integrate optimization-based test-time adaptation into learning-based MVS using meta-learning. The code will be available at https://github.com/mart87987-svg/MVS-TTA.

</details>


### [91] [VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging](https://arxiv.org/abs/2511.18121)
*Ming Zhong,Yuanlei Wang,Liuzhou Zhang,Arctanx An,Renrui Zhang,Hao Liang,Ming Lu,Ying Shen,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出VCU-Bridge框架与HVCU-Bench基准，按“感知→语义桥接→抽象涵义”的层级评估与训练MLLM，并用MCTS引导的数据生成进行指令微调；结果显示层级越高越难，但强化低层能力能带动高层与通用基准提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评测将低层感知与高层推理割裂，忽视两者的语义与因果依赖，导致诊断性不足，无法暴露真实瓶颈；而人类理解视觉含义具有自下而上的层级整合能力，亟需一种能显式连接证据与抽象结论的评估/训练范式。

Method: 1) 提出VCU-Bridge：将视觉涵义理解分为基础感知、语义桥接、抽象涵义三层，并要求从具体线索到抽象结论的可追踪推理链。2) 构建HVCU-Bench：按层级提供显式诊断的评测集。3) 设计基于MCTS的数据生成与指令微调流水线，优先探索能加强低层到高层衔接的数据。

Result: 在HVCU-Bench上，各模型随层级上升性能稳定下降，验证了高层推理难度与瓶颈；使用MCTS指导的数据进行指令微调后，低层能力增强带来高层与通用基准的可测提升：总体平均+2.53%，在MMStar上+7.26%。

Conclusion: 层级化、证据到推理链式的评估与训练能更真实地诊断并提升MLLM的视觉涵义理解；通过强化低层并以MCTS引导数据生成，能够促进上层能力与泛化表现，体现“分层思维”对MLLM的重要性与有效性。

Abstract: While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .

</details>


### [92] [Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models](https://arxiv.org/abs/2511.18123)
*Dachuan Zhao,Weiyue Li,Zhenda Shen,Yushu Qiu,Bowen Xu,Haoyu Chen,Yongchao Chen*

Main category: cs.CV

TL;DR: 提出一种几何化的子空间投影去偏方法SPD，针对VLM表征存在的分布式偏见子空间而非单一坐标，实验证明在多任务上提升公平性18.5%且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态推理中广泛使用，但其表征编码并放大人口属性偏见，导致下游任务不公与视觉-语言对齐失真。现有事后去偏多以坐标替换为中性值，但存在特征缠结、跨数据集泛化差、去偏不彻底等问题，亟需一种能捕捉分布式偏见结构且泛化更好的方法。

Method: 系统分析发现偏见并非集中在少数坐标，而是分布在若干线性子空间。提出Subspace Projection Debiasing（SPD）：1）识别可线性解码的偏见子空间；2）将表征在该子空间上的分量移除（投影消除）；3）重新注入一个中性均值成分以保持语义保真。该框架几何上可解释、与模型后处理兼容。

Result: 在零样本分类、图文检索、图像生成等任务上，SPD相较最佳基线在四项公平性指标上平均提升18.5%，同时对主任务性能的影响最小。

Conclusion: 偏见在VLM中呈子空间分布，应以子空间级的投影去偏并配合中性均值校正。SPD在多任务与数据集上实现更稳健的去偏与更好的公平-效能权衡。

Abstract: Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\textbf{S}$ubspace $\textbf{P}$rojection $\textbf{D}$ebiasing ($\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.

</details>


### [93] [SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation](https://arxiv.org/abs/2511.18127)
*Ruicong Liu,Yifei Huang,Liangyang Ouyang,Caixin Kang,Yoichi Sato*

Main category: cs.CV

TL;DR: SFHand是首个支持语言引导、实时流式的3D手部预测框架，结合自回归流式结构与ROI增强记忆，从连续视频与指令中预测未来手部类型、2D框、3D姿态与轨迹；并发布配套大规模EgoHaFL数据集，在预测与下游操控任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部预测多依赖离线整段视频，难以实时；且无法利用语言来表达任务意图，限制了AR与助理机器人等交互场景的实用性。

Method: 提出SFHand：流式自回归架构+ROI增强记忆层，持续吸收视频流与文本指令，聚焦手相关区域并建模时间上下文；统一预测未来的手类型、2D bbox、3D姿态、轨迹。并构建EgoHaFL数据集，包含同步的3D手姿与语言指令用于训练与评估。

Result: 在3D手部预测上达SOTA，最高优于前作35.8%；其学习到的表征可迁移至具身操控任务，在多基准上提升成功率最高13.4%。

Conclusion: 流式+语言引导的统一预测框架有效解决实时交互需求，数据与方法共同推动3D手预测与下游操控；为AR与机器人提供更实用的前瞻性手部理解能力。

Abstract: Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.

</details>


### [94] [Video4Edit: Viewing Image Editing as a Degenerate Temporal Process](https://arxiv.org/abs/2511.18131)
*Xiaofan Li,Yanpeng Sun,Chenming Wu,Fan Duan,YuAn Wang,Weihao Bo,Yumeng Zhang,Dingkang Liang*

Main category: cs.CV

TL;DR: 将图像编辑视作“退化的视频时间过程”，把视频预训练得到的单帧演化先验迁移到图像编辑上，从而在极少监督下实现与主流方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型推动了基于指令的图像生成/编辑，但主流编辑流程代价高：既要大模型训练，又要大量{指令、源图、编辑图}三元组，且编辑质量高度依赖指令对目标语义的精确指代。作者希望降低数据与标注成本，并缓解指令精确指代带来的瓶颈。

Method: 将图像编辑重新表述为时间建模问题的特殊情形：把编辑过程看成视频中的单帧演化。利用视频预训练中学到的“帧间演化先验”，在极少量编辑监督下进行高效微调，从而提升指令驱动的视觉替换与编辑能力。

Result: 在仅使用约1%的主流编辑模型监督数据量的情况下，实证结果与开源领先基线相当，证明了数据效率与性能的兼得。

Conclusion: 时间建模视角为指令驱动图像编辑提供了高性价比路径：借助视频先验可显著减少三元组数据需求，同时保持与先进方法匹配的编辑质量。

Abstract: We observe that recent advances in multimodal foundation models have propelled instruction-driven image generation and editing into a genuinely cross-modal, cooperative regime. Nevertheless, state-of-the-art editing pipelines remain costly: beyond training large diffusion/flow models, they require curating massive high-quality triplets of \{instruction, source image, edited image\} to cover diverse user intents. Moreover, the fidelity of visual replacements hinges on how precisely the instruction references the target semantics. We revisit this challenge through the lens of temporal modeling: if video can be regarded as a full temporal process, then image editing can be seen as a degenerate temporal process. This perspective allows us to transfer single-frame evolution priors from video pre-training, enabling a highly data-efficient fine-tuning regime. Empirically, our approach matches the performance of leading open-source baselines while using only about one percent of the supervision demanded by mainstream editing models.

</details>


### [95] [SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation](https://arxiv.org/abs/2511.18136)
*Chunming He,Rihan Zhang,Longxiang Tang,Ziyun Yang,Kai Li,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出SCALER，一个将一致性约束与SAM监督联合、并实现双向协作学习的统一框架，用于标签稀缺的隐匿目标分割；通过两阶段交替优化平均教师分割器与可学习的SAM，在8个半/弱监督任务上取得一致提升。


<details>
  <summary>Details</summary>
Motivation: LDCOS在目标隐匿与标注稀缺下表现受限；现有方法要么用一致性约束，要么用SAM伪标，却各有短板。作者想回答：能否把两者结合以互补提升？更进一步，能否让分割器反向指导SAM，实现互惠优化？

Method: 提出SCALER统一协作框架，交替两阶段优化：Phase I固定SAM，用其伪标监督平均教师分割器；通过基于熵的图像级与基于不确定性的像素级加权，筛选可靠区域并聚焦困难样本。Phase II更新可学习的SAM，设计增广不变性与噪声鲁棒损失，利用SAM对扰动的内在鲁棒性；形成分割器↔SAM的互监督闭环。

Result: 在八个半监督与弱监督的隐匿目标分割任务中取得一致且显著的性能提升；对轻量分割器和大规模基础模型均有效。

Conclusion: 一致性约束与SAM监督可以在统一框架中协同，并通过双向监督实现互促；SCALER为标签稀缺条件下提升COS的通用训练范式，代码将开源。

Abstract: Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.

</details>


### [96] [Compact neural networks for astronomy with optimal transport bias correction](https://arxiv.org/abs/2511.18139)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li*

Main category: cs.CV

TL;DR: 提出WaveletMamba：将小波分解与状态空间模型、数学正则化和多级偏差校正结合，以低分辨率输入实现接近高分辨率的天文影像分类与红移预测性能，并显著提升计算效率与偏差鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 天文影像在分辨率与效率之间存在基本权衡，限制了大规模形态学分类与红移推断；需要一种在低算力与低分辨率下仍能保持高精度并纠正测量与选择偏差的方法。

Method: 构建“WaveletMamba”框架：1) 以多尺度小波分解实现频段解耦与信息压缩；2) 结合状态空间建模（Mamba类架构）进行长程依赖建模；3) 数学正则化稳定训练与泛化；4) 多级偏差校正：分布层面用Hellinger–Kantorovich距离的最优传输对齐，样本层面以颜色感知加权进行微调；并研究“分辨率多稳态”现象。

Result: 在64x64输入、3.54M参数下达81.72%±0.53%分类准确率，接近244x244的80.93%±0.27%高分辨率表现；计算效率提升约9.7倍。展现“分辨率多稳态”：低分辨率训练的模型在不同输入尺度上准确率一致但内部表征不同。多级偏差校正在无显式选择函数建模下带来22.96%对数MSE改善与26.10%离群点减少。

Conclusion: 以小波+状态空间为核心、辅以严格数学正则与多级偏差校正，可在天文影像上实现前所未有的效率与准确性统一，跨越计算机视觉与天体物理的鸿沟，为大规模科学发现提供实用范式。

Abstract: Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.

</details>


### [97] [UnfoldLDM: Deep Unfolding-based Blind Image Restoration with Latent Diffusion Priors](https://arxiv.org/abs/2511.18152)
*Chunming He,Rihan Zhang,Zheng Chen,Bowen Yang,CHengyu Fang,Yunlong Lin,Fengyang Xiao,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出UnfoldLDM，将深度展开网络与潜空间扩散模型结合，面向盲图像复原，兼顾退化识别与细节重建，取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度展开网络依赖已知退化模型，难以处理盲复原；且梯度下降输出偏低频直接送入近端步导致过度平滑、纹理丢失。

Method: 在每个展开阶段：1）多粒度退化感知（MGDA）模块作为梯度步，将BIR建模为未知退化估计，同时估计整体退化矩阵及其分解形态；2）近端步采用退化抗性潜扩散（DR-LDM），从MGDA输出中提取紧致、与退化无关的先验；3）在该先验引导下的过平滑校正Transformer（OCFormer）显式恢复高频与纹理。整体形成可插拔的DUN+LDM框架。

Result: 在多类盲图像复原任务上取得领先性能，并能提升下游任务表现；与现有DUN方法兼容，可作为即插即用组件。

Conclusion: 通过MGDA的退化估计与DR-LDM+OCFormer的先验与高频补偿，UnfoldLDM兼具可解释性与强表征，有效缓解退化依赖与过平滑问题，实现更清晰、细节丰富的复原。

Abstract: Deep unfolding networks (DUNs) combine the interpretability of model-based methods with the learning ability of deep networks, yet remain limited for blind image restoration (BIR). Existing DUNs suffer from: (1) \textbf{Degradation-specific dependency}, as their optimization frameworks are tied to a known degradation model, making them unsuitable for BIR tasks; and (2) \textbf{Over-smoothing bias}, resulting from the direct feeding of gradient descent outputs, dominated by low-frequency content, into the proximal term, suppressing fine textures. To overcome these issues, we propose UnfoldLDM to integrate DUNs with latent diffusion model (LDM) for BIR. In each stage, UnfoldLDM employs a multi-granularity degradation-aware (MGDA) module as the gradient descent step. MGDA models BIR as an unknown degradation estimation problem and estimates both the holistic degradation matrix and its decomposed forms, enabling robust degradation removal. For the proximal step, we design a degradation-resistant LDM (DR-LDM) to extract compact degradation-invariant priors from the MGDA output. Guided by this prior, an over-smoothing correction transformer (OCFormer) explicitly recovers high-frequency components and enhances texture details. This unique combination ensures the final result is degradation-free and visually rich. Experiments show that our UnfoldLDM achieves a leading place on various BIR tasks and benefits downstream tasks. Moreover, our design is compatible with existing DUN-based methods, serving as a plug-and-play framework. Code will be released.

</details>


### [98] [Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design](https://arxiv.org/abs/2511.18163)
*Pasquale De Marinis,Uzay Kaymak,Rogier Brussee,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: 提出首个针对匹配式FSS模型的可解释方法“Affinity Explainer”，利用多层特征间的匹配分数生成支持-查询像素归因图；在扩展既有解释性指标并新增FSS特有指标的评测中，显著优于改造的标准归因方法。


<details>
  <summary>Details</summary>
Motivation: FSS在少样本情境下性能强，但其决策过程不透明；现有XAI多聚焦常规视觉任务，FSS的可解释性基本空白。解释对于理解模型行为与在数据稀缺时合理选择支持集尤为关键。

Method: 面向匹配式FSS架构，利用其内在的支持-查询特征匹配结构：在多层特征尺度上计算支持与查询的匹配分数，并据此提取归因图，标示哪些支持图像像素对查询分割预测贡献最大。提出将标准解释性评估指标扩展到FSS的方案，并设计额外、贴合少样本使用场景的新指标。

Result: 在多个FSS基准与不同模型上进行全面实验，“Affinity Explainer”在量化指标上明显超过改造后的通用归因方法；定性结果显示其解释具有结构化、一致的注意模式，与模型架构相符，便于诊断。

Conclusion: 工作为FSS可解释性研究奠定基础，使研究者能更好地理解与诊断少样本分割系统，提高其可靠性；代码已开源。

Abstract: Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.

</details>


### [99] [Nested Unfolding Network for Real-World Concealed Object Segmentation](https://arxiv.org/abs/2511.18164)
*Chunming He,Rihan Zhang,Dingming Zhang,Fengyang Xiao,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出NUN：在每个分割展开阶段内嵌入去劣化展开网络，以解耦修复与分割，并通过VLM与自一致机制提升真实场景隐蔽目标分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度展开的隐蔽目标分割方法把背景估计与图像修复耦合，目标冲突且需预设劣化类型，不适用于真实复杂退化；需要一个既能适配未知退化、又能服务分割任务的统一框架。

Method: 提出嵌套展开网络NUN：外层为面向分割的SODUN（可逆前景/背景估计），每一阶段内嵌退化鲁棒的DeRUN，实现修复-分割解耦且相互促进。DeRUN由视觉-语言模型指导，动态推断退化语义，在无显式先验下复原高质图像；利用多阶段结构，引入基于图像质量评估的最佳DeRUN输出选择，用于后续阶段，并形成自一致损失以增强鲁棒性。

Result: 在干净与多种真实退化基准上取得领先性能，实验表明方法的鲁棒性与泛化能力优于现有DUN类方法。

Conclusion: 通过DUN-in-DUN嵌套与VLM引导的自适应去劣化，NUN有效解耦修复与分割并相互促进，适配真实世界未知退化，在COS任务上达到SOTA表现。

Abstract: Deep unfolding networks (DUNs) have recently advanced concealed object segmentation (COS) by modeling segmentation as iterative foreground-background separation. However, existing DUN-based methods (RUN) inherently couple background estimation with image restoration, leading to conflicting objectives and requiring pre-defined degradation types, which are unrealistic in real-world scenarios. To address this, we propose the nested unfolding network (NUN), a unified framework for real-world COS. NUN adopts a DUN-in-DUN design, embedding a degradation-resistant unfolding network (DeRUN) within each stage of a segmentation-oriented unfolding network (SODUN). This design decouples restoration from segmentation while allowing mutual refinement. Guided by a vision-language model (VLM), DeRUN dynamically infers degradation semantics and restores high-quality images without explicit priors, whereas SODUN performs reversible estimation to refine foreground and background. Leveraging the multi-stage nature of unfolding, NUN employs image-quality assessment to select the best DeRUN outputs for subsequent stages, naturally introducing a self-consistency loss that enhances robustness. Extensive experiments show that NUN achieves a leading place on both clean and degraded benchmarks. Code will be released.

</details>


### [100] [EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses](https://arxiv.org/abs/2511.18173)
*Enrico Pallotta,Sina Mokhtarzadeh Azar,Lars Doorenbos,Serdar Ozsoy,Umar Iqbal,Juergen Gall*

Main category: cs.CV

TL;DR: 提出EgoControl：一种可由3D人体姿态精细控制的第一视角视频扩散生成模型，能在给定少量观测帧与目标姿态序列下，生成与姿态一致、时序连贯的未来帧。


<details>
  <summary>Details</summary>
Motivation: 具身智能需要能在第一视角里根据身体动作进行可控的视频模拟、预测与规划；现有方法难以在自我运动与肢体关节运动并存的第一视角场景中实现高保真、精细的姿态控制与视频连贯性。

Method: 训练一个基于扩散的视频预测模型，以显式3D人体姿态序列作为条件。提出新的姿态表示，联合编码全局相机(自运动)动态与关节级肢体运动；并在扩散过程内引入专门的控制模块，将该姿态控制精确地注入生成。输入为短观测帧序列+目标姿态序列，输出为未来帧。

Result: 实验证明生成视频视觉质量高、时间一致性好，与给定姿态控制高度匹配，优于现有第一视角生成的姿态一致性。

Conclusion: EgoControl实现了对第一视角视频的精细姿态可控生成，为可控的具身视频模拟、理解与下游规划任务奠定基础。

Abstract: Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.

</details>


### [101] [Unified Spherical Frontend: Learning Rotation-Equivariant Representations of Spherical Images from Any Camera](https://arxiv.org/abs/2511.18174)
*Mukai Yu,Mosam Dabhi,Liuyue Xie,Sebastian Scherer,László A. Jeni*

Main category: cs.CV

TL;DR: 提出统一球面前端（USF），将任意已标定相机的图像映射到单位球，在空间域进行球面采样、卷积与池化，避免频域SH变换，提供可控旋转等变性，并在多任务与多数据集上验证其效率、鲁棒性与跨镜头泛化。


<details>
  <summary>Details</summary>
Motivation: 广角/鱼眼/全景相机日益普及，但现有以针孔假设和二维平面网格为中心的CNN在图像空间邻域与物理邻近不一致，对全局旋转敏感；频域球CNN虽能缓解但依赖高代价的球谐变换，限制分辨率与效率。

Method: 提出USF：1) 通过光线方向建立任意标定相机到单位球的统一映射；2) 在球面空间域执行可分离模块化流程（投影、采样位置、插值、分辨率控制解耦）；3) 采用仅依赖球面测地距离的卷积核，实现可配置的旋转等变性，避免球谐/FFT；4) 在分类、检测、分割上用平面骨干与其球面对应版本比较，并在极端畸变、不同FoV与任意旋转下进行鲁棒性测试。

Result: USF能高效处理高分辨率球面图像；在随机测试时旋转（无旋转增强）下性能下降<1%；对镜头类型变化具有零样本泛化能力，从一种镜头迁移到未知广角镜头时性能仅有微小下降；在Spherical MNIST、PANDORA、Stanford 2D-3D-S等数据集上取得强竞争性能。

Conclusion: USF以空间域的距离仅依赖球核与模块化球面重采样替代频域方法，实现高效、可扩展、旋转等变的球面感知，并在多任务、多数据集与跨镜头设置中展现出显著鲁棒性与泛化能力。

Abstract: Modern perception increasingly relies on fisheye, panoramic, and other wide field-of-view (FoV) cameras, yet most pipelines still apply planar CNNs designed for pinhole imagery on 2D grids, where image-space neighborhoods misrepresent physical adjacency and models are sensitive to global rotations. Frequency-domain spherical CNNs partially address this mismatch but require costly spherical harmonic transforms that constrain resolution and efficiency. We introduce the Unified Spherical Frontend (USF), a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation via ray-direction correspondences, and performs spherical resampling, convolution, and pooling directly in the spatial domain. USF is modular: projection, location sampling, interpolation, and resolution control are fully decoupled. Its distance-only spherical kernels offer configurable rotation-equivariance (mirroring translation-equivariance in planar CNNs) while avoiding harmonic transforms entirely. We compare standard planar backbones with their spherical counterparts across classification, detection, and segmentation tasks on synthetic (Spherical MNIST) and real-world datasets (PANDORA, Stanford 2D-3D-S), and stress-test robustness to extreme lens distortions, varying FoV, and arbitrary rotations. USF processes high-resolution spherical imagery efficiently and maintains less than 1% performance drop under random test-time rotations, even without rotational augmentation, and even enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.

</details>


### [102] [Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching](https://arxiv.org/abs/2511.18185)
*Yutong Wu,Yifan Wang,Qining Zhang,Chuan Zhou,Lei Ying*

Main category: cs.CV

TL;DR: 提出CorrFlowNet：从基线CT生成“虚拟一年后随访CT”，助力早期肺癌良恶性判断。用相关自编码器学习基线-随访潜在相关与演变，再用潜空间流匹配（神经常微分方程）进行生成，并配合辅助分类器提升诊断。实验证明较基线模型显著提升风险评估，接近真实随访的诊断准确度。


<details>
  <summary>Details</summary>
Motivation: 早期肺癌诊断困难，单次早期CT难以分辨良恶性；临床通常需等待一年随访导致可能错过最佳治疗窗口。现有AI多依赖单时相放射组学，忽视时序演化线索。

Method: 1) 相关自编码器同时编码基线与真实随访CT至共享潜空间，捕获结节进展动力学与两者相关性；2) 在潜空间上采用flow matching与神经ODE学习从基线潜表示到随访潜表示的生成路径，从而从基线生成“虚拟随访”；3) 辅助分类器基于生成结果与潜特征进行良恶性判别；4) 在真实临床数据上训练与评估。

Result: 与现有基线方法相比，CorrFlowNet在结节风险评估上显著提升；生成的虚拟随访在诊断准确度上与真实随访相当。

Conclusion: 通过在潜空间建模结节时序演化并生成虚拟随访图像，可在无需等待真实随访的情况下提升早期良恶性鉴别，具有潜在临床价值以加速肺癌早筛与诊断流程。

Abstract: Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.

</details>


### [103] [ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization](https://arxiv.org/abs/2511.18192)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: ARIAL 提出一个基于代理式编排的模块化框架，同时提升文档VQA的文本答案准确度与空间定位可解释性，在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文档VQA要么文本准确但定位不可靠，要么为可解释性牺牲性能；高风险场景需要既准且可审计的空间对齐与理由链。

Method: 以LLM规划代理编排专用工具：TrOCR做OCR；语义检索做上下文选择；微调的Gemma 3-27B负责答案生成；通过文本-区域对齐显式给出包围框；模块化设计提供可追溯的工具级日志与独立优化。

Result: 在DocVQA、FUNSD、CORD、SROIE上评测，兼顾ANLS与mAP（IoU 0.50-0.95）。取得SOTA：DocVQA 88.7 ANLS / 50.1 mAP；FUNSD 90.0 / 50.3；CORD 85.5 / 60.2；SROIE 93.1 ANLS，并较DLaVA在DocVQA上+2.8 ANLS、+3.9 mAP。

Conclusion: 代理式编排专用工具可同时提升性能与可解释性，为可信、可解释的文档AI提供有效路径。

Abstract: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.

</details>


### [104] [InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity](https://arxiv.org/abs/2511.18200)
*Haoming Wang,Qiyao Xue,Wei Gao*

Main category: cs.CV

TL;DR: InfiniBench提出一个可无限扩展、完全可定制的3D场景生成式基准，专用于评估VLM在多样且高复杂度空间推理任务上的能力。通过LLM代理+聚类布局优化+任务感知相机轨迹，实现从自然语言到物理可信、照片级视频的数据生成，并在高复杂度场景的忠实度与物理合理性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM空间推理评测数据集在多样性、可扩展性与可控性方面受限，无法独立操控场景复杂度并诊断特定失败模式，难以系统、细粒度地评估VLM的空间能力。

Method: 构建自动化基准生成器：1) LLM代理迭代解析与细化自然语言场景描述为可执行的程序化约束；2) 引入基于聚类的布局优化器，能生成密集、杂乱且先前程序化方法难以处理的复杂3D布局；3) 任务感知的相机轨迹优化，确保渲染成的视频对目标对象全覆盖，便于下游VLM评测。

Result: 在提示一致性（prompt fidelity）和物理可信度上，尤其是高复杂度场景中，优于SOTA的程序化与LLM驱动3D生成方法。可从自然语言稳定产出照片级、物理合理的视频。

Conclusion: InfiniBench为评估VLM空间推理提供了可无限扩展、完全可定制且用户友好的生成式基准工具，能够系统地调控复杂度、隔离失败模式，并已在测量、换位思考与时空跟踪等任务上展示实用性。

Abstract: Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.

</details>


### [105] [Generating Synthetic Human Blastocyst Images for In-Vitro Fertilization Blastocyst Grading](https://arxiv.org/abs/2511.18204)
*Pavan Narahari,Suraj Rajendran,Lorena Bori,Jonas E. Malmsten,Qiansheng Zhan,Zev Rosenwaks,Nikica Zaninovic,Iman Hajirasouliha*

Main category: cs.CV

TL;DR: 提出DIA：一种面向第5天胚囊的潜空间扩散生成框架，可按Gardner形态类别与焦深条件生成高保真合成图像，并用于缓解数据稀缺与类别不平衡，实证显示能显著提升下游分类性能，甚至在部分情形以合成数据替代40%真实数据而无显著性能损失。


<details>
  <summary>Details</summary>
Motivation: IVF依赖胚囊形态学评估，但该评估主观且不一致；训练稳健AI模型又受限于数据稀缺、类别不平衡与隐私约束。现有生成模型图像质量与评估不够、数据集规模小、缺乏临床相关可控生成，难以有效用于增强。

Method: 构建DIA：一组潜空间扩散模型，支持以Gardner形态学类别与z轴焦深为条件的可控生成；用FID、记忆化度量、胚胎学家“图灵测试”、以及三个下游分类任务进行严格评估；在不平衡与大规模平衡数据集上进行合成数据增强与替代实验。

Result: DIA生成的胚囊图像逼真，胚胎学家难以可靠区分真伪；在不平衡数据集上加入合成图显著提升分类准确率（p<0.05）；在已大且平衡的数据集上仍有统计学显著增益；在部分任务中可用至多40%的合成数据替代真实数据而准确率无显著下降。

Conclusion: DIA通过可控、高保真的合成胚囊图像缓解数据稀缺与类别失衡，提升胚胎评估AI的性能、公平性与标准化，具备明确的临床与工程价值。

Abstract: The success of in vitro fertilization (IVF) at many clinics relies on the accurate morphological assessment of day 5 blastocysts, a process that is often subjective and inconsistent. While artificial intelligence can help standardize this evaluation, models require large, diverse, and balanced datasets, which are often unavailable due to data scarcity, natural class imbalance, and privacy constraints. Existing generative embryo models can mitigate these issues but face several limitations, such as poor image quality, small training datasets, non-robust evaluation, and lack of clinically relevant image generation for effective data augmentation. Here, we present the Diffusion Based Imaging Model for Artificial Blastocysts (DIA) framework, a set of latent diffusion models trained to generate high-fidelity, novel day 5 blastocyst images. Our models provide granular control by conditioning on Gardner-based morphological categories and z-axis focal depth. We rigorously evaluated the models using FID, a memorization metric, an embryologist Turing test, and three downstream classification tasks. Our results show that DIA models generate realistic images that embryologists could not reliably distinguish from real images. Most importantly, we demonstrated clear clinical value. Augmenting an imbalanced dataset with synthetic images significantly improved classification accuracy (p < 0.05). Also, adding synthetic images to an already large, balanced dataset yielded statistically significant performance gains, and synthetic data could replace up to 40% of real data in some cases without a statistically significant loss in accuracy. DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets. By generating novel, high-fidelity, and controllable synthetic images, our models can improve the performance, fairness, and standardization of AI embryo assessment tools.

</details>


### [106] [Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI](https://arxiv.org/abs/2511.18208)
*Ahmed Gomaa,Annette Schwarz,Ludwig Singer,Arnd Dörfler,Matthias Stefan May,Pluvio Stephan,Ishita Sheth,Juliane Szkitsak,Katharina Breininger,Yixing Huang,Benjamin Frey,Oliver Schnell,Daniel Delev,Roland Coras,Daniel Höfler,Philipp Schubert,Jenny Stritzelberger,Sabine Semrau,Andreas Maier,Dieter H Heiland,Udo S. Gaipl,Andrea Wittig,Rainer Fietkau,Christoph Bert,Stefanie Corradini,Florian Putz*

Main category: cs.CV

TL;DR: 该研究采用两阶段自监督+微调的ViT模型，利用大规模未标注T1CE MRI预训练，并在小样本公开数据上微调用于区分SRS后放射性坏死与肿瘤进展。相比完全监督与放射组学基线，模型在同中心与外中心测试集均取得更高AUC，且多模态融合进一步提升表现，并通过注意力图展现可解释性。结论是：基于未标注数据的大规模预训练显著提升跨中心泛化与临床可及性，值得进一步验证。


<details>
  <summary>Details</summary>
Motivation: SRS后脑转移灶影像上放射性坏死与肿瘤进展难以区分；病理学虽为金标准但具侵袭性、不可普及。监督学习受限于缺乏活检标注数据。随着未标注影像库增大，需一种能利用无标注数据提升模型性能与泛化的方案。

Method: 采用“基础模型”范式的两阶段流程：1) 自监督预训练Vision Transformer，在10,167个多源T1CE MRI子体积上学习表示；2) 在MOLAB数据集（n=109）上用两通道输入（T1CE与分割掩码）微调进行RN分类，20%同中心留出测试；并在第二中心外部测试集（n=28）验证。同时与完全监督ViT和放射组学比较，并进行注意力图可解释性分析；还进行了影像+临床数据的多模态融合评估。

Result: 自监督模型在同中心/外中心AUC分别为0.916/0.764，优于完全监督ViT（0.624/0.496，p=0.001/0.008）与放射组学（0.807/0.691，p=0.005/0.014）。多模态融合后AUC提升至0.947/0.821（p=0.073/0.001）。注意力图显示模型聚焦于临床相关病灶亚区，增强可解释性。

Conclusion: 基于未标注脑转移影像的大规模自监督预训练可显著提高区分RN与肿瘤进展的准确性与跨中心泛化；两阶段多模态策略仅依赖常规T1CE与标准临床数据，具备可解释性与临床可达性，但仍需更大规模、多中心前瞻性验证。

Abstract: Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.

</details>


### [107] [Using MLIR Transform to Design Sliced Convolution Algorithm](https://arxiv.org/abs/2511.18222)
*Victor Ferrari,Marcio Pereira,Lucas Alvarenga,Gustavo Leite,Guido Araujo*

Main category: cs.CV

TL;DR: 提出SConvTransform：在MLIR Transform方言中以声明式流水线将Linalg卷积自动切块与打包，跨架构获得“够用”的性能（ARM SME达峰值60%，AVX512达67%）。


<details>
  <summary>Details</summary>
Motivation: 2D卷积在不同硬件上需要复杂的切块、打包与布局变换；手写或专用pass难以复用与分析。作者希望在MLIR中用可组合、可分析、可移植的声明式变换，自动依据形状与目标架构参数生成高效实现。

Method: 扩展Transform方言，定义SConvOp：先对卷积做Convolution Slicing Analysis，基于输入/滤波器形状和架构参数得到tile大小与数据布局；再将Linalg卷积下降为带切块与打包的generic ops。使用一组参数化仿射方程统一生成打包与tiling；通过分割不规则边界区与调整affine map处理边缘情况；全流程声明式、可重用。

Result: 在标准卷积配置上，生成代码在ARM SME达峰值60%、在Intel AVX512达67%。展示了该方法在多架构上“功能正确且性能可接受”。

Conclusion: 静态形状分析结合结构化tiling与打包、并以MLIR Transform方言实现的声明式流水线是有效路径；SConvTransform模块化、易扩展，便于后续性能强化与移植到更多设备。

Abstract: This paper proposes SConvTransform, a Transform dialect extension that provides operations for optimizing 2D convolutions in MLIR. Its main operation, SConvOp, lowers Linalg convolutions into tiled and packed generic operations through a fully declarative transformation pipeline. The process is guided by a Convolution Slicing Analysis that determines tile sizes and data layout strategies based on input and filter shapes, as well as target architecture parameters. SConvOp handles edge cases by splitting irregular regions and adjusting affine maps where needed. All packing and tiling operations are derived from a parametric set of affine equations, enabling reusable and analyzable transformations. Although functional correctness was the primary goal of this work, the experimental evaluation demonstrates the effectiveness of SConvTransform, achieving good enough performance across different target architectures. Future work will focus on optimizing performance and porting to other target devices. When applied to standard convolution configurations, the generated code achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512. These results validate the benefit of combining static shape analysis with structured tiling and packing strategies within the MLIR Transform dialect. Furthermore, the modular design of SConvTransform facilitates integration with future extensions, enabling continued optimization of convolution workloads through MLIR's extensible compilation infrastructure.

</details>


### [108] [Parallel qMRI Reconstruction from 4x Accelerated Acquisitions](https://arxiv.org/abs/2511.18232)
*Mingi Kang*

Main category: cs.CV

TL;DR: 提出端到端深度学习框架，从仅有的4×欠采样k空间数据同时估计线圈敏感度图并重建MRI图像，在多线圈脑部数据上得到与SENSE相当的视觉质量但客观指标略低，并讨论错位等挑战与改进方向。


<details>
  <summary>Details</summary>
Motivation: MRI采集时间长、对运动敏感，限制通量；并行成像虽可欠采样加速，但依赖稳健的重建及精确的线圈敏感度图。传统SENSE需要预先估计敏感度图且在欠采样下易受误差影响，动机是减少先验依赖并提升重建鲁棒性和效率。

Method: 设计两模块端到端网络：1）CSM估计模块，从欠采样多线圈k空间直接估计每个线圈的敏感度图；2）基于U-Net的重建模块，利用估计的CSM和欠采样数据重建图像。训练/评估在10名受试者、每人8个echo的多线圈脑部数据上，以2× SENSE重建作为“真值”，目标场景为4×加速。

Result: 重建结果在视觉上更平滑，伪影较少，相比传统SENSE有可比的主观质量，但客观PSNR/SSIM略低；发现不同加速因子间的空间错位等问题影响评估与训练。

Conclusion: 联合估计CSM与图像重建的端到端方法在仅有4×欠采样数据条件下可产生高质量、视觉友好的MRI图像，显示出减少先验依赖的潜力。未来需解决跨加速因子的配准/对齐、改进客观指标、以及更大规模评测以提升鲁棒性与泛化。

Abstract: Magnetic Resonance Imaging (MRI) acquisitions require extensive scan times, limiting patient throughput and increasing susceptibility to motion artifacts. Accelerated parallel MRI techniques reduce acquisition time by undersampling k-space data, but require robust reconstruction methods to recover high-quality images. Traditional approaches like SENSE require both undersampled k-space data and pre-computed coil sensitivity maps. We propose an end-to-end deep learning framework that jointly estimates coil sensitivity maps and reconstructs images from only undersampled k-space measurements at 4x acceleration. Our two-module architecture consists of a Coil Sensitivity Map (CSM) estimation module and a U-Net-based MRI reconstruction module. We evaluate our method on multi-coil brain MRI data from 10 subjects with 8 echoes each, using 2x SENSE reconstructions as ground truth. Our approach produces visually smoother reconstructions compared to conventional SENSE output, achieving comparable visual quality despite lower PSNR/SSIM metrics. We identify key challenges including spatial misalignment between different acceleration factors and propose future directions for improved reconstruction quality.

</details>


### [109] [EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning](https://arxiv.org/abs/2511.18242)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: 提出EgoVITA：在自/他视角切换下，用强化学习强化“先规划、后校验”的视频推理框架，显著提升第一人称视频中的意图与行动推理。


<details>
  <summary>Details</summary>
Motivation: 第一人称(自中心)视频存在视野受限、部分可观测、自身体动等问题，导致MLLM在意图与动作推理上困难；现有多来自第三人称数据与静态监督，缺乏因果、可验证的时序规划。

Method: 基于GRPO的两阶段RL：1) 自中心规划阶段，从第一人称视角生成逐步未来行动计划；2) 他中心校验阶段，切换到第三人称视角对计划进行视觉与逻辑一致性检查；通过GRPO优化，使计划对即将到来的视觉观测具有因果可预测性。

Result: 在自中心推理任务上显著优于Qwen2.5-VL-7B：EgoBlind +7.7，EgoOrient +4.4；同时在第三人称视频任务上保持良好泛化。

Conclusion: 将规划-校验与视角切换结合到RL中，可提升MLLM在自中心视频中的因果、可视化对齐推理能力，兼顾对他中心任务的泛化。

Abstract: Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion. We introduce $\textbf{EgoVITA}$, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an $\textbf{egocentric planning phase}$, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an $\textbf{exocentric verification phase}$, where it switches to a third-person perspective to check the visual and logical consistency of that plan. Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by $\mathbf{+7.7}$ on EgoBlind and $\mathbf{+4.4}$ on EgoOrient, while maintaining strong generalization on exocentric video tasks.

</details>


### [110] [UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization](https://arxiv.org/abs/2511.18254)
*Siyi Li,Qingwen Zhang,Ishan Khatri,Kyle Vedder,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 提出UniFlow：在多数据集上联合训练的LiDAR场景流模型，打破“多数据集训练会降性能”的常识，在Waymo、nuScenes与未见过的TruckScenes上均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有场景流方法多在单一传感器/数据集上训练评测，跨传感器泛化差。语义分割/3D检测领域的经验表明多数据集合训会降性能，作者想验证在更低层的运动估计任务上是否仍成立，并学习可迁移的通用运动先验。

Method: 系统性跨数据集分析：按对象运动模式（如高速目标）考察跨数据迁移规律；据此提出UniFlow，一组前馈架构的统一场景流模型，在多种LiDAR数据集（传感器布置与点云密度多样）上联合训练；无花哨技巧，强调“简单统一”的训练策略。

Result: 跨数据集训练不仅不降反升，使SOTA场景流方法显著获益；UniFlow在Waymo与nuScenes上分别超越前作5.1%与35.2%；在未见过的TruckScenes上也达SOTA，较专用模型提升30.1%。

Conclusion: 运动估计作为低层任务对传感器差异更不敏感，跨数据集联合训练能学习通用运动先验并显著提升泛化与精度；简单统一的前馈模型即可取得新SOTA。

Abstract: LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.

</details>


### [111] [Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization](https://arxiv.org/abs/2511.18255)
*Sina Mokhtarzadeh Azar,Emad Bahrami,Enrico Pallotta,Gianpiero Francesca,Radu Timofte,Juergen Gall*

Main category: cs.CV

TL;DR: 提出SAVi-DNO：在推理时优化扩散噪声、冻结参数，实现连续视频流上的自适应视频预测，显著提升FVD/SSIM/PSNR。


<details>
  <summary>Details</summary>
Motivation: 扩散式视频预测模型在真实连续视频流中会不断接收新样本，但常规做法缺乏在线自适应能力；完整微调大型扩散模型代价高，亟需低开销的连续适应方法。

Method: 在不改动预训练扩散模型参数的情况下，于推理过程中对扩散采样噪声进行优化与自适应调整，使模型在长序列上持续校准预测；并在Ego4D上提出同时适应与评估的连续视频设置进行验证。

Result: 在Ego4D与OpenDV-YouTube的长视频上，以及UCF-101与SkyTimelapse数据上，SAVi-DNO在FVD、SSIM、PSNR上均优于基线，显示更好的长期预测质量与稳定性。

Conclusion: 无需昂贵微调，仅通过推理期的噪声优化即可让扩散视频预测模型在连续视频流中实现在线适应并提升性能，适合长视频与资源受限场景。

Abstract: In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.

</details>


### [112] [MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2511.18262)
*Tao Shen,Xin Wan,Taicai Chen,Rui Zhang,Junwen Pan,Dawei Lu,Fanding Lei,Zhilin Lu,Yunfei Yang,Chen Cheng,Qi She,Chang Liu,Zhenbang Sun*

Main category: cs.CV

TL;DR: Mammoth2 提出统一的自回归-扩散（AR-Diffusion）框架，把离散语义规划与高保真图像生成紧密耦合，端到端训练，兼顾理解、生成与编辑，少参数与数据下取得强指标。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型常难以同时做到强语义推理（离散标记）与高保真视觉合成（连续潜变量）。现有方法要么理解强生成弱，要么依赖外部预训练生成器、管线割裂、训练复杂或效率低。作者希望通过一个紧耦合、端到端的框架弥合离散-连续鸿沟，实现既会“想”（规划）又会“画”（合成）的单一模型。

Method: 提出 MammothModa2（Mammoth2）：串行 AR-Diffusion 设计。1）AR 路径带“生成专家”进行全局语义建模与离散token规划；2）单流 DiT 解码器进行高保真扩散式图像合成；3）AR-Diffusion 对齐模块：多层特征聚合、统一条件编码、in-context 条件，稳定把 AR 表征对齐到扩散连续潜空间；4）训练：端到端联合目标——下一词预测（NTP）+ Flow Matching；之后进行有监督微调与强化学习，覆盖生成与编辑；数据规模约6000万监督生成样本，不依赖外部预训练生成器。

Result: 在公开基准上取得强性能：文本生成图像与指令式编辑方面 GenEval 0.87、DPGBench 87.2、ImgEdit 4.06；在多模态理解任务上与理解专长骨干（如 Qwen3-VL-8B）保持竞争力；总体呈现参数与数据效率佳、能力均衡。

Conclusion: 精心耦合的 AR-Diffusion 统一架构可在单一模型内同时实现高保真生成/编辑与强多模态理解。Mammoth2 通过结构化对齐与联合训练目标，减少对外部生成器依赖，在效率与效果间取得良好权衡。

Abstract: Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.

</details>


### [113] [SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors](https://arxiv.org/abs/2511.18264)
*Ruijie Fan,Junyan Ye,Huan Chen,Zilong Huang,Xiaolei Wang,Weijia Li*

Main category: cs.CV

TL;DR: SatSAM2是基于SAM2的零样本卫星视频跟踪器，通过引入卡尔曼约束运动模块与运动约束状态机，提高在遮挡与复杂运动下的稳健性，并发布合成大规模评测集MVOT；在多基准上优于现有方法，OOTB上AUC提升5.84%。


<details>
  <summary>Details</summary>
Motivation: 现有卫星视频跟踪方法泛化性差、需特定场景训练，且在遮挡等情况下易丢失目标；需要一种无需再训练、能稳健处理复杂动态的通用跟踪方案与大规模评测基准。

Method: 在SAM2基础上适配遥感：1) KFCMM（基于卡尔曼滤波的受限运动模块）利用时序运动线索并抑制漂移；2) MCSM（运动约束状态机）依据运动动态与置信度调度跟踪状态（如跟踪、重检测、失踪等）。同时构建合成数据集MVOT（1500+序列、15.7万标注帧，涵盖多视角、光照与遮挡）。

Result: 在两个卫星跟踪基准与MVOT上综合优于传统与基于基础模型的跟踪器（含SAM2变体）；在OOTB数据集上AUC提升5.84%。

Conclusion: 将基础模型迁移到卫星遥感视频可通过运动先验与状态机显著提升零样本跟踪稳健性；发布的MVOT为大规模评测提供支撑，方法与数据将开源促进后续研究。

Abstract: Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.

</details>


### [114] [Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models](https://arxiv.org/abs/2511.18271)
*Tianyang Han,Junhao Su,Junjie Hu,Peizhen Yang,Hengyu Shi,Junfeng Luo,Jialin Gao*

Main category: cs.CV

TL;DR: PicWorld提出针对文本到图像模型的世界知识与物理因果推理能力评测基准，并用多智能体证据评估器（PW-Agent）细粒度打分，实测17个主流模型均在隐含知识与物理推理上存在系统性短板。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评估多聚焦组合一致性或单轮VQA打分，忽视了隐含世界知识、物理交互、多物理效应以及可审计的证据支撑等关键维度，导致模型在“常识/物理”类需求下的失败未被充分暴露与量化。

Method: 构建PicWorld基准，包含1100条覆盖三大类别的提示词；设计PW-Agent为基于证据的多智能体分层评估器：先将复杂提示分解为可验证的视觉证据，再分别评估物理真实感与逻辑一致性，并提供可追溯的判据；用该框架系统测评17个主流T2I模型。

Result: 在PicWorld上，所有被测T2I模型在隐式世界知识理解与物理因果推理方面均表现不佳，程度不一；细粒度证据评估揭示了具体失配点与失败模式。

Conclusion: 当前T2I模型普遍缺乏隐式知识与物理因果推理能力，需要面向推理与知识整合的架构改进；可审计、证据驱动的评估对未来系统开发与对齐具有指导意义。

Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.

</details>


### [115] [Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation](https://arxiv.org/abs/2511.18272)
*Richard J. Young*

Main category: cs.CV

TL;DR: 评估在医疗OCR中对视觉token进行推理时屏蔽是否能减少PHI泄露：多种掩蔽策略在DeepSeek-OCR上最多仅带来约42.9%的PHI减少，长文本/分散标识可被抑制，但短结构化标识仍完全泄露；扩大掩蔽范围无助提升。结合NLP后处理可达约88.6%减少，提示需转向解码器级微调与混合防御。


<details>
  <summary>Details</summary>
Motivation: 医疗文档OCR常涉及受保护健康信息（PHI），在使用大规模视觉-语言模型时存在隐私泄露风险。现有工作缺少对“仅在视觉侧进行推理时掩蔽”的系统性评估，亟需明确该类方法的有效边界与适用PHI类型，为HIPAA合规提供实践与研究指引。

Method: 基于DeepSeek-OCR，提出并实现7种视觉token掩蔽策略（V3-V9），覆盖SAM编码器块、压缩层、双视觉编码器、投影器融合等不同架构层级；在100份具完美标注的合成医疗账单（来自38,517文档语料）上评估，按HIPAA类别度量PHI减少率。进行了消融实验，改变掩蔽扩张半径r=1,2,3，分析空间覆盖对泄露的影响；并模拟将视觉掩蔽与NLP后处理结合的混合架构与假设准确率。

Result: 所有视觉掩蔽策略的PHI减少率均收敛至约42.9%的上限：能100%抑制长形式、空间分散的标识（如姓名、出生日期、住址），但对短的结构化标识（病历号、社保号、邮箱、账号等）抑制为0%。扩大掩蔽半径并未突破该上限，说明泄露主要由语言模型的上下文推断而非视觉掩蔽不足导致。模拟的视觉掩蔽+NLP后处理（对剩余标识80%准确）可达约88.6%的总体PHI减少。

Conclusion: 单纯依赖视觉侧掩蔽在VLM医疗OCR中的隐私保护上存在硬上限，难以防止结构化标识泄露。应根据PHI类型区分：长文本/分散标识适合视觉层处理，短结构化标识需语言层干预。未来应聚焦于解码器级微调与“视觉掩蔽+NLP后处理”的深度防御架构，以实现更接近HIPAA合规的处理流程。

Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.

</details>


### [116] [Point-to-Point: Sparse Motion Guidance for Controllable Video Editing](https://arxiv.org/abs/2511.18277)
*Yeji Song,Jaehyun Lee,Mijin Koo,JunHoo Lee,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出“anchor tokens”点轨迹表示，从视频扩散模型先验中提取少量关键运动锚点，可在编辑中重定位以保持运动与语义一致，改善编辑/运动保真权衡。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑在保持主体运动时面临编辑与运动保真的权衡：要么运动表示过度拟合版式布局，要么仅隐式存在，难以在新主体或场景中泛化。需要一种既紧凑、可控、又可迁移的显式运动表示。

Method: 回顾点基运动表示并提出“anchor tokens”：利用视频扩散模型的先验，自动选择并编码少量最具信息量的点轨迹，作为运动锚点；这些锚点可被灵活重定位到新主体上，实现跨场景泛化。基于此构建编辑方法Point-to-Point，以锚点为条件进行视频编辑。

Result: 大量实验表明，与以往方法相比，锚点令编辑更可控、语义更对齐，并在编辑保真与运动保真两方面取得更优性能。

Conclusion: 通过锚点令点基运动表示重获有效性：用紧凑、可重定位的轨迹捕捉视频核心动态，使视频编辑在多样场景中同时保持编辑与运动的一致性与高保真。

Abstract: Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.

</details>


### [117] [Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation](https://arxiv.org/abs/2511.18281)
*Yara Bahram,Melodie Desbos,Mohammadhadi Shateri,Eric Granger*

Main category: cs.CV

TL;DR: Uni-DAD 提出单阶段同时完成扩散模型的蒸馏与域适配：用双域蒸馏匹配源与目标分布，并辅以多头GAN损稳定与逼真度，实现少步采样下的高质量与高多样性，优于现有两阶段方案与SoTA适配方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在新域采样成本高；蒸馏虽快但受限于教师域，跨域需要“先适配后蒸馏”或“先蒸馏后适配”的两阶段流程，带来复杂度与质量/多样性退化。需要一个在保持速度的同时实现跨域高质量生成的统一方案。

Method: 单阶段框架Uni-DAD：1) 双域分布匹配蒸馏目标，同时向源教师与目标教师分布对齐，保留源域多样性并引导向目标域结构靠拢；2) 多头GAN损，在多尺度特征上判别，提升目标域逼真度、稳定训练、尤其在少样本时抑制过拟合。

Result: 在少样本图像生成与主体个性化任务上，Uni-DAD在<4步采样即可取得更高质量；相较SoTA适配方法与两阶段管线，在质量与多样性上均有优势。

Conclusion: 统一蒸馏与适配的单阶段训练可在极少采样步数下实现跨域高质量且多样化生成；双域蒸馏与多头GAN的结合有效稳定并防过拟合，优于传统两阶段流程。

Abstract: Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.

</details>


### [118] [RoadSceneVQA: Benchmarking Visual Question Answering in Roadside Perception Systems for Intelligent Transportation System](https://arxiv.org/abs/2511.18286)
*Runwei Guan,Rongsheng Hu,Shangshu Chen,Ningyuan Xiao,Xue Xia,Jiayang Liu,Beibei Chen,Ziren Tang,Ningwei Ouyang,Shaofeng Liang,Yuxuan Fan,Wanjie Sun,Yutao Yue*

Main category: cs.CV

TL;DR: 提出RoadSceneVQA路侧场景VQA数据集与RoadMind基线，结合CAF融合与AD-CoT推理策略，在RoadSceneVQA与CODA-LM上达SOTA并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有路侧感知多聚焦实例级识别，难以通过自然语言交互并进行情境化交通行为推理；缺乏可支持规则/意图/交互等高层语义与常识推理的大规模数据与有效多模态融合与推理方法。

Method: 1) 构建RoadSceneVQA数据集：34,736条问答，覆盖多天气/光照/车流条件，问题涵盖目标属性、意图、合法性、交互模式与上下文依赖。2) 设计CogniAnchor Fusion（CAF）：受“场景锚定”启发的视觉-语言融合模块，增强对关键实体与关系的对齐与聚合。3) 提出Assisted Decoupled Chain-of-Thought（AD-CoT）：结合CoT提示与多任务学习，将识别与推理解耦并相互辅助，提升推理稳定性与可控性。4) 基于上述组件构建基线模型RoadMind，并在MLLM框架中训练/评测。

Result: 在RoadSceneVQA与CODA-LM基准上，RoadMind管线在结构化交通感知与推理任务上取得SOTA，显著提升推理准确率与计算效率（相较既有MLLM/融合策略）。

Conclusion: 面向路侧场景的VQA与推理可通过专门数据集与“锚定式”融合+解耦式链式思维有效提升；所提RoadMind为MLLM在交通规则与上下文推理中的强基线，推动从实例识别迈向可解释的语义/规则级认知。

Abstract: Current roadside perception systems mainly focus on instance-level perception, which fall short in enabling interaction via natural language and reasoning about traffic behaviors in context. To bridge this gap, we introduce RoadSceneVQA, a large-scale and richly annotated visual question answering (VQA) dataset specifically tailored for roadside scenarios. The dataset comprises 34,736 diverse QA pairs collected under varying weather, illumination, and traffic conditions, targeting not only object attributes but also the intent, legality, and interaction patterns of traffic participants. RoadSceneVQA challenges models to perform both explicit recognition and implicit commonsense reasoning, grounded in real-world traffic rules and contextual dependencies. To fully exploit the reasoning potential of Multi-modal Large Language Models (MLLMs), we further propose CogniAnchor Fusion (CAF), a vision-language fusion module inspired by human-like scene anchoring mechanisms. Moreover, we propose the Assisted Decoupled Chain-of-Thought (AD-CoT) to enhance the reasoned thinking via CoT prompting and multi-task learning. Based on the above, we propose the baseline model RoadMind. Experiments on RoadSceneVQA and CODA-LM benchmark show that the pipeline consistently improves both reasoning accuracy and computational efficiency, allowing the MLLM to achieve state-of-the-art performance in structural traffic perception and reasoning tasks.

</details>


### [119] [SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes](https://arxiv.org/abs/2511.18290)
*Jungho Lee,Minhyeok Lee,Sunghun Yang,Minseok Kang,Sangyoun Lee*

Main category: cs.CV

TL;DR: SwiftVGGT 是一个无需训练的快速大规模场景三维重建方法，在保持高质量的同时将推理时间降至同类方法的约 33%。其核心通过内置环路闭合与单次 Sim(3)-SVD 对齐替代传统 IRLS，减少冗余计算并提升全局一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模三维重建在精度与效率间存在难以兼顾的权衡：要么速度快但质量差，要么质量高但推理缓慢。需要一种既能在公里级环境保持全局一致性，又能显著降低推理时间、减少外部依赖的方案。

Method: 提出训练免（training-free）的 SwiftVGGT：1) 内建环路闭合，不依赖外部视觉地点识别（VPR），以降低冗余计算并确保大范围全局一致性；2) 设计简洁的点采样策略，使相邻块（chunk）之间可用一次基于 Sim(3) 的 SVD 完成对齐，取代以往常用的 IRLS 迭代优化，从而显著加速；3) 在 VGGT 框架下进行大规模密集重建。

Result: 在多数据集上评测，SwiftVGGT 在重建质量上达到 SOTA，同时相较近期基于 VGGT 的大规模重建方法仅需约 33% 的推理时间。

Conclusion: SwiftVGGT 在不牺牲重建质量的前提下大幅提升大规模密集三维重建的推理效率，通过内建环路闭合与单次 Sim(3)-SVD 对齐替代 IRLS，消除外部 VPR 依赖并实现公里级环境的准确重建。

Abstract: 3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.

</details>


### [120] [DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition](https://arxiv.org/abs/2511.18305)
*Raja Kumar,Arka Sadhu,Ram Nevatia*

Main category: cs.CV

TL;DR: 提出DiVE-k：用模型自身top-k预测构造多选题，通过RL训练选择正确项，促使细粒度差异化推理，减少死记与提升泛化；在5个细粒度数据集显著优于现有方法，基到新类设定下Harmonic Mean超QWEN2.5-VL-7B 10.04%、超ViRFT 6.16%，混合域与小样本同样收益。


<details>
  <summary>Details</summary>
Motivation: LVLM具备丰富文本知识但在细粒度识别上难以区分相似类别；现有用精确匹配奖励的RL微调脆弱、易记忆训练类别、无法激发面向未见类的差异化推理。

Method: DiVE-k：对每个训练图像获取模型top-k类别输出，构造成多选题；以RL优化让模型在这些候选中选对，奖励可验证且迫使在相似选项间进行细粒度差异化推理，缓解记忆、提升泛化。

Result: 在5个细粒度基准上显著优于现有方法；在base-to-novel泛化设定，Harmonic Mean较QWEN2.5-VL-7B提升10.04%，较ViRFT提升6.16%；混合域与few-shot场景也有类似增益。

Conclusion: 利用模型自举的top-k候选构造判别式训练信号，可稳定训练LVLM进行细粒度差异化视觉推理，减少记忆倾向并提升跨域与少样本泛化。

Abstract: Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\textbf{DiVE-k}$, $\textbf{Di}$fferential $\textbf{V}$isual r$\textbf{E}$asoning using top-$\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.

</details>


### [121] [ScriptViT: Vision Transformer-Based Personalized Handwriting Generation](https://arxiv.org/abs/2511.18307)
*Sajjan Acharya,Rajendra Baskota*

Main category: cs.CV

TL;DR: 提出一种融合ViT风格编码与跨注意力的手写体风格化生成框架，兼顾全局风格一致性与内容准确性，并用可解释的笔画显著性分析揭示风格迁移关注点。


<details>
  <summary>Details</summary>
Motivation: 现有GAN/Transformer/扩散方法难以捕捉手写体中跨长程依赖的全局风格要素（如整体倾斜、曲率、笔压一致性），导致风格不连贯或细节缺失，影响真实性与风格忠实度。

Method: 1) 以Vision Transformer作为风格编码器，从多张参考图学习全局风格表征；2) 通过跨注意力将风格线索与目标文本内容对齐与融合；3) 提出Salient Stroke Attention Analysis（SSAA）以在笔画层面可视化、解释模型关注的风格特征。

Result: 生成的手写图像更好地保持全局风格一致性，细微书写特征（倾斜、曲率、笔压）更忠实，且通过SSAA实现了对风格迁移过程的可解释性。

Conclusion: 统一框架有效提升手写体风格化生成的风格保真与可解释性；ViT风格编码与跨注意力是关键，SSAA为分析与调优提供了可视化依据。

Abstract: Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.

</details>


### [122] [Stro-VIGRU: Defining the Vision Recurrent-Based Baseline Model for Brain Stroke Classification](https://arxiv.org/abs/2511.18316)
*Subhajeet Das,Pritam Paul,Rohit Bahadur,Sohan Das*

Main category: cs.CV

TL;DR: 提出一种基于预训练ViT并结合Bi-GRU的迁移学习框架，用于早期脑卒中识别，在不平衡数据上通过增强训练，在Stroke Dataset上达94.06%准确率。


<details>
  <summary>Details</summary>
Motivation: CT诊断卒中快捷可及，但人工判读耗时且易错，需要自动化、准确且可早期识别的模型以辅助临床决策。

Method: 采用预训练Vision Transformer作为特征提取器，部分编码器块冻结、其余微调以学习卒中特征；将提取的特征输入单层双向GRU进行分类；通过数据增强缓解类别不平衡。

Result: 在公开的Stroke Dataset上，所提模型实现94.06%的分类准确率。

Conclusion: ViT+Bi-GRU的迁移学习方案在CT卒中早期识别任务上表现良好，说明冻结部分ViT层并微调其余层的策略可行，数据增强有助于应对类不平衡。

Abstract: Stroke majorly causes death and disability worldwide, and early recognition is one of the key elements of successful treatment of the same. It is common to diagnose strokes using CT scanning, which is fast and readily available, however, manual analysis may take time and may result in mistakes. In this work, a pre-trained Vision Transformer-based transfer learning framework is proposed for the early identification of brain stroke. A few of the encoder blocks of the ViT model are frozen, and the rest are allowed to be fine-tuned in order to learn brain stroke-specific features. The features that have been extracted are given as input to a single-layer Bi-GRU to perform classification. Class imbalance is handled by data augmentation. The model has achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.

</details>


### [123] [Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement](https://arxiv.org/abs/2511.18317)
*Dongcai Tan,Shunkun Liang,Bin Li,Banglei Guan,Ang Su,Yuan Lin,Dapeng Zhang,Minggang Wan,Zibin Liu,Chenglong Wang,Jiajian Zhu,Zhang Li,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种交互式立体标定框架，通过自动生成下一最佳姿态，实现高精度、少图像的3D变形测量标定；以协方差迹最小为目标，联合优化相对与绝对外参，并提供可视化引导，实验与应用均优于随机姿态并与FEA结果吻合。


<details>
  <summary>Details</summary>
Motivation: 现有立体标定缺乏对“下一步拍哪儿”的直观指导，导致采集低效、精度受限，尤其影响DIC等3D变形测量的可靠性与效率。因此需要一种能面向非专家、在不同视场下仍稳健，并能减少标定张数同时提高精度的姿态指导方法。

Method: 构建交互式标定框架：1) 提出姿态优化策略，将相对与绝对外参联合纳入优化；2) 以标定参数不确定性为核心指标，采用投影参数协方差矩阵的迹作为损失函数，求解下一最佳拍摄姿态；3) 将算法集成至可视化界面，在线引导用户获得合格的标定图像；4) 通过仿真与实测在不同FOV条件下评估。

Result: 相比随机姿态采集，所提方法在达到同等精度时需更少图像，并在整体测量误差上更低且对视场变化具有鲁棒性；在S形试样热变形测试中，测得的变形幅值与演化趋势与FEA高度一致。

Conclusion: 交互式“下一最佳姿态”指导能显著提升立体标定效率与精度，适用于DIC等3D变形测量；仿真、实测与应用验证表明其具有广泛应用潜力。

Abstract: Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.
  Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation

</details>


### [124] [General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification](https://arxiv.org/abs/2511.18326)
*Helia Abedini,Saba Rahimi,Reza Vaziri*

Main category: cs.CV

TL;DR: 比较三类预训练CNN用于小样本脑肿瘤MRI分类，ConvNeXt-Tiny表现最佳，EfficientNetV2S次之，RadImageNet DenseNet121泛化最差，提示通用大规模预训练优于医疗域预训练。


<details>
  <summary>Details</summary>
Motivation: 在小数据场景下，尚不清楚“医疗域预训练”与“通用大规模预训练”哪种迁移策略更适合脑肿瘤MRI分类，需系统对比以指导模型选择与临床应用。

Method: 选取三种预训练CNN：RadImageNet DenseNet121（医疗域预训练）、EfficientNetV2S与ConvNeXt-Tiny（通用大规模预训练）。在相同训练与微调条件下、使用同一小规模脑MRI数据集进行公平比较，评估分类性能（准确率与损失）。

Result: ConvNeXt-Tiny取得最高准确率；EfficientNetV2S次之；RadImageNet DenseNet121准确率最低且损失更高，显示泛化能力不足。

Conclusion: 在小数据下，医疗域预训练不一定占优；现代更深的通用预训练CNN在医学影像迁移学习中可能更具优势，应优先考虑此类模型。

Abstract: Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.

</details>


### [125] [SciPostLayoutTree: A Dataset for Structural Analysis of Scientific Posters](https://arxiv.org/abs/2511.18329)
*Shohei Tanaka,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 提出SciPostLayoutTree数据集（约8000张科学海报）与Layout Tree Decoder模型，用于解析海报的阅读顺序与父子层级关系，并在空间复杂关系（上向、水平、长距离）上显著提升预测准确度，成为海报结构分析的强基线。


<details>
  <summary>Details</summary>
Motivation: 学术海报是重要的学术交流媒介，但结构分析研究多集中于论文，对海报的阅读顺序和层级关系缺乏系统研究，尤其在存在非自上而下的复杂空间关系时，现有方法效果不足。

Method: 1) 构建SciPostLayoutTree数据集：标注阅读顺序与父子关系，强调上向、水平、长距离等空间挑战样例；2) 提出Layout Tree Decoder：融合视觉特征与边界框（位置与类别）特征，采用束搜索在关系预测中建模序列级可行性，输出关系与层级树结构。

Result: 相较既有结构分析数据与方法，在空间上具挑战的关系预测上取得更高准确率；为海报结构分析任务提供了强基线。数据集与代码均已开源。

Conclusion: 针对海报结构分析的空白，提供大规模标注数据与有效模型，显著提升复杂空间关系的预测性能，并为后续结构感知界面与下游理解任务奠定基础。

Abstract: Scientific posters play a vital role in academic communication by presenting ideas through visual summaries. Analyzing reading order and parent-child relations of posters is essential for building structure-aware interfaces that facilitate clear and accurate understanding of research content. Despite their prevalence in academic communication, posters remain underexplored in structural analysis research, which has primarily focused on papers. To address this gap, we constructed SciPostLayoutTree, a dataset of approximately 8,000 posters annotated with reading order and parent-child relations. Compared to an existing structural analysis dataset, SciPostLayoutTree contains more instances of spatially challenging relations, including upward, horizontal, and long-distance relations. As a solution to these challenges, we develop Layout Tree Decoder, which incorporates visual features as well as bounding box features including position and category information. The model also uses beam search to predict relations while capturing sequence-level plausibility. Experimental results demonstrate that our model improves the prediction accuracy for spatially challenging relations and establishes a solid baseline for poster structure analysis. The dataset is publicly available at https://huggingface.co/datasets/omron-sinicx/scipostlayouttree. The code is also publicly available at https://github.com/omron-sinicx/scipostlayouttree.

</details>


### [126] [ConsistCompose: Unified Multimodal Layout Control for Image Composition](https://arxiv.org/abs/2511.18333)
*Xuanke Shi,Boxuan Li,Xiaoyang Han,Zhongang Cai,Lei Yang,Dahua Lin,Quan Wang*

Main category: cs.CV

TL;DR: ConsistCompose是一种将布局坐标直接嵌入语言提示的统一多模态生成框架，并配套构建了大规模布局与身份标注数据集（ConsistCompose3M），显著提升多实例图像生成的空间可控性，同时保持身份一致与通用多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型多聚焦于视觉定位（语言到图像区域对齐），而对其生成对偶任务——在语言/图像条件下、可控布局的多实例生成（LELG）研究不足，导致难以实现精确的组合式空间控制。

Method: 1) 提出ConsistCompose，将实例坐标直接编码进语言提示，实现单一生成式接口的布局可控多实例生成；2) 构建ConsistCompose3M数据集（340万对：260万文本引导、80万图像引导），含布局与身份标注，为布局条件生成提供大规模监督；3) 通过“实例-坐标绑定”提示与“坐标感知的无分类器引导（CFG）”，把语言中的布局线索转化为精确的空间控制，无需任务特定分支。

Result: 在COCO-Position与MS-Bench上，相比布局控制基线显著提升空间准确性，同时保持身份一致性，并在通用多模态理解指标上具备竞争力。

Conclusion: ConsistCompose确立了一个统一的多模态布局可控图像生成范式：用语言嵌入坐标进行LELG，以简化接口、提升空间控制与身份保真，并配合大规模数据集促进该方向发展。

Abstract: Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.

</details>


### [127] [A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles](https://arxiv.org/abs/2511.18344)
*Tianyang Xu,Jinjie Gu,Xuefeng Zhu,XiaoJun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 提出首个大规模多模态无人机多目标跟踪基准MM-UAV（含RGB/红外/事件），并给出含对齐与自适应融合及事件增强关联的基线框架，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 单一可见光在低照、杂乱背景、快速运动等条件下易失效；多模态更鲁棒但缺乏公开数据集与专用方法，制约领域发展。

Method: 构建MM-UAV数据集：覆盖30+场景、1321条同步多模态序列、280万+标注帧。提出针对UAV多目标跟踪的基线框架：1）偏移引导的自适应对齐模块，缓解多传感器空间错位；2）自适应动态融合模块，按场景权衡RGB/IR/事件互补信息；3）事件增强的关联机制，利用事件流的运动线索提升身份保持。

Result: 在全面实验中，所提框架在多项指标上持续超越现有SOTA，多场景下表现稳健。

Conclusion: MM-UAV填补多模态UAV跟踪数据与基线空白；所提方法验证多模态对齐、动态融合与事件驱动关联的有效性。数据与代码开源以促进后续研究。

Abstract: With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.

</details>


### [128] [FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement](https://arxiv.org/abs/2511.18346)
*Wenshuo Gao,Junyi Fan,Jiangyue Zeng,Shuai Yang*

Main category: cs.CV

TL;DR: FlowPortal：一种免训练的基于光流的视频重光照与背景替换框架，通过残差校正流、条件解耦与高频细节转移，在保证结构一致与时间连贯的同时，实现精确光照控制与高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照与背景替换方法很难同时兼顾时间一致性、空间结构保真与光照自然性，且效率受限。需要一种无需再训练即可稳定编辑的视频方法，既能精确控制光照又能保留细节，并与背景生成分离。

Method: 提出FlowPortal：1) 残差校正流（Residual-Corrected Flow），将标准流模型转化为可编辑模型，在条件相同时可完美重建，条件变化时实现忠实重光照并保持结构一致；2) 条件解耦设计（Decoupled Condition Design）将光照控制与其他条件分离，便于精确调节；3) 高频传递（High-Frequency Transfer）在编辑后恢复细节纹理；4) 掩膜策略将前景重光照与背景纯生成/替换隔离，减少互扰；整体为免训练、推理高效的流程。

Result: 实验表明该方法在时间连贯性、结构保留与光照真实感上优于现有方法，同时具备较高推理效率。

Conclusion: FlowPortal能在不训练的前提下实现高质量的视频重光照与背景替换，兼顾结构与时间一致性、光照可控性和细节保真，适用于影视与创意制作场景。

Abstract: Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.

</details>


### [129] [MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference](https://arxiv.org/abs/2511.18352)
*Zitong Xu,Dake Shen,Yaosong Du,Kexiang Hao,Jinghan Huang,Xiande Huang*

Main category: cs.CV

TL;DR: 提出UniPrefer-100K数据集与UniPreferBench基准，并基于此构建MagicWand通用生成与评估代理，以用户偏好对齐为核心，提升AIGC图像/视频生成与评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC虽能高质量生成图像/视频，但用户难以用精细提示词准确表达偏好，且缺乏持久化偏好建模与对齐评估机制，导致生成结果与个体偏好不一致。

Method: 1) 构建包含图像、视频及偏好风格描述的大规模数据集UniPrefer-100K；2) 设计MagicWand代理：对用户提示进行偏好增强，调用先进生成模型生成内容，并基于偏好进行评价与迭代优化；3) 提出UniPreferBench基准，含12万+标注，用于多样AIGC任务的偏好对齐评测。

Result: 在UniPreferBench上，MagicWand在多种场景中能稳定生成更符合用户偏好的内容，并在评估环节也展现出与用户偏好一致的判断。

Conclusion: 通过数据集+基准+代理的整体方案，实现对用户偏好的建模、生成与评测闭环，显著提升AIGC在图像/视频任务中的个性化对齐能力。

Abstract: Recent advances in AIGC (Artificial Intelligence Generated Content) models have enabled significant progress in image and video generation. However, users still struggle to obtain content that aligns with their preferences due to the difficulty of crafting detailed prompts and the lack of mechanisms to retain their preferences. To address these challenges, we construct \textbf{UniPrefer-100K}, a large-scale dataset comprising images, videos, and associated text that describes the styles users tend to prefer. Based on UniPrefer-100K, we propose \textbf{MagicWand}, a universal generation and evaluation agent that enhances prompts based on user preferences, leverages advanced generation models for high-quality content, and applies preference-aligned evaluation and refinement. In addition, we introduce \textbf{UniPreferBench}, the first large-scale benchmark with over 120K annotations for assessing user preference alignment across diverse AIGC tasks. Experiments on UniPreferBench demonstrate that MagicWand consistently generates content and evaluations that are well aligned with user preferences across a wide range of scenarios.

</details>


### [130] [TRANSPORTER: Transferring Visual Semantics from VLM Manifolds](https://arxiv.org/abs/2511.18359)
*Alexandros Stergiou*

Main category: cs.CV

TL;DR: 提出L2V（logits-to-video）任务与模型无关的方法TRANSPORTER，用最优传输将VLM的语义嵌入对齐到T2V生成空间，使logits方向直接驱动条件视频生成，从而可视化并解释VLM内部决策规则。


<details>
  <summary>Details</summary>
Motivation: 当前VLM能够处理复杂视频，但其内部推理过程难以理解与操控。T2V模型拥有高保真视觉合成能力，若能将VLM的预测信号映射为可生成的视频变化，或可直观揭示VLM对对象属性、动作方式、场景语境等因素的敏感性与规则。

Method: 提出L2V任务与TRANSPORTER框架：学习一个最优传输耦合，将VLM的高语义嵌入空间与T2V的生成潜空间对齐；利用VLM的logit分数作为嵌入方向，作为条件信号驱动视频生成。通过调节caption中的词项（对象属性、动作副词、场景上下文）来观察生成视频的系统性变化，从而“运输”VLM的内部偏好到可视化视频。

Result: 在多个VLM上进行定量与定性实验，生成视频能随caption变化呈现一致的视觉差异，反映VLM对不同要素的判别依据；证明该方法可高保真地复现和揭示VLM预测背后的规则。

Conclusion: L2V与TRANSPORTER为模型可解释性提供了一条新路径：用高保真视频生成来直观呈现VLM的决策依据，具模型无关性且能控制对象属性、动作方式与场景语境等维度的变化。

Abstract: How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.

</details>


### [131] [Alias-free 4D Gaussian Splatting](https://arxiv.org/abs/2511.18367)
*Zilong Chen,Huan-ang Gao,Delin Qu,Haohan Chi,Hao Tang,Kai Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 提出一种4D高斯Splatting的抗混叠方案：通过最大采样频率公式、尺度自适应滤波与尺度损失，消除提高渲染频率时的伪影，并减少冗余高斯。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gaussian Splatting的动态场景重建在改变焦距或相机与高斯的距离（提升渲染分辨率/频率）时，受4D高斯频率上限与2D膨胀滤波带来的尺度失配影响，产生明显高频伪影与别名；同时存在冗余高斯问题。需要一种在不同渲染频率下稳定且无伪影的方案。

Method: 1) 推导4D Gaussian Splatting的最大采样频率上界，建立频率调度原则；2) 设计4D尺度自适应滤波器，依据相机尺度/距离动态调节高斯有效带宽与投影核大小，避免尺度失配；3) 引入尺度损失，约束学习到的高斯尺度与采样频率一致；整体在单目与多目视频的重建训练/渲染管线中端到端集成。

Result: 在变焦与近景情况下，显著抑制高频伪影与别名；同时在多视角视频重建中减少冗余高斯数量。实验覆盖单目与多目设置，并实现实时渲染品质的稳定提升。

Conclusion: 通过频率上界建模与尺度自适应机制，方法在提高渲染频率时实现alias-free的4D高斯渲染，兼顾画质与模型紧凑性，适用于单目与多目动态场景重建。

Abstract: Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/

</details>


### [132] [MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer](https://arxiv.org/abs/2511.18370)
*Zenghao Chai,Chen Tang,Yongkang Wong,Xulei Yang,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: 提出MimiCAT，一个级联Transformer，用语义关键点的软对应与条件生成，实现跨品类3D姿态迁移（如人形到四足），在大规模多角色姿态数据上训练，显著优于仅限同类迁移的方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D姿态迁移方法依赖严格的一一对应与相似骨架结构，难以在结构差异显著的跨品类角色（如人形与四足）间泛化，导致区域错配与质量差。

Method: 构建百万级、涵盖数百种不同角色的大规模姿态数据集；提出MimiCAT级联Transformer：利用语义关键点标签学习“软对应”（多对多匹配），先将源变换通过软对应投影到目标，再以形状条件表示进行细化，将姿态迁移表述为条件生成过程。

Result: 在跨角色（含跨品类）场景中，MimiCAT在定性与定量评测上均取得更合理、可信的姿态转移效果，超过以往仅支持窄类迁移（如人形到人形）的方法。

Conclusion: 通过软对应与形状条件生成，MimiCAT突破了结构与变换多样性带来的限制，实现了类别无关的3D姿态迁移，并在广泛角色上表现稳健优越。

Abstract: 3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).

</details>


### [133] [MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models](https://arxiv.org/abs/2511.18373)
*Xiyang Wu,Zongxia Li,Jihui Jin,Guangyao Shi,Gouthaman KV,Vishnu Raj,Nilotpal Sinha,Jingxi Chen,Fan Du,Dinesh Manocha*

Main category: cs.CV

TL;DR: 提出MASS与MASS-Bench：在视频物理推理上，通过深度/3D编码、视觉定位和运动跟踪把时空信号注入VLM语言空间，并用强化微调提升跨模态对齐，在新基准上显著超越同类与更大模型，接近闭源SoTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在常规视频任务表现好，但在涉及动力学与空间交互的物理推理上薄弱，限制了其对真实与AIGC视频的理解与物理一致内容生成能力。需要一种能将物理世界线索转化为VLM可解释表示的方法与相应评测基准。

Method: 1) 构建MASS-Bench：4350个真实+AIGC视频，8361个自由问答，含检测、片段级定位、整序列3D运动跟踪等细粒度标注；2) 提出MASS：与模型无关的注入策略，利用深度驱动的3D编码与视觉定位，把空间-时间信号映射到语言空间，并配合运动追踪器建模物体动力学；3) 通过强化微调强化跨模态对齐与推理能力。

Result: 在物理推理与理解任务上，经MASS增强的VLM相较同等与更大基线分别提升8.7%与6.0%，并达到接近闭源SoTA（如Gemini-2.5-Flash）的水平；消融实验验证各组件有效。

Conclusion: 将3D深度/定位/追踪等物理线索注入VLM并以强化微调对齐，能显著提升视频物理推理能力；MASS-Bench为系统评测提供了标准，方法对模型通用且有效。

Abstract: Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.

</details>


### [134] [Synthetic Curriculum Reinforces Compositional Text-to-Image Generation](https://arxiv.org/abs/2511.18378)
*Shijian Wang,Runhao Fu,Siyi Zhao,Qingqin Zhan,Xingjian Wang,Jiarui Jin,Yuan Lu,Hanqian Wu,Cunjian Chen*

Main category: cs.CV

TL;DR: 提出CompGen：利用基于场景图的难度度量与自适应MCMC采样构建“由易到难”的课程，通过GRPO强化学习优化T2I模型，显著提升多物体、属性与关系的合成能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2I在组合性（多物体、属性、空间/语义关系）上表现薄弱，易出现对象遗漏、属性绑定错误与关系错位。需要一种可度量组合难度并据此系统训练的机制，提升复杂场景生成的一致性与可控性。

Method: 1) 以场景图刻画文本的对象、属性与关系，并提出新的“组合难度”指标；2) 设计自适应MCMC场景图采样器，按难度生成课程数据；3) 将课程学习融入GRPO，作为强化学习范式对T2I进行策略优化；4) 研究多种课程调度（由易到难、高斯、随机）并比较缩放曲线。

Result: 不同课程调度呈现不同的scale曲线，其中“由易到难”和“高斯”采样优于随机。方法在扩散式与自回归式T2I上均带来显著的组合生成提升。

Conclusion: 构建基于场景图难度的课程并以强化学习优化T2I，可系统增强组合性；调度策略对可扩展性关键，“易→难”和高斯采样更有效，方法对多类T2I范式通用。

Abstract: Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.

</details>


### [135] [RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models](https://arxiv.org/abs/2511.18380)
*Timing Yang,Guoyizhe Wei,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: 论文系统性分析视觉领域中 Mamba 的表征机制：理论上将其视为Softmax注意力的低秩近似并连接线性注意力；提出用于激活图评估的二值分割指标量化长程依赖；用DINO自监督预训练获得更清晰可解释的激活图，并在ImageNet上线性探测达78.5%精度。


<details>
  <summary>Details</summary>
Motivation: 尽管Mamba在视觉任务中表现出色，但其在视觉域的工作机理与表征能力缺乏系统、可量化的理解与解释，尤其是与Softmax与线性注意力的关系、对长程依赖的建模能力，以及可解释性方面的评估。

Method: 1) 理论分析：推导Mamba与Softmax/线性注意力的关系，证明其可被视为Softmax注意力的低秩近似，从而弥合二者表征差异；2) 指标设计：提出针对激活图的二值分割评估指标，将以往定性可视化转化为量化评测，以检验长程依赖建模能力；3) 训练方案：采用DINO进行自监督预训练，对比标准监督训练，分析所得激活图的清晰度与可解释性；4) 实证：在ImageNet上进行线性探测评估。

Result: - 理论上建立Mamba与Softmax/线性注意力的统一视角，给出低秩近似解释；- 新指标显示Mamba能有效建模长程依赖；- DINO预训练产生的激活图更清晰、可解释性更强；- 在ImageNet上取得78.5%的线性探测精度。

Conclusion: Mamba可被理解为Softmax注意力的低秩近似，从而在表达力与效率间取得平衡；所提激活图二值分割指标为量化评估长程依赖与可解释性提供工具；自监督（DINO）能显著提升Mamba的可解释性与下游线性探测表现，为后续Mamba视觉架构研究提供方向与证据。

Abstract: Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.

</details>


### [136] [ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access](https://arxiv.org/abs/2511.18382)
*Timing Yang,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CV

TL;DR: ViMix-14M 提供约1400万对、可直接下载的多源视频-文本数据，解决开源文生视频训练的数据瓶颈；通过去重、质检与多粒度重描述，显著提升与视频动作/场景/时间结构的一致性，并在检索、生成、视频问答上优于同类数据集。


<details>
  <summary>Details</summary>
Motivation: 开源文生视频模型受限于缺乏大规模、高质量、易获取且许可清晰的视频-文本语料。现有公开集依赖手动抓取YouTube，存在链接失效、访问限制和版权不确定，且有效样本量低、字幕质量与视频对齐差。

Method: 整合多种开放视频来源，统一去重与质量过滤；设计多粒度、以“近真值”为引导的再标注（re-captioning）流水线，细化并校正对动作、场景与时间结构的描述，使字幕与视频强对齐；随后在多任务上评测其有效性。

Result: 在多模态检索、文本生成视频、视频问答三类任务上，相比同类数据集取得一致性能提升，表明数据质量与对齐度更高。

Conclusion: ViMix-14M 缓解了开源视频基础模型训练的关键数据障碍，提供可直接使用的大规模高质视频-文本对，并给出构建通用、可扩展视频-文本数据集的实践经验。

Abstract: Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.

</details>


### [137] [Can a Second-View Image Be a Language? Geometric and Semantic Cross-Modal Reasoning for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.18385)
*Chuang Peng,Renshuai Tao,Zhongwei Ren,Xianglong Liu,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出DualXrayBench与GSR模型，用双视角X光图像将第二视图当作“类语言”信号，结合几何与语义跨模态推理，在八个跨视任务上显著提升检测与推理性能。


<details>
  <summary>Details</summary>
Motivation: 实际安检常用双视角X光，但既有方法多依赖单视图视觉或以自然语言辅助单视图，难以处理复杂遮挡与威胁。因此作者探究：第二视图能否像语言一样提供约束与指导，从而提升跨视推理与检测？

Method: 1) 构建DualXrayBench：包含多视多模态、八项跨视推理任务与4.56万对双视图—文本标注。2) 构建GSXray数据：用结构化链式思考序列<top>, <side>, <conclusion>。3) 提出GSR（Geometric-Semantic Reasoner）：把第二视角当“语言样”模态，联合学习跨视几何对应与跨模态语义对齐，进行端到端多模态推理。

Result: 在DualXrayBench的所有X光任务上，GSR均达到显著优于现有方法的性能，验证了双视约束与结构化推理的有效性。

Conclusion: 将第二视图视作类语言模态并联合几何-语义推理，可系统提升X光安检的检测与推理能力；DualXrayBench与GSXray为后续研究提供了标准基准与数据。

Abstract: Automatic X-ray prohibited items detection is vital for security inspection and has been widely studied. Traditional methods rely on visual modality, often struggling with complex threats. While recent studies incorporate language to guide single-view images, human inspectors typically use dual-view images in practice. This raises the question: can the second view provide constraints similar to a language modality? In this work, we introduce DualXrayBench, the first comprehensive benchmark for X-ray inspection that includes multiple views and modalities. It supports eight tasks designed to test cross-view reasoning. In DualXrayBench, we introduce a caption corpus consisting of 45,613 dual-view image pairs across 12 categories with corresponding captions. Building upon these data, we propose the Geometric (cross-view)-Semantic (cross-modality) Reasoner (GSR), a multimodal model that jointly learns correspondences between cross-view geometry and cross-modal semantics, treating the second-view images as a "language-like modality". To enable this, we construct the GSXray dataset, with structured Chain-of-Thought sequences: <top>, <side>, <conclusion>. Comprehensive evaluations on DualXrayBench demonstrate that GSR achieves significant improvements across all X-ray tasks, offering a new perspective for real-world X-ray inspection.

</details>


### [138] [SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation](https://arxiv.org/abs/2511.18386)
*Peter Siegel,Federico Tombari,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: SegSplat提出一种一次前馈的3D重建与开放词汇语义融合框架：为每个3D高斯同时预测几何/外观与离散语义索引，并由多视角2D基础模型特征构建紧凑语义记忆库，从而在无需逐场景优化的情况下实现可查询的语义分割，同时保持与SOTA前馈3D Gaussian Splatting相当的几何质量。


<details>
  <summary>Details</summary>
Motivation: 现有快速前馈3D重建方法缺乏丰富的开放词汇语义，需额外的逐场景优化才能注入语义，限制了机器人、AR等需要即时、语义感知3D环境的应用。

Method: 1) 从多视角2D基础模型（如CLIP/视觉大模型）的特征构建紧凑语义记忆库；2) 将每个3D高斯的预测扩展为同时输出几何、外观与离散语义索引；3) 在单次前向过程中将3D高斯与记忆库对齐，实现开放集合的语义赋予；4) 全流程避免对每个场景进行语义特征的特定优化。

Result: 在实验中，SegSplat的几何重建精度与SOTA前馈3D Gaussian Splatting相当；同时实现鲁棒的开放集语义分割，并且不需逐场景语义优化。

Conclusion: SegSplat有效地将快速3D重建与开放词汇语义理解融合，实现无需逐场景优化的可查询语义3D场景，为机器人交互与AR等实时智能系统的语义化3D环境构建迈出关键一步。

Abstract: We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.

</details>


### [139] [Exploring Weak-to-Strong Generalization for CLIP-based Classification](https://arxiv.org/abs/2511.18396)
*Jinhao Li,Sarah M. Erfani,Lei Feng,James Bailey,Feng Liu*

Main category: cs.CV

TL;DR: 提出在CLIP分类任务中，将“弱监督→强模型”的对齐思路落地为类原型学习（CPL），在低预训练/弱监督场景下显著提升分类效果，较强基线平均提升约3.67%。


<details>
  <summary>Details</summary>
Motivation: 大模型对齐依赖人工监督但成本高、难扩展；当模型能力超越人类时，人工反馈不准且低效。既有工作在纯文本中验证“弱监督指导强模型”的可行性，亟需把该范式扩展到多模态，缓解人工标注压力并提升视觉语言模型的安全与性能。

Method: 在CLIP的零/少样本分类框架中引入类原型学习（CPL）：通过弱监督信号优化每一类别的表征原型，使文本/图像在共同嵌入空间中与更具代表性的类别原型对齐；采用简单的损失函数进行原型更新与对齐训练，以增强类别可分性与泛化。

Result: 在有限预训练与弱监督设置下，CPL在针对性场景中稳定优于强基线，整体平均提高约3.67%，显示出弱监督下改进CLIP分类的有效性与鲁棒性。

Conclusion: 弱到强的对齐范式可扩展到视觉语言模型；通过CPL学习更优类原型，在弱监督下也能显著改善CLIP分类性能，特别适用于预训练受限的场景。

Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.

</details>


### [140] [ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering](https://arxiv.org/abs/2511.18399)
*Yuxiang Nie,Han Wang,Yongjie Ye,Haiyang Yu,Weitao Jia,Tao Zeng,Hao Feng,Xiang Fei,Yang Li,Xiaohui Lv,Guozhi Tang,Jingqun Tang,Jinghui Lu,Zehui Dai,Jiacong Wang,Dingkang Yang,An-Lan Wang,Can Huang*

Main category: cs.CV

TL;DR: 提出ChineseVideoBench，一个专为中文视频问答（VideoQA）的多模态大模型评测基准，含8大类12子类任务，强调视频理解与中文语言文化要素，实测对现有MLLM具有挑战性；Gemini 2.5 Pro得分77.9%最佳，InternVL-38B为最强开源。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA评测多以英文与通用场景为主，缺乏对中文语境与文化细节的系统性衡量；随着MLLM在视频理解中的应用增长，需要一个覆盖中文语料、捕捉文化与语言细腻性的权威基准与指标体系，以客观比较模型能力并发现短板。

Method: 构建面向中文视频的评测集与指标：划分8大类、12子类任务，题目设计兼顾长时序视频理解、跨镜头推理、细粒度语义与中文文化知识；使用统一评分协议对多款MLLM进行测试，并给出总体与分项成绩。

Result: ChineseVideoBench对当前MLLM形成显著挑战：总体最优为Gemini 2.5 Pro，得分77.9%；开源模型中InternVL-38B表现最具竞争力。

Conclusion: ChineseVideoBench填补中文VideoQA评测空白，提供结构化任务与指标以全面检验模型的时序理解、语义推理与文化适配能力；现有模型仍有明显提升空间，基准可作为今后改进与对比的标准。

Abstract: This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.

</details>


### [141] [4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation](https://arxiv.org/abs/2511.18416)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 提出4D-VGGT，一个分而治之的时空表征基础模型，用自适应视觉网格支持任意视角与时间步输入，采用跨视角全局融合与跨时间局部融合进行多级表示，并通过多任务头联合预测多种动态场景几何；在多数据集上训练并在多基准上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景几何估计通常将空间与时间特征强行对齐到统一潜变量空间，但空间与时间的异质性导致表征不匹配、判别力不足，限制了在多任务与多设置下的泛化。

Method: 设计一个通用4D基础模型4D-VGGT，核心包括：1) 多设置输入：自适应视觉网格，支持任意视角数量与时间步的序列；2) 多层级表征：空间维度做跨视角全局融合，时间维度做跨时间局部融合，分别建模以减少耦合；3) 多任务预测：在时空特征上接入多个任务特定头，实现对动态场景的综合几何估计。并整合多几何数据集进行联合训练。

Result: 在多个动态场景几何基准与多任务上进行广泛实验，方法展示出优越或有效的性能；能够在不同输入设置下稳健工作。

Conclusion: 分而治之的时空表征与多设置输入、多任务预测的统一框架提升了动态场景几何估计的判别性与通用性，证明该范式在多数据集、基准上的有效性。

Abstract: We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.

</details>


### [142] [NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI](https://arxiv.org/abs/2511.18422)
*Mohammad Jafari Vayeghan,Niloufar Delfan,Mehdi Tale Masouleh,Mansour Parvaresh Rizi,Behzad Moshiri*

Main category: cs.CV

TL;DR: 提出NeuroVascU-Net，在标准T1CE MRI上直接进行脑血管3D分割，兼顾高精度与低计算量；在137例神经肿瘤患者数据上达Dice 0.861、Precision 0.884，参数仅12.4M，优于体量更大的Transformer方案。


<details>
  <summary>Details</summary>
Motivation: 临床神经外科规划需要精确的脑血管分割。手工标注耗时且主观差异大；现有自动化方法多依赖TOF-MRA或计算开销大，限制临床落地。需要一种在常规T1CE MRI上即可高效、准确分割血管的模型。

Method: 基于扩张U-Net，设计两大模块：1) MSC^2F（多尺度上下文特征融合）在瓶颈层用多尺度空洞卷积联合捕获局部与全局信息；2) CDA^2F（跨域自适应特征融合）在深层级动态整合域特异特征以增强表示并控制计算量。使用137例脑肿瘤活检患者的T1CE数据，专家标注，进行训练与验证。

Result: 在T1CE扫描数据上，模型实现Dice 0.8609、Precision 0.8841，能准确分割主干与细小血管；参数量仅12.4M，显著小于如Swin U-NetR等Transformer模型。

Conclusion: NeuroVascU-Net在标准T1CE MRI上实现了高精度且高效的脑血管分割，兼顾准确性与计算成本，具备临床神经外科计算机辅助手术规划的实用潜力。

Abstract: Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.

</details>


### [143] [CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images](https://arxiv.org/abs/2511.18424)
*Avishka Perera,Kumal Hewagamage,Saeedha Nazar,Kavishka Abeywardana,Hasitha Gallella,Ranga Rodrigo,Mohamed Afham*

Main category: cs.CV

TL;DR: 提出CrossJEPA：一种轻量高效的图像-点云跨模态JEPA预训练框架，通过从冻结图像教师的渲染视图嵌入中蒸馏，预测点云对应视角的2D嵌入，实现小模型、快训练、强表征。


<details>
  <summary>Details</summary>
Motivation: 现有利用2D数据增强3D表征的方法往往模型庞大、训练缓慢、计算/内存开销大，不利于在资源受限环境部署；同时JEPA虽以高效著称，却在跨模态场景中少被探索，且被误解为必须依赖masking。作者希望设计一种既高效又性能强的跨模态预训练范式，打破对JEPA的误解，并降低训练和推理成本。

Method: 提出CrossJEPA：以强大的图像基础模型为冻结教师，先对3D点云进行多视角渲染得到2D图像，计算其图像嵌入并一次性缓存为目标；训练一个跨模态预测器（含轻量点云编码器和预测头），条件化地利用投影/几何对齐信息，从点云直接预测指定视角的2D嵌入（JEPA式目标，无需mask）。采用冻结教师+目标缓存实现摊销式高效训练，并通过条件化设计抑制目标域独有语义带来的噪声。

Result: 在ModelNet40线性探测达94.2%，在ScanObjectNN达88.3%，均创SOTA；仅14.1M预训练参数（其中点云编码器8.5M），单卡约6小时完成预训练，展现优异的速度、内存与性能权衡。

Conclusion: CrossJEPA将JEPA理念扩展到图像-点云跨模态蒸馏，避免大模型与遮挡训练的负担，通过目标缓存与条件化投影显著提升效率与鲁棒性。其小参数量、快速预训练与SOTA表现使其成为3D表征学习的实用高效方案。

Abstract: Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

</details>


### [144] [LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection](https://arxiv.org/abs/2511.18425)
*Mansur Yerzhanuly*

Main category: cs.CV

TL;DR: LungX将EfficientNet多尺度特征、CBAM注意力与ViT全局上下文融合，在胸片肺炎检测上取得SOTA（Acc 86.5%，AUC 0.943），较EfficientNet-B0提升6.7% AUC，并提供更可解释的病灶定位；计划进行多中心验证与架构优化以达88%临床级准确率。


<details>
  <summary>Details</summary>
Motivation: 肺炎诊断需及时且准确，传统或单一CNN模型在全局上下文与病灶定位上存在不足，亟需一种既能捕获多尺度局部征象又能整合全局依赖、且具可解释性的模型以提升临床实用性。

Method: 提出混合架构LungX：以EfficientNet提取多尺度特征，引入CBAM通道与空间注意力增强判别性，再结合Vision Transformer获取全局上下文；在RSNA与CheXpert共2万张胸片上训练与评估，并用注意力/显著图进行可视化定位分析。

Result: 在测试集上达到86.5%准确率与0.943 AUC，相比EfficientNet-B0基线AUC提升6.7%；可视化显示病灶定位更精准、解释性更强。

Conclusion: LungX在肺炎检测上实现SOTA性能与更佳可解释性，具备作为AI辅诊的潜力；后续需多中心外部验证与模型/工程优化，目标达88%准确率以满足临床部署标准。

Abstract: Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.

</details>


### [145] [DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation](https://arxiv.org/abs/2511.18434)
*Yongkun Du,Pinxuan Chen,Xuye Ying,Zhineng Chen*

Main category: cs.CV

TL;DR: DocPTBench提出针对拍照文档解析与翻译的评测基准，揭示MLLM与专用解析模型在真实拍摄条件下显著性能下滑。


<details>
  <summary>Details</summary>
Motivation: 现有基准以干净的扫描/电子文档为主，无法反映真实拍摄中的几何畸变、光照变化等挑战，导致对模型鲁棒性的评估不足。

Method: 构建DocPTBench：收集1300+高分辨率拍照文档，覆盖多领域与8种翻译场景；提供面向解析与翻译的人审标注；用主流MLLM与专用解析模型在数字文档与拍照文档上对比评测端到端解析与翻译性能。

Result: 从数字文档转到拍照文档时，MLLM端到端解析准确率平均下降18%，翻译下降12%；专用解析模型平均下降25%。

Conclusion: 真实拍摄文档带来独特困难，现有模型鲁棒性不足；DocPTBench为评估与推动更稳健的文档解析与翻译研究提供了标准数据与代码。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.

</details>


### [146] [When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection](https://arxiv.org/abs/2511.18436)
*Hao Shen,Jikang Cheng,Renye Yan,Zhongyuan Wang,Wei Peng,Baojin Huang*

Main category: cs.CV

TL;DR: 研究针对增量人脸伪造检测引入“生成回放”，提出域感知相对加权（DARW），区分生成样本的域安全/域风险并自适应加权，显著缓解域重叠带来的遗忘与混淆，提升不同回放设置下的增量性能。


<details>
  <summary>Details</summary>
Motivation: 现有增量伪造检测依赖样本回放，受限于隐私与多样性；改用生成回放可规避存储真实旧样本，但其在伪造检测中的可行性与最佳使用方式尚不明确，尤其是生成样本与新域伪造的相似度可能引发域边界模糊与灾难性遗忘。

Method: 系统分析生成回放的两种情形：生成器与新伪造模型相似时产生“域风险”样本，易引发混淆；差异较大时为“域安全”样本，可直接监督。基于此提出DARW：对域安全样本采用直接监督；对域风险样本引入相对分离损失，在监督信号与潜在混淆之间权衡。进一步以域混淆分数动态调节各样本权重，使训练在不同生成回放设置下自适应。

Result: 在多种生成回放场景与数据设置下，DARW稳定提升增量伪造检测性能，相比基线显著降低域重叠导致的性能退化与遗忘。

Conclusion: 生成回放在伪造检测中是可行且有效的，但需区分样本域风险并自适应加权。DARW通过域感知与相对分离机制，有效缓解域重叠带来的混淆，提升增量学习的鲁棒性。

Abstract: The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.

</details>


### [147] [Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning](https://arxiv.org/abs/2511.18437)
*Chi Zhang,Haibo Qiu,Qiming Zhang,Yufei Xu,Zhixiong Zeng,Siqi Yang,Peng Shi,Lin Ma,Jing Zhang*

Main category: cs.CV

TL;DR: 提出PEARL：在VLM的RLVR框架中，将推理显式锚定到可验证的感知证据，通过“感知清单+感知奖励+忠实门控”同步优化感知与推理，显著提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM版RLVR只验证最终文本答案，忽视视觉感知环节，导致视觉幻觉与奖励投机；基于错误感知的链式推理不可靠，需要在训练中显式校验并强化感知。

Method: 提出PEARL双分支感知-推理协同：1) 对每个推理型QA生成“感知清单”（可验证的感知子问题）；2) 对清单进行辅助rollout，得到“感知奖励”；3) 将感知奖励作为“忠实门”（fidelity gate）：通过则偏置策略更新至基于证据的推理，未通过则停止推理更新；4) 可无缝接入GRPO、DAPO等RL算法。

Result: 在多模态推理基准上显著提升，MathVerse上较基线+9.7%，较GRPO再提升+6.6%。

Conclusion: 将推理训练与经验证的视觉证据绑定，可有效缓解视觉幻觉与奖励投机，增强VLM的可验证、稳健的多模态推理能力；方法通用、可与主流RL优化兼容。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.

</details>


### [148] [ReCoGS: Real-time ReColoring for Gaussian Splatting scenes](https://arxiv.org/abs/2511.18441)
*Lorenzo Rutayisire,Nicola Capodieci,Fabio Pellacini*

Main category: cs.CV

TL;DR: 提出一个针对高斯点云场景的交互式重着色编辑管线，强调精确区域选择、实时性能与易用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D扩散模型的多视角数据生成用于3D编辑常导致视角不一致、难以精细控制且计算开销高；需要一种在Gaussian Splatting表示上直接进行高效、可控的编辑方法，聚焦于重着色任务。

Method: 在已训练好的Gaussian Splatting场景上，提供用户友好的交互式管线：允许精确选择场景局部区域并对其进行颜色修改；实现实时反馈与交互工具以演示性能。未依赖昂贵的多视角扩散再训练，而是直接对现有GS表示进行编辑。

Result: 实现了一个实时交互的重着色工具，用户可在GS场景中精确选区并改变颜色，保持高质量重建与快速响应；代码开源。

Conclusion: 针对GS场景的重着色编辑提供了实用、快速且一致性的方案，克服了2D扩散驱动方法在一致性、可控性和成本上的不足，并具备实时应用潜力。

Abstract: Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.

</details>


### [149] [SineProject: Machine Unlearning for Stable Vision Language Alignment](https://arxiv.org/abs/2511.18444)
*Arpit Garg,Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TL;DR: 提出 SineProject：在不重训全模型的情况下，让多模态大模型选择性遗忘（安全/隐私）而不破坏对齐；通过给冻结的投影器加入正弦调制的可训练参数，改善雅可比谱条件，稳定对齐，达成更好“忘—保”权衡。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法常破坏视觉-语言对齐，导致对有害与良性查询一刀切拒答。分析发现问题源于遗忘过程使投影器的雅可比变得病态，优化不稳，跨模态嵌入漂移。需要一种在不牺牲对齐的前提下进行选择性遗忘的方法。

Method: 冻结原投影器结构，在其上增添正弦调制（sinusoidally modulated）的少量可训练参数，对输入/特征进行轻量正弦扰动与重参数化，改善投影器雅可比的谱条件数，从而在遗忘微调中保持稳定；其余大模型基本冻结或最小更新，计算开销极小。

Result: 在 LLaVA v1.5 7B/13B 上、多个安全与隐私遗忘基准中，SineProject 在完全忘却目标信息的同时，大幅降低对良性查询的误拒，并实现当前最优的“遗忘-保留”权衡，且几乎无额外计算开销。

Conclusion: 导致对齐失稳的核心是投影器雅可比病态；通过在冻结投影器上加入正弦调制可训练层，可稳定光谱性质，维持跨模态对齐，在选择性遗忘中兼顾安全与可用性，并达成 SOTA 表现。

Abstract: Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.

</details>


### [150] [EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs](https://arxiv.org/abs/2511.18448)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventBench 是一个面向事件相机/事件流的多模态大模型统一评测基准，涵盖8类任务指标与百万级事件-文本配对数据；评估显示当前模型擅长事件理解，但在细粒度识别与三维空间推理上仍薄弱。


<details>
  <summary>Details</summary>
Motivation: 现有事件视觉领域缺乏对多模态大模型能力的统一、全面且可复现的评测与大规模数据支撑，尤其对三维空间推理与多任务覆盖不足，难以比较闭源、开源与专用事件模型的真实能力差异。

Method: 构建公开可访问的基准 EventBench：发布完整原始事件流与八项任务指令/指标；任务覆盖理解、识别、空间推理；首次引入事件域的3D空间推理评测；提供超百万事件-文本对训练/评测集。使用该基准对闭源、开源与事件专用MLLM（如GPT-5、Gemini-2.5 Pro、Qwen2.5-VL、InternVL3、EventGPT等）进行系统评测。

Result: 在EventBench上，现有事件型MLLM在事件流理解任务表现强，但在细粒度目标识别与空间（尤其3D）推理任务上明显落后；不同模型间在多维任务上的差距被量化揭示。

Conclusion: EventBench为事件视觉MLLM提供了开放、全面、可扩展的统一评测与训练资源，推进标准化比较；当前研究应重点攻克细粒度识别与三维空间推理，以提升事件域多模态模型的整体能力。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.

</details>


### [151] [NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering](https://arxiv.org/abs/2511.18452)
*Loick Chambon,Paul Couairon,Eloi Zablocki,Alexandre Boulch,Nicolas Thome,Matthieu Cord*

Main category: cs.CV

TL;DR: NAF 提出一种零样本、VFM 无关的特征上采样方法，在无需为每个视觉基础模型重新训练的情况下，利用自适应邻域注意力与RoPE指导，从高分辨率输入图像中学习空间与内容权重，达到SOTA精度与高效率。


<details>
  <summary>Details</summary>
Motivation: 现有上采样要在泛化与精度间取舍：固定形式的经典滤波器通用且高效但不够准确；针对特定VFM训练的可学习上采样器精度高却需为每个VFM单独再训练，缺乏灵活性与可扩展性。

Method: 提出 Neighborhood Attention Filtering（NAF）：通过跨尺度邻域注意力（Cross-Scale Neighborhood Attention）结合旋转位置编码（RoPE），仅以高分辨率输入图像为引导，学习对低分辨率特征进行自适应的空间与内容加权；无需访问或微调VFM参数，作为外部上采样模块零样本应用于任意VFM特征。

Result: NAF 在多项下游任务上超过面向特定VFM训练的上采样器，成为首个VFM无关且优于VFM特定方法的体系；可扩展至2K特征图，支持以约18 FPS重建中等分辨率特征图；同时在图像复原任务上表现强劲。

Conclusion: NAF 成功打破“通用性-精度”权衡，提供了无需重训、可移植、高效且精度SOTA的特征上采样方案，并在图像复原等任务上展现通用能力。

Abstract: Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.

</details>


### [152] [RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading](https://arxiv.org/abs/2511.18454)
*Ming-Jhe Lee*

Main category: cs.CV

TL;DR: 提出RegDeepLab：将DeepLabV3+分割与多尺度回归头融合，通过两阶段解耦训练与特征注入实现可解释且精确的胚胎碎片化分级；引入Range Loss用于半监督离散分级数据，获得高精度分级与SOTA级分割（MAE=0.046，Dice=0.729）。


<details>
  <summary>Details</summary>
Motivation: IVF临床需依据胚胎碎片化程度判断发育潜力。人工分级耗时且主观性强，深度学习要么缺乏可解释性（纯回归），要么难以直达临床分级（纯分割）。需要兼顾可解释性与精确分级的自动化方案，并缓解多任务训练中的梯度冲突与负迁移。

Method: 提出双分支多任务学习框架RegDeepLab：主干采用DeepLabV3+进行语义分割，辅以多尺度回归头输出碎片化分级；设计“特征注入”机制促进回归与分割信息交互；为解决多任务梯度冲突/负迁移，提出“两阶段解耦训练策略”（先/分别优化任务，再联合或按策略微调）；引入“Range Loss”以利用大规模离散等级标签开展半监督学习。

Result: 端到端MTL结合特征注入可将分级误差降至MAE=0.046，但分割边界受损；采用解耦训练后，既保持高精度分级，又维持SOTA级分割性能（Dice=0.729）。

Conclusion: RegDeepLab实现“可解释分割+高精度分级”的临床辅助系统，通过解耦训练缓解多任务冲突，并用Range Loss充分挖掘离散分级数据，达成兼顾性能与可解释性的自动化胚胎碎片化评估。

Abstract: The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of "Gradient Conflict" and "Negative Transfer" in multi-task training, we propose a "Two-Stage Decoupled Training Strategy." Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed "Feature Injection" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a "Range Loss" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.

</details>


### [153] [Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding](https://arxiv.org/abs/2511.18463)
*Bowei Pu,Chuanbin Liu,Yifan Ge,Peichen Zhou,Yiwei Sun,Zhiyin Lu,Jiankang Wang,Hongtao Xie*

Main category: cs.CV

TL;DR: 提出Video-PLR：用“感知-分析-决策”循环与反幻觉奖励，缓解视频推理中的感知捷径与幻觉，达成SOTA且数据效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理大模型多采用“一步感知后推理”的流水线：先整体描述视频，再基于描述推理。这种做法易出现两类问题：1) 证据不足——整体描述难以覆盖细粒度时序与关键片段；2) 幻觉——在缺证据时模型会编造细节。作者希望通过新的训练与推理范式，促使模型在推理过程中主动获取充分、可验证的证据，并用客观指标惩罚幻觉。

Method: 1) 感知循环推理（PLR）：将视频分解为多轮循环。每轮要求模型：a) 指定精确时间戳并描述该段；b) 对该段进行分析；c) 决定下一步行动（继续查证哪一段或进入推理）。这一过程使证据采集与推理交替进行。2) 事实感知评估器（FAE）：对每轮“感知结果”进行打分，作为反幻觉奖励，鼓励“充足且精准”的视频证据。FAE在作者构建的AnetHallu-117K（大规模幻觉判断偏好数据集）上微调，效果可媲美GPT-4o。3) 训练：将PLR范式与FAE奖励结合，优化3B/7B模型。

Result: 在多项视频推理基准上，Video-PLR在3B与7B规模均达成SOTA，并显示出最佳数据效率；FAE评测能力接近GPT-4o。

Conclusion: 将“循证感知”引入视频推理，通过循环式细粒度取证与反幻觉奖励，有效缓解证据不足与幻觉问题，提升小中型模型的推理上限；代码、模型与数据已开源。

Abstract: Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.

</details>


### [154] [Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span](https://arxiv.org/abs/2511.18470)
*Heeseung Yun,Joonil Na,Jaeyeon Kim,Calvin Murdock,Gunhee Kim*

Main category: cs.CV

TL;DR: 提出EgoSpanLift，将自我中心视觉注视/视觉跨度预测从2D提升到3D，用SLAM关键点构建与凝视兼容的几何体，在体素网格中用3D U-Net与单向Transformer进行时空预测，并建立36.46万样本基准；在3D与投影回2D任务上优于/可比现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心理解多关注运动与接触行为，较少直接预测视觉感知/注视走向；而视觉注意对行动决策至关重要，且对AR/VR与辅助技术有潜在价值。因此需要能在三维环境中前瞻性预测人将关注的空间区域，而非仅在2D图像平面。

Method: 1) 提出EgoSpanLift：将SLAM得到的关键点与相机/凝视几何对齐，生成与注视兼容的3D几何，并提取体素化的视觉跨度区域；2) 采用3D U-Net进行空间特征编码，结合单向Transformer进行时序建模与跨时空融合，在3D网格上预测未来视觉跨度；3) 构建包含36.46万样本的多模态自我中心数据基准，用于3D视觉跨度预测评测；4) 将预测可投影回2D以比较2D基线。

Result: 在自我中心2D凝视预判与3D定位基线上取得更优性能；将3D预测投影回2D（无需额外2D训练）也达到可比SOTA的结果，显示方法的跨维度有效性与数据效率。

Conclusion: 将视觉跨度预测从2D扩展到3D是可行且有效的；EgoSpanLift通过SLAM几何与3D时空建模显著提升前瞻性注视预测能力，并提供大规模基准。该方法对AR/VR与辅助场景具潜在应用价值。

Abstract: People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.

</details>


### [155] [Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale](https://arxiv.org/abs/2511.18471)
*Liav Hen,Tom Tirer,Raja Giryes,Shady Abu-Hussein*

Main category: cs.CV

TL;DR: 提出AdaPS：一种自适应似然步长的扩散后验采样策略，通过观测依赖的权重平衡先验与数据保真，在多种成像逆问题上无调参提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型做逆问题需要同时依赖生成先验与数据一致性，但固定或手动设定的似然更新步长易失衡：步长大易生伪影、步长小收敛慢或欠重构。缺少一种能随时间步和噪声自适应、对调度与随机性鲁棒的步长策略。

Method: 在扩散采样中，对中间时刻的不可达似然梯度采用两种不同近似，并根据二者一致程度构造观测依赖的权重，得到自适应的似然步长。该权重自然适配扩散时间表、重采样步长与注入随机性。整体算法称为Adaptive Posterior diffusion Sampling（AdaPS），无需额外超参与任务特定调参。

Result: 在CelebA-HQ与ImageNet-256验证集上的超分、Gaussian去模糊与运动去模糊任务中，AdaPS普遍优于现有扩散基线，感知指标显著提升，失真指标基本不降或仅极小下降。消融显示对扩散步数、观测噪声和随机性水平具有鲁棒性。

Conclusion: 自适应似然步长能在扩散逆问题中稳定平衡先验与数据项，带来更高感知质量且几乎无额外调参成本；方法通用、鲁棒，可作为扩散逆问题求解的默认策略。

Abstract: Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.

</details>


### [156] [Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements](https://arxiv.org/abs/2511.18473)
*Juan Romero,Qiang Fu,Matteo Ravasi,Wolfgang Heidrich*

Main category: cs.CV

TL;DR: 提出HSDiff：以像素级无条件扩散先验与后验采样实现不确定性感知的高光谱重建，配合“变色同色”增强与有效光谱编码，提升多样性与校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动HSI重建在数据光谱多样性不足下易产生幻觉，尤其在同色异谱（metamerism）评测时表现不佳；需要能表达不确定性、适配不同前向模型并缓解同色问题的方法。

Method: 将HSI重建表述为贝叶斯推断：以无条件训练的像素级扩散模型作为先验，结合后验扩散采样与测量一致性约束，适配多种成像前向模型。提出区域化“metameric black”和并合分区的光谱上采样以生成物理有效的同色谱用于数据增强，从而增强先验多样性与不确定性校准。分析不同前向模型（是否有效光谱编码）对后验分布的影响。

Result: HSDiff在多种快照式HSI成像与非编码模型上给出符合测量的一致性重建，并能产生多样样本与校准的不确定性；实验显示有效的光谱编码显著提升不确定性信息量与校准优度，相比未编码模型更可靠。整体性能与不确定性感知能力优异。

Conclusion: 在贝叶斯视角下，HSDiff提供了完整高性能的不确定性感知HSI重建方案；“有效光谱编码”对缓解同色和提高不确定性质量至关重要，且所提同色增强提升了先验多样性与校准。

Abstract: Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.

</details>


### [157] [Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression](https://arxiv.org/abs/2511.18504)
*Md Tasnin Tanvir,Soumitra Das,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.CV

TL;DR: 提出两种自适应压缩技术（STTF 与 ANC），在边缘设备上显著降低VLM的FLOPs/延迟与内存，同时保持或提升精度；3B TinyGPT-STTF在COCO超越7B LLaVA-1.5并大幅降算力。


<details>
  <summary>Details</summary>
Motivation: 边缘AI需要在功耗、内存受限的设备上实时运行视觉-语言模型，现有方法多为静态剪枝或均匀缩放，难以根据场景动态调整计算，导致性能/效率折中不佳。

Method: 1) Sparse Temporal Token Fusion（STTF）：基于事件驱动的变化检测，跨时间复用视觉token，仅在检测到显著变化时更新少量新token，并进行稀疏融合，减少时序冗余输入；2) Adaptive Neural Compression（ANC）：在编码器内设置多分支通道，使用学习到的路由器按场景复杂度条件激活分支，实现细粒度、输入自适应的计算分配；并进行硬件感知优化以最小化设备端FLOPs/内存占用。

Result: TinyGPT-STTF（3B）在COCO2017测试集达到 CIDEr 131.2、BLEU-4 0.38、METEOR 0.31、ROUGE-L 0.56，较LLaVA-1.5 7B提升17.6 CIDEr，参数少2.3倍、设备端FLOPs少62倍；TinyGPT-ANC CIDEr 128.5。事件视觉上，STTF将平均token数从196降至31（-84%），在DVS128 Gesture保留95.6%准确率；ANC在低运动场景FLOPs降至10%。相较强基线，精度最高+4.4%，延迟最高降13倍。

Conclusion: 动态、输入自适应的时序token复用与条件分支激活，结合硬件友好优化，可在边缘设备上以极低算力实现强VLM性能，显著优于静态剪枝/均匀缩放，为实际部署提供可行路径。

Abstract: The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.

</details>


### [158] [Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives](https://arxiv.org/abs/2511.18507)
*Kai Jiang,Siqi Huang,Xiangyu Chen,Jiawei Shao,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出MSVQA多场景多视角数据集与UNIFIER连续学习方法，解决MLLM在场景迁移中灾难性遗忘，通过场景分支解耦与特征一致性约束，显著缓解跨场景遗忘并在同场景实现知识积累。


<details>
  <summary>Details</summary>
Motivation: 部署在设备端的MLLM需要在真实世界动态场景（背景、视角变化）中持续适应，但现有方法在场景转移时易发生灾难性遗忘，缺少针对多场景视觉理解的数据与系统性研究。

Method: 构建覆盖高空、水下、低空、室内四类场景与视角的MSVQA数据集；提出UNIFIER：在每个视觉块中为不同场景设置独立分支以解耦视觉信息，并将各分支投影到同一特征空间；对各分支特征施加一致性约束，稳定跨场景表征，支持连续学习。

Result: 在MSVQA上的大量实验显示，UNIFIER显著降低跨场景任务的遗忘程度，同时在同一场景内实现知识累积，优于基线。

Conclusion: 多场景解耦与特征一致性是缓解MLLM跨场景遗忘的有效范式；MSVQA为研究连续多模态视觉理解提供了基准，UNIFIER在动态数据流中实现更稳健的适应与迁移。

Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.

</details>


### [159] [LRDUN: A Low-Rank Deep Unfolding Network for Efficient Spectral Compressive Imaging](https://arxiv.org/abs/2511.18513)
*He Huang,Yujun Guo,Wei He*

Main category: cs.CV

TL;DR: 提出LRDUN：把低秩分解融入SCI成像模型，通过展开的PGD联合估计谱基与子空间图像，并用GFUM解耦物理秩与先验特征维度，达到SOTA质量且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有SCI的深度展开网络直接在完整高光谱数据立方体上迭代，需从单幅二维编码测量恢复三维HSI，既计算冗余又高度病态，限制性能与效率。

Method: 提出两种新成像模型：在传感模型中显式嵌入低秩分解，将HSI表示为谱基与子空间图像；在此基础上构建LRDUN，在展开的近端梯度下降框架下，交替/联合求解两子问题。进一步提出GFUM，将数据保真项中的物理秩与先验模块的特征维度解耦，提升表示力与灵活性。

Result: 在模拟与真实数据上，相比现有方法，LRDUN以更低计算量获得更优或SOTA的重建质量。

Conclusion: 面向SCI重建，基于低秩建模与GFUM的LRDUN有效缓解2D到3D反演的病态性，在保持或提升精度的同时显著降低计算开销，提供了高效且可扩展的新范式。

Abstract: Deep unfolding networks (DUNs) have achieved remarkable success and become the mainstream paradigm for spectral compressive imaging (SCI) reconstruction. Existing DUNs are derived from full-HSI imaging models, where each stage operates directly on the high-dimensional HSI, refining the entire data cube based on the single 2D coded measurement. However, this paradigm leads to computational redundancy and suffers from the ill-posed nature of mapping 2D residuals back to 3D space of HSI. In this paper, we propose two novel imaging models corresponding to the spectral basis and subspace image by explicitly integrating low-rank (LR) decomposition with the sensing model. Compared to recovering the full HSI, estimating these compact low-dimensional components significantly mitigates the ill-posedness. Building upon these novel models, we develop the Low-Rank Deep Unfolding Network (LRDUN), which jointly solves the two subproblems within an unfolded proximal gradient descent (PGD) framework. Furthermore, we introduce a Generalized Feature Unfolding Mechanism (GFUM) that decouples the physical rank in the data-fidelity term from the feature dimensionality in the prior module, enhancing the representational capacity and flexibility of the network. Extensive experiments on simulated and real datasets demonstrate that the proposed LRDUN achieves state-of-the-art (SOTA) reconstruction quality with significantly reduced computational cost.

</details>


### [160] [Unified Deep Learning Platform for Dust and Fault Diagnosis in Solar Panels Using Thermal and Visual Imaging](https://arxiv.org/abs/2511.18514)
*Abishek Karthik,Sreya Mynampati,Pandiyaraju V*

Main category: cs.CV

TL;DR: 提出一个集中式平台，利用图像与热成像结合CNN/ResNet与带自注意力的KerNet，对太阳能板进行灰尘与故障（裂纹、失效等）检测；经多种对比实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 太阳能板输出受灰尘、裂纹、温度等影响显著，现有方法往往单一、分散或效果有限；需要一个统一、高效且可用于例行维护与不同规模场景的自动检测系统。

Method: 对获取的图像进行伽马校正与高斯滤波及归一化等预处理；基于可见光图像检测灰尘（阴影、树叶、鸟粪、空气污染、人为污渍等细粒度类别）；基于热成像检测故障（裂纹、单元失效等）；采用CNN、ResNet，并融合自注意力机制的KerNet进行分类，构建单平台多应用检测框架，并通过多指标比较评估性能。

Result: 实验对比显示该多应用模型在效率与准确率方面整体优于现有模型；可在不同地理与规模（家庭到大型电站）环境中有效运行。

Conclusion: 所提出的集中式多任务模型在太阳能板灰尘与故障检测上表现优化、准确且实用，可作为常规运维与状态监测的有效工具。

Abstract: Solar energy is one of the most abundant and tapped sources of renewable energies with enormous future potential. Solar panel output can vary widely with factors like intensity, temperature, dirt, debris and so on affecting it. We have implemented a model on detecting dust and fault on solar panels. These two applications are centralized as a single-platform and can be utilized for routine-maintenance and any other checks. These are checked against various parameters such as power output, sinusoidal wave (I-V component of solar cell), voltage across each solar cell and others. Firstly, we filter and preprocess the obtained images using gamma removal and Gaussian filtering methods alongside some predefined processes like normalization. The first application is to detect whether a solar cell is dusty or not based on various pre-determined metrics like shadowing, leaf, droppings, air pollution and from other human activities to extent of fine-granular solar modules. The other one is detecting faults and other such occurrences on solar panels like faults, cracks, cell malfunction using thermal imaging application. This centralized platform can be vital since solar panels have different efficiency across different geography (air and heat affect) and can also be utilized for small-scale house requirements to large-scale solar farm sustentation effectively. It incorporates CNN, ResNet models that with self-attention mechanisms-KerNet model which are used for classification and results in a fine-tuned system that detects dust or any fault occurring. Thus, this multi-application model proves to be efficient and optimized in detecting dust and faults on solar panels. We have performed various comparisons and findings that demonstrates that our model has better efficiency and accuracy results overall than existing models.

</details>


### [161] [Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion](https://arxiv.org/abs/2511.18516)
*Haidong Kang,Ketong Qian,Yi Lu*

Main category: cs.CV

TL;DR: 提出一种无需梯度训练的FSCIL新范式：用条件扩散过程替代传统梯度更新，并结合LLM生成文本的多模态表示，达到SOTA性能且显著降低计算/内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有FSCIL主要依赖梯度优化以适应新增类，但在小样本下会引发灾难性遗忘、难以适应新类，且随着增量类增多训练成本爆炸。作者希望在极少数据情形下摆脱梯度学习的局限与代价。

Method: 观察到梯度优化与条件扩散过程存在对应关系，提出CD-FSCIL框架：用条件扩散驱动的生成式“过渡”来替代参数梯度更新，实现无训练的增量适应；同时引入多模态策略，将视觉特征与由LLM自动生成的类别自然语言描述融合，以缓解样本稀缺、提升泛化。

Result: 在主流FSCIL基准上达到SOTA，同时显著降低计算与内存开销；减少遗忘并提升对新类的适应能力。

Conclusion: 通过条件扩散实现的训练自由增量范式可有效替代梯度更新，结合LLM文本增强能在FSCIL中兼顾稳定与可塑性，标志着向训练自由的持续适应转变。

Abstract: Efforts to overcome catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) have primarily focused on developing more effective gradient-based optimization strategies. In contrast, little attention has been paid to the training cost explosion that inevitably arises as the number of novel classes increases, a consequence of relying on gradient learning even under extreme data scarcity. More critically, since FSCIL typically provides only a few samples for each new class, gradient-based updates not only induce severe catastrophic forgetting on base classes but also hinder adaptation to novel ones. This paper seeks to break this long-standing limitation by asking: Can we design a training-free FSCIL paradigm that entirely removes gradient optimization? We provide an affirmative answer by uncovering an intriguing connection between gradient-based optimization and the Conditional Diffusion process. Building on this observation, we propose a Conditional Diffusion-driven FSCIL (CD-FSCIL) framework that substitutes the conventional gradient update process with a diffusion-based generative transition, enabling training-free incremental adaptation while effectively mitigating forgetting. Furthermore, to enhance representation under few-shot constraints, we introduce a multimodal learning strategy that integrates visual features with natural language descriptions automatically generated by Large Language Models (LLMs). This synergy substantially alleviates the sample scarcity issue and improves generalization across novel classes. Extensive experiments on mainstream FSCIL benchmarks demonstrate that our method not only achieves state-of-the-art performance but also drastically reduces computational and memory overhead, marking a paradigm shift toward training-free continual adaptation.

</details>


### [162] [DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation](https://arxiv.org/abs/2511.18533)
*Md Mizanur Rahman Mustakim,Jianwu Li,Sumya Bhuiyan,Mohammad Mehedi Hasan,Bing Han*

Main category: cs.CV

TL;DR: 提出DE-KAN双编码器+KAN瓶颈，用于全景牙片单牙分割，融合全局与局部特征，显著提升mIoU/Dice等指标，较SOTA最高提升Dice 4.7%。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在牙齿全景片分割中受解剖差异、形状不规则、重叠遮挡影响，难以同时捕获全局布局与局部细节，导致精度与鲁棒性不足。需要一种能兼顾多尺度、增强表达能力与可解释性的网络。

Method: 构建Dual Encoder Kolmogorov Arnold Network：对原始输入用定制CNN编码器提取细粒度局部特征；对增强输入用ResNet-18编码器提取全局上下文。两路特征在KAN式瓶颈融合，采用基于Kolmogorov–Arnold表示定理的可学习非线性激活以提升表达力与可解释性，随后进行解码实现语义分割。

Result: 在两个牙科X光基准数据集上优于SOTA：mIoU 94.5%、Dice 97.1%、Accuracy 98.91%、Recall 97.36%；Dice较现有方法最高提升+4.7%。

Conclusion: 双编码器的全局-局部互补与KAN瓶颈的非线性表征提升了单牙分割精度与稳健性，证明DE-KAN在复杂全景牙片上具有领先表现与潜在临床应用价值。

Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.

</details>


### [163] [HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2511.18534)
*Pengcheng Fang,Hongli Chen,Guangzhen Yao,Jian Shi,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: cs.CV

TL;DR: HiFi-MambaV2 是一种用于欠采样 MRI 重建的层次式 MoE-Mamba 架构，结合频率分解与内容自适应计算，通过SF-Lap稳健地分离低/高频并用共享路由MoE实现逐像素稀疏派发，在多数据集/设置下显著提升PSNR/SSIM/NMSE与高频细节和结构保真。


<details>
  <summary>Details</summary>
Motivation: 欠采样k-space导致混叠与高频细节丢失，现有CNN/Transformer/Mamba方法在高频恢复与跨深度稳定性、计算效率和长程依赖建模之间存在权衡，需要一种兼顾高频保真、结构一致性与鲁棒性的重建框架。

Method: 提出HiFi-MambaV2，包括：1) 可分离频率一致的拉普拉斯金字塔（SF-Lap），提供抗混叠、稳定的低/高频分支；2) 分层共享路由的MoE，进行逐像素top-1稀疏派发到共享专家与局部路由器，实现有效专家特化与跨深度稳定；并在解卷积式（unrolled）含数据一致性正则的主干中融合轻量全局上下文路径，增强长程推理与解剖一致性。

Result: 在fastMRI、CC359、ACDC、M4Raw、Prostate158上，单/多线圈、不同加速因子场景中，相比CNN、Transformer及已有Mamba基线，PSNR/SSIM/NMSE均有一致提升，尤其在高频细节与整体结构保真上更优。

Conclusion: HiFi-MambaV2通过频率分解与内容自适应稀疏计算的协同，在保持解剖一致性的同时可靠恢复高频细节，展现了强鲁棒性与泛化性，可作为MRI重建的有效新范式。

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.

</details>


### [164] [Zero-Shot Video Deraining with Video Diffusion Models](https://arxiv.org/abs/2511.18537)
*Tuomas Varanka,Juan Luis Gonzalez,Hyeongwoo Kim,Pablo Garrido,Xu Yao*

Main category: cs.CV

TL;DR: 提出首个零样本视频去雨方法：不需合成数据与微调，利用预训练文本到视频扩散模型，通过视频反演+负向提示与注意力切换，去除复杂动态场景中的雨并保持结构一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频去雨要么依赖合成成对数据，导致对真实雨泛化差；要么用静态相机数据，难以处理背景/相机运动；微调扩散模型虽有效，但会削弱生成先验，泛化受限。

Method: 将输入视频反演到预训练文本到视频扩散模型的潜空间，在重建过程中使用负向提示将生成分布“远离”雨的概念。同时提出注意力切换机制，在去雨时维护动态背景与结构一致性，缓解直接负向提示带来的伪影。全流程无需监督训练或模型微调。

Result: 在真实世界雨视频数据集上进行大量实验，显著优于现有方法，并在各种动态场景中表现出稳健的泛化能力。

Conclusion: 基于预训练文本到视频扩散模型与注意力切换的零样本视频去雨框架，可在无需合成数据与微调的前提下，有效去除复杂动态场景中的雨并保持结构一致性，展现出强泛化与实际应用潜力。

Abstract: Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.

</details>


### [165] [C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction](https://arxiv.org/abs/2511.18559)
*Kuan Wei Huang,Brandon Li,Bharath Hariharan,Noah Snavely*

Main category: cs.CV

TL;DR: 论文提出跨模态几何匹配数据集C3，用于照片与平面图间的像素级对应学习，并在该数据上训练显著提升现有方法（RMSE提升34%），同时揭示跨模态几何推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有几何模型（如DUSt3R）在训练分布内表现好，但对极端视角差异（地面vs.航拍）或模态差异（照片vs.示意图）泛化差；现有联合理解数据集要么模态单一（VIGOR），要么缺乏明确对应（WAFFLE），无法支撑跨模态几何对应研究。

Method: 构建新数据集C3：从互联网照片集通过SfM重建多场景3D，再将重建结果人工配准到从互联网收集的平面图，自动派生出照片与平面图间的像素级对应与相机位姿；随后用该数据训练并评估最先进的对应模型。

Result: C3包含597个场景、9万对照片-平面图、1.53亿像素级对应与8.5万相机姿态。SOTA对应模型在该任务上表现不佳；在C3上训练后，最佳方法的RMSE进一步降低34%。

Conclusion: C3为跨模态（照片-平面图）几何匹配提供大规模、带对应和姿态标注的数据，显著提升模型性能，并暴露了跨模态几何推理中的开放问题，为后续研究提供基准与方向。

Abstract: Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos. However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs. ground) or modalities (e.g., photos vs. abstract drawings) compared to what was observed during training. This paper addresses a challenging version of this problem: predicting correspondences between ground-level photos and floor plans. Current datasets for joint photo--floor plan reasoning are limited, either lacking in varying modalities (VIGOR) or lacking in correspondences (WAFFLE). To address these limitations, we introduce a new dataset, C3, created by first reconstructing a number of scenes in 3D from Internet photo collections via structure-from-motion, then manually registering the reconstructions to floor plans gathered from the Internet, from which we can derive correspondence between images and floor plans. C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. We find that state-of-the-art correspondence models struggle on this task. By training on our new data, we can improve on the best performing method by 34% in RMSE. We also identify open challenges in cross-modal geometric reasoning that our dataset aims to help address.

</details>


### [166] [PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation](https://arxiv.org/abs/2511.18570)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Dinesh Manocha*

Main category: cs.CV

TL;DR: PhysGS 将物理属性估计融入 3D Gaussian Splatting，通过贝叶斯推断从视觉与视觉-语言先验中推断密集的每点物理属性，并显式建模不确定性，在多数据集上显著提升质量、硬度与摩擦等估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建主要关注几何与外观，无法从图像中推断摩擦、硬度、弹性、材料组成等底层物理属性；而可靠的人机与机器人交互需要这些属性，且需要不确定性感知以指导安全决策。

Method: 基于3D Gaussian Splatting，提出 PhysGS：将每个高斯点关联材料与物理属性的概率分布，结合视觉线索与视觉-语言先验做贝叶斯更新，随着新观测迭代细化；同时区分并建模 aleatoric 与 epistemic 不确定性，实现不确定性感知的场景解释。

Result: 在 ABO-500、室内与室外真实数据集上，相比确定性基线：质量估计精度提升最高 22.8%；肖氏硬度误差降低最高 61.2%；动摩擦系数误差降低最高 18.1%。

Conclusion: PhysGS 将3D重建、物理推理与不确定性建模统一到一个空间连续框架，实现从视觉和语言先验中密集估计物理属性，且具备不确定性感知，在多场景下显著优于确定性方法。

Abstract: Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.

</details>


### [167] [Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation](https://arxiv.org/abs/2511.18591)
*Wei Dong,Han Zhou,Junwei Lin,Jun Chen*

Main category: cs.CV

TL;DR: 提出一个由视觉自回归(VAR)与视觉-语言模型(VLM)先验引导的无监督生成式框架，用于真实暗光图像复原，在动态照明与模糊建模上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 真实暗光图像同时存在低可见度、低对比度、复杂噪声与模糊。现有方法依赖成对数据或不能刻画动态照明与模糊，导致泛化差，亟需无需配对数据、可自适应建模光照与模糊的框架。

Method: 1) 用VLM估计可见度分数，驱动自适应曲线估计，对多样照明进行调制，为VAR提供信息性条件；2) 在VAR中引入动态、空间频率感知的旋转位置编码(SF-RoPE)以更好建模因模糊受损的结构；3) 设计递归相位域调制：在频域相位上进行有界迭代细化，并由VLM评估的模糊分数指导，抑制模糊伪影；整体为无监督训练/推理流程。

Result: 在多个基准数据集上获得最新最优(SOTA)表现，能在无配对监督下有效提升暗光图像的可见度、对比度与细节，并抑制噪声与模糊伪影。

Conclusion: VLM引导的VAR恢复框架，通过可见度/模糊评分驱动的照明调制、SF-RoPE及相位域递归细化，实现对真实暗光复杂退化的鲁棒建模与无监督SOTA性能。

Abstract: Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.

</details>


### [168] [Stage-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI](https://arxiv.org/abs/2511.18595)
*Wenhao Guo,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: 论文对胶质母细胞瘤放疗后复查MRI中真性进展与假进展的区分进行阶段化（不同随访时点）基准评测，发现第二次随访区分度更好；Mamba+CNN在准确率与效率上最均衡，Transformer AUC高但代价大，轻量CNN高效但稳定性欠佳；总体判别仍受限于任务难度与数据不均衡。


<details>
  <summary>Details</summary>
Motivation: 临床上在早期复查时难以区分真进展与假进展，传统影像学与单一时间点DL模型表现有限，且缺乏对“随访阶段”敏感的对比基准，影响模型选择与临床转化。

Method: 基于Burdenko GBM Progression队列（n=180），按放疗后不同随访时点分别训练与评估11类代表性深度学习架构（CNN、LSTM、混合、Transformer、选择性状态空间模型），在统一、质量控制驱动的流程与按患者层面的交叉验证下进行阶段特异的横向基准；比较准确率、F1、AUC与计算效率，并分析训练超参如batch size的敏感性。

Result: 两阶段整体准确率相近（约0.70–0.74），但第二次随访的F1与AUC更高，显示更好的可分性；Mamba+CNN在准确率-效率上最优；Transformer取得竞争性AUC但计算成本高；轻量CNN效率高但可靠性较差；性能对batch size较敏感；受任务固有难度与类别不平衡影响，绝对判别能力仍然有限。

Conclusion: 建立了考虑随访阶段的基准，为模型与训练协议的标准化提供参考；建议未来引入纵向建模、多序列MRI与更大多中心数据，以提升对真进展与假进展的判别。

Abstract: Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.

</details>


### [169] [NeAR: Coupled Neural Asset-Renderer Stack](https://arxiv.org/abs/2511.18600)
*Hong Li,Chongjie Ye,Houyuan Chen,Weiqing Xiao,Ziyang Yan,Lixing Xiao,Zhaoxi Chen,Jianfeng Xiang,Shaocong Xu,Xuhui Liu,Yikai Wang,Baochang Zhang,Xiaoguang Han,Jiaolong Yang,Hao Zhao*

Main category: cs.CV

TL;DR: 提出NeAR：将“神经资产表示”和“神经渲染器”耦合成端到端可学习的图形栈，实现实时、可重光照渲染，并在四项任务上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有两条独立路线：神经资产用于传统渲染管线；神经渲染器从传统资产到图像。二者分离限制了保真度、一致性与效率。作者认为共同设计资产表示与渲染器可解锁更高质量与端到端学习能力。

Method: 提出NeAR耦合栈。资产侧：在Trellis式Structured 3D Latents基础上，引入“光照同质化”的神经资产，通过rectified-flow骨干从随意光照输入预测Lighting-Homogenized SLAT，将几何与内在材质压缩到视角无关的潜空间。渲染侧：设计“光照感知”的神经渲染器，接收该神经资产、显式视角编码与HDR环境光贴图，实现实时可重光照渲染。

Result: 在四项任务上验证：（1）基于G-buffer的前向渲染，（2）随机光照单张重建，（3）未知光照单张重光照，（4）新视角重光照。定量指标与感知质量均优于SOTA。

Conclusion: 将神经资产与渲染器作为协同设计组件，可构建端到端可学习、具高保真与一致性的图形栈；NeAR展示了其实效与实时可重光照能力，并期望推动未来图形系统以耦合视角设计。

Abstract: Neural asset authoring and neural rendering have emerged as fundamentally disjoint threads: one generates digital assets using neural networks for traditional graphics pipelines, while the other develops neural renderers that map conventional assets to images. However, the potential of jointly designing the asset representation and renderer remains largely unexplored. We argue that coupling them can unlock an end-to-end learnable graphics stack with benefits in fidelity, consistency, and efficiency. In this paper, we explore this possibility with NeAR: a Coupled Neural Asset-Renderer Stack. On the asset side, we build on Trellis-style Structured 3D Latents and introduce a lighting-homogenized neural asset: from a casually lit input, a rectified-flow backbone predicts a Lighting-Homogenized SLAT that encodes geometry and intrinsic material cues in a compact, view-agnostic latent. On the renderer side, we design a lighting-aware neural renderer that uses this neural asset, along with explicit view embeddings and HDR environment maps, to achieve real-time, relightable rendering. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit single-image reconstruction, (3) unknown-lit single-image relighting, and (4) novel-view relighting. Our coupled stack surpasses state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.

</details>


### [170] [RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data](https://arxiv.org/abs/2511.18601)
*Wenchao Ma,Dario Kneubuehler,Maurice Chu,Ian Sachs,Haomiao Jiang,Sharon Xiaolei Huang*

Main category: cs.CV

TL;DR: 提出RigAnyFace，一种可扩展的神经自动面部rig框架，能将多拓扑（含多不连通部件）的中性网格变形到FACS表情形成blendshape，并通过3D+2D监督实现强泛化，在艺术家资产与野外数据上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 手工面部rig代价高、数据稀缺且难以覆盖多样拓扑（尤其包含眼球等不连通部件）；现有方法受限于三角划分与标注规模，泛化差、无法稳定支持复杂资产。

Method: 1) 设计与三角划分无关的表面学习网络，条件化FACS参数，专门结构处理不连通部件；2) 将中性网格变形到行业标准FACS姿态以生成表达丰富的blendshape；3) 构建带少量专业艺术家rig的3D真值数据集；4) 为无rig中性网格提出2D监督策略，引入大规模无标注数据以扩充训练并提升泛化。

Result: 大量实验显示在艺术家资产与野外样本上，RAF在精度与泛化方面均优于现有方法，并能稳健支持包含眼球等多个不连通部件的细节表情动画。

Conclusion: RAF实现对多样拓扑面部网格的自动rig，结合3D与2D监督实现可扩展训练，兼顾准确性与泛化，并将支持扩展到多不连通部件，推进了面部自动rig的实用化。

Abstract: In this paper, we present RigAnyFace (RAF), a scalable neural auto-rigging framework for facial meshes of diverse topologies, including those with multiple disconnected components. RAF deforms a static neutral facial mesh into industry-standard FACS poses to form an expressive blendshape rig. Deformations are predicted by a triangulation-agnostic surface learning network augmented with our tailored architecture design to condition on FACS parameters and efficiently process disconnected components. For training, we curated a dataset of facial meshes, with a subset meticulously rigged by professional artists to serve as accurate 3D ground truth for deformation supervision. Due to the high cost of manual rigging, this subset is limited in size, constraining the generalization ability of models trained exclusively on it. To address this, we design a 2D supervision strategy for unlabeled neutral meshes without rigs. This strategy increases data diversity and allows for scaled training, thereby enhancing the generalization ability of models trained on this augmented data. Extensive experiments demonstrate that RAF is able to rig meshes of diverse topologies on not only our artist-crafted assets but also in-the-wild samples, outperforming previous works in accuracy and generalizability. Moreover, our method advances beyond prior work by supporting multiple disconnected components, such as eyeballs, for more detailed expression animation. Project page: https://wenchao-m.github.io/RigAnyFace.github.io

</details>


### [171] [Functional Localization Enforced Deep Anomaly Detection Using Fundus Images](https://arxiv.org/abs/2511.18627)
*Jan Benedikt Ruhland,Thorsten Papenbrock,Jan-Peter Sowa,Ali Canbay,Nicole Eter,Bernd Freisleben,Dominik Heider*

Main category: cs.CV

TL;DR: 评估ViT在多数据集和多增强/增强策略下进行眼底疾病检测；ViT整体表现稳健（准确率约0.79–0.84），几何与颜色增强最有效，直方图均衡在结构细微数据集有益，拉普拉斯增强有害；在Papila上AUC 0.91超越卷积集成；并引入GANomaly异常检测（AUC 0.76）与GUESS校准以支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 眼底图像质量不一、早期病变微弱、跨数据集域偏移使自动化筛查困难；需要系统评估更具泛化能力的模型与可提升鲁棒性的训练策略，并提供可解释与可校准的输出以利临床落地。

Method: 以Vision Transformer分类器为核心，跨多公共数据集与自建高质量AEyeDB进行训练/测试，系统比较多种数据增强（几何、颜色、直方图均衡、拉普拉斯等）与增强策略；在Papila等基准上报告AUC/准确率；构建基于GANomaly的异常检测器并评估泛化与可解释性；使用GUESS进行概率校准，实现阈值无关的决策支持。

Result: ViT在各数据集/疾病上的准确率约0.789–0.843；DR与AMD检测可靠，青光眼误判最多；几何与颜色增强带来最稳定增益，直方图均衡对结构细微主导数据集有效，拉普拉斯增强普遍降绩；在Papila上，ViT+几何增强AUC 0.91，优于先前卷积集成AUC 0.87；GANomaly异常检测AUC 0.76并具重建可解释与对未见数据的鲁棒性。

Conclusion: Transformer结合合适的数据增强与多数据集训练能稳定优于卷积基线，尤其在Papila上显著领先；针对难检疾病（如青光眼）仍需改进；不建议使用拉普拉斯增强；直方图均衡在特定数据集有用；引入GANomaly与校准（GUESS）为临床提供阈值无关、可解释的决策支持。

Abstract: Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.
  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.

</details>


### [172] [Health system learning achieves generalist neuroimaging models](https://arxiv.org/abs/2511.18640)
*Akhil Kondepudi,Akshay Rao,Chenhui Zhao,Yiwei Lyu,Samir Harake,Soumyanil Banerjee,Rushikesh Joshi,Anna-Katharina Meissner,Renly Hou,Cheng Jiang,Asadur Chowdury,Ashok Srinivasan,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 论文提出NeuroVFM，一种在健康体系内“未筛选临床体积影像”上训练的大型视觉基础模型，用于神经影像任务；在多项临床任务（诊断、报告生成、分诊等）上超越前沿通用模型，并显著降低幻觉与关键错误，展示了“健康系统学习”范式的有效性。


<details>
  <summary>Details</summary>
Motivation: 前沿通用AI主要依赖公开互联网数据，缺乏对隐私受限的临床神经影像数据的直接访问，导致在神经影像任务中性能不足；需要一种既能规模化利用临床数据、又能提升安全与可解释性的途径。

Method: 提出“健康系统学习”范式：直接从医疗体系产生的未筛选、常规临床MRI/CT体积数据中学习。构建NeuroVFM，基于可扩展的三维体积联合嵌入预测架构，在524万例临床MRI/CT体积上训练；并通过轻量级视觉指令微调将其与开源语言模型耦合，以执行诊断、报告生成、分诊等任务，同时展示可解释的视觉定位。

Result: NeuroVFM学得全面的脑部解剖与病理表征，在放射诊断、报告生成等多项任务上达成SOTA；与开放语言模型配合后，在准确性、临床分诊和专家偏好上优于前沿通用模型，显示出涌现的神经解剖理解与可解释的诊断可视化定位，并减少幻觉与关键性错误。

Conclusion: 利用健康系统内大规模、未筛选临床影像进行训练可构建通用型临床基础模型；NeuroVFM验证了该范式的有效性，提供了可扩展、更加安全且可解释的临床AI框架。

Abstract: Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.

</details>


### [173] [From Healthy Scans to Annotated Tumors: A Tumor Fabrication Framework for 3D Brain MRI Synthesis](https://arxiv.org/abs/2511.18654)
*Nayu Dong,Townim Chowdhury,Hieu Phan,Mark Jenkinson,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: 提出TF框架：仅用少量标注和大量健康MRI，进行两阶段无配对3D脑肿瘤合成（粗生成+生成式细化），大规模生成成对图像-标签，用于增强下游分割，在低数据场景显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 临床AI常缺乏带标注的MRI肿瘤数据，手工建模费力且需专家知识；深度生成模型常需大量配对训练数据，不适合数据稀缺环境。因此需要一种无需大量配对数据、可自动生成高质量带标签样本的方法，以缓解分割训练的数据匮乏。

Method: 提出Tumor Fabrication (TF) 两阶段无配对3D合成：第一阶段进行粗肿瘤合成，将肿瘤结构注入健康脑MRI；第二阶段用生成式模型进行细化与逼真化。只依赖健康扫描和少量真实标注，自动生成大规模成对的合成图像-标签，用于监督分割训练的数据增强。

Result: 使用TF生成的合成对在低样本场景下显著提升下游脑肿瘤分割性能，相较仅用少量真实数据训练更好，显示出可扩展、稳健的数据增益效果。

Conclusion: TF为临床数据稀缺问题提供可扩展的无配对3D肿瘤合成方案，自动生成成对数据，有效增强监督分割，在低数据条件下显著改善性能，适合用于医学影像数据充实与临床AI落地。

Abstract: The scarcity of annotated Magnetic Resonance Imaging (MRI) tumor data presents a major obstacle to accurate and automated tumor segmentation. While existing data synthesis methods offer promising solutions, they often suffer from key limitations: manual modeling is labor intensive and requires expert knowledge. Deep generative models may be used to augment data and annotation, but they typically demand large amounts of training pairs in the first place, which is impractical in data limited clinical settings. In this work, we propose Tumor Fabrication (TF), a novel two-stage framework for unpaired 3D brain tumor synthesis. The framework comprises a coarse tumor synthesis process followed by a refinement process powered by a generative model. TF is fully automated and leverages only healthy image scans along with a limited amount of real annotated data to synthesize large volumes of paired synthetic data for enriching downstream supervised segmentation training. We demonstrate that our synthetic image-label pairs used as data enrichment can significantly improve performance on downstream tumor segmentation tasks in low-data regimes, offering a scalable and reliable solution for medical image enrichment and addressing critical challenges in data scarcity for clinical AI applications.

</details>


### [174] [Robust Physical Adversarial Patches Using Dynamically Optimized Clusters](https://arxiv.org/abs/2511.18656)
*Harrison Bagley,Will Meakin,Simon Lucey,Yee Wei Law,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 论文提出一种基于超像素的正则化方法，解决对抗贴纸在缩放时因插值导致的高频信息丢失与攻击退化问题，通过可微分的SLIC聚类与隐函数定理反传，使贴纸在多尺度下结构保持、颜色稳健，从而在数字与物理场景中均提升攻击效果，并通过新的物理评测协议验证。


<details>
  <summary>Details</summary>
Motivation: 现有物理对抗贴纸虽考虑可打印性、平滑性、视角与噪声等，但较少关注“尺度变化”。缩放引发的插值会混色、平滑细节，削弱对抗信号，导致数字与物理世界下性能不稳。需要一种能在多尺度保持结构的优化约束。

Method: 在贴纸优化过程中引入动态SLIC超像素分割：利用SLIC将贴纸像素自适应聚类，借助隐函数定理对SLIC过程反向传播以更新超像素边界与颜色，从而鼓励形成对尺度变化更稳定的块状结构，降低插值导致的高频损失。该正则与常规可实现性/鲁棒性正则可联合使用。

Result: 与基线方法相比，在数字环境中对抗成功率更高且对缩放更稳健；将贴纸打印/显示后在真实世界中，性能提升得以保留。作者构建了基于屏幕与纸板裁切的系统化物理评测流程，实证显示该方法在多种真实条件变化下更优。

Conclusion: 通过可微SLIC驱动的超像素正则，可引导对抗贴纸形成多尺度保持的结构，缓解缩放插值带来的对抗退化，实现数字与物理场景的一致性能提升，并提供了客观物理评测协议验证方法有效性。

Abstract: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.

</details>


### [175] [Data Augmentation Strategies for Robust Lane Marking Detection](https://arxiv.org/abs/2511.18668)
*Flora Lian,Dinh Quang Huynh,Hector Penades,J. Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 提出一种生成式AI数据增强管线，针对侧置摄像头场景，结合透视变换、AI修复与车体遮挡叠加，提升SCNN与UFLDv2在域移位下的车道线检测鲁棒性，实验显示精、召、F1均提升。


<details>
  <summary>Details</summary>
Motivation: 公开数据集（如CULane）多为前视摄像头，难以泛化到部署中的侧置摄像头（用于轮-车道监测），导致域移位并显著降低检测精度与稳定性。需要一种可扩展的方法在不重新采集大规模数据的情况下适配部署视角与环境。

Method: 构建生成式AI增强流水线：1) 几何透视变换以合成侧置/特定安装位姿视角；2) AI驱动的图像修复（inpainting）以补齐变换后缺失区域并保持语义与纹理一致；3) 车辆车身叠加（overlays）以模拟真实遮挡与边界条件，同时保持车道线连贯性。将增强数据与原始数据结合训练两种SOTA模型（SCNN、UFLDv2），比较不同条件（含阴影）的表现。

Result: 在含增强数据训练后，SCNN与UFLDv2在侧置摄像头与复杂光照（阴影等）条件下的精度、召回率与F1分数均明显提升，相较各自的仅用公开预训练权重的基线更稳健。

Conclusion: 该方法有效弥合公开数据与特定部署场景的视角差异，提供可扩展、实用的数据增强框架，能在试点部署中提高车道线检测的可靠性与鲁棒性。

Abstract: Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.
  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.

</details>


### [176] [Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement](https://arxiv.org/abs/2511.18672)
*Yuchen Xia,Souvik Kundu,Mosharaf Chowdhury,Nishil Talati*

Main category: cs.CV

TL;DR: Sphinx提出一种训练-free的混合推理框架，用回归式快速初始化+选择性扩散精炼（自适应噪声调度），以显著降低扩散式NVS的计算，同时保持接近扩散级画质，在小于5%感知下降下平均提速1.8倍，提供质量-延迟可调的新Pareto前沿。


<details>
  <summary>Details</summary>
Motivation: 扩散式NVS质量高但推理代价大；回归式NVS推理快但质量差。实际部署需要既高质量又高效率、可按场景动态权衡延迟与画质的方案。

Method: 训练-free混合推理：1) 用回归模型生成快速初始结果作为扩散模型的引导，减少去噪工作量；2) 选择性精炼与自适应噪声调度，对不确定区域/帧分配更多计算预算，从而在整体上降低步数与算力消耗，同时支持按需调节质量-性能。

Result: 在NVS任务上，相比纯扩散推理平均加速1.8×，感知质量下降<5%，在质量-延迟维度形成新的Pareto前沿。

Conclusion: 通过回归初始化+自适应选择性扩散精炼，Sphinx在无需额外训练的前提下实现接近扩散级画质与显著推理加速，并提供可调的质量-延迟权衡，适合动态推理场景。

Abstract: Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.

</details>


### [177] [Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers](https://arxiv.org/abs/2511.18673)
*Yiqing Shi,Yiren Song,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出Edit2Perceive：用基于编辑的扩散Transformer替代传统T2I生成器做稠密感知，统一解决深度、法线与抠图，单步确定性推理更快并在小数据集上训练，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有稠密感知多依赖为随机生成而设计的T2I扩散模型，缺乏与输入图像一致的结构约束；而图像编辑扩散模型天然具备图像到图像的一致性，更可能更好地服务几何与外观保真的感知任务。

Method: 基于FLUX.1 Kontext编辑架构，进行全参数微调；在扩散去噪的中间状态引入像素空间一致性损失，强化跨步的结构保持与细节精炼；将统一的编辑式扩散框架适配到深度估计、法线估计与图像抠图；推理时采用单步确定性推断以提升速度。

Result: 在三个任务（深度、法线、抠图）上进行大量实验，获得全面SOTA；在小规模数据上训练即可达到强性能；推理速度因单步确定性而显著提升。

Conclusion: 面向编辑的扩散Transformer比传统T2I生成模型更适合作为稠密感知的基础模型；统一的Edit2Perceive框架实现了结构一致、几何感知的高质量预测，并兼顾效率与数据效率。

Abstract: Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.

</details>


### [178] [MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis](https://arxiv.org/abs/2511.18676)
*Yongcheng Yao,Yongshuo Zong,Raman Dutt,Yongxin Yang,Sotirios A Tsaftaris,Timothy Hospedales*

Main category: cs.CV

TL;DR: 论文提出MedVision数据集与基准，专注于量化医学影像任务（检测、肿瘤大小估计、角度/距离测量），并显示在该数据上微调可显著提升VLM的量化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLM多偏向分类或定性描述，但临床决策常依赖定量测量（尺寸、角度等）。量化推理能力在现有VLM中欠缺，需要专门的数据与评测来推动进展。

Method: 构建大规模MedVision数据集与基准：覆盖22个公开数据集、不同解剖/模态，共约3080万图像-标注对；设定三类代表性量化任务（结构/异常检测、肿瘤/病灶尺寸估计、角度/距离测量）；评测现成VLM并在MedVision上进行有监督微调，比较前后性能。

Result: 现成VLM在量化任务上表现较差；在MedVision上进行SFT后，检测、T/L尺寸估计、A/D测量均显著提升，误差下降、精度提高（定量指标未给出但整体趋势明确）。

Conclusion: MedVision为发展具备稳健量化推理能力的医学VLM奠定基础；通过专门的数据与任务定义，VLM可从定性走向可用于临床相关的量化分析。

Abstract: Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., "Is this normal or abnormal?") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.

</details>


### [179] [A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification](https://arxiv.org/abs/2511.18677)
*Yunpeng Gong,Yongjie Hou,Jiangming Shi,Kim Long Diep,Min Jiang*

Main category: cs.CV

TL;DR: 提出KTCAA框架，通过对齐增强与知识迁移催化，在小样本草图-照片跨模态行人重识别中取得SOTA。核心是减小域差异与提升扰动不变性，并在元学习下从RGB域迁移到草图域。


<details>
  <summary>Details</summary>
Motivation: 草图与RGB图像存在显著模态差异且标注稀缺，导致跨模态重识别泛化困难。基于泛化理论，目标域风险主要受域差异与模型对模态扰动的鲁棒性影响，需要一个既能缩小分布差距又能提升对扰动不变性的框架，且可在小样本场景有效工作。

Method: 提出KTCAA，包括两大模块并在元学习范式下联合优化：1) 对齐增强（AA）：对RGB数据施加局部化的草图风格变换，构造逐步接近目标草图分布的中间域，促进分布对齐；2) 知识迁移催化（KTC）：引入最坏情况的扰动（adversarial/consistency-based），并通过一致性约束提升对模态转移的鲁棒性，从而增强扰动不变性。整体在少样本跨域任务上进行元训练，将RGB富数据域学到的对齐知识迁移到草图场景。

Result: 在多个基准数据集上达到最新性能，尤其在数据稀缺条件下显著优于现有方法。

Conclusion: 通过理论驱动的减少域差异与增强扰动不变性，KTCAA在小样本草图-照片跨模态ReID中实现有效的知识迁移与泛化，取得SOTA表现。

Abstract: Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.

</details>


### [180] [Neural Geometry Image-Based Representations with Optimal Transport (OT)](https://arxiv.org/abs/2511.18679)
*Xiang Gao,Yuanpeng Liu,Xinmu Wang,Jiazhi Li,Minghao Guo,Yu Guo,Xiyun Song,Heather Yu,Zhiqiang Lao,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 提出一种基于几何图像(geometry image)的神经表示，通过把不规则网格映射到规则图像网格，实现一次前向推理就可高质量重建；利用最优传输构建几何图像与mipmap，实现连续LoD，兼具高压缩比与低CD/HD误差。


<details>
  <summary>Details</summary>
Motivation: 现有神经网格压缩/重建依赖多级解码器与不规则拓扑，解码开销大、难以直接用高效的图像超分与复原框架。需要一种既能享受图像处理高效性、又适配网格几何的统一表示。

Method: 将3D网格通过geometry image表示成规则像素网格，存储低分辨率的几何图像mipmap，不使用解码器；单次前向过程从低分辨率重建高质量网格。为构造高质量geometry image，引入最优传输分配采样密度，缓解平坦区过采样与细节区欠采样，并支持连续LoD的mipmap。

Result: 在压缩率(CR)、Chamfer距离(CD)与Hausdorff距离(HD)上达到SOTA；实现单次推理重建，计算成本显著低于多次解码的过拟合式方法。

Conclusion: 几何图像化使不规则网格转化为规则图像域，从而可直接利用高效图像神经处理；所提无解码器、存储高效且支持连续LoD的方法，在重建精度与压缩效率上均优于现有方案。

Abstract: Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).

</details>


### [181] [Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework](https://arxiv.org/abs/2511.18682)
*Xiang Gao,Xinmu Wang,Zhou Zhao,Junqi Huang,Xianfeng David Gu*

Main category: cs.CV

TL;DR: 提出一种基于多重微分同胚域与层次化GraphCut的相位展开框架，实现实时级高精度4D面部动态扫描：将相位展开转化为像素标注问题，经多域投票融合得到更稳健的展开计数k，实验显示45.5倍加速且误差更低。


<details>
  <summary>Details</summary>
Motivation: 相位移结构光在VR/AR、数字人和医疗成像中因低光强与高精度而受青睐，但相位展开本质上是病态问题，仅有模2π测量，需假设表面连续性。现有方法在速度与精度间权衡：快速但不准、精准却难实时，亟需兼具两者的新方法。

Method: 将GraphCut相位展开重构为像素标注问题；在图像域对输入包裹相位构造奇数个微分同胚（结合共形映射与最优传输OT）得到多个域；在各域内运行层次化GraphCut估计每像素的展开计数k；跨域对得到的标签图进行多数投票融合，提升对噪声、遮挡与复杂几何的鲁棒性。

Result: 在真实与仿真实验中，相较基线实现约45.5×速度提升，并在L2误差上更低，显示出可达实时应用的潜力。

Conclusion: 多域微分同胚不变性+层次GraphCut+投票融合的框架显著提升相位展开的速度与精度，适用于4D面部动态等场景，缓解病态性与噪声/遮挡带来的困难。

Abstract: Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.

</details>


### [182] [Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation](https://arxiv.org/abs/2511.18684)
*Shristi Das Biswas,Arani Roy,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出ICE：一种无需训练、跨模态的一次性权重修改方法，可在T2I/T2V模型中精确、持久、零开销地移除目标概念，同时最小化对其余生成能力的影响，并更抗对抗绕过。


<details>
  <summary>Details</summary>
Motivation: 现有概念消除方法要么需高成本再训练或推理时开销大，要么易被红队对抗绕过；且很少显式建模待擦除概念与周边内容在潜在语义中的重叠，导致“误伤”；跨T2I与T2V统一有效的方法也匮乏。

Method: 提出Instant Concept Erasure（ICE）：通过各向异性能量加权缩放定义“擦除子空间”和“保留子空间”，并构造闭式的“重叠投影器”以显式正则二者交集；提出凸且Lipschitz有界的光谱去学习目标，在擦除与保留间达成权衡，且具有稳定唯一的解析解。将该解析解对应的“解耦算子”注入模型的文本条件层，使编辑永久化且推理零额外开销。方法无需训练、模态无关、一次性权重修改即可生效。

Result: 在针对风格、物体、身份与敏感（露骨）内容的定向移除上，ICE在T2I与T2V模型中均实现更强的擦除效果与更高的红队鲁棒性，同时仅造成极小的原始生成质量与多样性退化。

Conclusion: ICE提供了一个稳定、可解析、训练免的权重编辑框架，能跨图像与视频生成实现精确且持久的概念移除，减少语义误伤并避免推理时开销，为安全部署提供实用路径。

Abstract: Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.

</details>


### [183] [Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents](https://arxiv.org/abs/2511.18685)
*Dayong Liu,Chao Xu,Weihong Chen,Suyu Zhang,Juncheng Wang,Jiankang Deng,Baigui Sun,Yang Liu*

Main category: cs.CV

TL;DR: 提出CFG-Bench基准，以系统评估MLLM在具身场景中细粒度行动智能，发现现有模型在物理交互指令和高阶意图/评价推理上明显不足；在该数据上SFT可显著提升既有具身基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评测偏重高层规划或空间推理，忽视具身物理交互所需的细粒度行动智能，缺乏系统化评估框架。

Method: 构建CFG-Bench：包含1,368个视频与19,562条涵盖视觉、语言等三模态问答，围绕四类认知能力（物理交互、时序-因果、意图理解、评估判断）。对主流MLLM进行全面评测，并在该数据上进行监督微调以检验迁移收益。

Result: 主流MLLM难以生成细致可执行的物理交互指导，在意图与评价等高阶推理上表现不佳；对CFG-Bench进行SFT可显著提升其在既有具身基准上的成绩。

Conclusion: 细粒度行动表征与表达是MLLM具身智能的关键短板；CFG-Bench为诊断与提升该能力提供了系统工具，数据驱动的教学（SFT）能有效增强具身推理与执行能力。

Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.

</details>


### [184] [EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification](https://arxiv.org/abs/2511.18691)
*Kazi Reyazul Hasan,Md Nafiu Rahman,Wasif Jalal,Sadif Ahmed,Shahriar Raj,Mubasshira Musarrat,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: 提出EVCC多分支混合视觉架构，融合ViT、轻量ConvNeXt和CoAtNet，通过自适应token裁剪、双向跨注意力、辅助头、多路由门，实现SOTA精度同时降低25–35% FLOPs。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer/CNN混合模型在提升分类效果的同时计算开销大、难以在不同部署场景下灵活权衡精度与效率，需要一种能动态调度计算、融合全局与局部与层级特征的架构。

Method: 设计EVCC三分支：ViT、轻量ConvNeXt、CoAtNet。关键机制：1) 自适应token剪枝并保持信息；2) 门控的双向跨注意力在分支间细化特征；3) 辅助分类头进行多任务监督稳定训练；4) 动态路由门基于上下文与置信度加权融合分支输出。

Result: 在CIFAR-100、Tobacco3482、CelebA、Brain Cancer上，相比DeiT-Base、MaxViT-Base、CrossViT-Base，准确率最高提升约2个百分点，同时FLOPs降低25–35%。

Conclusion: EVCC通过自适应token减少与跨分支协同，兼顾全局与局部与层级信息，在多数据集上实现更高精度且更低计算量，能按部署需求动态调整计算，适合实际应用。

Abstract: Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.

</details>


### [185] [Exploring Surround-View Fisheye Camera 3D Object Detection](https://arxiv.org/abs/2511.18695)
*Changcai Li,Wenwei Lin,Zuoxun Hou,Gang Chen,Wei Zhang,Huihui Zhou,Weishi Zheng*

Main category: cs.CV

TL;DR: 研究了将端到端3D目标检测扩展到环视鱼眼相机的可行性，提出基于BEV的FisheyeBEVDet与基于查询的FisheyePETR，两者采用球面空间表征以适配鱼眼几何；并发布CARLA合成的Fisheye3DOD数据集。实验在该数据集上相较移植的针孔模型基线最高提升6.2%。


<details>
  <summary>Details</summary>
Motivation: 主流3D目标检测普遍假设针孔相机成像，直接迁移到鱼眼图像会显著掉点；环视鱼眼在自动驾驶具备成本与视场优势，但缺乏适配模型与评测基准，因此需要专门的几何建模与数据集支持。

Method: 1) 量化针孔检测器迁移到鱼眼图像的性能下降；2) 设计两条适配路线：- FisheyeBEVDet：遵循BEV范式，在特征投影/重建中引入球面空间表示与鱼眼成像模型；- FisheyePETR：遵循query-based范式，在3D查询与图像特征对齐中采用球面坐标/投影；3) 构建CARLA合成数据集Fisheye3DOD，含针孔与鱼眼相机阵列，用于训练与评测。

Result: 在Fisheye3DOD上，两种鱼眼兼容建模均优于直接迁移的针孔基线，最高精度提升达6.2%。

Conclusion: 面向鱼眼环视的端到端3D检测是可行的；采用球面空间表征可有效缓解几何失配；新数据集为后续研究提供了基准。

Abstract: In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.

</details>


### [186] [Dendritic Convolution for Noise Image Recognition](https://arxiv.org/abs/2511.18699)
*Jiarui Xue,Dongjian Yang,Ye Sun,Gang Liu*

Main category: cs.CV

TL;DR: 提出一种模仿生物树突的“树突神经卷积”（DDC），通过邻域交互与类XOR非线性抑噪，从算子层面提升对噪声的鲁棒性；在多种分类与检测模型上显著提升噪声场景性能。


<details>
  <summary>Details</summary>
Motivation: 现实图像识别受噪声干扰严重，而现有主要通过网络结构或训练策略改进，抗噪效果遇到瓶颈；从神经元/树突计算机制汲取启发的底层算子层面方案仍缺乏探索。

Method: 设计一种“树突神经卷积”算子：在卷积底层引入邻域间的交互计算，模拟生物树突的XOR样非线性预处理，通过输入特征间的非线性交互来重构特征提取范式，使噪声不再直接主导卷积响应。将传统卷积替换为DDC，嵌入到YOLOv11-cls、VGG16、EfficientNet-B0等分类网络与YOLOv11/8/5等检测网络中进行评估。

Result: 在含噪数据集上，相比使用传统卷积：EfficientNet-B0分类精度相对提升11.23%；YOLOv8检测mAP提升19.80%。总体显示在复杂噪声环境中显著优于传统卷积。

Conclusion: 从神经元树突计算出发构建的DDC算子，能通过邻域交互与XOR式非线性有效抑制噪声干扰，在分类与检测任务上显著增强鲁棒性，提示从生物启发的算子层面是突破抗噪瓶颈的可行路径。

Abstract: In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.

</details>


### [187] [ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction](https://arxiv.org/abs/2511.18701)
*Mustafa Munir,Harsh Goel,Xiwen Wei,Minkyu Choi,Sahil Shah,Kartikeya Bhardwaj,Paul Whatmough,Sandeep Chinchali,Radu Marculescu*

Main category: cs.CV

TL;DR: ObjectAlign通过结合可学习阈值的感知度量与神经-符号验证，检测并修复视频编辑导致的物体不一致与时间不稳定，并用自适应插帧重建异常片段，在DAVIS与Pexels上显著提升CLIP分数与warp误差。


<details>
  <summary>Details</summary>
Motivation: 视频编辑/生成常引入物体级与时序层面的不一致（闪烁、身份漂移），现有方法缺少对“对象一致性+时序逻辑正确性”的联合、可验证保证与针对性修复。

Method: 1) 为对象一致性度量（CLIP语义相似度、LPIPS、直方图相关、SAM掩码IoU）学习阈值；2) 神经-符号验证器：a) 基于掩码对象嵌入的SMT形式化检查，提供身份不漂移的可证明保障；b) 概率模型检验结合时序逻辑，对视频的形式化表示进行时序一致性核验；将二者与学得阈值统一为单一一致性断言判定帧间过渡；3) 对连续异常帧块，采用神经插帧自适应修复，按需选择插帧深度，用上/下一个有效关键帧重建。

Result: 在DAVIS与Pexels数据集上，相比SOTA，CLIP Score最高提高1.4分，warp error最高提升6.1分，同时实现更稳定的物体身份与更少的帧闪烁。

Conclusion: 融合感知指标与形式化神经-符号验证可有效检测并约束对象与时序一致性，配合自适应插帧对异常片段进行针对性修复，带来可量化的画质与一致性提升。

Abstract: Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed "consistent" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.

</details>


### [188] [CoD: A Diffusion Foundation Model for Image Compression](https://arxiv.org/abs/2511.18706)
*Zhaoyang Jia,Zihan Zheng,Naifu Xue,Jiahao Li,Bin Li,Zongyu Guo,Xiaoyi Zhang,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出CoD：面向压缩的扩散基础模型，替代以文本条件为核心的Stable Diffusion，在超低码率下显著提升扩缩编性能，同时训练成本极低且可复现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散编解码多基于Stable Diffusion等文本条件模型，但文本条件对压缩并不理想，限制了超低码率下的重建质量与效率；需要一个从压缩目标出发、可端到端优化的扩散基础模型。

Method: 从零训练一个“Compression-oriented Diffusion”(CoD)基础模型，不作为固定编解码器，而是可插拔地作为各类扩散编解码框架的核心。采用像素域扩散与仅图像开放数据，优化目标兼顾压缩与生成，可直接替换下游如DiffC的Stable Diffusion组件。

Result: 将Stable Diffusion替换为CoD后，在如0.0039 bpp等超低码率上达到SOTA；训练成本约为20个A100 GPU天，比Stable Diffusion的约6250天快约300倍；像素域扩散能在保持高感知质量的同时达到接近VTM级PSNR，并以更少参数超越GAN类编解码器。

Conclusion: CoD为面向压缩的扩散基础模型提供了可行路径：高压缩效率、低成本可复现，并带来对像素域扩散与传统/对抗式编解码对比的新洞见，为后续扩散编解码研究奠定基础，代码将开源。

Abstract: Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \textbf{CoD}, the first \textbf{Co}mpression-oriented \textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \textbf{Low-cost and reproducible training}, 300$\times$ faster training than Stable Diffusion ($\sim$ 20 vs. $\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.

</details>


### [189] [Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation](https://arxiv.org/abs/2511.18711)
*Yuyang Wanyan,Xiaoshan Yang,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: 提出MC-LRD框架解决少样本视频域适应中多模态对齐与协同难题，通过多级共享的低秩分解器与路由器自适应地拆分模态特有/共享成分，并配合正交去相关与跨域激活一致性损失实现稳健对齐，在三大基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 少样本视频域适应既要跨域对齐又要多模态协同。作者观察到：单模态与多模态融合在域移下均泛化受限，原因在于每个模态由多个耦合成分组成，且各成分的域移强度不一，导致对齐复杂、融合失效。因此需要在模态内拆分“域移程度不同的特征成分”，再分别对齐与协同。

Method: 引入MC-LRD框架：为每个模态设置多组低秩分解器与多模态分解路由器（MDR）。分解器在跨模态间采用逐级共享参数设计，路由器选择性激活分解器以产生模态特有与模态共享特征。为避免冗余、提升可分性，对分解器与子路由器分别施加正交去相关约束；同时提出跨域激活一致性损失，使同类的源/目标样本在分解器激活模式上保持一致，促进域对齐。

Result: 在三个公开基准上进行大量实验，MC-LRD在少样本视频域适应任务上取得显著优于现有SOTA的方法性能提升。

Conclusion: 通过面向模态成分级的低秩分解与可学习路由，并辅以正交约束和跨域激活一致性，能够有效拆解并对齐不同域移程度的特征，从而提升少样本多模态视频域适应的泛化与融合效果。

Abstract: In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.

</details>


### [190] [DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2511.18713)
*Hongbin Lin,Yiming Yang,Chaoda Zheng,Yifan Zhang,Shuaicheng Niu,Zilu Guo,Yafeng Li,Gui Gui,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 提出DriveFlow：一种基于预训练Text-to-Image Rectified Flow模型的训练数据增强方法，通过频率分解分别保护前景高频几何与优化背景，提升自动驾驶视觉3D检测在OOD场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉中心的3D目标检测在自动驾驶中易受OOD分布偏移影响；标注成本高且场景多样导致训练集覆盖不足。希望利用“训练无关”的图像编辑增强数据而不改动预训练扩散/流模型，但现有基于反演的方法效果有限且不准确，近期基于rectified flow的方法又难以保留精确3D几何。

Method: 提出DriveFlow，对预训练的Text-to-Image rectified flow进行适配：利用频率分解构造无噪编辑路径，并设计两项策略：1) 前景高频保持：对前景施加高频对齐损失，保留物体精确3D几何细节；2) 背景双频优化：同时在高/低频进行优化，兼顾编辑灵活性与语义一致性。

Result: 在多个OOD场景和所有类别上均获得全面性能提升，验证了方法的有效性与效率；提供开源代码。

Conclusion: 通过频率分解下的前景高频保持与背景双频优化，DriveFlow能在不训练预训练模型的前提下进行高保真数据增强，从而提升自动驾驶3D检测在OOD下的鲁棒性。

Abstract: In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.

</details>


### [191] [Seeing What Matters: Visual Preference Policy Optimization for Visual Generation](https://arxiv.org/abs/2511.18719)
*Ziqi Ni,Yuanzhi Liang,Rui Li,Yi Zhou,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出ViPO，将GRPO从单一标量奖励扩展为像素级优势图，强化对视觉生成中局部与时空细节的对齐，显著优于原始GRPO并具备通用、轻量、可无缝集成特点。


<details>
  <summary>Details</summary>
Motivation: 现有用于视觉生成模型偏好对齐的GRPO仅依赖每个样本的单一标量奖励，忽视图像/视频的空间与时间结构，导致难以修正局部伪影与刻画细粒度感知线索。需要一种能把人类偏好信号细化到局部、并在不牺牲稳定性的前提下提升学习效率的方法。

Method: 提出Visual Preference Policy Optimization（ViPO）：在GRPO框架内构造“感知结构化模块”，利用预训练视觉骨干提取空间与时序特征，将标量反馈提升为像素（或时空体素）级优势图；据此在优化中对感知重要区域施加更大更新压力，同时保持GRPO的稳定性与兼容性。方法与体系结构无关、轻量，直接插入现有GRPO流水线。

Result: 在图像与视频基准上，ViPO相较原始GRPO在域内的人类偏好奖励显著提升，并在跨域评测中展现更强泛化；在不改变底层架构的情况下获得更好的局部伪影修正与细节建模能力。

Conclusion: 将标量偏好反馈结构化为像素级优势能提供更表达性的学习信号，改善视觉生成的局部与时空对齐；ViPO稳定、通用且易集成，为基于偏好的视觉生成强化学习提供更有效的训练范式。

Abstract: Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.

</details>


### [192] [GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.18729)
*Lin Liu,Caiyan Jia,Guanyi Yu,Ziying Song,JunQiao Li,Feiyang Jia,Peiliang Wu,Xiaoshuai Hao,Yandan Luo*

Main category: cs.CV

TL;DR: GuideFlow 是一种用于端到端自动驾驶规划的受约束流匹配框架，通过在生成过程中直接注入物理与安全约束，并联合能量基模型训练，缓解多模态轨迹塌缩、提升可控性与可行性，并在多基准上取得SOTA（如 NavSim Navhard EPDMS 43.0）。


<details>
  <summary>Details</summary>
Motivation: 模仿式 E2E 规划存在多模态轨迹模式塌缩，难以给出多样且可行的轨迹；生成式 E2E 规划又难以在生成阶段直接融入安全与物理约束，通常需后处理优化，带来效率与稳定性问题。需要一种在生成过程中就能显式施加约束、同时保持多样性与可控性的统一方法。

Method: 提出 GuideFlow：基于 Constrained Flow Matching 的规划框架。1) 显式建模流匹配过程，天然缓解模式塌缩，并允许多种条件信号引导；2) 在流匹配生成中直接施加显式约束（而非隐式编码），并与能量基模型（EBM）联合训练，以增强对物理/安全约束的自优化能力；3) 将驾驶“激进度”参数化为控制信号，实现轨迹风格可控。

Result: 在 Bench2Drive、NuScenes、NavSim、ADV-NuScenes 上进行广泛评测，证明有效；在 NavSim 的 test hard（Navhard）上取得 SOTA，EPDMS=43.0。

Conclusion: GuideFlow 通过受约束流匹配与 EBM 联训，在生成阶段即可满足显式约束，兼顾多样性、可行性与风格控制，并在多项基准上达到或刷新 SOTA。

Abstract: Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \textit{\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \textit{\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \textit{\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \textit{\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \textit{\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \textit{\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.

</details>


### [193] [Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion](https://arxiv.org/abs/2511.18734)
*Keyang Lu,Sifan Zhou,Hongbin Xu,Gang Xu,Zhifei Yang,Yikai Wang,Zhen Xiao,Jieyi Long,Ming Li*

Main category: cs.CV

TL;DR: Yo'City提出一种基于大模型推理与组合能力的代理式框架，实现可定制且可无限扩展的3D城市生成。框架先做“城市-分区-网格”分层规划，再以“生成-细化-评估”的等距图像闭环生成网格级图像并转3D，并通过关系引导的交互式扩展机制保持全局语义与空间一致性。构建基准与六维指标，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多依赖单一扩散模型，难以个性化与城市级无界扩展，且全局规划、语义一致性与演化式扩展不足；需要一个能规划、组合、多阶段生成与交互扩展的系统。

Method: 1) 规划层：自顶向下“City-District-Grid”结构；Global Planner确定总体布局与功能分区；Local Designer细化到网格级文本描述。2) 生成层：对每个网格采用“生成-细化-评估”闭环等距图像合成，再做图像到3D。3) 扩展层：用户交互+关系引导的扩展机制，基于场景图进行距离与语义感知的布局优化，保证增长过程中的空间与语义连贯。4) 评测：构建多样基准与六维指标，从语义、几何、纹理、布局等维度评估。

Result: 在构建的基准上，Yo'City在六个维度的指标上全面优于现有SOTA，生成质量、语义一致性、几何与纹理细节以及大规模布局连贯性均更佳。

Conclusion: 通过代理式分层规划与闭环网格生成加关系引导的演化扩展，Yo'City实现了可个性化、可无限扩展且空间连贯的3D城市生成，并在全面评测中显著优于现有方法。

Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.

</details>


### [194] [Thinking Ahead: Foresight Intelligence in MLLMs and World Models](https://arxiv.org/abs/2511.18735)
*Zhantao Gong,Liaoyuan Fan,Qing Guo,Xun Xu,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 提出“前瞻智能”（Foresight Intelligence）概念，并发布用于评估该能力的VQA数据集FSU-QA；实验表明现有VLM在面向未来推理上表现不足，而在FSU-QA上微调的小模型可超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言研究多聚焦于当前或已发生事实的识别与理解，缺乏对“将会发生什么”的推理与解释能力的系统评测与训练，而这对自动驾驶等需要预测的场景至关重要。

Method: 1) 定义“前瞻智能”能力；2) 构建FSU-QA数据集，通过面向未来的问答任务诱导并评估该能力；3) 用FSU-QA系统评测多种SOTA VLM；4) 将世界模型的生成预测作为外部信息增强VLM，并以语义一致性带来的性能提升量化世界模型质量；5) 在FSU-QA上对小型VLM进行微调并与更大型模型比较。

Result: (a) 当前VLM在前瞻推理任务上显著受限；(b) 通过融合世界模型预测可带来可观性能提升，提升幅度可作为世界模型语义一致性的度量；(c) 在FSU-QA上微调的小模型显著超越更大、更先进的未调模型。

Conclusion: FSU-QA为研究与评测前瞻智能提供了首个系统化基准；结合世界模型与有针对性的微调能显著提升对未来事件的推理能力，为面向真实应用的下一代模型奠定基础。

Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.

</details>


### [195] [ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion](https://arxiv.org/abs/2511.18742)
*Zhenghan Fang,Jian Zheng,Qiaozi Gao,Xiaofeng Gao,Jeremias Sulam*

Main category: cs.CV

TL;DR: 提出ProxT2I：用“反向离散化+条件近端算子”替代传统得分函数的T2I扩散采样器，并结合强化学习优化采样策略；同时发布1500万高质量人像-文本数据集LAION-Face-T2I-15M。性能与SOTA相当但更轻量、更高采样效率、更对齐人类偏好。


<details>
  <summary>Details</summary>
Motivation: 传统基于得分函数的扩散采样常依赖对反向过程的正向显式离散化，步数多、易不稳定、代价高；同时缺少面向任务/偏好优化的采样策略与大规模细粒度人像数据。作者希望在不牺牲质量的前提下降低计算和模型规模、提升采样效率与偏好对齐。

Method: 1) 将采样过程重构为“反向离散化”框架，引入学习到的、条件化的近端算子（proximal operators）替代得分函数，实现隐式更新；2) 利用强化学习/策略优化，对采样器按任务相关奖励进行端到端调优，提升人类偏好对齐；3) 构建并开源LAION-Face-T2I-15M（1500万人像、细粒度文本描述）用于训练与评测。

Result: 在多项评测中相较得分基线，ProxT2I采样步数更少、稳定性更好、与人类偏好一致性更高；在整体质量上与现有SOTA与开源T2I模型相当，同时所需计算与模型规模更低。

Conclusion: 以近端算子与反向离散化为核心的ProxT2I能在更低算力和更小模型下达到接近SOTA的T2I效果，并通过RL优化实现对任务/偏好的高效对齐；新的人像数据集为相关研究提供了开放基准。

Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.

</details>


### [196] [Any4D: Open-Prompt 4D Generation from Natural Language and Images](https://arxiv.org/abs/2511.18746)
*Hao Li,Qiao Sun*

Main category: cs.CV

TL;DR: 提出Primitive Embodied World Models（PEWM），将视频生成限制在固定短时程原语层面，通过VLM规划与起止热力图引导，实现细粒度语义-动作对齐、降低学习复杂度、提升数据效率与推理速度，并支持可组合的长任务控制。


<details>
  <summary>Details</summary>
Motivation: 当前基于视频生成的具身世界模型受限于大规模交互数据的稀缺与高维性，导致语言-动作对齐粗糙、长时程视频生成困难，阻碍具身领域出现类似“GPT时刻”。作者观察到具身数据的表观多样性远大于可行原始运动的空间，提示可在原语层面建模以缓解数据与对齐问题。

Method: 提出PEWM：1）将视频生成限制为固定短时程的运动原语，进行细粒度语言-视觉动作对齐；2）采用模块化VLM规划器分解高层指令为原语序列；3）引入Start-Goal热力图引导（SGG）以进行闭环控制与目标约束；4）利用视频模型的时空先验与VLM的语义能力实现原语级策略的可组合泛化。

Result: PEWM在数据效率、学习稳定性与推理延迟上优于长时程端到端视频世界模型，并在复杂任务上通过原语组合实现更强的闭环控制与泛化（摘要未给出具体数值）。

Conclusion: 以原语为单位的短时程视频世界模型能在资源受限场景下实现更细粒度的语言-动作对齐、提升数据与推理效率，并通过VLM规划与SGG实现灵活闭环与组合泛化，为可扩展、可解释的通用具身智能奠定基础。

Abstract: While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \textit{"GPT moment"} in the embodied domain. There is a naive observation: \textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \textit{2) reduces} learning complexity, \textit{3) improves} data efficiency in embodied data collection, and \textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.

</details>


### [197] [From Features to Reference Points: Lightweight and Adaptive Fusion for Cooperative Autonomous Driving](https://arxiv.org/abs/2511.18757)
*Yongqi Zhu,Morui Zhu,Qi Chen,Deyuan Qu,Song Fu,Qing Yang*

Main category: cs.CV

TL;DR: RefPtsFusion提出以“参考点”而非特征/查询为核心的协同感知框架，通过交换目标的位姿、速度与尺度等紧凑信息，大幅降低带宽并提升跨车型/跨模型兼容性，同时引入选择性Top-K高置信查询补充信息，在M3CAD上以极低通信量保持稳定感知性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有车车协同多采用特征级/查询级共享，带宽消耗巨大、跨传感器与异构模型适配差，且实时性受限。需要一种既低通信开销又能在异构系统间稳定工作的协同感知接口。

Method: 1) 参考点驱动的协同：车辆仅共享目标参考点（位置、速度、尺寸等），把注意力从“看到了什么”转向“应该看哪里”；形成与传感器/模型解耦的接口。2) 选择性Top-K查询融合：从发送端挑选高置信查询补充共享信息，在准确率与带宽间取得平衡。3) 在M3CAD上进行系统评测与鲁棒性、传输一致性实验。

Result: 在M3CAD上，相比特征级融合，通信从数百MB/s降至每秒数KB（5 FPS），降低约五个数量级，同时保持稳定的感知性能；在多项实验中表现出较强鲁棒性与一致的传输行为。

Conclusion: RefPtsFusion以参考点为核心的轻量化、可解释协同方案，实现跨模型/传感器兼容与极低带宽下的稳健感知；选择性Top-K策略进一步在性能与通信成本间取得优良折中，具备面向可扩展、实时车路协同系统的应用潜力。

Abstract: We present RefPtsFusion, a lightweight and interpretable framework for cooperative autonomous driving. Instead of sharing large feature maps or query embeddings, vehicles exchange compact reference points, e.g., objects' positions, velocities, and size information. This approach shifts the focus from "what is seen" to "where to see", creating a sensor- and model-independent interface that works well across vehicles with heterogeneous perception models while greatly reducing communication bandwidth. To enhance the richness of shared information, we further develop a selective Top-K query fusion that selectively adds high-confidence queries from the sender. It thus achieves a strong balance between accuracy and communication cost. Experiments on the M3CAD dataset show that RefPtsFusion maintains stable perception performance while reducing communication overhead by five orders of magnitude, dropping from hundreds of MB/s to only a few KB/s at 5 FPS (frame per second), compared to traditional feature-level fusion methods. Extensive experiments also demonstrate RefPtsFusion's strong robustness and consistent transmission behavior, highlighting its potential for scalable, real-time cooperative driving systems.

</details>


### [198] [VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement](https://arxiv.org/abs/2511.18763)
*Xuanzhao Dong,Wenhui Zhu,Yujian Xiong,Xiwen Chen,Hao Wang,Xin Li,Jiajun Cheng,Zhipeng Wang,Shao Tang,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: 提出VAOT用于无配对视网膜彩照增强，以保持血管结构并减少噪声，优于多种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: CFP获取受光照等变化影响导致质量下降。现有无配对GAN增强常破坏关键血管结构（拓扑、端点），影响临床与下游分割任务，亟需既能增强又保结构的方案。

Method: 构建Vessel-Aware Optimal Transport (VAOT) 框架：以最优传输目标对齐退化域与高质量域分布，并加入两类结构保持正则——(i) 基于血管骨架的损失维持全局连通性；(ii) 基于端点的损失稳定局部末端。无配对学习中以这些约束引导网络，抑制噪声同时避免血管拓扑扭曲。

Result: 在合成退化基准上，VAOT在保持血管拓扑与端点完整性的指标上优于多种SOTA无配对增强方法；在后续血管与病灶分割任务中亦显著提升性能，显示增强后的图像更利于临床相关分析。

Conclusion: VAOT通过OT目标与结构感知正则实现无配对增强的结构保真，兼顾去噪与血管完整性，较现有方法更稳健且对下游任务更友好；代码已开源。

Abstract: Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT

</details>


### [199] [NI-Tex: Non-isometric Image-based Garment Texture Generation](https://arxiv.org/abs/2511.18765)
*Hui Shan,Ming Li,Haitao Yang,Kai Zheng,Sizhe Zheng,Yanwei Fu,Xiangru Huang*

Main category: cs.CV

TL;DR: 提出一种无需严格拓扑一致性的服装PBR材质生成流程：构建“3D Garment Videos”数据集+采用Nano Banana进行非等距图像编辑+基于不确定性的多视角烘焙融合，最终生成可生产级的无缝PBR纹理。


<details>
  <summary>Details</summary>
Motivation: 工业界已有大量高质量服装网格，但缺乏多样、真实的PBR纹理。现有方法依赖图像与网格的拓扑/姿态严格对齐或精准变形，限制了纹理生成的质量与灵活性。需要一种能跨姿态、跨拓扑稳健生成纹理的方案。

Method: 1) 数据集：构建物理仿真的“3D Garment Videos”，在多样形变下提供一致的几何与材质监督，支持跨姿态学习。2) 非等距图像编辑：使用Nano Banana进行高质量非等距编辑，实现图像与几何之间的跨拓扑纹理迁移。3) 模型：前馈双分支架构，进行图像条件的PBR材质预测。4) 迭代烘焙：基于不确定性引导的视角选择与重加权，将多视图预测融合为无缝、生产可用的PBR纹理。

Result: 在广泛实验中，方法在非等距、跨姿态/跨拓扑场景下生成空间对齐、细节丰富的PBR材质；相较依赖拓扑一致或精确变形的基线，质量与灵活性显著提升，输出可直接用于工业级3D服装设计流程。

Conclusion: 通过数据、编辑工具与不确定性引导烘焙的组合，消除了图像—几何拓扑对齐的刚性要求，实现稳健的跨姿态/跨拓扑PBR纹理生成，并产出可投入生产的无缝材质。

Abstract: Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.

</details>


### [200] [Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment](https://arxiv.org/abs/2511.18766)
*Xintao Chen,Xiaohao Xu,Bozhong Zheng,Yun Liu,Yingna Wu*

Main category: cs.CV

TL;DR: 提出VSAD，通过几何一致性建模实现多视角无监督异常检测，利用MVAM同态映射对齐特征、VALDM在扩散去噪过程中多阶段对齐、FRM加强全局一致性，并与正常原型记忆库比对，SOTA性能提升并鲁棒于大视角变化与复杂纹理。


<details>
  <summary>Details</summary>
Motivation: 多视角图像中视角变化引入的外观差异会被误判为缺陷；现有方法多按单视角处理，导致跨视角特征不一致和高误报，缺乏显式几何建模与一致对齐。

Method: 提出ViewSense-AD框架：1) 多视对齐模块MVAM基于单应性将邻近视角对应区域投影与对齐；2) 将MVAM嵌入视对齐潜变量扩散模型VALDM，在去噪过程中由粗到细进行渐进、多阶段对齐，构建目标表面的整体表征；3) 轻量融合精炼模块FRM提升全局一致性、抑制噪声并增强判别力；4) 以扩散模型多层特征与正常原型记忆库匹配进行异常检测。

Result: 在RealIAD与MANTA数据集上实现新的SOTA，在像素级、视角级与样本级均显著优于现有方法，展现对大视角偏移与复杂纹理的强鲁棒性。

Conclusion: 显式建模跨视角几何一致性并在扩散框架中进行渐进对齐，可显著降低多视无监督异常检测的误报并提升检测精度，适用于视角变化大、纹理复杂的场景。

Abstract: Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.

</details>


### [201] [Rethinking Garment Conditioning in Diffusion-based Virtual Try-On](https://arxiv.org/abs/2511.18775)
*Kihyun Na,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 提出Re-CatVTON：在单UNet框架下通过重新设计条件特征学习与指导策略，达到接近或优于双UNet VTON性能，但显著更省算力与显存。


<details>
  <summary>Details</summary>
Motivation: 现有双UNet扩散式VTON（如Leffa）保真度高，但结构臃肿、计算与显存开销大；单UNet方法（如CatVTON）效率高但性能落后。作者希望理解并改进在扩散去噪中“上下文条件特征”的学习与利用方式，寻找单UNet的效率-性能新平衡。

Method: 1) 通过可视化与理论分析提出三条关于条件上下文特征如何影响扩散去噪的假设；2) 基于此设计Re-CatVTON：在单UNet中重构条件特征的拼接与利用；3) 引入针对空间拼接条件的改进版classifier-free guidance（CFG），使指导更契合VTON的空间依赖；4) 直接注入由干净服装潜变量得到的“真值服装潜变量”，减少预测误差累积；总体实现轻量单UNet、保持高保真。

Result: 在FID、KID、LPIPS上优于对比方法，较CatVTON大幅提升；与高性能双UNet（Leffa）相比，计算与显存需求显著降低，同时SSIM仅小幅下降。

Conclusion: 通过重新建模条件特征与定制化CFG，并用真值服装潜变量抑制误差传播，单UNet的VTON可在效率与性能间取得新的最优折衷，为后续高效VTON提供可行路径。

Abstract: Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.

</details>


### [202] [ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780)
*Ruize Ma,Minghong Cai,Yilei Jiang,Jiaming Han,Yi Feng,Yingshui Tan,Xiaoyong Zhu,Bo Zhang,Bo Zheng,Xiangyu Yue*

Main category: cs.CV

TL;DR: ConceptGuard提出一个统一的多模态视频生成安全框架，通过对图像-文本联合输入进行风险概念检测并在生成前介入抑制不安全语义，在两项新基准上实现领先的风险检测与安全生成表现。


<details>
  <summary>Details</summary>
Motivation: 多模态（文本+图像）到视频的生成更可控但带来新的安全风险：风险可来自任一模态或其组合，现有方法多为仅文本、需预知风险类别或事后审计，难以前置与组合性风险防控。

Method: 两阶段框架：1) 对比式检测模块，将融合后的图像-文本输入投射到结构化“概念空间”，识别潜在不安全概念；2) 语义抑制机制，在生成过程中对多模态条件进行干预，引导模型远离不安全概念。并构建两个基准：ConceptRisk（大规模多模态风险训练数据集）与T2VSafetyBench-TI2V（首个适配文本+图像到视频安全场景的评测）。

Result: 在两个基准上，ConceptGuard在风险检测与安全视频生成方面均优于现有基线，达成SOTA性能。

Conclusion: 面向多模态视频生成的组合性安全问题，ConceptGuard可前瞻性检测与缓解不安全语义，较现有方法更有效与统一，并有数据与基准支撑其实用性。

Abstract: Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.

</details>


### [203] [A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data](https://arxiv.org/abs/2511.18781)
*Haotian Yan,Bocheng Guo,Jianzhong He,Nir A. Sochen,Ofer Pasternak,Lauren J O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: 提出一种结合dMRI与fMRI的双流网络来进行纤维流线分类，显著提升对功能上可区分但几何上相似的纤维束（示例：CST四个体部位亚区）的分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有流线分类主要依赖几何特征，难以区分在路径上相似但功能不同的白质纤维束，导致分割的功能一致性不足。需要引入功能信息来提升分割的神经功能意义。

Method: 构建双流（dual-stream）框架：主干为预训练的全流线路径分类模型，辅以处理纤维端点区域fMRI信号的辅助网络；两路特征联合用于流线的类别判定。以皮质脊髓束（CST）为例，将其分割为四个体部位（体表映射）亚区。包含消融实验与SOTA对比。

Result: 在CST四分区任务上，该方法在多项指标上优于当前最先进方法；消融显示加入fMRI辅助分支显著提升功能一致性与总体精度。

Conclusion: 几何+功能的联合建模能有效提升流线分类的功能特异性与准确性；所提双流框架为功能一致的白质束划分提供了有效范式，可推广至其他束群与任务。

Abstract: Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.

</details>


### [204] [STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution](https://arxiv.org/abs/2511.18786)
*Junyang Chen,Jiangxin Dong,Long Sun,Yixin Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: STCDiT 是基于预训练视频扩散模型的视频超分框架，通过运动感知的分段重建与锚帧引导，兼顾时间稳定性与结构保真，在复杂相机运动下实现高质量复原。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分在复杂相机运动下易产生时序闪烁与结构失真；扩散模型虽强大但难以同时保证时序稳定与结构保真。需要一种既能适配复杂运动，又能在生成阶段保持清晰结构的方案。

Method: 1) 运动感知VAE重建：将视频按运动一致性划分为片段，分别重建以应对复杂相机运动；2) 锚帧引导：利用每个片段首帧（未受时间压缩、结构信息更丰富）的潜变量作为锚，约束后续帧的生成以增强结构保真；二者结合到预训练视频扩散模型实现超分。

Result: 在多组实验与基准上，方法在结构保真度与时间一致性上优于现有SOTA。

Conclusion: 通过分段重建与锚帧引导，STCDiT 有效提升视频超分在复杂运动场景下的时序稳定与结构保真，取得SOTA表现。

Abstract: We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.

</details>


### [205] [Understanding Task Transfer in Vision-Language Models](https://arxiv.org/abs/2511.18787)
*Bhuvan Sachdeva,Karan Uppal,Abhinav Java,Vineeth N. Balasubramanian*

Main category: cs.CV

TL;DR: 研究系统分析在视觉语言模型（VLM）上进行单一感知任务微调对其他感知任务零样本表现的影响，并提出度量迁移广度与幅度的新指标PGF，构建任务迁移图揭示正/负迁移结构与“任务人格”，为数据选择和高效训练提供指导。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态基准上表现强，但在深度估计、计数等纯视觉感知任务上落后于人类和专用模型；对单一任务微调会不可预测地改变其他任务表现，给任务特定微调带来风险与成本。因此需要系统化地度量与理解任务间的可迁移性，以避免负迁移、利用正迁移。

Method: 选择三个开源权重VLM，覆盖13个视觉感知任务；对每个任务进行微调并在其他任务上做零样本评估，构造完整的任务×任务影响矩阵；提出Perfection Gap Factor（PGF）作为统一指标，综合刻画“距离完美”的改变量（迁移幅度）与影响任务数量（迁移广度）；据此构建任务迁移图，进行聚类与图分析，定义和识别表现出相似迁移模式的“任务人格”。

Result: 发现稳定的正/负迁移模式与互相影响的任务群；在不同VLM上复现出一致的结构性关系；用PGF可识别能带来广泛正迁移或强负干扰的关键任务，并据此指导数据选择以在更少训练预算下获得更好的综合性能。

Conclusion: PGF与任务迁移图为VLM感知任务的微调策略提供可操作的参考：可利用正迁移的“源任务”，规避冲突较大的“互斥任务”，并依据“任务人格”进行分组或阶段式训练，从而在不牺牲泛化的前提下更高效地提升VLM感知能力。

Abstract: Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.

</details>


### [206] [StereoDETR: Stereo-based Transformer for 3D Object Detection](https://arxiv.org/abs/2511.18788)
*Shiyi Mu,Zichong Gu,Zhiqi Ai,Anqi Liu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: StereoDETR是一种高效的基于DETR的双目3D检测框架，通过单目DETR分支与低成本视差特征的立体分支协同，实现实时推理，在速度上首次超过单目方法，同时在KITTI上精度具有竞争力，并在行人/骑行者上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有双目3D检测虽精度高于单目，但计算开销与延迟大，最先进方法虽精度翻倍却推理速度减半，亟需在保持/提升精度的同时显著提升速度与效率。

Method: 构建两分支架构：1) 单目DETR分支，基于2D DETR扩展通道以预测尺度、朝向与采样点；2) 立体分支，利用多尺度、低成本视差特征预测目标级深度图。两分支仅通过可微的深度采样策略耦合。为应对遮挡，引入受约束的采样点监督，无需额外标注。

Result: 实现实时推理，成为首个在速度上超过单目方法的双目方案；在KITTI上达到具有竞争力的精度，并在行人与骑行者子集上刷新SOTA。

Conclusion: 通过轻量级立体分支与DETR式单目分支的松耦合，以及可微深度采样与受约束监督，StereoDETR在维持/提升精度的同时显著降低推理延迟，推动双目3D检测在高效实时场景中的实用化。

Abstract: Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.

</details>


### [207] [Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing](https://arxiv.org/abs/2511.18792)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Chun Tung Chou,Wen Hu*

Main category: cs.CV

TL;DR: 论文提出将MAE式预训练应用于大规模、异构Wi‑Fi CSI数据以应对跨域泛化不佳的问题，系统性评估“数据多样性与规模”与“模型容量”的作用，发现数据规模主导跨域性能提升，模型做大收益有限，跨域下三类任务准确率提升2.2%–15.7%。


<details>
  <summary>Details</summary>
Motivation: 现有Wi‑Fi感知模型在新环境/设备/用户上泛化差，受制于小而碎片化的数据集，导致“域移”问题严重，难以真实落地。需要一种能在多域稳健泛化的训练范式。

Method: 采用基础模型思路：收集覆盖14个数据集、4种设备、2.4/5/6GHz与20–160MHz带宽、超130万样本的最大规模Wi‑Fi CSI集合；以Masked Autoencoding进行自监督预训练，再在多任务上微调；设计大规模实验，分离考察数据多样性/规模与模型容量对跨域性能的影响。

Result: 1) 预训练数据增多带来未见域性能按对数-线性规律提升，数据规模与多样性是关键；2) 在当前数据量下，增大模型容量仅带来微弱跨域增益，瓶颈在数据而非模型；3) 在人类活动识别、手势识别、用户识别等跨域评测中，相比监督学习基线，预训练带来2.2%–15.7%的准确率提升。

Conclusion: Wi‑Fi感知跨域泛化的核心在于数据规模与多样性，当前阶段优先扩充与丰富预训练数据优于一味做大模型。大规模MAE式预训练为面向真实部署的稳健Wi‑Fi感知系统指明方向。

Abstract: While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.

</details>


### [208] [PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion](https://arxiv.org/abs/2511.18801)
*Yichen Yang,Hong Li,Haodong Zhu,Linin Yang,Guojun Lei,Sheng Xu,Baochang Zhang*

Main category: cs.CV

TL;DR: 提出PartDiffuser：一种面向点云到网格生成的半自回归扩散框架，分“部件间自回归、部件内并行离散扩散”，兼顾全局拓扑一致与局部高频细节，基于DiT并引入部件感知跨注意力，以点云分层作为条件。实验显著优于SOTA，生成细节丰富、可用于实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有自回归网格生成在全局结构与局部细节间难以兼顾，且易累积误差；需要一种既能保持全局拓扑一致性又能精细重构局部几何细节、并降低误差扩散的生成方法。

Method: 1) 先对目标网格进行语义分割得到部件。2) 跨部件采用自回归生成，确保全局结构与拓扑关系。3) 部件内采用并行的离散扩散过程重建高频几何。4) 基于DiT架构，引入“部件感知”跨注意力；以点云作为分层几何条件动态调控生成，从而解耦全局/局部任务。

Result: 在点云到网格生成上显著优于SOTA，生成的3D网格在细节保真度与整体一致性方面均有明显提升，细节表现尤为突出。

Conclusion: 半自回归+部件内并行扩散的PartDiffuser有效缓解误差累积并同时优化全局拓扑与局部细节；其部件感知注意力与点云条件使得方法在真实应用中具备更强的细节表达与稳定性。

Abstract: Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a "part-wise" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.

</details>


### [209] [TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging](https://arxiv.org/abs/2511.18806)
*Qinglei Cao,Ziyao Tang,Xiaoqin Tang*

Main category: cs.CV

TL;DR: 提出一种利用“目标先验”(target prior)增强隐式学习的3D稀视角CT重建框架，通过位置与结构编码进行体素级隐式重建，并以CUDA快速从稀疏投影估计高质量3D先验；在学习效率上较NAF提升10倍，重建质量较NeRP在10/20/30视角下分别提升3.57/5.42/5.70 dB。


<details>
  <summary>Details</summary>
Motivation: 现有将NeRF及其变体用于CT内部结构重建的方法在超稀视角下重建精度与训练效率受限，主要因为忽略了解剖结构等目标先验信息对隐式表示学习的重要性。

Method: 1) 从对象稀疏投影数据中快速估计三维“目标先验”，并以CUDA实现高效推断；2) 结合位置编码与结构编码，进行体素级隐式重建；3) 以目标先验引导体素采样并丰富结构编码，从而在训练过程中重点关注解剖相关区域与边界结构。

Result: 在复杂腹部数据上：训练效率较当前SOTA NAF快约10倍；重建质量优于最准确的NeRP，在10/20/30个投影视角下PSNR分别提升3.57/5.42/5.70 dB。

Conclusion: 利用从稀疏投影中估计的三维目标先验，结合位置与结构编码，可显著提升超稀视角CT隐式重建的效率与质量；所提框架为稀视角CT提供了有效先验注入路径，并具备实用的加速实现。

Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.

</details>


### [210] [Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache](https://arxiv.org/abs/2511.18811)
*Yuqiu Jiang,Xiaozhen Qiao,Tianyu Mei,Haojian Huang,Yifan Chen,Ye Zheng,Zhe Sun*

Main category: cs.CV

TL;DR: 提出一种无需训练的自适应多样性缓存（ADC）模块，为HOI检测在推理阶段收集并利用类别特定的高置信、多样化特征，通过频次感知的缓存调节缓解长尾偏置，在HICO-DET与V-COCO上显著提升，尤其对稀有类别mAP提升可达+8.57%。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的HOI检测虽有进展，但依赖额外训练/提示微调，计算开销大、可扩展性差；尤其在长尾分布下稀有交互样本匮乏，模型偏向头部类别，导致性能受限。

Method: 提出训练免（plug-and-play）的Adaptive Diversity Cache（ADC）：在推理过程中为每个类别构建缓存，动态累积高置信且多样化的跨模态特征表示；引入频率感知的缓存自适应策略，对低频（稀有）类别给予更高权重或更积极的缓存更新与检索，从而在无需再训练的情况下对预测进行校准。

Result: 在HICO-DET与V-COCO数据集上，ADC持续提升多种现有HOI检测器的性能：稀有类别mAP最高+8.57%，全量mAP最高+4.39%，同时保持总体性能不受损甚至提升。

Conclusion: ADC作为即插即用、训练免的模块，可在推理期通过类别特定与频率感知的特征缓存有效缓解长尾偏置，提升HOI检测对稀有交互的识别能力，并具备良好的通用性与可扩展性。

Abstract: Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\% mAP gain on rare categories and +4.39\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.

</details>


### [211] [DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video](https://arxiv.org/abs/2511.18814)
*Jiawei Hou,Shenghao Zhang,Can Wang,Zheng Gu,Yonggen Ling,Taiping Zeng,Xiangyang Xue,Jingbo Zhang*

Main category: cs.CV

TL;DR: 提出DA4D大规模4D检测数据集与DetAny4D端到端开放集4D检测框架，融合多模态与时空几何建模，显著提升视频中3D目标检测的稳定性与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有开放集4D（视频中的3D）目标检测要么逐帧推理缺乏时序一致性，要么采用多阶段流水线导致误差传递；同时缺乏带连续高质量3D框框的大规模数据集，限制了方法进展。

Method: 1) 构建DA4D：涵盖多样场景、>28万序列、连续高质量3D框标注；2) 提出DetAny4D：端到端开放集框架，直接从序列输入预测3D框，融合来自预训练基础模型的多模态特征；3) 设计几何感知的时空解码器，联合建模空间与时间动态；4) 多任务学习与专门训练策略，确保不同序列长度下的全局一致性。

Result: 在广泛实验中，DetAny4D在检测精度上具备竞争力，并在时间稳定性方面显著优于现有方法，减少抖动与不一致问题。

Conclusion: 大规模DA4D数据集与DetAny4D方法共同推动开放集4D检测：以端到端和几何时空建模提升视频3D检测的稳定性与一致性；代码与数据将于论文接收后发布。

Abstract: Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.

</details>


### [212] [SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2511.18816)
*Nimeshika Udayangani,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

TL;DR: 提出SupLID：在语义分割中用LID引导分类器置信度的像素级OOD检测，基于超像素与几何coreset实现高效、平滑、可后验集成，刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 像素级OOD检测比图像级更贴近自动驾驶等应用，但将图像级基于分类器置信度（能量/熵）的方法直接迁移到像素层面会过度自信、易误判，且LID虽能刻画高维局部结构，却难以直接用于像素级。需要一种兼顾几何结构、效率与与现有分类器兼容的方案。

Method: 提出SupLID：1) 从ID数据的语义特征空间中构建几何coreset，捕获ID子空间的内在结构；2) 在超像素层级计算线性内在维度（LID）并得到几何OOD分数，实现空间一致性与实时推理；3) 将几何LID信号与传统分类器置信度（能量/熵等）进行引导/融合；4) 以后验打分方式对任意现有语义分割模型无缝集成。

Result: 在多个基准上，作为对分类器分数的补充信号，SupLID显著提升AUR、FPR、AUP等关键指标，达成SOTA；推理高效并带来更平滑的空间预测。代码已开源。

Conclusion: 几何结构（LID）与分类器置信度的互补可有效缓解过度自信问题，提高像素级OOD检测的鲁棒性与实用性；SupLID通用、可部署、性能优越。

Abstract: Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.

</details>


### [213] [Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring](https://arxiv.org/abs/2511.18817)
*Siyuan Wei,Chunjie Wang,Xiao Liu,Xiaosheng Yan,Zhishan Zhou,Rui Huang*

Main category: cs.CV

TL;DR: 该论文提出一条全自动流水线，将原始3D扫描转换为高质量、无歧义的多任务对话数据（Disc3D），以弥补3D多模态大模型训练数据匮乏的问题。通过规则约束+2D MLLM+LLM协同，流水线在不依赖人工标注的情况下生成覆盖场景/视角/目标级任务的200万+样本，显著提升多项公开基准与自建任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D MLLM落后于2D模型，关键在于缺少大规模高质的3D场景-对话数据。以往依赖昂贵人工标注，且存在两大歧义：1) 视角歧义——空间语言默认未知相机位姿；2) 指代歧义——非排他性描述导致目标与干扰项混淆。作者动机是以自动化、可控、可扩展方式生成无歧义数据，降低成本并提高数据可用性。

Method: 提出四阶段全自动数据生成流水线，结合规则约束、2D MLLM与LLM：1) 元注释采集：收集对象级、帧级、场景级描述；2) 场景图构建与关系校正：捕获并更正近邻对象关系；3) 判别式目标指代：生成排他且紧凑的对象描述，解决指代歧义；4) 多任务数据合成：合成多样对话，覆盖场景/视角/对象描述、视觉定位与五类以对象为中心的QA。并系统修复原始数据缺陷，最终构建Disc3D数据集。

Result: 得到Disc3D数据集：包含2万5千个混合3D场景、超过200万样本，覆盖多任务（场景/视图/对象字幕、视觉指向、五类对象中心QA）。在多个公开基准与Disc3D-QA上训练并评测，均带来一致且显著的性能提升。

Conclusion: 自动化、规则与模型协同的生成管线能有效缓解3D数据稀缺与歧义问题，Disc3D为3D MLLM提供大规模高质训练语料，带来稳定提升；代码、数据与模型将开源，便于复现与扩展。

Abstract: 3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.

</details>


### [214] [DiP: Taming Diffusion Models in Pixel Space](https://arxiv.org/abs/2511.18822)
*Zhennan Chen,Junwei Zhu,Xu Chen,Jiangning Zhang,Xiaobin Hu,Hanzhen Zhao,Chengjie Wang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 提出DiP：在像素空间中分两阶段（全局+局部）扩散生成，兼顾质量与效率，避免VAE的信息损失与非端到端训练；在ImageNet 256上获1.90 FID，推理提速达10×且仅增加0.3%参数。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在质量与计算效率间权衡明显：LDM高效但依赖VAE导致信息损失且非端到端；像素空间模型虽无VAE但在高分辨率下计算代价过高。需要一种既高效又保持像素细节的端到端方案。

Method: 设计DiP框架，将生成解耦为两阶段：1）全局阶段：以DiT在大patch上建模，快速构建全局结构；2）局部阶段：联合训练的轻量Patch Detailer Head，利用上下文特征补全与还原细粒度局部细节。整体不使用VAE，保持像素空间端到端训练。

Result: 在ImageNet 256×256上达到1.90 FID；相较以往方法推理速度最高提升10倍；模型总参数量仅增加约0.3%，在效率与质量上同时提升。

Conclusion: 通过全局-局部协同的像素空间扩散（DiP），在不引入VAE的前提下实现接近LDM的计算效率并保持细节质量，缓解了像素空间模型的高计算开销与LDM信息损失的矛盾，适用于高分辨率图像生成。

Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.

</details>


### [215] [VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models](https://arxiv.org/abs/2511.18823)
*Fufangchen Zhao,Liao Zhang,Daiqi Shi,Yuanjun Gao,Chen Ye,Yang Cai,Jian Gao,Danfeng Yan*

Main category: cs.CV

TL;DR: VideoPerceiver通过“SFT+RL”两阶段训练与成对完整/缺失关键信息视频对比，显著提升VMLLM对细粒度动作与短暂事件的感知与描述能力，超越现有方法并保持通用性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大模型对短片中的瞬时动作或长视频中的稀有瞬态事件感知不足，导致细粒度动作理解与时间精确描述薄弱，需要一种训练范式让模型优先关注与任务相关、时间敏感的视觉线索。

Method: 两阶段框架：1) SFT阶段，先基于字幕提取事件-动作关键词，定位对应关键帧并以邻近帧替换构造“关键信息缺失”的视频；将原始与修改后视频与文本共同编码，并用辅助对比损失在中间视觉表征上对齐关键词，强化对细微运动线索的敏感性。2) RL阶段，同时输入成对视频生成描述，引入相对奖励，使对完整视频的回答优于对退化视频的回答，显式训练模型恢复时间精确的动作细节。另构建包含8万条细粒度动作与瞬态事件的数据集。

Result: 在细粒度动作理解与稀有事件字幕生成基准上显著优于SOTA VMLLM，同时在标准任务上保持强劲表现，显示其对任务相关视觉特征的优先感知能力。

Conclusion: 通过“缺失关键信息”的对比训练与相对奖励的RL，VideoPerceiver有效强化了时间与动作细节感知，提供了面向细粒度视频理解的新型训练范式。

Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.

</details>


### [216] [Assessing the alignment between infants' visual and linguistic experience using multimodal language models](https://arxiv.org/abs/2511.18824)
*Alvin Wei Ming Tan,Jane Yang,Tarun Sepuri,Khai Loong Aw,Robert Z. Sparks,Zi Yin,Virginia A. Marchman,Michael C. Frank,Bria Long*

Main category: cs.CV

TL;DR: 使用CLIP评估婴儿视角日常视频中的“词-视觉”对齐，发现类似“看球—视野有球”的理想学习时刻很少，且个体差异大，提示词汇学习并非主要发生在完美对齐场景。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为幼儿通过环境中词与物体的共现来学会早期词汇，但我们缺乏关于日常生活中语言与视觉经验在时间上究竟有多对齐的量化证据；人工标注代价高昂，限制了大规模研究。

Method: 用CLIP这种对比语言-图像预训练模型对婴儿家庭环境中的头戴式（自我中心）视频进行自动对齐评分；先用人类判断验证CLIP对齐分数的有效性，再将其应用于大规模婴儿视角视频语料以量化“词-物”对齐。

Result: 验证表明CLIP的对齐分数与人类对齐判断一致；在大规模数据上，理想化的学习时刻（例如有人说“看球”，婴儿视野中确有球）在自然环境中相对稀少，远低于现代机器学习数据集的对齐程度；对齐程度在同一儿童内部随时间波动，且儿童之间差异显著。

Conclusion: 幼儿日常经验中语言与视觉的不频繁对齐对基于简单共现的词汇学习模型构成约束；该方法为研究儿童多模态环境提供了可扩展的新工具，并提示需要考虑稀疏与变动的对齐信号来解释早期词汇习得。

Abstract: Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., "look at the ball" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.

</details>


### [217] [Q-Save: Towards Scoring and Attribution for Generated Video Evaluation](https://arxiv.org/abs/2511.18825)
*Xiele Wu,Zicheng Zhang,Mingtao Chen,Yixian Liu,Yiming Liu,Shushi Wang,Zhichao Hu,Yuhong Liu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: Q-Save 提出一个面向AIGV（AI生成视频）质量评估的全新数据集与统一评估模型，既给出分数也给出可解释的归因理由，并在精度与效率上达成平衡，达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有AIGV评估缺乏同时具有整体质量评分与可解释归因的标准；不同维度（视觉、动态、文图一致性）难以统一衡量且评估模型常缺少效率与稳定性。

Method: 1) 数据：近万条视频，带MOS与三维细粒度归因标注（视觉质量、动态质量、文-视频对齐）。2) 模型：基于SlowFast结构，慢分支高分辨率、快分支低分辨率，实现精度-效率权衡；联合学习质量打分与归因解释。3) 训练：COT格式数据，三阶段策略——先SFT，再GRPO强化归因与评分，最后SFT稳态化。

Result: 在视频质量预测上达到SOTA，并能输出与人类一致的可解释理由；在计算效率与评测稳定性上也表现良好。

Conclusion: Q-Save建立了可解释的AIGV评估基准与统一模型，为生成视频评测提供标准化与可信框架，促进多模态生成与可信AI发展；代码与数据将随论文发布。

Abstract: We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.

</details>


### [218] [Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification](https://arxiv.org/abs/2511.18826)
*Aakash Gore,Anoushka Dey,Aryan Mishra*

Main category: cs.CV

TL;DR: 提出一种不确定性感知的双学生蒸馏框架：用教师预测的不确定性来选择性指导学习，并让ResNet‑18与MobileNetV2互为同伴共同学习；在ImageNet‑100上优于传统单学生蒸馏。


<details>
  <summary>Details</summary>
Motivation: 传统蒸馏把教师的所有预测一视同仁，忽略了教师在不同样本/类别上的置信度差异，可能把不可靠的信号同等传给学生，削弱压缩与迁移效果。

Method: 引入不确定性感知的蒸馏：对教师输出估计不确定性（如通过温度/方差等），对高不确定样本降低蒸馏权重或采用不同损失；并设计双学生同伴学习机制，让异构学生（ResNet‑18、MobileNetV2）彼此互蒸馏与协同训练，同时接受教师指导。

Result: 在ImageNet‑100上，ResNet‑18达83.84% top-1，MobileNetV2达81.46% top-1，分别较传统单学生蒸馏提升2.04%与0.92%。

Conclusion: 利用教师不确定性进行选择性蒸馏，配合同步的异构双学生同伴学习，可在模型压缩场景中显著提升学生模型精度，优于常规蒸馏基线。

Abstract: Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\% top-1 accuracy and MobileNetV2 achieving 81.46\% top-1 accuracy, representing improvements of 2.04\% and 0.92\% respectively over traditional single-student distillation approaches.

</details>


### [219] [Leveraging Metaheuristic Approaches to Improve Deep Learning Systems for Anxiety Disorder Detection](https://arxiv.org/abs/2511.18827)
*Mohammadreza Amiri,Monireh Hosseini*

Main category: cs.CV

TL;DR: 提出一种融合深度学习与群智能优化的混合框架，用多模态可穿戴数据（生理/情绪/行为信号）自动检测焦虑，较单纯深度网络显著提升准确率与跨个体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统焦虑评估依赖访谈与量表，主观、耗时且评估者间一致性差；AI发展提供更客观自动化的检测机会，但单一深度学习可能受特征冗余、超参数敏感与泛化不足限制。

Method: 构建混合模型：1) 以深度学习对时序多源输入学习分层判别表示；2) 引入群智能（遗传算法、粒子群优化）做特征空间选择与超参数搜索；3) 使用多模态与可穿戴传感数据整合生理、情绪、行为信号进行训练与评估。

Result: 与仅用深度网络相比，混合框架在准确率上有显著提升，并在跨个体的泛化表现更好。

Conclusion: 将元启发式群智能优化与深度学习融合，可构建可扩展、客观且具有临床意义的焦虑检测系统，值得在真实环境与更广泛人群中进一步验证与部署。

Abstract: Despite being among the most common psychological disorders, anxiety-related conditions are still primarily identified through subjective assessments, such as clinical interviews and self-evaluation questionnaires. These conventional methods often require significant time and may vary depending on the evaluator. However, the emergence of advanced artificial intelligence techniques has created new opportunities for detecting anxiety in a more consistent and automated manner. To address the limitations of traditional approaches, this study introduces a comprehensive model that integrates deep learning architectures with optimization strategies inspired by swarm intelligence. Using multimodal and wearable-sensor datasets, the framework analyzes physiological, emotional, and behavioral signals. Swarm intelligence techniques including genetic algorithms and particle swarm optimization are incorporated to refine the feature space and optimize hyperparameters. Meanwhile, deep learning components are tasked with deriving layered and discriminative representations from sequential, multi-source inputs. Our evaluation shows that the fusion of these two computational paradigms significantly enhances detection performance compared with using deep networks alone. The hybrid model achieves notable improvements in accuracy and demonstrates stronger generalization across various individuals. Overall, the results highlight the potential of combining metaheuristic optimization with deep learning to develop scalable, objective, and clinically meaningful solutions for assessing anxiety disorders

</details>


### [220] [VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction](https://arxiv.org/abs/2511.18831)
*Shaobo Wang,Tianle Niu,Runkang Yang,Deshan Liu,Xu He,Zichen Wen,Conghui He,Xuming Hu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出VideoCompressa：将视频数据合成重塑为“动态潜变量压缩”，通过可微关键帧选择+冻结VAE编码，仅用极少数据与计算达到与甚至超越全量训练的效果。


<details>
  <summary>Details</summary>
Motivation: 大规模视频数据存储与训练成本高，现有图像合成的高效性难以直接迁移到视频，因视频存在时间冗余与复杂时空动态。作者洞察：主要低效源自单个视频内部的帧级冗余（而非样本间冗余），因此需直接面向“挑帧+压缩”的更细粒度视频合成。

Method: 提出VideoCompressa框架：1) 可微关键帧选择器（轻量ConvNet+Gumbel-Softmax采样）从视频中选最具信息的帧；2) 使用预训练且冻结的VAE将关键帧编码为紧凑、语义丰富的潜变量；3) 这些潜变量输入压缩网络，整体端到端反传；4) 关键帧选择器与合成潜变量联合优化，以最大化与下游任务相关的信息保留。

Result: 在UCF101（ConvNet设置）上，仅用原始数据0.13%即超过全量训练2.34个百分点，且较传统合成方法加速约5800倍；在HMDB51上微调Qwen2.5-7B-VL，仅用0.41%训练数据即可达到全量性能，并较零样本提升10.61%。

Conclusion: 面向视频的动态潜变量压缩与可微关键帧选择能显著减少数据与算力开销，同时保持/提升下游性能，验证了“帧内冗余是主要低效源”的观点，并为高效视频数据合成提供可扩展路径。

Abstract: The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\% points using only 0.13\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\% of the training data-outperforming zero-shot baseline by 10.61\%.

</details>


### [221] [FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories](https://arxiv.org/abs/2511.18834)
*Lei Ke,Hubery Yin,Gongye Liu,Zhengyao Lv,Jingcai Guo,Chen Li,Wenhan Luo,Yujiu Yang,Jing Lyu*

Main category: cs.CV

TL;DR: 提出FlowSteer：在ReFlow蒸馏框架中，通过对齐并跟随教师真实生成轨迹，显著提升少步采样效率与质量；并修复常用调度器的缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管Flow Matching在视觉生成上成功，但采样效率是应用瓶颈。现有加速方法中，理论上与Flow Matching一致的ReFlow在实践中劣于一致性蒸馏与分数蒸馏，原因未明，限制了其应用。

Method: 1) 诊断：指出分段式ReFlow训练阶段存在关键的分布不匹配。2) 提出在线轨迹对齐（OTA）：在训练中在线对齐学生与教师的生成轨迹，消除不匹配。3) 设计基于ODE轨迹的对抗蒸馏目标，使学生更贴合教师真实生成路径。4) 揭示并修复FlowMatchEulerDiscreteScheduler中的缺陷，提升少步推理质量。

Result: 在SD3上的实验显示：FlowSteer在相同步数下显著优于基线ReFlow，并在少步推理质量上接近或超过一致性/分数蒸馏；修复的调度器显著改善few-step性能。

Conclusion: 通过沿教师真实轨迹的引导与在线对齐，ReFlow蒸馏的效率与质量被大幅提升，证明ReFlow在实践中可与主流蒸馏方法竞争；同时对社区常用调度器的修复具有普适价值。

Abstract: With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.

</details>


### [222] [FVAR: Visual Autoregressive Modeling via Next Focus Prediction](https://arxiv.org/abs/2511.18838)
*Xiaofan Li,Chenming Wu,Yanpeng Sun,Jiaming Zhou,Delin Qu,Yansong Qu,Weihao Bo,Haibao Yu,Dingkang Liang*

Main category: cs.CV

TL;DR: 提出FVAR：把视觉自回归从“下一尺度预测”改为“下一焦点预测”，用物理一致的离焦模糊金字塔替代均匀降采样，配合高频残差教师蒸馏，显著减少走样与摩尔纹、提升细节与文字可读性，在ImageNet上优于现有VAR且兼容其框架。


<details>
  <summary>Details</summary>
Motivation: 传统视觉自回归依赖多尺度金字塔的均匀降采样进行下一尺度预测，但下采样引入混叠，造成锯齿与摩尔纹，损害细节与文本质量。需要一种既保留结构又避免混叠的多尺度表示与预测范式。

Method: 1) 下一焦点预测：按“由虚到实”的焦点推进替代尺度推进；2) 渐进再聚焦金字塔：用物理一致的离焦PSF核构造低通视图，半径逐步减小形成平滑模糊-清晰过渡，源头抑制混叠；3) 高频残差学习：训练期引入残差教师，学习干净结构与混叠残差，将其蒸馏到部署时的标准VAR网络，实现无额外推理开销。

Result: 在ImageNet上，相比常规VAR，FVAR显著降低混叠伪影（锯齿、摩尔纹），更好保细节与文本可读性；总体生成质量和多项指标取得更优表现，并与现有VAR框架完全兼容。

Conclusion: 以“下一焦点预测”替代“下一尺度预测”，结合基于PSF的无混叠多尺度表示与高频残差教师蒸馏，可从源头消除混叠并提升细节与可读性，为视觉自回归提供与现有框架兼容、性能更佳的新范式。

Abstract: Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moiré patterns. To tackle this issue, we present \textbf{FVAR}, which reframes the paradigm from \emph{next-scale prediction} to \emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.

</details>


### [223] [Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification](https://arxiv.org/abs/2511.18839)
*Yasiru Laksara,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 用深度集成替代不稳的MC Dropout，在ChestX-ray14上实现更高性能与更好校准，并可分解不确定性，提升临床可用性。


<details>
  <summary>Details</summary>
Motivation: 临床高风险场景需要模型不仅高准确，还需可靠的不确定性度量；现有如CheXNet等确定性模型缺乏可信的置信度，限制临床决策支持。

Method: 在ChestX-ray14的14类胸部疾病诊断中，先尝试MCD进行不确定性量化但校准失败（ECE=0.7588），转而设计高多样性的9成员深度集成（Deep Ensemble），评估AUROC、F1、ECE、NLL，并分解总不确定性为异质性与认知性。

Result: MCD方案不稳定且校准差；深度集成达到SOTA平均AUROC 0.8559、平均F1 0.3857，显著改进校准（ECE 0.0728、NLL 0.1916），估计平均认知性不确定性EU为0.0240。

Conclusion: 深度集成提供稳健预测与可信不确定性量化，可解释地分解不确定性，将模型提升为可靠的临床决策支持系统，优于MCD路线。

Abstract: The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.

</details>


### [224] [Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration](https://arxiv.org/abs/2511.18847)
*Ishmam Tashdeed,Md. Atiqur Rahman,Sabrina Islam,Md. Azam Hossain*

Main category: cs.CV

TL;DR: 提出FedOAP：一种面向器官无关肿瘤分割的个性化联邦学习方法，利用去耦跨注意力共享全局特征并结合扰动边界损失提升边界一致性，在多器官肿瘤任务上优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习在医疗分割中虽能保护隐私并缓解Non-IID，但常忽视不同客户端间可共享的跨器官通用特征；各客户端数据来自不同器官，若能建模其共性与长程依赖，可提升分割性能与泛化。

Method: 提出FedOAP框架：1) 去耦跨注意力（DCA）：客户端保留本地query，同时使用从所有客户端聚合的全局key-value进行跨注意力，建模跨器官长程依赖并共享有益特征；2) 扰动边界损失（PBL）：关注预测掩膜边界不一致性，通过对边界进行扰动与惩罚，引导模型更精准地定位肿瘤边缘；整体在联邦个性化设置下训练。

Result: 在多种器官的肿瘤分割基准上进行广泛实验，FedOAP在整体指标上持续优于现有联邦与个性化分割方法，达到新的SOTA性能。

Conclusion: 跨客户端共享特征的建模与边界敏感优化能够有效提升器官无关的肿瘤分割性能；FedOAP验证了在PFL中结合DCA与PBL的有效性，为医疗影像的隐私保护与异构数据场景提供了更优解。

Abstract: Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.

</details>


### [225] [Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization](https://arxiv.org/abs/2511.18851)
*Yilin Wen,Kechuan Dong,Yusuke Sugano*

Main category: cs.CV

TL;DR: 提出一种针对3D人体姿态估计的在线测试时自适应（TTA）方法，通过运动离散化与软重置机制缓解自监督误差累积，稳定长期适应并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 在线TTA在无监督流式测试数据上逐步自适应，但3D姿态估计依赖自身预测进行自监督，易产生并累积误差，导致性能随时间劣化。需要一种机制稳定学习、避免漂移，并充分利用同一受试者在长序列中的个体形体与运动一致性。

Method: 1) 在潜在运动表示空间进行无监督聚类，得到一组“锚定运动”原型，用其规则性作为辅助监督并实现高效自回放；2) 在连续适应中引入软重置：定期将估计器回滚到其指数滑动平均（EMA）权重，抑制漂移；3) 设定长期在线场景：对同一人的跨域流式视频持续适应，以捕捉稳定的个体形体与运动特征。

Result: 在长序列、跨域的在线测试中，相比已有在线TTA方法取得更高精度；消融验证表明运动离散化（锚定运动与自回放）和软重置均显著降低误差累积并带来增益。

Conclusion: 通过运动离散化与EMA软重置，解决了3D姿态在线TTA的误差累积与漂移问题，实现鲁棒的长期适应，并能更好利用个体一致性以提升精度。

Abstract: Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.

</details>


### [226] [Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos](https://arxiv.org/abs/2511.18856)
*Sana Alamgeer*

Main category: cs.CV

TL;DR: 该项目提出在360°视频中预测兴趣区域(ROI)的模型，通过预处理提帧、混合显著性模型预测、以及后处理生成每帧ROI，并与360RAT数据集的人工标注对比评估。


<details>
  <summary>Details</summary>
Motivation: 360°视频流中用户仅观察视口的一小部分，若能提前预测ROI，可用于视口预测与智能裁剪，降低带宽、减少头部移动、提升观看质量；现有方法或数据集需要更准确/高效的ROI预测。

Method: 构建混合显著性模型作为次要任务以识别显著区域：1) 视频预处理获取帧；2) 设计并训练融合多源线索（推测为空间/时间/语义）的混合显著性网络来预测每帧显著图；3) 对显著图进行后处理生成最终ROI；4) 将结果与360RAT数据集的主观标注进行比较评测。

Result: 实验将模型预测与360RAT的人工标注进行对比，报告指出能够得到与主观标注相符的ROI（摘要未给出具体指标或数值）。

Conclusion: 混合显著性方法可用于360°视频ROI预测，并有潜力用于视口预测与视频智能裁剪以提高流媒体效率与用户体验；需要进一步给出定量指标与更广泛数据验证。

Abstract: The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.

</details>


### [227] [Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling](https://arxiv.org/abs/2511.18858)
*Xiao Cui,Yulei Qin,Xinyue Li,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 提出一种面向长尾分布的数据集蒸馏方法，从统计对齐角度缓解类别不平衡带来的表示偏置与BN统计失真，结合增强的专家模型、动量重校准的BN前传、与多轮高置信多样化初始化，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏在均衡数据上表现良好，但在长尾分布下易因类别频次不均导致模型偏置和BN统计失真，致使轨迹匹配类方法难以提供公平监督与稳定估计，亟需新的框架来同时矫正偏置并恢复可靠监督信号。

Method: 放弃单纯的训练轨迹匹配，转为统计对齐视角，提出三组件：1) 增强专家模型（用于恢复的观察者模型与用于软重标注的教师模型），提升统计估计与软标签质量；2) 通过全量前向传播并动态调整动量，重校准BN统计以降低表示偏斜；3) 采用多轮机制，从高置信且多样的增强样本中增量式选择，用于合成图像初始化以覆盖度与多样性兼顾。

Result: 在四个长尾基准上相对SOTA持续领先；在CIFAR-100-LT与Tiny-ImageNet-LT（IPC=10, IF=10）分别带来+15.6%与+11.8%的Top-1准确率提升。

Conclusion: 统计对齐驱动的长尾数据集蒸馏，通过专家增强、BN重校准与多样化初始化，能有效缓解类别不平衡的偏置问题，稳定恢复蒸馏图像与软标签，在多基准上显著超越现有方法。

Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.

</details>


### [228] [DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection](https://arxiv.org/abs/2511.18865)
*Yu Zhang,Haoan Ping,Yuchen Li,Zhenshan Bing,Fuchun Sun,Alois Knoll*

Main category: cs.CV

TL;DR: 提出DualGazeNet：一种受人类视觉启发的纯Transformer显著性目标检测模型，在五个RGB数据集上优于25个SOTA，推理更快、FLOPs更低，并具备良好跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有SOD方法为追求性能引入多阶段、复杂融合、边界引导与繁复注意力等组件，导致特征冗余与部件间干扰，性能触顶且可解释性与效率下降。受人类视觉系统简单高效的启发，作者希望构建一个生物学驱动但架构简洁的框架，同时兼顾精度、效率与可解释性。

Method: 提出DualGazeNet：纯Transformer架构，建模两大生物学原则——稳健表征学习与视网膜-皮层的M/P双通路处理，并配以皮层注意调制。整体去除繁复工程模块，通过双路径与注意机制实现高效、鲁棒的显著性线索抽取与融合。

Result: 在五个RGB SOD基准上全面超越25个CNN与Transformer方法；相对四个相近容量的Transformer基线（VST++、MDSAM、Sam2unet、BiRefNet），平均推理速度提升约60%，FLOPs降低53.4%；在伪装与水下SOD上也取得领先或高度竞争的零额外模态表现。

Conclusion: 生物学启发且架构简洁的纯Transformer（DualGazeNet）能在不依赖复杂工程堆叠的前提下，同时实现SOTA精度、计算效率与跨域泛化，并提升可解释性潜力。

Abstract: Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\% higher inference speed and 53.4\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.

</details>


### [229] [HunyuanVideo 1.5 Technical Report](https://arxiv.org/abs/2511.18870)
*Bing Wu,Chang Zou,Changlin Li,Duojun Huang,Fang Yang,Hao Tan,Jack Peng,Jianbing Wu,Jiangfeng Xiong,Jie Jiang,Linus,Patrol,Peizhen Zhang,Peng Chen,Penghao Zhao,Qi Tian,Songtao Liu,Weijie Kong,Weiyan Wang,Xiao He,Xin Li,Xinchi Deng,Xuefei Zhe,Yang Li,Yanxin Long,Yuanbo Peng,Yue Wu,Yuhong Liu,Zhenyu Wang,Zuozhuo Dai,Bo Peng,Coopers Li,Gu Gong,Guojian Xiao,Jiahe Tian,Jiaxin Lin,Jie Liu,Jihong Zhang,Jiesong Lian,Kaihang Pan,Lei Wang,Lin Niu,Mingtao Chen,Mingyang Chen,Mingzhe Zheng,Miles Yang,Qiangqiang Hu,Qi Yang,Qiuyong Xiao,Runzhou Wu,Ryan Xu,Rui Yuan,Shanshan Sang,Shisheng Huang,Siruis Gong,Shuo Huang,Weiting Guo,Xiang Yuan,Xiaojia Chen,Xiawei Hu,Wenzhi Sun,Xiele Wu,Xianshun Ren,Xiaoyan Yuan,Xiaoyue Mi,Yepeng Zhang,Yifu Sun,Yiting Lu,Yitong Li,You Huang,Yu Tang,Yixuan Li,Yuhang Deng,Yuan Zhou,Zhichao Hu,Zhiguang Liu,Zhihe Yang,Zilin Yang,Zhenzhi Lu,Zixiang Zhou,Zhao Zhong*

Main category: cs.CV

TL;DR: HunyuanVideo 1.5 是一款仅 83 亿参数的开源视频生成模型，在消费级 GPU 上高效推理，同时在画质与运动一致性上达到了开源 SOTA。通过数据治理、改进的 DiT 架构（SSTA 选择/滑动块注意力）、字形感知的双语文本编码、分阶段预/后训练与高效视频超分，统一支持多时长多分辨率的文生视频与图生视频。代码与权重已开源。


<details>
  <summary>Details</summary>
Motivation: 在保持高质量与强运动建模的前提下，显著降低参数规模与推理成本，填补开源视频生成模型在效率、可用性与双语理解上的不足，降低视频创作与研究门槛。

Method: 1) 严格数据筛选与构造；2) 改进 DiT，提出选择与滑动块注意力（SSTA）以在大分辨率/长时序下平衡算力与全局依赖；3) 字形感知文本编码，增强中英双语理解；4) 渐进式预训练与后训练策略提升稳健性与质量；5) 轻量高效的视频超分网络；6) 构建统一框架，兼容 T2V 与 I2V，多时长多分辨率。

Result: 在广泛实验中，在开源视频生成模型中达成新的 SOTA：更高的视觉质量与运动连贯性，同时仅 8.3B 参数即可在消费级 GPU 上高效推理；统一框架在不同任务和分辨率/时长上表现稳定。

Conclusion: HunyuanVideo 1.5 以紧凑架构实现高质高效的视频生成，推动开源生态可复现与易用性；开源代码与权重为社区提供高性能基座，扩大先进视频生成技术的可达性。

Abstract: We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.

</details>


### [230] [Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction](https://arxiv.org/abs/2511.18873)
*Yiming Wang,Shaofei Wang,Marko Mihajlovic,Siyu Tang*

Main category: cs.CV

TL;DR: 提出 Neural Texture Splatting (NTS)，用全局神经场为每个高斯基元预测局部纹理与几何，使3DGS在稀疏/稠密、静态/动态重建任务上均取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS用3D高斯核表示局部变化，表达力受限；增加“每个基元纹理”的做法在稠密新视角合成有效，但在更一般的重建任务与稀疏输入下易失效且参数量大、缺乏全局信息共享。需要一种既提升表达力又具良好泛化与高效性的表示。

Method: 构建一个共享的全局神经场（混合三平面tri-plane与小型解码器），对每个高斯基元预测其局部外观与几何字段，从而形成“神经纹理”——可视角/时间相关的局部纹理与几何。通过全局-局部的共享表示减少参数、实现跨基元信息交换，并可用于NVS、几何重建、动态重建等多任务、稀疏/稠密输入设置。

Result: 在多个基准上，相比现有3DGS及其变体，NTS在新视角合成、几何与动态重建等任务中一致性提升，达到SOTA，同时模型体量更小、效率更高，表现出更强泛化与稳健性。

Conclusion: 利用全局神经场为局部高斯基元提供可视角/时间依赖的神经纹理与几何，可在不增加大量每基元参数的情况下显著提升3DGS表达力与跨任务性能，实现高质量、参数高效、可泛化的3D/4D重建。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.

</details>


### [231] [Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference](https://arxiv.org/abs/2511.18875)
*Wengyi Zhan,Mingbao Lin,Zhihang Lin,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出 ParVTS：在推理时并行调度图像token，将视觉token按主体/非主体分组，并在中途丢弃非主体路径，以在几乎不降性能下显著加速多模态大模型。


<details>
  <summary>Details</summary>
Motivation: MLLM 推理慢的根源在于自注意力对序列长度二次复杂度，以及高分辨率图像带来的海量视觉token。简单裁剪会损失上下文与细粒度信息，影响问答与推理准确性。因此需要一种既能减负、又能保留关键语义的高效调度策略，且最好无需额外训练或启发式规则，并能通用于现有架构。

Method: 训练无关的并行视觉token调度：将视觉token按语义角色划分为主体（subject）与非主体（non-subject）两路，二者并行与问题token交互，把各自语义“转移/蒸馏”到问题token；在中途阶段丢弃非主体分支，从而在后续层仅保留主体相关计算，达到降算目的。无需新增模块或复杂启发式，适配多种 MLLM。

Result: 在多种 MLLM 骨干上，最多裁剪 88.9% 视觉token，FLOPs 降低约 70%，推理加速约 1.77×，同时性能仅有极小下降。

Conclusion: ParVTS 能以简单的并行调度与中途丢弃策略，在不训练的前提下大幅减少视觉计算开销并保持准确度，对广泛 MLLM 兼容，适合在资源受限或低延迟场景部署。

Abstract: Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.

</details>


### [232] [Facade Segmentation for Solar Photovoltaic Suitability](https://arxiv.org/abs/2511.18882)
*Ayca Duran,Christoph Waibel,Bernd Bickel,Iro Armeni,Arno Schlueter*

Main category: cs.CV

TL;DR: 该论文提出一条将立面建筑语义信息融入BIPV可安装性评估的自动化管线：用SegFormer-B5在CMP Facades上微调做立面语义分割，生成可安装性掩膜与考虑组件尺寸/间距的排布方案；在10城373个有尺寸标注的立面上验证，发现可安装的BIPV潜力显著低于理论值，为城市能源规划提供更可靠估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的PV规划主要聚焦屋顶，立面端方法稀缺且过度简化；在屋顶面积不足、地面不适合布置的城市环境中，BIPV立面是脱碳重要路径，但需要细粒度的建筑构成信息来提高可行性评估与潜力估算的准确性。

Method: 构建端到端管线：1) 使用SegFormer-B5在CMP Facades数据集上微调，实现对窗、门、墙体等立面构件的语义分割；2) 将像素级预测转换为立面级BIPV适配掩膜，过滤不可用区域；3) 依据实际模块尺寸与安装净距，自动生成组件排布与可安装面积；4) 汇总估算太阳能潜力。

Result: 在来自10个城市、373个具备真实尺度的立面数据上评估，表明“可安装”BIPV潜力远低于理想化的“理论”潜力，揭示以往忽略构件与安装约束会显著高估立面光伏资源。

Conclusion: 利用立面图像和语义分割生成BIPV适配掩膜与排布，可规模化、自动化评估城市立面光伏潜力；该方法能提供更保守且可信的潜力估计，支持更可靠的城市能源规划，并可随立面影像数据愈加丰富而推广至全球城市。

Abstract: Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.

</details>


### [233] [MagicWorld: Interactive Geometry-driven Video World Exploration](https://arxiv.org/abs/2511.18886)
*Guangyuan Li,Siming Zheng,Shuolin Xu,Jinwei Chen,Bo Li,Xiaobin Hu,Lei Zhao,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicWorld 是一种交互式视频世界模型，引入动作引导的3D几何模块和历史缓存检索，以提升多步交互中的结构稳定性与语义连续性。


<details>
  <summary>Details</summary>
Motivation: 现有交互式视频世界模型在两方面薄弱：1) 指令驱动的运动未与底层3D几何充分对齐，导致视角变化下的结构不稳定；2) 多步交互时容易遗忘历史，误差累积引发语义与结构漂移。

Method: 从单张场景图像出发，基于用户动作自回归生成场景序列。核心包括：- AG3D（动作引导3D几何模块）：在每次交互的首帧与动作条件下构建点云，作为显式几何约束，增强视角切换下的结构一致性；- HCR（历史缓存检索）：在生成时检索相关历史帧并作为条件信号注入，促进利用既往信息、抑制误差累积。

Result: 实验显示，相较基线，MagicWorld 在多轮交互中显著提升了场景稳定性与连续性，减缓了语义与结构漂移。

Conclusion: 融合3D几何先验与历史检索能有效缓解交互式视频世界模型的结构不稳与遗忘问题，MagicWorld 提供了稳健的多步交互场景演化能力。

Abstract: Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.

</details>


### [234] [MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model](https://arxiv.org/abs/2511.18888)
*Qian Jiang,Qianqian Wang,Xin Jin,Michal Wozniak,Shaowen Yao,Wei Zhou*

Main category: cs.CV

TL;DR: 提出MFmamba，一种可在仅输入PAN情况下同时完成超分辨、光谱恢复与联合任务的统一模型，基于UNet++骨干并引入MUB、DPA与MHCB模块，在多项实验中取得竞争性指标与视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有遥感成像受单传感器限制：PAN高空间但无色彩，MS有色彩但空间低。超分辨提升空间却不增光谱，上色提升光谱却不增空间，传统融合需双输入且难以SR。因此需要一个在单一PAN输入下同时兼顾空间与光谱提升的一体化方法。

Method: 以UNet++为骨干，融合三项关键设计：1) Mamba Upsample Block (MUB) 用于更高质量的上采样与细节恢复；2) Dual Pool Attention (DPA) 替代UNet++跳连，利用双池化注意力增强多尺度与通道/空间特征交互；3) Multi-scale Hybrid Cross Block (MHCB) 做初始多尺度特征提取与跨尺度信息融合。模型以三种不同输入配置实现SR、光谱恢复与联合任务。

Result: 在多组实验中，MFmamba在客观指标与主观视觉上均具竞争力；在仅有PAN输入的条件下，三类任务（SR、光谱恢复、联合）均表现良好。

Conclusion: MFmamba提供了一个统一框架，在单一PAN输入下实现空间与光谱的同时提升，弥补了SR、上色与传统融合方法的局限，并在实验中验证了有效性。

Abstract: Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.

</details>


### [235] [MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting](https://arxiv.org/abs/2511.18894)
*Chenyu Mu,Guihai Chen,Xun Yang,Erkun Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出MetaDCSeg，通过动态像素权重与边界不确定性建模（DCD）抑制噪声标注、强化边界像素，显著提升医学图像分割鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 医学分割训练常受噪声标注与模糊解剖边界影响，导致不稳定与性能下降。现有方法多依赖全局噪声假设或基于置信度的样本筛选，难以细粒度处理边界区域的噪声与不确定性。

Method: 提出MetaDCSeg：1) 元学习式动态像素级权重学习，降低噪声标签影响、保留可靠监督；2) 动态中心距离（DCD）机制，显式建模前景/背景/边界的中心与加权特征距离，引导关注难分割的边界像素；3) 利用权重化的特征距离度量，联合优化分割与边界鲁棒性。

Result: 在四个具有不同噪声水平的基准数据集上，MetaDCSeg在分割指标上稳定优于现有SOTA，尤其在边界区域表现更佳。

Conclusion: 动态像素权重结合边界不确定性建模可有效抑制噪声标注、强化对模糊边界的处理，提升医学图像分割的鲁棒性与精度；方法在多数据集上具有一致优势。

Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.

</details>


### [236] [Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation](https://arxiv.org/abs/2511.18919)
*Ruiying Liu,Yuanzhi Liang,Haibin Huang,Tianshu Yu,Chi Zhang*

Main category: cs.CV

TL;DR: BPGO 在 GRPO 基础上显式建模奖励不确定性，通过贝叶斯先验锚定，分别在组间分配“信任”权重、组内重标定分数，从而更稳健地利用可靠反馈、抑制含糊信号，提升对齐、质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 视觉生成后训练常依赖 GRPO，但文本-视觉对应具多对多歧义：同一提示可对应多种图像/视频，同一图像也能有多种合理描述。由此训练的奖励模型信号不确定、判别性弱，导致 GRPO 容易过拟合噪声、浪费可靠反馈，性能受限。

Method: 提出 Bayesian Prior-Guided Optimization（BPGO）。核心是引入语义先验锚点以刻画奖励不确定性，并在两层面自适应调制优化信任度：1）组间（inter-group）贝叶斯信任分配：对与先验一致的样本组赋予更高更新权重，削弱含糊组影响；2）组内（intra-group）先验锚定重归一：放大先验确信的偏差、压缩不确定评分，增强样本区分度。方法为 GRPO 的轻量扩展，适用于图像与视频生成。

Result: 在图像与视频生成任务上，相比标准 GRPO 与近期变体，BPGO 实现更强的语义对齐、更好的感知质量，并带来更快的收敛。

Conclusion: 显式引入先验并以贝叶斯方式调制优化，可缓解文本-视觉歧义导致的奖励不确定，提升 GRPO 框架的稳定性与效率；BPGO 是通用、轻量且有效的后训练方案。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.

</details>


### [237] [EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models](https://arxiv.org/abs/2511.18920)
*Wenhao Xu,Xin Dong,Yue Li,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 提出EventSTU：受事件相机启发的、训练免调的事件引导视频LLM高效时空理解框架，以事件触发变化选择关键帧并基于事件显著性自适应裁剪空间token，在问题相关性指导下统一分配预算；并构建事件多模态基准EventBench；在模拟与真实事件下实现约3x算力/预填速率优化且性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在长视频上理解能力强，但因帧数与token数庞大导致推理成本高；需要一种能在保持或提升性能的同时显著降低时空冗余的通用、高效方案。事件相机天然对变化敏感、稀疏高效，启发用其作为零成本先验来指导时空压缩。

Method: 1) 时间域：设计“粗到细”的关键帧采样，利用事件相机的变化触发特性过滤冗余帧；2) 空间域：提出自适应token剪枝，借助事件显著性作为零成本先验进行空间压缩；3) 时空一体：将由关键帧采样得到的“问题相关性”用于自适应分配各帧/区域的token预算；4) 构建EventBench，覆盖多场景、包含真实事件数据与人工标注，并支持通过模拟事件用于通用视频理解；整体为训练免调，兼容现有视频LLM。

Result: 在全面实验中，相比最强基线，FLOPs降低约3.01倍、prefilling阶段加速约3.10倍，同时任务性能不降反升；在真实与模拟事件场景下均有效。

Conclusion: EventSTU证明了事件引导的训练免调时空压缩可在视频LLM中显著降本增效；EventBench为评测提供了首个事件多模态基准。方法通用于物理与模拟事件，具备实际部署与研究价值。

Abstract: Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.

</details>


### [238] [BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models](https://arxiv.org/abs/2511.18921)
*Juncheng Li,Yige Li,Hanxun Huang,Yunhao Chen,Xin Wang,Yixu Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出BackdoorVLM：首个系统性评估视觉-语言模型后门攻击的基准，覆盖多任务、五类威胁与多种触发方式；发现文本触发主导双模态后门，低至1%投毒即可在多数任务上达90%+攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有后门研究多聚焦单模态，对多模态基础模型（VLM）的安全性与后门脆弱性缺乏系统评估与统一视角；需要一个标准基准来刻画不同任务、触发与攻击类型下的风险。

Method: 构建BackdoorVLM基准：统一框架在图像描述与VQA等核心VL任务中注入并评估后门；将多模态后门威胁归纳为五类（定向拒答、恶意注入、越狱、概念替换、感知劫持）；实现12种代表性攻击（文本/图像/双模态触发），在2个开源VLM与3个多模态数据集上系统测试。

Result: 发现VLM对文本指令极为敏感；在双模态后门中，文本触发通常压倒图像触发决定后门映射；文本相关后门极具攻击力，投毒率仅1%即可在大多数任务上获得90%+成功率。

Conclusion: 当前VLM存在显著且被低估的多模态后门脆弱性；BackdoorVLM为分析与缓解此类威胁提供标准基准与统一视角，呼吁关注文本触发主导性并发展更有效的防御。

Abstract: Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\% yielding over 90\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .

</details>


### [239] [One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control](https://arxiv.org/abs/2511.18922)
*Zhenxing Mi,Yuxin Wang,Dan Xu*

Main category: cs.CV

TL;DR: One4D 提出一个统一的4D生成与重建框架，能同时输出时序一致的RGB视频帧与点图（pointmap），通过统一掩码条件机制在单图生成、全视频重建与稀疏帧混合任务间无缝切换，并用解耦LoRA控制稳定联合生成，达到高质量外观与几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有4D方法往往只做外观视频生成或只做几何重建，难以在同一模型中稳定地联合生成RGB与几何（点/深度），且在不同条件稀疏度（单帧、稀疏帧、全视频）之间缺乏统一处理；直接用扩散微调做点/深度会破坏视频生成能力。

Method: 1) 统一掩码条件（UMC）：对输入帧的不同稀疏度进行一致化编码，让模型在单图、稀疏、多帧条件下以同一机制处理；2) 将强视频扩散基座改造为联合RGB与pointmap双模态头；3) 解耦LoRA控制（DLC）：为RGB与pointmap各自设置LoRA分支，以零初始化的轻量跨分支控制连接逐步学习像素级一致性，避免相互干扰导致退化；4) 在合成与真实4D数据混合集上训练。

Result: 在生成与重建两类任务上，模型能输出高质量的RGB帧与精确点图，保持时间一致与几何-外观对齐；相较直接用扩散微调的基线，联合生成稳定且不出现明显退化，并在有限算力下取得强效果。

Conclusion: One4D展示了以视频扩散模型为基础的通用4D世界建模可行路径：通过统一条件与解耦控制，实现面向不同条件密度的高质量外观与几何联合建模，为未来通用、基于几何的一体化4D生成与重建奠定基础。

Abstract: We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D

</details>


### [240] [AttenDence: Maximizing Attention Confidence for Test Time Adaptation](https://arxiv.org/abs/2511.18925)
*Yash Mali*

Main category: cs.CV

TL;DR: 提出在测试时自适应中，最小化CLS到图像patch的注意力分布熵，作为新的无监督目标，提升在分布偏移和各种腐化下的鲁棒性，同时不损害干净数据表现，即使只有单张测试图像也有效。


<details>
  <summary>Details</summary>
Motivation: 传统TTA多用输出熵最小化，但Transformer有可利用的注意力信号；在分布移位时模型往往注意力分散或偏移，若能让注意力更集中于相关区域，或可提高稳健性。

Method: 在推理阶段，对每个输入图像计算CLS对各patch的注意力权重，最小化其熵以促使注意力分布更“尖锐”。该目标与标准输出熵最小化并行或替代使用，无需标签，可在单样本流场景在线更新模型参数。

Result: 在多种图像腐化/分布偏移基准上，注意力熵最小化提升了鲁棒性；在干净数据上没有性能下降；即使仅有单张测试图像也能带来有效适应。

Conclusion: 利用Transformer注意力的熵最小化作为TTA目标是简洁有效的，能在分布移位下引导模型更自信地关注相关区域，增强稳健性且不牺牲原始性能。

Abstract: Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.

</details>


### [241] [FineXtrol: Controllable Motion Generation via Fine-Grained Text](https://arxiv.org/abs/2511.18927)
*Keming Shen,Bizhu Wu,Junliang Chen,Xiaoqin Wang,Linlin Shen*

Main category: cs.CV

TL;DR: FineXtrol提出一种以时间感知、细粒度文本控制信号为核心的高效文本驱动动作生成框架，通过层级对比学习提升文本编码判别性，实现对特定身体部位随时间的精准控制，兼顾可控性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖LLM扩写文本但引入细节错配且缺乏显式时间线索，要么使用全局3D坐标作为控制信号导致从坐标到标准动作表示的计算代价高。需要一种既具时间对齐、细粒度且用户友好的文本控制，同时效率更高的方案。

Method: 提出FineXtrol框架：1) 设计“时间感知、细粒度、用户友好”的文本控制信号，明确描述各身体部位随时间的运动；2) 构建层级对比学习模块，督促文本编码器在多层级（例如动作级、部位级、时间片级）产生更具判别性的嵌入，从而提升控制一致性；3) 将该文本嵌入作为控制条件引导动作生成模型，实现高效可控的动作合成。

Result: 定量实验显示在可控性指标上表现强，定性结果表明能灵活、准确地指挥特定身体部位的运动，且推理效率优于依赖全局坐标的方案。

Conclusion: FineXtrol通过细粒度时间文本控制与层级对比学习，有效提升文本驱动动作生成的可控性与效率，减少错配并实现对局部身体部位随时间的精准操控。

Abstract: Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.

</details>


### [242] [Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search](https://arxiv.org/abs/2511.18929)
*Zijian Song,Xiaoxin Lin,Tao Pu,Zhenlong Yuan,Guangrun Wang,Liang Lin*

Main category: cs.CV

TL;DR: 论文提出“以人为中心的开放未来任务发现（HOTD）”问题，构建包含2K+真实视频与仿真评测协议的基准，并提出多智能体协同搜索树（CMAST）框架，以多代理推理与可扩展搜索树发现能在多种未来情境下减轻人类负担的任务；在基准上显著优于现有LMM并可与其兼容增益。


<details>
  <summary>Details</summary>
Motivation: 当前LMM在机器人与具身智能中表现突出，但欠缺对“开放未来”场景中人类意图并发、动态变化时如何主动发现能直接帮助人的任务的能力。需要一个体系化问题定义、数据与评测协议，以及能处理多未来不确定性的推理方法。

Method: 1) 问题定义：提出HOTD，目标是在多种可能未来中识别可最大化减轻人类努力的任务。2) 基准：构建HOTD-Bench，含2000+真实世界视频、半自动标注流程、面向开放集合未来的仿真式评测协议。3) 算法：提出CMAST框架——以多智能体分解复杂推理，并用可扩展的搜索树组织与协同这些代理的推理与决策过程，实现对潜在任务的系统性探索与评估。

Result: 在HOTD-Bench上，CMAST取得最佳成绩，显著超越现有LMM；同时可作为上层策略与不同LMM集成，持续带来性能提升。

Conclusion: 面向开放未来的人本任务发现可通过多智能体协同与结构化搜索有效实现；所提基准与框架为具身AI在真实动态场景中主动助人提供了可评测、可扩展的路径，并验证了与主流LMM的互补性与增益。

Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.

</details>


### [243] [VeCoR - Velocity Contrastive Regularization for Flow Matching](https://arxiv.org/abs/2511.18942)
*Zong-Wei Hong,Jing-lun Li,Lin-Ze Li,Shen Zhang,Yao Tang*

Main category: cs.CV

TL;DR: VeCoR在流匹配（FM）生成模型中加入对比式“吸-斥”双向监督，既对齐正确速度方向又远离离散/离流形方向，从而减少轨迹累积误差并提升低步数、轻量化设置下的稳定性与感知质量。


<details>
  <summary>Details</summary>
Motivation: 标准FM仅提供单侧“吸引”式监督（沿目标方向前进），易在积分过程中累积偏差，把样本推离数据流形，导致感知质量下降，尤其在小步数或轻量模型时更明显。需要一种既告诉模型“往哪走”又明确“不要往哪走”的机制，以提升稳定性、泛化与收敛。

Method: 提出Velocity Contrastive Regularization（VeCoR）：在FM训练目标上加入对比式双向监督。正样本监督将预测速度与稳定参考方向对齐；负样本监督将其与不一致、离流形的方向拉开距离。将FM从单侧吸引变为“吸-斥”平衡的两侧信号，显式正则化生成轨迹的演化。

Result: 在ImageNet-1K 256x256上，SiT-XL/2与REPA-SiT-XL/2的FID相对下降22%与35%；在MS-COCO文本生成上进一步带来32%相对FID提升。表现为稳定性、收敛速度和图像质量的全面改进，尤以低步数和轻量骨干设置最显著。

Conclusion: 对比式速度正则可有效抑制轨迹偏移与离流形错误，显著提升FM模型在不同数据集与骨干上的感知质量与稳定性，是对标准FM目标的通用且高效的补充。

Abstract: Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.
  To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both "where to go" and "where not to go." To be formal, we propose \textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.
  On ImageNet-1K 256$\times$256, VeCoR yields 22\% and 35\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/

</details>


### [244] [Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining](https://arxiv.org/abs/2511.18946)
*José Teixeira,Pascal Klöckner,Diana Montezuma,Melis Erdal Cesur,João Fraga,Hugo M. Horlings,Jaime S. Cardoso,Sara P. Oliveira*

Main category: cs.CV

TL;DR: 提出CSSP2P GAN用于H&E到IHC的虚拟染色，强调对抗损失至关重要，并通过盲法病理专家评估与更合适的指标证明其病理保真度与整体性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 免疫组化(IHC)能检测特异蛋白但成本高、流程繁琐；虚拟染色作为图像到图像翻译可替代，但现有研究多依赖连续切片对、采用复杂cGAN而忽视对抗损失作用，并使用SSIM/PSNR等不可靠指标评估，导致对病理质量的结论存疑。

Method: 提出CSSP2P GAN（基于条件GAN的改进框架）。在模型迭代开发中系统性地消融与调控对抗损失，分析其对虚拟染色质量的影响；并采用盲法病理专家评估以衡量病理保真度，同时与参考方法比较并审视传统指标的局限。

Result: CSSP2P GAN在盲法专家评估中表现最佳，显示更高病理保真度；实验表明对抗损失对生成质量至关重要；与同领域方法对比显示其整体性能更优，并揭示SSIM/PSNR不足以可靠评估虚拟染色质量。

Conclusion: CSSP2P GAN在虚拟IHC染色任务中实现更高病理相关质量；对抗损失是关键组成；传统像素/结构相似度指标不足，需采用专家评估或更合适的任务相关指标来评测虚拟染色成像质量。

Abstract: In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.

</details>


### [245] [Eevee: Towards Close-up High-resolution Video-based Virtual Try-on](https://arxiv.org/abs/2511.18957)
*Jianhao Zeng,Yancheng Bai,Ruidong Chen,Xuanpu Zhang,Lei Sun,Dongyang Jin,Ryan Xu,Nannan Zhang,Dan Song,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出高分辨率视频试衣数据集与新一致性指标VGID，支持全身与特写，显著提升纹理与结构保真并完成系统基准评测。


<details>
  <summary>Details</summary>
Motivation: 现有视频试衣多仅用单张服装图，难以还原细节；且只关注全身镜头，忽略电商对细节特写的需求；缺乏能细粒度评估纹理/结构一致性的指标与数据。

Method: 构建高分辨率视频试衣数据集：提供高保真服装图（含特写）与文本描述，并配对真人全身+特写试穿视频；提出视频服装一致性指标VGID，度量纹理与结构的保留；用该数据驱动并评测现有视频生成模型。

Result: 利用新数据的细节图像，现有模型可更好提取与融合纹理特征，显著提升虚拟试穿的真实感与细节保真；基准评测揭示当前方法在纹理与结构保留上的不足。

Conclusion: 高分辨率数据与VGID共同填补了细节建模与评估的空白，既满足电商全身+特写需求，又为后续方法提供标准化评测与改进方向。

Abstract: Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.

</details>


### [246] [CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery](https://arxiv.org/abs/2511.18968)
*Bhuvan Sachdeva,Sneha Kumari,Rudransh Agarwal,Shalaka Kumaraswamy,Niharika Singri Prasad,Simon Mueller,Raphael Lechtenboehmer,Maximilian W. M. Wintergerst,Thomas Schultz,Kaushik Murali,Mohit Jain*

Main category: cs.CV

TL;DR: 提出CataractCompDetect框架与CataComp数据集，在白内障手术视频中自动检测罕见但高影响的术中并发症（虹膜脱垂、后囊破裂、玻璃体脱出），在53例数据上平均F1达70.63%。


<details>
  <summary>Details</summary>
Motivation: 术中并发症是白内障手术不良结局的重要来源，早期自动识别可用于预警与客观化培训；现有缺乏标注并发症的视频数据与有效检测框架。

Method: 构建CataractCompDetect：1) 相位感知的局部化（结合手术阶段先验缩小搜索空间）；2) 基于SAM 2的目标/区域跟踪；3) 针对并发症的风险评分模块；4) 视觉-语言推理进行最终分类。并发布首个标注并发症的白内障手术视频集CataComp（53例，含23例并发症）。

Result: 在CataComp上，平均F1=70.63%；分项F1：虹膜脱垂81.8%、后囊破裂60.87%、玻璃体脱出69.23%。

Conclusion: 融合结构化外科先验与视觉-语言推理能有效识别罕见但关键的术中并发症；数据集与代码将公开，促进该领域研究与临床预警/教学应用。

Abstract: Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.

</details>


### [247] [Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs](https://arxiv.org/abs/2511.18976)
*Huaming Ling,Ying Wang,Si Chen,Junfeng Fan*

Main category: cs.CV

TL;DR: 提出一种将通用CNN转为FHE友好网络的单阶段微调策略，并发明通用交错打包与配套算子，突破密文容量与非线性近似两大瓶颈，实现在CIFAR-10、ImageNet、MS COCO上的高精度同态推理，并首次把低阶多项式激活用于YOLO目标检测的FHE推理。


<details>
  <summary>Details</summary>
Motivation: FHE推理避免明文暴露但受两大痛点制约：1）ReLU等非线性在FHE中不可直接计算，需以低阶多项式近似但常损精度；2）密文槽位/容量限制导致高分辨率特征图难以处理，阻碍主流CNN在FHE端的可扩展与效率。

Method: - 单阶段微调（SFT）：直接用低阶多项式替换ReLU/SiLU，对预训练CNN进行一次性微调以恢复精度，避免多阶段复杂蒸馏或重训。
- 广义交错打包（GIP）：设计可适配几乎任意空间分辨率特征图的打包布局，并配套同态算子（卷积、激活、池化等）以保持GIP格式贯穿计算，从而提升带宽利用并绕过容量瓶颈。

Result: 在CIFAR-10、ImageNet、MS COCO上，使用SFT得到的FHE友好CNN与原始ReLU/SiLU基线精度相当；并首次在YOLO目标检测中实现基于低阶多项式激活的端到端FHE推理。

Conclusion: SFT显著降低将主流CNN迁移到FHE的工程门槛与训练开销；GIP与配套算子突破密文容量限制，支持高分辨率与多架构；两者结合为通用、高效的FHE CNN推理提供可行路径，并在检测任务上首次落地。

Abstract: We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.

</details>


### [248] [Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models](https://arxiv.org/abs/2511.18978)
*Santiago Moreno,Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: 提出ZEUS：一种利用冻结VLM与文本提示集，在全切片病理图像上实现零样本高分辨率分割的自动流程。通过将WSI切成重叠小块，提取视觉嵌入并与类特定文本嵌入做余弦相似度，融合生成肿瘤掩膜；在两套皮肤相关数据上获得有竞争力结果，显著降低标注负担。


<details>
  <summary>Details</summary>
Motivation: 皮肤肿瘤形态多变、良恶边界细微、形态学模式重叠，精确像素级标注昂贵且困难。现有VLM多停留在切片级或交互式粗粒度任务，难以在千兆像素WSI上产生细粒度分割，需要一种无需像素标注、可扩展、可解释的自动分割方案。

Method: 提出ZEUS零样本视觉-语言分割管线：冻结的VLM编码器；为每一类别构造文本提示集合；WSI切分为重叠patch，提取视觉嵌入；与文本嵌入计算余弦相似度并聚合，生成高分辨率分割掩膜。分析提示设计、域迁移与机构差异的影响。

Result: 在两套内部数据集（原发梭形细胞肿瘤与皮肤转移）上实现与现有方法相当的性能；证明了提示工程与域差异对VLM分割效果的显著影响。

Conclusion: ZEUS在无需像素级标注的前提下实现可扩展、可解释的肿瘤区域勾画，显著降低标注成本，适用于后续诊断流程；但性能受提示设计与跨机构域偏移影响，需要进一步稳健性研究。

Abstract: Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.

</details>


### [249] [UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection](https://arxiv.org/abs/2511.18983)
*Ching-Yi Lai,Chih-Yu Jian,Pei-Cheng Chuang,Chia-Ming Lee,Chih-Chung Hsu,Chiou-Ting Hsu,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出UMCL框架，通过把单一视觉模态生成三种互补特征，并用语义亲和对齐与跨质量相似性学习，实现跨压缩率的鲁棒深偽检测，性能与可解释性兼具。


<details>
  <summary>Details</summary>
Motivation: 社交平台存在不同程度的视频压缩，导致深偽检测在跨压缩率场景下泛化差；单模态在压缩下特征退化，多模态又成本高且模态质量/可得性不稳定，需要一种既用单一输入又能获得多模态鲁棒性的方案。

Method: 提出UMCL：从单一视觉模态自动生成三类互补特征——(1)抗压缩的rPPG信号；(2)人脸时序关键点动态；(3)来自预训练视觉-语言模型的语义嵌入。通过ASA（亲和驱动语义对齐）以亲和矩阵建模模态间关系，并用对比学习优化一致性；再用CQSL（跨质量相似性学习）提升不同压缩率下的特征稳健性。

Result: 在多种压缩率与多种操纵类型数据集上取得优于现有方法的SOTA性能，建立新的CCR基准；即便单一特征退化仍保持高准确率。

Conclusion: 单模态生成的多模态对比学习可有效缓解社交平台压缩带来的退化，兼顾鲁棒性与可解释性；ASA提供显式模态关系对齐，CQSL确保跨压缩率稳健，适用于真实复杂环境的深偽检测。

Abstract: In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.

</details>


### [250] [Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning](https://arxiv.org/abs/2511.18989)
*Wassim Benabbas,Mohammed Brahimi,Samir Akhrouf,Bilal Fortas*

Main category: cs.CV

TL;DR: 研究比较CNN、ViT与CLIP零样本模型在叶片病害分类中的域泛化能力，发现ViT较CNN更鲁棒，CLIP可通过文本描述实现零样本分类，具备强适应性与可解释性，适合作为实用的域迁移方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于PlantVillage等干净背景数据集训练的模型在真实田间图像上表现不佳，学术结果与应用需求存在显著落差，需要探索能跨域泛化的方法以提升农户提交图像的诊断效果。

Method: 针对“受控数据—真实场景”的域移位问题，对三类模型进行系统评估：传统CNN、具全局注意力的Vision Transformer，以及基于CLIP的图文对比预训练零样本模型；比较其在真实田间图像上的鲁棒性、泛化与可解释性。

Result: CNN在域移位下鲁棒性有限；ViT凭借捕获全局上下文的能力表现出更强的泛化；CLIP无需特定任务训练即可依据疾病文本描述完成分类，显示出较强适应性与可解释性。

Conclusion: 注意力架构（尤其ViT）提升泛化，零样本CLIP在复杂田间环境中具备实用且可扩展的域适配潜力，值得作为植物病害诊断的现实落地方案。

Abstract: Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.

</details>


### [251] [View-Consistent Diffusion Representations for 3D-Consistent Video Generation](https://arxiv.org/abs/2511.18991)
*Duolikun Danier,Ge Gao,Steven McDonagh,Changjian Li,Hakan Bilen,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: 提出ViCoDR，通过学习多视角一致的扩散表征，显著提升摄像机可控视频生成的3D一致性，减少物体因视角变化而变形的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在摄像机位姿变化时常出现三维不一致（如物体/结构变形），影响观看体验与仿真可靠性。近期工作显示表征对齐可改善扩散模型质量，因此作者假设：增强多视角一致性的扩散表征可提升视频的3D一致性。

Method: 对多种摄像机可控的视频扩散模型进行表征与输出的一致性分析，发现三维一致的内部表征与生成质量高度相关。基于此提出ViCoDR：在训练/微调中对扩散模型的时空/多视角表征施加一致性约束，使来自不同相机视角的潜在特征对齐，从而学习多视角一致的扩散表征。方法可用于图生视频、文生视频与多视图生成模型。

Result: 在多个任务（摄像机可控的图生视频、文生视频和多视图生成）上，ViCoDR显著提升视频的3D一致性，减少伪影与结构变形。实验证明表征一致性指标与三维一致视频质量之间存在强相关性。

Conclusion: 多视角一致的扩散表征是实现三维一致视频生成的关键。ViCoDR通过显式学习这类表征，在多种设置下均提升3D一致性与视觉稳定性，具有通用性并适用于现有模型。

Abstract: Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.

</details>


### [252] [AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization](https://arxiv.org/abs/2511.18993)
*Christos Koutlis,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 提出AuViRe方法：通过跨模态重建语音表征（用音频重构唇动或反之）来定位深度伪造在时间轴上的篡改片段，操纵段的重建误差显著放大，从而精确定位。实验在LAV-DF、AV-Deepfake1M及野外数据上显著超SOTA。


<details>
  <summary>Details</summary>
Motivation: 合成音视频内容愈发精细且可能包含细微恶意操纵，现有检测难以在时间维度精确定位伪造发生的片段，尤其在音画不一致或局部改动时。需要一种鲁棒、可泛化、可精细定位的时序深伪检测方法。

Method: 提出音视协同的语音表征跨模态重建框架AuViRe：从一种模态（如音频波形）重建另一模态的语音相关表示（如唇形/口型表征），并计算重建与真实表示的差异。由于伪造片段破坏音画对应关系，跨模态重建在被操纵段显著更难，误差增大。将该误差作为时序置信信号实现精细的篡改定位。

Result: 在三个基准上显著优于现有方法：LAV-DF上AP@0.95提升+8.9，AV-Deepfake1M上AP@0.5提升+9.6，野外实验AUC提升+5.1。

Conclusion: 跨模态语音表征重建能有效放大深伪片段的不一致性，提供稳健的时序定位信号，推动音视频深伪精细定位的性能上限；方法在多数据集和真实场景中均表现出强泛化性。

Abstract: With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.

</details>


### [253] [A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation](https://arxiv.org/abs/2511.19004)
*Wentao Qu,Guofeng Mei,Yang Wu,Yongshun Gong,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: 提出T2LDM文本到LiDAR扩散模型，通过自条件表示引导与新基准T2nuScenes提升细节、可控性与多条件适配，达成SOTA场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-LiDAR训练样本稀缺、文本质量参差，导致生成场景过于平滑、细节缺失且可控性差；缺少系统的可组合基准与衡量指标来研究文本提示对生成质量/可控性的影响。

Method: 提出T2LDM：在扩散框架中引入自条件表示引导（SCRG），训练时将去噪网络的中间表示与真实数据表示对齐，提供软监督以补充重建细节；推理时解耦引导，不增加开销。构建可内容组合的Text-LiDAR基准T2nuScenes与可控性度量；设计方向位置先验以缓解街景畸变；通过在冻结DN上学习条件编码器，使模型支持Sparse→Dense、Dense→Sparse、Semantic→LiDAR等多条件任务。

Result: 在无条件与多种条件生成任务上均优于现有方法，生成的3D场景更细致、几何结构更丰富、可控性更强，达到SOTA表现；实证分析给出有效的提示范式。

Conclusion: SCRG提升了文本到LiDAR生成的细节与结构感知，结合T2nuScenes基准与方向先验，T2LDM实现高保真且可控的场景生成，并具备多任务条件适配能力，适合下游3D应用。

Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.

</details>


### [254] [Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting](https://arxiv.org/abs/2511.19021)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min*

Main category: cs.CV

TL;DR: 提出Grc-ViT：依据图像复杂度自适应选择粗细粒度的ViT框架，在保证全局建模的同时更好捕获细节，提升精细辨识并兼顾精度与效率。


<details>
  <summary>Details</summary>
Motivation: ViT擅长全局依赖建模，但对细粒度局部细节表征不足；多尺度方法虽有改进，却依赖固定patch大小且计算冗余。需要一种能随图像复杂度动态调整视觉粒度、减少无效计算并提升细节表征的方案。

Method: 提出Granularity-driven ViT（Grc-ViT），包含两阶段：1）粗粒度评估（CGE）：利用边缘密度、熵、频域线索评估视觉复杂度，预测合适的patch与window大小；2）细粒度精炼（FR）：依据所选粒度调整注意力计算，实现更高效、精确的特征学习。引入两可学习参数α与η，端到端优化，在全局推理与局部感知之间自适应平衡。

Result: 综合实验显示，Grc-ViT在细粒度判别能力上更强，并在精度与计算效率之间取得更优折中（相较现有多尺度/混合方法）。

Conclusion: 动态按图像复杂度调节粒度、并在此基础上精炼注意力，可在不增加冗余开销的情况下增强局部细节建模，同时保持ViT的全局优势，达到更好的准确率—效率权衡。

Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.

</details>


### [255] [Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling](https://arxiv.org/abs/2511.19024)
*Long Tang,Guoquan Zhen,Jie Hao,Jianbo Zhang,Huiyu Duan,Liang Yuan,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出Life-IQA：通过GCN增强的层级交互与MoE特征解耦来改进无参考图像质量评估，较Transformer解码器在准确度与计算成本间达更优平衡并在多基准上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA多将主干网络的浅层与深层特征简单融合，忽视不同层对质量预测贡献不均；同时对“质量解码器”结构探索不足，影响性能与效率。

Method: 1) 设计GCN增强的层间交互：以经GCN处理的最深层特征作query，次深层特征作key/value，通过跨注意力实现信息交互与强化。2) 设计基于MoE的特征解耦模块：将融合表征送入多专家网络，不同专家专注特定失真类型或质量维度，实现表征解耦与专门化。整体构成高效质量特征解码框架Life-IQA。

Result: 在多个BIQA基准上达到SOTA；相较“普通Transformer解码器”，Life-IQA在精度与计算成本之间取得更优权衡。

Conclusion: 合理建模浅深层贡献并以GCN+跨注意力进行层交互，结合MoE实现特征解耦，可构建高效准确的BIQA质量解码器，推动无参考质量评估性能提升。

Abstract: Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \underline{l}ayer\underline{i}nteraction and MoE-based \underline{f}eature d\underline{e}coupling, termed \textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\texttt{Life-IQA}}.

</details>


### [256] [Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric](https://arxiv.org/abs/2511.19032)
*Xiangjie Sui,Songyang Li,Hanwei Zhu,Baoliang Chen,Yuming Fang,Xin Sun*

Main category: cs.CV

TL;DR: 提出Bench-C基准与RAS指标，聚焦高辨析样本与logit层面的结构退化，系统评估LVLM在视觉腐化下的鲁棒性并揭示失败/恢复模式。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM鲁棒性评测不足：数据集中低辨析样本占主导掩盖真实差距，且仅用准确率难以反映预测结构（不确定性与校准）的退化。

Method: 1) 构建Bench-C：通过样本选择策略同时考虑腐化前后预测不一致性与语义多样性，强调高辨析样本；2) 提出鲁棒性对齐分数RAS：在logit层面度量预测结构退化，综合不确定性变化与校准对齐偏移；3) 进行广泛实验，分解鲁棒性为破坏性与纠正性两个分量以分析模型行为模式。

Result: 发现：1) 模型在腐化下呈现可区分的行为模式，如错误自信与犹豫；2) 轻微腐化可能带来小幅准确率提升，但整体预测结构仍退化；3) 鲁棒性分解揭示不同模型在失败与恢复上的显著差异。

Conclusion: 基于Bench-C与RAS，可更真实、细粒度地衡量LVLM在腐化条件下的鲁棒性，发现传统准确率掩盖的结构性退化与模型差异，为诊断与改进提供方向。

Abstract: Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.

</details>


### [257] [ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay](https://arxiv.org/abs/2511.19033)
*Gengyuan Zhang,Mingcong Ding,Jingpei Wu,Ruotong Liao,Volker Tresp*

Main category: cs.CV

TL;DR: 提出ReEXplore：一种无需训练的具身探索框架，通过回顾式经验重放与分层前沿选择，显著提升MLLM驱动代理在未知环境中的探索成功率与效率（最高达3倍）。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的具身探索仍存在三大痛点：1）依赖预训练但陈旧的静态知识，难以适配新环境；2）模仿学习/强化学习在长时域、稀疏回报下训练成本高、效率低；3）基于前沿（frontier）的探索呈现大而细粒度的动作/候选空间，MLLM难以稳定做出可靠选择。

Method: 提出ReEXplore，完全训练免：1）回顾式经验重放（Retrospective Experience Replay）在推理时注入提炼的抽象经验，补充并更新MLLM的知识与策略；2）层级化前沿选择（Hierarchical Frontier Selection），将前沿排序分解为由粗到细的决策流程，先粗粒度筛选候选区域，再细粒度评估以降低复杂度并提高稳健性与可解释性。

Result: 在多项具身探索基准上，相较强MLLM基线，ReEXplore在开源骨干下实现高达3倍的成功率与导航效率提升，展现更强的稳健性与可追溯性。

Conclusion: 无需额外训练即可通过经验注入与层级决策显著增强MLLM驱动的具身探索，缓解知识陈旧、训练昂贵与决策维度过大问题，并在多基准上大幅优于强基线。

Abstract: Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.

</details>


### [258] [CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones](https://arxiv.org/abs/2511.19035)
*Kai Zhenga,Zhenkai Wu,Fupeng Wei,Miaolan Zhou,Kai Lie,Haitao Guo,Lei Ding,Wei Zhang,Hang-Cheng Dong*

Main category: cs.CV

TL;DR: 提出MC-DiSNet与CSD任务，结合DINOv3与多尺度跨注意力，用仅“变化区域语义像素”标注的新Gaza-Change数据集，在Gaza-Change与SECOND上验证，显著提升冲突区快速损伤评估能力。


<details>
  <summary>Details</summary>
Motivation: 冲突区损伤评估需要快速准确，但遥感场景中变化区域小、边界模糊、同类内部相似度高、语义变化不明确，且大规模时相标注昂贵。传统SCD依赖双时相全量语义标注，不经济也难以适配小范围破坏识别。

Method: 采用预训练DINOv3作为双时相特征提取骨干，构建多尺度跨注意力差分孪生网络（MC-DiSNet），在不同尺度上对两时相特征做跨注意力交互与差分以突出变化语义；提出只标注“变化区域语义像素”的任务设定CSD（变化语义检测），并发布高分辨率Gaza-Change数据集进行训练与评测。

Result: 在CSD框架下于Gaza-Change和SECOND数据集上取得优异性能（摘要未给出具体指标），验证了MC-DiSNet在小范围、边界模糊的冲突区损伤检测中的有效性与鲁棒性。

Conclusion: CSD将BCD直接扩展到语义层而无需大规模双时相语义标注；MC-DiSNet结合DINOv3与多尺度跨注意力有效缓解高内类相似与语义模糊问题，为冲突区快速损伤评估的实用落地提供了可行路径。

Abstract: Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.

</details>


### [259] [MedSAM3: Delving into Segment Anything with Medical Concepts](https://arxiv.org/abs/2511.19046)
*Anglin Liu,Rundong Xue,Xu R. Cao,Yifan Shen,Yi Lu,Xiang Li,Qianqian Chen,Jintai Chen*

Main category: cs.CV

TL;DR: 提出MedSAM-3：在SAM3上微调、支持文本提示的医疗图像/视频分割模型，并配合MLLM代理进行推理与迭代优化；在多模态医学影像上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法泛化性差、对新场景需大量手工标注，且多依赖几何提示（点/框），难以用开放词汇精准指定解剖结构；需要一种能以语义文本提示、跨模态泛化、并能自动化推理与微调的方案。

Method: 在SAM-3架构上用配对的医学图像与语义概念标签进行微调，获得“可文本提示的概念分割（PCS）”。同时提出MedSAM-3 Agent：将多模态大模型融入代理式闭环，执行复杂推理、生成与优化文本/几何提示，进行迭代细化，实现人-机在环工作流；在X-ray、MRI、超声、CT与视频上统一评估。

Result: 在多种医学影像模态与任务中，相较于专科模型与通用基础模型均取得显著提升（定量细节未给出），支持开放词汇文本定位目标并能在视频上稳定分割。

Conclusion: MedSAM-3把医学分割从几何提示扩展到语义文本提示，并通过代理式推理实现自动化迭代优化；展现了跨模态、跨任务的强泛化能力，具备实际临床应用潜力，代码与模型将开源。

Abstract: Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.

</details>


### [260] [Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation](https://arxiv.org/abs/2511.19049)
*Ruojun Xu,Yu Kai,Xuhua Ren,Jiaxiang Cheng,Bing Ma,Tianxiang Zheng,Qinhlin Lu*

Main category: cs.CV

TL;DR: 论文针对DPO在扩散模型（视频生成）中的似然位移问题，提出基于策略引导的PG-DPO（含自适应拒样缩放ARS与隐式偏好正则IPR），通过分析更新传播机制诊断两类失效模式并加以缓解，在实验中取得更优的定量与主观效果。


<details>
  <summary>Details</summary>
Motivation: DPO在对齐人类偏好方面有效，但存在“似然位移”：训练后被选样本的概率反而降低。该现象在自回归模型中已有研究，而在扩散模型（尤其视频生成）上鲜有系统分析，导致对齐效果与生成质量受限。作者动机是解释扩散框架中DPO的不良优化动态，并提出能稳定提升偏好对齐的改进方法。

Method: 提出在扩散模型中对DPO损失与策略更新的形式化分析框架，刻画单个训练样本更新对其他样本预测的影响（更新传播/影响函数视角），据此识别两类失效模式：小奖励差引发的优化冲突、大奖励差导致的次优极大化。基于分析提出PG-DPO：1）Adaptive Rejection Scaling（ARS）自适应放缩被拒样本的梯度/权重以缓解冲突；2）Implicit Preference Regularization（IPR）对偏好间距过大的情形加隐式正则，防止过度推高非目标方向并抑制似然位移。整体无须额外偏好模型，直接在扩散式视频生成中训练。

Result: 在视频生成任务上，PG-DPO在多项定量指标与主观评测上优于现有方法，并显著降低被选样本的似然位移，表现出更稳定的训练与更好的偏好对齐效果。

Conclusion: 似然位移在扩散模型的DPO中同样突出，源于奖励差异引发的两类失效模式。通过ARS与IPR构成的PG-DPO可有针对性地缓解这些问题，提升视频生成的偏好对齐与生成质量，提供了更稳健的训练范式。

Abstract: Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.

</details>


### [261] [LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space](https://arxiv.org/abs/2511.19057)
*Hai Wu,Shuai Tang,Jiale Wang,Longkun Zou,Mingyue Guo,Rongqin Liang,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 提出LAA3D：一个面向低空飞行器的3D检测与跟踪数据集与基准，并给出单目基线MonoLAA；含大规模真实+合成数据、统一评测协议，合成预训练可有效迁移到真实场景。


<details>
  <summary>Details</summary>
Motivation: 低空飞行器（eVTOL、微型无人机、直升机等）在城市/郊区等复杂场景的3D定位与行为理解需求增长，但缺乏专门的3D数据集与统一评测基准，限制了检测、跟踪与位姿估计研究与落地。

Method: 1) 构建LAA3D数据集：包含1.5万真实图像与60万合成帧，覆盖多类低空航行器与多场景；为每个实例标注3D框、类别与实例ID，支持3D检测、3D多目标跟踪与6-DoF位姿估计。2) 搭建LAA3D基准：提供多任务统一评测协议与对比方法。3) 提出单目3D检测基线MonoLAA：适配变焦相机、不同焦距，鲁棒3D定位；采用在合成数据上的预训练并对真实数据微调。

Result: MonoLAA在变焦相机设置下实现稳健的单目3D定位；在合成数据上预训练的模型经微调后可良好迁移到真实数据，展示出较强的sim-to-real泛化能力。

Conclusion: LAA3D填补低空飞行器3D感知数据缺口，提供统一基准与强力单目基线，促进3D检测、跟踪和位姿估计研究；合成到真实的迁移有效，为后续低空场景应用奠定基础。

Abstract: Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.

</details>


### [262] [Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation](https://arxiv.org/abs/2511.19062)
*Qiyang Yu,Yu Fang,Tianrui Li,Xuemei Cao,Yan Chen,Jianghao Li,Fan Min,Yi Zhang*

Main category: cs.CV

TL;DR: 提出Grc-SAM：以粒计算驱动的粗到细无提示图像分割框架，通过自适应粗定位+稀疏局部注意力细化+掩码潜在提示，提升SAM在无提示与高分辨率细粒度分割的准确性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有如SAM的预训练分割模型多在单一粒度上直接生成提示，存在两大痛点：缺乏自主区域定位（可定位性不足）与在高分辨率下细粒度建模受限（可扩展性差）。亟需一种能在无人工提示条件下实现可靠前景定位并兼顾高分辨率细节的框架。

Method: 构建Granular Computing驱动的粗到细框架Grc-SAM：1) 粗阶段：从特征中自适应提取高响应区域，实现前景精确定位，降低对外部提示依赖；2) 细阶段：更细的patch划分与稀疏局部swin式注意力以加强细节建模、支持高分辨率；3) 将细化后的掩码编码为潜在提示嵌入，送入SAM解码器，以自动化推理替代手工提示；并通过多粒度注意力把粒计算思想与ViT连接起来。

Result: 实验显示在准确性与可扩展性上均优于基线（含SAM），实现更强的无提示与高分辨率细粒度分割性能。

Conclusion: Grc-SAM通过粒度化的粗到细策略与潜在提示机制，克服SAM在自主定位与高分辨率细粒度建模的不足，为无提示分割提供了兼具精度与可扩展性的解决方案，并从粒计算角度丰富了视觉Transformer方法。

Abstract: Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.

</details>


### [263] [Understanding, Accelerating, and Improving MeanFlow Training](https://arxiv.org/abs/2511.19065)
*Jin-Young Kim,Hyojun Go,Lea Bogensperger,Julius Erbach,Nikolai Kalischek,Federico Tombari,Konrad Schindler,Dominik Narnhofer*

Main category: cs.CV

TL;DR: 论文研究MeanFlow训练动态：瞬时速度与平均速度的相互作用决定少步生成表现。提出分阶段、由短到长的平均速度训练方案，显著加快收敛并提升1步生成FID至2.87。


<details>
  <summary>Details</summary>
Motivation: MeanFlow通过联合学习瞬时速度与平均速度，实现少步高质量生成，但两者在训练中的互动机制不清晰，限制了方法的稳定性与效率，尤其在追求一步生成时对大间隔平均速度的学习难度高。

Method: 对两种速度的学习依赖关系进行实证与任务亲和性分析，得出三点关键观察；据此设计训练流程：先加速建立可靠的瞬时速度，然后逐步从小时间间隔的平均速度过渡到大间隔平均速度，动态调整训练重心。采用相同的DiT骨干（如DiT-XL），比较与常规MeanFlow基线。

Result: 在ImageNet 256x256上，使用相同DiT-XL骨干，1步（1-NFE）生成的FID从基线的3.43提升到2.87。另可在相同性能下将训练时间缩短约2.5倍，或在更小的DiT-L骨干下达到基线水平。

Conclusion: 学习动态表明：先稳固瞬时速度，再由短到长学习平均速度是实现平滑、大间隔平均速度、从而支持一步生成的关键。该训练策略带来更快收敛与显著更好的少步生成性能。

Abstract: MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.

</details>


### [264] [DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling](https://arxiv.org/abs/2511.19067)
*Timur Mamedov,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: 提出DynaMix，将少量人工标注的多相机数据与大规模单相机伪标签数据有效融合，通过动态适配数据结构与噪声，在百万级图像、十万级身份上高效训练，显著提升跨域行人重识别泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨域/未知环境的行人重识别在实际部署中常缺乏足够的多相机标注数据；单相机数据大量但伪标签噪声高且身份空间巨大，现有方法难以兼顾效率、鲁棒性与泛化。因此需要一种能够动态处理数据噪声、规模与混合来源的训练机制。

Method: 设计DynaMix框架，包含三模块：1) 在线重标注模块：在训练过程中对单相机伪标签进行动态校正与细化，降低噪声影响；2) 高效质心模块：在巨大身份空间下维护稳健的身份表示（质心/原型），提升聚类与对比学习稳定性与可扩展性；3) 数据采样模块：为每个mini-batch自适应配比多相机真实标注与单相机伪标注样本，控制学习难度并保证组内多样性。整体针对大规模高效运行而设计。

Result: 在大规模数据（百万级图像、十万级身份）上可高效训练，跨域Re-ID上多项基准与设置中稳定超越当前SOTA。

Conclusion: 通过动态重标注、稳健质心与精细采样，DynaMix有效融合真实与伪标注数据，显著提升一般化行人重识别表现，并具备良好的可扩展性与实用性。

Abstract: Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.

</details>


### [265] [DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation](https://arxiv.org/abs/2511.19071)
*Fangda Chen,Jintao Tang,Pancheng Wang,Ting Wang,Shasha Li,Ting Deng*

Main category: cs.CV

TL;DR: 提出DEAP-3DSAM，通过增强解码器与自动提示模块，在无需人工提示下实现更优3D医学分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有将SAM用于3D医学分割常用伪3D策略，导致空间特征丢失；且多数方法依赖人工提示，实际应用困难并需专家知识。需要一种既保留丰富3D空间信息、又能自动获取提示的方案。

Method: 提出DEAP-3DSAM：1) Feature Enhanced Decoder（特征增强解码器），融合原始图像特征与更丰富细致的空间信息以弥补伪3D带来的空间信息缺失；2) Dual Attention Prompter（双注意力提示器），通过空间注意力与通道注意力自动生成有效提示，免人工交互。

Result: 在四个公开腹部肿瘤分割数据集上达到SOTA，性能优于或持平基于手动提示的现有方法；消融实验（定量与可视化）验证两个模块的有效性。

Conclusion: DEAP-3DSAM在3D医学图像分割中实现了自动提示与空间信息增强的联合改进，在无需人工提示的前提下获得SOTA表现，具备更强实用性与泛化潜力。

Abstract: The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.

</details>


### [266] [Graph-based 3D Human Pose Estimation using WiFi Signals](https://arxiv.org/abs/2511.19105)
*Jichao Chen,YangYang Qu,Ruibo Tang,Dirk Slock*

Main category: cs.CV

TL;DR: GraphPose-Fi 利用图神经网络显式建模骨骼拓扑，从WiFi CSI中进行3D人体姿态估计，通过共享CNN编码、时空与天线注意力、以及GCN+自注意回归头，显著优于现有方法（在MM-Fi数据集）。


<details>
  <summary>Details</summary>
Motivation: 现有WiFi姿态估计多用端到端回归直接把CSI映射到3D关节坐标，忽略人体关节间的拓扑结构，限制了泛化与精度；需要一种能显式编码骨骼结构并充分利用多天线与时序信息的方法。

Method: 提出GraphPose-Fi框架：1) 在各天线上共享的CNN编码器抽取子载波-时间特征；2) 轻量注意力模块对时间和跨天线特征自适应加权；3) 图回归头将GCN层与自注意结合，捕获局部骨骼拓扑与全局依赖，从CSI到3D关节坐标；在MM-Fi数据集上评测。

Result: 在MM-Fi数据集的多种设置下，该方法显著优于现有WiFi-based HPE方法（具体指标未在摘要中给出）。

Conclusion: 显式建模骨骼拓扑并结合时序与多天线注意力能有效提升WiFi 3D姿态估计性能；GraphPose-Fi在公开数据集上验证了先进性，并开源代码以促进复现与扩展。

Abstract: WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.

</details>


### [267] [HABIT: Human Action Benchmark for Interactive Traffic in CARLA](https://arxiv.org/abs/2511.19109)
*Mohan Ramesh,Mark Azer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: HABIT 是一个将真实人类动作引入 CARLA 的高保真交互式交通基准，用于更真实地评测自动驾驶在行人互动中的安全与鲁棒性；结果显示多种 SOTA 代理在该基准上暴露出此前被忽略的碰撞、伤害风险与误刹车问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶仿真对人类行为的刻画过于简化，难以覆盖复杂、动态的行人意图与反应，导致安全与可靠性评估失真，无法揭示系统在真实互动场景中的失败模式。

Method: 构建 HABIT 基准：将来自动作捕捉与视频的真实人体运动，通过模块化、可扩展、物理一致的重定向管线，标准化为 SMPL 轨迹并无缝集成到 CARLA 与其 Leaderboard；从约3万条重定向动作中筛选出4730条与交通兼容的行人动作；提供自动化场景生成与评测，并设计安全指标（AIS伤害等级、FPBR误刹车率）。

Result: 在 HABIT 上评测 InterFuser、TransFuser、BEVDriver 等 SOTA 代理：尽管它们在 CARLA Leaderboard 上几乎零碰撞/公里，但在 HABIT 中碰撞可达7.43次/公里、AIS 3+ 伤害风险达12.94%，且误刹车比例最高达33%，暴露出先前基准未检出的失败模式。

Conclusion: 将高保真人体行为引入仿真能显著提升对自动驾驶行人互动安全性的检验力度；现有领先方法在更真实的人机互动场景下仍存在明显短板。HABIT 的数据、管线与评测公开，有助于可复现、以行人为中心的自动驾驶研究。

Abstract: Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.

</details>


### [268] [DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection](https://arxiv.org/abs/2511.19111)
*Hai Ci,Ziheng Peng,Pei Yang,Yingxin Xuan,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出DiffSeg30k数据集，用于对扩散模型局部编辑进行像素级定位与模型识别，推动AIGC检测从整图二分类转向语义分割，并给出基线与鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC检测多为整图真伪分类，难以定位由扩散模型引入的局部编辑，且对多轮编辑、真实场景、多模型来源下的细粒度检测缺乏基准。

Method: 构建DiffSeg30k：从COCO获取野外图像/提示；使用8个SOTA扩散模型进行局部编辑，支持最多三轮连续编辑；通过VLM自动选区并生成情境相关的编辑提示（添加/删除/属性修改）；提供像素级标注与编辑模型标签。以此将任务定义为语义分割并伴随编辑模型识别；基准评测三种分割方法并测试在失真条件下的鲁棒性与跨生成器泛化。

Result: 基线分割方法在像素级检测上面临显著挑战，尤其对图像失真敏感；尽管如此，这些分割模型在整图层面作为“是否被扩散编辑”的分类器表现强，优于既有伪造分类器，并在跨生成器上展现良好泛化。

Conclusion: DiffSeg30k为扩散编辑的细粒度检测提供标准数据与评测，证明分割范式可同时实现局部定位与模型识别，但当前方法鲁棒性不足；该数据集将推动更稳健的局部AIGC检测研究。

Abstract: Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k

</details>


### [269] [3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion](https://arxiv.org/abs/2511.19117)
*Minchong Chen,Xiaoyun Yuan,Junzhe Wan,Jianing Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 提出3M-TI：一种无需标定的多相机跨模态扩散框架，用RGB辅助提升移动端热像的超分辨率与纹理细节，在公开基准与真实设备上达SOTA，并显著提升检测与分割等下游任务。


<details>
  <summary>Details</summary>
Motivation: 移动端热成像受传感器小型化限制，空间分辨率与纹理保真差，单幅热图像SR难以恢复细节；RGB引导方法依赖精确且繁琐的跨相机标定，影响部署与鲁棒性。因此需要一种既能利用RGB结构信息，又不依赖显式标定的鲁棒SR方案。

Method: 提出3M-TI：在扩散模型的UNet中用跨模态自注意力模块（CSM）替换原自注意力层，使热/可见光特征在去噪各阶段自适应对齐与融合，利用扩散生成先验增强分辨率、结构与纹理，无需显式相机标定。方法在真实移动热相机与公开数据集上训练/评估。

Result: 在视觉质量与定量指标上取得SOTA；在真实场景中提升显著；用于下游目标检测与分割时有可观性能增益，证明其实用价值与鲁棒性。

Conclusion: 3M-TI通过跨模态自注意力与扩散先验，在无标定条件下有效融合RGB与热信息，显著提升热成像SR及下游感知性能，适合移动端部署。

Abstract: The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.

</details>


### [270] [MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images](https://arxiv.org/abs/2511.19119)
*Qirui Wang,Jingyi He,Yining Pan,Si Yong Yeo,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 提出MonoSR：一个涵盖室内、室外与物体场景、支持多类型问题的大规模单目空间推理数据集；基准评测显示现有视觉语言模型在单目SR上存在显著不足，并分析辅助信息的重要性与未来模型设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理研究多依赖多视角且偏室内，难以泛化到室外与真实世界常见的单目输入场景；缺乏开放世界、单目条件下的统一数据与评测基准。

Method: 构建MonoSR数据集，覆盖多样场景（室内、室外、物体级）与多种问题类型；在该基准上系统评测先进视觉-语言模型；进行消融/对比以检验辅助信息（如深度、语义、几何先验等）对单目SR的作用，并总结设计要点。

Result: 基线VLM在MonoSR上表现不佳，暴露出对单目3D理解与跨场景泛化能力的不足；实验表明适当的辅助信息可显著提升单目空间推理性能。

Conclusion: MonoSR为开放世界单目空间推理提供了标准化数据与评测平台；当前方法仍有较大改进空间，未来应更好地融合几何与语义先验及辅助信息，以提升真实世界单目SR能力。

Abstract: Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.

</details>


### [271] [When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP](https://arxiv.org/abs/2511.19126)
*Beilin Chu,Weike You,Mengtao Li,Tingting Zheng,Kehan Zhao,Xuan Xu,Zhigao Lu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CV

TL;DR: 提出SemAnti：通过Patch Shuffle抑制语义偏置、仅微调对伪造痕迹敏感层，显著提升CLIP在跨域AI图像检测中的鲁棒性并达SOTA。


<details>
  <summary>Details</summary>
Motivation: CLIP类检测器虽然泛化好，但往往依赖高层语义而非生成器伪迹，导致分布移位时性能脆弱；需要一种方法减弱语义依赖、放大伪迹特征以提升跨域检测能力。

Method: 1) 重新审视语义偏置，发现对CLIP输入做Patch Shuffle能打破全局语义连续性、保留局部伪迹线索，降低语义熵并拉近真/假图特征分布；2) 通过层级分析表明CLIP的深层语义结构在抑制语义后可稳定跨域表征；3) 提出SemAnti：冻结语义子空间，仅在打乱语义（Patch Shuffle）条件下微调对伪迹敏感的层，进行语义拮抗式微调。

Result: 在AIGCDetectBenchmark与GenImage上取得SOTA的跨域泛化表现，证明所提范式在不同生成模型与分布下均显著提升鲁棒性。

Conclusion: 调控并削弱语义、突出伪迹是释放CLIP用于AI生成图像检测潜力的关键；SemAnti以简单微调策略实现强跨域泛化，可作为鲁棒AIGC检测的新范式。

Abstract: The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.

</details>


### [272] [MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery](https://arxiv.org/abs/2511.19134)
*Shuyu Cao,Minxin Chen,Yucheng Song,Zhaozhong Chen,Xinyou Zhang*

Main category: cs.CV

TL;DR: 提出MambaRefine-YOLO，用双门控跨模态融合与分层特征聚合，在多模态DroneVehicle达83.2% mAP、较基线+7.9%，在单模态VisDrone仅用HFAN也有显著提升，兼顾精度与速度，适合UAV小目标检测。


<details>
  <summary>Details</summary>
Motivation: UAV图像小目标受低分辨率与复杂背景影响显著；RGB与红外融合可互补，但现有方法在跨模态交互充分性与计算效率之间权衡困难，难以在真实场景中兼顾精度与速度。

Method: 1) 设计DGC-MFM（Dual-Gated Complementary Mamba Fusion）：基于光照感知门与差异感知门，自适应平衡RGB/IR贡献，利用Mamba式时空/序列建模提高跨模态交互效率；2) 提出HFAN（Hierarchical Feature Aggregation Neck）：“先精炼、后融合”的多尺度特征增强策略，改进Neck阶段的信息聚合；并提供仅HFAN的单模态变体。

Result: 在DroneVehicle双模态数据集上取得83.2% mAP，较基线提升7.9%，达到SOTA；在VisDrone单模态上，仅使用HFAN也带来显著性能增益。整体在准确率与速度间实现更优平衡。

Conclusion: 双门控跨模态融合配合分层特征聚合，可有效缓解UAV小目标检测中的分辨率与背景扰动问题；方法通用于单/多模态场景，具备工程可落地性。

Abstract: Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.

</details>


### [273] [FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation](https://arxiv.org/abs/2511.19137)
*Zhifeng Xie,Keyi Zhang,Yiye Yan,Yuling Guo,Fan Yang,Jiting Zhou,Mengtian Li*

Main category: cs.CV

TL;DR: 提出FilmSceneDesigner：从文本自动生成电影布景的系统，包含代理链参数生成+程序化建模流水线，并配套电影资产库SetDepot-Pro，实验与人评显示结构合理、电影感强，可用于预演、施工图与情绪板。


<details>
  <summary>Details</summary>
Motivation: 传统电影布景建模高度依赖专家手工，耗时耗力，难以规模化与快速迭代；需要一种从自然语言高效、可靠地生成可用电影场景的方法，以对接影视前期预演与美术流程。

Method: 1) 代理式链式框架：根据文本描述（场景类型、时代、风格）生成符合美术工作流的结构化参数，使用提示策略保证准确与一致。2) 程序化生成流水线：按参数依次完成平面与结构生成、材质分配、门窗布置、物体检索与布局，端到端从零构建场景。3) 数据资源：构建SetDepot-Pro，包含6,862个电影化3D资产与733种材质，提升真实感与多样性。

Result: 系统可自动生成结构合理、具有电影感的场景；通过实验与人工评估验证其质量，并能支持预演（previz）、施工图与情绪板等下游任务。

Conclusion: FilmSceneDesigner将文本到电影布景的流程自动化，结合代理链参数化与程序化建模并辅以专用资产库，实现高保真、可用的电影场景生成，具备实际制作价值。

Abstract: Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.

</details>


### [274] [ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation](https://arxiv.org/abs/2511.19145)
*Dongha Lee,Jinhee Park,Minjun Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: ABM-LoRA提出一种针对LoRA的“激活边界匹配”初始化，通过在微调前让适配器的激活边界与预训练模型对齐，降低初始信息丢失、降低起始损失并加速收敛；在GLUE、WizardLM、VTAB-1K等多任务多模态上验证有效并在VTAB-1K上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA虽参数高效，但随机初始化使其梯度仅在与全参最优方向不匹配的切空间内更新，导致早期优化信息投影弱、损失高、收敛慢。需要一种在不增加显著开销的前提下，让LoRA子空间更好承接全参梯度信息的初始化策略。

Method: 提出Activation Boundary Matching (ABM) 初始化：在下游训练前，将LoRA适配器调至与预训练模型在激活边界（如非线性分段的判别超平面）上保持对齐，使得全参梯度在LoRA低秩子空间上的投影最大化。实质是通过对齐关键层的激活边界而非随机初始化，从而减少信息丢失。

Result: 在T5-Base的GLUE语言理解、LLaMA2-7B的对话生成（WizardLM数据）、ViT-B/16的VTAB-1K视觉识别等多种架构/任务上均显著加速收敛并提升初始性能；在VTAB-1K上达到最高准确率，尤其在需要几何推理的结构化任务上收益明显。

Conclusion: 通过在训练前对齐激活边界，ABM-LoRA显著提高了LoRA的优化效率与最终性能，证明精心设计的初始化可大幅缓解低秩子空间与全参优化方向不匹配的问题，并在跨模态任务上通用有效。

Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.

</details>


### [275] [Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation](https://arxiv.org/abs/2511.19147)
*Huisoo Lee,Jisu Han,Hyunsouk Cho,Wonjun Hwang*

Main category: cs.CV

TL;DR: 提出CoMA：协同多基础模型的源数据不可用域自适应框架，双向对齐并知识迁移CLIP与BLIP等互补FM，引入分解互信息(DMI)稳健小批量训练，显著优于SOTA于多个基准与多种设定。


<details>
  <summary>Details</summary>
Motivation: 单一基础模型在SFDA中易偏向有限语义覆盖，难以在域移位下捕获多样上下文；需要融合多FM的互补全球语义与局部上下文并稳定训练。

Method: CoMA：利用两种互补FM（如CLIP、BLIP）与目标模型进行双向适配：(1) 与目标模型进行对齐以完成任务，同时保持FM语义差异性；(2) 将FM的互补知识迁移到目标模型。为稳定小批量训练，提出分解互信息DMI，选择性增强真实依赖、抑制由于类别覆盖不全导致的伪依赖。

Result: 在Office-31、Office-Home、DomainNet-126、VisDA四个闭集基准上全面超越现有SOTA，并在partial-set与open-set变体上同样取得最佳表现。

Conclusion: 多FM协同与DMI能在SFDA中有效结合全球语义与局部上下文并缓解训练不稳定，带来稳健且优越的跨域迁移性能。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.

</details>


### [276] [From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation](https://arxiv.org/abs/2511.19149)
*Moazzam Umer Gondal,Hamad Ul Qudous,Daniya Siddiqui,Asma Ahmad Farhan*

Main category: cs.CV

TL;DR: 提出一种检索增强的时尚图像文本生成系统：用YOLO定位多服饰、聚类提取主色、CLIP-FAISS推断面料与性别，并把这些证据与风格示例一起喂给LLM生成更真实的描述与话题标签；性能与BLIP基线对比更少幻觉、更可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统端到端图像描述器在时尚领域易出现属性不准确（如品类、面料、颜色等）与跨域泛化差的问题，导致文案缺乏事实支撑与风格多样性。需要一个既具视觉对齐、又可解释、且可扩展到多服饰与多风格场景的生成范式。

Method: 构建检索增强流水线：1) YOLO进行多服饰检测定位（9类）；2) k-means从检测框提取主色；3) 基于CLIP特征+FAISS从结构化商品库检索，推断面料与性别等属性并取风格示例；4) 将检测与检索得到的“事实证据包”连同示例提示给LLM，生成描述与语境化标签；并以微调BLIP作为监督基线对比。

Result: YOLO在9类服饰mAP@0.5=0.71；RAG-LLM生成的文案在属性对齐上更佳，平均属性覆盖0.80；在标签生成任务上以50%阈值达到完整覆盖；相比之下，BLIP具有更高词汇重合但泛化较差。

Conclusion: 检索增强生成在时尚内容生成中能显著提升事实对齐、减少幻觉、并具可解释性与可扩展性，优于纯端到端模型；适用于多服饰、多风格与多品类的实际部署场景。

Abstract: This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

</details>


### [277] [Test-Time Preference Optimization for Image Restoration](https://arxiv.org/abs/2511.19169)
*Bingchen Li,Xin Li,Jiaqi Xu,Jiaming Guo,Wenbo Li,Renjing Pei,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出一种测试时偏好优化（TTPO）范式，在不重新训练和不依赖预收集偏好数据的前提下，使任意图像复原模型的输出更符合人类偏好。方法：对初复原结果做扩散反演与去噪生成候选，利用自动或人反馈做偏好选择，并以此作为奖励引导扩散去噪优化最终图像。多任务与多骨干上验证有效与灵活。


<details>
  <summary>Details</summary>
Motivation: 现有基于L1/LPIPS训练或零样本IR方法常与人类主观喜好不一致，导致复原图像虽客观指标尚可但观感欠佳；同时希望跨任务/骨干无须再训练，且避免昂贵的偏好数据采集。因此需要一种在测试时即可提升感知质量并对人类偏好自适应对齐的通用机制。

Method: 提出测试时偏好优化（TTPO）：训练无关、三阶段流程。(1) 以初始复原结果为条件进行扩散反演并去噪，在线生成多张“偏好候选图”；(2) 通过自动的偏好对齐指标（或人工反馈）从候选中选出“偏好/非偏好”样本；(3) 将所选样本作为奖励信号，指导后续扩散去噪轨迹，使输出沿着偏好梯度优化，得到更符合偏好的复原图像。方法可无缝适配任意IR骨干。

Result: 在多类图像复原任务（如去噪、去雨、去模糊等）与多种模型骨干上，TTPO显著提升感知质量与主观偏好一致性，相较预训练和零样本基线更优；同时展示出对不同退化分布和模型的广泛适配性。

Conclusion: TTPO实现了无需再训练与无需预建偏好数据的测试时偏好对齐，为IR提供一个通用即插即用的感知质量增强方案，能灵活应用于多任务多骨干，并更好符合人类偏好。

Abstract: Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.

</details>


### [278] [MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes](https://arxiv.org/abs/2511.19172)
*Kehua Chen,Tianlu Mao,Zhuxin Ma,Hao Jiang,Zehao Li,Zihan Liu,Shuqi Gao,Honglong Zhao,Feng Dai,Yucheng Zhang,Zhaoqi Wang*

Main category: cs.CV

TL;DR: MetroGS提出一种面向复杂城市场景的高效稳健重建框架：以分布式2D Gaussian Splatting为统一表示，结合结构化致密初始化、稀疏补偿、渐进式混合几何优化与深度引导的外观建模，在大规模数据集上实现更高几何精度与渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting在大规模场景（尤其是城市环境）中仍面临几何精度不足、稀疏区域重建不完整、外观不一致与优化不稳定、效率欠佳等难题。需要一个在复杂稀疏视角分布下仍能稳定、统一、可扩展地提高几何与外观质量的方案。

Method: 1) 以分布式2D Gaussian Splatting为统一骨干表示，便于大规模与并行处理；2) 结构化致密增强：结合SfM先验与pointmap模型进行更致密的初始化，并设计稀疏性补偿机制提高覆盖与完整性；3) 渐进式混合几何优化：将单目与多视几何优化有机融合，分阶段提升几何的效率与精度；4) 深度引导的外观建模：学习具3D一致性的空间特征，实现几何—外观解耦，缓解大场景外观不一致。

Result: 在大规模城市数据集上，MetroGS在几何精度与渲染质量上优于现有方法，并展现出统一、高效与稳定的重建能力。

Conclusion: MetroGS通过统一表示与多模块协同（致密初始化+稀疏补偿+混合几何优化+深度引导外观）有效解决大规模城市场景中的几何与外观难题，提供高保真、稳健、可扩展的重建方案。

Abstract: Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.

</details>


### [279] [Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification](https://arxiv.org/abs/2511.19180)
*Mansur Ozaman*

Main category: cs.CV

TL;DR: 论文比较三种来源相机识别（SCI）方法：PRNU、JPEG压缩伪影分析、以及CNN，并以设备分类准确率为核心指标评估；同时讨论其在真实应用中落地所需的技术发展。


<details>
  <summary>Details</summary>
Motivation: 在取证、版权保护、溯源与内容真实性检测中，需要从图像中判定拍摄设备。不同方法在鲁棒性、可解释性、与实际部署成本上差异明显，缺乏统一比较。本研究旨在系统评估主流SCI技术并为实际场景提供选择依据。

Method: 选取三类技术路线：1) 传统基于传感器噪声的PRNU；2) 基于编码残留的JPEG压缩伪影分析；3) 数据驱动的卷积神经网络。以设备分类准确率为主要指标进行对比，并讨论其对真实场景（如压缩、后处理、跨设备同型号等）的适应性与工程实现需求。

Result: 三种方法在准确率上各有优劣：PRNU在干净原图与同型号区分上表现强，但对强压缩/后处理敏感；JPEG伪影法在压缩图上有效但受再压缩与编辑影响；CNN在多样条件下具备较强判别力，但依赖大数据、易受域偏移影响。总体呈现数据条件与预处理对性能的显著影响。

Conclusion: SCI没有“一招通吃”的方案：PRNU适合高质量图像与法证取证，JPEG伪影法适合面向压缩平台的快速筛查，CNN适合大规模、复杂场景但需数据与正则化以防过拟合。实际落地需多模态融合、鲁棒特征学习、标准化数据集与评测协议，并考虑隐私与可解释性。

Abstract: One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.

</details>


### [280] [nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation](https://arxiv.org/abs/2511.19183)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jaeger,Fabian Isensee,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 提出nnActive开源主动学习框架，系统纠正评估陷阱，并在四数据集三标注策略上大规模验证：改进的“前景感知随机采样”成为强基线，常规AL虽优于标准随机但难以稳定超越该基线；预测熵整体最优但标注成本高，计算更强可提升AL效果。


<details>
  <summary>Details</summary>
Motivation: 3D生物医学分割标注成本高且需专家，现有主动学习是否优于随机抽样无共识；评估普遍存在数据与预算过少、用2D模型处理3D且无部分标注、随机基线不合理、仅以体素计成本等四类陷阱，导致结论不可靠。

Method: 提出nnActive：1) 在四个3D生物医学数据集、三种标注制度上大规模评测；2) 基于nnU‑Net，支持部分标注训练与3D patch级查询；3) 设计前景感知的随机采样策略以应对前景-背景失衡；4) 提出前景效率指标，衡量背景区域低标注成本；并比较多种AL策略（如预测熵），分析计算预算与设计选择的影响。

Result: 实验显示：A) 所有AL方法均优于“标准随机”，但无一能稳定优于“前景感知随机”；B) AL收益依赖具体任务参数（数据集、标签制度、预算等）；C) 预测熵总体表现最好，但往往需要更多标注工作量；D) 增加计算量与更重设计可提升AL表现。

Conclusion: nnActive提供了面向3D医学影像分割的系统化、可复现的AL评测与实现：应采用前景感知随机作为强基线，结合前景效率等更合理的成本度量；在实际应用中，需根据任务特点与可用计算/标注资源选择AL策略，预测熵虽强但代价高。

Abstract: Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive

</details>


### [281] [SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection](https://arxiv.org/abs/2511.19187)
*Nithira Jayarathne,Naveen Basnayake,Keshawa Jayasundara,Pasindu Dodampegama,Praveen Wijesinghe,Hirushika Pelagewatta,Kavishka Abeywardana,Sandushan Ranaweera,Chamira Edussooriya*

Main category: cs.CV

TL;DR: 提出一个基于EfficientNet-B6的轻量级二分类深伪图像检测模型，通过数据增强与处理缓解类别极不平衡，取得高准确率与良好泛化；傅里叶相位/幅度特征增益不明显，但整体框架对非专业者友好。


<details>
  <summary>Details</summary>
Motivation: 深伪图像广泛传播带来虚假信息风险，现有方法往往复杂或对分布外泛化差，且数据集类别极度不平衡，亟需一个轻量、稳健、易用并能应对不平衡与域外泛化的检测方案。

Method: 以EfficientNet-B6为骨干进行二分类微调，采用稳健预处理、过采样与多种优化策略（含数据增强/变换以缓解类别不平衡）；尝试引入基于傅里叶变换的相位与幅度特征融合，但效果有限。

Result: 模型在准确率、稳定性与泛化能力方面表现优异（相对基线显著提升），且在非专家使用场景中易于部署；傅里叶特征加入带来的性能提升很小或不显著。

Conclusion: 该轻量框架能有效检测深伪图像并具有良好泛化与可用性；虽然频域特征未显著改善性能，但整体方案推动了可及、可靠的深伪检测应用。

Abstract: Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.

</details>


### [282] [Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks](https://arxiv.org/abs/2511.19198)
*Ann-Sophia Müller,Moonkwang Jeong,Meng Zhang,Jiyuan Tian,Arkadiusz Miernik,Stefanie Speidel,Tian Qiu*

Main category: cs.CV

TL;DR: 提出一种基于物理器官模型与3D GAN的自动化流程，生成和扩增前列腺等软组织的3D解剖数据，用于手术规划/训练；通过定制超声和分割网络重建网格并提供术后反馈。


<details>
  <summary>Details</summary>
Motivation: 真实患者3D解剖数据获取受法律、伦理与技术限制，尤其是超声对比度差的软组织（如前列腺），限制了ML在手术规划与训练中的应用，需要一种可规模化、可控、具多样性的3D数据来源。

Method: 构建具多区域对比的仿生水凝胶前列腺实体模型；在定制超声平台中于术前/术后成像；用神经网络对超声图像进行分割（并与传统CV方法比较）；根据分割进行3D网格重建并生成性能反馈；同时训练3D GAN以在所得数据流形上进行形状生成，扩充下游3D任务数据。

Result: 在超声分割上，学习式方法的IoU优于传统非学习式方法；基于分割可重建高质量3D网格；3D GAN产生有用的形状样本以丰富数据集；流程可完整闭环地从物理模型到数字3D数据与反馈。

Conclusion: 物理仿生器官+定制成像+深度分割+3D重建+3D GAN的端到端流程，为软组织手术规划与训练提供可扩展的数据生成方案，减少对真实患者数据依赖，并提升下游ML任务的数据可用性与多样性。

Abstract: Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.

</details>


### [283] [CLASH: A Benchmark for Cross-Modal Contradiction Detection](https://arxiv.org/abs/2511.19199)
*Teodora Popordanoska,Jiameng Li,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 提出CLASH基准，用COCO图像配对含受控矛盾的字幕与问题，评测并提升多模态跨模态矛盾检测能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界常有图像与文本不一致，但现有基准大多假设一致，无法评估模型发现跨模态矛盾的能力，导致幻觉与不可靠。需要一个系统化数据集来诊断与提升这一关键能力。

Method: 构建CLASH：以COCO图像为基础，自动生成并控制对象级与属性级矛盾的字幕；为样本配套针对性问题，含多选与开放式两种评测形式；提供大规模经自动质量过滤的微调集与小规模人工核验的诊断集。对SOTA模型进行系统评测分析偏置与薄弱点，并进行有针对性的微调实验。

Result: 现有多模态SOTA在识别图文冲突上表现不佳，呈现系统性模态偏置与类别特定弱点。对CLASH进行定向微调可显著提升冲突检测能力。

Conclusion: 跨模态矛盾检测是可靠多模态AI的基础能力。CLASH作为包含受控矛盾与多样评测形式的基准与微调资源，可揭示并缓解模型偏置与弱点；在其上微调能有效增强冲突识别性能。

Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.

</details>


### [284] [Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?](https://arxiv.org/abs/2511.19200)
*Itay Cohen,Ethan Fetaya,Amir Rosenfeld*

Main category: cs.CV

TL;DR: 提出RoLA数据集，研究“像某物但并非该物”的判断；在CLIP嵌入空间估计“真实↔相似物”方向，用于区分与生成任务，提升跨模态检索与图文生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型虽在识别上表现强，但缺乏人类对“拟态/相似物”（玩具、雕塑、涂鸦、空想性错视等）与真实实体区分的细腻能力；需要检验和增强CLIP等视觉-语言模型在这方面的感知辨析力。

Method: 1) 构建RoLA数据集，包含多类别的真实与相似物样本配对；2) 用“real/ lookalike”提示词做基线评估；3) 在CLIP嵌入空间中估计一条能在“真实↔相似物”之间移动表示的方向向量，并将其应用到图像与文本嵌入；4) 将该方向用于跨模态检索与前缀式CLIP图文生成（captioner）以测试效用。

Result: 沿该方向调整嵌入后，Conceptual12M上的跨模态检索区分度提升；在前缀式CLIP描述生成中，生成的caption更能反映“真实/相似物”的差异。

Conclusion: CLIP嵌入空间中存在可线性操控的“真实-相似物”语义方向，利用该方向可增强模型对“看起来像但不是”的辨识，并改善检索与描述质量。

Abstract: Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.

</details>


### [285] [NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting](https://arxiv.org/abs/2511.19202)
*Brent Zoomers,Florian Hahlbohm,Joni Vanherck,Lode Jorissen,Marcus Magnor,Nick Michiels*

Main category: cs.CV

TL;DR: 提出一种针对3D Gaussian Splatting的可见性学习与实例化光栅化方法，实现视锥内的神经遮挡剔除，结合实例化软件光栅器与Tensor Core加速，在多资产组合场景中以更低VRAM和更高画质优于现有方法，并与LoD互补。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯渲染可用视锥剔除与层次细节加速，但因半透明特性难以使用效果显著的遮挡剔除，导致包含大量原语的大场景渲染效率受限。需要一种能在不破坏半透明特性的前提下实现有效遮挡剔除的机制，并在多实例/资产复用场景中具备可扩展性与内存效率。

Method: 学习每个高斯的视点相关可见性函数：为场景中资产共享一个小型MLP，输入视点与高斯信息，输出可见性；在光栅化前对视锥内高斯进行神经查询，剔除不可见（被遮挡）原语。将这一查询集成到新的“实例化软件光栅器”中，并利用Tensor Cores加速MLP推理与并行处理，从而在渲染管线早期执行遮挡剔除。

Result: 在组合（多资产实例）场景上，相比现有最先进方法，所提方法在VRAM占用更低、图像质量更好；与LoD技术结合或并行使用时表现互补，整体渲染效率和可扩展性提升。

Conclusion: 通过学习可见性函数的神经遮挡剔除与实例化软件光栅化相结合，弥补了3D高斯半透明导致无法做传统遮挡剔除的缺陷，显著改进大规模场景渲染的性能与质量，并与现有LoD方案形成互补。

Abstract: 3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.

</details>


### [286] [ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment](https://arxiv.org/abs/2511.19217)
*Wanjiang Weng,Xiaofeng Tan,Junbo Wang,Guo-Sen Xie,Pan Zhou,Hongsong Wang*

Main category: cs.CV

TL;DR: 提出ReAlign：在扩散采样过程中用“步感知”奖励模型引导，显著提升文本-动作对齐与动作质量。


<details>
  <summary>Details</summary>
Motivation: 扩散式文本到动作生成虽多样且逼真，但文本与动作分布存在错配，导致语义不一致或质量欠佳，需要在采样阶段动态纠偏并兼顾语义一致性与运动真实感。

Method: 设计Reward-guided sampling Alignment（ReAlign）：1）步感知（step-aware）奖励模型，在每个去噪时刻评估当前候选动作的对齐质量；2）奖励由两部分组成：文本对齐模块保证语义一致，动作对齐模块保证运动真实；3）在采样中使用奖励引导策略，将扩散过程朝最优对齐分布推进，在每一步对噪声样本进行细化，平衡概率密度与对齐程度。

Result: 在动作生成与检索两类任务上，较现有SOTA显著提升文本-动作对齐度与动作质量（更高一致性与逼真度、更多样性）。

Conclusion: 通过在扩散采样阶段引入步感知奖励与对齐引导，ReAlign有效缓解文本-动作分布错配问题，获得更高质量且语义一致的3D动作；方法通用于生成与检索场景。

Abstract: Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.

</details>


### [287] [Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering](https://arxiv.org/abs/2511.19220)
*Federico Felizzi,Olivia Riccomi,Michele Ferramola,Francesco Andrea Causio,Manuel Del Medico,Vittorio De Vita,Lorenzo De Mori,Alessandra Piscitelli Pietro Eric Risuleo,Bianca Destro Castaniti,Antonio Cristiano Alessia Longo,Luigi De Angelis,Mariapia Vassalli,Marcello Di Pumpo*

Main category: cs.CV

TL;DR: 研究评估前沿大型视觉语言模型在意大利语医学VQA中的“看图”依赖度。将需图像解读的问题替换为空白图，比较有图/无图的准确率差异，发现模型间视觉依赖显著不同。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在医学VQA上表现亮眼，但其是否真正依赖视觉信息仍存疑，尤其在临床高风险场景中必须验证模型是否进行真实的视觉扎根而非依赖文本捷径。

Method: 选取EuropeMedQA意大利语数据集中60道明确需要图像解读的问题。对四个SOTA模型（Claude Sonnet 4.5、GPT-4o、GPT-5-mini、Gemini 2.0 flash exp）分别在“原始图像”和“空白占位图”两种条件下作答，比较准确率并分析模型生成的推理内容。

Result: GPT-4o表现出最强视觉依赖：准确率从83.2%降至55.3%，下降27.9个百分点。GPT-5-mini、Gemini、Claude下降分别为8.5、2.4、5.6个百分点，仍保持较高准确率。所有模型在无图条件下仍给出自信但虚构的“视觉”解释。

Conclusion: 不同VLM在视觉扎根能力与鲁棒性上差异显著；部分模型可能依赖文本模式匹配而非真实图像理解。临床部署前需采用更严格的多模态评测与防幻觉机制。

Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.

</details>


### [288] [Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving](https://arxiv.org/abs/2511.19221)
*Jianhua Han,Meng Tian,Jiangtong Zhu,Fan He,Huixin Zhang,Sitong Guo,Dechang Zhu,Hao Tang,Pei Xu,Yuze Guo,Minzhe Niu,Haojie Zhu,Qichao Dong,Xuechao Yan,Siyuan Dong,Lu Hou,Qingqiu Huang,Xiaosong Jia,Hang Xu*

Main category: cs.CV

TL;DR: 提出Percept-WAM：将2D/3D感知隐式融入单一VLM，通过World-PV/World-BEV令牌统一感知与定位，配合网格条件预测、IoU感知评分与并行自回归解码，显著提升长尾、远距、小目标稳定性；在COCO与nuScenes上达SOTA级表现，并提升规划指标，兼具开放词汇与泛化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的空间感知易在长尾与复杂交互中失稳；现有VLM空间落地与定位能力弱，导致VLA系统感知/定位受限，难以可靠地支持规划与控制。

Method: 构建Percept-WAM世界-感知-行动一体模型：1) 用World-PV与World-BEV令牌统一2D/3D感知，显式编码空间坐标与置信度；2) 提出基于网格的条件预测用于密集目标感知；3) 采用IoU感知评分与并行自回归解码以提升稳定性与效率；4) 继承预训练VLM参数以保留通用推理能力，并可直接输出感知结果与轨迹控制。

Result: 在COCO 2D检测达51.7 mAP，在nuScenes BEV 3D检测达58.9 mAP；与轨迹解码器结合，在nuScenes与NAVSIM上提升规划性能，NAVSIM上PMDS较DiffusionDrive提升2.1；定性结果显示开放词汇与长尾泛化更强。

Conclusion: 将2D/3D感知能力内嵌入VLM并以结构化令牌与新解码机制统一表征与预测，可在不牺牲通用智能的前提下提升感知鲁棒性与规划表现，对VLA一体化自动驾驶有实用价值。

Abstract: Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.

</details>


### [289] [Learning Plug-and-play Memory for Guiding Video Diffusion Models](https://arxiv.org/abs/2511.19229)
*Selena Song,Ziming Xu,Zijun Zhang,Kun Zhou,Jiaxian Guo,Lianhui Qin,Biwei Huang*

Main category: cs.CV

TL;DR: 提出DiT-Mem：给DiT视频扩散模型接入可插拔记忆，提升物理规律与常识动态遵循，同时提高画质与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有DiT视频生成虽清晰连贯，却常违背物理与常识，缺乏显式世界知识。受LLM上下文记忆启发，作者希望通过外部记忆向量为DiT注入世界知识并实现可控引导。

Method: 实证发现：对DiT隐状态的干预可引导生成；在嵌入空间中的低/高通滤波可自然分离低层外观与高层物理/语义线索。基于此，提出记忆编码器DiT-Mem：由堆叠3D CNN、低/高通滤波、自注意力组成，将参考视频编码为紧凑的记忆token，在DiT自注意力层中与原token拼接作为记忆。在训练中冻结扩散主干，仅训练记忆编码器（约1.5亿参数），用约1万条样本即可；推理时可插拔使用。

Result: 在多种SOTA模型上实验证明：引入DiT-Mem显著提升物理法则遵循与视频保真度，并保持时间一致性；训练高效、数据需求小。

Conclusion: 通过可学习的外部记忆与频域解耦，引导DiT更好利用世界知识，实现低成本、可插拔地提升视频生成的物理与语义合理性与画质。

Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.

</details>


### [290] [IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2511.19235)
*Carl Lindström,Mahan Rafidashti,Maryam Fatemi,Lars Hammarstrand,Martin R. Oswald,Lennart Svensson*

Main category: cs.CV

TL;DR: IDSplat是一种自监督的3D Gaussian Splatting方法，在无需人工标注的情况下，对动态驾驶场景进行实例级分解与可学习的刚体运动轨迹估计，实现高保真重建与静动态解耦。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景重建要么依赖昂贵的人为轨迹标注，要么采用时间变化但无显式对象分解的表示，导致静态与动态元素缠绕、难以分离与模拟；自动驾驶仿真需要可分解、传感器真实感且可扩展的重建方法。

Method: 将动态对象建模为执行刚体变换的连贯实例，而非无结构的时变原语；使用零样本、语言引导的视频跟踪并结合激光雷达将轨迹锚定到3D，通过特征对应估计一致姿态；提出“协调转弯”平滑以获得时间与物理一致的轨迹，缓解姿态错配与跟踪失败；最后联合优化对象姿态与高斯参数，实现实例级3D Gaussian Splatting重建。

Result: 在Waymo Open Dataset上达到有竞争力的重建质量，同时保持实例级分解；方法可在不同序列与视角密度间无需再训练即可泛化，具备大规模自动驾驶应用的实用性。

Conclusion: 通过自监督、实例显式建模与轨迹平滑/联合优化，IDSplat在无需人工标注的前提下实现动态场景的高质量、可分解重建，并具备良好泛化与规模化部署潜力；代码将开源。

Abstract: Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.

</details>


### [291] [Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation](https://arxiv.org/abs/2511.19254)
*Mohamed Rissal Hedna,Sesugh Samuel Nder*

Main category: cs.CV

TL;DR: 在全仿真3D场景中，用可微渲染优化实体对抗补丁，可显著欺骗挂车装载率分类模型：空→满DOS成功率达84.94%，满→空达30.32%。


<details>
  <summary>Details</summary>
Motivation: 物流中用计算机视觉评估挂车占用率，用于计划、路径与计费。然而实体对抗补丁可能被打印并贴在车厢内，带来安全风险。需要评估在物理逼真条件下，这类攻击对占用率分类器的可行性与影响。

Method: 构建全仿真的3D环境，目标是卷积式货舱占用率分类器。以Mitsuba 3可微渲染，在几何、光照、视角多样化下联合优化对抗补丁纹理；并与仅在2D图像上合成的补丁基线比较。评估空↔满两类攻击场景（拒绝服务：空→满；隐匿：满→空），并分析影响因素。

Result: 3D优化补丁显著优于2D合成基线：DOS（空→满）攻击成功率最高达84.94%；隐匿（满→空）较难但仍达30.32%。给出对成功率的影响因素分析。

Conclusion: 在物理逼真、全仿真3D场景中，对抗补丁能有效误导货舱占用率估计，提示自动化物流管线存在安全隐患。建议提升物理鲁棒性并提出未来加固方向。本研究据称为首个针对货舱占用率的3D对抗补丁攻击研究。

Abstract: Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.

</details>


### [292] [LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models](https://arxiv.org/abs/2511.19261)
*Shuai Wang,Daoan Zhang,Tianyi Bai,Shitong Shao,Jiebo Luo,Jiaheng Wei*

Main category: cs.CV

TL;DR: 提出LAST方法，通过在回答前构建“时空视觉思维轨迹”，显著提升通用VLM对3D空间与长视频的理解能力，支持零样本提示与带思维轨迹的微调，在多项基准上取得大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽在常规视觉-语言任务上表现强，但对三维空间理解与长视频推理仍薄弱；现有改进多为任务/架构专用且各自为政，缺乏统一方法可在不改架构、仅用2D输入下同时提升这两类能力。

Method: 提出LAST（LeArn to Think in Space and Time）：在模型最终作答前，引导其在三维空间与时间维度中进行“视觉思维”。具体做法是构建并输入/监督“时空思维轨迹”（从多张2D图像重建或联想形成的3D与时间线索），让VLM在空间与时间上逐步推理。应用于两种设置：1）零样本：对闭源模型通过提示工程触发时空思维；2）微调：用包含时空思维轨迹的数据对通用VLM进行训练。

Result: 在3类空间理解、4类视频理解与3类图像理解基准上带来显著收益。零样本条件下，GPT-4o在EgoSchema上提升15.8%；微调条件下，Qwen2.5-VL-7B在VSI-Bench上提升8.3分。

Conclusion: 让VLM在作答前显式进行“时空视觉思维”可在无需专用架构与仅用2D输入的条件下，同时增强3D空间与长视频理解，具有通用性与可扩展性。

Abstract: Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.

</details>


### [293] [BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment](https://arxiv.org/abs/2511.19268)
*Dewei Zhou,Mingwei Li,Zongxin Yang,Yu Lu,Yunqiu Xu,Zhizhong Wang,Zeyi Huang,Yi Yang*

Main category: cs.CV

TL;DR: 提出BideDPO：一种为条件图像生成中“文本-条件”冲突而设计的双向解耦偏好优化框架，通过解耦偏好对与自适应损失权衡，结合自动化冲突数据管线与迭代优化，显著提升文本对齐与条件遵循（文本成功率+35%等），并在DualAlign与COCO上验证。


<details>
  <summary>Details</summary>
Motivation: 条件图像生成常引入结构/空间/风格先验，但存在两类冲突：1）输入冲突：条件图与文本互相矛盾；2）模型偏置冲突：即使条件与文本一致，生成模型的内在偏置也破坏对齐。传统有监督微调难以提供细粒度的冲突解决；DPO等偏好优化虽有潜力，但面临文本与条件梯度纠缠、以及缺少多约束任务的解耦偏好数据。

Method: 提出BideDPO：1）双向解耦偏好对——分别为“条件遵循偏好对”和“文本对齐偏好对”，减少文本与条件信号的梯度纠缠；2）自适应损失平衡（Adaptive Loss Balancing），动态调节两类偏好对的影响，实现均衡优化；3）自动化数据管线，从模型采样输出并自动标注冲突感知的偏好数据；4）迭代优化策略，将模型更新与数据采样—标注闭环，逐步提升模型与数据质量；5）构建DualAlign基准，用于评估文本-条件冲突的解决能力。

Result: 在DualAlign基准上，BideDPO显著提升文本成功率（如+35%）与条件遵循；同时在COCO数据集上验证了方法的有效性与泛化。

Conclusion: 通过对偏好学习的双向解耦与自适应权衡，并配合自动化、迭代式的数据-模型联动，BideDPO有效缓解文本与条件的冲突，优于标准微调与传统DPO范式，为多约束条件生成提供可扩展的训练框架。

Abstract: Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.

</details>


### [294] [Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection](https://arxiv.org/abs/2511.19274)
*Mingyang Chen,Jiawei Du,Bo Huang,Yi Wang,Xiaobo Zhang,Wei Wang*

Main category: cs.CV

TL;DR: 用扩散模型估计数据似然，通过“部分反向去噪”的重建偏差作为打分，做分布感知的核心集选择；在ImageNet上显著优于现有方法，用50%数据接近全量训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择多依赖启发式信号（训练动态、不确定性），未显式建模数据似然，导致难以捕捉细微但关键的分布结构，影响子集对训练的有效性。

Method: 利用扩散模型的马尔可夫扩散过程与其ELBO建立理论联系，证明样本在“部分反向去噪”下的重建误差可作为数据似然的代理。提出信息论方法自动选择最优重建时间步，使偏差信号稳定可靠；据此为每个样本打分并选出核心集。

Result: 在ImageNet上，基于重建偏差的打分在不同选择比例下稳定优于各基线；只用50%数据即可达到接近全量训练的性能。

Conclusion: 将似然信息引入数据选择能更好反映分布结构与模型学习偏好，重建偏差提供了简单有效、理论支撑的评分准则，并可作为理解数据分布与训练表现关系的工具。

Abstract: Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.

</details>


### [295] [ReMatch: Boosting Representation through Matching for Multimodal Retrieval](https://arxiv.org/abs/2511.19278)
*Qianying Liu,Xiao Liang,Zhiqiang Zhang,Yibo Chen,Xu Tang,Zhongfei Qing,Fengfan Zhou,Yao Hu,Paul Henderson*

Main category: cs.CV

TL;DR: ReMatch 将多模态大模型（MLLM）的“生成能力”引入检索：同一模型既产出嵌入，又以对话式自回归方式判定相关性，显著提升检索与零样本泛化，在 MMEB 上达新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往把 MLLM 当作纯编码器，忽视其生成与组合推理、世界知识，导致对困难负例区分弱、监督稀疏、语义组合性损失。需要一种同时保留生成推理并提供更强判别监督的训练范式。

Method: 1) 端到端训练一个“嵌入-生成一体”的 MLLM；2) 设计聊天式生成匹配阶段：模型在多视角输入（原始多模态数据与其投影嵌入）上自回归地产生相关性判定，形成实例级判别监督，辅以标准对比损失；3) 为每个输入引入多颗可学习 token，得到细粒度、近正交的上下文化多向量嵌入，推理成本低；4) 在强基线之上整合上述要素形成完整训练配方。

Result: 在 Massive Multimodal Embedding Benchmark（MMEB）上取得新的 SOTA；在五个数据集上的零样本泛化尤为突出，显示鲁棒性与可迁移性强；对困难负例提供更强梯度，保留 MLLM 的组合能力。

Conclusion: 将生成式匹配与嵌入学习深度融合，可在不牺牲推理成本的前提下提升多模态检索的判别性与泛化性。ReMatch 证明了利用 MLLM 生成与世界知识进行“自回归相关性决策”的有效性，并给出实用的多向量嵌入训练范式。

Abstract: We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.

</details>


### [296] [DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting](https://arxiv.org/abs/2511.19294)
*Phurtivilai Patt,Leyang Huang,Yinqiang Zhang,Yang Lei*

Main category: cs.CV

TL;DR: 提出“预先致密化”替代3DGS训练中的自适应加密：用稀疏LiDAR与单目深度融合，并以ROI感知采样生成高质量致密点云，减少漂浮伪影、冗余高斯与训练资源，质量接近SOTA且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS依赖训练过程中的自适应密度控制，常产生漂浮伪影、冗余高斯与算力浪费；同时对重要区域的几何/语义保真不足，需一种在初始化阶段即可得到更合理几何分布的方法。

Method: 在优化前进行“致密化”：融合稀疏LiDAR与配对RGB图像的单目深度估计，构建更致密、更准确的初始点云；引入ROI感知采样，优先覆盖语义与几何关键区域；随后在3DGS优化中绕开传统的自适应密度控制，仅优化高斯的其他属性，降低重叠与冗余。

Result: 在四个新采集数据集上，通过对比与消融显示：在保持或接近SOTA视觉质量的同时，显著降低资源开销与训练时间，并减少漂浮伪影与冗余高斯。

Conclusion: 预先致密化结合ROI采样可替代3DGS中的自适应密度控制，在不牺牲视觉质量的前提下提升效率与稳定性，且对复杂场景的关注区域有更好保留。

Abstract: This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.

</details>


### [297] [IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection](https://arxiv.org/abs/2511.19301)
*Johannes Meier,Florian Günther,Riccardo Marin,Oussema Dhaouadi,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出IDEAL-M3D：首个面向单目3D目标检测的实例级主动学习框架；通过多样化快速集成与实例选择，减少标注60%仍达相当或更优AP3D（KITTI验证/测试）。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测部署易但3D标注昂贵；现有主动学习多按“整图”选择且偏向基于不确定性的深度歧义，导致远处目标被过度选择、近处被忽视，标注效率低。

Method: 构建实例级主动学习管线IDEAL-M3D：用异构骨干+任务无关特征、损失权重扰动、时间依赖bagging等，训练显式多样、训练快速的集成模型；以多样性驱动的实例选择替代整图与单纯不确定性选择，优先挑选信息量大的具体目标实例。

Result: 在KITTI上，以仅60%的标注量即可达到与全数据训练相当或更好的AP3D；在验证集与测试集均验证优越性与资源节省。

Conclusion: 实例级、多样性驱动的主动学习优于整图与纯不确定性策略，可有效缓解单目3D检测的标注成本与选择偏置问题。

Abstract: Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.
  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.

</details>


### [298] [Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection](https://arxiv.org/abs/2511.19306)
*Zixuan Wang,Haoran Sun,Jiaming Lu,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Xuelin Qian,Junwei Han*

Main category: cs.CV

TL;DR: 提出DGSPNet：一种端到端、语言提示驱动的红外小目标检测方法，通过双粒度语义提示和文本引导的通道/空间注意力，显著提升三大数据集上的检测精度。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测受限于弱表征与强背景干扰，以致效果不佳。受CLIP启发的现有方法尝试用文本指导，但文本描述不准且依赖人工标注，限制了泛化与实用性。

Method: 构建DGSPNet，融合双粒度语义提示：1) 粗粒度文本先验（如“infrared image”“small target”）；2) 通过视觉到文本映射得到的细粒度个性化描述。利用两种文本提示在推理时无须额外标注即可引导特征学习。进一步提出文本引导通道注意力（TGCA）与文本引导空间注意力（TGSA），在低/高层特征空间提升对潜在目标的响应。

Result: 在三个基准数据集上取得SOTA性能，相比现有方法显著提升检测精度（文中未给出具体数值）。

Conclusion: 语言提示的双粒度融合与文本引导注意力能有效缓解红外小目标检测中的表征不足与背景干扰问题，实现无标注条件下的更强泛化与更高精度。

Abstract: Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.

</details>


### [299] [Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach](https://arxiv.org/abs/2511.19316)
*Xincheng Wang,Hanchi Sun,Wenjun Sun,Kejun Xue,Wangqiu Zhou,Jianbo Zhang,Wei Sun,Dandan Zhu,Xiongkuo Min,Jun Jia,Zhijun Fang*

Main category: cs.CV

TL;DR: 论文提出针对扩散模型微调数据集水印的一套统一威胁模型与评测框架，系统评估现有方法的通用性、可传递性与鲁棒性，并揭示其在真实威胁场景下的脆弱性；同时给出一种可在不影响微调效果的前提下彻底移除水印的实用方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型微调可再现特定人脸或风格，带来版权与安全隐患。为确保可追溯性，社区提出在训练集植入“数据集水印”，期望微调后生成结果仍可检测。但缺乏统一威胁模型与评测基准，难以客观比较方法并理解其在真实攻击下的可靠性。

Method: 1) 建立通用威胁模型，明确攻击者能力与目标；2) 提出三维评测框架：通用性（不同数据/模型/任务适用）、可传递性（微调后生成是否保留水印）、鲁棒性（对常见图像处理与对抗操作的抵抗力）；3) 系统实验评估现有数据集水印方法；4) 设计一种实用的水印移除流程，在不损伤微调质量的情况下清除水印信号。

Result: 现有方案在通用性与可传递性上表现较好，对常规图像处理具有一定鲁棒性，但在更贴近真实世界的威胁场景下（更强攻击者、更复杂流程）容易失效。所提移除方法能彻底消除数据集水印，同时保持微调的再现能力。

Conclusion: 仅依赖当前数据集水印难以可靠追踪微调扩散模型来源。需要面向实际威胁的更强鲁棒机制与标准化评测；论文提出的框架为今后方法改进与公平比较提供基础，并通过移除攻击揭示现有方案的关键缺陷。

Abstract: Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.

</details>


### [300] [SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis](https://arxiv.org/abs/2511.19319)
*Lingwei Dang,Zonghan Li,Juntong Li,Hongwen Zhang,Liang An,Yebin Liu,Qingyao Wu*

Main category: cs.CV

TL;DR: 提出SyncMV4D：首个联合生成同步多视角HOI视频与4D运动的模型，通过闭环联合扩散与点轨对齐，实现更高的真实感、可动性与多视一致性。


<details>
  <summary>Details</summary>
Motivation: 单视角视频生成难以感知完整3D几何，易出现畸变与不合理动作；现有3D HOI方法依赖实验室级高质量3D数据，泛化差。需要能在真实场景下同时兼顾视觉质量、运动动力学与多视几何一致性的生成方法。

Method: 提出统一视觉先验、运动动力学和多视几何的框架SyncMV4D：1) 多视联合扩散MJD，共同生成多视HOI视频与中间运动表征；2) 扩散点对齐DPA，将粗糙中间运动精炼为全局对齐的4D度量点轨。通过闭环互促：扩散去噪中，生成视频用于约束4D运动精炼；对齐后的4D点轨再投影，反过来引导下一步联合生成。

Result: 在多项评测中，相较SOTA表现更优，体现在视觉真实感、运动合理性以及多视一致性方面。

Conclusion: 联合生成与闭环互导机制有效耦合2D外观与4D动态，缓解单视几何不足与3D数据依赖问题，为真实场景HOI的多视视频与4D运动生成提供了高质量与一致性的解决方案。

Abstract: Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.

</details>


### [301] [SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation](https://arxiv.org/abs/2511.19320)
*Jiaming Zhang,Shengming Cao,Rui Li,Xiaotong Zhao,Yutao Cui,Xinglin Hou,Gangshan Wu,Haolan Chen,Yu Xu,Limin Wang,Kai Ma*

Main category: cs.CV

TL;DR: SteadyDancer是一种面向人像动画的I2V框架，通过“条件协调+姿态协同调制+分阶段解耦训练”三管齐下，首次稳健地保持首帧身份，同时实现精确运动控制与高时序一致性，资源消耗更低，性能SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有R2V范式在真实场景中存在图像与运动条件的时空错配，导致身份漂移、伪影与控制不准。需要一种能在精确运动驱动下仍严格保持首帧外观身份、并兼顾时序稳定与训练效率的方法。

Method: 1) 条件协调机制：调和参考图像与运动条件这两类相互冲突的约束，确保控制精度与外观保真兼得；2) 协同姿态调制模块：生成对参考图像高度兼容的自适应姿态表征，缓解错配并提升连贯性；3) 分阶段解耦目标训练：分层次优化运动保真、视觉质量与时间一致性，逐步收敛。整体基于I2V范式而非R2V。

Result: 在外观保真与运动控制两方面达到SOTA，同时在训练资源需求上显著低于同类方法。实验表明动画连贯、身份保持稳定，伪影减少。

Conclusion: 通过从范式与训练流程的系统性设计，SteadyDancer有效解决R2V中的时空错配问题，实现稳健的首帧身份保持与精确运动控制，为人像动画提供更高质量且更高效的方案。

Abstract: Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.

</details>


### [302] [MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation](https://arxiv.org/abs/2511.19326)
*Farnoosh Koleini,Hongfei Xue,Ahmed Helmy,Pu Wang*

Main category: cs.CV

TL;DR: MonoMSK提出一种将数据驱动学习与物理仿真结合的单目视频3D人体运动重建框架，利用肌骨模型同时恢复运动学与动力学，并通过物理一致性循环与新型前向-逆向一致性损失提升生物力学真实性与准确度，在多个数据集上优于SOTA并首次实现精确单目动力学估计。


<details>
  <summary>Details</summary>
Motivation: 现有单目方法多依赖简化、解剖不准确的SMPL类模型且忽略物理约束，只能得到运动学，缺乏对力与力矩的可靠估计，限制了生物力学可信度；而标记点系统昂贵、受限于实验室。亟需一种既高保真、又可从单目视频恢复动力学的方案。

Method: 构建名为MonoMSK的混合框架：1) 使用基于Transformer的逆动力学网络从视频估计关节/肌肉相关的动力学量；2) 将可微的前向运动学与动力学层（基于ODE仿真）集成，形成“物理调控的逆-前向闭环”；3) 通过前向-逆向一致性损失，将重建的运动与底层动力学推理对齐，保证因果与物理可行；采用解剖准确的肌骨模型而非SMPL。

Result: 在BML-MoVi、BEDLAM、OpenCap上，运动学准确性显著超过现有方法；并首次在单目设定下实现精确的力与力矩（动力学）估计。

Conclusion: 将学习与可微物理仿真耦合、并使用肌骨级模型，可从单目视频联合恢复可信的运动学与动力学；物理一致性约束是提升生物力学真实性与泛化能力的关键。

Abstract: Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.

</details>


### [303] [POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse](https://arxiv.org/abs/2511.19339)
*Anjie Le,Can Peng,Yuyuan Liu,J. Alison Noble*

Main category: cs.CV

TL;DR: 提出POUR方法，在表示层面进行可证最优的机器遗忘，通过几何投影（闭式POUR-P与蒸馏版POUR-D）实现对目标概念的有效忘却，同时保持其余知识，实验在CIFAR-10/100与PathMNIST上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘多仅调整分类器，内部表示未被有效清除，导致遗忘不彻底；缺乏对表示层遗忘-保留权衡的理论刻画与可度量指标。

Method: 基于Neural Collapse与ETF几何结构，推导出忘却效能、保留保真度与类别分离的三元权衡；证明简单x正交投影能将ETF在低维仍保持ETF，从而得到最优忘却算子。提出表示遗忘评分RUS，用于量化表示层的遗忘与保留。给出两种实现：闭式几何投影POUR-P，以及结合特征蒸馏的特征级遗忘POUR-D。

Result: 在CIFAR-10/100与PathMNIST上，POUR能有效清除目标概念影响，同时保持非遗忘类别的表示结构与分类性能，在分类层与表示层指标（含RUS）上均优于现有方法。

Conclusion: 表示层遗忘可通过ETF正交投影实现可证最优的几何解；RUS提供了统一评估框架；POUR在实践中兼顾遗忘与保留，推动机器遗忘从分类器层面走向表示层与理论可证范式。

Abstract: In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.

</details>


### [304] [Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning](https://arxiv.org/abs/2511.19343)
*Qihan Huang,Haofei Zhang,Rong Wei,Yi Wang,Rui Tang,Mingli Song,Jie Song*

Main category: cs.CV

TL;DR: 提出Syn-GRPO：在GRPO强化学习框架中引入在线数据合成服务器，生成可诱发多样化回答的高质量样本，从而提升MLLM感知能力与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有针对MLLM感知能力的RL方法受限于训练数据质量：样本难以诱发多样化响应，导致探索不足；以熵约束等手段仅治标不治本，缺少从数据源头提高多样性的机制。

Method: 在GRPO中集成“数据服务器+GRPO工作流”的两段式体系：数据服务器用图像生成模型从现有样本合成新样本，采用解耦、异步的高效生成；GRPO端产出新的图像描述并提供给数据服务器，同时引入“多样性奖励”监督MLLM预测图像描述，从而驱动生成多样响应的数据。

Result: 在三项视觉感知任务上，Syn-GRPO显著提升数据质量并优于现有MLLM感知方法，展示出在长期自进化RL上的可扩展潜力。

Conclusion: 通过在线合成高质量且能诱发多样响应的数据，并配合多样性奖励的GRPO训练，Syn-GRPO从根源缓解数据同质化与探索不足问题，带来显著性能提升与良好扩展前景。

Abstract: RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.

</details>


### [305] [CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting](https://arxiv.org/abs/2511.19351)
*Abdurahman Ali Mohammed,Catherine Fonder,Ying Wei,Wallapak Tavanapong,Donald S Sakaguchi,Qi Li,Surya K. Mallapragada*

Main category: cs.CV

TL;DR: 提出一个包含3,023张免疫细胞化学图像、约43万细胞点标注的大规模细胞计数数据集；基于此对回归、群体计数与细胞计数方法进行基准评测，并将SAM改造成密度图计数器（SAM-Counter），在测试集上达到MAE 22.12，优于现有方法（第二名MAE 27.46）。


<details>
  <summary>Details</summary>
Motivation: 手工细胞计数费时易错，深度学习自动计数依赖高质量大数据，但现有数据集规模小（常少于500张），且细胞形态复杂、密度高、染色多样，限制了算法可靠性与泛化能力。

Method: 1) 构建并发布大规模点标注数据集（3,023张、>430k细胞），涵盖高密度、重叠、多形态、长尾计数分布与染色差异；2) 设计系统性基准：评测回归式、群体计数与细胞计数三类方法，测试图像计数范围10–2,126；3) 探索仅用点标注将SAM用于显微细胞计数，提出基于密度图的SAM改造（SAM-Counter）。

Result: 在该基准上，SAM-Counter实现MAE 22.12，优于现有方法（第二名MAE 27.46）。实验展示了在高密度、长尾和染色变化场景下的鲁棒性与优势。

Conclusion: 该数据集与基准为自动细胞计数研究提供了关键资源与统一评测框架；SAM的密度图改造在点标注条件下效果领先，表明大模型与密度估计结合是提升显微计数性能的有效方向。

Abstract: Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.

</details>


### [306] [Growing with the Generator: Self-paced GRPO for Video Generation](https://arxiv.org/abs/2511.19356)
*Rui Li,Yuanzhi Liang,Ziqi Ni,Haibing Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出Self-Paced GRPO：让奖励模型与视频生成器共同进化，按能力自适应地从粗到细调整奖励重点，提升稳定性与效果。


<details>
  <summary>Details</summary>
Motivation: 现有用于视频生成后训练的GRPO依赖静态、容量固定且在训练中冻结的奖励模型。随着生成器变强，这类奖励会分布外偏置、迅速饱和，导致奖励-策略不匹配和被利用，限制了对齐与稳定性。

Method: 引入“自定进度”的能力感知GRPO：奖励反馈与生成器同步演化，采用渐进式奖励机制。训练早期更关注粗粒度视觉保真，随后逐步提高对时间一致性和细粒度文-视语义对齐的权重，形成从易到难的奖励课程，缓解奖励饱和与投机。

Result: 在VBench上、跨多种视频生成骨干网络，一致性提升视觉质量与语义对齐，相比使用静态奖励的GRPO基线获得更稳定、更优的指标表现。

Conclusion: 自定进度的GRPO通过动态课程化奖励，减少奖励-策略错配和奖励漏洞利用，提升视频生成后训练的稳定性与泛化，具有方法通用性与有效性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.

</details>


### [307] [DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation](https://arxiv.org/abs/2511.19365)
*Zehong Ma,Longhui Wei,Shuai Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 提出一种频率解耦的端到端像素扩散框架DeCo：用DiT专注低频语义、轻量像素解码器补高频细节，并配合频率感知的flow-matching损失，显著提速提效，ImageNet上FID达1.62/2.22，逼近潜变量扩散。


<details>
  <summary>Details</summary>
Motivation: 现有像素扩散需在同一DiT中同时建模低频语义与高频细节，导致训练/推理缓慢、效率低，难以与潜变量扩散在性能与速度上竞争。希望在保持像素域端到端优势与更高模型容量的同时，提升效率与质量，缩小与latent diffusion的差距。

Method: 频率解耦生成：1) 让主干DiT仅建模低频语义表示；2) 设计轻量像素解码器，接收来自DiT的语义引导，负责生成高频细节；3) 提出频率感知的flow-matching损失，对视觉显著频段赋予更大权重，抑制不重要频率噪声；整体形成端到端的像素扩散训练与推理流程。

Result: 在ImageNet上实现像素扩散中的领先结果：FID 1.62（256×256）与2.22（512×512），训练与推理更高效；在文本到图像任务上，预训练模型在GenEval系统级对比的总体得分达0.86；代码已开源。

Conclusion: 通过频率解耦与频率感知损失，DeCo使像素扩散在保持端到端像素空间优势的同时显著提升效率与质量，性能逼近甚至缩小与潜变量扩散的差距，并展现良好的通用性（包括T2I）。

Abstract: Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.

</details>


### [308] [An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification](https://arxiv.org/abs/2511.19367)
*Saniah Kayenat Chowdhury,Rusab Sarmun,Muhammad E. H. Chowdhury,Sohaib Bassam Zoghoul,Israa Al-Hashimi,Adam Mushtak,Amith Khandakar*

Main category: cs.CV

TL;DR: 提出一个结合分割与规则的混合管线：先精准分割肺部与周围解剖结构，再量化肿瘤尺寸与邻近距离，最后按指南做规则化分期，优于端到端分类，准确率91.36%，各T期F1为0.93/0.89/0.96/0.90，兼具可解释性。


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习对肺癌分期忽视空间与解剖信息，而TNM分期强依赖肿瘤最大径与与关键解剖结构的距离，细微差异即可能改变分期，需一个嵌入显式临床上下文的方案。

Method: 构建医学驱动的混合流程：1) 使用专用编码器-解码器网络分别分割肺叶、肿瘤、纵隔、膈肌等；2) 基于分割掩膜定量提取肿瘤属性，包括最大径与与邻近解剖结构的最小距离；3) 依据临床指南进行基于规则的T分期判定。

Result: 在Lung-PET-CT-Dx数据集上，整体分类准确率91.36%；分期F1：T1=0.93、T2=0.89、T3=0.96、T4=0.90；相较传统端到端模型表现更优。

Conclusion: 将显式临床上下文与量化测量嵌入分期流程，兼具SOTA性能与透明可解释决策，首次实现基于显式解剖-度量驱动的肺癌T分期自动化。

Abstract: Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable "black box" manner, our method offers both state-of-the-art performance and transparent decision support.

</details>


### [309] [UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval](https://arxiv.org/abs/2511.19380)
*Maroun Ayli,Youssef Bakouny,Tushar Sharma,Nader Jalloul,Hani Seifeddine,Rima Kilany*

Main category: cs.CV

TL;DR: 论文提出将UI截图转换为属性图，并用对比图自动编码器学习结构化嵌入，在大规模企业软件UI检索上优于主流视觉编码器，支撑多模态可组合查询（Top-5准确率0.92，毫秒级延迟）。


<details>
  <summary>Details</summary>
Motivation: 企业级软件拥有成千上万界面屏幕，跨产品与版本的一致性、模式发现与合规检查困难。现有方法依赖视觉相似或文本语义，缺乏对界面结构属性的显式建模，难以做细粒度区分与复杂检索。

Method: 将UI截图解析为层次与空间关系的属性图；提出对比图自动编码器，联合对齐视觉、结构、语义多层相似性，学习结构化嵌入；在UISearch中将结构嵌入与语义检索通过可组合查询语言融合，并采用混合索引架构以支持复杂查询与规模化。

Result: 在20,396个金融软件UI数据集上，UISearch取得Top-5准确率0.92，P50延迟47.5ms（P95: 124ms），可扩展至2万+屏幕；结构嵌入在判别力上优于SOTA视觉编码器，实现细粒度UI区分。

Conclusion: 显式结构建模+对比图自编码的嵌入显著提升UI表示的表达力，结合语义检索与可组合查询，支持高效且可扩展的复杂UI搜索与合规/一致性分析，优于仅依赖视觉的方法。

Abstract: Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.

</details>


### [310] [BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation](https://arxiv.org/abs/2511.19394)
*Rachit Saluja,Asli Cihangir,Ruining Deng,Johannes C. Paetzold,Fengbei Liu,Mert R. Sabuncu*

Main category: cs.CV

TL;DR: 该文提出用细粒度“背景拆分”（BackSplit）来提升小病灶分割：把传统二分类中的整体背景细分为若干解剖学类别，训练时利用这些辅助标签，但推理阶段仍保持原有开销不变；理论上提升费舍信息、经验上多数据集显著增益。


<details>
  <summary>Details</summary>
Motivation: 小病灶在医学影像中体积小、对比低、形态多变，容易被误判为背景。现有方法多从网络结构、损失、增强或数据量入手，却忽视了“背景”内部的强异质性（器官、组织、结构）。若把所有非病灶像素都当成一个类，模型难以学到与病灶可区分的上下文线索。

Method: 提出BackSplit：在训练阶段将原本的“背景”细分为多个语义子类（如器官/组织），这些细粒度标签可来自人工标注或利用现成预训练分割模型、交互式分割框架自动生成；以多类训练以注入丰富上下文；但推理阶段保持对病灶的二分类输出，不增加推理成本。并从信息论角度证明：与二分类相比，BackSplit能提升期望费舍信息，带来更紧的渐近误差界和更稳定的优化。

Result: 跨多数据集、多网络架构的实验显示：BackSplit在小病灶分割上稳定提升性能；即便辅助标签由预训练模型或交互式分割自动生成，仍能获得一致增益，且无需增加推理时复杂度。

Conclusion: 细粒度背景建模是提升小病灶分割的简单而通用范式。BackSplit通过训练时的背景拆分增强信息量与优化稳定性，在不增加推理成本的前提下显著提升小目标性能，具有鲁棒性与广泛适用性。

Abstract: Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single "background" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.
  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.

</details>


### [311] [In-Video Instructions: Visual Signals as Generative Control](https://arxiv.org/abs/2511.19401)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CV

TL;DR: 论文提出“视频内指令”范式：把用户意图以可视元素（文本、箭头、轨迹等）直接嵌入帧中，引导大规模视频生成模型进行可控的图像到视频生成，验证在多对象场景中有效。


<details>
  <summary>Details</summary>
Motivation: 现有以文本提示为主的控制方式往往全局、粗粒度且存在语义歧义，难以对复杂场景中多个对象进行精确、空间感知的动作指定。作者观察到最新大规模视频生成模型具备较强的物理与逻辑预测能力，因而探索能否通过“把指令变成画面的一部分”来更细粒度地控制生成。

Method: 提出“In-Video Instruction（视频内指令）”：在输入图像/帧中直接叠加可视信号（如文字、箭头、轨迹）来表达对不同对象的独立行动要求；将该范式应用并测试于三种SOTA视频生成模型（Veo 3.1、Kling 2.5、Wan 2.2），评估模型对这些视觉指令的解析与执行能力，尤其在多对象复杂场景下。

Result: 实验证明这些视频模型能够可靠地解析并执行嵌入帧中的视觉指令，在复杂多对象情形中表现尤为稳定，体现出明确的主客体对应与空间感知控制能力。

Conclusion: 将控制信号可视化并直接注入输入帧，可实现比文本提示更精细、明确、空间对齐的图像到视频控制；现有大模型已具备理解和执行此类“视频内指令”的能力。

Abstract: Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.

</details>


### [312] [Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens](https://arxiv.org/abs/2511.19418)
*Yiming Qin,Bomin Wei,Jiaxin Ge,Konstantinos Kallidromitis,Stephanie Fu,Trevor Darrell,Xudong Wang*

Main category: cs.CV

TL;DR: 提出COVT（Chain-of-Visual-Thought），在VLM中引入连续视觉token进行“视觉思维”，以小-token预算蒸馏多种感知专家知识，显著提升多模态感知与推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM擅长语言推理但缺乏致密视觉感知能力（如空间与几何推理），主要因其难以在空间维度捕获高密度视觉信息。需要一种既高效又能表达致密视觉线索的中间表征与推理机制。

Method: 构建连续视觉token作为紧凑潜在表征，在约20个token预算内从轻量视觉专家（2D外观、3D几何、空间布局、边缘结构等）蒸馏知识；训练时让VLM自回归预测这些视觉token以重建深度、分割、边缘、DINO特征等致密监督信号；推理时在连续视觉token空间中推理，必要时可解码为致密预测以增强可解释性，同时保持效率。

Result: 将COVT集成到Qwen2.5-VL、LLaVA等强VLM后，在CV-Bench、MMVP、RealWorldQA、MMStar、WorldMedQA、HRBench等10余个感知基准上带来3%~16%的稳定提升，并提供更精确与扎实的多模态推理表现。

Conclusion: 紧凑的连续视觉token使VLM具备“视觉思维”，在不显著增加计算的情况下提升空间/几何等致密感知与可解释性，是增强VLM感知-推理一体化能力的有效范式。

Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.

</details>


### [313] [SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation](https://arxiv.org/abs/2511.19425)
*Tianrun Chen,Runlong Cao,Xinda Yu,Lanyun Zhu,Chaotao Ding,Deyi Ji,Cheng Chen,Qi Zhu,Chunyan Xu,Papa Mao,Ying Zang*

Main category: cs.CV

TL;DR: SAM3-Adapter 是为 Segment Anything 3 量身定制的适配器框架，在保持更低计算开销的同时，大幅提升在细粒度与低层次分割任务（如医学、伪装/隐匿目标、细胞、阴影）的精度与鲁棒性，刷新多项下游任务 SOTA，并延续模块化、可组合的设计以增强泛化与任务适配能力。


<details>
  <summary>Details</summary>
Motivation: SAM/SAM2 在通用分割上具备高适应性，但在细粒度、低层次的难例场景（医学分割、伪装目标、细胞分割、阴影检测等）表现欠佳。作者此前提出的 SAM-Adapter 曾显著改进这些任务。随着架构与训练流程更优的 SAM3 出现，亟需一个专为 SAM3 设计的适配器以充分释放其潜力并兼顾效率。

Method: 提出 SAM3-Adapter：遵循原 SAM-Adapter 的模块化与可组合理念，为 SAM3 设计轻量适配模块，在不显著增加计算的前提下，对特征表征与任务特化进行高效注入/调优；通过与 SAM3 深度耦合的训练与推理策略，提升对低层次细节与多样任务的适配能力。提供代码、预训练模型与数据流水线。

Result: 在多项下游任务（医学影像、伪装/隐匿目标分割、阴影检测等）上，SAM3-Adapter 持续超越基于 SAM 与 SAM2 的方案，达到新的 SOTA；同时降低计算开销，展现更高的精度、鲁棒性与效率。

Conclusion: SAM3-Adapter 解锁了 SAM3 在精细分割场景的能力，以更强泛化性与任务适配性显著提升分割精度并降低计算成本，可作为未来研究与实际应用的基础方案。

Abstract: The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.

</details>


### [314] [Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction](https://arxiv.org/abs/2511.19426)
*Yun Zhou,Yaoting Wang,Guangquan Jie,Jinyu Liu,Henghui Ding*

Main category: cs.CV

TL;DR: Ref-SAM3D 在 SAM3D 的基础上加入文本描述作为高层先验，实现从单张 RGB 图像进行文本引导的三维重建，在零样本场景下取得高保真、具竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有 SAM3D 虽然重建能力强，但无法按自然语言指示重建“特定目标”，限制了在 3D 编辑、游戏开发与虚拟环境等需要“指代/检索+重建”的实际应用中的可用性。

Method: 提出 Ref-SAM3D：在 SAM3D 框架上融入文本先验，将自然语言作为参考条件，结合单视角 2D 视觉线索，实现文本引导的单张图像到 3D 重建。总体为简单扩展，强调参考/指代能力增强。

Result: 大量定性实验表明，仅用自然语言与单视图即可获得高保真、具竞争力的零样本三维重建效果，表现显示文本与视觉线索有效对齐并提升可控性。

Conclusion: Ref-SAM3D 将文本指代与 2D 视觉线索结合，弥合从 2D 到 3D 的理解鸿沟，为参考引导的三维重建提供更灵活、易用的范式；代码已开源，便于复现与扩展。

Abstract: SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.

</details>


### [315] [Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution](https://arxiv.org/abs/2511.19430)
*Dingkang Liang,Cheng Zhang,Xiaopeng Xu,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: 提出ORS3D任务与ORS3D-60K数据集，并发布多模态LLM框架GRANT，通过并行化子任务实现更高效的具身任务调度与3D落地，实验显示在理解、定位与调度效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能任务规划多忽略运筹学（OR）中的并行调度与总时长最小化目标，也弱化了真实3D场景中的空间落地，导致“能做但不高效”，与现实效率需求脱节。

Method: 1) 定义ORS3D任务：在自然语言指令与3D场景约束下，结合OR思想最小化完工时间（允许设备/子任务并行，如加热与清洁并行）。2) 构建ORS3D-60K：涵盖4K真实场景、60K复合任务，包含语言、3D标注与调度监督。3) 提出GRANT：具身多模态LLM，引入简单有效的“调度token”机制，生成可并行的任务计划与具身动作序列，同时进行3D grounding。

Result: 在ORS3D-60K上，大量实验显示GRANT在语言理解、3D定位与调度效率（更短完工时间、利用并行性）方面优于基线方法。

Conclusion: 将运筹学调度思想引入具身智能可显著提升执行效率；ORS3D与ORS3D-60K为研究提供基准；GRANT用调度token实现高效计划与落地，验证了多模态LLM在并行调度与3D grounding上的潜力。

Abstract: Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT

</details>


### [316] [Cloud4D](https://arxiv.org/abs/2511.19431)
*Jacob Lin,Edward Gryspeerdt,Ronald Clark*

Main category: cs.CV

TL;DR: Cloud4D提出首个仅用多台地基同步相机重建物理一致的四维（3D空间+时间）云场的方法，在25 m空间、5 s时间分辨率下估计液态含水量，并由时序跟踪推断水平风矢量，精度与雷达对比相对误差<10%，时空分辨率较卫星提升约一个数量级。


<details>
  <summary>Details</summary>
Motivation: 千米级全球数值模式难以解析单体云与极端降水、阵风、湍流、地表辐照等关键近端过程；向高分辨率发展需要高分辨率实测资料，而现有仪器（如卫星、雷达）在时空分辨率、覆盖和成本上受限。亟需一种低成本、可部署、能提供高时空分辨率且物理一致的云三维观测方法。

Method: 提出Cloud4D：利用多台仰视地基相机的同步图像，通过“单应性引导的2D到3D Transformer”从多视角亮度信息重建三维液态含水量（LWC）体素场（25 m/5 s）。模型在物理一致性约束下进行训练/推断；随后在时间维跟踪3D LWC体场以估计水平风矢量。部署包含6台天空相机，连续两个月运行。

Result: 在两个月、六相机场景中，相比先进卫星产品实现约一个数量级的时空分辨率提升；与同地点雷达对比的相对误差为个位数（<10%）。同时可产出水平风矢量场。代码与数据已开源。

Conclusion: Cloud4D验证了仅凭地基相机即可重建高分辨率、物理一致的4D云场并估计风场，显著弥补卫星/雷达在近地高时空观测的不足，为高分辨率天气与气候建模、极端事件监测与再分析提供新型数据源。

Abstract: There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.

</details>


### [317] [Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts](https://arxiv.org/abs/2511.19434)
*Yasin Esfandiari,Stefan Bauer,Sebastian U. Stich,Andrea Dittadi*

Main category: cs.CV

TL;DR: 提出一种无需再训练的“专家切换”采样方法：在扩散去噪轨迹的高噪声段用提升感知质量的专家，低噪声段改用提升似然的专家，从而同时改善图像质量与似然。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在感知质量与数据似然间存在权衡：强调高噪声去噪的训练带来更逼真图像但似然差；面向似然的训练偏重低噪声步骤导致视觉质量下降。需要一种方法在不再训练的前提下，兼得两者优势。

Method: 采用“即插即用”的采样方案，结合两个已训练好的扩散专家：在去噪初期（高噪声）使用图像质量专家以塑造全局结构；在后期（低噪声）切换到似然专家以校正像素统计。唯一超参是中间的切换步，无需微调或再训练。

Result: 在 CIFAR-10 与 ImageNet32 上，合并模型稳定地匹配或优于各自基线：相较任一单一专家，能同时提升或保持数据似然与样本感知质量。

Conclusion: 沿噪声尺度进行专家切换是一种打破扩散模型似然-质量权衡的有效策略，且实现简单、代价低、可直接用于现有模型。

Abstract: Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.

</details>


### [318] [Are Image-to-Video Models Good Zero-Shot Image Editors?](https://arxiv.org/abs/2511.19435)
*Zechuan Zhang,Zhenyuan Chen,Zongxin Yang,Yi Yang*

Main category: cs.CV

TL;DR: 提出 IF-Edit：无需微调，把图生视频扩散模型改造成指令驱动图像编辑器；通过提示增强、时间潜变量丢弃和自一致后期精修，提升推理型编辑效果与效率。


<details>
  <summary>Details</summary>
Motivation: 大规模视频扩散模型具备世界模拟与时间推理优势，但作为零样本图像编辑器的潜力尚未系统探索；现存挑战包括编辑指令与模型时序对齐差、冗余时间潜变量导致低效与漂移、后段帧模糊。

Method: IF-Edit 的三模块：1) 链式思维提示增强，把静态编辑指令转为具时序推理的提示；2) 时间潜变量 dropout，在“专家切换点”之后压缩帧潜变量以加速去噪并保持语义/时序一致；3) 自一致后期精修，用短“静态视频”轨迹锐化后段帧。

Result: 在四个公开基准（含非刚性编辑、物理与时间推理、通用指令编辑）上，IF-Edit 在推理型任务显著领先，并在通用编辑上具竞争力；同时加速去噪并改善后期清晰度与一致性。

Conclusion: 视频扩散模型可无缝复用为图像编辑器；简单的提示增强+时间潜变量压缩+自一致精修形成统一的视频-图像生成式推理配方，兼顾效果与效率。

Abstract: Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.

</details>


### [319] [VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection](https://arxiv.org/abs/2511.19436)
*Qiang Wang,Xinyuan Gao,SongLin Dong,Jizhou Han,Jiangyang Li,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: VDC-Agent 是一个无需人工标注与大型教师模型的视频细粒度描述自演化框架，通过“生成-原则打分与建议-提示优化”的闭环迭代，自动构建偏好数据并微调多模态大模型，取得VDC基准SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前视频细粒度描述依赖人工标注或更大教师模型监督，成本高、可扩展性差。作者希望在无标注、无教师的条件下，通过自监督与偏好优化生成高质量视频描述，并稳定提升中小规模MLLM的能力与鲁棒性。

Method: 提出VDC-Agent：1) 生成候选描述；2) 基于原则的打分器同时给出数值评分与文本建议；3) 利用建议改写提示继续生成；4) 若质量退化，沿用先前思路的自反思路径修正更新；5) 在未标注视频上形成(描述, 评分)轨迹，转为偏好三元组；6) 过滤JSON解析错误样本得到VDC-Agent-19K数据集；7) 对基础模型采用由易到难的Curriculum DPO微调。基座为Qwen2.5-VL-7B-Instruct，产出VDC-Agent-7B。

Result: 构建了包含18,886对自动偏好样本的VDC-Agent-19K；在VDC基准上，VDC-Agent-7B取得平均准确率49.08%、平均分2.50，较基座提升+5.13%准确率与+0.27分，推理成本相近，并超越专用视频描述器。

Conclusion: 无需人工标注与大型教师模型，通过闭环自演化与偏好优化即可有效提升视频细粒度描述性能；小型MLLM经此流程能在标准基准上达到SOTA，具备可扩展性与成本优势。

Abstract: We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.

</details>


### [320] [LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context](https://arxiv.org/abs/2511.19437)
*Jingzhi Bao,Hongze Chen,Lingting Zhu,Chenyu Liu,Runze Zhang,Keyang Luo,Zeyu Hu,Weikai Chen,Yingda Yin,Xin Wang,Zehong Lin,Jun Zhang,Xiaoguang Han*

Main category: cs.CV

TL;DR: LumiTex 提出一个端到端的 PBR 纹理生成与补全框架，解决从有限光照的图像提示中分解材质参数，以及无缝、视角一致的纹理补全两大难题，在纹理质量上超过现有开源与商业方法。


<details>
  <summary>Details</summary>
Motivation: 现有 PBR 纹理生成在两方面薄弱：一是从单/少量视图、光照信息有限的图像提示中准确分解材质（反照率、金属度、粗糙度）；二是在 UV/多视图上的纹理补全常出现接缝、失真与视角不一致。

Method: 提出 LumiTex，包括三部分：1) 多分支生成：在共享光照先验下解耦反照率与金属-粗糙度分支，增强材质理解与稳健分解；2) 感光材质注意力：在解码阶段显式注入光照上下文，物理约束地产生反照率、金属度、粗糙度贴图；3) 几何引导修补：基于大规模视图合成模型进行纹理补洞，利用几何与多视图一致性实现无缝、视角一致的 UV 完成。

Result: 在多项实验中，LumiTex 的纹理质量达到 SOTA，优于主流开源与商业方案；实现更准确的材质分解与更高的视角一致性与无缝性。

Conclusion: 通过光照感知的解耦生成与几何引导的多视图补全，LumiTex 有效解决 PBR 材质分解与无缝视角一致纹理生成难题，为真实感材质重建提供了更稳健的端到端方案。

Abstract: Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.

</details>
